{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d0d1bf-f8cc-4712-9fb2-0f02bb37c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d5d83a-b78f-4298-a660-a558e1915ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e215ae45-1f94-4599-8ead-b4673391b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4290, 256, 768) (4290,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last2, wav2vec_last3, wav2vec_last4),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last2,label_last3,label_last4))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a89c90-bd1d-46c1-8698-811352539831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6014da-5c64-42e6-bd6a-e8b71c9a05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eea6d79-deec-4a3f-befe-47e98a9ab069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        #先将数据流降维再放入模型中训练\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0)  \n",
    "            pred_var=prediction.var(axis=0)   \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)    \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32eda317-debf-41f8-b6e2-ae40c5936696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a888b635-0561-4c16-acfa-da202e0cf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last5], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last5\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76dd3aa0-3567-4e21-a65f-11a769be158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1737b15-7242-4003-94fb-27ceb4bf2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ebcacb9-fdf9-4761-a4e8-a2b3f89c5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4290,\n",
    "    dataset_test_size=1241,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=4789,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        total_acc = 0\n",
    "        total_num = total_size\n",
    "        predicted_class = jnp.argmax(pred, axis=1)\n",
    "        total_acc += jnp.sum(predicted_class == label_i)\n",
    "        return total_acc / total_num\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac4cf4ab-5120-48d4-891f-e39f9028ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.4018158912658691, Accuracy: 0.25, Computation time: 13.267816543579102\n",
      "Step: 1, Loss: 1.3236908912658691, Accuracy: 0.4375, Computation time: 2.965935230255127\n",
      "Step: 2, Loss: 1.4229700565338135, Accuracy: 0.25, Computation time: 3.218531847000122\n",
      "Step: 3, Loss: 1.3986167907714844, Accuracy: 0.21875, Computation time: 2.9299488067626953\n",
      "Step: 4, Loss: 1.3491419553756714, Accuracy: 0.3125, Computation time: 3.2408111095428467\n",
      "Step: 5, Loss: 1.3611093759536743, Accuracy: 0.375, Computation time: 3.056216239929199\n",
      "Step: 6, Loss: 1.3709213733673096, Accuracy: 0.25, Computation time: 3.7086267471313477\n",
      "Step: 7, Loss: 1.3752771615982056, Accuracy: 0.3125, Computation time: 2.7546262741088867\n",
      "Step: 8, Loss: 1.3587499856948853, Accuracy: 0.1875, Computation time: 4.468810081481934\n",
      "Step: 9, Loss: 1.3004024028778076, Accuracy: 0.625, Computation time: 3.5245232582092285\n",
      "Step: 10, Loss: 1.3185092210769653, Accuracy: 0.375, Computation time: 2.7698097229003906\n",
      "Step: 11, Loss: 1.3758091926574707, Accuracy: 0.4375, Computation time: 2.7918431758880615\n",
      "Step: 12, Loss: 1.3596196174621582, Accuracy: 0.4375, Computation time: 3.0247862339019775\n",
      "Step: 13, Loss: 1.3005512952804565, Accuracy: 0.53125, Computation time: 3.7023932933807373\n",
      "Step: 14, Loss: 1.208168387413025, Accuracy: 0.46875, Computation time: 3.2682480812072754\n",
      "Step: 15, Loss: 1.2665913105010986, Accuracy: 0.59375, Computation time: 3.1005940437316895\n",
      "Step: 16, Loss: 1.3374907970428467, Accuracy: 0.40625, Computation time: 3.226581573486328\n",
      "Step: 17, Loss: 1.2199918031692505, Accuracy: 0.5625, Computation time: 3.1427125930786133\n",
      "Step: 18, Loss: 1.160173773765564, Accuracy: 0.625, Computation time: 3.586066961288452\n",
      "Step: 19, Loss: 1.1637014150619507, Accuracy: 0.53125, Computation time: 3.3730058670043945\n",
      "Step: 20, Loss: 1.2141085863113403, Accuracy: 0.53125, Computation time: 3.1517865657806396\n",
      "Step: 21, Loss: 1.0793805122375488, Accuracy: 0.875, Computation time: 3.480149269104004\n",
      "Step: 22, Loss: 1.1636852025985718, Accuracy: 0.75, Computation time: 3.0138797760009766\n",
      "Step: 23, Loss: 1.2227317094802856, Accuracy: 0.625, Computation time: 3.1277332305908203\n",
      "Step: 24, Loss: 1.1215039491653442, Accuracy: 0.75, Computation time: 3.4794600009918213\n",
      "Step: 25, Loss: 0.9876838326454163, Accuracy: 0.96875, Computation time: 2.5792555809020996\n",
      "Step: 26, Loss: 1.0699228048324585, Accuracy: 0.84375, Computation time: 3.645319938659668\n",
      "Step: 27, Loss: 1.0802992582321167, Accuracy: 0.78125, Computation time: 4.209461212158203\n",
      "Step: 28, Loss: 1.1339044570922852, Accuracy: 0.6875, Computation time: 3.49577260017395\n",
      "Step: 29, Loss: 1.0229837894439697, Accuracy: 0.84375, Computation time: 3.2138192653656006\n",
      "Step: 30, Loss: 1.0175235271453857, Accuracy: 0.84375, Computation time: 3.0164525508880615\n",
      "Step: 31, Loss: 1.1567373275756836, Accuracy: 0.59375, Computation time: 3.8532979488372803\n",
      "Step: 32, Loss: 1.0935882329940796, Accuracy: 0.71875, Computation time: 3.757195472717285\n",
      "Step: 33, Loss: 0.9861186742782593, Accuracy: 0.90625, Computation time: 2.5597546100616455\n",
      "Step: 34, Loss: 1.0081804990768433, Accuracy: 0.90625, Computation time: 3.7701213359832764\n",
      "Step: 35, Loss: 1.0062813758850098, Accuracy: 0.96875, Computation time: 3.240729331970215\n",
      "Step: 36, Loss: 0.9908996224403381, Accuracy: 0.96875, Computation time: 3.259340286254883\n",
      "Step: 37, Loss: 0.9503738284111023, Accuracy: 1.0, Computation time: 2.9287514686584473\n",
      "Step: 38, Loss: 0.9470944404602051, Accuracy: 0.96875, Computation time: 2.9562203884124756\n",
      "Step: 39, Loss: 0.9471260905265808, Accuracy: 1.0, Computation time: 2.9597737789154053\n",
      "Step: 40, Loss: 0.963980495929718, Accuracy: 1.0, Computation time: 3.5215237140655518\n",
      "Step: 41, Loss: 1.0121607780456543, Accuracy: 0.90625, Computation time: 3.558926582336426\n",
      "Step: 42, Loss: 0.9518947005271912, Accuracy: 1.0, Computation time: 3.31703782081604\n",
      "Step: 43, Loss: 0.968069314956665, Accuracy: 0.9375, Computation time: 3.353616237640381\n",
      "Step: 44, Loss: 0.9550372362136841, Accuracy: 0.9375, Computation time: 3.299736976623535\n",
      "Step: 45, Loss: 0.9446119666099548, Accuracy: 1.0, Computation time: 2.782007932662964\n",
      "Step: 46, Loss: 0.9271525740623474, Accuracy: 1.0, Computation time: 2.8573219776153564\n",
      "Step: 47, Loss: 0.9450775384902954, Accuracy: 0.96875, Computation time: 3.231051445007324\n",
      "Step: 48, Loss: 0.9472091794013977, Accuracy: 0.9375, Computation time: 2.591916084289551\n",
      "Step: 49, Loss: 0.9245277047157288, Accuracy: 1.0, Computation time: 3.1549155712127686\n",
      "Step: 50, Loss: 0.9552913308143616, Accuracy: 0.96875, Computation time: 2.973947525024414\n",
      "Step: 51, Loss: 0.9200361371040344, Accuracy: 1.0, Computation time: 2.6046345233917236\n",
      "Step: 52, Loss: 0.9365004897117615, Accuracy: 1.0, Computation time: 2.534932851791382\n",
      "Step: 53, Loss: 0.946774423122406, Accuracy: 0.96875, Computation time: 2.8258118629455566\n",
      "Step: 54, Loss: 0.9253836870193481, Accuracy: 1.0, Computation time: 2.1613845825195312\n",
      "Step: 55, Loss: 0.921596884727478, Accuracy: 1.0, Computation time: 2.607048988342285\n",
      "Step: 56, Loss: 0.92319256067276, Accuracy: 1.0, Computation time: 2.1914610862731934\n",
      "Step: 57, Loss: 0.9241241812705994, Accuracy: 1.0, Computation time: 3.20762038230896\n",
      "Step: 58, Loss: 0.9194284081459045, Accuracy: 1.0, Computation time: 2.313721179962158\n",
      "Step: 59, Loss: 0.9294396042823792, Accuracy: 0.96875, Computation time: 3.0878002643585205\n",
      "Step: 60, Loss: 0.9224456548690796, Accuracy: 1.0, Computation time: 2.918248176574707\n",
      "Step: 61, Loss: 0.9451143145561218, Accuracy: 0.96875, Computation time: 2.6271114349365234\n",
      "Step: 62, Loss: 0.941697895526886, Accuracy: 0.96875, Computation time: 2.866079807281494\n",
      "Step: 63, Loss: 0.9350841045379639, Accuracy: 0.96875, Computation time: 2.4643630981445312\n",
      "Step: 64, Loss: 0.9386085271835327, Accuracy: 0.96875, Computation time: 2.7385823726654053\n",
      "Step: 65, Loss: 0.9189936518669128, Accuracy: 1.0, Computation time: 2.383873701095581\n",
      "Step: 66, Loss: 0.919442892074585, Accuracy: 1.0, Computation time: 1.9047200679779053\n",
      "Step: 67, Loss: 0.9205512404441833, Accuracy: 1.0, Computation time: 2.9742591381073\n",
      "Step: 68, Loss: 0.9317779541015625, Accuracy: 1.0, Computation time: 2.349796772003174\n",
      "Step: 69, Loss: 0.9383721947669983, Accuracy: 0.96875, Computation time: 2.2392959594726562\n",
      "Step: 70, Loss: 0.9199070930480957, Accuracy: 1.0, Computation time: 2.339418649673462\n",
      "Step: 71, Loss: 0.9236114621162415, Accuracy: 1.0, Computation time: 2.7018468379974365\n",
      "Step: 72, Loss: 0.920390248298645, Accuracy: 1.0, Computation time: 2.4741170406341553\n",
      "Step: 73, Loss: 0.9179132580757141, Accuracy: 1.0, Computation time: 1.8342621326446533\n",
      "Step: 74, Loss: 0.920085608959198, Accuracy: 1.0, Computation time: 2.0002148151397705\n",
      "Step: 75, Loss: 0.9383329749107361, Accuracy: 0.96875, Computation time: 2.410003185272217\n",
      "Step: 76, Loss: 0.917034387588501, Accuracy: 1.0, Computation time: 2.6329102516174316\n",
      "Step: 77, Loss: 0.9370787143707275, Accuracy: 0.96875, Computation time: 2.808277130126953\n",
      "Step: 78, Loss: 0.9176608920097351, Accuracy: 1.0, Computation time: 2.459264039993286\n",
      "Step: 79, Loss: 0.9256771802902222, Accuracy: 1.0, Computation time: 3.82544207572937\n",
      "Step: 80, Loss: 0.9329122304916382, Accuracy: 0.96875, Computation time: 2.0688517093658447\n",
      "Step: 81, Loss: 0.9174027442932129, Accuracy: 1.0, Computation time: 2.3231046199798584\n",
      "Step: 82, Loss: 0.9415845274925232, Accuracy: 0.9375, Computation time: 2.2676384449005127\n",
      "Step: 83, Loss: 0.9195354580879211, Accuracy: 1.0, Computation time: 2.031320095062256\n",
      "Step: 84, Loss: 0.9175615310668945, Accuracy: 1.0, Computation time: 2.290285110473633\n",
      "Step: 85, Loss: 0.923488974571228, Accuracy: 1.0, Computation time: 2.304642677307129\n",
      "Step: 86, Loss: 0.9180724024772644, Accuracy: 1.0, Computation time: 2.141930103302002\n",
      "Step: 87, Loss: 0.9176129698753357, Accuracy: 1.0, Computation time: 2.471762180328369\n",
      "Step: 88, Loss: 0.92237389087677, Accuracy: 1.0, Computation time: 3.1515519618988037\n",
      "Step: 89, Loss: 0.9207660555839539, Accuracy: 1.0, Computation time: 2.7730987071990967\n",
      "Step: 90, Loss: 0.9177523851394653, Accuracy: 1.0, Computation time: 2.5086069107055664\n",
      "Step: 91, Loss: 0.9169127345085144, Accuracy: 1.0, Computation time: 2.337960958480835\n",
      "Step: 92, Loss: 0.9303258061408997, Accuracy: 1.0, Computation time: 4.140480995178223\n",
      "Step: 93, Loss: 0.9510303735733032, Accuracy: 0.9375, Computation time: 2.298039436340332\n",
      "Step: 94, Loss: 0.9263644814491272, Accuracy: 1.0, Computation time: 2.222970962524414\n",
      "Step: 95, Loss: 0.9271178245544434, Accuracy: 1.0, Computation time: 2.420197010040283\n",
      "Step: 96, Loss: 0.9323174357414246, Accuracy: 0.96875, Computation time: 2.32002592086792\n",
      "Step: 97, Loss: 0.9194042682647705, Accuracy: 1.0, Computation time: 2.544588088989258\n",
      "Step: 98, Loss: 0.9221065044403076, Accuracy: 1.0, Computation time: 2.5650222301483154\n",
      "Step: 99, Loss: 0.9178730845451355, Accuracy: 1.0, Computation time: 2.1603639125823975\n",
      "Step: 100, Loss: 0.9514098763465881, Accuracy: 0.9375, Computation time: 2.2875537872314453\n",
      "Step: 101, Loss: 0.9191914796829224, Accuracy: 1.0, Computation time: 2.4406280517578125\n",
      "Step: 102, Loss: 0.9410006999969482, Accuracy: 0.96875, Computation time: 2.2475693225860596\n",
      "Step: 103, Loss: 0.9221385717391968, Accuracy: 1.0, Computation time: 2.768397808074951\n",
      "Step: 104, Loss: 0.9176168441772461, Accuracy: 1.0, Computation time: 2.3431174755096436\n",
      "Step: 105, Loss: 0.9239687323570251, Accuracy: 1.0, Computation time: 2.5414655208587646\n",
      "Step: 106, Loss: 0.9277765154838562, Accuracy: 1.0, Computation time: 2.4586637020111084\n",
      "Step: 107, Loss: 0.9298160076141357, Accuracy: 0.96875, Computation time: 2.1535274982452393\n",
      "Step: 108, Loss: 0.91669762134552, Accuracy: 1.0, Computation time: 2.265587568283081\n",
      "Step: 109, Loss: 0.9331063032150269, Accuracy: 0.96875, Computation time: 2.288414716720581\n",
      "Step: 110, Loss: 0.9163820147514343, Accuracy: 1.0, Computation time: 2.7040317058563232\n",
      "Step: 111, Loss: 0.9337588548660278, Accuracy: 0.96875, Computation time: 2.35410475730896\n",
      "Step: 112, Loss: 0.9173417091369629, Accuracy: 1.0, Computation time: 2.955034017562866\n",
      "Step: 113, Loss: 0.9282791614532471, Accuracy: 1.0, Computation time: 2.537257194519043\n",
      "Step: 114, Loss: 0.9413678050041199, Accuracy: 0.9375, Computation time: 2.435105800628662\n",
      "Step: 115, Loss: 0.9593067169189453, Accuracy: 0.9375, Computation time: 2.810338020324707\n",
      "Step: 116, Loss: 0.9166616201400757, Accuracy: 1.0, Computation time: 2.540046215057373\n",
      "Step: 117, Loss: 0.9193767309188843, Accuracy: 1.0, Computation time: 1.975487232208252\n",
      "Step: 118, Loss: 0.9189725518226624, Accuracy: 1.0, Computation time: 2.5430681705474854\n",
      "Step: 119, Loss: 0.9168672561645508, Accuracy: 1.0, Computation time: 2.7316243648529053\n",
      "Step: 120, Loss: 0.9401304125785828, Accuracy: 0.96875, Computation time: 2.313833236694336\n",
      "Step: 121, Loss: 0.9228836297988892, Accuracy: 1.0, Computation time: 1.8680446147918701\n",
      "Step: 122, Loss: 0.935443103313446, Accuracy: 0.96875, Computation time: 2.4761464595794678\n",
      "Step: 123, Loss: 0.9360489249229431, Accuracy: 0.96875, Computation time: 2.20393443107605\n",
      "Step: 124, Loss: 0.9181520938873291, Accuracy: 1.0, Computation time: 2.3521108627319336\n",
      "Step: 125, Loss: 0.9354563355445862, Accuracy: 0.96875, Computation time: 2.0395872592926025\n",
      "Step: 126, Loss: 0.9168863892555237, Accuracy: 1.0, Computation time: 2.1924664974212646\n",
      "Step: 127, Loss: 0.9166736006736755, Accuracy: 1.0, Computation time: 2.1375319957733154\n",
      "Step: 128, Loss: 0.9251702427864075, Accuracy: 1.0, Computation time: 2.592123508453369\n",
      "Step: 129, Loss: 0.9178950786590576, Accuracy: 1.0, Computation time: 2.091881513595581\n",
      "Step: 130, Loss: 0.9187349081039429, Accuracy: 1.0, Computation time: 2.4189517498016357\n",
      "Step: 131, Loss: 0.9165731072425842, Accuracy: 1.0, Computation time: 2.601621627807617\n",
      "Step: 132, Loss: 0.9238917231559753, Accuracy: 1.0, Computation time: 2.4951982498168945\n",
      "Step: 133, Loss: 0.9528159499168396, Accuracy: 0.9375, Computation time: 2.7795820236206055\n",
      "Step: 134, Loss: 0.9170626997947693, Accuracy: 1.0, Computation time: 2.1549508571624756\n",
      "Step: 135, Loss: 0.9223988652229309, Accuracy: 1.0, Computation time: 3.0325305461883545\n",
      "Step: 136, Loss: 0.9166239500045776, Accuracy: 1.0, Computation time: 2.42744517326355\n",
      "Step: 137, Loss: 0.9169829487800598, Accuracy: 1.0, Computation time: 2.8007326126098633\n",
      "Step: 138, Loss: 0.9173122048377991, Accuracy: 1.0, Computation time: 1.8354268074035645\n",
      "Step: 139, Loss: 0.941739559173584, Accuracy: 0.9375, Computation time: 2.6312453746795654\n",
      "########################\n",
      "Test loss: 1.1214735507965088, Test Accuracy_epoch1: 0.6929895281791687\n",
      "########################\n",
      "Step: 140, Loss: 0.9179953336715698, Accuracy: 1.0, Computation time: 3.051995277404785\n",
      "Step: 141, Loss: 0.9178839921951294, Accuracy: 1.0, Computation time: 2.1528282165527344\n",
      "Step: 142, Loss: 0.9186162948608398, Accuracy: 1.0, Computation time: 2.352973222732544\n",
      "Step: 143, Loss: 0.9167669415473938, Accuracy: 1.0, Computation time: 2.560992479324341\n",
      "Step: 144, Loss: 0.9202843308448792, Accuracy: 1.0, Computation time: 2.2031502723693848\n",
      "Step: 145, Loss: 0.9302522540092468, Accuracy: 0.96875, Computation time: 2.1989328861236572\n",
      "Step: 146, Loss: 0.9205490350723267, Accuracy: 1.0, Computation time: 2.178245782852173\n",
      "Step: 147, Loss: 0.9163500666618347, Accuracy: 1.0, Computation time: 2.3265087604522705\n",
      "Step: 148, Loss: 0.9170556664466858, Accuracy: 1.0, Computation time: 1.933821439743042\n",
      "Step: 149, Loss: 0.9168830513954163, Accuracy: 1.0, Computation time: 2.8809995651245117\n",
      "Step: 150, Loss: 0.9195537567138672, Accuracy: 1.0, Computation time: 2.2507965564727783\n",
      "Step: 151, Loss: 0.916854977607727, Accuracy: 1.0, Computation time: 2.424036979675293\n",
      "Step: 152, Loss: 0.9168398976325989, Accuracy: 1.0, Computation time: 2.1159510612487793\n",
      "Step: 153, Loss: 0.9510675072669983, Accuracy: 0.9375, Computation time: 1.8954596519470215\n",
      "Step: 154, Loss: 0.9204542636871338, Accuracy: 1.0, Computation time: 1.969778299331665\n",
      "Step: 155, Loss: 0.9162206053733826, Accuracy: 1.0, Computation time: 2.1720831394195557\n",
      "Step: 156, Loss: 0.918296217918396, Accuracy: 1.0, Computation time: 2.089430093765259\n",
      "Step: 157, Loss: 0.916771411895752, Accuracy: 1.0, Computation time: 2.209195375442505\n",
      "Step: 158, Loss: 0.9163825511932373, Accuracy: 1.0, Computation time: 2.3838136196136475\n",
      "Step: 159, Loss: 0.9166706800460815, Accuracy: 1.0, Computation time: 2.37263560295105\n",
      "Step: 160, Loss: 0.9351591467857361, Accuracy: 0.96875, Computation time: 2.186743974685669\n",
      "Step: 161, Loss: 0.9392372369766235, Accuracy: 0.96875, Computation time: 2.1335859298706055\n",
      "Step: 162, Loss: 0.9164364337921143, Accuracy: 1.0, Computation time: 1.9526128768920898\n",
      "Step: 163, Loss: 0.9518445134162903, Accuracy: 0.9375, Computation time: 2.2459020614624023\n",
      "Step: 164, Loss: 0.9389715194702148, Accuracy: 0.96875, Computation time: 2.753826379776001\n",
      "Step: 165, Loss: 0.9164603352546692, Accuracy: 1.0, Computation time: 2.064718246459961\n",
      "Step: 166, Loss: 0.9164115190505981, Accuracy: 1.0, Computation time: 1.9940948486328125\n",
      "Step: 167, Loss: 0.9349256157875061, Accuracy: 0.96875, Computation time: 2.0090951919555664\n",
      "Step: 168, Loss: 0.9257079362869263, Accuracy: 1.0, Computation time: 2.452030897140503\n",
      "Step: 169, Loss: 0.9166840314865112, Accuracy: 1.0, Computation time: 2.132176637649536\n",
      "Step: 170, Loss: 0.9553735852241516, Accuracy: 0.9375, Computation time: 2.3531877994537354\n",
      "Step: 171, Loss: 0.9283621311187744, Accuracy: 0.96875, Computation time: 1.8281466960906982\n",
      "Step: 172, Loss: 0.9163715243339539, Accuracy: 1.0, Computation time: 1.8972837924957275\n",
      "Step: 173, Loss: 0.932206392288208, Accuracy: 0.96875, Computation time: 2.8150932788848877\n",
      "Step: 174, Loss: 0.9396782517433167, Accuracy: 0.96875, Computation time: 2.2352612018585205\n",
      "Step: 175, Loss: 0.9168031215667725, Accuracy: 1.0, Computation time: 2.409682273864746\n",
      "Step: 176, Loss: 0.9380463361740112, Accuracy: 0.96875, Computation time: 2.40909743309021\n",
      "Step: 177, Loss: 0.916634738445282, Accuracy: 1.0, Computation time: 2.1447789669036865\n",
      "Step: 178, Loss: 0.9376189112663269, Accuracy: 0.96875, Computation time: 2.052766799926758\n",
      "Step: 179, Loss: 0.924028754234314, Accuracy: 1.0, Computation time: 2.292383909225464\n",
      "Step: 180, Loss: 0.9167202711105347, Accuracy: 1.0, Computation time: 2.1481130123138428\n",
      "Step: 181, Loss: 0.916674017906189, Accuracy: 1.0, Computation time: 2.9870800971984863\n",
      "Step: 182, Loss: 0.9163945913314819, Accuracy: 1.0, Computation time: 2.1740262508392334\n",
      "Step: 183, Loss: 0.9166524410247803, Accuracy: 1.0, Computation time: 2.1505680084228516\n",
      "Step: 184, Loss: 0.9188960790634155, Accuracy: 1.0, Computation time: 2.839963436126709\n",
      "Step: 185, Loss: 0.9172483682632446, Accuracy: 1.0, Computation time: 2.5556423664093018\n",
      "Step: 186, Loss: 0.9162736535072327, Accuracy: 1.0, Computation time: 2.5874252319335938\n",
      "Step: 187, Loss: 0.9505065083503723, Accuracy: 0.96875, Computation time: 2.263935089111328\n",
      "Step: 188, Loss: 0.9176760315895081, Accuracy: 1.0, Computation time: 2.750392198562622\n",
      "Step: 189, Loss: 0.9306735992431641, Accuracy: 0.96875, Computation time: 2.117234706878662\n",
      "Step: 190, Loss: 0.9163059592247009, Accuracy: 1.0, Computation time: 2.130465507507324\n",
      "Step: 191, Loss: 0.9184603095054626, Accuracy: 1.0, Computation time: 2.1172726154327393\n",
      "Step: 192, Loss: 0.9387192726135254, Accuracy: 0.9375, Computation time: 3.053452730178833\n",
      "Step: 193, Loss: 0.9198254346847534, Accuracy: 1.0, Computation time: 2.2398617267608643\n",
      "Step: 194, Loss: 0.9166936278343201, Accuracy: 1.0, Computation time: 2.2180633544921875\n",
      "Step: 195, Loss: 0.9195773005485535, Accuracy: 1.0, Computation time: 2.45304274559021\n",
      "Step: 196, Loss: 0.9164366126060486, Accuracy: 1.0, Computation time: 2.1185436248779297\n",
      "Step: 197, Loss: 0.9188792109489441, Accuracy: 1.0, Computation time: 2.0498745441436768\n",
      "Step: 198, Loss: 0.9376823902130127, Accuracy: 0.96875, Computation time: 2.0106165409088135\n",
      "Step: 199, Loss: 0.9164721965789795, Accuracy: 1.0, Computation time: 2.431922197341919\n",
      "Step: 200, Loss: 0.9580686688423157, Accuracy: 0.9375, Computation time: 2.468714475631714\n",
      "Step: 201, Loss: 0.9164376258850098, Accuracy: 1.0, Computation time: 2.09553599357605\n",
      "Step: 202, Loss: 0.9402391314506531, Accuracy: 0.96875, Computation time: 2.586735963821411\n",
      "Step: 203, Loss: 0.9529520273208618, Accuracy: 0.9375, Computation time: 3.03277325630188\n",
      "Step: 204, Loss: 0.9172397255897522, Accuracy: 1.0, Computation time: 2.2247540950775146\n",
      "Step: 205, Loss: 0.9172757267951965, Accuracy: 1.0, Computation time: 2.7354583740234375\n",
      "Step: 206, Loss: 0.9163364171981812, Accuracy: 1.0, Computation time: 1.959730863571167\n",
      "Step: 207, Loss: 0.9199905395507812, Accuracy: 1.0, Computation time: 2.243830680847168\n",
      "Step: 208, Loss: 0.9173402190208435, Accuracy: 1.0, Computation time: 2.0652518272399902\n",
      "Step: 209, Loss: 0.9163509011268616, Accuracy: 1.0, Computation time: 1.6214039325714111\n",
      "Step: 210, Loss: 0.9185271859169006, Accuracy: 1.0, Computation time: 2.0987906455993652\n",
      "Step: 211, Loss: 0.9180129766464233, Accuracy: 1.0, Computation time: 2.25954532623291\n",
      "Step: 212, Loss: 0.9179335236549377, Accuracy: 1.0, Computation time: 2.0355560779571533\n",
      "Step: 213, Loss: 0.9360435605049133, Accuracy: 0.96875, Computation time: 2.223376989364624\n",
      "Step: 214, Loss: 0.9165349006652832, Accuracy: 1.0, Computation time: 2.1261324882507324\n",
      "Step: 215, Loss: 0.9177519083023071, Accuracy: 1.0, Computation time: 1.815497875213623\n",
      "Step: 216, Loss: 0.9185622334480286, Accuracy: 1.0, Computation time: 2.774249315261841\n",
      "Step: 217, Loss: 0.9363213777542114, Accuracy: 0.96875, Computation time: 1.916008710861206\n",
      "Step: 218, Loss: 0.916774570941925, Accuracy: 1.0, Computation time: 1.9395625591278076\n",
      "Step: 219, Loss: 0.9213412404060364, Accuracy: 1.0, Computation time: 2.3003532886505127\n",
      "Step: 220, Loss: 0.9166198968887329, Accuracy: 1.0, Computation time: 1.8945598602294922\n",
      "Step: 221, Loss: 0.9171599745750427, Accuracy: 1.0, Computation time: 2.1839330196380615\n",
      "Step: 222, Loss: 0.9164579510688782, Accuracy: 1.0, Computation time: 1.8062400817871094\n",
      "Step: 223, Loss: 0.918908417224884, Accuracy: 1.0, Computation time: 2.2191481590270996\n",
      "Step: 224, Loss: 0.9166229963302612, Accuracy: 1.0, Computation time: 2.2316815853118896\n",
      "Step: 225, Loss: 0.9382871389389038, Accuracy: 0.96875, Computation time: 1.8762586116790771\n",
      "Step: 226, Loss: 0.916996419429779, Accuracy: 1.0, Computation time: 2.2437613010406494\n",
      "Step: 227, Loss: 0.9382161498069763, Accuracy: 0.96875, Computation time: 2.214224338531494\n",
      "Step: 228, Loss: 0.9176746010780334, Accuracy: 1.0, Computation time: 2.26058292388916\n",
      "Step: 229, Loss: 0.9213211536407471, Accuracy: 1.0, Computation time: 2.6718616485595703\n",
      "Step: 230, Loss: 0.9240996837615967, Accuracy: 1.0, Computation time: 2.722470760345459\n",
      "Step: 231, Loss: 0.9167240858078003, Accuracy: 1.0, Computation time: 2.2213046550750732\n",
      "Step: 232, Loss: 0.9164695739746094, Accuracy: 1.0, Computation time: 2.0104877948760986\n",
      "Step: 233, Loss: 0.917243242263794, Accuracy: 1.0, Computation time: 2.3224093914031982\n",
      "Step: 234, Loss: 0.9198865294456482, Accuracy: 1.0, Computation time: 2.14349102973938\n",
      "Step: 235, Loss: 0.9533863663673401, Accuracy: 0.9375, Computation time: 2.2435860633850098\n",
      "Step: 236, Loss: 0.9183346629142761, Accuracy: 1.0, Computation time: 2.0160069465637207\n",
      "Step: 237, Loss: 0.9162842631340027, Accuracy: 1.0, Computation time: 1.6976399421691895\n",
      "Step: 238, Loss: 0.9162592887878418, Accuracy: 1.0, Computation time: 1.972081184387207\n",
      "Step: 239, Loss: 0.9162947535514832, Accuracy: 1.0, Computation time: 1.7635109424591064\n",
      "Step: 240, Loss: 0.9162296056747437, Accuracy: 1.0, Computation time: 1.9696359634399414\n",
      "Step: 241, Loss: 0.9425133466720581, Accuracy: 0.96875, Computation time: 2.296736717224121\n",
      "Step: 242, Loss: 0.9163679480552673, Accuracy: 1.0, Computation time: 2.2682929039001465\n",
      "Step: 243, Loss: 0.9228543639183044, Accuracy: 1.0, Computation time: 2.654073476791382\n",
      "Step: 244, Loss: 0.9316138029098511, Accuracy: 0.96875, Computation time: 2.390925645828247\n",
      "Step: 245, Loss: 0.9392269849777222, Accuracy: 0.96875, Computation time: 2.89467453956604\n",
      "Step: 246, Loss: 0.9359475374221802, Accuracy: 0.96875, Computation time: 1.743802785873413\n",
      "Step: 247, Loss: 0.9162305593490601, Accuracy: 1.0, Computation time: 1.841660499572754\n",
      "Step: 248, Loss: 0.9165080785751343, Accuracy: 1.0, Computation time: 2.1751797199249268\n",
      "Step: 249, Loss: 0.9165372848510742, Accuracy: 1.0, Computation time: 1.9788575172424316\n",
      "Step: 250, Loss: 0.9170966744422913, Accuracy: 1.0, Computation time: 1.922490119934082\n",
      "Step: 251, Loss: 0.9172847270965576, Accuracy: 1.0, Computation time: 2.3451719284057617\n",
      "Step: 252, Loss: 0.9166064262390137, Accuracy: 1.0, Computation time: 2.440619945526123\n",
      "Step: 253, Loss: 0.9254873991012573, Accuracy: 1.0, Computation time: 2.0849783420562744\n",
      "Step: 254, Loss: 0.9253189563751221, Accuracy: 1.0, Computation time: 1.9768669605255127\n",
      "Step: 255, Loss: 0.916583776473999, Accuracy: 1.0, Computation time: 1.8316802978515625\n",
      "Step: 256, Loss: 0.9178408980369568, Accuracy: 1.0, Computation time: 2.1294941902160645\n",
      "Step: 257, Loss: 0.9162111282348633, Accuracy: 1.0, Computation time: 2.0220563411712646\n",
      "Step: 258, Loss: 0.937729001045227, Accuracy: 0.96875, Computation time: 3.1317214965820312\n",
      "Step: 259, Loss: 0.9162216782569885, Accuracy: 1.0, Computation time: 1.622009515762329\n",
      "Step: 260, Loss: 0.9161416888237, Accuracy: 1.0, Computation time: 2.1085383892059326\n",
      "Step: 261, Loss: 0.9164432883262634, Accuracy: 1.0, Computation time: 1.995771884918213\n",
      "Step: 262, Loss: 0.9378557801246643, Accuracy: 0.96875, Computation time: 2.089653491973877\n",
      "Step: 263, Loss: 0.9525918960571289, Accuracy: 0.9375, Computation time: 2.910066604614258\n",
      "Step: 264, Loss: 0.935608446598053, Accuracy: 0.96875, Computation time: 1.9856109619140625\n",
      "Step: 265, Loss: 0.9166820049285889, Accuracy: 1.0, Computation time: 1.7918436527252197\n",
      "Step: 266, Loss: 0.9206477403640747, Accuracy: 1.0, Computation time: 2.42680025100708\n",
      "Step: 267, Loss: 0.9161249399185181, Accuracy: 1.0, Computation time: 2.3979616165161133\n",
      "Step: 268, Loss: 0.9162473082542419, Accuracy: 1.0, Computation time: 2.1258628368377686\n",
      "Step: 269, Loss: 0.9162535071372986, Accuracy: 1.0, Computation time: 1.9074969291687012\n",
      "Step: 270, Loss: 0.9256373047828674, Accuracy: 1.0, Computation time: 2.025607109069824\n",
      "Step: 271, Loss: 0.9379118084907532, Accuracy: 0.96875, Computation time: 2.3410985469818115\n",
      "Step: 272, Loss: 0.9373508095741272, Accuracy: 0.96875, Computation time: 2.4358508586883545\n",
      "Step: 273, Loss: 0.916412889957428, Accuracy: 1.0, Computation time: 2.042166233062744\n",
      "Step: 274, Loss: 0.9161321520805359, Accuracy: 1.0, Computation time: 1.8051939010620117\n",
      "Step: 275, Loss: 0.9165723919868469, Accuracy: 1.0, Computation time: 2.104398012161255\n",
      "Step: 276, Loss: 0.9456891417503357, Accuracy: 0.96875, Computation time: 2.939323663711548\n",
      "Step: 277, Loss: 0.9358711838722229, Accuracy: 0.96875, Computation time: 2.223432779312134\n",
      "Step: 278, Loss: 0.9312493801116943, Accuracy: 0.96875, Computation time: 2.2034316062927246\n",
      "########################\n",
      "Test loss: 1.1189172267913818, Test Accuracy_epoch2: 0.7010475397109985\n",
      "########################\n",
      "Step: 279, Loss: 0.9362342953681946, Accuracy: 0.96875, Computation time: 2.345893144607544\n",
      "Step: 280, Loss: 0.9164696931838989, Accuracy: 1.0, Computation time: 2.1526095867156982\n",
      "Step: 281, Loss: 0.9163620471954346, Accuracy: 1.0, Computation time: 2.575209856033325\n",
      "Step: 282, Loss: 0.9164619445800781, Accuracy: 1.0, Computation time: 1.8969426155090332\n",
      "Step: 283, Loss: 0.9164413213729858, Accuracy: 1.0, Computation time: 1.8084301948547363\n",
      "Step: 284, Loss: 0.9391669631004333, Accuracy: 0.96875, Computation time: 2.2877776622772217\n",
      "Step: 285, Loss: 0.917447030544281, Accuracy: 1.0, Computation time: 2.0574350357055664\n",
      "Step: 286, Loss: 0.9266489744186401, Accuracy: 0.96875, Computation time: 2.9790287017822266\n",
      "Step: 287, Loss: 0.9349439144134521, Accuracy: 0.96875, Computation time: 2.16898512840271\n",
      "Step: 288, Loss: 0.9215909242630005, Accuracy: 1.0, Computation time: 2.300874948501587\n",
      "Step: 289, Loss: 0.916836142539978, Accuracy: 1.0, Computation time: 1.987635850906372\n",
      "Step: 290, Loss: 0.916867196559906, Accuracy: 1.0, Computation time: 1.8231217861175537\n",
      "Step: 291, Loss: 0.9163024425506592, Accuracy: 1.0, Computation time: 1.9093961715698242\n",
      "Step: 292, Loss: 0.9386893510818481, Accuracy: 0.96875, Computation time: 2.2426979541778564\n",
      "Step: 293, Loss: 0.9255635738372803, Accuracy: 1.0, Computation time: 2.3777198791503906\n",
      "Step: 294, Loss: 0.9269160032272339, Accuracy: 0.96875, Computation time: 2.131140947341919\n",
      "Step: 295, Loss: 0.9290596842765808, Accuracy: 1.0, Computation time: 2.3857228755950928\n",
      "Step: 296, Loss: 0.9470003843307495, Accuracy: 0.9375, Computation time: 2.6068387031555176\n",
      "Step: 297, Loss: 0.916110098361969, Accuracy: 1.0, Computation time: 1.6911191940307617\n",
      "Step: 298, Loss: 0.9168749451637268, Accuracy: 1.0, Computation time: 2.0197901725769043\n",
      "Step: 299, Loss: 0.9166341423988342, Accuracy: 1.0, Computation time: 2.002107620239258\n",
      "Step: 300, Loss: 0.920889675617218, Accuracy: 1.0, Computation time: 2.2390847206115723\n",
      "Step: 301, Loss: 0.9248452186584473, Accuracy: 1.0, Computation time: 2.274900436401367\n",
      "Step: 302, Loss: 0.9162550568580627, Accuracy: 1.0, Computation time: 1.8829295635223389\n",
      "Step: 303, Loss: 0.9338268041610718, Accuracy: 0.96875, Computation time: 3.0276901721954346\n",
      "Step: 304, Loss: 0.9160319566726685, Accuracy: 1.0, Computation time: 2.3698885440826416\n",
      "Step: 305, Loss: 0.9163137674331665, Accuracy: 1.0, Computation time: 1.871103286743164\n",
      "Step: 306, Loss: 0.9397605061531067, Accuracy: 0.96875, Computation time: 2.11006498336792\n",
      "Step: 307, Loss: 0.9179452061653137, Accuracy: 1.0, Computation time: 1.877350091934204\n",
      "Step: 308, Loss: 0.9289828538894653, Accuracy: 1.0, Computation time: 2.804872989654541\n",
      "Step: 309, Loss: 0.9165157079696655, Accuracy: 1.0, Computation time: 2.5881664752960205\n",
      "Step: 310, Loss: 0.9180328845977783, Accuracy: 1.0, Computation time: 2.027561664581299\n",
      "Step: 311, Loss: 0.9164538979530334, Accuracy: 1.0, Computation time: 2.177157163619995\n",
      "Step: 312, Loss: 0.9180696606636047, Accuracy: 1.0, Computation time: 2.2218291759490967\n",
      "Step: 313, Loss: 0.9163570404052734, Accuracy: 1.0, Computation time: 1.966672420501709\n",
      "Step: 314, Loss: 0.921021044254303, Accuracy: 1.0, Computation time: 2.343755006790161\n",
      "Step: 315, Loss: 0.9382047057151794, Accuracy: 0.96875, Computation time: 2.009122371673584\n",
      "Step: 316, Loss: 0.9168345928192139, Accuracy: 1.0, Computation time: 2.0387980937957764\n",
      "Step: 317, Loss: 0.9165980219841003, Accuracy: 1.0, Computation time: 2.2871787548065186\n",
      "Step: 318, Loss: 0.9227513670921326, Accuracy: 1.0, Computation time: 2.473543882369995\n",
      "Step: 319, Loss: 0.9161519408226013, Accuracy: 1.0, Computation time: 2.157426595687866\n",
      "Step: 320, Loss: 0.9162735939025879, Accuracy: 1.0, Computation time: 1.8538649082183838\n",
      "Step: 321, Loss: 0.9164202213287354, Accuracy: 1.0, Computation time: 2.440075635910034\n",
      "Step: 322, Loss: 0.918068528175354, Accuracy: 1.0, Computation time: 2.2185773849487305\n",
      "Step: 323, Loss: 0.9474055767059326, Accuracy: 0.9375, Computation time: 2.4835474491119385\n",
      "Step: 324, Loss: 0.9178912043571472, Accuracy: 1.0, Computation time: 1.9701836109161377\n",
      "Step: 325, Loss: 0.9416797757148743, Accuracy: 0.96875, Computation time: 3.6902143955230713\n",
      "Step: 326, Loss: 0.9168158769607544, Accuracy: 1.0, Computation time: 2.4966928958892822\n",
      "Step: 327, Loss: 0.9162973761558533, Accuracy: 1.0, Computation time: 2.418431520462036\n",
      "Step: 328, Loss: 0.920954704284668, Accuracy: 1.0, Computation time: 2.5533645153045654\n",
      "Step: 329, Loss: 0.916969895362854, Accuracy: 1.0, Computation time: 2.0484325885772705\n",
      "Step: 330, Loss: 0.9165233969688416, Accuracy: 1.0, Computation time: 1.9108307361602783\n",
      "Step: 331, Loss: 0.9325452446937561, Accuracy: 0.96875, Computation time: 3.255168914794922\n",
      "Step: 332, Loss: 0.9169085025787354, Accuracy: 1.0, Computation time: 2.4618499279022217\n",
      "Step: 333, Loss: 0.916549563407898, Accuracy: 1.0, Computation time: 2.369448661804199\n",
      "Step: 334, Loss: 0.9168180823326111, Accuracy: 1.0, Computation time: 2.2406418323516846\n",
      "Step: 335, Loss: 0.9164543747901917, Accuracy: 1.0, Computation time: 1.7717368602752686\n",
      "Step: 336, Loss: 0.9161939024925232, Accuracy: 1.0, Computation time: 1.9312167167663574\n",
      "Step: 337, Loss: 0.9171525239944458, Accuracy: 1.0, Computation time: 2.094804048538208\n",
      "Step: 338, Loss: 0.9454931616783142, Accuracy: 0.96875, Computation time: 2.166415214538574\n",
      "Step: 339, Loss: 0.9162400364875793, Accuracy: 1.0, Computation time: 2.020326852798462\n",
      "Step: 340, Loss: 0.9161319732666016, Accuracy: 1.0, Computation time: 1.8888099193572998\n",
      "Step: 341, Loss: 0.9160827398300171, Accuracy: 1.0, Computation time: 2.063575506210327\n",
      "Step: 342, Loss: 0.9163139462471008, Accuracy: 1.0, Computation time: 2.398132801055908\n",
      "Step: 343, Loss: 0.9173669815063477, Accuracy: 1.0, Computation time: 2.410712480545044\n",
      "Step: 344, Loss: 0.9274002909660339, Accuracy: 0.96875, Computation time: 2.8215160369873047\n",
      "Step: 345, Loss: 0.9165681004524231, Accuracy: 1.0, Computation time: 2.6005327701568604\n",
      "Step: 346, Loss: 0.9436118602752686, Accuracy: 0.96875, Computation time: 3.1092844009399414\n",
      "Step: 347, Loss: 0.916523277759552, Accuracy: 1.0, Computation time: 1.8875586986541748\n",
      "Step: 348, Loss: 0.9169806838035583, Accuracy: 1.0, Computation time: 2.3332722187042236\n",
      "Step: 349, Loss: 0.9161202311515808, Accuracy: 1.0, Computation time: 2.785553216934204\n",
      "Step: 350, Loss: 0.9333264827728271, Accuracy: 0.96875, Computation time: 2.7537460327148438\n",
      "Step: 351, Loss: 0.9165528416633606, Accuracy: 1.0, Computation time: 2.217703342437744\n",
      "Step: 352, Loss: 0.9599962830543518, Accuracy: 0.9375, Computation time: 2.5577175617218018\n",
      "Step: 353, Loss: 0.9214781522750854, Accuracy: 1.0, Computation time: 2.581003189086914\n",
      "Step: 354, Loss: 0.916060209274292, Accuracy: 1.0, Computation time: 1.8479444980621338\n",
      "Step: 355, Loss: 0.9162916541099548, Accuracy: 1.0, Computation time: 2.0499966144561768\n",
      "Step: 356, Loss: 0.9377976655960083, Accuracy: 0.96875, Computation time: 1.9283177852630615\n",
      "Step: 357, Loss: 0.9373346567153931, Accuracy: 0.96875, Computation time: 2.4281091690063477\n",
      "Step: 358, Loss: 0.9163336157798767, Accuracy: 1.0, Computation time: 1.9909546375274658\n",
      "Step: 359, Loss: 0.9379357695579529, Accuracy: 0.96875, Computation time: 2.5244667530059814\n",
      "Step: 360, Loss: 0.9581130743026733, Accuracy: 0.9375, Computation time: 2.2889461517333984\n",
      "Step: 361, Loss: 0.9161990880966187, Accuracy: 1.0, Computation time: 2.4061152935028076\n",
      "Step: 362, Loss: 0.9318631291389465, Accuracy: 0.96875, Computation time: 2.7900781631469727\n",
      "Step: 363, Loss: 0.9187153577804565, Accuracy: 1.0, Computation time: 2.19327974319458\n",
      "Step: 364, Loss: 0.940331757068634, Accuracy: 0.96875, Computation time: 2.1758487224578857\n",
      "Step: 365, Loss: 0.9160409569740295, Accuracy: 1.0, Computation time: 1.8800325393676758\n",
      "Step: 366, Loss: 0.9330668449401855, Accuracy: 0.96875, Computation time: 2.411996841430664\n",
      "Step: 367, Loss: 0.9181389808654785, Accuracy: 1.0, Computation time: 2.50543475151062\n",
      "Step: 368, Loss: 0.9159485697746277, Accuracy: 1.0, Computation time: 1.7800812721252441\n",
      "Step: 369, Loss: 0.9220178127288818, Accuracy: 1.0, Computation time: 2.717301607131958\n",
      "Step: 370, Loss: 0.918351948261261, Accuracy: 1.0, Computation time: 2.2364907264709473\n",
      "Step: 371, Loss: 0.9382625818252563, Accuracy: 0.96875, Computation time: 2.0095882415771484\n",
      "Step: 372, Loss: 0.9378537535667419, Accuracy: 0.96875, Computation time: 1.8943796157836914\n",
      "Step: 373, Loss: 0.9374565482139587, Accuracy: 0.96875, Computation time: 2.2624385356903076\n",
      "Step: 374, Loss: 0.9166573286056519, Accuracy: 1.0, Computation time: 2.2341809272766113\n",
      "Step: 375, Loss: 0.9163815975189209, Accuracy: 1.0, Computation time: 2.1035187244415283\n",
      "Step: 376, Loss: 0.9286622405052185, Accuracy: 0.96875, Computation time: 2.338486671447754\n",
      "Step: 377, Loss: 0.916408896446228, Accuracy: 1.0, Computation time: 1.8264853954315186\n",
      "Step: 378, Loss: 0.9162359833717346, Accuracy: 1.0, Computation time: 1.9310569763183594\n",
      "Step: 379, Loss: 0.9170064926147461, Accuracy: 1.0, Computation time: 1.949936866760254\n",
      "Step: 380, Loss: 0.9176762700080872, Accuracy: 1.0, Computation time: 2.3034069538116455\n",
      "Step: 381, Loss: 0.9373229742050171, Accuracy: 0.96875, Computation time: 2.7251017093658447\n",
      "Step: 382, Loss: 0.9163833260536194, Accuracy: 1.0, Computation time: 1.991804599761963\n",
      "Step: 383, Loss: 0.9160895943641663, Accuracy: 1.0, Computation time: 2.1974825859069824\n",
      "Step: 384, Loss: 0.9169663786888123, Accuracy: 1.0, Computation time: 2.313861846923828\n",
      "Step: 385, Loss: 0.9348278045654297, Accuracy: 0.96875, Computation time: 2.3337910175323486\n",
      "Step: 386, Loss: 0.9199076890945435, Accuracy: 1.0, Computation time: 2.359536647796631\n",
      "Step: 387, Loss: 0.9174538254737854, Accuracy: 1.0, Computation time: 1.9344074726104736\n",
      "Step: 388, Loss: 0.9161059260368347, Accuracy: 1.0, Computation time: 2.183328151702881\n",
      "Step: 389, Loss: 0.9159958362579346, Accuracy: 1.0, Computation time: 2.2193570137023926\n",
      "Step: 390, Loss: 0.9162349104881287, Accuracy: 1.0, Computation time: 2.3853373527526855\n",
      "Step: 391, Loss: 0.925036609172821, Accuracy: 1.0, Computation time: 2.844205856323242\n",
      "Step: 392, Loss: 0.9172345399856567, Accuracy: 1.0, Computation time: 2.800473928451538\n",
      "Step: 393, Loss: 0.9163536429405212, Accuracy: 1.0, Computation time: 2.228149890899658\n",
      "Step: 394, Loss: 0.916077733039856, Accuracy: 1.0, Computation time: 2.314574718475342\n",
      "Step: 395, Loss: 0.9231950640678406, Accuracy: 1.0, Computation time: 2.539933681488037\n",
      "Step: 396, Loss: 0.9161482453346252, Accuracy: 1.0, Computation time: 2.099970817565918\n",
      "Step: 397, Loss: 0.9160750508308411, Accuracy: 1.0, Computation time: 2.198150157928467\n",
      "Step: 398, Loss: 0.9160708785057068, Accuracy: 1.0, Computation time: 1.8371632099151611\n",
      "Step: 399, Loss: 0.9160598516464233, Accuracy: 1.0, Computation time: 2.031874418258667\n",
      "Step: 400, Loss: 0.9162299036979675, Accuracy: 1.0, Computation time: 2.164339542388916\n",
      "Step: 401, Loss: 0.9160476922988892, Accuracy: 1.0, Computation time: 1.7280290126800537\n",
      "Step: 402, Loss: 0.9160590767860413, Accuracy: 1.0, Computation time: 2.175858974456787\n",
      "Step: 403, Loss: 0.9163921475410461, Accuracy: 1.0, Computation time: 2.2209699153900146\n",
      "Step: 404, Loss: 0.9364830851554871, Accuracy: 0.96875, Computation time: 2.1059319972991943\n",
      "Step: 405, Loss: 0.9264089465141296, Accuracy: 0.96875, Computation time: 2.167309522628784\n",
      "Step: 406, Loss: 0.9159814119338989, Accuracy: 1.0, Computation time: 2.3712360858917236\n",
      "Step: 407, Loss: 0.9161217212677002, Accuracy: 1.0, Computation time: 2.5438122749328613\n",
      "Step: 408, Loss: 0.9348593354225159, Accuracy: 0.96875, Computation time: 3.4917075634002686\n",
      "Step: 409, Loss: 0.9362447261810303, Accuracy: 0.96875, Computation time: 3.8229000568389893\n",
      "Step: 410, Loss: 0.9166051745414734, Accuracy: 1.0, Computation time: 3.026371479034424\n",
      "Step: 411, Loss: 0.917949914932251, Accuracy: 1.0, Computation time: 2.3239052295684814\n",
      "Step: 412, Loss: 0.9164485335350037, Accuracy: 1.0, Computation time: 2.5035018920898438\n",
      "Step: 413, Loss: 0.91607666015625, Accuracy: 1.0, Computation time: 1.8578264713287354\n",
      "Step: 414, Loss: 0.9585740566253662, Accuracy: 0.9375, Computation time: 1.8773996829986572\n",
      "Step: 415, Loss: 0.9163865447044373, Accuracy: 1.0, Computation time: 2.220161199569702\n",
      "Step: 416, Loss: 0.9161739945411682, Accuracy: 1.0, Computation time: 2.1269452571868896\n",
      "Step: 417, Loss: 0.9532375931739807, Accuracy: 0.9375, Computation time: 2.4032158851623535\n",
      "########################\n",
      "Test loss: 1.127287745475769, Test Accuracy_epoch3: 0.6865431070327759\n",
      "########################\n",
      "Step: 418, Loss: 0.9183167815208435, Accuracy: 1.0, Computation time: 2.537261486053467\n",
      "Step: 419, Loss: 0.9162538051605225, Accuracy: 1.0, Computation time: 2.1097092628479004\n",
      "Step: 420, Loss: 0.9161753058433533, Accuracy: 1.0, Computation time: 2.197037935256958\n",
      "Step: 421, Loss: 0.9250730872154236, Accuracy: 1.0, Computation time: 2.8592519760131836\n",
      "Step: 422, Loss: 0.9161003828048706, Accuracy: 1.0, Computation time: 1.99784517288208\n",
      "Step: 423, Loss: 0.9373020529747009, Accuracy: 0.96875, Computation time: 2.727113962173462\n",
      "Step: 424, Loss: 0.9381383061408997, Accuracy: 0.96875, Computation time: 2.3322677612304688\n",
      "Step: 425, Loss: 0.9201744794845581, Accuracy: 1.0, Computation time: 3.0486574172973633\n",
      "Step: 426, Loss: 0.9589948058128357, Accuracy: 0.9375, Computation time: 2.1939806938171387\n",
      "Step: 427, Loss: 0.9389261603355408, Accuracy: 0.96875, Computation time: 1.8509538173675537\n",
      "Step: 428, Loss: 0.9161298871040344, Accuracy: 1.0, Computation time: 2.2800228595733643\n",
      "Step: 429, Loss: 0.9160977005958557, Accuracy: 1.0, Computation time: 2.25689697265625\n",
      "Step: 430, Loss: 0.9161773324012756, Accuracy: 1.0, Computation time: 2.1481196880340576\n",
      "Step: 431, Loss: 0.9161096811294556, Accuracy: 1.0, Computation time: 2.2944958209991455\n",
      "Step: 432, Loss: 0.947924017906189, Accuracy: 0.9375, Computation time: 3.0530834197998047\n",
      "Step: 433, Loss: 0.9187173247337341, Accuracy: 1.0, Computation time: 1.9547255039215088\n",
      "Step: 434, Loss: 0.916343629360199, Accuracy: 1.0, Computation time: 2.2306454181671143\n",
      "Step: 435, Loss: 0.916682243347168, Accuracy: 1.0, Computation time: 2.0059289932250977\n",
      "Step: 436, Loss: 0.9179643988609314, Accuracy: 1.0, Computation time: 2.458040952682495\n",
      "Step: 437, Loss: 0.9167522192001343, Accuracy: 1.0, Computation time: 2.1388731002807617\n",
      "Step: 438, Loss: 0.9274059534072876, Accuracy: 0.96875, Computation time: 2.1735262870788574\n",
      "Step: 439, Loss: 0.9168115854263306, Accuracy: 1.0, Computation time: 2.1031441688537598\n",
      "Step: 440, Loss: 0.9165481328964233, Accuracy: 1.0, Computation time: 2.4593234062194824\n",
      "Step: 441, Loss: 0.9247731566429138, Accuracy: 1.0, Computation time: 2.9790289402008057\n",
      "Step: 442, Loss: 0.9172475337982178, Accuracy: 1.0, Computation time: 2.4531829357147217\n",
      "Step: 443, Loss: 0.9161819219589233, Accuracy: 1.0, Computation time: 2.120603322982788\n",
      "Step: 444, Loss: 0.9162406921386719, Accuracy: 1.0, Computation time: 2.476135730743408\n",
      "Step: 445, Loss: 0.9162612557411194, Accuracy: 1.0, Computation time: 2.246772050857544\n",
      "Step: 446, Loss: 0.9162024855613708, Accuracy: 1.0, Computation time: 2.072204828262329\n",
      "Step: 447, Loss: 0.916192352771759, Accuracy: 1.0, Computation time: 2.4719700813293457\n",
      "Step: 448, Loss: 0.9162025451660156, Accuracy: 1.0, Computation time: 2.047239303588867\n",
      "Step: 449, Loss: 0.9160259962081909, Accuracy: 1.0, Computation time: 2.2321152687072754\n",
      "Step: 450, Loss: 0.9159854650497437, Accuracy: 1.0, Computation time: 1.9980099201202393\n",
      "Step: 451, Loss: 0.9377826452255249, Accuracy: 0.96875, Computation time: 2.39186954498291\n",
      "Step: 452, Loss: 0.9163249731063843, Accuracy: 1.0, Computation time: 2.2046780586242676\n",
      "Step: 453, Loss: 0.9378255605697632, Accuracy: 0.96875, Computation time: 2.3264689445495605\n",
      "Step: 454, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 1.9611809253692627\n",
      "Step: 455, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.9459724426269531\n",
      "Step: 456, Loss: 0.9160820841789246, Accuracy: 1.0, Computation time: 2.243410587310791\n",
      "Step: 457, Loss: 0.9284291863441467, Accuracy: 0.96875, Computation time: 2.6332995891571045\n",
      "Step: 458, Loss: 0.916293203830719, Accuracy: 1.0, Computation time: 2.675682783126831\n",
      "Step: 459, Loss: 0.924200177192688, Accuracy: 1.0, Computation time: 3.023252010345459\n",
      "Step: 460, Loss: 0.9160940051078796, Accuracy: 1.0, Computation time: 1.9273366928100586\n",
      "Step: 461, Loss: 0.9378300905227661, Accuracy: 0.96875, Computation time: 2.089735984802246\n",
      "Step: 462, Loss: 0.9205540418624878, Accuracy: 1.0, Computation time: 2.2321736812591553\n",
      "Step: 463, Loss: 0.9165480136871338, Accuracy: 1.0, Computation time: 2.089599847793579\n",
      "Step: 464, Loss: 0.9165777564048767, Accuracy: 1.0, Computation time: 2.348283052444458\n",
      "Step: 465, Loss: 0.9161059856414795, Accuracy: 1.0, Computation time: 1.8155982494354248\n",
      "Step: 466, Loss: 0.9341670274734497, Accuracy: 0.96875, Computation time: 1.9112215042114258\n",
      "Step: 467, Loss: 0.9161266088485718, Accuracy: 1.0, Computation time: 1.96921968460083\n",
      "Step: 468, Loss: 0.9168001413345337, Accuracy: 1.0, Computation time: 2.810328960418701\n",
      "Step: 469, Loss: 0.9376645088195801, Accuracy: 0.96875, Computation time: 2.4574437141418457\n",
      "Step: 470, Loss: 0.916086733341217, Accuracy: 1.0, Computation time: 1.6536369323730469\n",
      "Step: 471, Loss: 0.9160187244415283, Accuracy: 1.0, Computation time: 1.9262733459472656\n",
      "Step: 472, Loss: 0.9161627292633057, Accuracy: 1.0, Computation time: 2.1227993965148926\n",
      "Step: 473, Loss: 0.9160394072532654, Accuracy: 1.0, Computation time: 1.9766077995300293\n",
      "Step: 474, Loss: 0.924919068813324, Accuracy: 1.0, Computation time: 2.6653096675872803\n",
      "Step: 475, Loss: 0.9220510125160217, Accuracy: 1.0, Computation time: 2.133596420288086\n",
      "Step: 476, Loss: 0.9365558624267578, Accuracy: 0.96875, Computation time: 2.486860990524292\n",
      "Step: 477, Loss: 0.9160216450691223, Accuracy: 1.0, Computation time: 2.088230609893799\n",
      "Step: 478, Loss: 0.9161365628242493, Accuracy: 1.0, Computation time: 1.866853952407837\n",
      "Step: 479, Loss: 0.9339230060577393, Accuracy: 0.96875, Computation time: 2.4726157188415527\n",
      "Step: 480, Loss: 0.957944393157959, Accuracy: 0.9375, Computation time: 2.7543230056762695\n",
      "Step: 481, Loss: 0.9204997420310974, Accuracy: 1.0, Computation time: 2.0437538623809814\n",
      "Step: 482, Loss: 0.9163694381713867, Accuracy: 1.0, Computation time: 2.4484004974365234\n",
      "Step: 483, Loss: 0.9173108339309692, Accuracy: 1.0, Computation time: 2.174753189086914\n",
      "Step: 484, Loss: 0.9349798560142517, Accuracy: 0.96875, Computation time: 2.0851616859436035\n",
      "Step: 485, Loss: 0.9373292922973633, Accuracy: 0.96875, Computation time: 3.013707160949707\n",
      "Step: 486, Loss: 0.9162192344665527, Accuracy: 1.0, Computation time: 1.9414112567901611\n",
      "Step: 487, Loss: 0.9175882339477539, Accuracy: 1.0, Computation time: 1.8787503242492676\n",
      "Step: 488, Loss: 0.9163277745246887, Accuracy: 1.0, Computation time: 2.204119920730591\n",
      "Step: 489, Loss: 0.9164095520973206, Accuracy: 1.0, Computation time: 2.153646945953369\n",
      "Step: 490, Loss: 0.9160143136978149, Accuracy: 1.0, Computation time: 1.9567809104919434\n",
      "Step: 491, Loss: 0.9166833162307739, Accuracy: 1.0, Computation time: 1.864631175994873\n",
      "Step: 492, Loss: 0.9171749949455261, Accuracy: 1.0, Computation time: 2.1315393447875977\n",
      "Step: 493, Loss: 0.9161148071289062, Accuracy: 1.0, Computation time: 2.072115421295166\n",
      "Step: 494, Loss: 0.9177917838096619, Accuracy: 1.0, Computation time: 2.1472301483154297\n",
      "Step: 495, Loss: 0.9179892539978027, Accuracy: 1.0, Computation time: 2.062640905380249\n",
      "Step: 496, Loss: 0.937635064125061, Accuracy: 0.96875, Computation time: 1.8121774196624756\n",
      "Step: 497, Loss: 0.9160493016242981, Accuracy: 1.0, Computation time: 1.7123231887817383\n",
      "Step: 498, Loss: 0.9164244532585144, Accuracy: 1.0, Computation time: 2.30442476272583\n",
      "Step: 499, Loss: 0.917397141456604, Accuracy: 1.0, Computation time: 1.970233678817749\n",
      "Step: 500, Loss: 0.9168272018432617, Accuracy: 1.0, Computation time: 1.8339638710021973\n",
      "Step: 501, Loss: 0.9160776138305664, Accuracy: 1.0, Computation time: 2.241546630859375\n",
      "Step: 502, Loss: 0.950164794921875, Accuracy: 0.9375, Computation time: 2.0078420639038086\n",
      "Step: 503, Loss: 0.9164606928825378, Accuracy: 1.0, Computation time: 1.9880516529083252\n",
      "Step: 504, Loss: 0.9298309683799744, Accuracy: 0.96875, Computation time: 2.4984679222106934\n",
      "Step: 505, Loss: 0.9162091016769409, Accuracy: 1.0, Computation time: 2.046757936477661\n",
      "Step: 506, Loss: 0.9160060882568359, Accuracy: 1.0, Computation time: 2.0837347507476807\n",
      "Step: 507, Loss: 0.9162958264350891, Accuracy: 1.0, Computation time: 1.767639398574829\n",
      "Step: 508, Loss: 0.9162273406982422, Accuracy: 1.0, Computation time: 1.8440697193145752\n",
      "Step: 509, Loss: 0.9297402501106262, Accuracy: 0.96875, Computation time: 2.802863359451294\n",
      "Step: 510, Loss: 0.9163711667060852, Accuracy: 1.0, Computation time: 2.5878379344940186\n",
      "Step: 511, Loss: 0.923092782497406, Accuracy: 1.0, Computation time: 3.1052277088165283\n",
      "Step: 512, Loss: 0.9161954522132874, Accuracy: 1.0, Computation time: 2.076430082321167\n",
      "Step: 513, Loss: 0.9391883611679077, Accuracy: 0.96875, Computation time: 1.9087448120117188\n",
      "Step: 514, Loss: 0.9161295294761658, Accuracy: 1.0, Computation time: 1.6582796573638916\n",
      "Step: 515, Loss: 0.9160119295120239, Accuracy: 1.0, Computation time: 2.043625593185425\n",
      "Step: 516, Loss: 0.9162497520446777, Accuracy: 1.0, Computation time: 1.959099292755127\n",
      "Step: 517, Loss: 0.9160204529762268, Accuracy: 1.0, Computation time: 2.1348209381103516\n",
      "Step: 518, Loss: 0.9159910082817078, Accuracy: 1.0, Computation time: 1.9595019817352295\n",
      "Step: 519, Loss: 0.9342795014381409, Accuracy: 0.96875, Computation time: 2.324422597885132\n",
      "Step: 520, Loss: 0.9177311658859253, Accuracy: 1.0, Computation time: 2.3599212169647217\n",
      "Step: 521, Loss: 0.9239324927330017, Accuracy: 1.0, Computation time: 2.058694839477539\n",
      "Step: 522, Loss: 0.9178106188774109, Accuracy: 1.0, Computation time: 2.371062994003296\n",
      "Step: 523, Loss: 0.9597225189208984, Accuracy: 0.9375, Computation time: 2.3869247436523438\n",
      "Step: 524, Loss: 0.9177616238594055, Accuracy: 1.0, Computation time: 2.229910373687744\n",
      "Step: 525, Loss: 0.9566985964775085, Accuracy: 0.9375, Computation time: 1.9262638092041016\n",
      "Step: 526, Loss: 0.9164509773254395, Accuracy: 1.0, Computation time: 2.195730447769165\n",
      "Step: 527, Loss: 0.9440702199935913, Accuracy: 0.9375, Computation time: 2.1847000122070312\n",
      "Step: 528, Loss: 0.926382303237915, Accuracy: 0.96875, Computation time: 2.7517151832580566\n",
      "Step: 529, Loss: 0.916391134262085, Accuracy: 1.0, Computation time: 2.1185009479522705\n",
      "Step: 530, Loss: 0.9164714217185974, Accuracy: 1.0, Computation time: 2.2287285327911377\n",
      "Step: 531, Loss: 0.9169458150863647, Accuracy: 1.0, Computation time: 2.3048439025878906\n",
      "Step: 532, Loss: 0.9274837970733643, Accuracy: 1.0, Computation time: 1.9931342601776123\n",
      "Step: 533, Loss: 0.9311327338218689, Accuracy: 0.96875, Computation time: 2.144069194793701\n",
      "Step: 534, Loss: 0.9160771369934082, Accuracy: 1.0, Computation time: 1.8984453678131104\n",
      "Step: 535, Loss: 0.9385308027267456, Accuracy: 0.96875, Computation time: 2.3353633880615234\n",
      "Step: 536, Loss: 0.9380014538764954, Accuracy: 0.96875, Computation time: 2.5050151348114014\n",
      "Step: 537, Loss: 0.9174093008041382, Accuracy: 1.0, Computation time: 2.357419729232788\n",
      "Step: 538, Loss: 0.929408848285675, Accuracy: 0.96875, Computation time: 2.581454277038574\n",
      "Step: 539, Loss: 0.9227684736251831, Accuracy: 1.0, Computation time: 2.8870415687561035\n",
      "Step: 540, Loss: 0.9166181683540344, Accuracy: 1.0, Computation time: 2.519892930984497\n",
      "Step: 541, Loss: 0.9168443083763123, Accuracy: 1.0, Computation time: 2.0381548404693604\n",
      "Step: 542, Loss: 0.9163787961006165, Accuracy: 1.0, Computation time: 2.2695155143737793\n",
      "Step: 543, Loss: 0.9164664149284363, Accuracy: 1.0, Computation time: 1.8895978927612305\n",
      "Step: 544, Loss: 0.9161386489868164, Accuracy: 1.0, Computation time: 2.098375082015991\n",
      "Step: 545, Loss: 0.9164050221443176, Accuracy: 1.0, Computation time: 2.4656689167022705\n",
      "Step: 546, Loss: 0.9166935086250305, Accuracy: 1.0, Computation time: 1.9804422855377197\n",
      "Step: 547, Loss: 0.9180344939231873, Accuracy: 1.0, Computation time: 2.486238718032837\n",
      "Step: 548, Loss: 0.9185631275177002, Accuracy: 1.0, Computation time: 2.9492013454437256\n",
      "Step: 549, Loss: 0.9161846041679382, Accuracy: 1.0, Computation time: 2.6456336975097656\n",
      "Step: 550, Loss: 0.9380995631217957, Accuracy: 0.96875, Computation time: 2.1615073680877686\n",
      "Step: 551, Loss: 0.9167904853820801, Accuracy: 1.0, Computation time: 2.2787201404571533\n",
      "Step: 552, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 2.086918354034424\n",
      "Step: 553, Loss: 0.9371904134750366, Accuracy: 0.96875, Computation time: 2.394082546234131\n",
      "Step: 554, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 1.9870593547821045\n",
      "Step: 555, Loss: 0.916376531124115, Accuracy: 1.0, Computation time: 2.6827027797698975\n",
      "Step: 556, Loss: 0.9374553561210632, Accuracy: 0.96875, Computation time: 2.2767627239227295\n",
      "########################\n",
      "Test loss: 1.1223621368408203, Test Accuracy_epoch4: 0.6954069137573242\n",
      "########################\n",
      "Step: 557, Loss: 0.9159584641456604, Accuracy: 1.0, Computation time: 2.05519700050354\n",
      "Step: 558, Loss: 0.9266282320022583, Accuracy: 0.96875, Computation time: 2.2382991313934326\n",
      "Step: 559, Loss: 0.9168857932090759, Accuracy: 1.0, Computation time: 2.122147798538208\n",
      "Step: 560, Loss: 0.9159789085388184, Accuracy: 1.0, Computation time: 2.0735220909118652\n",
      "Step: 561, Loss: 0.9161313772201538, Accuracy: 1.0, Computation time: 1.8626580238342285\n",
      "Step: 562, Loss: 0.9160584211349487, Accuracy: 1.0, Computation time: 1.940662145614624\n",
      "Step: 563, Loss: 0.9336074590682983, Accuracy: 0.96875, Computation time: 2.2925808429718018\n",
      "Step: 564, Loss: 0.9162203669548035, Accuracy: 1.0, Computation time: 2.1244733333587646\n",
      "Step: 565, Loss: 0.9160153269767761, Accuracy: 1.0, Computation time: 2.2844815254211426\n",
      "Step: 566, Loss: 0.9351044297218323, Accuracy: 0.96875, Computation time: 2.114779233932495\n",
      "Step: 567, Loss: 0.9192093014717102, Accuracy: 1.0, Computation time: 2.38405442237854\n",
      "Step: 568, Loss: 0.9285853505134583, Accuracy: 0.96875, Computation time: 2.549272060394287\n",
      "Step: 569, Loss: 0.920501172542572, Accuracy: 1.0, Computation time: 2.2683756351470947\n",
      "Step: 570, Loss: 0.9161139130592346, Accuracy: 1.0, Computation time: 2.0986578464508057\n",
      "Step: 571, Loss: 0.9300353527069092, Accuracy: 0.96875, Computation time: 2.2956736087799072\n",
      "Step: 572, Loss: 0.916014552116394, Accuracy: 1.0, Computation time: 2.1032094955444336\n",
      "Step: 573, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 2.128380060195923\n",
      "Step: 574, Loss: 0.9203272461891174, Accuracy: 1.0, Computation time: 2.641266345977783\n",
      "Step: 575, Loss: 0.9161276817321777, Accuracy: 1.0, Computation time: 2.185978651046753\n",
      "Step: 576, Loss: 0.9163633584976196, Accuracy: 1.0, Computation time: 1.9125912189483643\n",
      "Step: 577, Loss: 0.9160251021385193, Accuracy: 1.0, Computation time: 2.2555174827575684\n",
      "Step: 578, Loss: 0.925646960735321, Accuracy: 0.96875, Computation time: 2.615403890609741\n",
      "Step: 579, Loss: 0.9159542918205261, Accuracy: 1.0, Computation time: 2.3183350563049316\n",
      "Step: 580, Loss: 0.9180148839950562, Accuracy: 1.0, Computation time: 2.262321710586548\n",
      "Step: 581, Loss: 0.9358627200126648, Accuracy: 0.96875, Computation time: 2.282153606414795\n",
      "Step: 582, Loss: 0.915958821773529, Accuracy: 1.0, Computation time: 2.1014132499694824\n",
      "Step: 583, Loss: 0.916128933429718, Accuracy: 1.0, Computation time: 2.33730411529541\n",
      "Step: 584, Loss: 0.9367491602897644, Accuracy: 0.96875, Computation time: 2.187234878540039\n",
      "Step: 585, Loss: 0.9161248803138733, Accuracy: 1.0, Computation time: 2.423029899597168\n",
      "Step: 586, Loss: 0.941945493221283, Accuracy: 0.96875, Computation time: 2.2518179416656494\n",
      "Step: 587, Loss: 0.9162279963493347, Accuracy: 1.0, Computation time: 2.0835697650909424\n",
      "Step: 588, Loss: 0.9376704692840576, Accuracy: 0.96875, Computation time: 2.0434043407440186\n",
      "Step: 589, Loss: 0.9282717704772949, Accuracy: 0.96875, Computation time: 2.5655975341796875\n",
      "Step: 590, Loss: 0.9427139163017273, Accuracy: 0.96875, Computation time: 2.032175302505493\n",
      "Step: 591, Loss: 0.9191383123397827, Accuracy: 1.0, Computation time: 2.4475512504577637\n",
      "Step: 592, Loss: 0.9182667136192322, Accuracy: 1.0, Computation time: 2.5193262100219727\n",
      "Step: 593, Loss: 0.9160343408584595, Accuracy: 1.0, Computation time: 1.8106591701507568\n",
      "Step: 594, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 1.8455073833465576\n",
      "Step: 595, Loss: 0.9163727164268494, Accuracy: 1.0, Computation time: 2.1315057277679443\n",
      "Step: 596, Loss: 0.9162458777427673, Accuracy: 1.0, Computation time: 1.9929041862487793\n",
      "Step: 597, Loss: 0.9160854816436768, Accuracy: 1.0, Computation time: 1.9967477321624756\n",
      "Step: 598, Loss: 0.9160597324371338, Accuracy: 1.0, Computation time: 2.0108084678649902\n",
      "Step: 599, Loss: 0.9372406601905823, Accuracy: 0.96875, Computation time: 2.6417136192321777\n",
      "Step: 600, Loss: 0.9436264038085938, Accuracy: 0.96875, Computation time: 3.0750436782836914\n",
      "Step: 601, Loss: 0.9380511045455933, Accuracy: 0.96875, Computation time: 2.1324808597564697\n",
      "Step: 602, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.9771111011505127\n",
      "Step: 603, Loss: 0.9326578378677368, Accuracy: 0.96875, Computation time: 2.073185682296753\n",
      "Step: 604, Loss: 0.9160983562469482, Accuracy: 1.0, Computation time: 2.6673429012298584\n",
      "Step: 605, Loss: 0.9160833358764648, Accuracy: 1.0, Computation time: 2.020718574523926\n",
      "Step: 606, Loss: 0.9193862676620483, Accuracy: 1.0, Computation time: 2.406826972961426\n",
      "Step: 607, Loss: 0.931952714920044, Accuracy: 0.96875, Computation time: 2.270648717880249\n",
      "Step: 608, Loss: 0.9174327850341797, Accuracy: 1.0, Computation time: 1.877565860748291\n",
      "Step: 609, Loss: 0.916039764881134, Accuracy: 1.0, Computation time: 2.1168107986450195\n",
      "Step: 610, Loss: 0.9161266088485718, Accuracy: 1.0, Computation time: 2.2627789974212646\n",
      "Step: 611, Loss: 0.92051762342453, Accuracy: 1.0, Computation time: 2.1999001502990723\n",
      "Step: 612, Loss: 0.9160047173500061, Accuracy: 1.0, Computation time: 1.978574514389038\n",
      "Step: 613, Loss: 0.9160438179969788, Accuracy: 1.0, Computation time: 2.0847716331481934\n",
      "Step: 614, Loss: 0.9304055571556091, Accuracy: 0.96875, Computation time: 2.089796304702759\n",
      "Step: 615, Loss: 0.916316032409668, Accuracy: 1.0, Computation time: 2.0493581295013428\n",
      "Step: 616, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 2.052229881286621\n",
      "Step: 617, Loss: 0.9160222411155701, Accuracy: 1.0, Computation time: 2.100764274597168\n",
      "Step: 618, Loss: 0.9162032604217529, Accuracy: 1.0, Computation time: 2.088819980621338\n",
      "Step: 619, Loss: 0.9161689877510071, Accuracy: 1.0, Computation time: 2.5511178970336914\n",
      "Step: 620, Loss: 0.924924910068512, Accuracy: 1.0, Computation time: 2.997093915939331\n",
      "Step: 621, Loss: 0.9211710095405579, Accuracy: 1.0, Computation time: 2.6305220127105713\n",
      "Step: 622, Loss: 0.9355754256248474, Accuracy: 0.96875, Computation time: 3.003328800201416\n",
      "Step: 623, Loss: 0.9163030982017517, Accuracy: 1.0, Computation time: 2.5341360569000244\n",
      "Step: 624, Loss: 0.9368773698806763, Accuracy: 0.96875, Computation time: 2.509913444519043\n",
      "Step: 625, Loss: 0.9165334105491638, Accuracy: 1.0, Computation time: 2.2972352504730225\n",
      "Step: 626, Loss: 0.917090117931366, Accuracy: 1.0, Computation time: 2.43497371673584\n",
      "Step: 627, Loss: 0.9163233041763306, Accuracy: 1.0, Computation time: 1.7338523864746094\n",
      "Step: 628, Loss: 0.9560507535934448, Accuracy: 0.9375, Computation time: 1.9476268291473389\n",
      "Step: 629, Loss: 0.9590184092521667, Accuracy: 0.9375, Computation time: 2.4358396530151367\n",
      "Step: 630, Loss: 0.916357159614563, Accuracy: 1.0, Computation time: 2.435645341873169\n",
      "Step: 631, Loss: 0.9512607455253601, Accuracy: 0.9375, Computation time: 2.049185276031494\n",
      "Step: 632, Loss: 0.9160326719284058, Accuracy: 1.0, Computation time: 2.1376147270202637\n",
      "Step: 633, Loss: 0.9173996448516846, Accuracy: 1.0, Computation time: 2.622281551361084\n",
      "Step: 634, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 1.7063133716583252\n",
      "Step: 635, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.9643747806549072\n",
      "Step: 636, Loss: 0.9160019755363464, Accuracy: 1.0, Computation time: 2.9317305088043213\n",
      "Step: 637, Loss: 0.9375513195991516, Accuracy: 0.96875, Computation time: 2.041365385055542\n",
      "Step: 638, Loss: 0.9160254597663879, Accuracy: 1.0, Computation time: 2.0005412101745605\n",
      "Step: 639, Loss: 0.9160975217819214, Accuracy: 1.0, Computation time: 1.7176647186279297\n",
      "Step: 640, Loss: 0.9179999828338623, Accuracy: 1.0, Computation time: 2.209260940551758\n",
      "Step: 641, Loss: 0.9211580157279968, Accuracy: 1.0, Computation time: 2.247187852859497\n",
      "Step: 642, Loss: 0.9162471294403076, Accuracy: 1.0, Computation time: 2.4523043632507324\n",
      "Step: 643, Loss: 0.9161843061447144, Accuracy: 1.0, Computation time: 2.0979561805725098\n",
      "Step: 644, Loss: 0.9160972237586975, Accuracy: 1.0, Computation time: 1.960674524307251\n",
      "Step: 645, Loss: 0.9370293617248535, Accuracy: 0.96875, Computation time: 1.7472624778747559\n",
      "Step: 646, Loss: 0.9303315877914429, Accuracy: 0.96875, Computation time: 2.33594012260437\n",
      "Step: 647, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 2.1496567726135254\n",
      "Step: 648, Loss: 0.9164107441902161, Accuracy: 1.0, Computation time: 1.9068927764892578\n",
      "Step: 649, Loss: 0.9179673790931702, Accuracy: 1.0, Computation time: 2.180889368057251\n",
      "Step: 650, Loss: 0.9571210741996765, Accuracy: 0.9375, Computation time: 2.484724760055542\n",
      "Step: 651, Loss: 0.9165205359458923, Accuracy: 1.0, Computation time: 2.41782283782959\n",
      "Step: 652, Loss: 0.9161723256111145, Accuracy: 1.0, Computation time: 1.785463809967041\n",
      "Step: 653, Loss: 0.9181371331214905, Accuracy: 1.0, Computation time: 2.4598379135131836\n",
      "Step: 654, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 1.8000257015228271\n",
      "Step: 655, Loss: 0.9160483479499817, Accuracy: 1.0, Computation time: 2.1955463886260986\n",
      "Step: 656, Loss: 0.918221652507782, Accuracy: 1.0, Computation time: 2.202547073364258\n",
      "Step: 657, Loss: 0.9161279797554016, Accuracy: 1.0, Computation time: 2.421062707901001\n",
      "Step: 658, Loss: 0.9159688949584961, Accuracy: 1.0, Computation time: 2.0106077194213867\n",
      "Step: 659, Loss: 0.9293883442878723, Accuracy: 0.96875, Computation time: 2.086822509765625\n",
      "Step: 660, Loss: 0.9239887595176697, Accuracy: 1.0, Computation time: 2.4677324295043945\n",
      "Step: 661, Loss: 0.9160671234130859, Accuracy: 1.0, Computation time: 1.992236614227295\n",
      "Step: 662, Loss: 0.9215367436408997, Accuracy: 1.0, Computation time: 2.400282859802246\n",
      "Step: 663, Loss: 0.9374915361404419, Accuracy: 0.96875, Computation time: 1.9485437870025635\n",
      "Step: 664, Loss: 0.9371911883354187, Accuracy: 0.96875, Computation time: 2.276092529296875\n",
      "Step: 665, Loss: 0.9317425489425659, Accuracy: 0.96875, Computation time: 3.879465103149414\n",
      "Step: 666, Loss: 0.9192774295806885, Accuracy: 1.0, Computation time: 3.4461658000946045\n",
      "Step: 667, Loss: 0.9164589643478394, Accuracy: 1.0, Computation time: 2.056448221206665\n",
      "Step: 668, Loss: 0.9165098071098328, Accuracy: 1.0, Computation time: 1.9045524597167969\n",
      "Step: 669, Loss: 0.9170210361480713, Accuracy: 1.0, Computation time: 2.1705379486083984\n",
      "Step: 670, Loss: 0.9236438274383545, Accuracy: 1.0, Computation time: 2.805905342102051\n",
      "Step: 671, Loss: 0.9385616779327393, Accuracy: 0.96875, Computation time: 1.6782917976379395\n",
      "Step: 672, Loss: 0.9170308113098145, Accuracy: 1.0, Computation time: 2.630124807357788\n",
      "Step: 673, Loss: 0.91853928565979, Accuracy: 1.0, Computation time: 3.2747585773468018\n",
      "Step: 674, Loss: 0.917644739151001, Accuracy: 1.0, Computation time: 2.123565435409546\n",
      "Step: 675, Loss: 0.9165197610855103, Accuracy: 1.0, Computation time: 1.933957576751709\n",
      "Step: 676, Loss: 0.9162053465843201, Accuracy: 1.0, Computation time: 2.0101685523986816\n",
      "Step: 677, Loss: 0.9163833856582642, Accuracy: 1.0, Computation time: 2.655902624130249\n",
      "Step: 678, Loss: 0.9313886761665344, Accuracy: 0.96875, Computation time: 2.4269940853118896\n",
      "Step: 679, Loss: 0.9355947375297546, Accuracy: 0.96875, Computation time: 2.184772491455078\n",
      "Step: 680, Loss: 0.9165389537811279, Accuracy: 1.0, Computation time: 2.052396059036255\n",
      "Step: 681, Loss: 0.9339739680290222, Accuracy: 0.96875, Computation time: 4.745480298995972\n",
      "Step: 682, Loss: 0.9374180436134338, Accuracy: 0.96875, Computation time: 1.7805700302124023\n",
      "Step: 683, Loss: 0.9413992762565613, Accuracy: 0.96875, Computation time: 2.3883779048919678\n",
      "Step: 684, Loss: 0.9161614775657654, Accuracy: 1.0, Computation time: 2.0530800819396973\n",
      "Step: 685, Loss: 0.9160614013671875, Accuracy: 1.0, Computation time: 1.7941310405731201\n",
      "Step: 686, Loss: 0.9162726998329163, Accuracy: 1.0, Computation time: 1.993706464767456\n",
      "Step: 687, Loss: 0.9159640669822693, Accuracy: 1.0, Computation time: 2.0989561080932617\n",
      "Step: 688, Loss: 0.9385015964508057, Accuracy: 0.96875, Computation time: 2.249080181121826\n",
      "Step: 689, Loss: 0.9161452054977417, Accuracy: 1.0, Computation time: 1.9736194610595703\n",
      "Step: 690, Loss: 0.9321618676185608, Accuracy: 0.96875, Computation time: 2.501410961151123\n",
      "Step: 691, Loss: 0.9357783198356628, Accuracy: 0.96875, Computation time: 2.5961337089538574\n",
      "Step: 692, Loss: 0.9162436127662659, Accuracy: 1.0, Computation time: 2.352104663848877\n",
      "Step: 693, Loss: 0.9160685539245605, Accuracy: 1.0, Computation time: 2.4480254650115967\n",
      "Step: 694, Loss: 0.9163317680358887, Accuracy: 1.0, Computation time: 1.732337236404419\n",
      "Step: 695, Loss: 0.9161055684089661, Accuracy: 1.0, Computation time: 1.8029005527496338\n",
      "########################\n",
      "Test loss: 1.1237276792526245, Test Accuracy_epoch5: 0.6937953233718872\n",
      "########################\n",
      "Step: 696, Loss: 0.9160887598991394, Accuracy: 1.0, Computation time: 2.2479429244995117\n",
      "Step: 697, Loss: 0.9170593023300171, Accuracy: 1.0, Computation time: 2.47861385345459\n",
      "Step: 698, Loss: 0.9160934686660767, Accuracy: 1.0, Computation time: 2.017642021179199\n",
      "Step: 699, Loss: 0.9161015748977661, Accuracy: 1.0, Computation time: 2.002181053161621\n",
      "Step: 700, Loss: 0.9162111878395081, Accuracy: 1.0, Computation time: 2.0806009769439697\n",
      "Step: 701, Loss: 0.9159488081932068, Accuracy: 1.0, Computation time: 1.8910901546478271\n",
      "Step: 702, Loss: 0.9159741401672363, Accuracy: 1.0, Computation time: 2.402250289916992\n",
      "Step: 703, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 1.9033782482147217\n",
      "Step: 704, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 2.2053158283233643\n",
      "Step: 705, Loss: 0.9160679578781128, Accuracy: 1.0, Computation time: 1.834742546081543\n",
      "Step: 706, Loss: 0.9160281419754028, Accuracy: 1.0, Computation time: 2.4355225563049316\n",
      "Step: 707, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.8913938999176025\n",
      "Step: 708, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 2.871016263961792\n",
      "Step: 709, Loss: 0.9159919023513794, Accuracy: 1.0, Computation time: 2.1456356048583984\n",
      "Step: 710, Loss: 0.9406713843345642, Accuracy: 0.96875, Computation time: 2.320007562637329\n",
      "Step: 711, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.9230775833129883\n",
      "Step: 712, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 2.1130645275115967\n",
      "Step: 713, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.956315279006958\n",
      "Step: 714, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 2.033582925796509\n",
      "Step: 715, Loss: 0.9165970683097839, Accuracy: 1.0, Computation time: 2.2233917713165283\n",
      "Step: 716, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 2.061873197555542\n",
      "Step: 717, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 2.389857053756714\n",
      "Step: 718, Loss: 0.9160382151603699, Accuracy: 1.0, Computation time: 1.9896819591522217\n",
      "Step: 719, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.9180505275726318\n",
      "Step: 720, Loss: 0.9160659313201904, Accuracy: 1.0, Computation time: 1.8409883975982666\n",
      "Step: 721, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.8843517303466797\n",
      "Step: 722, Loss: 0.9378247857093811, Accuracy: 0.96875, Computation time: 2.140641450881958\n",
      "Step: 723, Loss: 0.9160083532333374, Accuracy: 1.0, Computation time: 2.3073959350585938\n",
      "Step: 724, Loss: 0.9351756572723389, Accuracy: 0.96875, Computation time: 2.4512321949005127\n",
      "Step: 725, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.8906464576721191\n",
      "Step: 726, Loss: 0.916132390499115, Accuracy: 1.0, Computation time: 2.128279209136963\n",
      "Step: 727, Loss: 0.9381569027900696, Accuracy: 0.96875, Computation time: 3.0401949882507324\n",
      "Step: 728, Loss: 0.9160870313644409, Accuracy: 1.0, Computation time: 2.4150469303131104\n",
      "Step: 729, Loss: 0.9356998205184937, Accuracy: 0.96875, Computation time: 1.9473152160644531\n",
      "Step: 730, Loss: 0.9161977767944336, Accuracy: 1.0, Computation time: 2.0254971981048584\n",
      "Step: 731, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 2.085775136947632\n",
      "Step: 732, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 2.268829345703125\n",
      "Step: 733, Loss: 0.9162072539329529, Accuracy: 1.0, Computation time: 3.045517921447754\n",
      "Step: 734, Loss: 0.9371333718299866, Accuracy: 0.96875, Computation time: 2.506844997406006\n",
      "Step: 735, Loss: 0.9372300505638123, Accuracy: 0.96875, Computation time: 2.0363686084747314\n",
      "Step: 736, Loss: 0.916040301322937, Accuracy: 1.0, Computation time: 1.8889384269714355\n",
      "Step: 737, Loss: 0.916636049747467, Accuracy: 1.0, Computation time: 2.4751031398773193\n",
      "Step: 738, Loss: 0.916327178478241, Accuracy: 1.0, Computation time: 1.8114469051361084\n",
      "Step: 739, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 2.4662182331085205\n",
      "Step: 740, Loss: 0.916059672832489, Accuracy: 1.0, Computation time: 2.0664596557617188\n",
      "Step: 741, Loss: 0.9161105751991272, Accuracy: 1.0, Computation time: 1.8885281085968018\n",
      "Step: 742, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 2.008936643600464\n",
      "Step: 743, Loss: 0.9372507929801941, Accuracy: 0.96875, Computation time: 2.2265877723693848\n",
      "Step: 744, Loss: 0.9159486889839172, Accuracy: 1.0, Computation time: 2.0982024669647217\n",
      "Step: 745, Loss: 0.916225016117096, Accuracy: 1.0, Computation time: 2.4397335052490234\n",
      "Step: 746, Loss: 0.9160827994346619, Accuracy: 1.0, Computation time: 2.2735097408294678\n",
      "Step: 747, Loss: 0.91615229845047, Accuracy: 1.0, Computation time: 1.8496966361999512\n",
      "Step: 748, Loss: 0.9160986542701721, Accuracy: 1.0, Computation time: 1.9694654941558838\n",
      "Step: 749, Loss: 0.9371728897094727, Accuracy: 0.96875, Computation time: 2.1418495178222656\n",
      "Step: 750, Loss: 0.9590867757797241, Accuracy: 0.9375, Computation time: 1.7803423404693604\n",
      "Step: 751, Loss: 0.9185171127319336, Accuracy: 1.0, Computation time: 2.4858548641204834\n",
      "Step: 752, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.720043659210205\n",
      "Step: 753, Loss: 0.9288241863250732, Accuracy: 0.96875, Computation time: 2.427607297897339\n",
      "Step: 754, Loss: 0.9220568537712097, Accuracy: 1.0, Computation time: 1.892845630645752\n",
      "Step: 755, Loss: 0.929785430431366, Accuracy: 0.96875, Computation time: 1.7422077655792236\n",
      "Step: 756, Loss: 0.9160255789756775, Accuracy: 1.0, Computation time: 2.1883344650268555\n",
      "Step: 757, Loss: 0.9160451292991638, Accuracy: 1.0, Computation time: 2.8832504749298096\n",
      "Step: 758, Loss: 0.9170128107070923, Accuracy: 1.0, Computation time: 2.526033639907837\n",
      "Step: 759, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 2.516094923019409\n",
      "Step: 760, Loss: 0.9159814715385437, Accuracy: 1.0, Computation time: 2.2447218894958496\n",
      "Step: 761, Loss: 0.9193969368934631, Accuracy: 1.0, Computation time: 2.1477270126342773\n",
      "Step: 762, Loss: 0.9178923964500427, Accuracy: 1.0, Computation time: 2.434649705886841\n",
      "Step: 763, Loss: 0.9535703659057617, Accuracy: 0.9375, Computation time: 1.905712366104126\n",
      "Step: 764, Loss: 0.9375211596488953, Accuracy: 0.96875, Computation time: 2.076146125793457\n",
      "Step: 765, Loss: 0.9590507745742798, Accuracy: 0.9375, Computation time: 1.8786976337432861\n",
      "Step: 766, Loss: 0.9266821146011353, Accuracy: 0.96875, Computation time: 2.35622239112854\n",
      "Step: 767, Loss: 0.9162384271621704, Accuracy: 1.0, Computation time: 2.2093570232391357\n",
      "Step: 768, Loss: 0.9591171741485596, Accuracy: 0.9375, Computation time: 2.166654109954834\n",
      "Step: 769, Loss: 0.9170674085617065, Accuracy: 1.0, Computation time: 2.2052574157714844\n",
      "Step: 770, Loss: 0.9161185622215271, Accuracy: 1.0, Computation time: 2.037069797515869\n",
      "Step: 771, Loss: 0.9181039333343506, Accuracy: 1.0, Computation time: 2.1786866188049316\n",
      "Step: 772, Loss: 0.9163451790809631, Accuracy: 1.0, Computation time: 1.9122281074523926\n",
      "Step: 773, Loss: 0.9376314282417297, Accuracy: 0.96875, Computation time: 2.1372203826904297\n",
      "Step: 774, Loss: 0.916610836982727, Accuracy: 1.0, Computation time: 2.2778851985931396\n",
      "Step: 775, Loss: 0.9159658551216125, Accuracy: 1.0, Computation time: 2.461113691329956\n",
      "Step: 776, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 2.1861586570739746\n",
      "Step: 777, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 2.1717634201049805\n",
      "Step: 778, Loss: 0.9159733057022095, Accuracy: 1.0, Computation time: 2.466548204421997\n",
      "Step: 779, Loss: 0.9253968000411987, Accuracy: 1.0, Computation time: 2.2118940353393555\n",
      "Step: 780, Loss: 0.9366868138313293, Accuracy: 0.96875, Computation time: 2.346478223800659\n",
      "Step: 781, Loss: 0.9162374138832092, Accuracy: 1.0, Computation time: 2.3806562423706055\n",
      "Step: 782, Loss: 0.9160284399986267, Accuracy: 1.0, Computation time: 1.8947300910949707\n",
      "Step: 783, Loss: 0.9438338875770569, Accuracy: 0.9375, Computation time: 2.0890164375305176\n",
      "Step: 784, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 2.1105377674102783\n",
      "Step: 785, Loss: 0.9159628748893738, Accuracy: 1.0, Computation time: 1.7848656177520752\n",
      "Step: 786, Loss: 0.9173766374588013, Accuracy: 1.0, Computation time: 2.0392754077911377\n",
      "Step: 787, Loss: 0.9159579277038574, Accuracy: 1.0, Computation time: 2.186929941177368\n",
      "Step: 788, Loss: 0.9160701632499695, Accuracy: 1.0, Computation time: 2.0932013988494873\n",
      "Step: 789, Loss: 0.9165693521499634, Accuracy: 1.0, Computation time: 2.228625774383545\n",
      "Step: 790, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 2.180800199508667\n",
      "Step: 791, Loss: 0.9160230755805969, Accuracy: 1.0, Computation time: 2.490139961242676\n",
      "Step: 792, Loss: 0.9171777963638306, Accuracy: 1.0, Computation time: 2.617142915725708\n",
      "Step: 793, Loss: 0.9376646876335144, Accuracy: 0.96875, Computation time: 2.699650287628174\n",
      "Step: 794, Loss: 0.9159905314445496, Accuracy: 1.0, Computation time: 2.357872724533081\n",
      "Step: 795, Loss: 0.9429871439933777, Accuracy: 0.96875, Computation time: 2.0768349170684814\n",
      "Step: 796, Loss: 0.9691847562789917, Accuracy: 0.90625, Computation time: 2.811901569366455\n",
      "Step: 797, Loss: 0.9159610867500305, Accuracy: 1.0, Computation time: 2.070183753967285\n",
      "Step: 798, Loss: 0.9160584211349487, Accuracy: 1.0, Computation time: 1.965329885482788\n",
      "Step: 799, Loss: 0.9163939356803894, Accuracy: 1.0, Computation time: 2.527304172515869\n",
      "Step: 800, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 2.228091239929199\n",
      "Step: 801, Loss: 0.9341437220573425, Accuracy: 0.96875, Computation time: 2.0525801181793213\n",
      "Step: 802, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 2.138400077819824\n",
      "Step: 803, Loss: 0.9159297347068787, Accuracy: 1.0, Computation time: 1.9658658504486084\n",
      "Step: 804, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.8422789573669434\n",
      "Step: 805, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 2.3179662227630615\n",
      "Step: 806, Loss: 0.9161481857299805, Accuracy: 1.0, Computation time: 2.2665257453918457\n",
      "Step: 807, Loss: 0.9180372953414917, Accuracy: 1.0, Computation time: 2.3886830806732178\n",
      "Step: 808, Loss: 0.9372831583023071, Accuracy: 0.96875, Computation time: 2.282193422317505\n",
      "Step: 809, Loss: 0.9169172644615173, Accuracy: 1.0, Computation time: 2.9246938228607178\n",
      "Step: 810, Loss: 0.92232346534729, Accuracy: 1.0, Computation time: 2.264341115951538\n",
      "Step: 811, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 2.1548805236816406\n",
      "Step: 812, Loss: 0.9168389439582825, Accuracy: 1.0, Computation time: 2.587233066558838\n",
      "Step: 813, Loss: 0.9167447686195374, Accuracy: 1.0, Computation time: 1.8649728298187256\n",
      "Step: 814, Loss: 0.9159121513366699, Accuracy: 1.0, Computation time: 2.273052930831909\n",
      "Step: 815, Loss: 0.9162726402282715, Accuracy: 1.0, Computation time: 2.040067672729492\n",
      "Step: 816, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.9245421886444092\n",
      "Step: 817, Loss: 0.930216372013092, Accuracy: 0.96875, Computation time: 2.144883632659912\n",
      "Step: 818, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 2.532658576965332\n",
      "Step: 819, Loss: 0.916156530380249, Accuracy: 1.0, Computation time: 2.1575608253479004\n",
      "Step: 820, Loss: 0.9160122275352478, Accuracy: 1.0, Computation time: 2.3490512371063232\n",
      "Step: 821, Loss: 0.9543466567993164, Accuracy: 0.9375, Computation time: 3.1588757038116455\n",
      "Step: 822, Loss: 0.9160034656524658, Accuracy: 1.0, Computation time: 1.75506591796875\n",
      "Step: 823, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 2.2191243171691895\n",
      "Step: 824, Loss: 0.9160200357437134, Accuracy: 1.0, Computation time: 1.9201130867004395\n",
      "Step: 825, Loss: 0.9161303639411926, Accuracy: 1.0, Computation time: 2.2685706615448\n",
      "Step: 826, Loss: 0.9160677790641785, Accuracy: 1.0, Computation time: 2.033734083175659\n",
      "Step: 827, Loss: 0.9386283159255981, Accuracy: 0.96875, Computation time: 2.0223584175109863\n",
      "Step: 828, Loss: 0.9229429364204407, Accuracy: 1.0, Computation time: 2.9032065868377686\n",
      "Step: 829, Loss: 0.934482991695404, Accuracy: 0.96875, Computation time: 2.305396795272827\n",
      "Step: 830, Loss: 0.915972113609314, Accuracy: 1.0, Computation time: 1.986293077468872\n",
      "Step: 831, Loss: 0.9375187754631042, Accuracy: 0.96875, Computation time: 2.3753976821899414\n",
      "Step: 832, Loss: 0.9372943043708801, Accuracy: 0.96875, Computation time: 2.2274365425109863\n",
      "Step: 833, Loss: 0.9160900712013245, Accuracy: 1.0, Computation time: 2.297219753265381\n",
      "Step: 834, Loss: 0.9162039756774902, Accuracy: 1.0, Computation time: 1.9441606998443604\n",
      "########################\n",
      "Test loss: 1.1255444288253784, Test Accuracy_epoch6: 0.6937953233718872\n",
      "########################\n",
      "Step: 835, Loss: 0.9396094679832458, Accuracy: 0.96875, Computation time: 1.793210506439209\n",
      "Step: 836, Loss: 0.9160686135292053, Accuracy: 1.0, Computation time: 2.2458693981170654\n",
      "Step: 837, Loss: 0.9373946189880371, Accuracy: 0.96875, Computation time: 1.9939029216766357\n",
      "Step: 838, Loss: 0.9159484505653381, Accuracy: 1.0, Computation time: 1.8073787689208984\n",
      "Step: 839, Loss: 0.916006863117218, Accuracy: 1.0, Computation time: 2.139960527420044\n",
      "Step: 840, Loss: 0.9162156581878662, Accuracy: 1.0, Computation time: 2.0776760578155518\n",
      "Step: 841, Loss: 0.9376112222671509, Accuracy: 0.96875, Computation time: 1.9429867267608643\n",
      "Step: 842, Loss: 0.915977418422699, Accuracy: 1.0, Computation time: 1.8590342998504639\n",
      "Step: 843, Loss: 0.91600501537323, Accuracy: 1.0, Computation time: 2.167527437210083\n",
      "Step: 844, Loss: 0.9160292148590088, Accuracy: 1.0, Computation time: 2.065981149673462\n",
      "Step: 845, Loss: 0.9168316721916199, Accuracy: 1.0, Computation time: 2.0226786136627197\n",
      "Step: 846, Loss: 0.9164501428604126, Accuracy: 1.0, Computation time: 1.8237714767456055\n",
      "Step: 847, Loss: 0.937715470790863, Accuracy: 0.96875, Computation time: 2.0200724601745605\n",
      "Step: 848, Loss: 0.9160444736480713, Accuracy: 1.0, Computation time: 2.0949461460113525\n",
      "Step: 849, Loss: 0.9364890456199646, Accuracy: 0.96875, Computation time: 2.3291428089141846\n",
      "Step: 850, Loss: 0.9159848690032959, Accuracy: 1.0, Computation time: 1.980851650238037\n",
      "Step: 851, Loss: 0.9169161319732666, Accuracy: 1.0, Computation time: 2.9221723079681396\n",
      "Step: 852, Loss: 0.940424919128418, Accuracy: 0.96875, Computation time: 3.2432355880737305\n",
      "Step: 853, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 2.3466155529022217\n",
      "Step: 854, Loss: 0.9160555601119995, Accuracy: 1.0, Computation time: 1.9261088371276855\n",
      "Step: 855, Loss: 0.9160275459289551, Accuracy: 1.0, Computation time: 1.7188677787780762\n",
      "Step: 856, Loss: 0.9161308407783508, Accuracy: 1.0, Computation time: 2.130952835083008\n",
      "Step: 857, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 1.507472276687622\n",
      "Step: 858, Loss: 0.9219450950622559, Accuracy: 1.0, Computation time: 2.640627861022949\n",
      "Step: 859, Loss: 0.9363375902175903, Accuracy: 0.96875, Computation time: 2.2085766792297363\n",
      "Step: 860, Loss: 0.9160208702087402, Accuracy: 1.0, Computation time: 1.9988834857940674\n",
      "Step: 861, Loss: 0.9160216450691223, Accuracy: 1.0, Computation time: 2.0012929439544678\n",
      "Step: 862, Loss: 0.9160064458847046, Accuracy: 1.0, Computation time: 1.8869590759277344\n",
      "Step: 863, Loss: 0.9343357682228088, Accuracy: 0.96875, Computation time: 2.9628429412841797\n",
      "Step: 864, Loss: 0.9376093745231628, Accuracy: 0.96875, Computation time: 1.941960096359253\n",
      "Step: 865, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 2.2141804695129395\n",
      "Step: 866, Loss: 0.9181538224220276, Accuracy: 1.0, Computation time: 2.2129063606262207\n",
      "Step: 867, Loss: 0.937768280506134, Accuracy: 0.96875, Computation time: 2.367062568664551\n",
      "Step: 868, Loss: 0.9159799218177795, Accuracy: 1.0, Computation time: 1.7268855571746826\n",
      "Step: 869, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 1.9552984237670898\n",
      "Step: 870, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 1.825226068496704\n",
      "Step: 871, Loss: 0.916248619556427, Accuracy: 1.0, Computation time: 2.2457568645477295\n",
      "Step: 872, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 1.8812181949615479\n",
      "Step: 873, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 2.204643726348877\n",
      "Step: 874, Loss: 0.9256173372268677, Accuracy: 1.0, Computation time: 2.0193748474121094\n",
      "Step: 875, Loss: 0.9198276996612549, Accuracy: 1.0, Computation time: 2.7828242778778076\n",
      "Step: 876, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 2.2040953636169434\n",
      "Step: 877, Loss: 0.9167947769165039, Accuracy: 1.0, Computation time: 2.0169637203216553\n",
      "Step: 878, Loss: 0.9379881620407104, Accuracy: 0.96875, Computation time: 2.8465187549591064\n",
      "Step: 879, Loss: 0.9159964323043823, Accuracy: 1.0, Computation time: 1.8984851837158203\n",
      "Step: 880, Loss: 0.9373616576194763, Accuracy: 0.96875, Computation time: 2.0606014728546143\n",
      "Step: 881, Loss: 0.9159740805625916, Accuracy: 1.0, Computation time: 1.9524881839752197\n",
      "Step: 882, Loss: 0.9200321435928345, Accuracy: 1.0, Computation time: 1.9810740947723389\n",
      "Step: 883, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 2.2874021530151367\n",
      "Step: 884, Loss: 0.922555148601532, Accuracy: 1.0, Computation time: 3.179567813873291\n",
      "Step: 885, Loss: 0.9159725904464722, Accuracy: 1.0, Computation time: 2.0114009380340576\n",
      "Step: 886, Loss: 0.9582567811012268, Accuracy: 0.9375, Computation time: 1.8416430950164795\n",
      "Step: 887, Loss: 0.916221022605896, Accuracy: 1.0, Computation time: 2.21907377243042\n",
      "Step: 888, Loss: 0.9159509539604187, Accuracy: 1.0, Computation time: 2.0252954959869385\n",
      "Step: 889, Loss: 0.9160197973251343, Accuracy: 1.0, Computation time: 1.8680388927459717\n",
      "Step: 890, Loss: 0.9160657525062561, Accuracy: 1.0, Computation time: 1.8958840370178223\n",
      "Step: 891, Loss: 0.9164382219314575, Accuracy: 1.0, Computation time: 2.1736373901367188\n",
      "Step: 892, Loss: 0.9160177111625671, Accuracy: 1.0, Computation time: 1.8662059307098389\n",
      "Step: 893, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.8822202682495117\n",
      "Step: 894, Loss: 0.9160820841789246, Accuracy: 1.0, Computation time: 1.9861488342285156\n",
      "Step: 895, Loss: 0.9159339666366577, Accuracy: 1.0, Computation time: 1.9782459735870361\n",
      "Step: 896, Loss: 0.9376600384712219, Accuracy: 0.96875, Computation time: 2.080838441848755\n",
      "Step: 897, Loss: 0.9355732202529907, Accuracy: 0.96875, Computation time: 2.302799701690674\n",
      "Step: 898, Loss: 0.9383817315101624, Accuracy: 0.96875, Computation time: 1.8852522373199463\n",
      "Step: 899, Loss: 0.9342556595802307, Accuracy: 0.96875, Computation time: 3.058884620666504\n",
      "Step: 900, Loss: 0.9162231087684631, Accuracy: 1.0, Computation time: 1.9020204544067383\n",
      "Step: 901, Loss: 0.9160171747207642, Accuracy: 1.0, Computation time: 1.821742057800293\n",
      "Step: 902, Loss: 0.9163170456886292, Accuracy: 1.0, Computation time: 2.120944023132324\n",
      "Step: 903, Loss: 0.9159883856773376, Accuracy: 1.0, Computation time: 1.9971013069152832\n",
      "Step: 904, Loss: 0.9198004007339478, Accuracy: 1.0, Computation time: 2.386247396469116\n",
      "Step: 905, Loss: 0.9201806783676147, Accuracy: 1.0, Computation time: 2.522792100906372\n",
      "Step: 906, Loss: 0.9159808158874512, Accuracy: 1.0, Computation time: 1.9888768196105957\n",
      "Step: 907, Loss: 0.9160726070404053, Accuracy: 1.0, Computation time: 2.2824344635009766\n",
      "Step: 908, Loss: 0.9213356971740723, Accuracy: 1.0, Computation time: 2.251420259475708\n",
      "Step: 909, Loss: 0.9160982370376587, Accuracy: 1.0, Computation time: 1.9902801513671875\n",
      "Step: 910, Loss: 0.9166396856307983, Accuracy: 1.0, Computation time: 2.2012696266174316\n",
      "Step: 911, Loss: 0.9160746335983276, Accuracy: 1.0, Computation time: 1.9706661701202393\n",
      "Step: 912, Loss: 0.9164144992828369, Accuracy: 1.0, Computation time: 2.1787593364715576\n",
      "Step: 913, Loss: 0.9162650108337402, Accuracy: 1.0, Computation time: 2.069736957550049\n",
      "Step: 914, Loss: 0.9289838671684265, Accuracy: 1.0, Computation time: 2.4602484703063965\n",
      "Step: 915, Loss: 0.9286115765571594, Accuracy: 0.96875, Computation time: 2.2756834030151367\n",
      "Step: 916, Loss: 0.9160446524620056, Accuracy: 1.0, Computation time: 2.2487831115722656\n",
      "Step: 917, Loss: 0.9163388013839722, Accuracy: 1.0, Computation time: 1.8776788711547852\n",
      "Step: 918, Loss: 0.918310821056366, Accuracy: 1.0, Computation time: 2.536473512649536\n",
      "Step: 919, Loss: 0.916276752948761, Accuracy: 1.0, Computation time: 1.875058889389038\n",
      "Step: 920, Loss: 0.9213020205497742, Accuracy: 1.0, Computation time: 4.274321794509888\n",
      "Step: 921, Loss: 0.9163640737533569, Accuracy: 1.0, Computation time: 2.0838077068328857\n",
      "Step: 922, Loss: 0.9374802708625793, Accuracy: 0.96875, Computation time: 1.9107451438903809\n",
      "Step: 923, Loss: 0.9201940298080444, Accuracy: 1.0, Computation time: 2.4362735748291016\n",
      "Step: 924, Loss: 0.9160462617874146, Accuracy: 1.0, Computation time: 2.109997510910034\n",
      "Step: 925, Loss: 0.9372368454933167, Accuracy: 0.96875, Computation time: 2.359865665435791\n",
      "Step: 926, Loss: 0.9366092681884766, Accuracy: 0.96875, Computation time: 2.2290663719177246\n",
      "Step: 927, Loss: 0.9425733089447021, Accuracy: 0.96875, Computation time: 2.198983669281006\n",
      "Step: 928, Loss: 0.9159934520721436, Accuracy: 1.0, Computation time: 2.148843288421631\n",
      "Step: 929, Loss: 0.9159839153289795, Accuracy: 1.0, Computation time: 2.150127649307251\n",
      "Step: 930, Loss: 0.9159694910049438, Accuracy: 1.0, Computation time: 1.8166120052337646\n",
      "Step: 931, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 2.1676814556121826\n",
      "Step: 932, Loss: 0.9159460067749023, Accuracy: 1.0, Computation time: 2.376657247543335\n",
      "Step: 933, Loss: 0.9159393906593323, Accuracy: 1.0, Computation time: 2.292262077331543\n",
      "Step: 934, Loss: 0.9159572720527649, Accuracy: 1.0, Computation time: 1.8827834129333496\n",
      "Step: 935, Loss: 0.9380565285682678, Accuracy: 0.96875, Computation time: 3.7251296043395996\n",
      "Step: 936, Loss: 0.9376914501190186, Accuracy: 0.96875, Computation time: 1.9534156322479248\n",
      "Step: 937, Loss: 0.9180905818939209, Accuracy: 1.0, Computation time: 1.9114172458648682\n",
      "Step: 938, Loss: 0.9168466925621033, Accuracy: 1.0, Computation time: 2.2713050842285156\n",
      "Step: 939, Loss: 0.9365840554237366, Accuracy: 0.96875, Computation time: 1.946713924407959\n",
      "Step: 940, Loss: 0.9279503226280212, Accuracy: 1.0, Computation time: 3.17504620552063\n",
      "Step: 941, Loss: 0.9160316586494446, Accuracy: 1.0, Computation time: 2.0880887508392334\n",
      "Step: 942, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 2.067169666290283\n",
      "Step: 943, Loss: 0.9160593152046204, Accuracy: 1.0, Computation time: 2.4791059494018555\n",
      "Step: 944, Loss: 0.9162887930870056, Accuracy: 1.0, Computation time: 1.9930541515350342\n",
      "Step: 945, Loss: 0.9161420464515686, Accuracy: 1.0, Computation time: 2.27951979637146\n",
      "Step: 946, Loss: 0.9230362176895142, Accuracy: 1.0, Computation time: 2.596062660217285\n",
      "Step: 947, Loss: 0.917667806148529, Accuracy: 1.0, Computation time: 3.82542085647583\n",
      "Step: 948, Loss: 0.9264426827430725, Accuracy: 0.96875, Computation time: 2.7740907669067383\n",
      "Step: 949, Loss: 0.9162022471427917, Accuracy: 1.0, Computation time: 1.8848724365234375\n",
      "Step: 950, Loss: 0.9234241247177124, Accuracy: 1.0, Computation time: 2.920849323272705\n",
      "Step: 951, Loss: 0.9160163402557373, Accuracy: 1.0, Computation time: 1.7078723907470703\n",
      "Step: 952, Loss: 0.9162654876708984, Accuracy: 1.0, Computation time: 1.7151055335998535\n",
      "Step: 953, Loss: 0.91620272397995, Accuracy: 1.0, Computation time: 2.5035948753356934\n",
      "Step: 954, Loss: 0.9435392022132874, Accuracy: 0.96875, Computation time: 2.3147523403167725\n",
      "Step: 955, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.716545581817627\n",
      "Step: 956, Loss: 0.9376440644264221, Accuracy: 0.96875, Computation time: 1.816659688949585\n",
      "Step: 957, Loss: 0.9191995859146118, Accuracy: 1.0, Computation time: 2.1964540481567383\n",
      "Step: 958, Loss: 0.9160402417182922, Accuracy: 1.0, Computation time: 2.2228801250457764\n",
      "Step: 959, Loss: 0.9159700870513916, Accuracy: 1.0, Computation time: 2.437917709350586\n",
      "Step: 960, Loss: 0.9244934916496277, Accuracy: 1.0, Computation time: 2.089353084564209\n",
      "Step: 961, Loss: 0.9314352869987488, Accuracy: 0.96875, Computation time: 2.6121280193328857\n",
      "Step: 962, Loss: 0.9376580119132996, Accuracy: 0.96875, Computation time: 2.0543198585510254\n",
      "Step: 963, Loss: 0.916102409362793, Accuracy: 1.0, Computation time: 2.009503126144409\n",
      "Step: 964, Loss: 0.9161725640296936, Accuracy: 1.0, Computation time: 2.012997627258301\n",
      "Step: 965, Loss: 0.9163641333580017, Accuracy: 1.0, Computation time: 2.509342670440674\n",
      "Step: 966, Loss: 0.9173285961151123, Accuracy: 1.0, Computation time: 2.898998975753784\n",
      "Step: 967, Loss: 0.9167336225509644, Accuracy: 1.0, Computation time: 2.1085662841796875\n",
      "Step: 968, Loss: 0.9494032859802246, Accuracy: 0.9375, Computation time: 4.9288434982299805\n",
      "Step: 969, Loss: 0.9162160754203796, Accuracy: 1.0, Computation time: 1.9114105701446533\n",
      "Step: 970, Loss: 0.9376339912414551, Accuracy: 0.96875, Computation time: 1.8456447124481201\n",
      "Step: 971, Loss: 0.9380605816841125, Accuracy: 0.96875, Computation time: 1.9872472286224365\n",
      "Step: 972, Loss: 0.9162294864654541, Accuracy: 1.0, Computation time: 1.6167583465576172\n",
      "Step: 973, Loss: 0.9171176552772522, Accuracy: 1.0, Computation time: 2.203415632247925\n",
      "########################\n",
      "Test loss: 1.1417484283447266, Test Accuracy_epoch7: 0.6720386743545532\n",
      "########################\n",
      "Step: 974, Loss: 0.9174045920372009, Accuracy: 1.0, Computation time: 2.336120367050171\n",
      "Step: 975, Loss: 0.9166181683540344, Accuracy: 1.0, Computation time: 1.7862462997436523\n",
      "Step: 976, Loss: 0.9204519987106323, Accuracy: 1.0, Computation time: 2.1400089263916016\n",
      "Step: 977, Loss: 0.9163844585418701, Accuracy: 1.0, Computation time: 1.8546693325042725\n",
      "Step: 978, Loss: 0.9770764708518982, Accuracy: 0.90625, Computation time: 2.18782639503479\n",
      "Step: 979, Loss: 0.9381612539291382, Accuracy: 0.96875, Computation time: 3.1435489654541016\n",
      "Step: 980, Loss: 0.9163669347763062, Accuracy: 1.0, Computation time: 1.312753677368164\n",
      "Step: 981, Loss: 0.9162347316741943, Accuracy: 1.0, Computation time: 2.6218810081481934\n",
      "Step: 982, Loss: 0.9366017580032349, Accuracy: 0.96875, Computation time: 3.5832324028015137\n",
      "Step: 983, Loss: 0.9164904356002808, Accuracy: 1.0, Computation time: 1.9557480812072754\n",
      "Step: 984, Loss: 0.9378871917724609, Accuracy: 0.96875, Computation time: 2.8685526847839355\n",
      "Step: 985, Loss: 0.916851818561554, Accuracy: 1.0, Computation time: 2.0418474674224854\n",
      "Step: 986, Loss: 0.9170759320259094, Accuracy: 1.0, Computation time: 2.0390443801879883\n",
      "Step: 987, Loss: 0.9163838028907776, Accuracy: 1.0, Computation time: 1.6534297466278076\n",
      "Step: 988, Loss: 0.9599339962005615, Accuracy: 0.90625, Computation time: 2.8295743465423584\n",
      "Step: 989, Loss: 0.9620491862297058, Accuracy: 0.9375, Computation time: 1.8417763710021973\n",
      "Step: 990, Loss: 0.9165092706680298, Accuracy: 1.0, Computation time: 1.741943359375\n",
      "Step: 991, Loss: 0.9191737174987793, Accuracy: 1.0, Computation time: 2.450122356414795\n",
      "Step: 992, Loss: 0.9165760278701782, Accuracy: 1.0, Computation time: 1.4570434093475342\n",
      "Step: 993, Loss: 0.9166640043258667, Accuracy: 1.0, Computation time: 1.803168535232544\n",
      "Step: 994, Loss: 0.9198639392852783, Accuracy: 1.0, Computation time: 2.750032663345337\n",
      "Step: 995, Loss: 0.9165086150169373, Accuracy: 1.0, Computation time: 1.4352478981018066\n",
      "Step: 996, Loss: 0.9237443208694458, Accuracy: 1.0, Computation time: 2.807438373565674\n",
      "Step: 997, Loss: 0.9161227941513062, Accuracy: 1.0, Computation time: 2.777503728866577\n",
      "Step: 998, Loss: 0.934399425983429, Accuracy: 0.96875, Computation time: 2.1110241413116455\n",
      "Step: 999, Loss: 0.9165907502174377, Accuracy: 1.0, Computation time: 1.8823778629302979\n",
      "Step: 1000, Loss: 0.9163755774497986, Accuracy: 1.0, Computation time: 2.7160706520080566\n",
      "Step: 1001, Loss: 0.9160915613174438, Accuracy: 1.0, Computation time: 2.012462854385376\n",
      "Step: 1002, Loss: 0.9376420974731445, Accuracy: 0.96875, Computation time: 2.316243886947632\n",
      "Step: 1003, Loss: 0.9162273406982422, Accuracy: 1.0, Computation time: 2.4227092266082764\n",
      "Step: 1004, Loss: 0.916609525680542, Accuracy: 1.0, Computation time: 2.2391302585601807\n",
      "Step: 1005, Loss: 0.9160779714584351, Accuracy: 1.0, Computation time: 2.2043683528900146\n",
      "Step: 1006, Loss: 0.9162839651107788, Accuracy: 1.0, Computation time: 1.8958415985107422\n",
      "Step: 1007, Loss: 0.9368957281112671, Accuracy: 0.96875, Computation time: 2.34462833404541\n",
      "Step: 1008, Loss: 0.9177746772766113, Accuracy: 1.0, Computation time: 1.8786985874176025\n",
      "Step: 1009, Loss: 0.916392982006073, Accuracy: 1.0, Computation time: 1.8486993312835693\n",
      "Step: 1010, Loss: 0.9192087650299072, Accuracy: 1.0, Computation time: 2.4874653816223145\n",
      "Step: 1011, Loss: 0.9163254499435425, Accuracy: 1.0, Computation time: 1.8124291896820068\n",
      "Step: 1012, Loss: 0.9160453677177429, Accuracy: 1.0, Computation time: 2.3005857467651367\n",
      "Step: 1013, Loss: 0.916010320186615, Accuracy: 1.0, Computation time: 1.9312283992767334\n",
      "Step: 1014, Loss: 0.9161697626113892, Accuracy: 1.0, Computation time: 2.3108787536621094\n",
      "Step: 1015, Loss: 0.9166402220726013, Accuracy: 1.0, Computation time: 1.5525896549224854\n",
      "Step: 1016, Loss: 0.935380220413208, Accuracy: 0.96875, Computation time: 2.2003328800201416\n",
      "Step: 1017, Loss: 0.916374683380127, Accuracy: 1.0, Computation time: 1.9801170825958252\n",
      "Step: 1018, Loss: 0.9163652658462524, Accuracy: 1.0, Computation time: 1.877709150314331\n",
      "Step: 1019, Loss: 0.9170151352882385, Accuracy: 1.0, Computation time: 2.2777209281921387\n",
      "Step: 1020, Loss: 0.9164989590644836, Accuracy: 1.0, Computation time: 1.8352141380310059\n",
      "Step: 1021, Loss: 0.9164026379585266, Accuracy: 1.0, Computation time: 1.70731782913208\n",
      "Step: 1022, Loss: 0.9215905070304871, Accuracy: 1.0, Computation time: 2.698592185974121\n",
      "Step: 1023, Loss: 0.9172977805137634, Accuracy: 1.0, Computation time: 1.9129769802093506\n",
      "Step: 1024, Loss: 0.9376170039176941, Accuracy: 0.96875, Computation time: 2.0387930870056152\n",
      "Step: 1025, Loss: 0.9365489482879639, Accuracy: 0.96875, Computation time: 2.222510814666748\n",
      "Step: 1026, Loss: 0.9160323143005371, Accuracy: 1.0, Computation time: 1.783735990524292\n",
      "Step: 1027, Loss: 0.9163797497749329, Accuracy: 1.0, Computation time: 2.0319273471832275\n",
      "Step: 1028, Loss: 0.9160139560699463, Accuracy: 1.0, Computation time: 1.5036900043487549\n",
      "Step: 1029, Loss: 0.9160107374191284, Accuracy: 1.0, Computation time: 2.1013951301574707\n",
      "Step: 1030, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 1.5879499912261963\n",
      "Step: 1031, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.8980236053466797\n",
      "Step: 1032, Loss: 0.916022539138794, Accuracy: 1.0, Computation time: 2.0843966007232666\n",
      "Step: 1033, Loss: 0.9373843669891357, Accuracy: 0.96875, Computation time: 1.6429595947265625\n",
      "Step: 1034, Loss: 0.9161307215690613, Accuracy: 1.0, Computation time: 1.768566370010376\n",
      "Step: 1035, Loss: 0.93558269739151, Accuracy: 0.96875, Computation time: 2.1952199935913086\n",
      "Step: 1036, Loss: 0.916032075881958, Accuracy: 1.0, Computation time: 2.2337491512298584\n",
      "Step: 1037, Loss: 0.9591943025588989, Accuracy: 0.9375, Computation time: 1.9698050022125244\n",
      "Step: 1038, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.7309517860412598\n",
      "Step: 1039, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 1.864640235900879\n",
      "Step: 1040, Loss: 0.9375328421592712, Accuracy: 0.96875, Computation time: 1.7233173847198486\n",
      "Step: 1041, Loss: 0.9161592721939087, Accuracy: 1.0, Computation time: 1.7385258674621582\n",
      "Step: 1042, Loss: 0.9160692691802979, Accuracy: 1.0, Computation time: 2.341026544570923\n",
      "Step: 1043, Loss: 0.9380428194999695, Accuracy: 0.96875, Computation time: 3.0469558238983154\n",
      "Step: 1044, Loss: 0.9373366236686707, Accuracy: 0.96875, Computation time: 2.4895009994506836\n",
      "Step: 1045, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.7303833961486816\n",
      "Step: 1046, Loss: 0.9375906586647034, Accuracy: 0.96875, Computation time: 1.736480712890625\n",
      "Step: 1047, Loss: 0.9159851670265198, Accuracy: 1.0, Computation time: 1.8730337619781494\n",
      "Step: 1048, Loss: 0.9159586429595947, Accuracy: 1.0, Computation time: 1.8606414794921875\n",
      "Step: 1049, Loss: 0.9159717559814453, Accuracy: 1.0, Computation time: 1.874588966369629\n",
      "Step: 1050, Loss: 0.91606605052948, Accuracy: 1.0, Computation time: 2.0133848190307617\n",
      "Step: 1051, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 2.112740993499756\n",
      "Step: 1052, Loss: 0.9337528944015503, Accuracy: 0.96875, Computation time: 2.024512767791748\n",
      "Step: 1053, Loss: 0.9159716367721558, Accuracy: 1.0, Computation time: 1.7428674697875977\n",
      "Step: 1054, Loss: 0.9375170469284058, Accuracy: 0.96875, Computation time: 1.6966891288757324\n",
      "Step: 1055, Loss: 0.9161381721496582, Accuracy: 1.0, Computation time: 2.007784366607666\n",
      "Step: 1056, Loss: 0.9275626540184021, Accuracy: 0.96875, Computation time: 2.558481216430664\n",
      "Step: 1057, Loss: 0.9163438677787781, Accuracy: 1.0, Computation time: 2.119239330291748\n",
      "Step: 1058, Loss: 0.916421115398407, Accuracy: 1.0, Computation time: 1.9982638359069824\n",
      "Step: 1059, Loss: 0.9372068643569946, Accuracy: 0.96875, Computation time: 1.7162139415740967\n",
      "Step: 1060, Loss: 0.9163233041763306, Accuracy: 1.0, Computation time: 2.0423407554626465\n",
      "Step: 1061, Loss: 0.9160637259483337, Accuracy: 1.0, Computation time: 1.6350131034851074\n",
      "Step: 1062, Loss: 0.9235248565673828, Accuracy: 1.0, Computation time: 1.651547908782959\n",
      "Step: 1063, Loss: 0.915986955165863, Accuracy: 1.0, Computation time: 1.8433539867401123\n",
      "Step: 1064, Loss: 0.9160601496696472, Accuracy: 1.0, Computation time: 2.021472454071045\n",
      "Step: 1065, Loss: 0.9170353412628174, Accuracy: 1.0, Computation time: 1.841935396194458\n",
      "Step: 1066, Loss: 0.9380471110343933, Accuracy: 0.96875, Computation time: 1.9935669898986816\n",
      "Step: 1067, Loss: 0.9166021347045898, Accuracy: 1.0, Computation time: 2.605764389038086\n",
      "Step: 1068, Loss: 0.9160947799682617, Accuracy: 1.0, Computation time: 1.797727346420288\n",
      "Step: 1069, Loss: 0.915972888469696, Accuracy: 1.0, Computation time: 1.8210113048553467\n",
      "Step: 1070, Loss: 0.9162417650222778, Accuracy: 1.0, Computation time: 1.8120684623718262\n",
      "Step: 1071, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 2.202270030975342\n",
      "Step: 1072, Loss: 0.9161033630371094, Accuracy: 1.0, Computation time: 1.7028968334197998\n",
      "Step: 1073, Loss: 0.9160839915275574, Accuracy: 1.0, Computation time: 2.1047608852386475\n",
      "Step: 1074, Loss: 0.9172249436378479, Accuracy: 1.0, Computation time: 1.6993076801300049\n",
      "Step: 1075, Loss: 0.916496753692627, Accuracy: 1.0, Computation time: 2.326495885848999\n",
      "Step: 1076, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 1.4457757472991943\n",
      "Step: 1077, Loss: 0.9164031744003296, Accuracy: 1.0, Computation time: 2.2459495067596436\n",
      "Step: 1078, Loss: 0.9159678816795349, Accuracy: 1.0, Computation time: 1.619797706604004\n",
      "Step: 1079, Loss: 0.9159396290779114, Accuracy: 1.0, Computation time: 2.1715266704559326\n",
      "Step: 1080, Loss: 0.916115939617157, Accuracy: 1.0, Computation time: 1.7713782787322998\n",
      "Step: 1081, Loss: 0.9159791469573975, Accuracy: 1.0, Computation time: 2.1653809547424316\n",
      "Step: 1082, Loss: 0.917975902557373, Accuracy: 1.0, Computation time: 1.7298088073730469\n",
      "Step: 1083, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 1.8590972423553467\n",
      "Step: 1084, Loss: 0.9159483909606934, Accuracy: 1.0, Computation time: 2.6175947189331055\n",
      "Step: 1085, Loss: 0.9162764549255371, Accuracy: 1.0, Computation time: 2.2199816703796387\n",
      "Step: 1086, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.8514020442962646\n",
      "Step: 1087, Loss: 0.9159333109855652, Accuracy: 1.0, Computation time: 1.6627254486083984\n",
      "Step: 1088, Loss: 0.9187062978744507, Accuracy: 1.0, Computation time: 1.9688446521759033\n",
      "Step: 1089, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.5435481071472168\n",
      "Step: 1090, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.8462982177734375\n",
      "Step: 1091, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.8677079677581787\n",
      "Step: 1092, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 2.006380081176758\n",
      "Step: 1093, Loss: 0.931980550289154, Accuracy: 0.96875, Computation time: 2.1581051349639893\n",
      "Step: 1094, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 2.2085347175598145\n",
      "Step: 1095, Loss: 0.9160178899765015, Accuracy: 1.0, Computation time: 2.2686753273010254\n",
      "Step: 1096, Loss: 0.91592937707901, Accuracy: 1.0, Computation time: 2.0303168296813965\n",
      "Step: 1097, Loss: 0.916340172290802, Accuracy: 1.0, Computation time: 1.9350383281707764\n",
      "Step: 1098, Loss: 0.9159990549087524, Accuracy: 1.0, Computation time: 2.19527006149292\n",
      "Step: 1099, Loss: 0.9159420132637024, Accuracy: 1.0, Computation time: 1.762869119644165\n",
      "Step: 1100, Loss: 0.9159961938858032, Accuracy: 1.0, Computation time: 1.8109815120697021\n",
      "Step: 1101, Loss: 0.9159744381904602, Accuracy: 1.0, Computation time: 1.9858403205871582\n",
      "Step: 1102, Loss: 0.9160647988319397, Accuracy: 1.0, Computation time: 2.5292279720306396\n",
      "Step: 1103, Loss: 0.9169659614562988, Accuracy: 1.0, Computation time: 2.4072649478912354\n",
      "Step: 1104, Loss: 0.9247004985809326, Accuracy: 1.0, Computation time: 2.2782108783721924\n",
      "Step: 1105, Loss: 0.9218694567680359, Accuracy: 1.0, Computation time: 2.3603925704956055\n",
      "Step: 1106, Loss: 0.9387159943580627, Accuracy: 0.96875, Computation time: 2.0523481369018555\n",
      "Step: 1107, Loss: 0.9159911870956421, Accuracy: 1.0, Computation time: 1.8536431789398193\n",
      "Step: 1108, Loss: 0.9161527156829834, Accuracy: 1.0, Computation time: 2.0490407943725586\n",
      "Step: 1109, Loss: 0.9350736737251282, Accuracy: 0.96875, Computation time: 2.415982484817505\n",
      "Step: 1110, Loss: 0.9161646962165833, Accuracy: 1.0, Computation time: 2.28225040435791\n",
      "Step: 1111, Loss: 0.9160081148147583, Accuracy: 1.0, Computation time: 1.9654204845428467\n",
      "Step: 1112, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.7551355361938477\n",
      "########################\n",
      "Test loss: 1.1271172761917114, Test Accuracy_epoch8: 0.6921837329864502\n",
      "########################\n",
      "Step: 1113, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 1.8990836143493652\n",
      "Step: 1114, Loss: 0.9376156330108643, Accuracy: 0.96875, Computation time: 1.8163807392120361\n",
      "Step: 1115, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 1.8682527542114258\n",
      "Step: 1116, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 1.9180779457092285\n",
      "Step: 1117, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.91801118850708\n",
      "Step: 1118, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.7863576412200928\n",
      "Step: 1119, Loss: 0.9374898076057434, Accuracy: 0.96875, Computation time: 1.8378279209136963\n",
      "Step: 1120, Loss: 0.9374076128005981, Accuracy: 0.96875, Computation time: 1.6777026653289795\n",
      "Step: 1121, Loss: 0.9375576972961426, Accuracy: 0.96875, Computation time: 2.1416633129119873\n",
      "Step: 1122, Loss: 0.9173152446746826, Accuracy: 1.0, Computation time: 1.966975212097168\n",
      "Step: 1123, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 2.342780351638794\n",
      "Step: 1124, Loss: 0.9374726414680481, Accuracy: 0.96875, Computation time: 1.9926490783691406\n",
      "Step: 1125, Loss: 0.9376988410949707, Accuracy: 0.96875, Computation time: 2.157536745071411\n",
      "Step: 1126, Loss: 0.9159696102142334, Accuracy: 1.0, Computation time: 1.7351360321044922\n",
      "Step: 1127, Loss: 0.9257655143737793, Accuracy: 0.96875, Computation time: 2.3108222484588623\n",
      "Step: 1128, Loss: 0.9374281764030457, Accuracy: 0.96875, Computation time: 1.6663463115692139\n",
      "Step: 1129, Loss: 0.9408737421035767, Accuracy: 0.96875, Computation time: 2.5089282989501953\n",
      "Step: 1130, Loss: 0.9164494276046753, Accuracy: 1.0, Computation time: 2.1851091384887695\n",
      "Step: 1131, Loss: 0.9161859750747681, Accuracy: 1.0, Computation time: 2.110365390777588\n",
      "Step: 1132, Loss: 0.9176367521286011, Accuracy: 1.0, Computation time: 2.7073442935943604\n",
      "Step: 1133, Loss: 0.9160218834877014, Accuracy: 1.0, Computation time: 1.8798131942749023\n",
      "Step: 1134, Loss: 0.9160447716712952, Accuracy: 1.0, Computation time: 1.7515182495117188\n",
      "Step: 1135, Loss: 0.9160391092300415, Accuracy: 1.0, Computation time: 2.0167996883392334\n",
      "Step: 1136, Loss: 0.937601625919342, Accuracy: 0.96875, Computation time: 1.7480032444000244\n",
      "Step: 1137, Loss: 0.9159212708473206, Accuracy: 1.0, Computation time: 2.088621139526367\n",
      "Step: 1138, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 2.1455137729644775\n",
      "Step: 1139, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.9849319458007812\n",
      "Step: 1140, Loss: 0.9574124813079834, Accuracy: 0.9375, Computation time: 2.16161847114563\n",
      "Step: 1141, Loss: 0.9160801768302917, Accuracy: 1.0, Computation time: 2.066570520401001\n",
      "Step: 1142, Loss: 0.9160454869270325, Accuracy: 1.0, Computation time: 1.9802072048187256\n",
      "Step: 1143, Loss: 0.9160613417625427, Accuracy: 1.0, Computation time: 1.9386906623840332\n",
      "Step: 1144, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 2.059587240219116\n",
      "Step: 1145, Loss: 0.9161304235458374, Accuracy: 1.0, Computation time: 2.0307271480560303\n",
      "Step: 1146, Loss: 0.9375391006469727, Accuracy: 0.96875, Computation time: 3.207773447036743\n",
      "Step: 1147, Loss: 0.9201306104660034, Accuracy: 1.0, Computation time: 2.392150402069092\n",
      "Step: 1148, Loss: 0.9159765243530273, Accuracy: 1.0, Computation time: 2.1188297271728516\n",
      "Step: 1149, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.8737478256225586\n",
      "Step: 1150, Loss: 0.9377705454826355, Accuracy: 0.96875, Computation time: 2.220247745513916\n",
      "Step: 1151, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 2.75454044342041\n",
      "Step: 1152, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 1.7893455028533936\n",
      "Step: 1153, Loss: 0.9160234332084656, Accuracy: 1.0, Computation time: 1.930833339691162\n",
      "Step: 1154, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 1.8397319316864014\n",
      "Step: 1155, Loss: 0.9336875677108765, Accuracy: 0.96875, Computation time: 2.2010691165924072\n",
      "Step: 1156, Loss: 0.9360492825508118, Accuracy: 0.96875, Computation time: 1.8540410995483398\n",
      "Step: 1157, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.7942712306976318\n",
      "Step: 1158, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.9294157028198242\n",
      "Step: 1159, Loss: 0.9159348607063293, Accuracy: 1.0, Computation time: 1.9577891826629639\n",
      "Step: 1160, Loss: 0.936246395111084, Accuracy: 0.96875, Computation time: 1.9259827136993408\n",
      "Step: 1161, Loss: 0.9159369468688965, Accuracy: 1.0, Computation time: 1.9042456150054932\n",
      "Step: 1162, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.534123420715332\n",
      "Step: 1163, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.6872482299804688\n",
      "Step: 1164, Loss: 0.9356990456581116, Accuracy: 0.96875, Computation time: 2.2247982025146484\n",
      "Step: 1165, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 1.8786721229553223\n",
      "Step: 1166, Loss: 0.936873197555542, Accuracy: 0.96875, Computation time: 2.1420369148254395\n",
      "Step: 1167, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.7341954708099365\n",
      "Step: 1168, Loss: 0.940281331539154, Accuracy: 0.96875, Computation time: 2.1469295024871826\n",
      "Step: 1169, Loss: 0.9218956828117371, Accuracy: 1.0, Computation time: 2.6245546340942383\n",
      "Step: 1170, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 2.0155320167541504\n",
      "Step: 1171, Loss: 0.917643666267395, Accuracy: 1.0, Computation time: 1.7508127689361572\n",
      "Step: 1172, Loss: 0.958876371383667, Accuracy: 0.9375, Computation time: 1.7357523441314697\n",
      "Step: 1173, Loss: 0.9159224629402161, Accuracy: 1.0, Computation time: 2.099902391433716\n",
      "Step: 1174, Loss: 0.9463098645210266, Accuracy: 0.96875, Computation time: 3.497243642807007\n",
      "Step: 1175, Loss: 0.9344443678855896, Accuracy: 0.96875, Computation time: 2.049586296081543\n",
      "Step: 1176, Loss: 0.9517959356307983, Accuracy: 0.9375, Computation time: 2.2239859104156494\n",
      "Step: 1177, Loss: 0.9161255955696106, Accuracy: 1.0, Computation time: 2.0557126998901367\n",
      "Step: 1178, Loss: 0.9159919619560242, Accuracy: 1.0, Computation time: 2.1183652877807617\n",
      "Step: 1179, Loss: 0.9160643815994263, Accuracy: 1.0, Computation time: 1.8655478954315186\n",
      "Step: 1180, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 1.7702805995941162\n",
      "Step: 1181, Loss: 0.9367601275444031, Accuracy: 0.96875, Computation time: 2.185210943222046\n",
      "Step: 1182, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 2.1599109172821045\n",
      "Step: 1183, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 2.135117292404175\n",
      "Step: 1184, Loss: 0.9187129735946655, Accuracy: 1.0, Computation time: 2.2431395053863525\n",
      "Step: 1185, Loss: 0.922520101070404, Accuracy: 1.0, Computation time: 1.8165531158447266\n",
      "Step: 1186, Loss: 0.9192993640899658, Accuracy: 1.0, Computation time: 2.5727057456970215\n",
      "Step: 1187, Loss: 0.940861701965332, Accuracy: 0.96875, Computation time: 3.1663525104522705\n",
      "Step: 1188, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 1.620429277420044\n",
      "Step: 1189, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 1.7463569641113281\n",
      "Step: 1190, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 1.5050926208496094\n",
      "Step: 1191, Loss: 0.9159897565841675, Accuracy: 1.0, Computation time: 1.6760940551757812\n",
      "Step: 1192, Loss: 0.9160470962524414, Accuracy: 1.0, Computation time: 1.975865125656128\n",
      "Step: 1193, Loss: 0.9159348011016846, Accuracy: 1.0, Computation time: 2.098889112472534\n",
      "Step: 1194, Loss: 0.9160066843032837, Accuracy: 1.0, Computation time: 2.5574710369110107\n",
      "Step: 1195, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.787550687789917\n",
      "Step: 1196, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.7809700965881348\n",
      "Step: 1197, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 2.162818193435669\n",
      "Step: 1198, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 1.9532415866851807\n",
      "Step: 1199, Loss: 0.9159119129180908, Accuracy: 1.0, Computation time: 1.6981701850891113\n",
      "Step: 1200, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 2.057438611984253\n",
      "Step: 1201, Loss: 0.9159863591194153, Accuracy: 1.0, Computation time: 2.0869991779327393\n",
      "Step: 1202, Loss: 0.9174132347106934, Accuracy: 1.0, Computation time: 2.0912437438964844\n",
      "Step: 1203, Loss: 0.9543440937995911, Accuracy: 0.9375, Computation time: 2.911287307739258\n",
      "Step: 1204, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 2.496859073638916\n",
      "Step: 1205, Loss: 0.9267919659614563, Accuracy: 0.96875, Computation time: 2.090583562850952\n",
      "Step: 1206, Loss: 0.9160277247428894, Accuracy: 1.0, Computation time: 2.0354318618774414\n",
      "Step: 1207, Loss: 0.9160113334655762, Accuracy: 1.0, Computation time: 1.973449945449829\n",
      "Step: 1208, Loss: 0.9377923607826233, Accuracy: 0.96875, Computation time: 1.7620184421539307\n",
      "Step: 1209, Loss: 0.9167079925537109, Accuracy: 1.0, Computation time: 2.256340980529785\n",
      "Step: 1210, Loss: 0.9374225735664368, Accuracy: 0.96875, Computation time: 1.9805803298950195\n",
      "Step: 1211, Loss: 0.9160317182540894, Accuracy: 1.0, Computation time: 1.6435072422027588\n",
      "Step: 1212, Loss: 0.9374439716339111, Accuracy: 0.96875, Computation time: 2.225663423538208\n",
      "Step: 1213, Loss: 0.9159925580024719, Accuracy: 1.0, Computation time: 1.6883158683776855\n",
      "Step: 1214, Loss: 0.9172911643981934, Accuracy: 1.0, Computation time: 2.776329755783081\n",
      "Step: 1215, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.5820391178131104\n",
      "Step: 1216, Loss: 0.9165525436401367, Accuracy: 1.0, Computation time: 2.2889764308929443\n",
      "Step: 1217, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 1.9050326347351074\n",
      "Step: 1218, Loss: 0.935552716255188, Accuracy: 0.96875, Computation time: 2.172198534011841\n",
      "Step: 1219, Loss: 0.9160975813865662, Accuracy: 1.0, Computation time: 2.1682770252227783\n",
      "Step: 1220, Loss: 0.9159929156303406, Accuracy: 1.0, Computation time: 2.518963098526001\n",
      "Step: 1221, Loss: 0.9234063029289246, Accuracy: 1.0, Computation time: 1.869004726409912\n",
      "Step: 1222, Loss: 0.9167318344116211, Accuracy: 1.0, Computation time: 3.154574394226074\n",
      "Step: 1223, Loss: 0.917262077331543, Accuracy: 1.0, Computation time: 1.9359896183013916\n",
      "Step: 1224, Loss: 0.9319702386856079, Accuracy: 0.96875, Computation time: 2.069284200668335\n",
      "Step: 1225, Loss: 0.9590173959732056, Accuracy: 0.9375, Computation time: 2.516862154006958\n",
      "Step: 1226, Loss: 0.9159907698631287, Accuracy: 1.0, Computation time: 2.7665960788726807\n",
      "Step: 1227, Loss: 0.9294602274894714, Accuracy: 0.96875, Computation time: 2.577760696411133\n",
      "Step: 1228, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 2.027019739151001\n",
      "Step: 1229, Loss: 0.9159241914749146, Accuracy: 1.0, Computation time: 1.7619047164916992\n",
      "Step: 1230, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 1.84201979637146\n",
      "Step: 1231, Loss: 0.9203112721443176, Accuracy: 1.0, Computation time: 2.057645797729492\n",
      "Step: 1232, Loss: 0.9370705485343933, Accuracy: 0.96875, Computation time: 1.7237017154693604\n",
      "Step: 1233, Loss: 0.9159746766090393, Accuracy: 1.0, Computation time: 1.9939889907836914\n",
      "Step: 1234, Loss: 0.9712334871292114, Accuracy: 0.90625, Computation time: 2.3432462215423584\n",
      "Step: 1235, Loss: 0.9250710010528564, Accuracy: 1.0, Computation time: 2.3317041397094727\n",
      "Step: 1236, Loss: 0.9298087954521179, Accuracy: 0.96875, Computation time: 2.7592639923095703\n",
      "Step: 1237, Loss: 0.9347284436225891, Accuracy: 0.96875, Computation time: 2.020616054534912\n",
      "Step: 1238, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 1.8018438816070557\n",
      "Step: 1239, Loss: 0.9375837445259094, Accuracy: 0.96875, Computation time: 1.6808371543884277\n",
      "Step: 1240, Loss: 0.9374297857284546, Accuracy: 0.96875, Computation time: 2.5528440475463867\n",
      "Step: 1241, Loss: 0.9277384877204895, Accuracy: 0.96875, Computation time: 1.9222776889801025\n",
      "Step: 1242, Loss: 0.9160570502281189, Accuracy: 1.0, Computation time: 2.0513651371002197\n",
      "Step: 1243, Loss: 0.9354016780853271, Accuracy: 0.96875, Computation time: 1.7964715957641602\n",
      "Step: 1244, Loss: 0.9191381931304932, Accuracy: 1.0, Computation time: 2.2371914386749268\n",
      "Step: 1245, Loss: 0.9429425001144409, Accuracy: 0.96875, Computation time: 1.8747141361236572\n",
      "Step: 1246, Loss: 0.9161515831947327, Accuracy: 1.0, Computation time: 1.9914569854736328\n",
      "Step: 1247, Loss: 0.9161169528961182, Accuracy: 1.0, Computation time: 1.788529396057129\n",
      "Step: 1248, Loss: 0.915996253490448, Accuracy: 1.0, Computation time: 1.9458160400390625\n",
      "Step: 1249, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 2.2051892280578613\n",
      "Step: 1250, Loss: 0.91600501537323, Accuracy: 1.0, Computation time: 2.0902068614959717\n",
      "Step: 1251, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 1.9336097240447998\n",
      "########################\n",
      "Test loss: 1.1265015602111816, Test Accuracy_epoch9: 0.6905721426010132\n",
      "########################\n",
      "Step: 1252, Loss: 0.9161036014556885, Accuracy: 1.0, Computation time: 1.9894838333129883\n",
      "Step: 1253, Loss: 0.9198521971702576, Accuracy: 1.0, Computation time: 2.1357662677764893\n",
      "Step: 1254, Loss: 0.9360395669937134, Accuracy: 0.96875, Computation time: 1.8991284370422363\n",
      "Step: 1255, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.7429876327514648\n",
      "Step: 1256, Loss: 0.9160369634628296, Accuracy: 1.0, Computation time: 2.1875216960906982\n",
      "Step: 1257, Loss: 0.929025411605835, Accuracy: 0.96875, Computation time: 1.982534408569336\n",
      "Step: 1258, Loss: 0.915956974029541, Accuracy: 1.0, Computation time: 2.117995023727417\n",
      "Step: 1259, Loss: 0.9160231351852417, Accuracy: 1.0, Computation time: 1.5222735404968262\n",
      "Step: 1260, Loss: 0.9161630868911743, Accuracy: 1.0, Computation time: 1.8680651187896729\n",
      "Step: 1261, Loss: 0.9160236716270447, Accuracy: 1.0, Computation time: 1.6449048519134521\n",
      "Step: 1262, Loss: 0.9160274267196655, Accuracy: 1.0, Computation time: 1.751420021057129\n",
      "Step: 1263, Loss: 0.9578217267990112, Accuracy: 0.9375, Computation time: 1.8274669647216797\n",
      "Step: 1264, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 1.9068810939788818\n",
      "Step: 1265, Loss: 0.9159566164016724, Accuracy: 1.0, Computation time: 1.6718170642852783\n",
      "Step: 1266, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 1.9921941757202148\n",
      "Step: 1267, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.4250450134277344\n",
      "Step: 1268, Loss: 0.9175988435745239, Accuracy: 1.0, Computation time: 1.9962773323059082\n",
      "Step: 1269, Loss: 0.9591484665870667, Accuracy: 0.9375, Computation time: 2.171816110610962\n",
      "Step: 1270, Loss: 0.9161357283592224, Accuracy: 1.0, Computation time: 1.7067770957946777\n",
      "Step: 1271, Loss: 0.9163316488265991, Accuracy: 1.0, Computation time: 2.363292932510376\n",
      "Step: 1272, Loss: 0.9288977384567261, Accuracy: 0.96875, Computation time: 2.644456148147583\n",
      "Step: 1273, Loss: 0.9176579117774963, Accuracy: 1.0, Computation time: 1.6849956512451172\n",
      "Step: 1274, Loss: 0.9375462532043457, Accuracy: 0.96875, Computation time: 1.9718501567840576\n",
      "Step: 1275, Loss: 0.9171526432037354, Accuracy: 1.0, Computation time: 1.956294059753418\n",
      "Step: 1276, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 1.7690374851226807\n",
      "Step: 1277, Loss: 0.9159823060035706, Accuracy: 1.0, Computation time: 1.8515846729278564\n",
      "Step: 1278, Loss: 0.9361788630485535, Accuracy: 0.96875, Computation time: 1.6729381084442139\n",
      "Step: 1279, Loss: 0.9160135984420776, Accuracy: 1.0, Computation time: 2.283168315887451\n",
      "Step: 1280, Loss: 0.9159709215164185, Accuracy: 1.0, Computation time: 2.0072476863861084\n",
      "Step: 1281, Loss: 0.9189764261245728, Accuracy: 1.0, Computation time: 1.8101208209991455\n",
      "Step: 1282, Loss: 0.9368776082992554, Accuracy: 0.96875, Computation time: 2.2416818141937256\n",
      "Step: 1283, Loss: 0.932439923286438, Accuracy: 0.96875, Computation time: 2.007378101348877\n",
      "Step: 1284, Loss: 0.9371880292892456, Accuracy: 0.96875, Computation time: 2.115798234939575\n",
      "Step: 1285, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 1.8705267906188965\n",
      "Step: 1286, Loss: 0.915976345539093, Accuracy: 1.0, Computation time: 2.437619924545288\n",
      "Step: 1287, Loss: 0.9160564541816711, Accuracy: 1.0, Computation time: 1.9562692642211914\n",
      "Step: 1288, Loss: 0.9161574244499207, Accuracy: 1.0, Computation time: 2.4020326137542725\n",
      "Step: 1289, Loss: 0.9182522296905518, Accuracy: 1.0, Computation time: 1.8118047714233398\n",
      "Step: 1290, Loss: 0.9376680254936218, Accuracy: 0.96875, Computation time: 2.5628252029418945\n",
      "Step: 1291, Loss: 0.9168190360069275, Accuracy: 1.0, Computation time: 2.160477876663208\n",
      "Step: 1292, Loss: 0.9368517994880676, Accuracy: 0.96875, Computation time: 2.6590898036956787\n",
      "Step: 1293, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 2.166393280029297\n",
      "Step: 1294, Loss: 0.9161579012870789, Accuracy: 1.0, Computation time: 2.0904059410095215\n",
      "Step: 1295, Loss: 0.9174468517303467, Accuracy: 1.0, Computation time: 2.1506497859954834\n",
      "Step: 1296, Loss: 0.9159741997718811, Accuracy: 1.0, Computation time: 2.5806381702423096\n",
      "Step: 1297, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 2.2154011726379395\n",
      "Step: 1298, Loss: 0.916069507598877, Accuracy: 1.0, Computation time: 2.121847152709961\n",
      "Step: 1299, Loss: 0.9379159808158875, Accuracy: 0.96875, Computation time: 2.182934284210205\n",
      "Step: 1300, Loss: 0.916051983833313, Accuracy: 1.0, Computation time: 2.8103957176208496\n",
      "Step: 1301, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 1.8015716075897217\n",
      "Step: 1302, Loss: 0.9161131381988525, Accuracy: 1.0, Computation time: 2.0763351917266846\n",
      "Step: 1303, Loss: 0.9160528779029846, Accuracy: 1.0, Computation time: 1.7104823589324951\n",
      "Step: 1304, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 2.0634500980377197\n",
      "Step: 1305, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 1.9137318134307861\n",
      "Step: 1306, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.93326735496521\n",
      "Step: 1307, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 1.875586748123169\n",
      "Step: 1308, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 2.265012741088867\n",
      "Step: 1309, Loss: 0.9183077216148376, Accuracy: 1.0, Computation time: 1.8432998657226562\n",
      "Step: 1310, Loss: 0.9159002900123596, Accuracy: 1.0, Computation time: 1.9219326972961426\n",
      "Step: 1311, Loss: 0.9159652590751648, Accuracy: 1.0, Computation time: 2.16412091255188\n",
      "Step: 1312, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 2.1811649799346924\n",
      "Step: 1313, Loss: 0.9159168601036072, Accuracy: 1.0, Computation time: 1.9097950458526611\n",
      "Step: 1314, Loss: 0.9160283207893372, Accuracy: 1.0, Computation time: 2.147733211517334\n",
      "Step: 1315, Loss: 0.9159823656082153, Accuracy: 1.0, Computation time: 2.455995559692383\n",
      "Step: 1316, Loss: 0.9376638531684875, Accuracy: 0.96875, Computation time: 1.799262285232544\n",
      "Step: 1317, Loss: 0.9164113402366638, Accuracy: 1.0, Computation time: 2.146662473678589\n",
      "Step: 1318, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.8815996646881104\n",
      "Step: 1319, Loss: 0.916252851486206, Accuracy: 1.0, Computation time: 1.629986047744751\n",
      "Step: 1320, Loss: 0.9186784625053406, Accuracy: 1.0, Computation time: 1.957263708114624\n",
      "Step: 1321, Loss: 0.9174602031707764, Accuracy: 1.0, Computation time: 2.6757187843322754\n",
      "Step: 1322, Loss: 0.9176725745201111, Accuracy: 1.0, Computation time: 2.5614850521087646\n",
      "Step: 1323, Loss: 0.9170644879341125, Accuracy: 1.0, Computation time: 2.4499003887176514\n",
      "Step: 1324, Loss: 0.9159743189811707, Accuracy: 1.0, Computation time: 2.0437002182006836\n",
      "Step: 1325, Loss: 0.9349345564842224, Accuracy: 0.96875, Computation time: 2.9005885124206543\n",
      "Step: 1326, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 1.61399507522583\n",
      "Step: 1327, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 2.02390718460083\n",
      "Step: 1328, Loss: 0.9160880446434021, Accuracy: 1.0, Computation time: 2.135885000228882\n",
      "Step: 1329, Loss: 0.9164823889732361, Accuracy: 1.0, Computation time: 2.177399158477783\n",
      "Step: 1330, Loss: 0.9159950017929077, Accuracy: 1.0, Computation time: 1.7619102001190186\n",
      "Step: 1331, Loss: 0.9167988300323486, Accuracy: 1.0, Computation time: 2.4659640789031982\n",
      "Step: 1332, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 2.1325559616088867\n",
      "Step: 1333, Loss: 0.937483549118042, Accuracy: 0.96875, Computation time: 1.585482120513916\n",
      "Step: 1334, Loss: 0.915923535823822, Accuracy: 1.0, Computation time: 1.9724721908569336\n",
      "Step: 1335, Loss: 0.9332762360572815, Accuracy: 0.96875, Computation time: 2.243551254272461\n",
      "Step: 1336, Loss: 0.9224461317062378, Accuracy: 1.0, Computation time: 1.9617345333099365\n",
      "Step: 1337, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 2.3219237327575684\n",
      "Step: 1338, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.867887020111084\n",
      "Step: 1339, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 1.5925962924957275\n",
      "Step: 1340, Loss: 0.9159461855888367, Accuracy: 1.0, Computation time: 1.9166626930236816\n",
      "Step: 1341, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 2.4307701587677\n",
      "Step: 1342, Loss: 0.9261636137962341, Accuracy: 0.96875, Computation time: 2.454230308532715\n",
      "Step: 1343, Loss: 0.9162167310714722, Accuracy: 1.0, Computation time: 1.848341703414917\n",
      "Step: 1344, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 2.010981321334839\n",
      "Step: 1345, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.9563219547271729\n",
      "Step: 1346, Loss: 0.937156081199646, Accuracy: 0.96875, Computation time: 2.0904135704040527\n",
      "Step: 1347, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 2.1933960914611816\n",
      "Step: 1348, Loss: 0.9160739183425903, Accuracy: 1.0, Computation time: 2.1421775817871094\n",
      "Step: 1349, Loss: 0.9346326589584351, Accuracy: 0.96875, Computation time: 2.044755220413208\n",
      "Step: 1350, Loss: 0.9158980846405029, Accuracy: 1.0, Computation time: 1.95161771774292\n",
      "Step: 1351, Loss: 0.9378712177276611, Accuracy: 0.96875, Computation time: 2.202395439147949\n",
      "Step: 1352, Loss: 0.9587036371231079, Accuracy: 0.9375, Computation time: 1.81895112991333\n",
      "Step: 1353, Loss: 0.9159984588623047, Accuracy: 1.0, Computation time: 1.8301525115966797\n",
      "Step: 1354, Loss: 0.9166263341903687, Accuracy: 1.0, Computation time: 2.2235498428344727\n",
      "Step: 1355, Loss: 0.9376419186592102, Accuracy: 0.96875, Computation time: 2.166703701019287\n",
      "Step: 1356, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.9449188709259033\n",
      "Step: 1357, Loss: 0.9184511303901672, Accuracy: 1.0, Computation time: 1.7214336395263672\n",
      "Step: 1358, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 1.9959237575531006\n",
      "Step: 1359, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.9416015148162842\n",
      "Step: 1360, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.6162405014038086\n",
      "Step: 1361, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 2.0273454189300537\n",
      "Step: 1362, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.6124422550201416\n",
      "Step: 1363, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 2.2086381912231445\n",
      "Step: 1364, Loss: 0.916792094707489, Accuracy: 1.0, Computation time: 1.9534859657287598\n",
      "Step: 1365, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.6785287857055664\n",
      "Step: 1366, Loss: 0.9392736554145813, Accuracy: 0.96875, Computation time: 2.371424913406372\n",
      "Step: 1367, Loss: 0.9180980324745178, Accuracy: 1.0, Computation time: 2.5397841930389404\n",
      "Step: 1368, Loss: 0.9375623464584351, Accuracy: 0.96875, Computation time: 1.9180960655212402\n",
      "Step: 1369, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.9702560901641846\n",
      "Step: 1370, Loss: 0.9163051843643188, Accuracy: 1.0, Computation time: 1.871739149093628\n",
      "Step: 1371, Loss: 0.9180876016616821, Accuracy: 1.0, Computation time: 1.96592116355896\n",
      "Step: 1372, Loss: 0.915945291519165, Accuracy: 1.0, Computation time: 1.963153600692749\n",
      "Step: 1373, Loss: 0.9240037798881531, Accuracy: 1.0, Computation time: 2.5088613033294678\n",
      "Step: 1374, Loss: 0.9159616231918335, Accuracy: 1.0, Computation time: 2.0565757751464844\n",
      "Step: 1375, Loss: 0.9159448146820068, Accuracy: 1.0, Computation time: 2.1542837619781494\n",
      "Step: 1376, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.7904257774353027\n",
      "Step: 1377, Loss: 0.9159273505210876, Accuracy: 1.0, Computation time: 1.7908670902252197\n",
      "Step: 1378, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.8457083702087402\n",
      "Step: 1379, Loss: 0.9364829063415527, Accuracy: 0.96875, Computation time: 2.3207767009735107\n",
      "Step: 1380, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.4405755996704102\n",
      "Step: 1381, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.786952018737793\n",
      "Step: 1382, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 2.0181543827056885\n",
      "Step: 1383, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 2.203646183013916\n",
      "Step: 1384, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.8354885578155518\n",
      "Step: 1385, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 2.0115058422088623\n",
      "Step: 1386, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.6257264614105225\n",
      "Step: 1387, Loss: 0.9164608716964722, Accuracy: 1.0, Computation time: 2.133326768875122\n",
      "Step: 1388, Loss: 0.915925920009613, Accuracy: 1.0, Computation time: 2.4703330993652344\n",
      "Step: 1389, Loss: 0.9158779978752136, Accuracy: 1.0, Computation time: 2.250947952270508\n",
      "Step: 1390, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.8632371425628662\n",
      "########################\n",
      "Test loss: 1.1273455619812012, Test Accuracy_epoch10: 0.6897662878036499\n",
      "########################\n",
      "Step: 1391, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.541893482208252\n",
      "Step: 1392, Loss: 0.9159025549888611, Accuracy: 1.0, Computation time: 1.832202434539795\n",
      "Step: 1393, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.6939680576324463\n",
      "Step: 1394, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 1.86729097366333\n",
      "Step: 1395, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 2.1716537475585938\n",
      "Step: 1396, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.979546070098877\n",
      "Step: 1397, Loss: 0.9387037754058838, Accuracy: 0.96875, Computation time: 2.446134090423584\n",
      "Step: 1398, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.9243431091308594\n",
      "Step: 1399, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 1.8823416233062744\n",
      "Step: 1400, Loss: 0.9163011908531189, Accuracy: 1.0, Computation time: 2.0490450859069824\n",
      "Step: 1401, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.6791892051696777\n",
      "Step: 1402, Loss: 0.9234699010848999, Accuracy: 1.0, Computation time: 1.8254737854003906\n",
      "Step: 1403, Loss: 0.9374471306800842, Accuracy: 0.96875, Computation time: 2.015381336212158\n",
      "Step: 1404, Loss: 0.9159567952156067, Accuracy: 1.0, Computation time: 1.6317088603973389\n",
      "Step: 1405, Loss: 0.9168863296508789, Accuracy: 1.0, Computation time: 2.0007591247558594\n",
      "Step: 1406, Loss: 0.9159457683563232, Accuracy: 1.0, Computation time: 1.915379285812378\n",
      "Step: 1407, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.5400443077087402\n",
      "Step: 1408, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.7715306282043457\n",
      "Step: 1409, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.9778614044189453\n",
      "Step: 1410, Loss: 0.9160727262496948, Accuracy: 1.0, Computation time: 2.1681554317474365\n",
      "Step: 1411, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 1.8070099353790283\n",
      "Step: 1412, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.729515552520752\n",
      "Step: 1413, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.71254301071167\n",
      "Step: 1414, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.875178575515747\n",
      "Step: 1415, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.8803162574768066\n",
      "Step: 1416, Loss: 0.9159327745437622, Accuracy: 1.0, Computation time: 1.6858630180358887\n",
      "Step: 1417, Loss: 0.9158667922019958, Accuracy: 1.0, Computation time: 1.7487776279449463\n",
      "Step: 1418, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 1.8732624053955078\n",
      "Step: 1419, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.8523950576782227\n",
      "Step: 1420, Loss: 0.9584977626800537, Accuracy: 0.9375, Computation time: 1.8874061107635498\n",
      "Step: 1421, Loss: 0.9170159101486206, Accuracy: 1.0, Computation time: 1.9903113842010498\n",
      "Step: 1422, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 1.9440925121307373\n",
      "Step: 1423, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 2.122187614440918\n",
      "Step: 1424, Loss: 0.9373708963394165, Accuracy: 0.96875, Computation time: 1.9006073474884033\n",
      "Step: 1425, Loss: 0.9309031367301941, Accuracy: 0.96875, Computation time: 1.892181634902954\n",
      "Step: 1426, Loss: 0.9160348176956177, Accuracy: 1.0, Computation time: 2.344282388687134\n",
      "Step: 1427, Loss: 0.9379341006278992, Accuracy: 0.96875, Computation time: 1.8647947311401367\n",
      "Step: 1428, Loss: 0.9159349203109741, Accuracy: 1.0, Computation time: 1.513439655303955\n",
      "Step: 1429, Loss: 0.9160386919975281, Accuracy: 1.0, Computation time: 1.527858018875122\n",
      "Step: 1430, Loss: 0.9317308664321899, Accuracy: 0.96875, Computation time: 1.7729506492614746\n",
      "Step: 1431, Loss: 0.9378410577774048, Accuracy: 0.96875, Computation time: 2.253695487976074\n",
      "Step: 1432, Loss: 0.9168238639831543, Accuracy: 1.0, Computation time: 1.788710594177246\n",
      "Step: 1433, Loss: 0.9380399584770203, Accuracy: 0.96875, Computation time: 1.5777826309204102\n",
      "Step: 1434, Loss: 0.9160258769989014, Accuracy: 1.0, Computation time: 1.6986260414123535\n",
      "Step: 1435, Loss: 0.9160285592079163, Accuracy: 1.0, Computation time: 1.903977394104004\n",
      "Step: 1436, Loss: 0.9161049723625183, Accuracy: 1.0, Computation time: 1.457005262374878\n",
      "Step: 1437, Loss: 0.9389362335205078, Accuracy: 0.96875, Computation time: 2.4856224060058594\n",
      "Step: 1438, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.8556413650512695\n",
      "Step: 1439, Loss: 0.9493927955627441, Accuracy: 0.9375, Computation time: 2.7613155841827393\n",
      "Step: 1440, Loss: 0.9166133999824524, Accuracy: 1.0, Computation time: 1.966322898864746\n",
      "Step: 1441, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.9951832294464111\n",
      "Step: 1442, Loss: 0.9159514904022217, Accuracy: 1.0, Computation time: 2.3684864044189453\n",
      "Step: 1443, Loss: 0.9375675916671753, Accuracy: 0.96875, Computation time: 2.5871434211730957\n",
      "Step: 1444, Loss: 0.921234667301178, Accuracy: 1.0, Computation time: 1.7730140686035156\n",
      "Step: 1445, Loss: 0.9371935725212097, Accuracy: 0.96875, Computation time: 1.880201816558838\n",
      "Step: 1446, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.57100248336792\n",
      "Step: 1447, Loss: 0.9370312094688416, Accuracy: 0.96875, Computation time: 1.6980760097503662\n",
      "Step: 1448, Loss: 0.9161394834518433, Accuracy: 1.0, Computation time: 1.7566392421722412\n",
      "Step: 1449, Loss: 0.91596919298172, Accuracy: 1.0, Computation time: 2.116584300994873\n",
      "Step: 1450, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 2.177443742752075\n",
      "Step: 1451, Loss: 0.9160308837890625, Accuracy: 1.0, Computation time: 2.118040084838867\n",
      "Step: 1452, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.7764182090759277\n",
      "Step: 1453, Loss: 0.937439501285553, Accuracy: 0.96875, Computation time: 1.6255385875701904\n",
      "Step: 1454, Loss: 0.9273693561553955, Accuracy: 0.96875, Computation time: 2.453805685043335\n",
      "Step: 1455, Loss: 0.9158763289451599, Accuracy: 1.0, Computation time: 1.7264525890350342\n",
      "Step: 1456, Loss: 0.9564724564552307, Accuracy: 0.9375, Computation time: 2.816765785217285\n",
      "Step: 1457, Loss: 0.9160316586494446, Accuracy: 1.0, Computation time: 1.9355804920196533\n",
      "Step: 1458, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 1.5935909748077393\n",
      "Step: 1459, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 1.8390872478485107\n",
      "Step: 1460, Loss: 0.9356112480163574, Accuracy: 0.96875, Computation time: 2.5228660106658936\n",
      "Step: 1461, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.7638003826141357\n",
      "Step: 1462, Loss: 0.9160052537918091, Accuracy: 1.0, Computation time: 1.9925739765167236\n",
      "Step: 1463, Loss: 0.9169007539749146, Accuracy: 1.0, Computation time: 1.7543597221374512\n",
      "Step: 1464, Loss: 0.9159720540046692, Accuracy: 1.0, Computation time: 1.6070566177368164\n",
      "Step: 1465, Loss: 0.9560956954956055, Accuracy: 0.9375, Computation time: 3.1637990474700928\n",
      "Step: 1466, Loss: 0.9159770607948303, Accuracy: 1.0, Computation time: 2.6165668964385986\n",
      "Step: 1467, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 1.976736068725586\n",
      "Step: 1468, Loss: 0.9179618954658508, Accuracy: 1.0, Computation time: 2.5610203742980957\n",
      "Step: 1469, Loss: 0.9160164594650269, Accuracy: 1.0, Computation time: 1.6648321151733398\n",
      "Step: 1470, Loss: 0.9163683652877808, Accuracy: 1.0, Computation time: 2.131927251815796\n",
      "Step: 1471, Loss: 0.9162451028823853, Accuracy: 1.0, Computation time: 1.8058757781982422\n",
      "Step: 1472, Loss: 0.9163240194320679, Accuracy: 1.0, Computation time: 2.4592180252075195\n",
      "Step: 1473, Loss: 0.9202221632003784, Accuracy: 1.0, Computation time: 2.964921474456787\n",
      "Step: 1474, Loss: 0.9164329171180725, Accuracy: 1.0, Computation time: 1.8923206329345703\n",
      "Step: 1475, Loss: 0.9174126386642456, Accuracy: 1.0, Computation time: 1.7848525047302246\n",
      "Step: 1476, Loss: 0.9164127707481384, Accuracy: 1.0, Computation time: 1.9223003387451172\n",
      "Step: 1477, Loss: 0.9161487221717834, Accuracy: 1.0, Computation time: 1.561143398284912\n",
      "Step: 1478, Loss: 0.9161034822463989, Accuracy: 1.0, Computation time: 1.793201208114624\n",
      "Step: 1479, Loss: 0.9325738549232483, Accuracy: 0.96875, Computation time: 2.4524784088134766\n",
      "Step: 1480, Loss: 0.9176076054573059, Accuracy: 1.0, Computation time: 2.3815693855285645\n",
      "Step: 1481, Loss: 0.9160369634628296, Accuracy: 1.0, Computation time: 1.7064273357391357\n",
      "Step: 1482, Loss: 0.9159570336341858, Accuracy: 1.0, Computation time: 1.8189365863800049\n",
      "Step: 1483, Loss: 0.91591876745224, Accuracy: 1.0, Computation time: 1.9998631477355957\n",
      "Step: 1484, Loss: 0.9375994801521301, Accuracy: 0.96875, Computation time: 1.5925078392028809\n",
      "Step: 1485, Loss: 0.9371263384819031, Accuracy: 0.96875, Computation time: 1.7500991821289062\n",
      "Step: 1486, Loss: 0.9159439206123352, Accuracy: 1.0, Computation time: 1.526900053024292\n",
      "Step: 1487, Loss: 0.9576283693313599, Accuracy: 0.9375, Computation time: 1.6170384883880615\n",
      "Step: 1488, Loss: 0.9159821271896362, Accuracy: 1.0, Computation time: 1.5937905311584473\n",
      "Step: 1489, Loss: 0.9162214994430542, Accuracy: 1.0, Computation time: 1.6945464611053467\n",
      "Step: 1490, Loss: 0.9347075819969177, Accuracy: 0.96875, Computation time: 2.8810925483703613\n",
      "Step: 1491, Loss: 0.9160186052322388, Accuracy: 1.0, Computation time: 1.842658519744873\n",
      "Step: 1492, Loss: 0.9161570072174072, Accuracy: 1.0, Computation time: 2.033912420272827\n",
      "Step: 1493, Loss: 0.9322987198829651, Accuracy: 0.96875, Computation time: 1.696882724761963\n",
      "Step: 1494, Loss: 0.9159656167030334, Accuracy: 1.0, Computation time: 1.634284257888794\n",
      "Step: 1495, Loss: 0.9159526228904724, Accuracy: 1.0, Computation time: 1.7443068027496338\n",
      "Step: 1496, Loss: 0.9243072867393494, Accuracy: 1.0, Computation time: 1.9811079502105713\n",
      "Step: 1497, Loss: 0.9159955978393555, Accuracy: 1.0, Computation time: 1.4180562496185303\n",
      "Step: 1498, Loss: 0.9561800360679626, Accuracy: 0.9375, Computation time: 3.8258962631225586\n",
      "Step: 1499, Loss: 0.9160624742507935, Accuracy: 1.0, Computation time: 1.8261635303497314\n",
      "Step: 1500, Loss: 0.9159670472145081, Accuracy: 1.0, Computation time: 1.6485412120819092\n",
      "Step: 1501, Loss: 0.9163077473640442, Accuracy: 1.0, Computation time: 1.701465129852295\n",
      "Step: 1502, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 2.02681827545166\n",
      "Step: 1503, Loss: 0.9159618616104126, Accuracy: 1.0, Computation time: 1.6646063327789307\n",
      "Step: 1504, Loss: 0.9159329533576965, Accuracy: 1.0, Computation time: 2.038179874420166\n",
      "Step: 1505, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.8146686553955078\n",
      "Step: 1506, Loss: 0.9360702633857727, Accuracy: 0.96875, Computation time: 1.7916266918182373\n",
      "Step: 1507, Loss: 0.9159978628158569, Accuracy: 1.0, Computation time: 1.8684735298156738\n",
      "Step: 1508, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.5099287033081055\n",
      "Step: 1509, Loss: 0.9160624742507935, Accuracy: 1.0, Computation time: 1.6186773777008057\n",
      "Step: 1510, Loss: 0.9368788003921509, Accuracy: 0.96875, Computation time: 1.8645012378692627\n",
      "Step: 1511, Loss: 0.9405132532119751, Accuracy: 0.96875, Computation time: 1.639223337173462\n",
      "Step: 1512, Loss: 0.9219046831130981, Accuracy: 1.0, Computation time: 1.8907215595245361\n",
      "Step: 1513, Loss: 0.9159258008003235, Accuracy: 1.0, Computation time: 1.4521186351776123\n",
      "Step: 1514, Loss: 0.9159371256828308, Accuracy: 1.0, Computation time: 1.5836613178253174\n",
      "Step: 1515, Loss: 0.9341431260108948, Accuracy: 0.96875, Computation time: 1.8953828811645508\n",
      "Step: 1516, Loss: 0.9160355925559998, Accuracy: 1.0, Computation time: 1.9130783081054688\n",
      "Step: 1517, Loss: 0.916046679019928, Accuracy: 1.0, Computation time: 1.7457964420318604\n",
      "Step: 1518, Loss: 0.9160187244415283, Accuracy: 1.0, Computation time: 1.376147747039795\n",
      "Step: 1519, Loss: 0.9160212874412537, Accuracy: 1.0, Computation time: 1.6509246826171875\n",
      "Step: 1520, Loss: 0.9168628454208374, Accuracy: 1.0, Computation time: 1.510662317276001\n",
      "Step: 1521, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 1.310821294784546\n",
      "Step: 1522, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.4784331321716309\n",
      "Step: 1523, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.7929120063781738\n",
      "Step: 1524, Loss: 0.9159319400787354, Accuracy: 1.0, Computation time: 1.5702354907989502\n",
      "Step: 1525, Loss: 0.9374421834945679, Accuracy: 0.96875, Computation time: 1.6563985347747803\n",
      "Step: 1526, Loss: 0.9159516096115112, Accuracy: 1.0, Computation time: 1.547043800354004\n",
      "Step: 1527, Loss: 0.9159935712814331, Accuracy: 1.0, Computation time: 1.6682112216949463\n",
      "Step: 1528, Loss: 0.9158980846405029, Accuracy: 1.0, Computation time: 1.2963409423828125\n",
      "Step: 1529, Loss: 0.9374595284461975, Accuracy: 0.96875, Computation time: 1.4927301406860352\n",
      "########################\n",
      "Test loss: 1.1242576837539673, Test Accuracy_epoch11: 0.6905721426010132\n",
      "########################\n",
      "Step: 1530, Loss: 0.9378200173377991, Accuracy: 0.96875, Computation time: 1.785684585571289\n",
      "Step: 1531, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.4316635131835938\n",
      "Step: 1532, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.3729255199432373\n",
      "Step: 1533, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.3844091892242432\n",
      "Step: 1534, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.4560997486114502\n",
      "Step: 1535, Loss: 0.959259569644928, Accuracy: 0.9375, Computation time: 1.896474838256836\n",
      "Step: 1536, Loss: 0.9170140027999878, Accuracy: 1.0, Computation time: 2.026860475540161\n",
      "Step: 1537, Loss: 0.9160634279251099, Accuracy: 1.0, Computation time: 1.261617660522461\n",
      "Step: 1538, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.618224859237671\n",
      "Step: 1539, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.8037467002868652\n",
      "Step: 1540, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.7943828105926514\n",
      "Step: 1541, Loss: 0.9326355457305908, Accuracy: 0.96875, Computation time: 2.008859395980835\n",
      "Step: 1542, Loss: 0.9159910678863525, Accuracy: 1.0, Computation time: 1.5916836261749268\n",
      "Step: 1543, Loss: 0.9224268198013306, Accuracy: 1.0, Computation time: 1.5795164108276367\n",
      "Step: 1544, Loss: 0.9160112738609314, Accuracy: 1.0, Computation time: 1.4499890804290771\n",
      "Step: 1545, Loss: 0.916277289390564, Accuracy: 1.0, Computation time: 1.0744690895080566\n",
      "Step: 1546, Loss: 0.9160053730010986, Accuracy: 1.0, Computation time: 1.6784863471984863\n",
      "Step: 1547, Loss: 0.9380443692207336, Accuracy: 0.96875, Computation time: 1.3633983135223389\n",
      "Step: 1548, Loss: 0.9159805178642273, Accuracy: 1.0, Computation time: 1.4584543704986572\n",
      "Step: 1549, Loss: 0.9159958958625793, Accuracy: 1.0, Computation time: 1.5872917175292969\n",
      "Step: 1550, Loss: 0.9159739017486572, Accuracy: 1.0, Computation time: 1.5232343673706055\n",
      "Step: 1551, Loss: 0.9161091446876526, Accuracy: 1.0, Computation time: 1.7414836883544922\n",
      "Step: 1552, Loss: 0.9207482933998108, Accuracy: 1.0, Computation time: 1.5647900104522705\n",
      "Step: 1553, Loss: 0.9348459839820862, Accuracy: 0.96875, Computation time: 2.004037857055664\n",
      "Step: 1554, Loss: 0.9195889830589294, Accuracy: 1.0, Computation time: 2.0799307823181152\n",
      "Step: 1555, Loss: 0.9166155457496643, Accuracy: 1.0, Computation time: 1.8537893295288086\n",
      "Step: 1556, Loss: 0.9160035848617554, Accuracy: 1.0, Computation time: 1.474966049194336\n",
      "Step: 1557, Loss: 0.9163036346435547, Accuracy: 1.0, Computation time: 1.6132762432098389\n",
      "Step: 1558, Loss: 0.9375237226486206, Accuracy: 0.96875, Computation time: 1.6552026271820068\n",
      "Step: 1559, Loss: 0.9172492027282715, Accuracy: 1.0, Computation time: 1.6843082904815674\n",
      "Step: 1560, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.725147008895874\n",
      "Step: 1561, Loss: 0.915983259677887, Accuracy: 1.0, Computation time: 1.8495824337005615\n",
      "Step: 1618, Loss: 0.9162250757217407, Accuracy: 1.0, Computation time: 1.8425137996673584\n",
      "Step: 1619, Loss: 0.9162445068359375, Accuracy: 1.0, Computation time: 1.7278802394866943\n",
      "Step: 1620, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.9034912586212158\n",
      "Step: 1621, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 1.5666475296020508\n",
      "Step: 1622, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.8418550491333008\n",
      "Step: 1623, Loss: 0.9370017647743225, Accuracy: 0.96875, Computation time: 2.170454263687134\n",
      "Step: 1624, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 1.9869771003723145\n",
      "Step: 1625, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 1.934959888458252\n",
      "Step: 1626, Loss: 0.9160591959953308, Accuracy: 1.0, Computation time: 1.733644962310791\n",
      "Step: 1627, Loss: 0.9160326719284058, Accuracy: 1.0, Computation time: 2.053382158279419\n",
      "Step: 1628, Loss: 0.9178754687309265, Accuracy: 1.0, Computation time: 2.216306447982788\n",
      "Step: 1629, Loss: 0.9169870018959045, Accuracy: 1.0, Computation time: 2.0821242332458496\n",
      "Step: 1630, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.9307043552398682\n",
      "Step: 1631, Loss: 0.9169148206710815, Accuracy: 1.0, Computation time: 1.7125582695007324\n",
      "Step: 1632, Loss: 0.9377211928367615, Accuracy: 0.96875, Computation time: 2.1795787811279297\n",
      "Step: 1633, Loss: 0.9163928031921387, Accuracy: 1.0, Computation time: 1.7470557689666748\n",
      "Step: 1634, Loss: 0.9160206913948059, Accuracy: 1.0, Computation time: 1.753920078277588\n",
      "Step: 1635, Loss: 0.9203069806098938, Accuracy: 1.0, Computation time: 2.1984503269195557\n",
      "Step: 1636, Loss: 0.9160908460617065, Accuracy: 1.0, Computation time: 2.002933979034424\n",
      "Step: 1637, Loss: 0.9159881472587585, Accuracy: 1.0, Computation time: 1.8891735076904297\n",
      "Step: 1638, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 1.8553216457366943\n",
      "Step: 1639, Loss: 0.9182514548301697, Accuracy: 1.0, Computation time: 1.7747907638549805\n",
      "Step: 1640, Loss: 0.9163505434989929, Accuracy: 1.0, Computation time: 1.992051601409912\n",
      "Step: 1641, Loss: 0.9375478029251099, Accuracy: 0.96875, Computation time: 1.8889734745025635\n",
      "Step: 1642, Loss: 0.9161067008972168, Accuracy: 1.0, Computation time: 1.5659818649291992\n",
      "Step: 1643, Loss: 0.9364599585533142, Accuracy: 0.96875, Computation time: 1.5527358055114746\n",
      "Step: 1644, Loss: 0.916425347328186, Accuracy: 1.0, Computation time: 2.290689706802368\n",
      "Step: 1645, Loss: 0.9375193119049072, Accuracy: 0.96875, Computation time: 1.6568090915679932\n",
      "Step: 1646, Loss: 0.9364244937896729, Accuracy: 0.96875, Computation time: 2.0693368911743164\n",
      "Step: 1647, Loss: 0.9159719944000244, Accuracy: 1.0, Computation time: 1.9332268238067627\n",
      "Step: 1648, Loss: 0.9159743189811707, Accuracy: 1.0, Computation time: 1.768630027770996\n",
      "Step: 1649, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.5901422500610352\n",
      "Step: 1650, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.6300499439239502\n",
      "Step: 1651, Loss: 0.9160030484199524, Accuracy: 1.0, Computation time: 1.88328218460083\n",
      "Step: 1652, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 1.7023389339447021\n",
      "Step: 1653, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.4141812324523926\n",
      "Step: 1654, Loss: 0.9160347580909729, Accuracy: 1.0, Computation time: 1.7830777168273926\n",
      "Step: 1655, Loss: 0.9375067949295044, Accuracy: 0.96875, Computation time: 1.5028879642486572\n",
      "Step: 1656, Loss: 0.9160460829734802, Accuracy: 1.0, Computation time: 1.7160468101501465\n",
      "Step: 1657, Loss: 0.9581101536750793, Accuracy: 0.9375, Computation time: 1.9148533344268799\n",
      "Step: 1658, Loss: 0.9431273937225342, Accuracy: 0.96875, Computation time: 1.8187522888183594\n",
      "Step: 1659, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.524902582168579\n",
      "Step: 1660, Loss: 0.9384337663650513, Accuracy: 0.96875, Computation time: 2.532518148422241\n",
      "Step: 1661, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.6915528774261475\n",
      "Step: 1662, Loss: 0.9277712106704712, Accuracy: 0.96875, Computation time: 2.2700748443603516\n",
      "Step: 1663, Loss: 0.950991690158844, Accuracy: 0.9375, Computation time: 2.161262035369873\n",
      "Step: 1664, Loss: 0.9163921475410461, Accuracy: 1.0, Computation time: 1.9288082122802734\n",
      "Step: 1665, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.940521240234375\n",
      "Step: 1666, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.5493221282958984\n",
      "Step: 1667, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 1.62424898147583\n",
      "Step: 1668, Loss: 0.9371619820594788, Accuracy: 0.96875, Computation time: 2.1238272190093994\n",
      "########################\n",
      "Test loss: 1.1208995580673218, Test Accuracy_epoch12: 0.701853334903717\n",
      "########################\n",
      "Step: 1669, Loss: 0.915941596031189, Accuracy: 1.0, Computation time: 1.6971385478973389\n",
      "Step: 1670, Loss: 0.9372475743293762, Accuracy: 0.96875, Computation time: 2.089017629623413\n",
      "Step: 1671, Loss: 0.915919303894043, Accuracy: 1.0, Computation time: 1.6254782676696777\n",
      "Step: 1672, Loss: 0.9159073233604431, Accuracy: 1.0, Computation time: 1.554046630859375\n",
      "Step: 1673, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.7686347961425781\n",
      "Step: 1674, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.7029435634613037\n",
      "Step: 1675, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.4617853164672852\n",
      "Step: 1676, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.7069239616394043\n",
      "Step: 1677, Loss: 0.9173182249069214, Accuracy: 1.0, Computation time: 1.6800858974456787\n",
      "Step: 1678, Loss: 0.9167556762695312, Accuracy: 1.0, Computation time: 1.8316888809204102\n",
      "Step: 1679, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.9347798824310303\n",
      "Step: 1680, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 2.32234787940979\n",
      "Step: 1681, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.8264334201812744\n",
      "Step: 1682, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.7158751487731934\n",
      "Step: 1683, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 2.053823709487915\n",
      "Step: 1684, Loss: 0.9167969822883606, Accuracy: 1.0, Computation time: 1.8440086841583252\n",
      "Step: 1685, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.6755867004394531\n",
      "Step: 1686, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.7512545585632324\n",
      "Step: 1687, Loss: 0.9175583124160767, Accuracy: 1.0, Computation time: 2.6376404762268066\n",
      "Step: 1688, Loss: 0.9251115918159485, Accuracy: 1.0, Computation time: 1.6911935806274414\n",
      "Step: 1689, Loss: 0.9528048038482666, Accuracy: 0.9375, Computation time: 1.5698115825653076\n",
      "Step: 1690, Loss: 0.9159178137779236, Accuracy: 1.0, Computation time: 1.8454170227050781\n",
      "Step: 1691, Loss: 0.9159400463104248, Accuracy: 1.0, Computation time: 2.2374684810638428\n",
      "Step: 1692, Loss: 0.9160073399543762, Accuracy: 1.0, Computation time: 2.1294665336608887\n",
      "Step: 1693, Loss: 0.9159521460533142, Accuracy: 1.0, Computation time: 1.9380543231964111\n",
      "Step: 1694, Loss: 0.9159392714500427, Accuracy: 1.0, Computation time: 1.4380486011505127\n",
      "Step: 1695, Loss: 0.9199762344360352, Accuracy: 1.0, Computation time: 2.202282190322876\n",
      "Step: 1696, Loss: 0.9159558415412903, Accuracy: 1.0, Computation time: 1.596193790435791\n",
      "Step: 1697, Loss: 0.9372981786727905, Accuracy: 0.96875, Computation time: 1.398528814315796\n",
      "Step: 1698, Loss: 0.9385986924171448, Accuracy: 0.96875, Computation time: 1.967453956604004\n",
      "Step: 1699, Loss: 0.9163392186164856, Accuracy: 1.0, Computation time: 1.5973429679870605\n",
      "Step: 1700, Loss: 0.9203067421913147, Accuracy: 1.0, Computation time: 1.7516541481018066\n",
      "Step: 1701, Loss: 0.9161133766174316, Accuracy: 1.0, Computation time: 1.7352194786071777\n",
      "Step: 1702, Loss: 0.9159944653511047, Accuracy: 1.0, Computation time: 1.6170766353607178\n",
      "Step: 1703, Loss: 0.9160555005073547, Accuracy: 1.0, Computation time: 1.4355361461639404\n",
      "Step: 1704, Loss: 0.9362733960151672, Accuracy: 0.96875, Computation time: 2.4749317169189453\n",
      "Step: 1705, Loss: 0.9376040697097778, Accuracy: 0.96875, Computation time: 2.3724138736724854\n",
      "Step: 1706, Loss: 0.9376566410064697, Accuracy: 0.96875, Computation time: 2.0780930519104004\n",
      "Step: 1707, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 2.095953941345215\n",
      "Step: 1708, Loss: 0.9159485101699829, Accuracy: 1.0, Computation time: 1.884340524673462\n",
      "Step: 1709, Loss: 0.916530430316925, Accuracy: 1.0, Computation time: 2.263359785079956\n",
      "Step: 1710, Loss: 0.9159725308418274, Accuracy: 1.0, Computation time: 1.8004815578460693\n",
      "Step: 1711, Loss: 0.937286913394928, Accuracy: 0.96875, Computation time: 1.5005419254302979\n",
      "Step: 1712, Loss: 0.9376522898674011, Accuracy: 0.96875, Computation time: 1.6424579620361328\n",
      "Step: 1713, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 2.0730507373809814\n",
      "Step: 1714, Loss: 0.9375184178352356, Accuracy: 0.96875, Computation time: 2.34550404548645\n",
      "Step: 1715, Loss: 0.9309561848640442, Accuracy: 0.96875, Computation time: 2.6322927474975586\n",
      "Step: 1716, Loss: 0.9159393310546875, Accuracy: 1.0, Computation time: 1.6032772064208984\n",
      "Step: 1717, Loss: 0.9159348607063293, Accuracy: 1.0, Computation time: 1.9859306812286377\n",
      "Step: 1718, Loss: 0.9159334301948547, Accuracy: 1.0, Computation time: 1.7947020530700684\n",
      "Step: 1719, Loss: 0.9377283453941345, Accuracy: 0.96875, Computation time: 2.196777820587158\n",
      "Step: 1720, Loss: 0.9326714873313904, Accuracy: 0.96875, Computation time: 1.7135696411132812\n",
      "Step: 1721, Loss: 0.9378159046173096, Accuracy: 0.96875, Computation time: 1.8493671417236328\n",
      "Step: 1722, Loss: 0.9375308156013489, Accuracy: 0.96875, Computation time: 1.808476448059082\n",
      "Step: 1723, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.9289581775665283\n",
      "Step: 1724, Loss: 0.9164051413536072, Accuracy: 1.0, Computation time: 1.7121388912200928\n",
      "Step: 1725, Loss: 0.9161452651023865, Accuracy: 1.0, Computation time: 2.4429469108581543\n",
      "Step: 1726, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.640415906906128\n",
      "Step: 1727, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.858414649963379\n",
      "Step: 1728, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.6957924365997314\n",
      "Step: 1729, Loss: 0.9159827828407288, Accuracy: 1.0, Computation time: 1.3521966934204102\n",
      "Step: 1730, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 2.0012240409851074\n",
      "Step: 1731, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 1.7747998237609863\n",
      "Step: 1732, Loss: 0.9159282445907593, Accuracy: 1.0, Computation time: 1.8439664840698242\n",
      "Step: 1733, Loss: 0.9389019012451172, Accuracy: 0.96875, Computation time: 1.9716062545776367\n",
      "Step: 1734, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.5613651275634766\n",
      "Step: 1735, Loss: 0.9160234928131104, Accuracy: 1.0, Computation time: 2.047823905944824\n",
      "Step: 1736, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 2.1122100353240967\n",
      "Step: 1737, Loss: 0.9328295588493347, Accuracy: 0.96875, Computation time: 2.624330997467041\n",
      "Step: 1738, Loss: 0.9159893989562988, Accuracy: 1.0, Computation time: 2.045170545578003\n",
      "Step: 1739, Loss: 0.9159731864929199, Accuracy: 1.0, Computation time: 1.8784470558166504\n",
      "Step: 1740, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.8554489612579346\n",
      "Step: 1741, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.9520025253295898\n",
      "Step: 1742, Loss: 0.9226544499397278, Accuracy: 1.0, Computation time: 2.09407901763916\n",
      "Step: 1743, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 1.8040602207183838\n",
      "Step: 1744, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.922215461730957\n",
      "Step: 1745, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 1.9469058513641357\n",
      "Step: 1746, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.7530419826507568\n",
      "Step: 1747, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.8860323429107666\n",
      "Step: 1748, Loss: 0.9159438014030457, Accuracy: 1.0, Computation time: 1.7042710781097412\n",
      "Step: 1749, Loss: 0.9161437749862671, Accuracy: 1.0, Computation time: 1.6001462936401367\n",
      "Step: 1750, Loss: 0.9181187152862549, Accuracy: 1.0, Computation time: 1.9555599689483643\n",
      "Step: 1751, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 2.03878116607666\n",
      "Step: 1752, Loss: 0.9159018993377686, Accuracy: 1.0, Computation time: 2.0612359046936035\n",
      "Step: 1753, Loss: 0.9374227523803711, Accuracy: 0.96875, Computation time: 2.0418803691864014\n",
      "Step: 1754, Loss: 0.9171305298805237, Accuracy: 1.0, Computation time: 1.7303850650787354\n",
      "Step: 1755, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 1.6563544273376465\n",
      "Step: 1756, Loss: 0.937233567237854, Accuracy: 0.96875, Computation time: 1.725799322128296\n",
      "Step: 1757, Loss: 0.9450607895851135, Accuracy: 0.96875, Computation time: 2.192451000213623\n",
      "Step: 1799, Loss: 0.9159119129180908, Accuracy: 1.0, Computation time: 1.876328706741333\n",
      "Step: 1800, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.5927660465240479\n",
      "Step: 1801, Loss: 0.9523400068283081, Accuracy: 0.9375, Computation time: 2.636070728302002\n",
      "Step: 1802, Loss: 0.9160259366035461, Accuracy: 1.0, Computation time: 1.717329740524292\n",
      "Step: 1803, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.6387591361999512\n",
      "Step: 1804, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.4763333797454834\n",
      "Step: 1805, Loss: 0.9592857360839844, Accuracy: 0.9375, Computation time: 1.894655466079712\n",
      "Step: 1806, Loss: 0.9159581065177917, Accuracy: 1.0, Computation time: 2.124083995819092\n",
      "Step: 1807, Loss: 0.9162108898162842, Accuracy: 1.0, Computation time: 2.1627418994903564\n",
      "########################\n",
      "Test loss: 1.1215996742248535, Test Accuracy_epoch13: 0.70024174451828\n",
      "########################\n",
      "Step: 1808, Loss: 0.9350252151489258, Accuracy: 0.96875, Computation time: 1.7597932815551758\n",
      "Step: 1809, Loss: 0.9384813904762268, Accuracy: 0.96875, Computation time: 2.2247283458709717\n",
      "Step: 1810, Loss: 0.9159119129180908, Accuracy: 1.0, Computation time: 2.4803578853607178\n",
      "Step: 1811, Loss: 0.9159383773803711, Accuracy: 1.0, Computation time: 1.995811939239502\n",
      "Step: 1812, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 2.579524278640747\n",
      "Step: 1813, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 1.954035758972168\n",
      "Step: 1814, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.6802988052368164\n",
      "Step: 1815, Loss: 0.9163907766342163, Accuracy: 1.0, Computation time: 1.9204421043395996\n",
      "Step: 1816, Loss: 0.9159978032112122, Accuracy: 1.0, Computation time: 1.8113832473754883\n",
      "Step: 1817, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 1.7966971397399902\n",
      "Step: 1818, Loss: 0.9159960746765137, Accuracy: 1.0, Computation time: 1.826944351196289\n",
      "Step: 1819, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 1.6421704292297363\n",
      "Step: 1820, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.6835718154907227\n",
      "Step: 1821, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.9168188571929932\n",
      "Step: 1822, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.787482738494873\n",
      "Step: 1823, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 2.234379768371582\n",
      "Step: 1824, Loss: 0.93680739402771, Accuracy: 0.96875, Computation time: 1.9031774997711182\n",
      "Step: 1825, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 1.7188949584960938\n",
      "Step: 1826, Loss: 0.9159491658210754, Accuracy: 1.0, Computation time: 2.0174720287323\n",
      "Step: 1827, Loss: 0.978800356388092, Accuracy: 0.90625, Computation time: 2.7648096084594727\n",
      "Step: 1828, Loss: 0.9361549615859985, Accuracy: 0.96875, Computation time: 2.3122708797454834\n",
      "Step: 1829, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.7611291408538818\n",
      "Step: 1830, Loss: 0.9184525609016418, Accuracy: 1.0, Computation time: 1.9923114776611328\n",
      "Step: 1831, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.8332037925720215\n",
      "Step: 1832, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 1.8033661842346191\n",
      "Step: 1833, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.8206450939178467\n",
      "Step: 1834, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 2.125934600830078\n",
      "Step: 1835, Loss: 0.921186089515686, Accuracy: 1.0, Computation time: 2.091921806335449\n",
      "Step: 1836, Loss: 0.9253551363945007, Accuracy: 0.96875, Computation time: 1.8842391967773438\n",
      "Step: 1837, Loss: 0.9247404336929321, Accuracy: 1.0, Computation time: 2.091629981994629\n",
      "Step: 1838, Loss: 0.9182650446891785, Accuracy: 1.0, Computation time: 2.4192285537719727\n",
      "Step: 1839, Loss: 0.937551736831665, Accuracy: 0.96875, Computation time: 1.8734610080718994\n",
      "Step: 1840, Loss: 0.9160565733909607, Accuracy: 1.0, Computation time: 1.7557744979858398\n",
      "Step: 1841, Loss: 0.9376447796821594, Accuracy: 0.96875, Computation time: 1.9159135818481445\n",
      "Step: 1842, Loss: 0.9161020517349243, Accuracy: 1.0, Computation time: 1.6559669971466064\n",
      "Step: 1843, Loss: 0.915992021560669, Accuracy: 1.0, Computation time: 2.1576671600341797\n",
      "Step: 1844, Loss: 0.9159599542617798, Accuracy: 1.0, Computation time: 1.8534364700317383\n",
      "Step: 1845, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 1.684168815612793\n",
      "Step: 1846, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 2.0255277156829834\n",
      "Step: 1847, Loss: 0.9167442917823792, Accuracy: 1.0, Computation time: 2.0264225006103516\n",
      "Step: 1848, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.9063851833343506\n",
      "Step: 1849, Loss: 0.9159082770347595, Accuracy: 1.0, Computation time: 1.95316481590271\n",
      "Step: 1850, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 2.0193986892700195\n",
      "Step: 1851, Loss: 0.9159225225448608, Accuracy: 1.0, Computation time: 1.8609235286712646\n",
      "Step: 1852, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 1.5208611488342285\n",
      "Step: 1853, Loss: 0.9375028610229492, Accuracy: 0.96875, Computation time: 1.837580680847168\n",
      "Step: 1854, Loss: 0.9159263968467712, Accuracy: 1.0, Computation time: 2.2424161434173584\n",
      "Step: 1855, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.6660082340240479\n",
      "Step: 1856, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.9755027294158936\n",
      "Step: 1857, Loss: 0.9159553050994873, Accuracy: 1.0, Computation time: 1.9449536800384521\n",
      "Step: 1858, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.4710493087768555\n",
      "Step: 1859, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 1.9813413619995117\n",
      "Step: 1860, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.6557097434997559\n",
      "Step: 1861, Loss: 0.9429903030395508, Accuracy: 0.96875, Computation time: 2.1268246173858643\n",
      "Step: 1862, Loss: 0.9377199411392212, Accuracy: 0.96875, Computation time: 2.047034740447998\n",
      "Step: 1863, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.6113371849060059\n",
      "Step: 1864, Loss: 0.9374985694885254, Accuracy: 0.96875, Computation time: 1.6755492687225342\n",
      "Step: 1865, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 1.891221523284912\n",
      "Step: 1866, Loss: 0.9159268140792847, Accuracy: 1.0, Computation time: 2.379639148712158\n",
      "Step: 1867, Loss: 0.9375646710395813, Accuracy: 0.96875, Computation time: 2.1099228858947754\n",
      "Step: 1868, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.7246382236480713\n",
      "Step: 1869, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 2.3903238773345947\n",
      "Step: 1870, Loss: 0.9159350395202637, Accuracy: 1.0, Computation time: 1.6583404541015625\n",
      "Step: 1871, Loss: 0.9351969361305237, Accuracy: 0.96875, Computation time: 1.6361045837402344\n",
      "Step: 1872, Loss: 0.9206663370132446, Accuracy: 1.0, Computation time: 1.8080122470855713\n",
      "Step: 1873, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.7113609313964844\n",
      "Step: 1874, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.5504803657531738\n",
      "Step: 1875, Loss: 0.9160529375076294, Accuracy: 1.0, Computation time: 1.8961598873138428\n",
      "Step: 1876, Loss: 0.9375767707824707, Accuracy: 0.96875, Computation time: 1.9798390865325928\n",
      "Step: 1877, Loss: 0.9178306460380554, Accuracy: 1.0, Computation time: 1.9925060272216797\n",
      "Step: 1878, Loss: 0.9163357019424438, Accuracy: 1.0, Computation time: 1.8568193912506104\n",
      "Step: 1879, Loss: 0.915947437286377, Accuracy: 1.0, Computation time: 1.741363525390625\n",
      "Step: 1880, Loss: 0.9164717197418213, Accuracy: 1.0, Computation time: 1.8671269416809082\n",
      "Step: 1881, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 1.5493528842926025\n",
      "Step: 1882, Loss: 0.9375285506248474, Accuracy: 0.96875, Computation time: 2.2765181064605713\n",
      "Step: 1883, Loss: 0.9159114956855774, Accuracy: 1.0, Computation time: 1.8693265914916992\n",
      "Step: 1884, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 2.171628713607788\n",
      "Step: 1885, Loss: 0.9160184264183044, Accuracy: 1.0, Computation time: 1.6722376346588135\n",
      "Step: 1886, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 2.037282705307007\n",
      "Step: 1887, Loss: 0.9159481525421143, Accuracy: 1.0, Computation time: 2.155301570892334\n",
      "Step: 1888, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.5517377853393555\n",
      "Step: 1889, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.8373026847839355\n",
      "Step: 1890, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 2.0428555011749268\n",
      "Step: 1891, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.8255271911621094\n",
      "Step: 1892, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.6259100437164307\n",
      "Step: 1893, Loss: 0.9370484948158264, Accuracy: 0.96875, Computation time: 2.2822601795196533\n",
      "Step: 1894, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.6742942333221436\n",
      "Step: 1895, Loss: 0.9165197610855103, Accuracy: 1.0, Computation time: 1.7535109519958496\n",
      "Step: 1896, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.5875885486602783\n",
      "Step: 1897, Loss: 0.9407010674476624, Accuracy: 0.96875, Computation time: 2.045707941055298\n",
      "Step: 1898, Loss: 0.9161148071289062, Accuracy: 1.0, Computation time: 1.678537368774414\n",
      "Step: 1899, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.9949171543121338\n",
      "Step: 1900, Loss: 0.9376079440116882, Accuracy: 0.96875, Computation time: 1.7176969051361084\n",
      "Step: 1901, Loss: 0.9374649524688721, Accuracy: 0.96875, Computation time: 1.8305766582489014\n",
      "Step: 1902, Loss: 0.9159547686576843, Accuracy: 1.0, Computation time: 1.8125762939453125\n",
      "Step: 1903, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.8631927967071533\n",
      "Step: 1904, Loss: 0.9160123467445374, Accuracy: 1.0, Computation time: 1.7420861721038818\n",
      "Step: 1905, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 2.0440478324890137\n",
      "Step: 1906, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 2.0023672580718994\n",
      "Step: 1907, Loss: 0.939285159111023, Accuracy: 0.96875, Computation time: 2.51059627532959\n",
      "Step: 1908, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.7160422801971436\n",
      "Step: 1909, Loss: 0.9160313010215759, Accuracy: 1.0, Computation time: 1.6679284572601318\n",
      "Step: 1910, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 2.0631232261657715\n",
      "Step: 1911, Loss: 0.923949122428894, Accuracy: 1.0, Computation time: 1.6006560325622559\n",
      "Step: 1912, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.7731587886810303\n",
      "Step: 1913, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.8595161437988281\n",
      "Step: 1914, Loss: 0.935393750667572, Accuracy: 0.96875, Computation time: 1.836371898651123\n",
      "Step: 1915, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 2.083601713180542\n",
      "Step: 1916, Loss: 0.9510676860809326, Accuracy: 0.9375, Computation time: 2.3862416744232178\n",
      "Step: 1917, Loss: 0.9162416458129883, Accuracy: 1.0, Computation time: 2.1746842861175537\n",
      "Step: 1918, Loss: 0.9375933408737183, Accuracy: 0.96875, Computation time: 1.8113977909088135\n",
      "Step: 1919, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 1.7518024444580078\n",
      "Step: 1920, Loss: 0.9160066246986389, Accuracy: 1.0, Computation time: 1.8443243503570557\n",
      "Step: 1921, Loss: 0.915989100933075, Accuracy: 1.0, Computation time: 2.088787794113159\n",
      "Step: 1922, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.849242925643921\n",
      "Step: 1923, Loss: 0.9268035888671875, Accuracy: 0.96875, Computation time: 1.993091344833374\n",
      "Step: 1924, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 1.695326805114746\n",
      "Step: 1925, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.718698263168335\n",
      "Step: 1926, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.8647568225860596\n",
      "Step: 1927, Loss: 0.9401201009750366, Accuracy: 0.96875, Computation time: 2.7330093383789062\n",
      "Step: 1928, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.929044485092163\n",
      "Step: 1929, Loss: 0.9160100221633911, Accuracy: 1.0, Computation time: 2.0200610160827637\n",
      "Step: 1930, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 1.767272710800171\n",
      "Step: 1931, Loss: 0.9159951210021973, Accuracy: 1.0, Computation time: 1.995077133178711\n",
      "Step: 1932, Loss: 0.9375236630439758, Accuracy: 0.96875, Computation time: 2.2653512954711914\n",
      "Step: 1933, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 2.16184401512146\n",
      "Step: 1934, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.977731704711914\n",
      "Step: 1935, Loss: 0.9228203296661377, Accuracy: 1.0, Computation time: 2.4593777656555176\n",
      "Step: 1936, Loss: 0.9375612139701843, Accuracy: 0.96875, Computation time: 1.7648365497589111\n",
      "Step: 1937, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 2.2408294677734375\n",
      "Step: 1938, Loss: 0.9161721467971802, Accuracy: 1.0, Computation time: 2.42935848236084\n",
      "Step: 1939, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.9095401763916016\n",
      "Step: 1940, Loss: 0.9182965755462646, Accuracy: 1.0, Computation time: 2.007329225540161\n",
      "Step: 1941, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 1.8341519832611084\n",
      "Step: 1942, Loss: 0.9159513115882874, Accuracy: 1.0, Computation time: 2.496229648590088\n",
      "Step: 1943, Loss: 0.9159311056137085, Accuracy: 1.0, Computation time: 1.8047730922698975\n",
      "Step: 1944, Loss: 0.9374224543571472, Accuracy: 0.96875, Computation time: 1.7212164402008057\n",
      "Step: 1945, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 2.0935885906219482\n",
      "Step: 1946, Loss: 0.937723696231842, Accuracy: 0.96875, Computation time: 1.7665412425994873\n",
      "########################\n",
      "Test loss: 1.1221157312393188, Test Accuracy_epoch14: 0.701853334903717\n",
      "########################\n",
      "Step: 1947, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.7035713195800781\n",
      "Step: 1948, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.94669771194458\n",
      "Step: 1949, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 2.127119779586792\n",
      "Step: 1950, Loss: 0.9159222841262817, Accuracy: 1.0, Computation time: 1.8512046337127686\n",
      "Step: 1951, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.8922605514526367\n",
      "Step: 1952, Loss: 0.9376375675201416, Accuracy: 0.96875, Computation time: 1.9459939002990723\n",
      "Step: 1953, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 1.783806562423706\n",
      "Step: 1954, Loss: 0.9162170886993408, Accuracy: 1.0, Computation time: 1.7878227233886719\n",
      "Step: 1955, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.7805469036102295\n",
      "Step: 1956, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 2.1299631595611572\n",
      "Step: 1957, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 2.065871477127075\n",
      "Step: 1958, Loss: 0.9376505017280579, Accuracy: 0.96875, Computation time: 1.7724294662475586\n",
      "Step: 1959, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.5725502967834473\n",
      "Step: 1960, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.8311216831207275\n",
      "Step: 1961, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.7230308055877686\n",
      "Step: 1962, Loss: 0.9371007084846497, Accuracy: 0.96875, Computation time: 1.7636959552764893\n",
      "Step: 1963, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.9505622386932373\n",
      "Step: 1964, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.9682588577270508\n",
      "Step: 1965, Loss: 0.9160473346710205, Accuracy: 1.0, Computation time: 2.360750436782837\n",
      "Step: 1966, Loss: 0.9364706873893738, Accuracy: 0.96875, Computation time: 1.7511775493621826\n",
      "Step: 1967, Loss: 0.9348568320274353, Accuracy: 0.96875, Computation time: 2.6682965755462646\n",
      "Step: 1968, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 1.8248770236968994\n",
      "Step: 1969, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.828333854675293\n",
      "Step: 1970, Loss: 0.9164681434631348, Accuracy: 1.0, Computation time: 1.8835058212280273\n",
      "Step: 1971, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.9500207901000977\n",
      "Step: 1972, Loss: 0.9177088141441345, Accuracy: 1.0, Computation time: 1.8305974006652832\n",
      "Step: 1973, Loss: 0.9160296320915222, Accuracy: 1.0, Computation time: 2.2017934322357178\n",
      "Step: 2014, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.8809599876403809\n",
      "Step: 2015, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.907811164855957\n",
      "Step: 2016, Loss: 0.9159270524978638, Accuracy: 1.0, Computation time: 1.8191816806793213\n",
      "Step: 2017, Loss: 0.915990948677063, Accuracy: 1.0, Computation time: 1.7140865325927734\n",
      "Step: 2018, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 2.4744763374328613\n",
      "Step: 2019, Loss: 0.9278417825698853, Accuracy: 0.96875, Computation time: 2.5120885372161865\n",
      "Step: 2020, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 1.6011879444122314\n",
      "Step: 2021, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 1.9624121189117432\n",
      "Step: 2022, Loss: 0.915916383266449, Accuracy: 1.0, Computation time: 1.7653164863586426\n",
      "Step: 2023, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.962183952331543\n",
      "Step: 2024, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 1.978442907333374\n",
      "Step: 2025, Loss: 0.915952205657959, Accuracy: 1.0, Computation time: 2.308511257171631\n",
      "Step: 2026, Loss: 0.9159286618232727, Accuracy: 1.0, Computation time: 1.7310218811035156\n",
      "Step: 2027, Loss: 0.916081964969635, Accuracy: 1.0, Computation time: 1.784888505935669\n",
      "Step: 2028, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 2.053196430206299\n",
      "Step: 2029, Loss: 0.9362429976463318, Accuracy: 0.96875, Computation time: 2.8338680267333984\n",
      "Step: 2030, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.9944570064544678\n",
      "Step: 2031, Loss: 0.9348491430282593, Accuracy: 0.96875, Computation time: 2.0030975341796875\n",
      "Step: 2032, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 2.0589537620544434\n",
      "Step: 2033, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 2.0699071884155273\n",
      "Step: 2034, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 2.362621784210205\n",
      "Step: 2035, Loss: 0.915958046913147, Accuracy: 1.0, Computation time: 2.2150537967681885\n",
      "Step: 2036, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 1.912898302078247\n",
      "Step: 2037, Loss: 0.920812726020813, Accuracy: 1.0, Computation time: 1.6237061023712158\n",
      "Step: 2038, Loss: 0.9381878972053528, Accuracy: 0.96875, Computation time: 2.533320665359497\n",
      "Step: 2039, Loss: 0.9376139640808105, Accuracy: 0.96875, Computation time: 2.0761008262634277\n",
      "Step: 2040, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 2.066941022872925\n",
      "Step: 2041, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.7699251174926758\n",
      "Step: 2042, Loss: 0.9159440398216248, Accuracy: 1.0, Computation time: 1.9579980373382568\n",
      "Step: 2043, Loss: 0.9375205039978027, Accuracy: 0.96875, Computation time: 2.0073952674865723\n",
      "Step: 2044, Loss: 0.9159947633743286, Accuracy: 1.0, Computation time: 2.102853775024414\n",
      "Step: 2045, Loss: 0.938169538974762, Accuracy: 0.96875, Computation time: 1.7718207836151123\n",
      "Step: 2046, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 1.9389569759368896\n",
      "Step: 2047, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.9476919174194336\n",
      "Step: 2048, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.9667389392852783\n",
      "Step: 2049, Loss: 0.9374051094055176, Accuracy: 0.96875, Computation time: 1.9495515823364258\n",
      "Step: 2050, Loss: 0.9573529362678528, Accuracy: 0.9375, Computation time: 2.2202367782592773\n",
      "Step: 2051, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.8934826850891113\n",
      "Step: 2052, Loss: 0.9159448146820068, Accuracy: 1.0, Computation time: 2.030677556991577\n",
      "Step: 2053, Loss: 0.916033148765564, Accuracy: 1.0, Computation time: 2.0967042446136475\n",
      "Step: 2054, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 2.9409031867980957\n",
      "Step: 2055, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 2.2957847118377686\n",
      "Step: 2056, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 2.208000659942627\n",
      "Step: 2057, Loss: 0.9165196418762207, Accuracy: 1.0, Computation time: 2.275130033493042\n",
      "Step: 2058, Loss: 0.9167453050613403, Accuracy: 1.0, Computation time: 2.587372303009033\n",
      "Step: 2059, Loss: 0.9159862399101257, Accuracy: 1.0, Computation time: 2.3011109828948975\n",
      "Step: 2060, Loss: 0.9198553562164307, Accuracy: 1.0, Computation time: 2.192859411239624\n",
      "Step: 2061, Loss: 0.916692316532135, Accuracy: 1.0, Computation time: 2.2785251140594482\n",
      "Step: 2062, Loss: 0.9171901345252991, Accuracy: 1.0, Computation time: 2.2416584491729736\n",
      "Step: 2063, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.7099194526672363\n",
      "Step: 2064, Loss: 0.9160212278366089, Accuracy: 1.0, Computation time: 1.9970006942749023\n",
      "Step: 2065, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 2.137385368347168\n",
      "Step: 2066, Loss: 0.9160316586494446, Accuracy: 1.0, Computation time: 1.9578378200531006\n",
      "Step: 2067, Loss: 0.9159680604934692, Accuracy: 1.0, Computation time: 1.7874505519866943\n",
      "Step: 2068, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 2.0685131549835205\n",
      "Step: 2069, Loss: 0.9162346720695496, Accuracy: 1.0, Computation time: 2.5268049240112305\n",
      "Step: 2070, Loss: 0.9427118301391602, Accuracy: 0.96875, Computation time: 2.2092859745025635\n",
      "Step: 2071, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 2.507554769515991\n",
      "Step: 2072, Loss: 0.9159888029098511, Accuracy: 1.0, Computation time: 1.7374825477600098\n",
      "Step: 2073, Loss: 0.9160054922103882, Accuracy: 1.0, Computation time: 1.924910068511963\n",
      "Step: 2074, Loss: 0.9160293340682983, Accuracy: 1.0, Computation time: 1.9374542236328125\n",
      "Step: 2075, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.801891803741455\n",
      "Step: 2076, Loss: 0.9159696698188782, Accuracy: 1.0, Computation time: 1.5847153663635254\n",
      "Step: 2077, Loss: 0.9307070970535278, Accuracy: 0.96875, Computation time: 2.8695099353790283\n",
      "Step: 2078, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 1.8788244724273682\n",
      "Step: 2079, Loss: 0.9160107970237732, Accuracy: 1.0, Computation time: 1.8290448188781738\n",
      "Step: 2080, Loss: 0.9399598836898804, Accuracy: 0.96875, Computation time: 2.318892240524292\n",
      "Step: 2081, Loss: 0.922150731086731, Accuracy: 1.0, Computation time: 1.7123463153839111\n",
      "Step: 2082, Loss: 0.9321480989456177, Accuracy: 0.96875, Computation time: 1.9345691204071045\n",
      "Step: 2083, Loss: 0.9160163998603821, Accuracy: 1.0, Computation time: 1.4926276206970215\n",
      "Step: 2084, Loss: 0.9160780310630798, Accuracy: 1.0, Computation time: 2.002995491027832\n",
      "Step: 2085, Loss: 0.9159862995147705, Accuracy: 1.0, Computation time: 1.866356372833252\n",
      "########################\n",
      "Test loss: 1.1268230676651, Test Accuracy_epoch15: 0.6929895281791687\n",
      "########################\n",
      "Step: 2086, Loss: 0.9160481095314026, Accuracy: 1.0, Computation time: 1.8914971351623535\n",
      "Step: 2087, Loss: 0.9159715175628662, Accuracy: 1.0, Computation time: 1.8378562927246094\n",
      "Step: 2088, Loss: 0.937076210975647, Accuracy: 0.96875, Computation time: 2.5257859230041504\n",
      "Step: 2089, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.7078862190246582\n",
      "Step: 2090, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 2.0499677658081055\n",
      "Step: 2091, Loss: 0.9163902997970581, Accuracy: 1.0, Computation time: 1.534867525100708\n",
      "Step: 2092, Loss: 0.937406063079834, Accuracy: 0.96875, Computation time: 1.9520678520202637\n",
      "Step: 2093, Loss: 0.9358760118484497, Accuracy: 0.96875, Computation time: 2.073300361633301\n",
      "Step: 2094, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 1.5602452754974365\n",
      "Step: 2095, Loss: 0.9244920015335083, Accuracy: 1.0, Computation time: 1.9129979610443115\n",
      "Step: 2096, Loss: 0.9375725388526917, Accuracy: 0.96875, Computation time: 2.2086310386657715\n",
      "Step: 2097, Loss: 0.9256592988967896, Accuracy: 0.96875, Computation time: 1.9203860759735107\n",
      "Step: 2098, Loss: 0.937517523765564, Accuracy: 0.96875, Computation time: 1.9825358390808105\n",
      "Step: 2099, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 1.515303611755371\n",
      "Step: 2100, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 1.5521292686462402\n",
      "Step: 2101, Loss: 0.9162867665290833, Accuracy: 1.0, Computation time: 1.6248016357421875\n",
      "Step: 2102, Loss: 0.9160609841346741, Accuracy: 1.0, Computation time: 1.3505675792694092\n",
      "Step: 2103, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.975219964981079\n",
      "Step: 2104, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 2.024538993835449\n",
      "Step: 2105, Loss: 0.9165130257606506, Accuracy: 1.0, Computation time: 1.9137043952941895\n",
      "Step: 2106, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.8382315635681152\n",
      "Step: 2107, Loss: 0.958741307258606, Accuracy: 0.9375, Computation time: 1.5103302001953125\n",
      "Step: 2108, Loss: 0.9160910248756409, Accuracy: 1.0, Computation time: 1.6935226917266846\n",
      "Step: 2109, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 2.0354506969451904\n",
      "Step: 2110, Loss: 0.9348471164703369, Accuracy: 0.96875, Computation time: 1.711944341659546\n",
      "Step: 2111, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.665060043334961\n",
      "Step: 2112, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 1.699838399887085\n",
      "Step: 2113, Loss: 0.915961742401123, Accuracy: 1.0, Computation time: 1.533311128616333\n",
      "Step: 2114, Loss: 0.9159968495368958, Accuracy: 1.0, Computation time: 1.690812587738037\n",
      "Step: 2115, Loss: 0.937484622001648, Accuracy: 0.96875, Computation time: 1.659907341003418\n",
      "Step: 2116, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.5459580421447754\n",
      "Step: 2117, Loss: 0.9375672936439514, Accuracy: 0.96875, Computation time: 1.663379430770874\n",
      "Step: 2118, Loss: 0.9164212346076965, Accuracy: 1.0, Computation time: 1.5181374549865723\n",
      "Step: 2119, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 1.6990737915039062\n",
      "Step: 2120, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.902529239654541\n",
      "Step: 2121, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.6974847316741943\n",
      "Step: 2122, Loss: 0.9796189069747925, Accuracy: 0.90625, Computation time: 1.9064853191375732\n",
      "Step: 2123, Loss: 0.9193121790885925, Accuracy: 1.0, Computation time: 3.1915321350097656\n",
      "Step: 2124, Loss: 0.9165810942649841, Accuracy: 1.0, Computation time: 1.6607213020324707\n",
      "Step: 2125, Loss: 0.9158894419670105, Accuracy: 1.0, Computation time: 2.000990867614746\n",
      "Step: 2126, Loss: 0.9159570336341858, Accuracy: 1.0, Computation time: 1.9561631679534912\n",
      "Step: 2127, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 2.0987071990966797\n",
      "Step: 2128, Loss: 0.915972113609314, Accuracy: 1.0, Computation time: 1.9131464958190918\n",
      "Step: 2129, Loss: 0.957614541053772, Accuracy: 0.9375, Computation time: 1.989147663116455\n",
      "Step: 2130, Loss: 0.9160391688346863, Accuracy: 1.0, Computation time: 1.844670057296753\n",
      "Step: 2131, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.7453570365905762\n",
      "Step: 2132, Loss: 0.9163992404937744, Accuracy: 1.0, Computation time: 1.7378051280975342\n",
      "Step: 2133, Loss: 0.9373956322669983, Accuracy: 0.96875, Computation time: 1.986422061920166\n",
      "Step: 2134, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.6488821506500244\n",
      "Step: 2135, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.5874955654144287\n",
      "Step: 2136, Loss: 0.9160774946212769, Accuracy: 1.0, Computation time: 1.3826556205749512\n",
      "Step: 2137, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.8010625839233398\n",
      "Step: 2138, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.7795729637145996\n",
      "Step: 2139, Loss: 0.9165747165679932, Accuracy: 1.0, Computation time: 1.7824511528015137\n",
      "Step: 2140, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.4376609325408936\n",
      "Step: 2141, Loss: 0.9349444508552551, Accuracy: 0.96875, Computation time: 2.618589401245117\n",
      "Step: 2142, Loss: 0.9159280061721802, Accuracy: 1.0, Computation time: 2.0477471351623535\n",
      "Step: 2143, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 1.8664336204528809\n",
      "Step: 2144, Loss: 0.932124137878418, Accuracy: 0.96875, Computation time: 1.5778400897979736\n",
      "Step: 2145, Loss: 0.9163761138916016, Accuracy: 1.0, Computation time: 2.0629186630249023\n",
      "Step: 2146, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 2.1536834239959717\n",
      "Step: 2147, Loss: 0.9186596870422363, Accuracy: 1.0, Computation time: 1.8122375011444092\n",
      "Step: 2148, Loss: 0.9159759879112244, Accuracy: 1.0, Computation time: 2.148282051086426\n",
      "Step: 2149, Loss: 0.9159124493598938, Accuracy: 1.0, Computation time: 1.4198341369628906\n",
      "Step: 2150, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.8829646110534668\n",
      "Step: 2151, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.5621006488800049\n",
      "Step: 2152, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 1.6507725715637207\n",
      "Step: 2153, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.6430106163024902\n",
      "Step: 2154, Loss: 0.9377521276473999, Accuracy: 0.96875, Computation time: 1.5738177299499512\n",
      "Step: 2155, Loss: 0.9159948229789734, Accuracy: 1.0, Computation time: 1.6754112243652344\n",
      "Step: 2156, Loss: 0.9361395835876465, Accuracy: 0.96875, Computation time: 1.8756215572357178\n",
      "Step: 2157, Loss: 0.9338893890380859, Accuracy: 0.96875, Computation time: 2.3687353134155273\n",
      "Step: 2158, Loss: 0.9159291982650757, Accuracy: 1.0, Computation time: 1.7728328704833984\n",
      "Step: 2159, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.3299145698547363\n",
      "Step: 2160, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.507460117340088\n",
      "Step: 2161, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 1.5991408824920654\n",
      "Step: 2162, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.652867317199707\n",
      "Step: 2163, Loss: 0.9188368916511536, Accuracy: 1.0, Computation time: 1.6727688312530518\n",
      "Step: 2164, Loss: 0.916837215423584, Accuracy: 1.0, Computation time: 1.5073206424713135\n",
      "Step: 2165, Loss: 0.9378157258033752, Accuracy: 0.96875, Computation time: 2.2258548736572266\n",
      "Step: 2166, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.7656641006469727\n",
      "Step: 2167, Loss: 0.9159870147705078, Accuracy: 1.0, Computation time: 1.8253467082977295\n",
      "Step: 2168, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.549957513809204\n",
      "Step: 2169, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.9207613468170166\n",
      "Step: 2170, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.7043766975402832\n",
      "Step: 2171, Loss: 0.9159113764762878, Accuracy: 1.0, Computation time: 1.4826748371124268\n",
      "Step: 2172, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 1.63474702835083\n",
      "Step: 2173, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.522345781326294\n",
      "Step: 2174, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.7400193214416504\n",
      "Step: 2175, Loss: 0.9161847829818726, Accuracy: 1.0, Computation time: 1.4903461933135986\n",
      "Step: 2176, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 1.821500301361084\n",
      "Step: 2177, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.7139537334442139\n",
      "Step: 2178, Loss: 0.9159965515136719, Accuracy: 1.0, Computation time: 2.244619131088257\n",
      "Step: 2179, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.7725610733032227\n",
      "Step: 2180, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 1.6497747898101807\n",
      "Step: 2181, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.5847859382629395\n",
      "Step: 2182, Loss: 0.9159556031227112, Accuracy: 1.0, Computation time: 1.6544926166534424\n",
      "Step: 2183, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.6740221977233887\n",
      "Step: 2184, Loss: 0.93741375207901, Accuracy: 0.96875, Computation time: 1.6726667881011963\n",
      "Step: 2185, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.5692932605743408\n",
      "Step: 2186, Loss: 0.9158987402915955, Accuracy: 1.0, Computation time: 1.9272117614746094\n",
      "Step: 2187, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 1.97230863571167\n",
      "Step: 2188, Loss: 0.9269643425941467, Accuracy: 0.96875, Computation time: 1.7482974529266357\n",
      "Step: 2189, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.9338717460632324\n",
      "Step: 2190, Loss: 0.9372173547744751, Accuracy: 0.96875, Computation time: 2.0984439849853516\n",
      "Step: 2191, Loss: 0.9373881220817566, Accuracy: 0.96875, Computation time: 1.64548921585083\n",
      "Step: 2192, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.4595947265625\n",
      "Step: 2193, Loss: 0.9572473764419556, Accuracy: 0.9375, Computation time: 5.308351039886475\n",
      "Step: 2194, Loss: 0.9374479651451111, Accuracy: 0.96875, Computation time: 1.970020055770874\n",
      "Step: 2195, Loss: 0.9159491062164307, Accuracy: 1.0, Computation time: 1.7477123737335205\n",
      "Step: 2196, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.752856969833374\n",
      "Step: 2197, Loss: 0.9176709055900574, Accuracy: 1.0, Computation time: 1.6460649967193604\n",
      "Step: 2198, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.8349504470825195\n",
      "Step: 2199, Loss: 0.9375426769256592, Accuracy: 0.96875, Computation time: 1.8636267185211182\n",
      "Step: 2200, Loss: 0.9160236120223999, Accuracy: 1.0, Computation time: 1.4811484813690186\n",
      "Step: 2201, Loss: 0.9257796406745911, Accuracy: 1.0, Computation time: 1.6629078388214111\n",
      "Step: 2202, Loss: 0.9159777760505676, Accuracy: 1.0, Computation time: 1.6249661445617676\n",
      "Step: 2203, Loss: 0.929391086101532, Accuracy: 0.96875, Computation time: 1.704479694366455\n",
      "Step: 2204, Loss: 0.9376668334007263, Accuracy: 0.96875, Computation time: 1.511504888534546\n",
      "Step: 2205, Loss: 0.9159778952598572, Accuracy: 1.0, Computation time: 1.4214003086090088\n",
      "Step: 2206, Loss: 0.9375144243240356, Accuracy: 0.96875, Computation time: 1.5991666316986084\n",
      "Step: 2207, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.621523141860962\n",
      "Step: 2208, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 1.8214490413665771\n",
      "Step: 2209, Loss: 0.9160213470458984, Accuracy: 1.0, Computation time: 1.6238980293273926\n",
      "Step: 2210, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 1.4959661960601807\n",
      "Step: 2211, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.526048183441162\n",
      "Step: 2212, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 2.0101630687713623\n",
      "Step: 2213, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 1.771345853805542\n",
      "Step: 2214, Loss: 0.9174907803535461, Accuracy: 1.0, Computation time: 1.5095374584197998\n",
      "Step: 2215, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 1.6098427772521973\n",
      "Step: 2216, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 2.0467288494110107\n",
      "Step: 2217, Loss: 0.9159061312675476, Accuracy: 1.0, Computation time: 1.841308355331421\n",
      "Step: 2218, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.6282732486724854\n",
      "Step: 2219, Loss: 0.9421689510345459, Accuracy: 0.96875, Computation time: 1.7262182235717773\n",
      "Step: 2220, Loss: 0.9166833758354187, Accuracy: 1.0, Computation time: 1.743595838546753\n",
      "Step: 2221, Loss: 0.916043758392334, Accuracy: 1.0, Computation time: 1.7450230121612549\n",
      "Step: 2222, Loss: 0.9163980484008789, Accuracy: 1.0, Computation time: 1.439103364944458\n",
      "Step: 2223, Loss: 0.9160444736480713, Accuracy: 1.0, Computation time: 1.4262700080871582\n",
      "########################\n",
      "Test loss: 1.1268237829208374, Test Accuracy_epoch16: 0.6889604926109314\n",
      "########################\n",
      "Step: 2224, Loss: 0.9159902334213257, Accuracy: 1.0, Computation time: 1.6275107860565186\n",
      "Step: 2225, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.491204023361206\n",
      "Step: 2226, Loss: 0.9329596757888794, Accuracy: 0.96875, Computation time: 1.48915433883667\n",
      "Step: 2227, Loss: 0.9233354330062866, Accuracy: 1.0, Computation time: 2.3130760192871094\n",
      "Step: 2228, Loss: 0.9374688267707825, Accuracy: 0.96875, Computation time: 1.5224037170410156\n",
      "Step: 2229, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 1.5883984565734863\n",
      "Step: 2230, Loss: 0.9200760722160339, Accuracy: 1.0, Computation time: 1.7515995502471924\n",
      "Step: 2231, Loss: 0.9247153997421265, Accuracy: 1.0, Computation time: 1.873805046081543\n",
      "Step: 2232, Loss: 0.9161416888237, Accuracy: 1.0, Computation time: 1.812591314315796\n",
      "Step: 2233, Loss: 0.9160251021385193, Accuracy: 1.0, Computation time: 1.7323884963989258\n",
      "Step: 2234, Loss: 0.9160581231117249, Accuracy: 1.0, Computation time: 1.5834619998931885\n",
      "Step: 2235, Loss: 0.9370067715644836, Accuracy: 0.96875, Computation time: 2.0235605239868164\n",
      "Step: 2236, Loss: 0.9162250757217407, Accuracy: 1.0, Computation time: 2.1281368732452393\n",
      "Step: 2237, Loss: 0.9584839344024658, Accuracy: 0.9375, Computation time: 1.423370599746704\n",
      "Step: 2238, Loss: 0.9159896969795227, Accuracy: 1.0, Computation time: 1.5786290168762207\n",
      "Step: 2239, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 1.6529567241668701\n",
      "Step: 2240, Loss: 0.9159526228904724, Accuracy: 1.0, Computation time: 1.5041382312774658\n",
      "Step: 2241, Loss: 0.915971040725708, Accuracy: 1.0, Computation time: 1.7132127285003662\n",
      "Step: 2242, Loss: 0.915991485118866, Accuracy: 1.0, Computation time: 1.7614655494689941\n",
      "Step: 2243, Loss: 0.9159674644470215, Accuracy: 1.0, Computation time: 1.5223190784454346\n",
      "Step: 2244, Loss: 0.9189602732658386, Accuracy: 1.0, Computation time: 2.086446762084961\n",
      "Step: 2245, Loss: 0.9378947019577026, Accuracy: 0.96875, Computation time: 1.6184091567993164\n",
      "Step: 2246, Loss: 0.9159124493598938, Accuracy: 1.0, Computation time: 1.727118730545044\n",
      "Step: 2247, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 1.9679391384124756\n",
      "Step: 2248, Loss: 0.9159530401229858, Accuracy: 1.0, Computation time: 1.4651720523834229\n",
      "Step: 2249, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.5301952362060547\n",
      "Step: 2250, Loss: 0.9159590005874634, Accuracy: 1.0, Computation time: 1.865154504776001\n",
      "Step: 2251, Loss: 0.9159773588180542, Accuracy: 1.0, Computation time: 1.814011812210083\n",
      "Step: 2252, Loss: 0.928821325302124, Accuracy: 0.96875, Computation time: 2.707292079925537\n",
      "Step: 2253, Loss: 0.9315307140350342, Accuracy: 0.96875, Computation time: 2.4958903789520264\n",
      "Step: 2254, Loss: 0.9159823060035706, Accuracy: 1.0, Computation time: 1.7520160675048828\n",
      "Step: 2255, Loss: 0.9162267446517944, Accuracy: 1.0, Computation time: 2.1987526416778564\n",
      "Step: 2256, Loss: 0.9161888957023621, Accuracy: 1.0, Computation time: 2.0622591972351074\n",
      "Step: 2257, Loss: 0.9199540019035339, Accuracy: 1.0, Computation time: 1.9518418312072754\n",
      "Step: 2258, Loss: 0.9163212776184082, Accuracy: 1.0, Computation time: 1.5038630962371826\n",
      "Step: 2259, Loss: 0.9211447834968567, Accuracy: 1.0, Computation time: 2.1079187393188477\n",
      "Step: 2260, Loss: 0.9161327481269836, Accuracy: 1.0, Computation time: 2.173084259033203\n",
      "Step: 2261, Loss: 0.91608726978302, Accuracy: 1.0, Computation time: 1.8844194412231445\n",
      "Step: 2262, Loss: 0.9166325926780701, Accuracy: 1.0, Computation time: 1.6277594566345215\n",
      "Step: 2263, Loss: 0.9162617921829224, Accuracy: 1.0, Computation time: 1.8468031883239746\n",
      "Step: 2264, Loss: 0.9165622591972351, Accuracy: 1.0, Computation time: 1.6042966842651367\n",
      "Step: 2265, Loss: 0.9165292978286743, Accuracy: 1.0, Computation time: 1.5482192039489746\n",
      "Step: 2266, Loss: 0.9376979470252991, Accuracy: 0.96875, Computation time: 1.5967509746551514\n",
      "Step: 2267, Loss: 0.9163327217102051, Accuracy: 1.0, Computation time: 1.9546468257904053\n",
      "Step: 2268, Loss: 0.9161693453788757, Accuracy: 1.0, Computation time: 1.7417950630187988\n",
      "Step: 2269, Loss: 0.9592881798744202, Accuracy: 0.9375, Computation time: 1.8128702640533447\n",
      "Step: 2270, Loss: 0.9159458875656128, Accuracy: 1.0, Computation time: 1.7935991287231445\n",
      "Step: 2271, Loss: 0.916619062423706, Accuracy: 1.0, Computation time: 1.5627844333648682\n",
      "Step: 2272, Loss: 0.9596546292304993, Accuracy: 0.9375, Computation time: 1.6319584846496582\n",
      "Step: 2273, Loss: 0.91600102186203, Accuracy: 1.0, Computation time: 1.5285799503326416\n",
      "Step: 2274, Loss: 0.9160279631614685, Accuracy: 1.0, Computation time: 1.9777991771697998\n",
      "Step: 2275, Loss: 0.9159426689147949, Accuracy: 1.0, Computation time: 1.6647613048553467\n",
      "Step: 2276, Loss: 0.951660692691803, Accuracy: 0.9375, Computation time: 2.7229270935058594\n",
      "Step: 2277, Loss: 0.9161285758018494, Accuracy: 1.0, Computation time: 1.7539167404174805\n",
      "Step: 2278, Loss: 0.9280949831008911, Accuracy: 0.96875, Computation time: 1.9801414012908936\n",
      "Step: 2279, Loss: 0.9162277579307556, Accuracy: 1.0, Computation time: 1.681694507598877\n",
      "Step: 2280, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 1.456191062927246\n",
      "Step: 2281, Loss: 0.9159672856330872, Accuracy: 1.0, Computation time: 1.4720008373260498\n",
      "Step: 2282, Loss: 0.9226321578025818, Accuracy: 1.0, Computation time: 1.6740424633026123\n",
      "Step: 2283, Loss: 0.93482905626297, Accuracy: 0.96875, Computation time: 2.0874240398406982\n",
      "Step: 2284, Loss: 0.9159833192825317, Accuracy: 1.0, Computation time: 1.5736441612243652\n",
      "Step: 2285, Loss: 0.9160615801811218, Accuracy: 1.0, Computation time: 1.7989511489868164\n",
      "Step: 2286, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.5356342792510986\n",
      "Step: 2287, Loss: 0.9161015748977661, Accuracy: 1.0, Computation time: 1.4634065628051758\n",
      "Step: 2288, Loss: 0.9471694231033325, Accuracy: 0.9375, Computation time: 1.683089256286621\n",
      "Step: 2289, Loss: 0.916027307510376, Accuracy: 1.0, Computation time: 1.5514633655548096\n",
      "Step: 2290, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 1.3815922737121582\n",
      "Step: 2291, Loss: 0.9177679419517517, Accuracy: 1.0, Computation time: 1.7266716957092285\n",
      "Step: 2292, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 1.632326602935791\n",
      "Step: 2293, Loss: 0.9159600138664246, Accuracy: 1.0, Computation time: 1.8982667922973633\n",
      "Step: 2294, Loss: 0.9159536361694336, Accuracy: 1.0, Computation time: 2.0399186611175537\n",
      "Step: 2295, Loss: 0.9267341494560242, Accuracy: 0.96875, Computation time: 3.0683445930480957\n",
      "Step: 2296, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.6463959217071533\n",
      "Step: 2297, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.4484198093414307\n",
      "Step: 2298, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.511453628540039\n",
      "Step: 2299, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 1.6747210025787354\n",
      "Step: 2300, Loss: 0.9160510301589966, Accuracy: 1.0, Computation time: 1.3067002296447754\n",
      "Step: 2301, Loss: 0.9159025549888611, Accuracy: 1.0, Computation time: 1.4180574417114258\n",
      "Step: 2302, Loss: 0.9169809818267822, Accuracy: 1.0, Computation time: 1.5850903987884521\n",
      "Step: 2303, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 1.390409231185913\n",
      "Step: 2304, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.404524564743042\n",
      "Step: 2305, Loss: 0.9160863757133484, Accuracy: 1.0, Computation time: 1.908557415008545\n",
      "Step: 2306, Loss: 0.9334837794303894, Accuracy: 0.96875, Computation time: 2.5234646797180176\n",
      "Step: 2307, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.709477186203003\n",
      "Step: 2308, Loss: 0.9159949421882629, Accuracy: 1.0, Computation time: 2.6761515140533447\n",
      "Step: 2309, Loss: 0.9352366328239441, Accuracy: 0.96875, Computation time: 4.084925889968872\n",
      "Step: 2310, Loss: 0.937829315662384, Accuracy: 0.96875, Computation time: 1.7090733051300049\n",
      "Step: 2311, Loss: 0.9159524440765381, Accuracy: 1.0, Computation time: 2.024789333343506\n",
      "Step: 2312, Loss: 0.9160734415054321, Accuracy: 1.0, Computation time: 1.5226213932037354\n",
      "Step: 2313, Loss: 0.91681969165802, Accuracy: 1.0, Computation time: 2.4040584564208984\n",
      "Step: 2314, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 1.5388548374176025\n",
      "Step: 2315, Loss: 0.91611248254776, Accuracy: 1.0, Computation time: 2.372544527053833\n",
      "Step: 2316, Loss: 0.9161748290061951, Accuracy: 1.0, Computation time: 1.8889923095703125\n",
      "Step: 2317, Loss: 0.9396386742591858, Accuracy: 0.96875, Computation time: 1.943039894104004\n",
      "Step: 2318, Loss: 0.9160773158073425, Accuracy: 1.0, Computation time: 2.016942262649536\n",
      "Step: 2319, Loss: 0.9376799464225769, Accuracy: 0.96875, Computation time: 2.2350869178771973\n",
      "Step: 2320, Loss: 0.915955662727356, Accuracy: 1.0, Computation time: 1.4769728183746338\n",
      "Step: 2321, Loss: 0.9473972916603088, Accuracy: 0.9375, Computation time: 1.8660273551940918\n",
      "Step: 2322, Loss: 0.9322490096092224, Accuracy: 0.96875, Computation time: 1.9557008743286133\n",
      "Step: 2323, Loss: 0.9373446702957153, Accuracy: 0.96875, Computation time: 1.688424825668335\n",
      "Step: 2324, Loss: 0.9356701970100403, Accuracy: 0.96875, Computation time: 2.0660219192504883\n",
      "Step: 2325, Loss: 0.9160486459732056, Accuracy: 1.0, Computation time: 1.5228190422058105\n",
      "Step: 2326, Loss: 0.9159694910049438, Accuracy: 1.0, Computation time: 1.7407374382019043\n",
      "Step: 2327, Loss: 0.9159500598907471, Accuracy: 1.0, Computation time: 1.4610233306884766\n",
      "Step: 2328, Loss: 0.9288724660873413, Accuracy: 0.96875, Computation time: 2.22629451751709\n",
      "Step: 2329, Loss: 0.9159349203109741, Accuracy: 1.0, Computation time: 1.6788883209228516\n",
      "Step: 2330, Loss: 0.9160266518592834, Accuracy: 1.0, Computation time: 1.4277935028076172\n",
      "Step: 2331, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.5054965019226074\n",
      "Step: 2332, Loss: 0.916053056716919, Accuracy: 1.0, Computation time: 1.8577361106872559\n",
      "Step: 2333, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.5889248847961426\n",
      "Step: 2334, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.5627093315124512\n",
      "Step: 2335, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.552095890045166\n",
      "Step: 2336, Loss: 0.9159997701644897, Accuracy: 1.0, Computation time: 1.9878876209259033\n",
      "Step: 2337, Loss: 0.934077262878418, Accuracy: 0.96875, Computation time: 2.459662437438965\n",
      "Step: 2338, Loss: 0.9381399154663086, Accuracy: 0.96875, Computation time: 2.370781898498535\n",
      "Step: 2339, Loss: 0.9340037703514099, Accuracy: 0.96875, Computation time: 3.2837820053100586\n",
      "Step: 2340, Loss: 0.9205037951469421, Accuracy: 1.0, Computation time: 2.3133082389831543\n",
      "Step: 2341, Loss: 0.9160359501838684, Accuracy: 1.0, Computation time: 1.6819710731506348\n",
      "Step: 2342, Loss: 0.9283982515335083, Accuracy: 0.96875, Computation time: 2.1085689067840576\n",
      "Step: 2343, Loss: 0.916088879108429, Accuracy: 1.0, Computation time: 1.5674996376037598\n",
      "Step: 2344, Loss: 0.9162266254425049, Accuracy: 1.0, Computation time: 1.552034854888916\n",
      "Step: 2345, Loss: 0.9361234903335571, Accuracy: 0.96875, Computation time: 1.545273780822754\n",
      "Step: 2346, Loss: 0.9161641001701355, Accuracy: 1.0, Computation time: 2.1180641651153564\n",
      "Step: 2347, Loss: 0.9159885048866272, Accuracy: 1.0, Computation time: 1.5762476921081543\n",
      "Step: 2348, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.7184185981750488\n",
      "Step: 2349, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.7689967155456543\n",
      "Step: 2350, Loss: 0.9165970087051392, Accuracy: 1.0, Computation time: 1.7192518711090088\n",
      "Step: 2351, Loss: 0.9201000332832336, Accuracy: 1.0, Computation time: 2.974336624145508\n",
      "Step: 2352, Loss: 0.93723464012146, Accuracy: 0.96875, Computation time: 1.3080158233642578\n",
      "Step: 2353, Loss: 0.9161731600761414, Accuracy: 1.0, Computation time: 1.5213184356689453\n",
      "Step: 2354, Loss: 0.9165956974029541, Accuracy: 1.0, Computation time: 1.7130613327026367\n",
      "Step: 2355, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 1.4652824401855469\n",
      "Step: 2356, Loss: 0.9176461696624756, Accuracy: 1.0, Computation time: 1.5013961791992188\n",
      "Step: 2357, Loss: 0.9565784931182861, Accuracy: 0.9375, Computation time: 1.281886100769043\n",
      "Step: 2358, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 1.2758231163024902\n",
      "Step: 2359, Loss: 0.9335556626319885, Accuracy: 0.96875, Computation time: 1.3445446491241455\n",
      "Step: 2360, Loss: 0.9159290194511414, Accuracy: 1.0, Computation time: 1.3740062713623047\n",
      "Step: 2361, Loss: 0.9162843227386475, Accuracy: 1.0, Computation time: 1.314851999282837\n",
      "Step: 2362, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.2290451526641846\n",
      "########################\n",
      "Test loss: 1.1217668056488037, Test Accuracy_epoch17: 0.6994359493255615\n",
      "########################\n",
      "Step: 2363, Loss: 0.9375590085983276, Accuracy: 0.96875, Computation time: 1.0498361587524414\n",
      "Step: 2364, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.2254652976989746\n",
      "Step: 2365, Loss: 0.9165811538696289, Accuracy: 1.0, Computation time: 1.4547357559204102\n",
      "Step: 2366, Loss: 0.9159473776817322, Accuracy: 1.0, Computation time: 1.431145429611206\n",
      "Step: 2367, Loss: 0.9378416538238525, Accuracy: 0.96875, Computation time: 1.5189776420593262\n",
      "Step: 2368, Loss: 0.9182175397872925, Accuracy: 1.0, Computation time: 1.1736795902252197\n",
      "Step: 2369, Loss: 0.9159685969352722, Accuracy: 1.0, Computation time: 1.2523987293243408\n",
      "Step: 2370, Loss: 0.9159056544303894, Accuracy: 1.0, Computation time: 1.5485281944274902\n",
      "Step: 2371, Loss: 0.9484063386917114, Accuracy: 0.9375, Computation time: 1.4748427867889404\n",
      "Step: 2372, Loss: 0.9159783720970154, Accuracy: 1.0, Computation time: 1.2892796993255615\n",
      "Step: 2373, Loss: 0.91666179895401, Accuracy: 1.0, Computation time: 1.3006923198699951\n",
      "Step: 2374, Loss: 0.9162783622741699, Accuracy: 1.0, Computation time: 1.2236158847808838\n",
      "Step: 2375, Loss: 0.9377068281173706, Accuracy: 0.96875, Computation time: 1.0994312763214111\n",
      "Step: 2376, Loss: 0.9173688292503357, Accuracy: 1.0, Computation time: 1.374882459640503\n",
      "Step: 2377, Loss: 0.9160301089286804, Accuracy: 1.0, Computation time: 1.6218163967132568\n",
      "Step: 2378, Loss: 0.9371136426925659, Accuracy: 0.96875, Computation time: 1.4060609340667725\n",
      "Step: 2379, Loss: 0.9335819482803345, Accuracy: 0.96875, Computation time: 1.6669666767120361\n",
      "Step: 2380, Loss: 0.9161765575408936, Accuracy: 1.0, Computation time: 1.3311636447906494\n",
      "Step: 2381, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 1.3774793148040771\n",
      "Step: 2382, Loss: 0.9169697761535645, Accuracy: 1.0, Computation time: 1.6797840595245361\n",
      "Step: 2383, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 1.6654016971588135\n",
      "Step: 2384, Loss: 0.9160533547401428, Accuracy: 1.0, Computation time: 1.3790879249572754\n",
      "Step: 2385, Loss: 0.9591624140739441, Accuracy: 0.9375, Computation time: 1.4185643196105957\n",
      "Step: 2386, Loss: 0.9165163040161133, Accuracy: 1.0, Computation time: 1.6076905727386475\n",
      "Step: 2387, Loss: 0.916044294834137, Accuracy: 1.0, Computation time: 1.6968867778778076\n",
      "Step: 2388, Loss: 0.9160679578781128, Accuracy: 1.0, Computation time: 1.8666882514953613\n",
      "Step: 2389, Loss: 0.9164629578590393, Accuracy: 1.0, Computation time: 1.7142293453216553\n",
      "Step: 2390, Loss: 0.9160404801368713, Accuracy: 1.0, Computation time: 2.1541268825531006\n",
      "Step: 2391, Loss: 0.9161874651908875, Accuracy: 1.0, Computation time: 1.7589411735534668\n",
      "Step: 2392, Loss: 0.9375923275947571, Accuracy: 0.96875, Computation time: 1.8419854640960693\n",
      "Step: 2393, Loss: 0.9159507155418396, Accuracy: 1.0, Computation time: 2.456125497817993\n",
      "Step: 2394, Loss: 0.9159964919090271, Accuracy: 1.0, Computation time: 1.8254356384277344\n",
      "Step: 2395, Loss: 0.9373199343681335, Accuracy: 0.96875, Computation time: 2.3185620307922363\n",
      "Step: 2396, Loss: 0.9159528613090515, Accuracy: 1.0, Computation time: 1.6533401012420654\n",
      "Step: 2397, Loss: 0.9367808699607849, Accuracy: 0.96875, Computation time: 1.7645809650421143\n",
      "Step: 2398, Loss: 0.9160550832748413, Accuracy: 1.0, Computation time: 1.8768057823181152\n",
      "Step: 2399, Loss: 0.9272153377532959, Accuracy: 0.96875, Computation time: 2.1931045055389404\n",
      "Step: 2400, Loss: 0.9162187576293945, Accuracy: 1.0, Computation time: 2.567964792251587\n",
      "Step: 2401, Loss: 0.9160317778587341, Accuracy: 1.0, Computation time: 1.9701812267303467\n",
      "Step: 2402, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.8425180912017822\n",
      "Step: 2403, Loss: 0.9165471792221069, Accuracy: 1.0, Computation time: 2.0376346111297607\n",
      "Step: 2404, Loss: 0.9159517884254456, Accuracy: 1.0, Computation time: 1.7474677562713623\n",
      "Step: 2405, Loss: 0.9199026823043823, Accuracy: 1.0, Computation time: 1.8339877128601074\n",
      "Step: 2406, Loss: 0.9218115210533142, Accuracy: 1.0, Computation time: 1.9932951927185059\n",
      "Step: 2407, Loss: 0.9165918231010437, Accuracy: 1.0, Computation time: 1.7329206466674805\n",
      "Step: 2408, Loss: 0.9160895943641663, Accuracy: 1.0, Computation time: 1.781559705734253\n",
      "Step: 2409, Loss: 0.9163974523544312, Accuracy: 1.0, Computation time: 2.333665370941162\n",
      "Step: 2410, Loss: 0.9160041213035583, Accuracy: 1.0, Computation time: 1.8702988624572754\n",
      "Step: 2411, Loss: 0.9159848093986511, Accuracy: 1.0, Computation time: 1.9063773155212402\n",
      "Step: 2412, Loss: 0.9161445498466492, Accuracy: 1.0, Computation time: 1.6145122051239014\n",
      "Step: 2413, Loss: 0.9159570932388306, Accuracy: 1.0, Computation time: 1.676546335220337\n",
      "Step: 2414, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 1.6682922840118408\n",
      "Step: 2415, Loss: 0.9167806506156921, Accuracy: 1.0, Computation time: 1.7541234493255615\n",
      "Step: 2416, Loss: 0.9372832775115967, Accuracy: 0.96875, Computation time: 1.9490303993225098\n",
      "Step: 2417, Loss: 0.9160678386688232, Accuracy: 1.0, Computation time: 1.6213724613189697\n",
      "Step: 2418, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.712899923324585\n",
      "Step: 2419, Loss: 0.9159331321716309, Accuracy: 1.0, Computation time: 1.4596879482269287\n",
      "Step: 2420, Loss: 0.9195576906204224, Accuracy: 1.0, Computation time: 1.8881077766418457\n",
      "Step: 2421, Loss: 0.9160192012786865, Accuracy: 1.0, Computation time: 1.7792117595672607\n",
      "Step: 2422, Loss: 0.937412440776825, Accuracy: 0.96875, Computation time: 1.8670196533203125\n",
      "Step: 2423, Loss: 0.9160232543945312, Accuracy: 1.0, Computation time: 1.6944773197174072\n",
      "Step: 2424, Loss: 0.9177682995796204, Accuracy: 1.0, Computation time: 2.501718282699585\n",
      "Step: 2425, Loss: 0.9159507155418396, Accuracy: 1.0, Computation time: 1.8407917022705078\n",
      "Step: 2426, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 1.318392038345337\n",
      "Step: 2427, Loss: 0.935575544834137, Accuracy: 0.96875, Computation time: 1.9373180866241455\n",
      "Step: 2428, Loss: 0.9159586429595947, Accuracy: 1.0, Computation time: 1.9022367000579834\n",
      "Step: 2429, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.4761974811553955\n",
      "Step: 2430, Loss: 0.918317437171936, Accuracy: 1.0, Computation time: 1.6488292217254639\n",
      "Step: 2431, Loss: 0.9161672592163086, Accuracy: 1.0, Computation time: 1.8824703693389893\n",
      "Step: 2432, Loss: 0.918775200843811, Accuracy: 1.0, Computation time: 2.178929567337036\n",
      "Step: 2433, Loss: 0.9313262104988098, Accuracy: 0.96875, Computation time: 1.9931271076202393\n",
      "Step: 2434, Loss: 0.9374776482582092, Accuracy: 0.96875, Computation time: 1.8059661388397217\n",
      "Step: 2435, Loss: 0.9159572124481201, Accuracy: 1.0, Computation time: 2.180023670196533\n",
      "Step: 2436, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.5537109375\n",
      "Step: 2437, Loss: 0.9302128553390503, Accuracy: 0.96875, Computation time: 1.7169365882873535\n",
      "Step: 2438, Loss: 0.9162254333496094, Accuracy: 1.0, Computation time: 2.0070860385894775\n",
      "Step: 2439, Loss: 0.9164113998413086, Accuracy: 1.0, Computation time: 1.9376797676086426\n",
      "Step: 2440, Loss: 0.9366351366043091, Accuracy: 0.96875, Computation time: 1.8413143157958984\n",
      "Step: 2441, Loss: 0.916030764579773, Accuracy: 1.0, Computation time: 1.5499629974365234\n",
      "Step: 2442, Loss: 0.915934145450592, Accuracy: 1.0, Computation time: 2.019658088684082\n",
      "Step: 2443, Loss: 0.9165495038032532, Accuracy: 1.0, Computation time: 1.936948299407959\n",
      "Step: 2444, Loss: 0.9182402491569519, Accuracy: 1.0, Computation time: 2.083601951599121\n",
      "Step: 2445, Loss: 0.9160366058349609, Accuracy: 1.0, Computation time: 1.5787451267242432\n",
      "Step: 2446, Loss: 0.9159090518951416, Accuracy: 1.0, Computation time: 1.893904685974121\n",
      "Step: 2447, Loss: 0.9369985461235046, Accuracy: 0.96875, Computation time: 2.0188100337982178\n",
      "Step: 2448, Loss: 0.9169449806213379, Accuracy: 1.0, Computation time: 1.924257755279541\n",
      "Step: 2449, Loss: 0.9300529956817627, Accuracy: 0.96875, Computation time: 1.700019121170044\n",
      "Step: 2450, Loss: 0.9181396961212158, Accuracy: 1.0, Computation time: 1.809647798538208\n",
      "Step: 2451, Loss: 0.9163833856582642, Accuracy: 1.0, Computation time: 1.4170656204223633\n",
      "Step: 2452, Loss: 0.9166623950004578, Accuracy: 1.0, Computation time: 1.7348790168762207\n",
      "Step: 2453, Loss: 0.9161917567253113, Accuracy: 1.0, Computation time: 1.7721190452575684\n",
      "Step: 2454, Loss: 0.9160805344581604, Accuracy: 1.0, Computation time: 1.348487138748169\n",
      "Step: 2455, Loss: 0.9161021113395691, Accuracy: 1.0, Computation time: 1.7030086517333984\n",
      "Step: 2456, Loss: 0.9166595935821533, Accuracy: 1.0, Computation time: 1.590989589691162\n",
      "Step: 2457, Loss: 0.9160104393959045, Accuracy: 1.0, Computation time: 1.9833214282989502\n",
      "Step: 2458, Loss: 0.9163602590560913, Accuracy: 1.0, Computation time: 1.8338112831115723\n",
      "Step: 2459, Loss: 0.9159739017486572, Accuracy: 1.0, Computation time: 1.4817190170288086\n",
      "Step: 2460, Loss: 0.9160209894180298, Accuracy: 1.0, Computation time: 1.7029061317443848\n",
      "Step: 2461, Loss: 0.9323268532752991, Accuracy: 0.96875, Computation time: 2.0052804946899414\n",
      "Step: 2462, Loss: 0.9162238240242004, Accuracy: 1.0, Computation time: 1.8400897979736328\n",
      "Step: 2463, Loss: 0.9159517884254456, Accuracy: 1.0, Computation time: 1.9892609119415283\n",
      "Step: 2464, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 1.6873178482055664\n",
      "Step: 2465, Loss: 0.9430229067802429, Accuracy: 0.96875, Computation time: 1.6264030933380127\n",
      "Step: 2466, Loss: 0.9176331162452698, Accuracy: 1.0, Computation time: 1.6793794631958008\n",
      "Step: 2467, Loss: 0.9376065135002136, Accuracy: 0.96875, Computation time: 1.4657244682312012\n",
      "Step: 2468, Loss: 0.916268527507782, Accuracy: 1.0, Computation time: 1.6500499248504639\n",
      "Step: 2469, Loss: 0.9375511407852173, Accuracy: 0.96875, Computation time: 1.4921517372131348\n",
      "Step: 2470, Loss: 0.9160742163658142, Accuracy: 1.0, Computation time: 1.3865501880645752\n",
      "Step: 2471, Loss: 0.916071355342865, Accuracy: 1.0, Computation time: 1.4876019954681396\n",
      "Step: 2472, Loss: 0.9159416556358337, Accuracy: 1.0, Computation time: 1.9416346549987793\n",
      "Step: 2473, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.7635276317596436\n",
      "Step: 2474, Loss: 0.9163147211074829, Accuracy: 1.0, Computation time: 2.152357578277588\n",
      "Step: 2475, Loss: 0.916010856628418, Accuracy: 1.0, Computation time: 1.984428882598877\n",
      "Step: 2476, Loss: 0.9575524926185608, Accuracy: 0.9375, Computation time: 2.2624995708465576\n",
      "Step: 2477, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 2.3299498558044434\n",
      "Step: 2478, Loss: 0.9160351753234863, Accuracy: 1.0, Computation time: 1.994858980178833\n",
      "Step: 2479, Loss: 0.9159415364265442, Accuracy: 1.0, Computation time: 2.316107749938965\n",
      "Step: 2480, Loss: 0.9160111546516418, Accuracy: 1.0, Computation time: 1.8771300315856934\n",
      "Step: 2481, Loss: 0.9159387946128845, Accuracy: 1.0, Computation time: 1.7217991352081299\n",
      "Step: 2482, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 1.7374444007873535\n",
      "Step: 2483, Loss: 0.9283028841018677, Accuracy: 0.96875, Computation time: 2.2196779251098633\n",
      "Step: 2484, Loss: 0.9159231185913086, Accuracy: 1.0, Computation time: 1.9832704067230225\n",
      "Step: 2485, Loss: 0.9373881220817566, Accuracy: 0.96875, Computation time: 2.063572406768799\n",
      "Step: 2486, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 2.4006199836730957\n",
      "Step: 2487, Loss: 0.9181980490684509, Accuracy: 1.0, Computation time: 2.0006635189056396\n",
      "Step: 2488, Loss: 0.9159809947013855, Accuracy: 1.0, Computation time: 2.059630870819092\n",
      "Step: 2489, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.66758131980896\n",
      "Step: 2490, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.7254033088684082\n",
      "Step: 2491, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.7204954624176025\n",
      "Step: 2492, Loss: 0.9159894585609436, Accuracy: 1.0, Computation time: 2.0581064224243164\n",
      "Step: 2493, Loss: 0.9374069571495056, Accuracy: 0.96875, Computation time: 1.5547540187835693\n",
      "Step: 2494, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.6403512954711914\n",
      "Step: 2495, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.6366333961486816\n",
      "Step: 2496, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 2.1185057163238525\n",
      "Step: 2497, Loss: 0.9377351999282837, Accuracy: 0.96875, Computation time: 1.9116551876068115\n",
      "Step: 2498, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 1.8440442085266113\n",
      "Step: 2499, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 2.0297834873199463\n",
      "Step: 2500, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.6257891654968262\n",
      "Step: 2501, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.7105278968811035\n",
      "########################\n",
      "Test loss: 1.1234170198440552, Test Accuracy_epoch18: 0.7010475397109985\n",
      "########################\n",
      "Step: 2502, Loss: 0.9178413152694702, Accuracy: 1.0, Computation time: 2.169951915740967\n",
      "Step: 2503, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 2.195448637008667\n",
      "Step: 2504, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.6523888111114502\n",
      "Step: 2505, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 2.0303702354431152\n",
      "Step: 2506, Loss: 0.9268788695335388, Accuracy: 0.96875, Computation time: 1.6750199794769287\n",
      "Step: 2507, Loss: 0.9431403875350952, Accuracy: 0.96875, Computation time: 1.796205759048462\n",
      "Step: 2508, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.8477489948272705\n",
      "Step: 2509, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.8969221115112305\n",
      "Step: 2510, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.7238719463348389\n",
      "Step: 2511, Loss: 0.9160303473472595, Accuracy: 1.0, Computation time: 1.771409273147583\n",
      "Step: 2512, Loss: 0.9160057902336121, Accuracy: 1.0, Computation time: 1.7120623588562012\n",
      "Step: 2513, Loss: 0.9376019835472107, Accuracy: 0.96875, Computation time: 1.67506742477417\n",
      "Step: 2514, Loss: 0.935877799987793, Accuracy: 0.96875, Computation time: 2.040755033493042\n",
      "Step: 2515, Loss: 0.9159793257713318, Accuracy: 1.0, Computation time: 1.8572773933410645\n",
      "Step: 2516, Loss: 0.9333092570304871, Accuracy: 0.96875, Computation time: 2.5201940536499023\n",
      "Step: 2517, Loss: 0.9159877300262451, Accuracy: 1.0, Computation time: 2.7132880687713623\n",
      "Step: 2518, Loss: 0.9160969257354736, Accuracy: 1.0, Computation time: 2.2103497982025146\n",
      "Step: 2519, Loss: 0.9159654378890991, Accuracy: 1.0, Computation time: 1.3858349323272705\n",
      "Step: 2520, Loss: 0.9159610867500305, Accuracy: 1.0, Computation time: 1.7797198295593262\n",
      "Step: 2521, Loss: 0.9377723932266235, Accuracy: 0.96875, Computation time: 2.0159268379211426\n",
      "Step: 2522, Loss: 0.916008472442627, Accuracy: 1.0, Computation time: 1.696211814880371\n",
      "Step: 2523, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 2.263065814971924\n",
      "Step: 2524, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 2.301327705383301\n",
      "Step: 2525, Loss: 0.9346672296524048, Accuracy: 0.96875, Computation time: 2.282350778579712\n",
      "Step: 2526, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 1.9732580184936523\n",
      "Step: 2527, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 2.0407660007476807\n",
      "Step: 2528, Loss: 0.91607266664505, Accuracy: 1.0, Computation time: 1.7032079696655273\n",
      "Step: 2529, Loss: 0.9359866976737976, Accuracy: 0.96875, Computation time: 2.9094936847686768\n",
      "Step: 2530, Loss: 0.9371274709701538, Accuracy: 0.96875, Computation time: 2.2467756271362305\n",
      "Step: 2531, Loss: 0.9166398048400879, Accuracy: 1.0, Computation time: 1.6512451171875\n",
      "Step: 2532, Loss: 0.937519371509552, Accuracy: 0.96875, Computation time: 1.7446022033691406\n",
      "Step: 2533, Loss: 0.9160009026527405, Accuracy: 1.0, Computation time: 1.7249634265899658\n",
      "Step: 2534, Loss: 0.9164760112762451, Accuracy: 1.0, Computation time: 1.8865139484405518\n",
      "Step: 2535, Loss: 0.9224557876586914, Accuracy: 1.0, Computation time: 2.670471668243408\n",
      "Step: 2536, Loss: 0.9159314036369324, Accuracy: 1.0, Computation time: 1.6022934913635254\n",
      "Step: 2537, Loss: 0.9159960746765137, Accuracy: 1.0, Computation time: 1.8142683506011963\n",
      "Step: 2538, Loss: 0.9160627126693726, Accuracy: 1.0, Computation time: 1.5460236072540283\n",
      "Step: 2539, Loss: 0.9161158204078674, Accuracy: 1.0, Computation time: 1.480442762374878\n",
      "Step: 2540, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 1.5050125122070312\n",
      "Step: 2541, Loss: 0.9373044371604919, Accuracy: 0.96875, Computation time: 1.67366623878479\n",
      "Step: 2542, Loss: 0.9376484751701355, Accuracy: 0.96875, Computation time: 1.7652602195739746\n",
      "Step: 2543, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 2.0684423446655273\n",
      "Step: 2544, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 1.5212063789367676\n",
      "Step: 2545, Loss: 0.9376822710037231, Accuracy: 0.96875, Computation time: 1.7177202701568604\n",
      "Step: 2546, Loss: 0.9162617921829224, Accuracy: 1.0, Computation time: 1.5926406383514404\n",
      "Step: 2547, Loss: 0.9160493016242981, Accuracy: 1.0, Computation time: 1.7772369384765625\n",
      "Step: 2548, Loss: 0.9375898241996765, Accuracy: 0.96875, Computation time: 1.565436601638794\n",
      "Step: 2549, Loss: 0.9197037220001221, Accuracy: 1.0, Computation time: 2.4164984226226807\n",
      "Step: 2550, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.762608289718628\n",
      "Step: 2551, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.5047883987426758\n",
      "Step: 2552, Loss: 0.9542051553726196, Accuracy: 0.9375, Computation time: 1.9191362857818604\n",
      "Step: 2553, Loss: 0.9375146627426147, Accuracy: 0.96875, Computation time: 1.4799561500549316\n",
      "Step: 2554, Loss: 0.9160091280937195, Accuracy: 1.0, Computation time: 1.4249706268310547\n",
      "Step: 2555, Loss: 0.9351134300231934, Accuracy: 0.96875, Computation time: 2.378030300140381\n",
      "Step: 2556, Loss: 0.9159659743309021, Accuracy: 1.0, Computation time: 1.7975711822509766\n",
      "Step: 2557, Loss: 0.9159274101257324, Accuracy: 1.0, Computation time: 1.7715861797332764\n",
      "Step: 2558, Loss: 0.916379988193512, Accuracy: 1.0, Computation time: 2.1285059452056885\n",
      "Step: 2559, Loss: 0.9375441670417786, Accuracy: 0.96875, Computation time: 1.613363265991211\n",
      "Step: 2560, Loss: 0.9161388278007507, Accuracy: 1.0, Computation time: 1.6328959465026855\n",
      "Step: 2561, Loss: 0.9186168313026428, Accuracy: 1.0, Computation time: 1.6893370151519775\n",
      "Step: 2562, Loss: 0.9159696698188782, Accuracy: 1.0, Computation time: 1.6460084915161133\n",
      "Step: 2563, Loss: 0.9160197377204895, Accuracy: 1.0, Computation time: 1.4845507144927979\n",
      "Step: 2564, Loss: 0.9160870313644409, Accuracy: 1.0, Computation time: 1.5065484046936035\n",
      "Step: 2565, Loss: 0.9160987138748169, Accuracy: 1.0, Computation time: 1.656937599182129\n",
      "Step: 2566, Loss: 0.9161320924758911, Accuracy: 1.0, Computation time: 1.465500831604004\n",
      "Step: 2567, Loss: 0.9374629855155945, Accuracy: 0.96875, Computation time: 2.0326828956604004\n",
      "Step: 2568, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.9144716262817383\n",
      "Step: 2569, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.5535335540771484\n",
      "Step: 2570, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.4160079956054688\n",
      "Step: 2571, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 1.4766693115234375\n",
      "Step: 2572, Loss: 0.9162005186080933, Accuracy: 1.0, Computation time: 1.6672234535217285\n",
      "Step: 2573, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.5773811340332031\n",
      "Step: 2574, Loss: 0.9159325957298279, Accuracy: 1.0, Computation time: 1.7224504947662354\n",
      "Step: 2575, Loss: 0.9160171747207642, Accuracy: 1.0, Computation time: 1.803107738494873\n",
      "Step: 2576, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.8909509181976318\n",
      "Step: 2577, Loss: 0.9230005741119385, Accuracy: 1.0, Computation time: 3.605710983276367\n",
      "Step: 2578, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.6409413814544678\n",
      "Step: 2579, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 1.5171246528625488\n",
      "Step: 2580, Loss: 0.9173086881637573, Accuracy: 1.0, Computation time: 1.6717090606689453\n",
      "Step: 2581, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.5836842060089111\n",
      "Step: 2582, Loss: 0.9160308241844177, Accuracy: 1.0, Computation time: 1.5888607501983643\n",
      "Step: 2583, Loss: 0.9160588383674622, Accuracy: 1.0, Computation time: 1.997342824935913\n",
      "Step: 2584, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 1.785640001296997\n",
      "Step: 2585, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.219334602355957\n",
      "Step: 2586, Loss: 0.9159776568412781, Accuracy: 1.0, Computation time: 1.908311128616333\n",
      "Step: 2587, Loss: 0.9163850545883179, Accuracy: 1.0, Computation time: 2.2117369174957275\n",
      "Step: 2588, Loss: 0.9314831495285034, Accuracy: 0.96875, Computation time: 2.616311550140381\n",
      "Step: 2589, Loss: 0.9376509189605713, Accuracy: 0.96875, Computation time: 1.6231458187103271\n",
      "Step: 2590, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.449561357498169\n",
      "Step: 2591, Loss: 0.9161075353622437, Accuracy: 1.0, Computation time: 1.510934829711914\n",
      "Step: 2592, Loss: 0.9217437505722046, Accuracy: 1.0, Computation time: 2.0148229598999023\n",
      "Step: 2593, Loss: 0.9161553978919983, Accuracy: 1.0, Computation time: 1.789008378982544\n",
      "Step: 2594, Loss: 0.9376090168952942, Accuracy: 0.96875, Computation time: 1.891000747680664\n",
      "Step: 2595, Loss: 0.922749936580658, Accuracy: 1.0, Computation time: 2.687136173248291\n",
      "Step: 2596, Loss: 0.9159892201423645, Accuracy: 1.0, Computation time: 2.0354156494140625\n",
      "Step: 2597, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 1.5206882953643799\n",
      "Step: 2598, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.3305234909057617\n",
      "Step: 2599, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 1.6337087154388428\n",
      "Step: 2600, Loss: 0.936612606048584, Accuracy: 0.96875, Computation time: 1.5444722175598145\n",
      "Step: 2601, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.567955732345581\n",
      "Step: 2602, Loss: 0.9159935116767883, Accuracy: 1.0, Computation time: 1.631850004196167\n",
      "Step: 2603, Loss: 0.9379466772079468, Accuracy: 0.96875, Computation time: 1.7987422943115234\n",
      "Step: 2604, Loss: 0.9161417484283447, Accuracy: 1.0, Computation time: 1.804802417755127\n",
      "Step: 2605, Loss: 0.9590827226638794, Accuracy: 0.9375, Computation time: 1.5102508068084717\n",
      "Step: 2606, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.583444356918335\n",
      "Step: 2607, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.963658094406128\n",
      "Step: 2608, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.5055315494537354\n",
      "Step: 2609, Loss: 0.917400062084198, Accuracy: 1.0, Computation time: 2.210334539413452\n",
      "Step: 2610, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.6158325672149658\n",
      "Step: 2611, Loss: 0.9159881472587585, Accuracy: 1.0, Computation time: 1.6978511810302734\n",
      "Step: 2612, Loss: 0.9809595346450806, Accuracy: 0.90625, Computation time: 1.560375452041626\n",
      "Step: 2613, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 1.3763089179992676\n",
      "Step: 2614, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.4893646240234375\n",
      "Step: 2615, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.7069685459136963\n",
      "Step: 2616, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.8023881912231445\n",
      "Step: 2617, Loss: 0.9159702062606812, Accuracy: 1.0, Computation time: 1.6614670753479004\n",
      "Step: 2618, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 1.4464495182037354\n",
      "Step: 2619, Loss: 0.9166246056556702, Accuracy: 1.0, Computation time: 1.7534847259521484\n",
      "Step: 2620, Loss: 0.9366459846496582, Accuracy: 0.96875, Computation time: 2.0505034923553467\n",
      "Step: 2621, Loss: 0.9374775290489197, Accuracy: 0.96875, Computation time: 1.9081830978393555\n",
      "Step: 2622, Loss: 0.9161350131034851, Accuracy: 1.0, Computation time: 1.6682472229003906\n",
      "Step: 2623, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 1.5686619281768799\n",
      "Step: 2624, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.6334359645843506\n",
      "Step: 2625, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 1.5697999000549316\n",
      "Step: 2626, Loss: 0.9381591081619263, Accuracy: 0.96875, Computation time: 1.777355670928955\n",
      "Step: 2627, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.8547728061676025\n",
      "Step: 2628, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.5402045249938965\n",
      "Step: 2629, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.5527937412261963\n",
      "Step: 2630, Loss: 0.956845760345459, Accuracy: 0.9375, Computation time: 1.7416579723358154\n",
      "Step: 2631, Loss: 0.9160430431365967, Accuracy: 1.0, Computation time: 1.998840570449829\n",
      "Step: 2632, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.8539972305297852\n",
      "Step: 2633, Loss: 0.9260149598121643, Accuracy: 0.96875, Computation time: 1.7751500606536865\n",
      "Step: 2634, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.9594507217407227\n",
      "Step: 2635, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.7905869483947754\n",
      "Step: 2636, Loss: 0.9305623173713684, Accuracy: 0.96875, Computation time: 1.9111826419830322\n",
      "Step: 2637, Loss: 0.9159083962440491, Accuracy: 1.0, Computation time: 2.0676262378692627\n",
      "Step: 2638, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 1.6849958896636963\n",
      "Step: 2639, Loss: 0.9159480333328247, Accuracy: 1.0, Computation time: 1.6562047004699707\n",
      "Step: 2640, Loss: 0.9159867763519287, Accuracy: 1.0, Computation time: 2.042994976043701\n",
      "########################\n",
      "Test loss: 1.1256788969039917, Test Accuracy_epoch19: 0.6921837329864502\n",
      "########################\n",
      "Step: 2641, Loss: 0.9159371256828308, Accuracy: 1.0, Computation time: 1.8713350296020508\n",
      "Step: 2642, Loss: 0.9215866923332214, Accuracy: 1.0, Computation time: 1.7517075538635254\n",
      "Step: 2643, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 1.7486932277679443\n",
      "Step: 2644, Loss: 0.9159567356109619, Accuracy: 1.0, Computation time: 1.6435935497283936\n",
      "Step: 2645, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.6703393459320068\n",
      "Step: 2646, Loss: 0.9372285604476929, Accuracy: 0.96875, Computation time: 1.7000553607940674\n",
      "Step: 2647, Loss: 0.9160506725311279, Accuracy: 1.0, Computation time: 1.7063536643981934\n",
      "Step: 2648, Loss: 0.9159897565841675, Accuracy: 1.0, Computation time: 1.5615978240966797\n",
      "Step: 2649, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.5676038265228271\n",
      "Step: 2650, Loss: 0.9305682182312012, Accuracy: 0.96875, Computation time: 2.228717088699341\n",
      "Step: 2651, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 1.4174995422363281\n",
      "Step: 2652, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 1.4333720207214355\n",
      "Step: 2653, Loss: 0.9159712791442871, Accuracy: 1.0, Computation time: 1.4432322978973389\n",
      "Step: 2654, Loss: 0.9159620404243469, Accuracy: 1.0, Computation time: 1.66807222366333\n",
      "Step: 2655, Loss: 0.9159412980079651, Accuracy: 1.0, Computation time: 1.7663376331329346\n",
      "Step: 2656, Loss: 0.9159408807754517, Accuracy: 1.0, Computation time: 1.5780816078186035\n",
      "Step: 2657, Loss: 0.9191814661026001, Accuracy: 1.0, Computation time: 1.3128716945648193\n",
      "Step: 2658, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.3714570999145508\n",
      "Step: 2659, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 1.5536811351776123\n",
      "Step: 2660, Loss: 0.918948233127594, Accuracy: 1.0, Computation time: 1.8770475387573242\n",
      "Step: 2661, Loss: 0.915938675403595, Accuracy: 1.0, Computation time: 1.7282378673553467\n",
      "Step: 2662, Loss: 0.9159739017486572, Accuracy: 1.0, Computation time: 1.5848743915557861\n",
      "Step: 2663, Loss: 0.9202448725700378, Accuracy: 1.0, Computation time: 1.7552645206451416\n",
      "Step: 2664, Loss: 0.915984570980072, Accuracy: 1.0, Computation time: 1.4130747318267822\n",
      "Step: 2665, Loss: 0.9160288572311401, Accuracy: 1.0, Computation time: 1.7051277160644531\n",
      "Step: 2666, Loss: 0.9385393261909485, Accuracy: 0.96875, Computation time: 1.9734971523284912\n",
      "Step: 2667, Loss: 0.9162459373474121, Accuracy: 1.0, Computation time: 1.6286568641662598\n",
      "Step: 2668, Loss: 0.9180250763893127, Accuracy: 1.0, Computation time: 1.8056426048278809\n",
      "Step: 2669, Loss: 0.9161022901535034, Accuracy: 1.0, Computation time: 1.5102550983428955\n",
      "Step: 2670, Loss: 0.9323603510856628, Accuracy: 0.96875, Computation time: 2.17802095413208\n",
      "Step: 2671, Loss: 0.9159536361694336, Accuracy: 1.0, Computation time: 1.3958206176757812\n",
      "Step: 2672, Loss: 0.9160457849502563, Accuracy: 1.0, Computation time: 1.642951250076294\n",
      "Step: 2673, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 1.520618200302124\n",
      "Step: 2674, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 2.047067880630493\n",
      "Step: 2675, Loss: 0.9159429669380188, Accuracy: 1.0, Computation time: 1.9981164932250977\n",
      "Step: 2676, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 1.8284587860107422\n",
      "Step: 2677, Loss: 0.9159955382347107, Accuracy: 1.0, Computation time: 1.5206255912780762\n",
      "Step: 2678, Loss: 0.9159624576568604, Accuracy: 1.0, Computation time: 1.7410461902618408\n",
      "Step: 2679, Loss: 0.9363821148872375, Accuracy: 0.96875, Computation time: 2.7994658946990967\n",
      "Step: 2680, Loss: 0.9160098433494568, Accuracy: 1.0, Computation time: 1.536332368850708\n",
      "Step: 2681, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 2.210365056991577\n",
      "Step: 2682, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.8470962047576904\n",
      "Step: 2683, Loss: 0.9453864693641663, Accuracy: 0.96875, Computation time: 1.9472124576568604\n",
      "Step: 2684, Loss: 0.9357016086578369, Accuracy: 0.96875, Computation time: 2.7956149578094482\n",
      "Step: 2685, Loss: 0.9159654974937439, Accuracy: 1.0, Computation time: 1.7767603397369385\n",
      "Step: 2686, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 1.8367919921875\n",
      "Step: 2687, Loss: 0.9168095588684082, Accuracy: 1.0, Computation time: 2.06339168548584\n",
      "Step: 2688, Loss: 0.9160276651382446, Accuracy: 1.0, Computation time: 1.677281141281128\n",
      "Step: 2689, Loss: 0.9163248538970947, Accuracy: 1.0, Computation time: 1.904268741607666\n",
      "Step: 2690, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.8688783645629883\n",
      "Step: 2691, Loss: 0.9344778656959534, Accuracy: 0.96875, Computation time: 2.215038537979126\n",
      "Step: 2692, Loss: 0.9159695506095886, Accuracy: 1.0, Computation time: 1.5911741256713867\n",
      "Step: 2693, Loss: 0.9160072803497314, Accuracy: 1.0, Computation time: 1.7646443843841553\n",
      "Step: 2694, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.3096725940704346\n",
      "Step: 2695, Loss: 0.9191643595695496, Accuracy: 1.0, Computation time: 2.472648859024048\n",
      "Step: 2696, Loss: 0.9592626094818115, Accuracy: 0.9375, Computation time: 1.536677360534668\n",
      "Step: 2697, Loss: 0.9377310276031494, Accuracy: 0.96875, Computation time: 1.8400120735168457\n",
      "Step: 2698, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 1.438603401184082\n",
      "Step: 2699, Loss: 0.9160317182540894, Accuracy: 1.0, Computation time: 1.3544182777404785\n",
      "Step: 2700, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.7526113986968994\n",
      "Step: 2701, Loss: 0.9160404205322266, Accuracy: 1.0, Computation time: 1.583235263824463\n",
      "Step: 2702, Loss: 0.9354052543640137, Accuracy: 0.96875, Computation time: 1.4088857173919678\n",
      "Step: 2703, Loss: 0.9159244894981384, Accuracy: 1.0, Computation time: 1.532564401626587\n",
      "Step: 2704, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.690840721130371\n",
      "Step: 2705, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.4020195007324219\n",
      "Step: 2706, Loss: 0.9161307215690613, Accuracy: 1.0, Computation time: 1.748168706893921\n",
      "Step: 2707, Loss: 0.9193589687347412, Accuracy: 1.0, Computation time: 1.8258426189422607\n",
      "Step: 2708, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.6508896350860596\n",
      "Step: 2709, Loss: 0.9376667141914368, Accuracy: 0.96875, Computation time: 1.4494638442993164\n",
      "Step: 2710, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.4200870990753174\n",
      "Step: 2711, Loss: 0.9159106612205505, Accuracy: 1.0, Computation time: 1.362036943435669\n",
      "Step: 2712, Loss: 0.9378594756126404, Accuracy: 0.96875, Computation time: 1.6831018924713135\n",
      "Step: 2713, Loss: 0.9227539896965027, Accuracy: 1.0, Computation time: 1.9373745918273926\n",
      "Step: 2714, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.4793508052825928\n",
      "Step: 2715, Loss: 0.9187700748443604, Accuracy: 1.0, Computation time: 3.159278392791748\n",
      "Step: 2716, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 1.640866756439209\n",
      "Step: 2717, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.6100668907165527\n",
      "Step: 2718, Loss: 0.9159334897994995, Accuracy: 1.0, Computation time: 1.6660678386688232\n",
      "Step: 2719, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 1.6172327995300293\n",
      "Step: 2720, Loss: 0.9377098679542542, Accuracy: 0.96875, Computation time: 1.615351915359497\n",
      "Step: 2721, Loss: 0.9376660585403442, Accuracy: 0.96875, Computation time: 1.9649240970611572\n",
      "Step: 2722, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.5910165309906006\n",
      "Step: 2723, Loss: 0.9159308075904846, Accuracy: 1.0, Computation time: 1.3380067348480225\n",
      "Step: 2724, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.5719630718231201\n",
      "Step: 2725, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 1.2718157768249512\n",
      "Step: 2726, Loss: 0.9375578761100769, Accuracy: 0.96875, Computation time: 2.3025503158569336\n",
      "Step: 2727, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.8263230323791504\n",
      "Step: 2728, Loss: 0.9164168834686279, Accuracy: 1.0, Computation time: 2.0754804611206055\n",
      "Step: 2729, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.7055144309997559\n",
      "Step: 2730, Loss: 0.9166629314422607, Accuracy: 1.0, Computation time: 1.4795165061950684\n",
      "Step: 2731, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 1.3531365394592285\n",
      "Step: 2732, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.6215393543243408\n",
      "Step: 2733, Loss: 0.9372861385345459, Accuracy: 0.96875, Computation time: 1.5883986949920654\n",
      "Step: 2734, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.460146427154541\n",
      "Step: 2735, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.7072010040283203\n",
      "Step: 2736, Loss: 0.9373785257339478, Accuracy: 0.96875, Computation time: 1.7210030555725098\n",
      "Step: 2737, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.5400524139404297\n",
      "Step: 2738, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.7663145065307617\n",
      "Step: 2739, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.852027177810669\n",
      "Step: 2740, Loss: 0.9373303651809692, Accuracy: 0.96875, Computation time: 1.6299574375152588\n",
      "Step: 2741, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.5877020359039307\n",
      "Step: 2742, Loss: 0.935818612575531, Accuracy: 0.96875, Computation time: 2.164287805557251\n",
      "Step: 2743, Loss: 0.9159329533576965, Accuracy: 1.0, Computation time: 1.3636982440948486\n",
      "Step: 2744, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.6192433834075928\n",
      "Step: 2745, Loss: 0.9375052452087402, Accuracy: 0.96875, Computation time: 1.872910976409912\n",
      "Step: 2746, Loss: 0.9160590767860413, Accuracy: 1.0, Computation time: 1.9770097732543945\n",
      "Step: 2747, Loss: 0.9158821105957031, Accuracy: 1.0, Computation time: 1.559882402420044\n",
      "Step: 2748, Loss: 0.9191704392433167, Accuracy: 1.0, Computation time: 2.2698538303375244\n",
      "Step: 2749, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 1.9378900527954102\n",
      "Step: 2750, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.5150883197784424\n",
      "Step: 2751, Loss: 0.9161234498023987, Accuracy: 1.0, Computation time: 1.5420262813568115\n",
      "Step: 2752, Loss: 0.9193622469902039, Accuracy: 1.0, Computation time: 2.175793409347534\n",
      "Step: 2753, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 1.8688461780548096\n",
      "Step: 2754, Loss: 0.9418500661849976, Accuracy: 0.96875, Computation time: 2.350571870803833\n",
      "Step: 2755, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 1.5725836753845215\n",
      "Step: 2756, Loss: 0.9375802874565125, Accuracy: 0.96875, Computation time: 1.297919750213623\n",
      "Step: 2757, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 2.2165756225585938\n",
      "Step: 2758, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 2.02347731590271\n",
      "Step: 2759, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 2.238100290298462\n",
      "Step: 2760, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.9159176349639893\n",
      "Step: 2761, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.932361125946045\n",
      "Step: 2762, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.6835453510284424\n",
      "Step: 2763, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 2.099508285522461\n",
      "Step: 2764, Loss: 0.9192555546760559, Accuracy: 1.0, Computation time: 1.9573249816894531\n",
      "Step: 2765, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 1.6656982898712158\n",
      "Step: 2766, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.730799913406372\n",
      "Step: 2767, Loss: 0.9159172773361206, Accuracy: 1.0, Computation time: 2.175046443939209\n",
      "Step: 2768, Loss: 0.9297499060630798, Accuracy: 0.96875, Computation time: 1.988227367401123\n",
      "Step: 2769, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 1.8513052463531494\n",
      "Step: 2770, Loss: 0.916018545627594, Accuracy: 1.0, Computation time: 1.773561716079712\n",
      "Step: 2771, Loss: 0.9331881403923035, Accuracy: 0.96875, Computation time: 1.6999320983886719\n",
      "Step: 2772, Loss: 0.9594711065292358, Accuracy: 0.9375, Computation time: 2.0146820545196533\n",
      "Step: 2773, Loss: 0.9390534162521362, Accuracy: 0.96875, Computation time: 1.9980497360229492\n",
      "Step: 2774, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.4367260932922363\n",
      "Step: 2775, Loss: 0.91660076379776, Accuracy: 1.0, Computation time: 1.6002442836761475\n",
      "Step: 2776, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.502608299255371\n",
      "Step: 2777, Loss: 0.9159581065177917, Accuracy: 1.0, Computation time: 1.9604778289794922\n",
      "Step: 2778, Loss: 0.9159801006317139, Accuracy: 1.0, Computation time: 1.6490724086761475\n",
      "Step: 2779, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.6016676425933838\n",
      "########################\n",
      "Test loss: 1.1232209205627441, Test Accuracy_epoch20: 0.698630154132843\n",
      "########################\n",
      "Step: 2780, Loss: 0.937373697757721, Accuracy: 0.96875, Computation time: 1.5849120616912842\n",
      "Step: 2781, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.6644189357757568\n",
      "Step: 2782, Loss: 0.9374688863754272, Accuracy: 0.96875, Computation time: 1.5410377979278564\n",
      "Step: 2783, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 1.847188949584961\n",
      "Step: 2784, Loss: 0.9160124063491821, Accuracy: 1.0, Computation time: 1.6060221195220947\n",
      "Step: 2785, Loss: 0.9160155653953552, Accuracy: 1.0, Computation time: 1.6501076221466064\n",
      "Step: 2786, Loss: 0.9160541296005249, Accuracy: 1.0, Computation time: 1.419294834136963\n",
      "Step: 2787, Loss: 0.9161537289619446, Accuracy: 1.0, Computation time: 1.8385310173034668\n",
      "Step: 2788, Loss: 0.9160459041595459, Accuracy: 1.0, Computation time: 1.2565875053405762\n",
      "Step: 2789, Loss: 0.9371310472488403, Accuracy: 0.96875, Computation time: 2.2624096870422363\n",
      "Step: 2790, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 1.496483564376831\n",
      "Step: 2791, Loss: 0.9159348607063293, Accuracy: 1.0, Computation time: 1.5689256191253662\n",
      "Step: 2792, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 1.8058669567108154\n",
      "Step: 2793, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.7447030544281006\n",
      "Step: 2794, Loss: 0.9163255095481873, Accuracy: 1.0, Computation time: 2.0250649452209473\n",
      "Step: 2795, Loss: 0.9336740374565125, Accuracy: 0.96875, Computation time: 2.5899999141693115\n",
      "Step: 2796, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.3470897674560547\n",
      "Step: 2797, Loss: 0.9159448146820068, Accuracy: 1.0, Computation time: 1.9159653186798096\n",
      "Step: 2798, Loss: 0.9160597920417786, Accuracy: 1.0, Computation time: 1.845705270767212\n",
      "Step: 2799, Loss: 0.9163854122161865, Accuracy: 1.0, Computation time: 1.719740390777588\n",
      "Step: 2800, Loss: 0.9159734845161438, Accuracy: 1.0, Computation time: 2.1294572353363037\n",
      "Step: 2801, Loss: 0.915967583656311, Accuracy: 1.0, Computation time: 1.6665880680084229\n",
      "Step: 2802, Loss: 0.916032612323761, Accuracy: 1.0, Computation time: 1.4329137802124023\n",
      "Step: 2803, Loss: 0.9159815907478333, Accuracy: 1.0, Computation time: 1.687058448791504\n",
      "Step: 2804, Loss: 0.9371352791786194, Accuracy: 0.96875, Computation time: 1.6915385723114014\n",
      "Step: 2805, Loss: 0.9385916590690613, Accuracy: 0.96875, Computation time: 1.8242003917694092\n",
      "Step: 2806, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.5156710147857666\n",
      "Step: 2807, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.8157739639282227\n",
      "Step: 2808, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.7259540557861328\n",
      "Step: 2809, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 2.269277811050415\n",
      "Step: 2810, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.9701037406921387\n",
      "Step: 2811, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.57926344871521\n",
      "Step: 2812, Loss: 0.9358208775520325, Accuracy: 0.96875, Computation time: 2.1014204025268555\n",
      "Step: 2813, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.757051706314087\n",
      "Step: 2814, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 1.5078990459442139\n",
      "Step: 2815, Loss: 0.9377117156982422, Accuracy: 0.96875, Computation time: 2.064636468887329\n",
      "Step: 2816, Loss: 0.9245213866233826, Accuracy: 1.0, Computation time: 3.8182532787323\n",
      "Step: 2817, Loss: 0.9159227013587952, Accuracy: 1.0, Computation time: 1.9495420455932617\n",
      "Step: 2818, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 1.6754627227783203\n",
      "Step: 2819, Loss: 0.9160282611846924, Accuracy: 1.0, Computation time: 1.7623136043548584\n",
      "Step: 2820, Loss: 0.9159629940986633, Accuracy: 1.0, Computation time: 1.6289918422698975\n",
      "Step: 2821, Loss: 0.9162071347236633, Accuracy: 1.0, Computation time: 1.583698034286499\n",
      "Step: 2822, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.519624948501587\n",
      "Step: 2823, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 2.015822410583496\n",
      "Step: 2824, Loss: 0.9164125323295593, Accuracy: 1.0, Computation time: 1.6344401836395264\n",
      "Step: 2825, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.7789306640625\n",
      "Step: 2826, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 1.6613988876342773\n",
      "Step: 2827, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.8670551776885986\n",
      "Step: 2828, Loss: 0.9346644878387451, Accuracy: 0.96875, Computation time: 2.8853399753570557\n",
      "Step: 2829, Loss: 0.9333438873291016, Accuracy: 0.96875, Computation time: 2.5042221546173096\n",
      "Step: 2830, Loss: 0.9159213304519653, Accuracy: 1.0, Computation time: 1.5556480884552002\n",
      "Step: 2831, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 2.024012804031372\n",
      "Step: 2832, Loss: 0.9160028696060181, Accuracy: 1.0, Computation time: 1.710587501525879\n",
      "Step: 2833, Loss: 0.9159974455833435, Accuracy: 1.0, Computation time: 1.6202962398529053\n",
      "Step: 2834, Loss: 0.9375652074813843, Accuracy: 0.96875, Computation time: 1.7262518405914307\n",
      "Step: 2835, Loss: 0.9354583024978638, Accuracy: 0.96875, Computation time: 1.7770040035247803\n",
      "Step: 2836, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.7514896392822266\n",
      "Step: 2837, Loss: 0.9159866571426392, Accuracy: 1.0, Computation time: 1.630061388015747\n",
      "Step: 2838, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 2.0214855670928955\n",
      "Step: 2839, Loss: 0.9160369038581848, Accuracy: 1.0, Computation time: 1.6509544849395752\n",
      "Step: 2840, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.5943331718444824\n",
      "Step: 2841, Loss: 0.9169384241104126, Accuracy: 1.0, Computation time: 1.6632134914398193\n",
      "Step: 2842, Loss: 0.9543030858039856, Accuracy: 0.9375, Computation time: 2.3219823837280273\n",
      "Step: 2843, Loss: 0.9159213900566101, Accuracy: 1.0, Computation time: 1.6398539543151855\n",
      "Step: 2844, Loss: 0.9356480240821838, Accuracy: 0.96875, Computation time: 1.573223352432251\n",
      "Step: 2845, Loss: 0.9376544952392578, Accuracy: 0.96875, Computation time: 1.4460375308990479\n",
      "Step: 2846, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 1.5770745277404785\n",
      "Step: 2847, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.5977294445037842\n",
      "Step: 2848, Loss: 0.9159864187240601, Accuracy: 1.0, Computation time: 1.406874656677246\n",
      "Step: 2849, Loss: 0.9160498380661011, Accuracy: 1.0, Computation time: 1.7346444129943848\n",
      "Step: 2850, Loss: 0.9161111116409302, Accuracy: 1.0, Computation time: 2.162231922149658\n",
      "Step: 2851, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 1.4129116535186768\n",
      "Step: 2852, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 1.7601542472839355\n",
      "Step: 2853, Loss: 0.9334171414375305, Accuracy: 0.96875, Computation time: 2.3870928287506104\n",
      "Step: 2854, Loss: 0.9168656468391418, Accuracy: 1.0, Computation time: 1.329244613647461\n",
      "Step: 2855, Loss: 0.9159414768218994, Accuracy: 1.0, Computation time: 1.6655685901641846\n",
      "Step: 2856, Loss: 0.9374088048934937, Accuracy: 0.96875, Computation time: 1.840878963470459\n",
      "Step: 2857, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.3562428951263428\n",
      "Step: 2858, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.790806531906128\n",
      "Step: 2859, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.6529395580291748\n",
      "Step: 2860, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.2150239944458008\n",
      "Step: 2861, Loss: 0.9166488647460938, Accuracy: 1.0, Computation time: 1.7292873859405518\n",
      "Step: 2862, Loss: 0.9166375398635864, Accuracy: 1.0, Computation time: 2.279433488845825\n",
      "Step: 2863, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 2.2426884174346924\n",
      "Step: 2864, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 1.6360971927642822\n",
      "Step: 2865, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 2.1624701023101807\n",
      "Step: 2866, Loss: 0.9374427795410156, Accuracy: 0.96875, Computation time: 1.4763758182525635\n",
      "Step: 2867, Loss: 0.9159500002861023, Accuracy: 1.0, Computation time: 1.750422477722168\n",
      "Step: 2868, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.3439290523529053\n",
      "Step: 2869, Loss: 0.9405956864356995, Accuracy: 0.96875, Computation time: 3.6615281105041504\n",
      "Step: 2870, Loss: 0.9243611693382263, Accuracy: 1.0, Computation time: 1.9397783279418945\n",
      "Step: 2871, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.861459732055664\n",
      "Step: 2872, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.3675692081451416\n",
      "Step: 2873, Loss: 0.917168140411377, Accuracy: 1.0, Computation time: 2.2643003463745117\n",
      "Step: 2874, Loss: 0.9337383508682251, Accuracy: 0.96875, Computation time: 1.709259271621704\n",
      "Step: 2875, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.4737040996551514\n",
      "Step: 2876, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.6910984516143799\n",
      "Step: 2877, Loss: 0.9165515303611755, Accuracy: 1.0, Computation time: 1.5980966091156006\n",
      "Step: 2878, Loss: 0.9375913143157959, Accuracy: 0.96875, Computation time: 1.5212311744689941\n",
      "Step: 2879, Loss: 0.9585508704185486, Accuracy: 0.9375, Computation time: 1.9034509658813477\n",
      "Step: 2880, Loss: 0.9159400463104248, Accuracy: 1.0, Computation time: 1.7943048477172852\n",
      "Step: 2881, Loss: 0.9375251531600952, Accuracy: 0.96875, Computation time: 1.570268154144287\n",
      "Step: 2882, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 1.5992529392242432\n",
      "Step: 2883, Loss: 0.9159964323043823, Accuracy: 1.0, Computation time: 2.0457711219787598\n",
      "Step: 2884, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.5661835670471191\n",
      "Step: 2885, Loss: 0.9160081744194031, Accuracy: 1.0, Computation time: 1.695098638534546\n",
      "Step: 2886, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.5323686599731445\n",
      "Step: 2887, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.793583869934082\n",
      "Step: 2888, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.5267856121063232\n",
      "Step: 2889, Loss: 0.9160014390945435, Accuracy: 1.0, Computation time: 1.8744428157806396\n",
      "Step: 2890, Loss: 0.9160445928573608, Accuracy: 1.0, Computation time: 1.7109296321868896\n",
      "Step: 2891, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.9305362701416016\n",
      "Step: 2892, Loss: 0.9160247445106506, Accuracy: 1.0, Computation time: 1.4181673526763916\n",
      "Step: 2893, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.7437002658843994\n",
      "Step: 2894, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.4092447757720947\n",
      "Step: 2895, Loss: 0.9159813523292542, Accuracy: 1.0, Computation time: 1.8095343112945557\n",
      "Step: 2896, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 2.005934238433838\n",
      "Step: 2897, Loss: 0.91590815782547, Accuracy: 1.0, Computation time: 1.553067922592163\n",
      "Step: 2898, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 2.0056204795837402\n",
      "Step: 2899, Loss: 0.9159169793128967, Accuracy: 1.0, Computation time: 1.605271577835083\n",
      "Step: 2900, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 2.667179822921753\n",
      "Step: 2901, Loss: 0.9159218668937683, Accuracy: 1.0, Computation time: 1.549243450164795\n",
      "Step: 2902, Loss: 0.937467098236084, Accuracy: 0.96875, Computation time: 1.3449759483337402\n",
      "Step: 2903, Loss: 0.9163730144500732, Accuracy: 1.0, Computation time: 2.0093166828155518\n",
      "Step: 2904, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.402566909790039\n",
      "Step: 2905, Loss: 0.9165173768997192, Accuracy: 1.0, Computation time: 1.8240375518798828\n",
      "Step: 2906, Loss: 0.9339465498924255, Accuracy: 0.96875, Computation time: 1.9695875644683838\n",
      "Step: 2907, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 1.7097771167755127\n",
      "Step: 2908, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.5883426666259766\n",
      "Step: 2909, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.6646921634674072\n",
      "Step: 2910, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 2.088498830795288\n",
      "Step: 2911, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 1.7955358028411865\n",
      "Step: 2912, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.6263790130615234\n",
      "Step: 2913, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 1.3566453456878662\n",
      "Step: 2914, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.8590342998504639\n",
      "Step: 2915, Loss: 0.937431812286377, Accuracy: 0.96875, Computation time: 2.01884126663208\n",
      "Step: 2916, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.660461187362671\n",
      "Step: 2917, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 2.3089609146118164\n",
      "Step: 2918, Loss: 0.9176038503646851, Accuracy: 1.0, Computation time: 2.157898426055908\n",
      "########################\n",
      "Test loss: 1.1252899169921875, Test Accuracy_epoch21: 0.6946011185646057\n",
      "########################\n",
      "Step: 2919, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.6270766258239746\n",
      "Step: 2920, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 2.0506646633148193\n",
      "Step: 2921, Loss: 0.916734516620636, Accuracy: 1.0, Computation time: 2.098773241043091\n",
      "Step: 2922, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 1.7382535934448242\n",
      "Step: 2923, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 1.864197015762329\n",
      "Step: 2924, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 2.126676082611084\n",
      "Step: 2925, Loss: 0.9158980846405029, Accuracy: 1.0, Computation time: 2.0114808082580566\n",
      "Step: 2926, Loss: 0.9373469948768616, Accuracy: 0.96875, Computation time: 2.0736746788024902\n",
      "Step: 2927, Loss: 0.9170443415641785, Accuracy: 1.0, Computation time: 1.8841133117675781\n",
      "Step: 2928, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.808126449584961\n",
      "Step: 2929, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.7393851280212402\n",
      "Step: 2930, Loss: 0.9353463053703308, Accuracy: 0.96875, Computation time: 2.416792154312134\n",
      "Step: 2931, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 1.8804214000701904\n",
      "Step: 2932, Loss: 0.9374168515205383, Accuracy: 0.96875, Computation time: 1.6373937129974365\n",
      "Step: 2933, Loss: 0.9375966191291809, Accuracy: 0.96875, Computation time: 1.7990071773529053\n",
      "Step: 2934, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 1.797645092010498\n",
      "Step: 2935, Loss: 0.9159391522407532, Accuracy: 1.0, Computation time: 1.416703701019287\n",
      "Step: 2936, Loss: 0.9160227179527283, Accuracy: 1.0, Computation time: 1.7657082080841064\n",
      "Step: 2937, Loss: 0.9159416556358337, Accuracy: 1.0, Computation time: 2.0563364028930664\n",
      "Step: 2938, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.7286269664764404\n",
      "Step: 2939, Loss: 0.9377707242965698, Accuracy: 0.96875, Computation time: 1.8451075553894043\n",
      "Step: 2940, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.8496899604797363\n",
      "Step: 2941, Loss: 0.9166111946105957, Accuracy: 1.0, Computation time: 2.0378739833831787\n",
      "Step: 2942, Loss: 0.9161299467086792, Accuracy: 1.0, Computation time: 1.613889455795288\n",
      "Step: 2943, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.670661449432373\n",
      "Step: 2944, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 1.5733985900878906\n",
      "Step: 2945, Loss: 0.9374690651893616, Accuracy: 0.96875, Computation time: 1.7716162204742432\n",
      "Step: 2946, Loss: 0.9374788403511047, Accuracy: 0.96875, Computation time: 1.724900484085083\n",
      "Step: 2947, Loss: 0.9374656081199646, Accuracy: 0.96875, Computation time: 1.7366292476654053\n",
      "Step: 2948, Loss: 0.9190618991851807, Accuracy: 1.0, Computation time: 1.9905803203582764\n",
      "Step: 2949, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.7091131210327148\n",
      "Step: 2950, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.4795892238616943\n",
      "Step: 2951, Loss: 0.9556598663330078, Accuracy: 0.9375, Computation time: 2.2171783447265625\n",
      "Step: 2952, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.7059109210968018\n",
      "Step: 2953, Loss: 0.9373843669891357, Accuracy: 0.96875, Computation time: 1.7359235286712646\n",
      "Step: 2954, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.986518383026123\n",
      "Step: 2955, Loss: 0.9373378753662109, Accuracy: 0.96875, Computation time: 2.112954616546631\n",
      "Step: 2956, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 1.8189678192138672\n",
      "Step: 2957, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.4594871997833252\n",
      "Step: 2958, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 1.691236972808838\n",
      "Step: 2959, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.479323148727417\n",
      "Step: 2960, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 1.8834011554718018\n",
      "Step: 2961, Loss: 0.9317107796669006, Accuracy: 0.96875, Computation time: 2.1162471771240234\n",
      "Step: 2962, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.9865002632141113\n",
      "Step: 2963, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.7397925853729248\n",
      "Step: 2964, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.826634407043457\n",
      "Step: 2965, Loss: 0.9357728958129883, Accuracy: 0.96875, Computation time: 3.6691081523895264\n",
      "Step: 2966, Loss: 0.9160486459732056, Accuracy: 1.0, Computation time: 2.4425575733184814\n",
      "Step: 2967, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.6445577144622803\n",
      "Step: 2968, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.9815688133239746\n",
      "Step: 2969, Loss: 0.915960431098938, Accuracy: 1.0, Computation time: 1.5582878589630127\n",
      "Step: 2970, Loss: 0.9159635901451111, Accuracy: 1.0, Computation time: 1.450812578201294\n",
      "Step: 2971, Loss: 0.9390634298324585, Accuracy: 0.96875, Computation time: 2.5322062969207764\n",
      "Step: 2972, Loss: 0.9159451723098755, Accuracy: 1.0, Computation time: 1.8988714218139648\n",
      "Step: 2973, Loss: 0.9160435199737549, Accuracy: 1.0, Computation time: 1.834869384765625\n",
      "Step: 2974, Loss: 0.9159609079360962, Accuracy: 1.0, Computation time: 1.9494502544403076\n",
      "Step: 2975, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 1.8116350173950195\n",
      "Step: 2976, Loss: 0.9159373044967651, Accuracy: 1.0, Computation time: 1.689662218093872\n",
      "Step: 2977, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 1.5784251689910889\n",
      "Step: 2978, Loss: 0.9373797178268433, Accuracy: 0.96875, Computation time: 1.3860108852386475\n",
      "Step: 2979, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.745718002319336\n",
      "Step: 2980, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 2.1083054542541504\n",
      "Step: 2981, Loss: 0.9375820755958557, Accuracy: 0.96875, Computation time: 2.0040507316589355\n",
      "Step: 2982, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.863678216934204\n",
      "Step: 2983, Loss: 0.9321169257164001, Accuracy: 0.96875, Computation time: 2.029224395751953\n",
      "Step: 2984, Loss: 0.9366014003753662, Accuracy: 0.96875, Computation time: 3.561697483062744\n",
      "Step: 2985, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 2.157071113586426\n",
      "Step: 2986, Loss: 0.9372784495353699, Accuracy: 0.96875, Computation time: 1.7846922874450684\n",
      "Step: 2987, Loss: 0.9168934226036072, Accuracy: 1.0, Computation time: 2.242799758911133\n",
      "Step: 2988, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.839723825454712\n",
      "Step: 2989, Loss: 0.9358082413673401, Accuracy: 0.96875, Computation time: 1.6860015392303467\n",
      "Step: 2990, Loss: 0.9159070253372192, Accuracy: 1.0, Computation time: 1.731032371520996\n",
      "Step: 2991, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.5459504127502441\n",
      "Step: 2992, Loss: 0.9159348011016846, Accuracy: 1.0, Computation time: 2.424551248550415\n",
      "Step: 2993, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.291114330291748\n",
      "Step: 2994, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 2.0757603645324707\n",
      "Step: 2995, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.4043083190917969\n",
      "Step: 2996, Loss: 0.9159566164016724, Accuracy: 1.0, Computation time: 1.5490553379058838\n",
      "Step: 2997, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.5677690505981445\n",
      "Step: 2998, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.371220588684082\n",
      "Step: 2999, Loss: 0.9381917119026184, Accuracy: 0.96875, Computation time: 2.1757614612579346\n",
      "Step: 3000, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.889486312866211\n",
      "Step: 3001, Loss: 0.9160633683204651, Accuracy: 1.0, Computation time: 2.0400068759918213\n",
      "Step: 3002, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 1.630603551864624\n",
      "Step: 3003, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.3015720844268799\n",
      "Step: 3004, Loss: 0.9282655715942383, Accuracy: 0.96875, Computation time: 2.8031387329101562\n",
      "Step: 3005, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.8441050052642822\n",
      "Step: 3006, Loss: 0.9374433159828186, Accuracy: 0.96875, Computation time: 1.5504074096679688\n",
      "Step: 3007, Loss: 0.9159964919090271, Accuracy: 1.0, Computation time: 1.7236239910125732\n",
      "Step: 3008, Loss: 0.9160442352294922, Accuracy: 1.0, Computation time: 2.1289801597595215\n",
      "Step: 3009, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 1.771442174911499\n",
      "Step: 3010, Loss: 0.9161707162857056, Accuracy: 1.0, Computation time: 2.239633321762085\n",
      "Step: 3011, Loss: 0.9159760475158691, Accuracy: 1.0, Computation time: 1.477180004119873\n",
      "Step: 3012, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.7146763801574707\n",
      "Step: 3013, Loss: 0.9162798523902893, Accuracy: 1.0, Computation time: 2.2566637992858887\n",
      "Step: 3014, Loss: 0.9523367881774902, Accuracy: 0.9375, Computation time: 1.6286499500274658\n",
      "Step: 3015, Loss: 0.9170964360237122, Accuracy: 1.0, Computation time: 1.619706153869629\n",
      "Step: 3016, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.482755422592163\n",
      "Step: 3017, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.5396909713745117\n",
      "Step: 3018, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.6433804035186768\n",
      "Step: 3019, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 1.8098180294036865\n",
      "Step: 3020, Loss: 0.9527567625045776, Accuracy: 0.9375, Computation time: 1.9858756065368652\n",
      "Step: 3021, Loss: 0.916910707950592, Accuracy: 1.0, Computation time: 1.8785996437072754\n",
      "Step: 3022, Loss: 0.9159510731697083, Accuracy: 1.0, Computation time: 1.4633867740631104\n",
      "Step: 3023, Loss: 0.9269770979881287, Accuracy: 0.96875, Computation time: 1.5247023105621338\n",
      "Step: 3024, Loss: 0.9160884022712708, Accuracy: 1.0, Computation time: 1.7318735122680664\n",
      "Step: 3025, Loss: 0.9166623950004578, Accuracy: 1.0, Computation time: 1.8607819080352783\n",
      "Step: 3026, Loss: 0.9161586165428162, Accuracy: 1.0, Computation time: 1.5796737670898438\n",
      "Step: 3027, Loss: 0.9159820675849915, Accuracy: 1.0, Computation time: 1.375159502029419\n",
      "Step: 3028, Loss: 0.9178575873374939, Accuracy: 1.0, Computation time: 1.7176785469055176\n",
      "Step: 3029, Loss: 0.9160020351409912, Accuracy: 1.0, Computation time: 1.3569493293762207\n",
      "Step: 3030, Loss: 0.9159765839576721, Accuracy: 1.0, Computation time: 1.487574577331543\n",
      "Step: 3031, Loss: 0.9592643976211548, Accuracy: 0.9375, Computation time: 1.51041579246521\n",
      "Step: 3032, Loss: 0.9159544706344604, Accuracy: 1.0, Computation time: 1.677037000656128\n",
      "Step: 3033, Loss: 0.915998101234436, Accuracy: 1.0, Computation time: 1.671623706817627\n",
      "Step: 3034, Loss: 0.9159427285194397, Accuracy: 1.0, Computation time: 1.4625236988067627\n",
      "Step: 3035, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.5051312446594238\n",
      "Step: 3036, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.525364875793457\n",
      "Step: 3037, Loss: 0.9159409999847412, Accuracy: 1.0, Computation time: 1.4198718070983887\n",
      "Step: 3038, Loss: 0.9159433841705322, Accuracy: 1.0, Computation time: 1.4613771438598633\n",
      "Step: 3039, Loss: 0.9159854054450989, Accuracy: 1.0, Computation time: 1.5643129348754883\n",
      "Step: 3040, Loss: 0.938088059425354, Accuracy: 0.96875, Computation time: 1.3720934391021729\n",
      "Step: 3041, Loss: 0.915991485118866, Accuracy: 1.0, Computation time: 1.3608582019805908\n",
      "Step: 3042, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 1.482860803604126\n",
      "Step: 3043, Loss: 0.9316014051437378, Accuracy: 0.96875, Computation time: 1.5810158252716064\n",
      "Step: 3044, Loss: 0.9159371852874756, Accuracy: 1.0, Computation time: 1.4034149646759033\n",
      "Step: 3045, Loss: 0.9569090604782104, Accuracy: 0.9375, Computation time: 1.3511948585510254\n",
      "Step: 3046, Loss: 0.9307388067245483, Accuracy: 0.96875, Computation time: 3.8524208068847656\n",
      "Step: 3047, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.395761251449585\n",
      "Step: 3048, Loss: 0.9159976840019226, Accuracy: 1.0, Computation time: 1.2520172595977783\n",
      "Step: 3049, Loss: 0.9376367330551147, Accuracy: 0.96875, Computation time: 1.18324613571167\n",
      "Step: 3050, Loss: 0.9160242676734924, Accuracy: 1.0, Computation time: 1.2619781494140625\n",
      "Step: 3051, Loss: 0.9295645356178284, Accuracy: 0.96875, Computation time: 1.7593228816986084\n",
      "Step: 3052, Loss: 0.9538623094558716, Accuracy: 0.9375, Computation time: 1.991231918334961\n",
      "Step: 3053, Loss: 0.916073739528656, Accuracy: 1.0, Computation time: 1.3293561935424805\n",
      "Step: 3054, Loss: 0.9159786701202393, Accuracy: 1.0, Computation time: 1.0984711647033691\n",
      "Step: 3055, Loss: 0.916018545627594, Accuracy: 1.0, Computation time: 1.239306926727295\n",
      "Step: 3056, Loss: 0.9160658121109009, Accuracy: 1.0, Computation time: 1.0644519329071045\n",
      "Step: 3057, Loss: 0.9162322878837585, Accuracy: 1.0, Computation time: 1.1015522480010986\n",
      "########################\n",
      "Test loss: 1.1273307800292969, Test Accuracy_epoch22: 0.6897662878036499\n",
      "########################\n",
      "Step: 3058, Loss: 0.9160504341125488, Accuracy: 1.0, Computation time: 0.9709000587463379\n",
      "Step: 3059, Loss: 0.9161624312400818, Accuracy: 1.0, Computation time: 1.2803106307983398\n",
      "Step: 3060, Loss: 0.9159421324729919, Accuracy: 1.0, Computation time: 1.0338916778564453\n",
      "Step: 3061, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 0.8976130485534668\n",
      "Step: 3062, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.009368896484375\n",
      "Step: 3063, Loss: 0.9372250437736511, Accuracy: 0.96875, Computation time: 1.214796781539917\n",
      "Step: 3064, Loss: 0.9172003269195557, Accuracy: 1.0, Computation time: 1.9594662189483643\n",
      "Step: 3065, Loss: 0.9160422086715698, Accuracy: 1.0, Computation time: 1.029200792312622\n",
      "Step: 3066, Loss: 0.9159339666366577, Accuracy: 1.0, Computation time: 1.015331745147705\n",
      "Step: 3067, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 1.1408684253692627\n",
      "Step: 3068, Loss: 0.9162898063659668, Accuracy: 1.0, Computation time: 1.145939826965332\n",
      "Step: 3069, Loss: 0.9160099029541016, Accuracy: 1.0, Computation time: 1.0793123245239258\n",
      "Step: 3070, Loss: 0.9161432981491089, Accuracy: 1.0, Computation time: 1.1478960514068604\n",
      "Step: 3071, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 1.209970235824585\n",
      "Step: 3072, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.2802655696868896\n",
      "Step: 3073, Loss: 0.9160541892051697, Accuracy: 1.0, Computation time: 1.5556092262268066\n",
      "Step: 3074, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.2948997020721436\n",
      "Step: 3075, Loss: 0.9162470102310181, Accuracy: 1.0, Computation time: 1.530257225036621\n",
      "Step: 3076, Loss: 0.9376386404037476, Accuracy: 0.96875, Computation time: 1.5478343963623047\n",
      "Step: 3077, Loss: 0.9375318288803101, Accuracy: 0.96875, Computation time: 0.8833944797515869\n",
      "Step: 3078, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.058760404586792\n",
      "Step: 3079, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.5322341918945312\n",
      "Step: 3080, Loss: 0.9160465598106384, Accuracy: 1.0, Computation time: 1.2127745151519775\n",
      "Step: 3081, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 1.6920928955078125\n",
      "Step: 3082, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.0937283039093018\n",
      "Step: 3083, Loss: 0.9158989787101746, Accuracy: 1.0, Computation time: 1.044494867324829\n",
      "Step: 3084, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.353863000869751\n",
      "Step: 3085, Loss: 0.932518720626831, Accuracy: 0.96875, Computation time: 1.2020337581634521\n",
      "Step: 3086, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 1.323315143585205\n",
      "Step: 3087, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.2444467544555664\n",
      "Step: 3088, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.0615158081054688\n",
      "Step: 3089, Loss: 0.9254958629608154, Accuracy: 0.96875, Computation time: 2.343791961669922\n",
      "Step: 3090, Loss: 0.9163030385971069, Accuracy: 1.0, Computation time: 1.2638030052185059\n",
      "Step: 3091, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.2784595489501953\n",
      "Step: 3092, Loss: 0.9159688949584961, Accuracy: 1.0, Computation time: 1.1723039150238037\n",
      "Step: 3093, Loss: 0.9377854466438293, Accuracy: 0.96875, Computation time: 1.4527642726898193\n",
      "Step: 3094, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 1.1744086742401123\n",
      "Step: 3095, Loss: 0.9378235936164856, Accuracy: 0.96875, Computation time: 1.3730614185333252\n",
      "Step: 3096, Loss: 0.9331545829772949, Accuracy: 0.96875, Computation time: 1.0232813358306885\n",
      "Step: 3097, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 1.0230333805084229\n",
      "Step: 3098, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.2420601844787598\n",
      "Step: 3099, Loss: 0.9219480156898499, Accuracy: 1.0, Computation time: 1.6080193519592285\n",
      "Step: 3100, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.3850555419921875\n",
      "Step: 3101, Loss: 0.9159819483757019, Accuracy: 1.0, Computation time: 1.4851300716400146\n",
      "Step: 3102, Loss: 0.9380931258201599, Accuracy: 0.96875, Computation time: 1.605353593826294\n",
      "Step: 3103, Loss: 0.9159125089645386, Accuracy: 1.0, Computation time: 1.2947208881378174\n",
      "Step: 3104, Loss: 0.9376015067100525, Accuracy: 0.96875, Computation time: 1.3787598609924316\n",
      "Step: 3105, Loss: 0.9160543084144592, Accuracy: 1.0, Computation time: 1.520338535308838\n",
      "Step: 3106, Loss: 0.9159213304519653, Accuracy: 1.0, Computation time: 1.5794363021850586\n",
      "Step: 3107, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.7392573356628418\n",
      "Step: 3108, Loss: 0.9161145687103271, Accuracy: 1.0, Computation time: 1.7100121974945068\n",
      "Step: 3109, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.5213162899017334\n",
      "Step: 3110, Loss: 0.9159508943557739, Accuracy: 1.0, Computation time: 1.4983367919921875\n",
      "Step: 3111, Loss: 0.9354463815689087, Accuracy: 0.96875, Computation time: 1.4872941970825195\n",
      "Step: 3112, Loss: 0.9373239874839783, Accuracy: 0.96875, Computation time: 1.5147864818572998\n",
      "Step: 3113, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.6469752788543701\n",
      "Step: 3114, Loss: 0.91622394323349, Accuracy: 1.0, Computation time: 1.452533483505249\n",
      "Step: 3115, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.4985759258270264\n",
      "Step: 3116, Loss: 0.9165552258491516, Accuracy: 1.0, Computation time: 1.9631352424621582\n",
      "Step: 3117, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.3686456680297852\n",
      "Step: 3118, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.7730703353881836\n",
      "Step: 3119, Loss: 0.9163532257080078, Accuracy: 1.0, Computation time: 1.7493057250976562\n",
      "Step: 3120, Loss: 0.9160342216491699, Accuracy: 1.0, Computation time: 1.5207247734069824\n",
      "Step: 3121, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.466489553451538\n",
      "Step: 3122, Loss: 0.9213093519210815, Accuracy: 1.0, Computation time: 2.026615858078003\n",
      "Step: 3123, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 2.0820398330688477\n",
      "Step: 3124, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 2.0991601943969727\n",
      "Step: 3125, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.2841477394104004\n",
      "Step: 3126, Loss: 0.937682032585144, Accuracy: 0.96875, Computation time: 1.7088322639465332\n",
      "Step: 3127, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.9512169361114502\n",
      "Step: 3128, Loss: 0.916314423084259, Accuracy: 1.0, Computation time: 1.6968824863433838\n",
      "Step: 3129, Loss: 0.9167848229408264, Accuracy: 1.0, Computation time: 1.499701738357544\n",
      "Step: 3130, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.7345058917999268\n",
      "Step: 3131, Loss: 0.9162034392356873, Accuracy: 1.0, Computation time: 1.9171545505523682\n",
      "Step: 3132, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.5122108459472656\n",
      "Step: 3133, Loss: 0.9159033894538879, Accuracy: 1.0, Computation time: 1.694115161895752\n",
      "Step: 3134, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.6096665859222412\n",
      "Step: 3135, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.854771614074707\n",
      "Step: 3136, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 2.2123680114746094\n",
      "Step: 3137, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 1.8490753173828125\n",
      "Step: 3138, Loss: 0.9526791572570801, Accuracy: 0.9375, Computation time: 1.6211154460906982\n",
      "Step: 3139, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 1.7208144664764404\n",
      "Step: 3140, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.7294254302978516\n",
      "Step: 3141, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.5044848918914795\n",
      "Step: 3142, Loss: 0.9175509810447693, Accuracy: 1.0, Computation time: 1.466167688369751\n",
      "Step: 3143, Loss: 0.9167891144752502, Accuracy: 1.0, Computation time: 1.53298020362854\n",
      "Step: 3144, Loss: 0.9159439206123352, Accuracy: 1.0, Computation time: 1.485609531402588\n",
      "Step: 3145, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.4980266094207764\n",
      "Step: 3146, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.592512845993042\n",
      "Step: 3147, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 1.2678031921386719\n",
      "Step: 3148, Loss: 0.9160323739051819, Accuracy: 1.0, Computation time: 1.4391779899597168\n",
      "Step: 3149, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.3171501159667969\n",
      "Step: 3150, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.9442574977874756\n",
      "Step: 3151, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.4276564121246338\n",
      "Step: 3152, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.8839104175567627\n",
      "Step: 3153, Loss: 0.9374243021011353, Accuracy: 0.96875, Computation time: 1.2418632507324219\n",
      "Step: 3154, Loss: 0.9374974966049194, Accuracy: 0.96875, Computation time: 1.2101209163665771\n",
      "Step: 3155, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.3513829708099365\n",
      "Step: 3156, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.3040285110473633\n",
      "Step: 3157, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.4745898246765137\n",
      "Step: 3158, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.7993922233581543\n",
      "Step: 3159, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.5932927131652832\n",
      "Step: 3160, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.7511894702911377\n",
      "Step: 3161, Loss: 0.9159587025642395, Accuracy: 1.0, Computation time: 1.2380621433258057\n",
      "Step: 3162, Loss: 0.9375734329223633, Accuracy: 0.96875, Computation time: 1.8005609512329102\n",
      "Step: 3163, Loss: 0.9372681975364685, Accuracy: 0.96875, Computation time: 1.6634280681610107\n",
      "Step: 3164, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.9945180416107178\n",
      "Step: 3165, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.2631518840789795\n",
      "Step: 3166, Loss: 0.9160727858543396, Accuracy: 1.0, Computation time: 1.5399949550628662\n",
      "Step: 3167, Loss: 0.9248185753822327, Accuracy: 1.0, Computation time: 1.6166396141052246\n",
      "Step: 3168, Loss: 0.9375476837158203, Accuracy: 0.96875, Computation time: 1.6651747226715088\n",
      "Step: 3169, Loss: 0.9164326190948486, Accuracy: 1.0, Computation time: 1.4676308631896973\n",
      "Step: 3170, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.6388731002807617\n",
      "Step: 3171, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.5918500423431396\n",
      "Step: 3172, Loss: 0.9159347414970398, Accuracy: 1.0, Computation time: 1.8140177726745605\n",
      "Step: 3173, Loss: 0.9561761617660522, Accuracy: 0.9375, Computation time: 2.134704351425171\n",
      "Step: 3174, Loss: 0.9254575371742249, Accuracy: 0.96875, Computation time: 2.2225341796875\n",
      "Step: 3175, Loss: 0.9372559785842896, Accuracy: 0.96875, Computation time: 1.6686630249023438\n",
      "Step: 3176, Loss: 0.9159969091415405, Accuracy: 1.0, Computation time: 1.5543806552886963\n",
      "Step: 3177, Loss: 0.9162536859512329, Accuracy: 1.0, Computation time: 1.4148743152618408\n",
      "Step: 3178, Loss: 0.9161974191665649, Accuracy: 1.0, Computation time: 1.5467638969421387\n",
      "Step: 3179, Loss: 0.9159516096115112, Accuracy: 1.0, Computation time: 1.3568475246429443\n",
      "Step: 3180, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.249241828918457\n",
      "Step: 3181, Loss: 0.9462366104125977, Accuracy: 0.96875, Computation time: 1.4116277694702148\n",
      "Step: 3182, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.3216540813446045\n",
      "Step: 3183, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.477013111114502\n",
      "Step: 3184, Loss: 0.9159572124481201, Accuracy: 1.0, Computation time: 1.2189874649047852\n",
      "Step: 3185, Loss: 0.9587626457214355, Accuracy: 0.9375, Computation time: 1.8460931777954102\n",
      "Step: 3186, Loss: 0.9227883815765381, Accuracy: 1.0, Computation time: 1.769134759902954\n",
      "Step: 3187, Loss: 0.9160485863685608, Accuracy: 1.0, Computation time: 1.4498841762542725\n",
      "Step: 3188, Loss: 0.9160041213035583, Accuracy: 1.0, Computation time: 1.4487018585205078\n",
      "Step: 3189, Loss: 0.9160082340240479, Accuracy: 1.0, Computation time: 1.5944011211395264\n",
      "Step: 3190, Loss: 0.9159530997276306, Accuracy: 1.0, Computation time: 1.770190954208374\n",
      "Step: 3191, Loss: 0.9168789386749268, Accuracy: 1.0, Computation time: 1.9426524639129639\n",
      "Step: 3192, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.4720005989074707\n",
      "Step: 3193, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.8053710460662842\n",
      "Step: 3194, Loss: 0.9160372614860535, Accuracy: 1.0, Computation time: 1.8096611499786377\n",
      "Step: 3195, Loss: 0.9376236796379089, Accuracy: 0.96875, Computation time: 1.402669906616211\n",
      "Step: 3196, Loss: 0.916162371635437, Accuracy: 1.0, Computation time: 2.0687506198883057\n",
      "########################\n",
      "Test loss: 1.1248172521591187, Test Accuracy_epoch23: 0.6962127089500427\n",
      "########################\n",
      "Step: 3197, Loss: 0.9159424304962158, Accuracy: 1.0, Computation time: 1.4194560050964355\n",
      "Step: 3198, Loss: 0.9162160754203796, Accuracy: 1.0, Computation time: 1.4426119327545166\n",
      "Step: 3199, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.4282550811767578\n",
      "Step: 3200, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.7030682563781738\n",
      "Step: 3201, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.5077931880950928\n",
      "Step: 3202, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.3661229610443115\n",
      "Step: 3203, Loss: 0.9162235856056213, Accuracy: 1.0, Computation time: 1.6412038803100586\n",
      "Step: 3204, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 1.5830020904541016\n",
      "Step: 3205, Loss: 0.9361475706100464, Accuracy: 0.96875, Computation time: 1.8349976539611816\n",
      "Step: 3206, Loss: 0.9331288933753967, Accuracy: 0.96875, Computation time: 2.6660614013671875\n",
      "Step: 3207, Loss: 0.9159835577011108, Accuracy: 1.0, Computation time: 2.0727083683013916\n",
      "Step: 3208, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.5946364402770996\n",
      "Step: 3209, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.8307650089263916\n",
      "Step: 3210, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.8768200874328613\n",
      "Step: 3211, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.7539317607879639\n",
      "Step: 3212, Loss: 0.9511454105377197, Accuracy: 0.9375, Computation time: 1.8210430145263672\n",
      "Step: 3213, Loss: 0.916032075881958, Accuracy: 1.0, Computation time: 1.7666168212890625\n",
      "Step: 3214, Loss: 0.9746813774108887, Accuracy: 0.90625, Computation time: 2.222343921661377\n",
      "Step: 3215, Loss: 0.9417058229446411, Accuracy: 0.96875, Computation time: 1.6090776920318604\n",
      "Step: 3216, Loss: 0.9160065054893494, Accuracy: 1.0, Computation time: 1.6647114753723145\n",
      "Step: 3217, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.488579511642456\n",
      "Step: 3218, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 1.4371638298034668\n",
      "Step: 3219, Loss: 0.9374982118606567, Accuracy: 0.96875, Computation time: 1.3674049377441406\n",
      "Step: 3220, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.3161797523498535\n",
      "Step: 3221, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.2796885967254639\n",
      "Step: 3222, Loss: 0.9159224629402161, Accuracy: 1.0, Computation time: 1.6817212104797363\n",
      "Step: 3223, Loss: 0.9159350991249084, Accuracy: 1.0, Computation time: 1.58021879196167\n",
      "Step: 3224, Loss: 0.9374405741691589, Accuracy: 0.96875, Computation time: 1.8786811828613281\n",
      "Step: 3225, Loss: 0.9172936081886292, Accuracy: 1.0, Computation time: 1.5787487030029297\n",
      "Step: 3226, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 1.437549114227295\n",
      "Step: 3227, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.5029492378234863\n",
      "Step: 3228, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.8264174461364746\n",
      "Step: 3229, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.5411415100097656\n",
      "Step: 3230, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.640761137008667\n",
      "Step: 3231, Loss: 0.9361169338226318, Accuracy: 0.96875, Computation time: 1.4912495613098145\n",
      "Step: 3232, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.2531228065490723\n",
      "Step: 3233, Loss: 0.9256978631019592, Accuracy: 0.96875, Computation time: 1.4021525382995605\n",
      "Step: 3234, Loss: 0.9160155057907104, Accuracy: 1.0, Computation time: 1.7297890186309814\n",
      "Step: 3235, Loss: 0.9160219430923462, Accuracy: 1.0, Computation time: 1.4169061183929443\n",
      "Step: 3236, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 1.612945556640625\n",
      "Step: 3237, Loss: 0.9376829862594604, Accuracy: 0.96875, Computation time: 1.5316431522369385\n",
      "Step: 3238, Loss: 0.9159300327301025, Accuracy: 1.0, Computation time: 1.6695842742919922\n",
      "Step: 3239, Loss: 0.9164430499076843, Accuracy: 1.0, Computation time: 1.5707981586456299\n",
      "Step: 3240, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.2844185829162598\n",
      "Step: 3241, Loss: 0.9170953035354614, Accuracy: 1.0, Computation time: 2.0461483001708984\n",
      "Step: 3242, Loss: 0.9159016013145447, Accuracy: 1.0, Computation time: 1.5544030666351318\n",
      "Step: 3243, Loss: 0.9374388456344604, Accuracy: 0.96875, Computation time: 1.616034984588623\n",
      "Step: 3244, Loss: 0.9161357879638672, Accuracy: 1.0, Computation time: 1.4822959899902344\n",
      "Step: 3245, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.2801311016082764\n",
      "Step: 3246, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.5612196922302246\n",
      "Step: 3247, Loss: 0.9187625646591187, Accuracy: 1.0, Computation time: 1.6063604354858398\n",
      "Step: 3248, Loss: 0.9158363342285156, Accuracy: 1.0, Computation time: 1.6902694702148438\n",
      "Step: 3249, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.5345969200134277\n",
      "Step: 3250, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.4139223098754883\n",
      "Step: 3251, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.3388197422027588\n",
      "Step: 3252, Loss: 0.93747878074646, Accuracy: 0.96875, Computation time: 1.712432622909546\n",
      "Step: 3253, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 1.2994370460510254\n",
      "Step: 3254, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.5800514221191406\n",
      "Step: 3255, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.4001080989837646\n",
      "Step: 3256, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.3101036548614502\n",
      "Step: 3257, Loss: 0.9158541560173035, Accuracy: 1.0, Computation time: 1.1976332664489746\n",
      "Step: 3258, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.3555188179016113\n",
      "Step: 3259, Loss: 0.9220585227012634, Accuracy: 1.0, Computation time: 2.2991714477539062\n",
      "Step: 3260, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.355438232421875\n",
      "Step: 3261, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.338308334350586\n",
      "Step: 3262, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.601792335510254\n",
      "Step: 3263, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.3356585502624512\n",
      "Step: 3264, Loss: 0.916359007358551, Accuracy: 1.0, Computation time: 1.509077787399292\n",
      "Step: 3265, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.5642673969268799\n",
      "Step: 3266, Loss: 0.9160016179084778, Accuracy: 1.0, Computation time: 1.821390151977539\n",
      "Step: 3267, Loss: 0.9344578981399536, Accuracy: 0.96875, Computation time: 2.52341628074646\n",
      "Step: 3268, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.5661320686340332\n",
      "Step: 3269, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.6200940608978271\n",
      "Step: 3270, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.3248920440673828\n",
      "Step: 3271, Loss: 0.9159359335899353, Accuracy: 1.0, Computation time: 1.7013237476348877\n",
      "Step: 3272, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.5680150985717773\n",
      "Step: 3273, Loss: 0.9162207245826721, Accuracy: 1.0, Computation time: 1.5360643863677979\n",
      "Step: 3274, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.6375987529754639\n",
      "Step: 3275, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 1.468839406967163\n",
      "Step: 3276, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 1.8576734066009521\n",
      "Step: 3277, Loss: 0.9333622455596924, Accuracy: 0.96875, Computation time: 2.858093738555908\n",
      "Step: 3278, Loss: 0.9161883592605591, Accuracy: 1.0, Computation time: 1.3297126293182373\n",
      "Step: 3279, Loss: 0.9662830233573914, Accuracy: 0.9375, Computation time: 1.4256317615509033\n",
      "Step: 3280, Loss: 0.915919303894043, Accuracy: 1.0, Computation time: 1.4227306842803955\n",
      "Step: 3281, Loss: 0.9367638230323792, Accuracy: 0.96875, Computation time: 1.3473081588745117\n",
      "Step: 3282, Loss: 0.9160845875740051, Accuracy: 1.0, Computation time: 1.6289472579956055\n",
      "Step: 3283, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 1.4371929168701172\n",
      "Step: 3284, Loss: 0.9231234192848206, Accuracy: 1.0, Computation time: 2.293614149093628\n",
      "Step: 3285, Loss: 0.9160500168800354, Accuracy: 1.0, Computation time: 1.5462279319763184\n",
      "Step: 3286, Loss: 0.93761146068573, Accuracy: 0.96875, Computation time: 1.4199049472808838\n",
      "Step: 3287, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.3188323974609375\n",
      "Step: 3288, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.2642016410827637\n",
      "Step: 3289, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.156816005706787\n",
      "Step: 3290, Loss: 0.9159976840019226, Accuracy: 1.0, Computation time: 1.0731680393218994\n",
      "Step: 3291, Loss: 0.9159491062164307, Accuracy: 1.0, Computation time: 1.055830955505371\n",
      "Step: 3292, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 0.9864461421966553\n",
      "Step: 3293, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 1.6037366390228271\n",
      "Step: 3294, Loss: 0.9518715143203735, Accuracy: 0.9375, Computation time: 1.2446751594543457\n",
      "Step: 3295, Loss: 0.9200263023376465, Accuracy: 1.0, Computation time: 1.2251951694488525\n",
      "Step: 3296, Loss: 0.9348287582397461, Accuracy: 0.96875, Computation time: 2.0589582920074463\n",
      "Step: 3297, Loss: 0.9191097021102905, Accuracy: 1.0, Computation time: 1.133314609527588\n",
      "Step: 3298, Loss: 0.9161845445632935, Accuracy: 1.0, Computation time: 1.3686878681182861\n",
      "Step: 3299, Loss: 0.9377673864364624, Accuracy: 0.96875, Computation time: 1.2918944358825684\n",
      "Step: 3300, Loss: 0.916030764579773, Accuracy: 1.0, Computation time: 1.4504005908966064\n",
      "Step: 3301, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 1.5168118476867676\n",
      "Step: 3302, Loss: 0.9593443870544434, Accuracy: 0.9375, Computation time: 1.5409717559814453\n",
      "Step: 3303, Loss: 0.9159756898880005, Accuracy: 1.0, Computation time: 1.6946008205413818\n",
      "Step: 3304, Loss: 0.9161368012428284, Accuracy: 1.0, Computation time: 1.3378241062164307\n",
      "Step: 3305, Loss: 0.916039764881134, Accuracy: 1.0, Computation time: 1.6040360927581787\n",
      "Step: 3306, Loss: 0.9160240888595581, Accuracy: 1.0, Computation time: 1.385988473892212\n",
      "Step: 3307, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 1.6180078983306885\n",
      "Step: 3308, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 1.170461654663086\n",
      "Step: 3309, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.4705722332000732\n",
      "Step: 3310, Loss: 0.9309489130973816, Accuracy: 0.96875, Computation time: 2.4219651222229004\n",
      "Step: 3311, Loss: 0.915971577167511, Accuracy: 1.0, Computation time: 1.2508478164672852\n",
      "Step: 3312, Loss: 0.9374803900718689, Accuracy: 0.96875, Computation time: 1.5373477935791016\n",
      "Step: 3313, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.2374658584594727\n",
      "Step: 3314, Loss: 0.9424583911895752, Accuracy: 0.96875, Computation time: 1.8899199962615967\n",
      "Step: 3315, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 1.478776216506958\n",
      "Step: 3316, Loss: 0.9198116064071655, Accuracy: 1.0, Computation time: 2.582387685775757\n",
      "Step: 3317, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.280757188796997\n",
      "Step: 3318, Loss: 0.9161822199821472, Accuracy: 1.0, Computation time: 1.9030559062957764\n",
      "Step: 3319, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.3448891639709473\n",
      "Step: 3320, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.680994987487793\n",
      "Step: 3321, Loss: 0.9160122871398926, Accuracy: 1.0, Computation time: 1.914527416229248\n",
      "Step: 3322, Loss: 0.9315372109413147, Accuracy: 0.96875, Computation time: 1.6362969875335693\n",
      "Step: 3323, Loss: 0.9163426756858826, Accuracy: 1.0, Computation time: 1.4306855201721191\n",
      "Step: 3324, Loss: 0.9205065369606018, Accuracy: 1.0, Computation time: 1.8750016689300537\n",
      "Step: 3325, Loss: 0.9161325097084045, Accuracy: 1.0, Computation time: 1.8882758617401123\n",
      "Step: 3326, Loss: 0.9239619374275208, Accuracy: 1.0, Computation time: 2.7882938385009766\n",
      "Step: 3327, Loss: 0.9159421920776367, Accuracy: 1.0, Computation time: 1.6771326065063477\n",
      "Step: 3328, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.8882441520690918\n",
      "Step: 3329, Loss: 0.9258733987808228, Accuracy: 0.96875, Computation time: 1.724883794784546\n",
      "Step: 3330, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 1.810291051864624\n",
      "Step: 3331, Loss: 0.935820996761322, Accuracy: 0.96875, Computation time: 1.928199291229248\n",
      "Step: 3332, Loss: 0.9159401655197144, Accuracy: 1.0, Computation time: 1.5970239639282227\n",
      "Step: 3333, Loss: 0.9160979986190796, Accuracy: 1.0, Computation time: 1.7971205711364746\n",
      "Step: 3334, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.603179931640625\n",
      "Step: 3335, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 1.8753089904785156\n",
      "########################\n",
      "Test loss: 1.1221139430999756, Test Accuracy_epoch24: 0.698630154132843\n",
      "########################\n",
      "Step: 3336, Loss: 0.9374505281448364, Accuracy: 0.96875, Computation time: 2.0561811923980713\n",
      "Step: 3337, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.9972004890441895\n",
      "Step: 3338, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.9130277633666992\n",
      "Step: 3339, Loss: 0.9324144124984741, Accuracy: 0.96875, Computation time: 4.155164480209351\n",
      "Step: 3340, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.9232165813446045\n",
      "Step: 3341, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 2.0774452686309814\n",
      "Step: 3342, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 2.17496657371521\n",
      "Step: 3343, Loss: 0.9375488758087158, Accuracy: 0.96875, Computation time: 2.0556726455688477\n",
      "Step: 3344, Loss: 0.9213656783103943, Accuracy: 1.0, Computation time: 2.6058924198150635\n",
      "Step: 3345, Loss: 0.9379943609237671, Accuracy: 0.96875, Computation time: 1.9125053882598877\n",
      "Step: 3346, Loss: 0.917340099811554, Accuracy: 1.0, Computation time: 2.7698168754577637\n",
      "Step: 3347, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 2.150960683822632\n",
      "Step: 3348, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 2.0562667846679688\n",
      "Step: 3349, Loss: 0.9574629068374634, Accuracy: 0.9375, Computation time: 3.692347764968872\n",
      "Step: 3350, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 1.9238295555114746\n",
      "Step: 3351, Loss: 0.9180189967155457, Accuracy: 1.0, Computation time: 2.2930350303649902\n",
      "Step: 3352, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 2.161389112472534\n",
      "Step: 3353, Loss: 0.9171318411827087, Accuracy: 1.0, Computation time: 2.6147775650024414\n",
      "Step: 3354, Loss: 0.930107831954956, Accuracy: 0.96875, Computation time: 1.9974803924560547\n",
      "Step: 3355, Loss: 0.9159277081489563, Accuracy: 1.0, Computation time: 1.9518592357635498\n",
      "Step: 3356, Loss: 0.9159505367279053, Accuracy: 1.0, Computation time: 1.5814595222473145\n",
      "Step: 3357, Loss: 0.9375692009925842, Accuracy: 0.96875, Computation time: 1.2594523429870605\n",
      "Step: 3358, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 2.0837576389312744\n",
      "Step: 3359, Loss: 0.9206215143203735, Accuracy: 1.0, Computation time: 2.372074842453003\n",
      "Step: 3360, Loss: 0.9377110004425049, Accuracy: 0.96875, Computation time: 3.0857675075531006\n",
      "Step: 3361, Loss: 0.9159653782844543, Accuracy: 1.0, Computation time: 1.771226406097412\n",
      "Step: 3362, Loss: 0.9374817609786987, Accuracy: 0.96875, Computation time: 1.3109736442565918\n",
      "Step: 3363, Loss: 0.9159987568855286, Accuracy: 1.0, Computation time: 1.4000272750854492\n",
      "Step: 3364, Loss: 0.9162371158599854, Accuracy: 1.0, Computation time: 2.772249221801758\n",
      "Step: 3365, Loss: 0.9375795722007751, Accuracy: 0.96875, Computation time: 1.8660972118377686\n",
      "Step: 3366, Loss: 0.9159805774688721, Accuracy: 1.0, Computation time: 1.5697035789489746\n",
      "Step: 3367, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.3956809043884277\n",
      "Step: 3368, Loss: 0.9159500598907471, Accuracy: 1.0, Computation time: 1.4848055839538574\n",
      "Step: 3369, Loss: 0.924264132976532, Accuracy: 1.0, Computation time: 1.5028669834136963\n",
      "Step: 3370, Loss: 0.9216869473457336, Accuracy: 1.0, Computation time: 2.0413658618927\n",
      "Step: 3371, Loss: 0.9160849452018738, Accuracy: 1.0, Computation time: 1.6715052127838135\n",
      "Step: 3372, Loss: 0.9347637295722961, Accuracy: 0.96875, Computation time: 3.4125804901123047\n",
      "Step: 3373, Loss: 0.9159423112869263, Accuracy: 1.0, Computation time: 1.698366403579712\n",
      "Step: 3374, Loss: 0.947715163230896, Accuracy: 0.9375, Computation time: 2.2714829444885254\n",
      "Step: 3375, Loss: 0.9215282201766968, Accuracy: 1.0, Computation time: 1.7672312259674072\n",
      "Step: 3376, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.8229734897613525\n",
      "Step: 3377, Loss: 0.9383224844932556, Accuracy: 0.96875, Computation time: 1.3908653259277344\n",
      "Step: 3378, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 1.731079339981079\n",
      "Step: 3379, Loss: 0.9159224629402161, Accuracy: 1.0, Computation time: 1.1701991558074951\n",
      "Step: 3380, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.6688463687896729\n",
      "Step: 3381, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 1.3984155654907227\n",
      "Step: 3382, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.7173044681549072\n",
      "Step: 3383, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 1.6255722045898438\n",
      "Step: 3384, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.5149860382080078\n",
      "Step: 3385, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.6623389720916748\n",
      "Step: 3386, Loss: 0.93028724193573, Accuracy: 0.96875, Computation time: 1.9888103008270264\n",
      "Step: 3387, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 1.4932739734649658\n",
      "Step: 3388, Loss: 0.937548816204071, Accuracy: 0.96875, Computation time: 1.991053819656372\n",
      "Step: 3389, Loss: 0.9159374237060547, Accuracy: 1.0, Computation time: 1.7705883979797363\n",
      "Step: 3390, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 1.6579391956329346\n",
      "Step: 3391, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 1.7896819114685059\n",
      "Step: 3392, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 1.829350471496582\n",
      "Step: 3393, Loss: 0.9161422848701477, Accuracy: 1.0, Computation time: 1.7238054275512695\n",
      "Step: 3394, Loss: 0.9159027934074402, Accuracy: 1.0, Computation time: 1.4504358768463135\n",
      "Step: 3395, Loss: 0.9161130785942078, Accuracy: 1.0, Computation time: 1.6187610626220703\n",
      "Step: 3396, Loss: 0.9807313680648804, Accuracy: 0.90625, Computation time: 1.607187032699585\n",
      "Step: 3397, Loss: 0.916458785533905, Accuracy: 1.0, Computation time: 1.6910126209259033\n",
      "Step: 3398, Loss: 0.9253577589988708, Accuracy: 0.96875, Computation time: 1.7165002822875977\n",
      "Step: 3399, Loss: 0.9190693497657776, Accuracy: 1.0, Computation time: 1.7959775924682617\n",
      "Step: 3400, Loss: 0.915959894657135, Accuracy: 1.0, Computation time: 1.604454517364502\n",
      "Step: 3401, Loss: 0.9175077676773071, Accuracy: 1.0, Computation time: 1.8803021907806396\n",
      "Step: 3402, Loss: 0.9161855578422546, Accuracy: 1.0, Computation time: 1.8832449913024902\n",
      "Step: 3403, Loss: 0.9159613847732544, Accuracy: 1.0, Computation time: 1.92919921875\n",
      "Step: 3404, Loss: 0.922863781452179, Accuracy: 1.0, Computation time: 1.5902197360992432\n",
      "Step: 3405, Loss: 0.9159423112869263, Accuracy: 1.0, Computation time: 1.8100402355194092\n",
      "Step: 3406, Loss: 0.916611909866333, Accuracy: 1.0, Computation time: 2.6372287273406982\n",
      "Step: 3407, Loss: 0.9160799384117126, Accuracy: 1.0, Computation time: 1.8939924240112305\n",
      "Step: 3408, Loss: 0.9160970449447632, Accuracy: 1.0, Computation time: 1.6627006530761719\n",
      "Step: 3409, Loss: 0.9762517213821411, Accuracy: 0.90625, Computation time: 1.6917409896850586\n",
      "Step: 3410, Loss: 0.9161600470542908, Accuracy: 1.0, Computation time: 1.7854232788085938\n",
      "Step: 3411, Loss: 0.916080892086029, Accuracy: 1.0, Computation time: 1.7224862575531006\n",
      "Step: 3412, Loss: 0.9160741567611694, Accuracy: 1.0, Computation time: 1.5498414039611816\n",
      "Step: 3413, Loss: 0.9167471528053284, Accuracy: 1.0, Computation time: 1.6649243831634521\n",
      "Step: 3414, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 1.7919492721557617\n",
      "Step: 3415, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 1.871504783630371\n",
      "Step: 3416, Loss: 0.9159577488899231, Accuracy: 1.0, Computation time: 1.4022226333618164\n",
      "Step: 3417, Loss: 0.9160484075546265, Accuracy: 1.0, Computation time: 1.8533024787902832\n",
      "Step: 3418, Loss: 0.9160667061805725, Accuracy: 1.0, Computation time: 1.5682168006896973\n",
      "Step: 3419, Loss: 0.937508761882782, Accuracy: 0.96875, Computation time: 2.4033796787261963\n",
      "Step: 3420, Loss: 0.919445812702179, Accuracy: 1.0, Computation time: 2.1518068313598633\n",
      "Step: 3421, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.4424614906311035\n",
      "Step: 3422, Loss: 0.915952205657959, Accuracy: 1.0, Computation time: 1.4141831398010254\n",
      "Step: 3423, Loss: 0.9159349203109741, Accuracy: 1.0, Computation time: 1.6937978267669678\n",
      "Step: 3424, Loss: 0.9162325859069824, Accuracy: 1.0, Computation time: 1.703488826751709\n",
      "Step: 3425, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.4979522228240967\n",
      "Step: 3426, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.5325374603271484\n",
      "Step: 3427, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.8260829448699951\n",
      "Step: 3428, Loss: 0.9159231185913086, Accuracy: 1.0, Computation time: 1.4918403625488281\n",
      "Step: 3429, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 1.7389225959777832\n",
      "Step: 3430, Loss: 0.9374759793281555, Accuracy: 0.96875, Computation time: 1.920576810836792\n",
      "Step: 3431, Loss: 0.916300892829895, Accuracy: 1.0, Computation time: 2.127448797225952\n",
      "Step: 3432, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 1.1593701839447021\n",
      "Step: 3433, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.504861831665039\n",
      "Step: 3434, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.446601152420044\n",
      "Step: 3435, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.4602141380310059\n",
      "Step: 3436, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.384392261505127\n",
      "Step: 3437, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.5593833923339844\n",
      "Step: 3438, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.4972472190856934\n",
      "Step: 3439, Loss: 0.9165806174278259, Accuracy: 1.0, Computation time: 1.8768188953399658\n",
      "Step: 3440, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 1.5884909629821777\n",
      "Step: 3441, Loss: 0.9375046491622925, Accuracy: 0.96875, Computation time: 1.4351904392242432\n",
      "Step: 3442, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.8082349300384521\n",
      "Step: 3443, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.0916504859924316\n",
      "Step: 3444, Loss: 0.9198688864707947, Accuracy: 1.0, Computation time: 2.186866283416748\n",
      "Step: 3445, Loss: 0.9165566563606262, Accuracy: 1.0, Computation time: 1.6727654933929443\n",
      "Step: 3446, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.629624843597412\n",
      "Step: 3447, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.184183120727539\n",
      "Step: 3448, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.4843261241912842\n",
      "Step: 3449, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.359647512435913\n",
      "Step: 3450, Loss: 0.9374659061431885, Accuracy: 0.96875, Computation time: 1.2792367935180664\n",
      "Step: 3451, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.4380459785461426\n",
      "Step: 3452, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 1.4135754108428955\n",
      "Step: 3453, Loss: 0.9374269247055054, Accuracy: 0.96875, Computation time: 1.6355695724487305\n",
      "Step: 3454, Loss: 0.9376477003097534, Accuracy: 0.96875, Computation time: 1.5571482181549072\n",
      "Step: 3455, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.3595094680786133\n",
      "Step: 3456, Loss: 0.9320788979530334, Accuracy: 0.96875, Computation time: 2.3505969047546387\n",
      "Step: 3457, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 1.7863819599151611\n",
      "Step: 3458, Loss: 0.9377767443656921, Accuracy: 0.96875, Computation time: 1.690455436706543\n",
      "Step: 3459, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 1.246415138244629\n",
      "Step: 3460, Loss: 0.9161003232002258, Accuracy: 1.0, Computation time: 1.446035385131836\n",
      "Step: 3461, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 1.7749779224395752\n",
      "Step: 3462, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.686326503753662\n",
      "Step: 3463, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 2.1160106658935547\n",
      "Step: 3464, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.6236567497253418\n",
      "Step: 3465, Loss: 0.9371118545532227, Accuracy: 0.96875, Computation time: 1.782930612564087\n",
      "Step: 3466, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.7383601665496826\n",
      "Step: 3467, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.902287483215332\n",
      "Step: 3468, Loss: 0.9354686737060547, Accuracy: 0.96875, Computation time: 1.4926538467407227\n",
      "Step: 3469, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 1.4304592609405518\n",
      "Step: 3470, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.7683539390563965\n",
      "Step: 3471, Loss: 0.9373498558998108, Accuracy: 0.96875, Computation time: 1.4141793251037598\n",
      "Step: 3472, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.6004858016967773\n",
      "Step: 3473, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.49989914894104\n",
      "Step: 3474, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.123920202255249\n",
      "########################\n",
      "Test loss: 1.12942636013031, Test Accuracy_epoch25: 0.6889604926109314\n",
      "########################\n",
      "Step: 3475, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.5371029376983643\n",
      "Step: 3476, Loss: 0.9187462329864502, Accuracy: 1.0, Computation time: 1.4097795486450195\n",
      "Step: 3477, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.48191237449646\n",
      "Step: 3478, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 1.4904441833496094\n",
      "Step: 3479, Loss: 0.9159356355667114, Accuracy: 1.0, Computation time: 1.836890697479248\n",
      "Step: 3480, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.992995023727417\n",
      "Step: 3481, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.6326818466186523\n",
      "Step: 3482, Loss: 0.9375278949737549, Accuracy: 0.96875, Computation time: 1.4421091079711914\n",
      "Step: 3483, Loss: 0.9172917008399963, Accuracy: 1.0, Computation time: 5.7645909786224365\n",
      "Step: 3484, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.7080225944519043\n",
      "Step: 3485, Loss: 0.9163329601287842, Accuracy: 1.0, Computation time: 1.4781410694122314\n",
      "Step: 3486, Loss: 0.9593717455863953, Accuracy: 0.9375, Computation time: 2.013603687286377\n",
      "Step: 3487, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.7868051528930664\n",
      "Step: 3488, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.551062822341919\n",
      "Step: 3489, Loss: 0.9376633167266846, Accuracy: 0.96875, Computation time: 1.6221847534179688\n",
      "Step: 3490, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 1.8583710193634033\n",
      "Step: 3491, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.6015541553497314\n",
      "Step: 3492, Loss: 0.9376397728919983, Accuracy: 0.96875, Computation time: 2.5370781421661377\n",
      "Step: 3493, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.2836346626281738\n",
      "Step: 3494, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 1.5901808738708496\n",
      "Step: 3495, Loss: 0.9384369254112244, Accuracy: 0.96875, Computation time: 1.827822208404541\n",
      "Step: 3496, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.5559906959533691\n",
      "Step: 3497, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.481229543685913\n",
      "Step: 3498, Loss: 0.933055579662323, Accuracy: 0.96875, Computation time: 1.9591891765594482\n",
      "Step: 3499, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.9297454357147217\n",
      "Step: 3500, Loss: 0.9159165024757385, Accuracy: 1.0, Computation time: 1.735776424407959\n",
      "Step: 3501, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 1.4961543083190918\n",
      "Step: 3502, Loss: 0.9159110188484192, Accuracy: 1.0, Computation time: 1.4176480770111084\n",
      "Step: 3503, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.9296510219573975\n",
      "Step: 3504, Loss: 0.9159528613090515, Accuracy: 1.0, Computation time: 1.566986322402954\n",
      "Step: 3505, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 1.840613842010498\n",
      "Step: 3506, Loss: 0.9160074591636658, Accuracy: 1.0, Computation time: 2.361278772354126\n",
      "Step: 3507, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 1.535830020904541\n",
      "Step: 3508, Loss: 0.9375401139259338, Accuracy: 0.96875, Computation time: 1.7331204414367676\n",
      "Step: 3509, Loss: 0.9168127179145813, Accuracy: 1.0, Computation time: 1.6003692150115967\n",
      "Step: 3510, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.3316154479980469\n",
      "Step: 3511, Loss: 0.9159145951271057, Accuracy: 1.0, Computation time: 2.438293933868408\n",
      "Step: 3512, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.3479244709014893\n",
      "Step: 3513, Loss: 0.9590288996696472, Accuracy: 0.9375, Computation time: 1.5857656002044678\n",
      "Step: 3514, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.467850923538208\n",
      "Step: 3515, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.4921214580535889\n",
      "Step: 3516, Loss: 0.9261001944541931, Accuracy: 0.96875, Computation time: 2.456315040588379\n",
      "Step: 3517, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.5406546592712402\n",
      "Step: 3518, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.5722622871398926\n",
      "Step: 3519, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 1.392636775970459\n",
      "Step: 3520, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.9696934223175049\n",
      "Step: 3521, Loss: 0.9159715175628662, Accuracy: 1.0, Computation time: 1.776597261428833\n",
      "Step: 3522, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.4348456859588623\n",
      "Step: 3523, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.5744006633758545\n",
      "Step: 3524, Loss: 0.9227200746536255, Accuracy: 1.0, Computation time: 2.440455913543701\n",
      "Step: 3525, Loss: 0.9170717597007751, Accuracy: 1.0, Computation time: 2.115570545196533\n",
      "Step: 3526, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.8524041175842285\n",
      "Step: 3527, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.3717174530029297\n",
      "Step: 3528, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.4897210597991943\n",
      "Step: 3529, Loss: 0.9166014194488525, Accuracy: 1.0, Computation time: 1.5891902446746826\n",
      "Step: 3530, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.3681676387786865\n",
      "Step: 3531, Loss: 0.9161187410354614, Accuracy: 1.0, Computation time: 2.447462558746338\n",
      "Step: 3532, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.4842479228973389\n",
      "Step: 3533, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.2610244750976562\n",
      "Step: 3534, Loss: 0.9186602830886841, Accuracy: 1.0, Computation time: 1.947784423828125\n",
      "Step: 3535, Loss: 0.9159225225448608, Accuracy: 1.0, Computation time: 1.2944762706756592\n",
      "Step: 3536, Loss: 0.9160159230232239, Accuracy: 1.0, Computation time: 1.659843921661377\n",
      "Step: 3537, Loss: 0.9159146547317505, Accuracy: 1.0, Computation time: 1.7892894744873047\n",
      "Step: 3538, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 1.490126132965088\n",
      "Step: 3539, Loss: 0.9165681600570679, Accuracy: 1.0, Computation time: 1.6016173362731934\n",
      "Step: 3540, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 1.7072021961212158\n",
      "Step: 3541, Loss: 0.9359114170074463, Accuracy: 0.96875, Computation time: 1.501451015472412\n",
      "Step: 3542, Loss: 0.9159690141677856, Accuracy: 1.0, Computation time: 1.5723955631256104\n",
      "Step: 3543, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.546508550643921\n",
      "Step: 3544, Loss: 0.9219675064086914, Accuracy: 1.0, Computation time: 2.3228940963745117\n",
      "Step: 3545, Loss: 0.9169687032699585, Accuracy: 1.0, Computation time: 2.2777187824249268\n",
      "Step: 3546, Loss: 0.9159901142120361, Accuracy: 1.0, Computation time: 2.257741689682007\n",
      "Step: 3547, Loss: 0.9160549640655518, Accuracy: 1.0, Computation time: 1.7636096477508545\n",
      "Step: 3548, Loss: 0.9162557721138, Accuracy: 1.0, Computation time: 1.3039007186889648\n",
      "Step: 3549, Loss: 0.9159643054008484, Accuracy: 1.0, Computation time: 1.635807752609253\n",
      "Step: 3550, Loss: 0.9159516096115112, Accuracy: 1.0, Computation time: 1.771341323852539\n",
      "Step: 3551, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.4963068962097168\n",
      "Step: 3552, Loss: 0.9160103797912598, Accuracy: 1.0, Computation time: 1.8669466972351074\n",
      "Step: 3553, Loss: 0.9159302115440369, Accuracy: 1.0, Computation time: 1.430614948272705\n",
      "Step: 3554, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.5833754539489746\n",
      "Step: 3555, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.3151850700378418\n",
      "Step: 3556, Loss: 0.9359101057052612, Accuracy: 0.96875, Computation time: 2.5694642066955566\n",
      "Step: 3557, Loss: 0.9372771978378296, Accuracy: 0.96875, Computation time: 1.2943124771118164\n",
      "Step: 3558, Loss: 0.9160261750221252, Accuracy: 1.0, Computation time: 1.8379642963409424\n",
      "Step: 3559, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 1.9116082191467285\n",
      "Step: 3560, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.6099505424499512\n",
      "Step: 3561, Loss: 0.9159424304962158, Accuracy: 1.0, Computation time: 1.800696849822998\n",
      "Step: 3562, Loss: 0.9159789085388184, Accuracy: 1.0, Computation time: 1.7857930660247803\n",
      "Step: 3563, Loss: 0.9162653684616089, Accuracy: 1.0, Computation time: 1.5647931098937988\n",
      "Step: 3564, Loss: 0.9350131750106812, Accuracy: 0.96875, Computation time: 2.9903836250305176\n",
      "Step: 3565, Loss: 0.9160817861557007, Accuracy: 1.0, Computation time: 1.5126357078552246\n",
      "Step: 3566, Loss: 0.915954053401947, Accuracy: 1.0, Computation time: 1.4793925285339355\n",
      "Step: 3567, Loss: 0.9164926409721375, Accuracy: 1.0, Computation time: 2.0975959300994873\n",
      "Step: 3568, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.3691449165344238\n",
      "Step: 3569, Loss: 0.9159784913063049, Accuracy: 1.0, Computation time: 1.519803524017334\n",
      "Step: 3570, Loss: 0.9378221035003662, Accuracy: 0.96875, Computation time: 1.7575194835662842\n",
      "Step: 3571, Loss: 0.915928065776825, Accuracy: 1.0, Computation time: 1.6326956748962402\n",
      "Step: 3572, Loss: 0.9160207509994507, Accuracy: 1.0, Computation time: 1.8397843837738037\n",
      "Step: 3573, Loss: 0.9183930158615112, Accuracy: 1.0, Computation time: 1.4157462120056152\n",
      "Step: 3574, Loss: 0.9374822974205017, Accuracy: 0.96875, Computation time: 1.4896504878997803\n",
      "Step: 3575, Loss: 0.9159698486328125, Accuracy: 1.0, Computation time: 1.8480339050292969\n",
      "Step: 3576, Loss: 0.916126012802124, Accuracy: 1.0, Computation time: 1.772979736328125\n",
      "Step: 3577, Loss: 0.9263708591461182, Accuracy: 0.96875, Computation time: 2.081913471221924\n",
      "Step: 3578, Loss: 0.9159853458404541, Accuracy: 1.0, Computation time: 1.7242441177368164\n",
      "Step: 3579, Loss: 0.9375613927841187, Accuracy: 0.96875, Computation time: 2.054795742034912\n",
      "Step: 3580, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.5569264888763428\n",
      "Step: 3581, Loss: 0.9159168601036072, Accuracy: 1.0, Computation time: 1.733325481414795\n",
      "Step: 3582, Loss: 0.915999174118042, Accuracy: 1.0, Computation time: 1.4998180866241455\n",
      "Step: 3583, Loss: 0.9159126877784729, Accuracy: 1.0, Computation time: 1.6168572902679443\n",
      "Step: 3584, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.3517727851867676\n",
      "Step: 3585, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.5031516551971436\n",
      "Step: 3586, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.7377686500549316\n",
      "Step: 3587, Loss: 0.937617838382721, Accuracy: 0.96875, Computation time: 1.5291593074798584\n",
      "Step: 3588, Loss: 0.9397299289703369, Accuracy: 0.96875, Computation time: 1.7514054775238037\n",
      "Step: 3589, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.649263858795166\n",
      "Step: 3590, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.757399320602417\n",
      "Step: 3591, Loss: 0.9374483227729797, Accuracy: 0.96875, Computation time: 1.5576930046081543\n",
      "Step: 3592, Loss: 0.9160544872283936, Accuracy: 1.0, Computation time: 1.3802127838134766\n",
      "Step: 3593, Loss: 0.9289666414260864, Accuracy: 0.96875, Computation time: 2.7566657066345215\n",
      "Step: 3594, Loss: 0.9160704016685486, Accuracy: 1.0, Computation time: 1.9498388767242432\n",
      "Step: 3595, Loss: 0.916052520275116, Accuracy: 1.0, Computation time: 1.4905056953430176\n",
      "Step: 3596, Loss: 0.9161772727966309, Accuracy: 1.0, Computation time: 1.8299102783203125\n",
      "Step: 3597, Loss: 0.9375248551368713, Accuracy: 0.96875, Computation time: 1.6295008659362793\n",
      "Step: 3598, Loss: 0.9400826096534729, Accuracy: 0.96875, Computation time: 2.327664852142334\n",
      "Step: 3599, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.573241949081421\n",
      "Step: 3600, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.8823421001434326\n",
      "Step: 3601, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.9503545761108398\n",
      "Step: 3602, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.620941162109375\n",
      "Step: 3603, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.762108325958252\n",
      "Step: 3604, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.3974308967590332\n",
      "Step: 3605, Loss: 0.9375675916671753, Accuracy: 0.96875, Computation time: 1.344177007675171\n",
      "Step: 3606, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 1.713428020477295\n",
      "Step: 3607, Loss: 0.9159315228462219, Accuracy: 1.0, Computation time: 1.9678730964660645\n",
      "Step: 3608, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.434506893157959\n",
      "Step: 3609, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.470890760421753\n",
      "Step: 3610, Loss: 0.9375010132789612, Accuracy: 0.96875, Computation time: 1.4015872478485107\n",
      "Step: 3611, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.5683059692382812\n",
      "Step: 3612, Loss: 0.9166803956031799, Accuracy: 1.0, Computation time: 1.4178617000579834\n",
      "Step: 3613, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.55611252784729\n",
      "########################\n",
      "Test loss: 1.1240077018737793, Test Accuracy_epoch26: 0.6970185041427612\n",
      "########################\n",
      "Step: 3614, Loss: 0.9368715882301331, Accuracy: 0.96875, Computation time: 2.207921266555786\n",
      "Step: 3615, Loss: 0.9329455494880676, Accuracy: 0.96875, Computation time: 1.387157917022705\n",
      "Step: 3616, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.3358526229858398\n",
      "Step: 3617, Loss: 0.9184842705726624, Accuracy: 1.0, Computation time: 1.6631603240966797\n",
      "Step: 3618, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 1.4737117290496826\n",
      "Step: 3619, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.543696641921997\n",
      "Step: 3620, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.925947666168213\n",
      "Step: 3621, Loss: 0.9159144163131714, Accuracy: 1.0, Computation time: 1.6979436874389648\n",
      "Step: 3622, Loss: 0.937477707862854, Accuracy: 0.96875, Computation time: 1.4674487113952637\n",
      "Step: 3623, Loss: 0.9375997185707092, Accuracy: 0.96875, Computation time: 1.5321502685546875\n",
      "Step: 3624, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.3943531513214111\n",
      "Step: 3625, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.4874825477600098\n",
      "Step: 3626, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.5597326755523682\n",
      "Step: 3627, Loss: 0.9376217126846313, Accuracy: 0.96875, Computation time: 2.0129992961883545\n",
      "Step: 3628, Loss: 0.915901780128479, Accuracy: 1.0, Computation time: 1.4574120044708252\n",
      "Step: 3629, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.8785839080810547\n",
      "Step: 3630, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.410651683807373\n",
      "Step: 3631, Loss: 0.9159142374992371, Accuracy: 1.0, Computation time: 1.5037298202514648\n",
      "Step: 3632, Loss: 0.9160083532333374, Accuracy: 1.0, Computation time: 2.018871545791626\n",
      "Step: 3633, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.7581539154052734\n",
      "Step: 3634, Loss: 0.9374772310256958, Accuracy: 0.96875, Computation time: 1.5008881092071533\n",
      "Step: 3635, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 1.4929604530334473\n",
      "Step: 3636, Loss: 0.9159325957298279, Accuracy: 1.0, Computation time: 1.4432523250579834\n",
      "Step: 3637, Loss: 0.9376574158668518, Accuracy: 0.96875, Computation time: 1.344985008239746\n",
      "Step: 3638, Loss: 0.9455318450927734, Accuracy: 0.96875, Computation time: 1.662132740020752\n",
      "Step: 3639, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.6465811729431152\n",
      "Step: 3640, Loss: 0.915916383266449, Accuracy: 1.0, Computation time: 1.566817045211792\n",
      "Step: 3641, Loss: 0.9159640073776245, Accuracy: 1.0, Computation time: 1.6302943229675293\n",
      "Step: 3642, Loss: 0.9159616231918335, Accuracy: 1.0, Computation time: 1.7523667812347412\n",
      "Step: 3643, Loss: 0.9370290637016296, Accuracy: 0.96875, Computation time: 1.9269192218780518\n",
      "Step: 3644, Loss: 0.9159549474716187, Accuracy: 1.0, Computation time: 1.8418946266174316\n",
      "Step: 3645, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 1.7948315143585205\n",
      "Step: 3646, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.7534990310668945\n",
      "Step: 3647, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.5026252269744873\n",
      "Step: 3648, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.3709657192230225\n",
      "Step: 3649, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.5439624786376953\n",
      "Step: 3650, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.4626483917236328\n",
      "Step: 3651, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.3368840217590332\n",
      "Step: 3652, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.6827597618103027\n",
      "Step: 3653, Loss: 0.9159621596336365, Accuracy: 1.0, Computation time: 1.457902431488037\n",
      "Step: 3654, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.4039700031280518\n",
      "Step: 3655, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.4684834480285645\n",
      "Step: 3656, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.418320655822754\n",
      "Step: 3657, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.597062110900879\n",
      "Step: 3658, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.4901916980743408\n",
      "Step: 3659, Loss: 0.9168028831481934, Accuracy: 1.0, Computation time: 2.0003926753997803\n",
      "Step: 3660, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.5071656703948975\n",
      "Step: 3661, Loss: 0.916352391242981, Accuracy: 1.0, Computation time: 1.838047742843628\n",
      "Step: 3662, Loss: 0.9221288561820984, Accuracy: 1.0, Computation time: 1.7960097789764404\n",
      "Step: 3663, Loss: 0.9160362482070923, Accuracy: 1.0, Computation time: 1.6105427742004395\n",
      "Step: 3664, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.8158950805664062\n",
      "Step: 3665, Loss: 0.9220855832099915, Accuracy: 1.0, Computation time: 1.9988040924072266\n",
      "Step: 3666, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.748464584350586\n",
      "Step: 3667, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.7951388359069824\n",
      "Step: 3668, Loss: 0.9160482883453369, Accuracy: 1.0, Computation time: 1.5063469409942627\n",
      "Step: 3669, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.606666088104248\n",
      "Step: 3670, Loss: 0.9376950263977051, Accuracy: 0.96875, Computation time: 1.268798589706421\n",
      "Step: 3671, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 1.7894923686981201\n",
      "Step: 3672, Loss: 0.9245023131370544, Accuracy: 1.0, Computation time: 1.6865041255950928\n",
      "Step: 3673, Loss: 0.9422945380210876, Accuracy: 0.96875, Computation time: 1.9386706352233887\n",
      "Step: 3674, Loss: 0.9159416556358337, Accuracy: 1.0, Computation time: 1.5882377624511719\n",
      "Step: 3675, Loss: 0.9159687161445618, Accuracy: 1.0, Computation time: 1.2078678607940674\n",
      "Step: 3676, Loss: 0.9160268306732178, Accuracy: 1.0, Computation time: 1.3129360675811768\n",
      "Step: 3677, Loss: 0.9160054326057434, Accuracy: 1.0, Computation time: 2.6940560340881348\n",
      "Step: 3678, Loss: 0.9160441756248474, Accuracy: 1.0, Computation time: 1.8820104598999023\n",
      "Step: 3679, Loss: 0.9186582565307617, Accuracy: 1.0, Computation time: 1.4059033393859863\n",
      "Step: 3680, Loss: 0.9295584559440613, Accuracy: 0.96875, Computation time: 1.7970633506774902\n",
      "Step: 3681, Loss: 0.9167783260345459, Accuracy: 1.0, Computation time: 1.9524238109588623\n",
      "Step: 3682, Loss: 0.9364564418792725, Accuracy: 0.96875, Computation time: 2.342442512512207\n",
      "Step: 3683, Loss: 0.9159905910491943, Accuracy: 1.0, Computation time: 1.6752848625183105\n",
      "Step: 3684, Loss: 0.916183352470398, Accuracy: 1.0, Computation time: 1.8654804229736328\n",
      "Step: 3685, Loss: 0.9162026047706604, Accuracy: 1.0, Computation time: 1.5680365562438965\n",
      "Step: 3686, Loss: 0.9208742380142212, Accuracy: 1.0, Computation time: 4.404075384140015\n",
      "Step: 3687, Loss: 0.938173770904541, Accuracy: 0.96875, Computation time: 1.5173454284667969\n",
      "Step: 3688, Loss: 0.9160916209220886, Accuracy: 1.0, Computation time: 1.8596394062042236\n",
      "Step: 3689, Loss: 0.9314787983894348, Accuracy: 0.96875, Computation time: 2.6725831031799316\n",
      "Step: 3690, Loss: 0.9160012006759644, Accuracy: 1.0, Computation time: 1.65006422996521\n",
      "Step: 3691, Loss: 0.9160135984420776, Accuracy: 1.0, Computation time: 1.6530110836029053\n",
      "Step: 3692, Loss: 0.916298508644104, Accuracy: 1.0, Computation time: 1.6547887325286865\n",
      "Step: 3693, Loss: 0.9178165197372437, Accuracy: 1.0, Computation time: 1.8851449489593506\n",
      "Step: 3694, Loss: 0.9161406755447388, Accuracy: 1.0, Computation time: 1.3919222354888916\n",
      "Step: 3695, Loss: 0.9160813093185425, Accuracy: 1.0, Computation time: 1.575941562652588\n",
      "Step: 3696, Loss: 0.9160332083702087, Accuracy: 1.0, Computation time: 1.4360148906707764\n",
      "Step: 3697, Loss: 0.9161102771759033, Accuracy: 1.0, Computation time: 2.382598400115967\n",
      "Step: 3698, Loss: 0.9160782098770142, Accuracy: 1.0, Computation time: 1.7327988147735596\n",
      "Step: 3699, Loss: 0.9378072619438171, Accuracy: 0.96875, Computation time: 1.5936758518218994\n",
      "Step: 3700, Loss: 0.9376003742218018, Accuracy: 0.96875, Computation time: 1.8442342281341553\n",
      "Step: 3701, Loss: 0.9162195324897766, Accuracy: 1.0, Computation time: 1.811075210571289\n",
      "Step: 3702, Loss: 0.9187015295028687, Accuracy: 1.0, Computation time: 1.5416173934936523\n",
      "Step: 3703, Loss: 0.9160373210906982, Accuracy: 1.0, Computation time: 1.4304101467132568\n",
      "Step: 3704, Loss: 0.916161298751831, Accuracy: 1.0, Computation time: 1.4293382167816162\n",
      "Step: 3705, Loss: 0.916092038154602, Accuracy: 1.0, Computation time: 2.1649272441864014\n",
      "Step: 3706, Loss: 0.9164095520973206, Accuracy: 1.0, Computation time: 1.643324613571167\n",
      "Step: 3707, Loss: 0.9543147087097168, Accuracy: 0.9375, Computation time: 2.552340269088745\n",
      "Step: 3708, Loss: 0.9164060950279236, Accuracy: 1.0, Computation time: 1.828953742980957\n",
      "Step: 3709, Loss: 0.9167447090148926, Accuracy: 1.0, Computation time: 1.5779922008514404\n",
      "Step: 3710, Loss: 0.9581417441368103, Accuracy: 0.9375, Computation time: 1.7725486755371094\n",
      "Step: 3711, Loss: 0.9163197875022888, Accuracy: 1.0, Computation time: 1.7618193626403809\n",
      "Step: 3712, Loss: 0.9166063666343689, Accuracy: 1.0, Computation time: 1.4666903018951416\n",
      "Step: 3713, Loss: 0.9368323087692261, Accuracy: 0.96875, Computation time: 1.8565032482147217\n",
      "Step: 3714, Loss: 0.9163239598274231, Accuracy: 1.0, Computation time: 2.0144121646881104\n",
      "Step: 3715, Loss: 0.9164398312568665, Accuracy: 1.0, Computation time: 1.7015841007232666\n",
      "Step: 3716, Loss: 0.937902569770813, Accuracy: 0.96875, Computation time: 1.9733421802520752\n",
      "Step: 3717, Loss: 0.9163482785224915, Accuracy: 1.0, Computation time: 1.46388840675354\n",
      "Step: 3718, Loss: 0.937897264957428, Accuracy: 0.96875, Computation time: 1.407099962234497\n",
      "Step: 3719, Loss: 0.9163036346435547, Accuracy: 1.0, Computation time: 1.6190617084503174\n",
      "Step: 3720, Loss: 0.9236346483230591, Accuracy: 1.0, Computation time: 2.1111721992492676\n",
      "Step: 3721, Loss: 0.9162604212760925, Accuracy: 1.0, Computation time: 1.6957578659057617\n",
      "Step: 3722, Loss: 0.9162181615829468, Accuracy: 1.0, Computation time: 1.3529157638549805\n",
      "Step: 3723, Loss: 0.9378722906112671, Accuracy: 0.96875, Computation time: 1.3877358436584473\n",
      "Step: 3724, Loss: 0.9163002967834473, Accuracy: 1.0, Computation time: 1.4049134254455566\n",
      "Step: 3725, Loss: 0.9163463711738586, Accuracy: 1.0, Computation time: 1.630687952041626\n",
      "Step: 3726, Loss: 0.9159843921661377, Accuracy: 1.0, Computation time: 1.8200387954711914\n",
      "Step: 3727, Loss: 0.9160634279251099, Accuracy: 1.0, Computation time: 1.3832077980041504\n",
      "Step: 3728, Loss: 0.9160396456718445, Accuracy: 1.0, Computation time: 1.325106143951416\n",
      "Step: 3729, Loss: 0.9175861477851868, Accuracy: 1.0, Computation time: 1.604858636856079\n",
      "Step: 3730, Loss: 0.9239089488983154, Accuracy: 1.0, Computation time: 1.832000970840454\n",
      "Step: 3731, Loss: 0.9161478281021118, Accuracy: 1.0, Computation time: 1.685591697692871\n",
      "Step: 3732, Loss: 0.9377009272575378, Accuracy: 0.96875, Computation time: 1.4829435348510742\n",
      "Step: 3733, Loss: 0.9162122011184692, Accuracy: 1.0, Computation time: 1.3931353092193604\n",
      "Step: 3734, Loss: 0.916191816329956, Accuracy: 1.0, Computation time: 1.4929075241088867\n",
      "Step: 3735, Loss: 0.9161291122436523, Accuracy: 1.0, Computation time: 1.4435880184173584\n",
      "Step: 3736, Loss: 0.9161103963851929, Accuracy: 1.0, Computation time: 1.2563157081604004\n",
      "Step: 3737, Loss: 0.9373254179954529, Accuracy: 0.96875, Computation time: 1.9849276542663574\n",
      "Step: 3738, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.850891351699829\n",
      "Step: 3739, Loss: 0.923916220664978, Accuracy: 1.0, Computation time: 2.433759927749634\n",
      "Step: 3740, Loss: 0.937232255935669, Accuracy: 0.96875, Computation time: 1.640470027923584\n",
      "Step: 3741, Loss: 0.9160414934158325, Accuracy: 1.0, Computation time: 1.2781851291656494\n",
      "Step: 3742, Loss: 0.9161167144775391, Accuracy: 1.0, Computation time: 1.4677910804748535\n",
      "Step: 3743, Loss: 0.9162235260009766, Accuracy: 1.0, Computation time: 1.8126928806304932\n",
      "Step: 3744, Loss: 0.916149377822876, Accuracy: 1.0, Computation time: 1.37178635597229\n",
      "Step: 3745, Loss: 0.9162667393684387, Accuracy: 1.0, Computation time: 1.2639110088348389\n",
      "Step: 3746, Loss: 0.9161034226417542, Accuracy: 1.0, Computation time: 1.5439956188201904\n",
      "Step: 3747, Loss: 0.91596919298172, Accuracy: 1.0, Computation time: 1.309152603149414\n",
      "Step: 3748, Loss: 0.9169868230819702, Accuracy: 1.0, Computation time: 1.5929994583129883\n",
      "Step: 3749, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 1.5240578651428223\n",
      "Step: 3750, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 1.4943182468414307\n",
      "Step: 3751, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 1.3571720123291016\n",
      "Step: 3752, Loss: 0.9160473942756653, Accuracy: 1.0, Computation time: 1.2796390056610107\n",
      "########################\n",
      "Test loss: 1.124763011932373, Test Accuracy_epoch27: 0.698630154132843\n",
      "########################\n",
      "Step: 3753, Loss: 0.93278568983078, Accuracy: 0.96875, Computation time: 1.6528544425964355\n",
      "Step: 3754, Loss: 0.9174534678459167, Accuracy: 1.0, Computation time: 1.3285925388336182\n",
      "Step: 3755, Loss: 0.9161058664321899, Accuracy: 1.0, Computation time: 1.387098789215088\n",
      "Step: 3756, Loss: 0.9163157939910889, Accuracy: 1.0, Computation time: 2.580237627029419\n",
      "Step: 3757, Loss: 0.916000485420227, Accuracy: 1.0, Computation time: 1.7037816047668457\n",
      "Step: 3758, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.1924207210540771\n",
      "Step: 3759, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 1.6188225746154785\n",
      "Step: 3760, Loss: 0.9371907114982605, Accuracy: 0.96875, Computation time: 1.6553637981414795\n",
      "Step: 3761, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.6905624866485596\n",
      "Step: 3762, Loss: 0.9180682301521301, Accuracy: 1.0, Computation time: 1.7343604564666748\n",
      "Step: 3763, Loss: 0.9162783622741699, Accuracy: 1.0, Computation time: 1.7325654029846191\n",
      "Step: 3764, Loss: 0.9159966111183167, Accuracy: 1.0, Computation time: 1.5624256134033203\n",
      "Step: 3765, Loss: 0.916484534740448, Accuracy: 1.0, Computation time: 1.5240397453308105\n",
      "Step: 3766, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.5902748107910156\n",
      "Step: 3767, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 1.4690077304840088\n",
      "Step: 3768, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.2924227714538574\n",
      "Step: 3769, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.62587571144104\n",
      "Step: 3770, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.7729811668395996\n",
      "Step: 3771, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.6741185188293457\n",
      "Step: 3772, Loss: 0.9159294962882996, Accuracy: 1.0, Computation time: 1.3023085594177246\n",
      "Step: 3773, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.8610856533050537\n",
      "Step: 3774, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.4150285720825195\n",
      "Step: 3775, Loss: 0.9159764051437378, Accuracy: 1.0, Computation time: 1.929887056350708\n",
      "Step: 3776, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.4806275367736816\n",
      "Step: 3777, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.376542329788208\n",
      "Step: 3778, Loss: 0.9375666379928589, Accuracy: 0.96875, Computation time: 1.823315143585205\n",
      "Step: 3779, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.7424333095550537\n",
      "Step: 3780, Loss: 0.9159648418426514, Accuracy: 1.0, Computation time: 1.4604883193969727\n",
      "Step: 3781, Loss: 0.9807606935501099, Accuracy: 0.90625, Computation time: 1.7038521766662598\n",
      "Step: 3782, Loss: 0.9374469518661499, Accuracy: 0.96875, Computation time: 1.7660455703735352\n",
      "Step: 3783, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 2.0185389518737793\n",
      "Step: 3784, Loss: 0.9201288223266602, Accuracy: 1.0, Computation time: 2.147571325302124\n",
      "Step: 3785, Loss: 0.9159599542617798, Accuracy: 1.0, Computation time: 1.511082410812378\n",
      "Step: 3786, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.5356979370117188\n",
      "Step: 3787, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.2830135822296143\n",
      "Step: 3788, Loss: 0.9166396260261536, Accuracy: 1.0, Computation time: 1.9788062572479248\n",
      "Step: 3789, Loss: 0.9359996318817139, Accuracy: 0.96875, Computation time: 1.622074842453003\n",
      "Step: 3790, Loss: 0.9162125587463379, Accuracy: 1.0, Computation time: 1.8569846153259277\n",
      "Step: 3791, Loss: 0.9209046363830566, Accuracy: 1.0, Computation time: 2.0271029472351074\n",
      "Step: 3792, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 1.377917766571045\n",
      "Step: 3793, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 1.6359186172485352\n",
      "Step: 3794, Loss: 0.9161409139633179, Accuracy: 1.0, Computation time: 1.7017617225646973\n",
      "Step: 3795, Loss: 0.9161011576652527, Accuracy: 1.0, Computation time: 1.782148838043213\n",
      "Step: 3796, Loss: 0.916255533695221, Accuracy: 1.0, Computation time: 1.9219136238098145\n",
      "Step: 3797, Loss: 0.9160231351852417, Accuracy: 1.0, Computation time: 1.9236669540405273\n",
      "Step: 3798, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 1.462580919265747\n",
      "Step: 3799, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.5072178840637207\n",
      "Step: 3800, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.6597435474395752\n",
      "Step: 3801, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.8352739810943604\n",
      "Step: 3802, Loss: 0.9159573316574097, Accuracy: 1.0, Computation time: 1.8802974224090576\n",
      "Step: 3803, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.561103343963623\n",
      "Step: 3804, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.8600423336029053\n",
      "Step: 3805, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 1.3098020553588867\n",
      "Step: 3806, Loss: 0.9368664026260376, Accuracy: 0.96875, Computation time: 2.0711631774902344\n",
      "Step: 3807, Loss: 0.915974497795105, Accuracy: 1.0, Computation time: 1.79158353805542\n",
      "Step: 3808, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.873086929321289\n",
      "Step: 3809, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 1.6984944343566895\n",
      "Step: 3810, Loss: 0.9374881982803345, Accuracy: 0.96875, Computation time: 1.7385339736938477\n",
      "Step: 3811, Loss: 0.9375612735748291, Accuracy: 0.96875, Computation time: 1.2367281913757324\n",
      "Step: 3812, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.394860029220581\n",
      "Step: 3813, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 1.6039745807647705\n",
      "Step: 3814, Loss: 0.9369742274284363, Accuracy: 0.96875, Computation time: 1.435765266418457\n",
      "Step: 3815, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.4769349098205566\n",
      "Step: 3816, Loss: 0.9264811277389526, Accuracy: 0.96875, Computation time: 1.5196757316589355\n",
      "Step: 3817, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.451186180114746\n",
      "Step: 3818, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 1.79539155960083\n",
      "Step: 3819, Loss: 0.937603235244751, Accuracy: 0.96875, Computation time: 1.6250267028808594\n",
      "Step: 3820, Loss: 0.9164085984230042, Accuracy: 1.0, Computation time: 2.14290189743042\n",
      "Step: 3821, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.6985642910003662\n",
      "Step: 3822, Loss: 0.9159696102142334, Accuracy: 1.0, Computation time: 1.542611837387085\n",
      "Step: 3823, Loss: 0.9483305215835571, Accuracy: 0.9375, Computation time: 2.040410280227661\n",
      "Step: 3824, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 2.6398351192474365\n",
      "Step: 3825, Loss: 0.9159667491912842, Accuracy: 1.0, Computation time: 1.7497389316558838\n",
      "Step: 3826, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 1.6530680656433105\n",
      "Step: 3827, Loss: 0.915937066078186, Accuracy: 1.0, Computation time: 1.394134521484375\n",
      "Step: 3828, Loss: 0.9346404671669006, Accuracy: 0.96875, Computation time: 2.764148473739624\n",
      "Step: 3829, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.3576674461364746\n",
      "Step: 3830, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.4690759181976318\n",
      "Step: 3831, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.515768051147461\n",
      "Step: 3832, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 2.7849550247192383\n",
      "Step: 3833, Loss: 0.9171783924102783, Accuracy: 1.0, Computation time: 2.294283628463745\n",
      "Step: 3834, Loss: 0.9505192041397095, Accuracy: 0.9375, Computation time: 2.3260338306427\n",
      "Step: 3835, Loss: 0.9159358143806458, Accuracy: 1.0, Computation time: 1.8376431465148926\n",
      "Step: 3836, Loss: 0.9160245060920715, Accuracy: 1.0, Computation time: 1.7768430709838867\n",
      "Step: 3837, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.6166095733642578\n",
      "Step: 3838, Loss: 0.9160968065261841, Accuracy: 1.0, Computation time: 1.7487874031066895\n",
      "Step: 3839, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.4976303577423096\n",
      "Step: 3840, Loss: 0.9160892963409424, Accuracy: 1.0, Computation time: 1.6456053256988525\n",
      "Step: 3841, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 1.5041217803955078\n",
      "Step: 3842, Loss: 0.9161199331283569, Accuracy: 1.0, Computation time: 1.3105370998382568\n",
      "Step: 3843, Loss: 0.9160259962081909, Accuracy: 1.0, Computation time: 1.878209114074707\n",
      "Step: 3844, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.4956672191619873\n",
      "Step: 3845, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 1.5308544635772705\n",
      "Step: 3846, Loss: 0.9160858988761902, Accuracy: 1.0, Computation time: 1.5934438705444336\n",
      "Step: 3847, Loss: 0.936409056186676, Accuracy: 0.96875, Computation time: 1.5594561100006104\n",
      "Step: 3848, Loss: 0.937917947769165, Accuracy: 0.96875, Computation time: 2.1966989040374756\n",
      "Step: 3849, Loss: 0.9167294502258301, Accuracy: 1.0, Computation time: 1.5784473419189453\n",
      "Step: 3850, Loss: 0.9159213900566101, Accuracy: 1.0, Computation time: 1.3271827697753906\n",
      "Step: 3851, Loss: 0.937167763710022, Accuracy: 0.96875, Computation time: 1.6076288223266602\n",
      "Step: 3852, Loss: 0.9201833009719849, Accuracy: 1.0, Computation time: 2.436903715133667\n",
      "Step: 3853, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.2522273063659668\n",
      "Step: 3854, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.6021578311920166\n",
      "Step: 3855, Loss: 0.9159354567527771, Accuracy: 1.0, Computation time: 1.493882417678833\n",
      "Step: 3856, Loss: 0.916024386882782, Accuracy: 1.0, Computation time: 1.478102207183838\n",
      "Step: 3857, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.5932965278625488\n",
      "Step: 3858, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.1863422393798828\n",
      "Step: 3859, Loss: 0.9285265207290649, Accuracy: 0.96875, Computation time: 1.7814114093780518\n",
      "Step: 3860, Loss: 0.9161426424980164, Accuracy: 1.0, Computation time: 1.5633502006530762\n",
      "Step: 3861, Loss: 0.9374445080757141, Accuracy: 0.96875, Computation time: 1.981848955154419\n",
      "Step: 3862, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 1.3653929233551025\n",
      "Step: 3863, Loss: 0.9572063684463501, Accuracy: 0.9375, Computation time: 1.815185308456421\n",
      "Step: 3864, Loss: 0.9163362979888916, Accuracy: 1.0, Computation time: 1.3371665477752686\n",
      "Step: 3865, Loss: 0.918884813785553, Accuracy: 1.0, Computation time: 1.3040764331817627\n",
      "Step: 3866, Loss: 0.91605544090271, Accuracy: 1.0, Computation time: 1.2790987491607666\n",
      "Step: 3867, Loss: 0.9159999489784241, Accuracy: 1.0, Computation time: 1.2256739139556885\n",
      "Step: 3868, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.3888235092163086\n",
      "Step: 3869, Loss: 0.9159253239631653, Accuracy: 1.0, Computation time: 1.724353551864624\n",
      "Step: 3870, Loss: 0.9375443458557129, Accuracy: 0.96875, Computation time: 1.8857698440551758\n",
      "Step: 3871, Loss: 0.9371172785758972, Accuracy: 0.96875, Computation time: 2.2768797874450684\n",
      "Step: 3872, Loss: 0.916008472442627, Accuracy: 1.0, Computation time: 2.17000150680542\n",
      "Step: 3873, Loss: 0.9159905314445496, Accuracy: 1.0, Computation time: 1.7348592281341553\n",
      "Step: 3874, Loss: 0.9160391092300415, Accuracy: 1.0, Computation time: 2.038701295852661\n",
      "Step: 3875, Loss: 0.9376915097236633, Accuracy: 0.96875, Computation time: 2.4129621982574463\n",
      "Step: 3876, Loss: 0.9375646710395813, Accuracy: 0.96875, Computation time: 2.0790305137634277\n",
      "Step: 3877, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.8792667388916016\n",
      "Step: 3878, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.9737331867218018\n",
      "Step: 3879, Loss: 0.9159741401672363, Accuracy: 1.0, Computation time: 2.233618974685669\n",
      "Step: 3880, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.964369773864746\n",
      "Step: 3881, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.7751188278198242\n",
      "Step: 3882, Loss: 0.9159536361694336, Accuracy: 1.0, Computation time: 2.9599850177764893\n",
      "Step: 3883, Loss: 0.9159270524978638, Accuracy: 1.0, Computation time: 1.9896893501281738\n",
      "Step: 3884, Loss: 0.9161673784255981, Accuracy: 1.0, Computation time: 2.2924249172210693\n",
      "Step: 3885, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 2.0964508056640625\n",
      "Step: 3886, Loss: 0.9205299615859985, Accuracy: 1.0, Computation time: 2.8421478271484375\n",
      "Step: 3887, Loss: 0.9179813861846924, Accuracy: 1.0, Computation time: 2.5736825466156006\n",
      "Step: 3888, Loss: 0.9375775456428528, Accuracy: 0.96875, Computation time: 2.625065565109253\n",
      "Step: 3889, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 1.8474516868591309\n",
      "Step: 3890, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.9805800914764404\n",
      "Step: 3891, Loss: 0.9159202575683594, Accuracy: 1.0, Computation time: 1.723031997680664\n",
      "########################\n",
      "Test loss: 1.1307151317596436, Test Accuracy_epoch28: 0.6881546974182129\n",
      "########################\n",
      "Step: 3892, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 2.2941579818725586\n",
      "Step: 3893, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.9634649753570557\n",
      "Step: 3894, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 2.287525177001953\n",
      "Step: 3895, Loss: 0.9160481095314026, Accuracy: 1.0, Computation time: 2.096975088119507\n",
      "Step: 3896, Loss: 0.9160378575325012, Accuracy: 1.0, Computation time: 2.6097030639648438\n",
      "Step: 3897, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 1.7338042259216309\n",
      "Step: 3898, Loss: 0.916664719581604, Accuracy: 1.0, Computation time: 3.903714895248413\n",
      "Step: 3899, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.8177051544189453\n",
      "Step: 3900, Loss: 0.9380688667297363, Accuracy: 0.96875, Computation time: 2.3406333923339844\n",
      "Step: 3901, Loss: 0.9375774264335632, Accuracy: 0.96875, Computation time: 2.1025655269622803\n",
      "Step: 3902, Loss: 0.9163200259208679, Accuracy: 1.0, Computation time: 2.605745315551758\n",
      "Step: 3903, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 2.019550085067749\n",
      "Step: 3904, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.8713617324829102\n",
      "Step: 3905, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.726775884628296\n",
      "Step: 3906, Loss: 0.9375541806221008, Accuracy: 0.96875, Computation time: 1.9709548950195312\n",
      "Step: 3907, Loss: 0.9368100762367249, Accuracy: 0.96875, Computation time: 2.3768696784973145\n",
      "Step: 3908, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 2.3563945293426514\n",
      "Step: 3909, Loss: 0.9164941310882568, Accuracy: 1.0, Computation time: 2.710008382797241\n",
      "Step: 3910, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 2.4304347038269043\n",
      "Step: 3911, Loss: 0.9159679412841797, Accuracy: 1.0, Computation time: 2.150611639022827\n",
      "Step: 3912, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 2.4049909114837646\n",
      "Step: 3913, Loss: 0.9160434007644653, Accuracy: 1.0, Computation time: 2.4684245586395264\n",
      "Step: 3914, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.8133466243743896\n",
      "Step: 3915, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 2.1309866905212402\n",
      "Step: 3916, Loss: 0.9196168780326843, Accuracy: 1.0, Computation time: 2.196397542953491\n",
      "Step: 3917, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 2.1143717765808105\n",
      "Step: 3918, Loss: 0.937538206577301, Accuracy: 0.96875, Computation time: 2.1515727043151855\n",
      "Step: 3919, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 2.1809651851654053\n",
      "Step: 3920, Loss: 0.9203383326530457, Accuracy: 1.0, Computation time: 3.3435044288635254\n",
      "Step: 3921, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.885552167892456\n",
      "Step: 3922, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 1.9213573932647705\n",
      "Step: 3923, Loss: 0.9375037550926208, Accuracy: 0.96875, Computation time: 2.908827066421509\n",
      "Step: 3924, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 1.8634560108184814\n",
      "Step: 3925, Loss: 0.9162814617156982, Accuracy: 1.0, Computation time: 2.1294143199920654\n",
      "Step: 3926, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.9579105377197266\n",
      "Step: 3927, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 2.3126566410064697\n",
      "Step: 3928, Loss: 0.9591929912567139, Accuracy: 0.9375, Computation time: 2.360109806060791\n",
      "Step: 3929, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 2.0693373680114746\n",
      "Step: 3930, Loss: 0.980668842792511, Accuracy: 0.90625, Computation time: 1.8447396755218506\n",
      "Step: 3931, Loss: 0.9253042936325073, Accuracy: 0.96875, Computation time: 2.187638998031616\n",
      "Step: 3932, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.5196340084075928\n",
      "Step: 3933, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.4649653434753418\n",
      "Step: 3934, Loss: 0.9161487221717834, Accuracy: 1.0, Computation time: 1.9605472087860107\n",
      "Step: 3935, Loss: 0.9359251856803894, Accuracy: 0.96875, Computation time: 1.7062971591949463\n",
      "Step: 3936, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.417090892791748\n",
      "Step: 3937, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 1.3218746185302734\n",
      "Step: 3938, Loss: 0.9159083962440491, Accuracy: 1.0, Computation time: 1.2138309478759766\n",
      "Step: 3939, Loss: 0.9159637689590454, Accuracy: 1.0, Computation time: 1.489499807357788\n",
      "Step: 3940, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.3013255596160889\n",
      "Step: 3941, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.4903099536895752\n",
      "Step: 3942, Loss: 0.9376246929168701, Accuracy: 0.96875, Computation time: 1.522641658782959\n",
      "Step: 3943, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.376763105392456\n",
      "Step: 3944, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.6913166046142578\n",
      "Step: 3945, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.3339557647705078\n",
      "Step: 3946, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.0868773460388184\n",
      "Step: 3947, Loss: 0.9159212708473206, Accuracy: 1.0, Computation time: 1.6255042552947998\n",
      "Step: 3948, Loss: 0.9374256134033203, Accuracy: 0.96875, Computation time: 1.3671483993530273\n",
      "Step: 3949, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.3022193908691406\n",
      "Step: 3950, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 1.505413293838501\n",
      "Step: 3951, Loss: 0.9375283122062683, Accuracy: 0.96875, Computation time: 1.2603635787963867\n",
      "Step: 3952, Loss: 0.9159725308418274, Accuracy: 1.0, Computation time: 1.772118330001831\n",
      "Step: 3953, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.49405837059021\n",
      "Step: 3954, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 1.665449857711792\n",
      "Step: 3955, Loss: 0.9484294056892395, Accuracy: 0.9375, Computation time: 3.278862476348877\n",
      "Step: 3956, Loss: 0.9160153865814209, Accuracy: 1.0, Computation time: 1.5497219562530518\n",
      "Step: 3957, Loss: 0.9273245334625244, Accuracy: 0.96875, Computation time: 2.333652973175049\n",
      "Step: 3958, Loss: 0.9263304471969604, Accuracy: 0.96875, Computation time: 2.3693010807037354\n",
      "Step: 3959, Loss: 0.9266747832298279, Accuracy: 0.96875, Computation time: 2.200939893722534\n",
      "Step: 3960, Loss: 0.9163867831230164, Accuracy: 1.0, Computation time: 1.8411448001861572\n",
      "Step: 3961, Loss: 0.9271393418312073, Accuracy: 0.96875, Computation time: 1.9366726875305176\n",
      "Step: 3962, Loss: 0.9172714352607727, Accuracy: 1.0, Computation time: 1.538952350616455\n",
      "Step: 3963, Loss: 0.9462168216705322, Accuracy: 0.96875, Computation time: 1.960921287536621\n",
      "Step: 3964, Loss: 0.9161387085914612, Accuracy: 1.0, Computation time: 1.447556734085083\n",
      "Step: 3965, Loss: 0.9160038232803345, Accuracy: 1.0, Computation time: 1.276580572128296\n",
      "Step: 3966, Loss: 0.916159987449646, Accuracy: 1.0, Computation time: 2.0820772647857666\n",
      "Step: 3967, Loss: 0.956571638584137, Accuracy: 0.9375, Computation time: 2.6588597297668457\n",
      "Step: 3968, Loss: 0.9170487523078918, Accuracy: 1.0, Computation time: 1.4737722873687744\n",
      "Step: 3969, Loss: 0.9175744652748108, Accuracy: 1.0, Computation time: 1.273345947265625\n",
      "Step: 3970, Loss: 0.932534396648407, Accuracy: 0.96875, Computation time: 1.6835963726043701\n",
      "Step: 3971, Loss: 0.9389960765838623, Accuracy: 0.96875, Computation time: 1.2734041213989258\n",
      "Step: 3972, Loss: 0.9171546697616577, Accuracy: 1.0, Computation time: 1.5863478183746338\n",
      "Step: 3973, Loss: 0.916512668132782, Accuracy: 1.0, Computation time: 1.633366584777832\n",
      "Step: 3974, Loss: 0.9163269400596619, Accuracy: 1.0, Computation time: 1.7057983875274658\n",
      "Step: 3975, Loss: 0.9162371754646301, Accuracy: 1.0, Computation time: 1.3163228034973145\n",
      "Step: 3976, Loss: 0.9189128875732422, Accuracy: 1.0, Computation time: 1.7952589988708496\n",
      "Step: 3977, Loss: 0.9162657260894775, Accuracy: 1.0, Computation time: 1.1401865482330322\n",
      "Step: 3978, Loss: 0.9163086414337158, Accuracy: 1.0, Computation time: 1.4025869369506836\n",
      "Step: 3979, Loss: 0.9163377285003662, Accuracy: 1.0, Computation time: 1.3306174278259277\n",
      "Step: 3980, Loss: 0.9163414239883423, Accuracy: 1.0, Computation time: 1.4511768817901611\n",
      "Step: 3981, Loss: 0.9162992238998413, Accuracy: 1.0, Computation time: 1.2249882221221924\n",
      "Step: 3982, Loss: 0.9161821603775024, Accuracy: 1.0, Computation time: 1.2879002094268799\n",
      "Step: 3983, Loss: 0.9159945845603943, Accuracy: 1.0, Computation time: 1.3352289199829102\n",
      "Step: 3984, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.2967267036437988\n",
      "Step: 3985, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.2361507415771484\n",
      "Step: 3986, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 1.2854785919189453\n",
      "Step: 3987, Loss: 0.916002094745636, Accuracy: 1.0, Computation time: 1.3709595203399658\n",
      "Step: 3988, Loss: 0.9358790516853333, Accuracy: 0.96875, Computation time: 1.71164870262146\n",
      "Step: 3989, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 1.3147621154785156\n",
      "Step: 3990, Loss: 0.9159847497940063, Accuracy: 1.0, Computation time: 1.4423143863677979\n",
      "Step: 3991, Loss: 0.9159600138664246, Accuracy: 1.0, Computation time: 1.1437878608703613\n",
      "Step: 3992, Loss: 0.9159421324729919, Accuracy: 1.0, Computation time: 1.146578311920166\n",
      "Step: 3993, Loss: 0.917332112789154, Accuracy: 1.0, Computation time: 2.008446455001831\n",
      "Step: 3994, Loss: 0.9159951210021973, Accuracy: 1.0, Computation time: 1.1644370555877686\n",
      "Step: 3995, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.0155494213104248\n",
      "Step: 3996, Loss: 0.9229173064231873, Accuracy: 1.0, Computation time: 1.7327284812927246\n",
      "Step: 3997, Loss: 0.9159889221191406, Accuracy: 1.0, Computation time: 1.0541784763336182\n",
      "Step: 3998, Loss: 0.9363669157028198, Accuracy: 0.96875, Computation time: 1.5130863189697266\n",
      "Step: 3999, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.1248009204864502\n",
      "Step: 4000, Loss: 0.9376951456069946, Accuracy: 0.96875, Computation time: 1.1651947498321533\n",
      "Step: 4001, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.302459716796875\n",
      "Step: 4002, Loss: 0.916003406047821, Accuracy: 1.0, Computation time: 1.318833589553833\n",
      "Step: 4003, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.3081026077270508\n",
      "Step: 4004, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.1303577423095703\n",
      "Step: 4005, Loss: 0.9181070923805237, Accuracy: 1.0, Computation time: 1.5610942840576172\n",
      "Step: 4006, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.1927120685577393\n",
      "Step: 4007, Loss: 0.9161524772644043, Accuracy: 1.0, Computation time: 1.6573359966278076\n",
      "Step: 4008, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.4026679992675781\n",
      "Step: 4009, Loss: 0.9163140654563904, Accuracy: 1.0, Computation time: 1.2017369270324707\n",
      "Step: 4010, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.3327836990356445\n",
      "Step: 4011, Loss: 0.9340132474899292, Accuracy: 0.96875, Computation time: 1.3280820846557617\n",
      "Step: 4012, Loss: 0.929893970489502, Accuracy: 0.96875, Computation time: 2.281449317932129\n",
      "Step: 4013, Loss: 0.9160593152046204, Accuracy: 1.0, Computation time: 1.4279217720031738\n",
      "Step: 4014, Loss: 0.9376457929611206, Accuracy: 0.96875, Computation time: 1.4058141708374023\n",
      "Step: 4015, Loss: 0.9160752296447754, Accuracy: 1.0, Computation time: 0.9437549114227295\n",
      "Step: 4016, Loss: 0.9161036014556885, Accuracy: 1.0, Computation time: 1.2523093223571777\n",
      "Step: 4017, Loss: 0.9161284565925598, Accuracy: 1.0, Computation time: 1.0626649856567383\n",
      "Step: 4018, Loss: 0.9161586761474609, Accuracy: 1.0, Computation time: 1.2008476257324219\n",
      "Step: 4019, Loss: 0.9160159230232239, Accuracy: 1.0, Computation time: 1.2451910972595215\n",
      "Step: 4020, Loss: 0.9160375595092773, Accuracy: 1.0, Computation time: 1.0536417961120605\n",
      "Step: 4021, Loss: 0.9159607291221619, Accuracy: 1.0, Computation time: 1.3244333267211914\n",
      "Step: 4022, Loss: 0.9373565316200256, Accuracy: 0.96875, Computation time: 1.8305633068084717\n",
      "Step: 4023, Loss: 0.9164099097251892, Accuracy: 1.0, Computation time: 2.095226287841797\n",
      "Step: 4024, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.2185792922973633\n",
      "Step: 4025, Loss: 0.9375123977661133, Accuracy: 0.96875, Computation time: 1.1789650917053223\n",
      "Step: 4026, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.302513837814331\n",
      "Step: 4027, Loss: 0.9160879254341125, Accuracy: 1.0, Computation time: 1.192063331604004\n",
      "Step: 4028, Loss: 0.916329562664032, Accuracy: 1.0, Computation time: 1.2912914752960205\n",
      "Step: 4029, Loss: 0.916141152381897, Accuracy: 1.0, Computation time: 1.5895600318908691\n",
      "Step: 4030, Loss: 0.9161297678947449, Accuracy: 1.0, Computation time: 1.5576457977294922\n",
      "########################\n",
      "Test loss: 1.1228524446487427, Test Accuracy_epoch29: 0.6970185041427612\n",
      "########################\n",
      "Step: 4031, Loss: 0.9594050049781799, Accuracy: 0.9375, Computation time: 1.353297233581543\n",
      "Step: 4032, Loss: 0.9298136830329895, Accuracy: 0.96875, Computation time: 1.8706910610198975\n",
      "Step: 4033, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.2380568981170654\n",
      "Step: 4034, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 1.1675491333007812\n",
      "Step: 4035, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 1.6249608993530273\n",
      "Step: 4036, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 1.13663649559021\n",
      "Step: 4037, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.315119981765747\n",
      "Step: 4038, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.1720104217529297\n",
      "Step: 4039, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.137812852859497\n",
      "Step: 4040, Loss: 0.9402432441711426, Accuracy: 0.96875, Computation time: 1.582815408706665\n",
      "Step: 4041, Loss: 0.9577733874320984, Accuracy: 0.9375, Computation time: 1.3824892044067383\n",
      "Step: 4042, Loss: 0.9161784648895264, Accuracy: 1.0, Computation time: 1.5863313674926758\n",
      "Step: 4043, Loss: 0.9159005284309387, Accuracy: 1.0, Computation time: 1.6189606189727783\n",
      "Step: 4044, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.638573169708252\n",
      "Step: 4045, Loss: 0.9359399676322937, Accuracy: 0.96875, Computation time: 1.824683666229248\n",
      "Step: 4046, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 1.5849411487579346\n",
      "Step: 4047, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.3131141662597656\n",
      "Step: 4048, Loss: 0.9374549984931946, Accuracy: 0.96875, Computation time: 1.5231711864471436\n",
      "Step: 4049, Loss: 0.9161142706871033, Accuracy: 1.0, Computation time: 1.850703239440918\n",
      "Step: 4050, Loss: 0.9159459471702576, Accuracy: 1.0, Computation time: 1.1706593036651611\n",
      "Step: 4051, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.506617784500122\n",
      "Step: 4052, Loss: 0.9160470366477966, Accuracy: 1.0, Computation time: 1.0970702171325684\n",
      "Step: 4053, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.0357639789581299\n",
      "Step: 4054, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.1456494331359863\n",
      "Step: 4055, Loss: 0.9167941808700562, Accuracy: 1.0, Computation time: 1.2179336547851562\n",
      "Step: 4056, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.2664670944213867\n",
      "Step: 4057, Loss: 0.9160805940628052, Accuracy: 1.0, Computation time: 1.7428538799285889\n",
      "Step: 4058, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.4225013256072998\n",
      "Step: 4059, Loss: 0.9375594258308411, Accuracy: 0.96875, Computation time: 1.3731811046600342\n",
      "Step: 4060, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 1.1507744789123535\n",
      "Step: 4061, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.2571179866790771\n",
      "Step: 4062, Loss: 0.9376022219657898, Accuracy: 0.96875, Computation time: 1.2028286457061768\n",
      "Step: 4063, Loss: 0.9158400297164917, Accuracy: 1.0, Computation time: 1.2169227600097656\n",
      "Step: 4064, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.5107495784759521\n",
      "Step: 4065, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.3990111351013184\n",
      "Step: 4066, Loss: 0.9232373833656311, Accuracy: 1.0, Computation time: 1.443094253540039\n",
      "Step: 4067, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.2655913829803467\n",
      "Step: 4068, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.233903169631958\n",
      "Step: 4069, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.4836452007293701\n",
      "Step: 4070, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.5559461116790771\n",
      "Step: 4071, Loss: 0.9382402300834656, Accuracy: 0.96875, Computation time: 1.5930519104003906\n",
      "Step: 4072, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.2314114570617676\n",
      "Step: 4073, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.0811188220977783\n",
      "Step: 4074, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.4013748168945312\n",
      "Step: 4075, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.6887471675872803\n",
      "Step: 4076, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.0550107955932617\n",
      "Step: 4077, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 1.6017584800720215\n",
      "Step: 4078, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.3451571464538574\n",
      "Step: 4079, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.3597970008850098\n",
      "Step: 4080, Loss: 0.9225059747695923, Accuracy: 1.0, Computation time: 1.9058270454406738\n",
      "Step: 4081, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 1.5954093933105469\n",
      "Step: 4082, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.7607917785644531\n",
      "Step: 4083, Loss: 0.9159777760505676, Accuracy: 1.0, Computation time: 1.4540345668792725\n",
      "Step: 4084, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 1.148306131362915\n",
      "Step: 4085, Loss: 0.9355630278587341, Accuracy: 0.96875, Computation time: 1.4358243942260742\n",
      "Step: 4086, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 1.4592323303222656\n",
      "Step: 4087, Loss: 0.9375534057617188, Accuracy: 0.96875, Computation time: 1.143406629562378\n",
      "Step: 4088, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.3828182220458984\n",
      "Step: 4089, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 1.3942909240722656\n",
      "Step: 4090, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 1.4671053886413574\n",
      "Step: 4091, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.4044830799102783\n",
      "Step: 4092, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.3212449550628662\n",
      "Step: 4093, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 1.4626591205596924\n",
      "Step: 4094, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.0740387439727783\n",
      "Step: 4095, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 1.511504888534546\n",
      "Step: 4096, Loss: 0.9376254081726074, Accuracy: 0.96875, Computation time: 1.3297169208526611\n",
      "Step: 4097, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.6420323848724365\n",
      "Step: 4098, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.3650691509246826\n",
      "Step: 4099, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 1.458202838897705\n",
      "Step: 4100, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.3177015781402588\n",
      "Step: 4101, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.4917120933532715\n",
      "Step: 4102, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.6771879196166992\n",
      "Step: 4103, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.3204503059387207\n",
      "Step: 4104, Loss: 0.9161722660064697, Accuracy: 1.0, Computation time: 1.774064302444458\n",
      "Step: 4105, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.5454254150390625\n",
      "Step: 4106, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.9359440803527832\n",
      "Step: 4107, Loss: 0.9160283803939819, Accuracy: 1.0, Computation time: 1.4490382671356201\n",
      "Step: 4108, Loss: 0.9158401489257812, Accuracy: 1.0, Computation time: 1.268754243850708\n",
      "Step: 4109, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.3192369937896729\n",
      "Step: 4110, Loss: 0.9188346862792969, Accuracy: 1.0, Computation time: 1.4867424964904785\n",
      "Step: 4111, Loss: 0.9159973859786987, Accuracy: 1.0, Computation time: 1.2136142253875732\n",
      "Step: 4112, Loss: 0.917015552520752, Accuracy: 1.0, Computation time: 1.7108197212219238\n",
      "Step: 4113, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.1473743915557861\n",
      "Step: 4114, Loss: 0.9375701546669006, Accuracy: 0.96875, Computation time: 1.8438599109649658\n",
      "Step: 4115, Loss: 0.9160196185112, Accuracy: 1.0, Computation time: 1.4915976524353027\n",
      "Step: 4116, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.2555103302001953\n",
      "Step: 4117, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.26491379737854\n",
      "Step: 4118, Loss: 0.9160322546958923, Accuracy: 1.0, Computation time: 1.222909927368164\n",
      "Step: 4119, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.2834222316741943\n",
      "Step: 4120, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.7822012901306152\n",
      "Step: 4121, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.5592563152313232\n",
      "Step: 4122, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 1.4855866432189941\n",
      "Step: 4123, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 1.4678280353546143\n",
      "Step: 4124, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.3444743156433105\n",
      "Step: 4125, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.4269087314605713\n",
      "Step: 4126, Loss: 0.9593390822410583, Accuracy: 0.9375, Computation time: 1.2363324165344238\n",
      "Step: 4127, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.3313937187194824\n",
      "Step: 4128, Loss: 0.9184433221817017, Accuracy: 1.0, Computation time: 1.33176851272583\n",
      "Step: 4129, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.1153433322906494\n",
      "Step: 4130, Loss: 0.9354785084724426, Accuracy: 0.96875, Computation time: 1.6563403606414795\n",
      "Step: 4131, Loss: 0.9376287460327148, Accuracy: 0.96875, Computation time: 1.6353158950805664\n",
      "Step: 4132, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.4196264743804932\n",
      "Step: 4133, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.2759945392608643\n",
      "Step: 4134, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.3861651420593262\n",
      "Step: 4135, Loss: 0.9375007152557373, Accuracy: 0.96875, Computation time: 1.6563074588775635\n",
      "Step: 4136, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 1.4525365829467773\n",
      "Step: 4137, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.2140514850616455\n",
      "Step: 4138, Loss: 0.9161918759346008, Accuracy: 1.0, Computation time: 1.4240517616271973\n",
      "Step: 4139, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.540034532546997\n",
      "Step: 4140, Loss: 0.9171538352966309, Accuracy: 1.0, Computation time: 2.1020681858062744\n",
      "Step: 4141, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.1756823062896729\n",
      "Step: 4142, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.4751534461975098\n",
      "Step: 4143, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.760087251663208\n",
      "Step: 4144, Loss: 0.9280754923820496, Accuracy: 0.96875, Computation time: 2.403167247772217\n",
      "Step: 4145, Loss: 0.9375773668289185, Accuracy: 0.96875, Computation time: 1.6179664134979248\n",
      "Step: 4146, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 1.4093785285949707\n",
      "Step: 4147, Loss: 0.9159926176071167, Accuracy: 1.0, Computation time: 1.287391185760498\n",
      "Step: 4148, Loss: 0.9160257577896118, Accuracy: 1.0, Computation time: 1.6942853927612305\n",
      "Step: 4149, Loss: 0.9160171151161194, Accuracy: 1.0, Computation time: 1.3450536727905273\n",
      "Step: 4150, Loss: 0.9160192012786865, Accuracy: 1.0, Computation time: 1.397014856338501\n",
      "Step: 4151, Loss: 0.9373070001602173, Accuracy: 0.96875, Computation time: 1.277174472808838\n",
      "Step: 4152, Loss: 0.9159685969352722, Accuracy: 1.0, Computation time: 1.3259522914886475\n",
      "Step: 4153, Loss: 0.9375389814376831, Accuracy: 0.96875, Computation time: 1.7007911205291748\n",
      "Step: 4154, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 1.2993121147155762\n",
      "Step: 4155, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.1940455436706543\n",
      "Step: 4156, Loss: 0.9162816405296326, Accuracy: 1.0, Computation time: 1.980262041091919\n",
      "Step: 4157, Loss: 0.9159454107284546, Accuracy: 1.0, Computation time: 1.899712085723877\n",
      "Step: 4158, Loss: 0.9160945415496826, Accuracy: 1.0, Computation time: 1.8061492443084717\n",
      "Step: 4159, Loss: 0.9159329533576965, Accuracy: 1.0, Computation time: 1.7913906574249268\n",
      "Step: 4160, Loss: 0.9161263704299927, Accuracy: 1.0, Computation time: 1.5664777755737305\n",
      "Step: 4161, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.475179672241211\n",
      "Step: 4162, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.4119904041290283\n",
      "Step: 4163, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.4008851051330566\n",
      "Step: 4164, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.3660423755645752\n",
      "Step: 4165, Loss: 0.9592216610908508, Accuracy: 0.9375, Computation time: 1.5891075134277344\n",
      "Step: 4166, Loss: 0.9375478625297546, Accuracy: 0.96875, Computation time: 1.600583553314209\n",
      "Step: 4167, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 1.3086628913879395\n",
      "Step: 4168, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.4118008613586426\n",
      "Step: 4169, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.5978171825408936\n",
      "########################\n",
      "Test loss: 1.1259961128234863, Test Accuracy_epoch30: 0.6946011185646057\n",
      "########################\n",
      "Step: 4170, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.9736359119415283\n",
      "Step: 4171, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.2956786155700684\n",
      "Step: 4172, Loss: 0.9179279208183289, Accuracy: 1.0, Computation time: 1.816450595855713\n",
      "Step: 4173, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 2.462692975997925\n",
      "Step: 4174, Loss: 0.9334550499916077, Accuracy: 0.96875, Computation time: 1.393202781677246\n",
      "Step: 4175, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 1.4587945938110352\n",
      "Step: 4176, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.4243321418762207\n",
      "Step: 4177, Loss: 0.9380577206611633, Accuracy: 0.96875, Computation time: 1.6543893814086914\n",
      "Step: 4178, Loss: 0.9159637689590454, Accuracy: 1.0, Computation time: 1.3581302165985107\n",
      "Step: 4179, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 1.4386258125305176\n",
      "Step: 4180, Loss: 0.9159959554672241, Accuracy: 1.0, Computation time: 1.6686336994171143\n",
      "Step: 4181, Loss: 0.9159427285194397, Accuracy: 1.0, Computation time: 1.4110941886901855\n",
      "Step: 4182, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.824981689453125\n",
      "Step: 4183, Loss: 0.9375712871551514, Accuracy: 0.96875, Computation time: 1.533066749572754\n",
      "Step: 4184, Loss: 0.9163584113121033, Accuracy: 1.0, Computation time: 1.5528638362884521\n",
      "Step: 4185, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.6145315170288086\n",
      "Step: 4186, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.8178296089172363\n",
      "Step: 4187, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.3334596157073975\n",
      "Step: 4188, Loss: 0.937590479850769, Accuracy: 0.96875, Computation time: 1.2108759880065918\n",
      "Step: 4189, Loss: 0.9542405009269714, Accuracy: 0.9375, Computation time: 1.8632245063781738\n",
      "Step: 4190, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 1.7508807182312012\n",
      "Step: 4191, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 1.3018345832824707\n",
      "Step: 4192, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.525075912475586\n",
      "Step: 4193, Loss: 0.9376469850540161, Accuracy: 0.96875, Computation time: 1.2953972816467285\n",
      "Step: 4194, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.29756760597229\n",
      "Step: 4195, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.3345527648925781\n",
      "Step: 4196, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.2423672676086426\n",
      "Step: 4197, Loss: 0.9338932633399963, Accuracy: 0.96875, Computation time: 1.3711657524108887\n",
      "Step: 4198, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 1.4189581871032715\n",
      "Step: 4199, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.4488701820373535\n",
      "Step: 4200, Loss: 0.9321733117103577, Accuracy: 0.96875, Computation time: 1.9013924598693848\n",
      "Step: 4201, Loss: 0.9345006942749023, Accuracy: 0.96875, Computation time: 1.626577615737915\n",
      "Step: 4202, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.4297544956207275\n",
      "Step: 4203, Loss: 0.9159515500068665, Accuracy: 1.0, Computation time: 1.4503288269042969\n",
      "Step: 4204, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 1.056084394454956\n",
      "Step: 4205, Loss: 0.9159564971923828, Accuracy: 1.0, Computation time: 1.4972078800201416\n",
      "Step: 4206, Loss: 0.9159693121910095, Accuracy: 1.0, Computation time: 1.359022855758667\n",
      "Step: 4207, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.2955963611602783\n",
      "Step: 4208, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.5903041362762451\n",
      "Step: 4209, Loss: 0.9159852266311646, Accuracy: 1.0, Computation time: 1.4274659156799316\n",
      "Step: 4210, Loss: 0.936757504940033, Accuracy: 0.96875, Computation time: 1.552328109741211\n",
      "Step: 4211, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.109943151473999\n",
      "Step: 4212, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.8806164264678955\n",
      "Step: 4213, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.1292779445648193\n",
      "Step: 4214, Loss: 0.9376232624053955, Accuracy: 0.96875, Computation time: 1.475391149520874\n",
      "Step: 4215, Loss: 0.9170049428939819, Accuracy: 1.0, Computation time: 1.8254766464233398\n",
      "Step: 4216, Loss: 0.937519371509552, Accuracy: 0.96875, Computation time: 1.1724603176116943\n",
      "Step: 4217, Loss: 0.9158755540847778, Accuracy: 1.0, Computation time: 1.5249121189117432\n",
      "Step: 4218, Loss: 0.9375693798065186, Accuracy: 0.96875, Computation time: 1.1117346286773682\n",
      "Step: 4219, Loss: 0.9593579173088074, Accuracy: 0.9375, Computation time: 1.9742405414581299\n",
      "Step: 4220, Loss: 0.927003800868988, Accuracy: 0.96875, Computation time: 1.656627893447876\n",
      "Step: 4221, Loss: 0.9159095883369446, Accuracy: 1.0, Computation time: 1.2070114612579346\n",
      "Step: 4222, Loss: 0.9375442266464233, Accuracy: 0.96875, Computation time: 1.4103152751922607\n",
      "Step: 4223, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 1.2678446769714355\n",
      "Step: 4224, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.1721413135528564\n",
      "Step: 4225, Loss: 0.91611647605896, Accuracy: 1.0, Computation time: 1.1731324195861816\n",
      "Step: 4226, Loss: 0.9344974756240845, Accuracy: 0.96875, Computation time: 2.1985907554626465\n",
      "Step: 4227, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.3226237297058105\n",
      "Step: 4228, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.3433690071105957\n",
      "Step: 4229, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.2627062797546387\n",
      "Step: 4230, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.8576030731201172\n",
      "Step: 4231, Loss: 0.9232995510101318, Accuracy: 1.0, Computation time: 2.1075453758239746\n",
      "Step: 4232, Loss: 0.9250836372375488, Accuracy: 1.0, Computation time: 1.3090095520019531\n",
      "Step: 4233, Loss: 0.9159319400787354, Accuracy: 1.0, Computation time: 1.197009563446045\n",
      "Step: 4234, Loss: 0.9161857962608337, Accuracy: 1.0, Computation time: 2.1712286472320557\n",
      "Step: 4235, Loss: 0.9159901738166809, Accuracy: 1.0, Computation time: 1.3937904834747314\n",
      "Step: 4236, Loss: 0.9163005352020264, Accuracy: 1.0, Computation time: 1.9580612182617188\n",
      "Step: 4237, Loss: 0.9380337595939636, Accuracy: 0.96875, Computation time: 1.5552489757537842\n",
      "Step: 4238, Loss: 0.9159680008888245, Accuracy: 1.0, Computation time: 1.1651842594146729\n",
      "Step: 4239, Loss: 0.9169086813926697, Accuracy: 1.0, Computation time: 1.5484929084777832\n",
      "Step: 4240, Loss: 0.9160193204879761, Accuracy: 1.0, Computation time: 1.856644868850708\n",
      "Step: 4241, Loss: 0.9213326573371887, Accuracy: 1.0, Computation time: 1.721703290939331\n",
      "Step: 4242, Loss: 0.9376399517059326, Accuracy: 0.96875, Computation time: 1.4691085815429688\n",
      "Step: 4243, Loss: 0.915917694568634, Accuracy: 1.0, Computation time: 1.3837416172027588\n",
      "Step: 4244, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 1.3636295795440674\n",
      "Step: 4245, Loss: 0.9354739189147949, Accuracy: 0.96875, Computation time: 1.952279806137085\n",
      "Step: 4246, Loss: 0.916252613067627, Accuracy: 1.0, Computation time: 1.3599529266357422\n",
      "Step: 4247, Loss: 0.9160231947898865, Accuracy: 1.0, Computation time: 1.3456981182098389\n",
      "Step: 4248, Loss: 0.915920078754425, Accuracy: 1.0, Computation time: 1.270261526107788\n",
      "Step: 4249, Loss: 0.9159248471260071, Accuracy: 1.0, Computation time: 1.4768097400665283\n",
      "Step: 4250, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.6968698501586914\n",
      "Step: 4251, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.5212535858154297\n",
      "Step: 4252, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 1.22715425491333\n",
      "Step: 4253, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.3327765464782715\n",
      "Step: 4254, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.2657172679901123\n",
      "Step: 4255, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.2229139804840088\n",
      "Step: 4256, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.4574434757232666\n",
      "Step: 4257, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.2738182544708252\n",
      "Step: 4258, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.1884615421295166\n",
      "Step: 4259, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.311448097229004\n",
      "Step: 4260, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 1.2044785022735596\n",
      "Step: 4261, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.4608676433563232\n",
      "Step: 4262, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.2526144981384277\n",
      "Step: 4263, Loss: 0.9367493391036987, Accuracy: 0.96875, Computation time: 2.1675052642822266\n",
      "Step: 4264, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.2161529064178467\n",
      "Step: 4265, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.2024915218353271\n",
      "Step: 4266, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.506584644317627\n",
      "Step: 4267, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.3564369678497314\n",
      "Step: 4268, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.9389441013336182\n",
      "Step: 4269, Loss: 0.9375540018081665, Accuracy: 0.96875, Computation time: 1.4594616889953613\n",
      "Step: 4270, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.442474126815796\n",
      "Step: 4271, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.1966612339019775\n",
      "Step: 4272, Loss: 0.9158387184143066, Accuracy: 1.0, Computation time: 1.287062406539917\n",
      "Step: 4273, Loss: 0.917616069316864, Accuracy: 1.0, Computation time: 2.0191457271575928\n",
      "Step: 4274, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.3305537700653076\n",
      "Step: 4275, Loss: 0.9160077571868896, Accuracy: 1.0, Computation time: 1.1083345413208008\n",
      "Step: 4276, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.4922356605529785\n",
      "Step: 4277, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.772151231765747\n",
      "Step: 4278, Loss: 0.9168107509613037, Accuracy: 1.0, Computation time: 2.2413928508758545\n",
      "Step: 4279, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.3508434295654297\n",
      "Step: 4280, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 1.5013792514801025\n",
      "Step: 4281, Loss: 0.959469735622406, Accuracy: 0.9375, Computation time: 1.7406258583068848\n",
      "Step: 4282, Loss: 0.9161289930343628, Accuracy: 1.0, Computation time: 1.3763105869293213\n",
      "Step: 4283, Loss: 0.9162242412567139, Accuracy: 1.0, Computation time: 1.4590167999267578\n",
      "Step: 4284, Loss: 0.9159793257713318, Accuracy: 1.0, Computation time: 2.281038761138916\n",
      "Step: 4285, Loss: 0.9159610271453857, Accuracy: 1.0, Computation time: 1.8626203536987305\n",
      "Step: 4286, Loss: 0.9159867167472839, Accuracy: 1.0, Computation time: 1.5092313289642334\n",
      "Step: 4287, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 1.4109532833099365\n",
      "Step: 4288, Loss: 0.9159798622131348, Accuracy: 1.0, Computation time: 1.9571592807769775\n",
      "Step: 4289, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.5951118469238281\n",
      "Step: 4290, Loss: 0.9375123381614685, Accuracy: 0.96875, Computation time: 2.042358875274658\n",
      "Step: 4291, Loss: 0.9190282225608826, Accuracy: 1.0, Computation time: 1.8450016975402832\n",
      "Step: 4292, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 1.3658673763275146\n",
      "Step: 4293, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.6212584972381592\n",
      "Step: 4294, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 1.394026756286621\n",
      "Step: 4295, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.2560451030731201\n",
      "Step: 4296, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 1.6117064952850342\n",
      "Step: 4297, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.4210655689239502\n",
      "Step: 4298, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.2379214763641357\n",
      "Step: 4299, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.2553162574768066\n",
      "Step: 4300, Loss: 0.9159842133522034, Accuracy: 1.0, Computation time: 1.4533567428588867\n",
      "Step: 4301, Loss: 0.9173834323883057, Accuracy: 1.0, Computation time: 2.2127254009246826\n",
      "Step: 4302, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.5021674633026123\n",
      "Step: 4303, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.4408717155456543\n",
      "Step: 4304, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.2153856754302979\n",
      "Step: 4305, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.631119966506958\n",
      "Step: 4306, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.2884621620178223\n",
      "Step: 4307, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.4733731746673584\n",
      "Step: 4308, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.3564698696136475\n",
      "########################\n",
      "Test loss: 1.1254018545150757, Test Accuracy_epoch31: 0.6946011185646057\n",
      "########################\n",
      "Step: 4309, Loss: 0.937329888343811, Accuracy: 0.96875, Computation time: 2.2034823894500732\n",
      "Step: 4310, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 1.3575305938720703\n",
      "Step: 4311, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.2154252529144287\n",
      "Step: 4312, Loss: 0.9159582257270813, Accuracy: 1.0, Computation time: 1.172905683517456\n",
      "Step: 4313, Loss: 0.9325652718544006, Accuracy: 0.96875, Computation time: 1.6329259872436523\n",
      "Step: 4314, Loss: 0.9158428907394409, Accuracy: 1.0, Computation time: 1.3084518909454346\n",
      "Step: 4315, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 1.5953609943389893\n",
      "Step: 4316, Loss: 0.9376549124717712, Accuracy: 0.96875, Computation time: 1.350733995437622\n",
      "Step: 4317, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 1.447108268737793\n",
      "Step: 4318, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 1.3875231742858887\n",
      "Step: 4319, Loss: 0.9161514043807983, Accuracy: 1.0, Computation time: 1.4463648796081543\n",
      "Step: 4320, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 1.9213168621063232\n",
      "Step: 4321, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.3059649467468262\n",
      "Step: 4322, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.196455717086792\n",
      "Step: 4323, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.3015108108520508\n",
      "Step: 4324, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.3781695365905762\n",
      "Step: 4325, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.6194884777069092\n",
      "Step: 4326, Loss: 0.9296560883522034, Accuracy: 0.96875, Computation time: 1.9097318649291992\n",
      "Step: 4327, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.4690828323364258\n",
      "Step: 4328, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.8710360527038574\n",
      "Step: 4329, Loss: 0.9159212112426758, Accuracy: 1.0, Computation time: 1.98012113571167\n",
      "Step: 4330, Loss: 0.9159227013587952, Accuracy: 1.0, Computation time: 1.7538580894470215\n",
      "Step: 4331, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 1.5100221633911133\n",
      "Step: 4332, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.4187064170837402\n",
      "Step: 4333, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 1.3444757461547852\n",
      "Step: 4334, Loss: 0.9161345362663269, Accuracy: 1.0, Computation time: 1.1741414070129395\n",
      "Step: 4335, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.234102725982666\n",
      "Step: 4336, Loss: 0.9385044574737549, Accuracy: 0.96875, Computation time: 2.5913662910461426\n",
      "Step: 4337, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.415410041809082\n",
      "Step: 4338, Loss: 0.9170642495155334, Accuracy: 1.0, Computation time: 1.4452948570251465\n",
      "Step: 4339, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.4695312976837158\n",
      "Step: 4340, Loss: 0.9159377217292786, Accuracy: 1.0, Computation time: 1.6047353744506836\n",
      "Step: 4341, Loss: 0.9158948063850403, Accuracy: 1.0, Computation time: 1.42641282081604\n",
      "Step: 4342, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.2789943218231201\n",
      "Step: 4343, Loss: 0.9592568874359131, Accuracy: 0.9375, Computation time: 1.3948259353637695\n",
      "Step: 4344, Loss: 0.9375717043876648, Accuracy: 0.96875, Computation time: 1.3517179489135742\n",
      "Step: 4345, Loss: 0.9376039505004883, Accuracy: 0.96875, Computation time: 1.517573595046997\n",
      "Step: 4346, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 1.3848187923431396\n",
      "Step: 4347, Loss: 0.9368584156036377, Accuracy: 0.96875, Computation time: 1.5814292430877686\n",
      "Step: 4348, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.5898053646087646\n",
      "Step: 4349, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.311058521270752\n",
      "Step: 4350, Loss: 0.9376104474067688, Accuracy: 0.96875, Computation time: 1.4168999195098877\n",
      "Step: 4351, Loss: 0.9269487261772156, Accuracy: 0.96875, Computation time: 2.1128156185150146\n",
      "Step: 4352, Loss: 0.9159950613975525, Accuracy: 1.0, Computation time: 1.5057361125946045\n",
      "Step: 4353, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.4501824378967285\n",
      "Step: 4354, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.5429277420043945\n",
      "Step: 4355, Loss: 0.9167566895484924, Accuracy: 1.0, Computation time: 1.318699598312378\n",
      "Step: 4356, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 1.737272024154663\n",
      "Step: 4357, Loss: 0.9351999759674072, Accuracy: 0.96875, Computation time: 1.939650058746338\n",
      "Step: 4358, Loss: 0.9375356435775757, Accuracy: 0.96875, Computation time: 1.1629493236541748\n",
      "Step: 4359, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 1.3226697444915771\n",
      "Step: 4360, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.4384393692016602\n",
      "Step: 4361, Loss: 0.9160600304603577, Accuracy: 1.0, Computation time: 1.2124762535095215\n",
      "Step: 4362, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.130448341369629\n",
      "Step: 4363, Loss: 0.9159107804298401, Accuracy: 1.0, Computation time: 1.354288101196289\n",
      "Step: 4364, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.4935534000396729\n",
      "Step: 4365, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.4007985591888428\n",
      "Step: 4366, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 1.6112182140350342\n",
      "Step: 4367, Loss: 0.9162673950195312, Accuracy: 1.0, Computation time: 1.5811376571655273\n",
      "Step: 4368, Loss: 0.9159455895423889, Accuracy: 1.0, Computation time: 1.534332513809204\n",
      "Step: 4369, Loss: 0.9161317944526672, Accuracy: 1.0, Computation time: 1.2567648887634277\n",
      "Step: 4370, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.2732057571411133\n",
      "Step: 4371, Loss: 0.9162415862083435, Accuracy: 1.0, Computation time: 1.2893576622009277\n",
      "Step: 4372, Loss: 0.9162054061889648, Accuracy: 1.0, Computation time: 1.5275585651397705\n",
      "Step: 4373, Loss: 0.9552733302116394, Accuracy: 0.9375, Computation time: 1.3452239036560059\n",
      "Step: 4374, Loss: 0.9375481009483337, Accuracy: 0.96875, Computation time: 1.2845404148101807\n",
      "Step: 4375, Loss: 0.915992259979248, Accuracy: 1.0, Computation time: 1.729203701019287\n",
      "Step: 4376, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 1.5283098220825195\n",
      "Step: 4377, Loss: 0.9174885749816895, Accuracy: 1.0, Computation time: 1.790879726409912\n",
      "Step: 4378, Loss: 0.9376767873764038, Accuracy: 0.96875, Computation time: 1.350820541381836\n",
      "Step: 4379, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 1.4502134323120117\n",
      "Step: 4380, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.588759183883667\n",
      "Step: 4381, Loss: 0.9160119295120239, Accuracy: 1.0, Computation time: 1.5300774574279785\n",
      "Step: 4382, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.8428266048431396\n",
      "Step: 4383, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.5267374515533447\n",
      "Step: 4384, Loss: 0.9159212112426758, Accuracy: 1.0, Computation time: 1.5379998683929443\n",
      "Step: 4385, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.569187879562378\n",
      "Step: 4386, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.8210678100585938\n",
      "Step: 4387, Loss: 0.9164234399795532, Accuracy: 1.0, Computation time: 1.2612295150756836\n",
      "Step: 4388, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.3895347118377686\n",
      "Step: 4389, Loss: 0.9378808736801147, Accuracy: 0.96875, Computation time: 1.9681158065795898\n",
      "Step: 4390, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.1473586559295654\n",
      "Step: 4391, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 1.3461055755615234\n",
      "Step: 4392, Loss: 0.9386972188949585, Accuracy: 0.96875, Computation time: 2.6984376907348633\n",
      "Step: 4393, Loss: 0.9158779978752136, Accuracy: 1.0, Computation time: 1.1573107242584229\n",
      "Step: 4394, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.7976763248443604\n",
      "Step: 4395, Loss: 0.9375303387641907, Accuracy: 0.96875, Computation time: 1.393681287765503\n",
      "Step: 4396, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.3567233085632324\n",
      "Step: 4397, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.5307557582855225\n",
      "Step: 4398, Loss: 0.915982186794281, Accuracy: 1.0, Computation time: 1.5994408130645752\n",
      "Step: 4399, Loss: 0.9163986444473267, Accuracy: 1.0, Computation time: 1.195012092590332\n",
      "Step: 4400, Loss: 0.9279471635818481, Accuracy: 0.96875, Computation time: 1.7760672569274902\n",
      "Step: 4401, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.200199842453003\n",
      "Step: 4402, Loss: 0.937481701374054, Accuracy: 0.96875, Computation time: 1.3733348846435547\n",
      "Step: 4403, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.5539956092834473\n",
      "Step: 4404, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.3847720623016357\n",
      "Step: 4405, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 1.0764777660369873\n",
      "Step: 4406, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.1076691150665283\n",
      "Step: 4407, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.454622507095337\n",
      "Step: 4408, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.3076796531677246\n",
      "Step: 4409, Loss: 0.9571081399917603, Accuracy: 0.9375, Computation time: 1.806307077407837\n",
      "Step: 4410, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.3843367099761963\n",
      "Step: 4411, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 1.2665953636169434\n",
      "Step: 4412, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.6611433029174805\n",
      "Step: 4413, Loss: 0.9161809682846069, Accuracy: 1.0, Computation time: 1.7481207847595215\n",
      "Step: 4414, Loss: 0.9374438524246216, Accuracy: 0.96875, Computation time: 1.3566012382507324\n",
      "Step: 4415, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 2.053560495376587\n",
      "Step: 4416, Loss: 0.9375812411308289, Accuracy: 0.96875, Computation time: 1.4905683994293213\n",
      "Step: 4417, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.2283668518066406\n",
      "Step: 4418, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 1.550166130065918\n",
      "Step: 4419, Loss: 0.9375985264778137, Accuracy: 0.96875, Computation time: 1.2459795475006104\n",
      "Step: 4420, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.4453153610229492\n",
      "Step: 4421, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.4427461624145508\n",
      "Step: 4422, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 1.4338655471801758\n",
      "Step: 4423, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 1.4680678844451904\n",
      "Step: 4424, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.3743226528167725\n",
      "Step: 4425, Loss: 0.9184384942054749, Accuracy: 1.0, Computation time: 1.7208919525146484\n",
      "Step: 4426, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.4666016101837158\n",
      "Step: 4427, Loss: 0.9375998377799988, Accuracy: 0.96875, Computation time: 1.4238243103027344\n",
      "Step: 4428, Loss: 0.9159373044967651, Accuracy: 1.0, Computation time: 1.456817626953125\n",
      "Step: 4429, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.4152452945709229\n",
      "Step: 4430, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 1.49485182762146\n",
      "Step: 4431, Loss: 0.9162480235099792, Accuracy: 1.0, Computation time: 2.046246290206909\n",
      "Step: 4432, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.4062507152557373\n",
      "Step: 4433, Loss: 0.9375304579734802, Accuracy: 0.96875, Computation time: 1.348191499710083\n",
      "Step: 4434, Loss: 0.9170880317687988, Accuracy: 1.0, Computation time: 1.5674166679382324\n",
      "Step: 4435, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.332763671875\n",
      "Step: 4436, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.666111946105957\n",
      "Step: 4437, Loss: 0.9196149706840515, Accuracy: 1.0, Computation time: 1.5092658996582031\n",
      "Step: 4438, Loss: 0.9159457683563232, Accuracy: 1.0, Computation time: 1.3410544395446777\n",
      "Step: 4439, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 1.3952581882476807\n",
      "Step: 4440, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.0092434883117676\n",
      "Step: 4441, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.4586546421051025\n",
      "Step: 4442, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.263456106185913\n",
      "Step: 4443, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 1.3949191570281982\n",
      "Step: 4444, Loss: 0.9274342060089111, Accuracy: 0.96875, Computation time: 1.2130088806152344\n",
      "Step: 4445, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.7122411727905273\n",
      "Step: 4446, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 1.5787630081176758\n",
      "########################\n",
      "Test loss: 1.1276874542236328, Test Accuracy_epoch32: 0.6921837329864502\n",
      "########################\n",
      "Step: 4447, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 1.486680269241333\n",
      "Step: 4448, Loss: 0.9159407615661621, Accuracy: 1.0, Computation time: 1.1967370510101318\n",
      "Step: 4449, Loss: 0.9161931872367859, Accuracy: 1.0, Computation time: 1.3722751140594482\n",
      "Step: 4450, Loss: 0.9159993529319763, Accuracy: 1.0, Computation time: 1.4231221675872803\n",
      "Step: 4451, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 1.8102569580078125\n",
      "Step: 4452, Loss: 0.9160592555999756, Accuracy: 1.0, Computation time: 1.9309232234954834\n",
      "Step: 4453, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.5925517082214355\n",
      "Step: 4454, Loss: 0.9568825960159302, Accuracy: 0.9375, Computation time: 1.9600458145141602\n",
      "Step: 4455, Loss: 0.937579870223999, Accuracy: 0.96875, Computation time: 1.4098610877990723\n",
      "Step: 4456, Loss: 0.9377431869506836, Accuracy: 0.96875, Computation time: 1.474280834197998\n",
      "Step: 4457, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.2922084331512451\n",
      "Step: 4458, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.320145606994629\n",
      "Step: 4459, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.6332504749298096\n",
      "Step: 4460, Loss: 0.9591948390007019, Accuracy: 0.9375, Computation time: 1.1790294647216797\n",
      "Step: 4461, Loss: 0.9177172780036926, Accuracy: 1.0, Computation time: 1.5570416450500488\n",
      "Step: 4462, Loss: 0.9159144163131714, Accuracy: 1.0, Computation time: 1.9643323421478271\n",
      "Step: 4463, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 1.533357858657837\n",
      "Step: 4464, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.150090217590332\n",
      "Step: 4465, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 1.7837340831756592\n",
      "Step: 4466, Loss: 0.9339588284492493, Accuracy: 0.96875, Computation time: 1.086860179901123\n",
      "Step: 4467, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 1.7577722072601318\n",
      "Step: 4468, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.2400939464569092\n",
      "Step: 4469, Loss: 0.9187930822372437, Accuracy: 1.0, Computation time: 1.7912144660949707\n",
      "Step: 4470, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.1716670989990234\n",
      "Step: 4471, Loss: 0.9159228801727295, Accuracy: 1.0, Computation time: 0.9692661762237549\n",
      "Step: 4472, Loss: 0.9376695156097412, Accuracy: 0.96875, Computation time: 1.1159162521362305\n",
      "Step: 4473, Loss: 0.91591876745224, Accuracy: 1.0, Computation time: 1.091597080230713\n",
      "Step: 4474, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.0789015293121338\n",
      "Step: 4475, Loss: 0.9324608445167542, Accuracy: 0.96875, Computation time: 1.4550824165344238\n",
      "Step: 4476, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.7074620723724365\n",
      "Step: 4477, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.507035255432129\n",
      "Step: 4478, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.3760838508605957\n",
      "Step: 4479, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 1.0244343280792236\n",
      "Step: 4480, Loss: 0.9159319400787354, Accuracy: 1.0, Computation time: 1.2158966064453125\n",
      "Step: 4481, Loss: 0.916364312171936, Accuracy: 1.0, Computation time: 1.6345205307006836\n",
      "Step: 4482, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 1.3613579273223877\n",
      "Step: 4483, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.2311372756958008\n",
      "Step: 4484, Loss: 0.9375147223472595, Accuracy: 0.96875, Computation time: 1.6253747940063477\n",
      "Step: 4485, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.3424484729766846\n",
      "Step: 4486, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.1456139087677002\n",
      "Step: 4487, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.2070057392120361\n",
      "Step: 4488, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.6971752643585205\n",
      "Step: 4489, Loss: 0.926279604434967, Accuracy: 0.96875, Computation time: 1.9560329914093018\n",
      "Step: 4490, Loss: 0.9160984754562378, Accuracy: 1.0, Computation time: 1.469803810119629\n",
      "Step: 4491, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 2.064966917037964\n",
      "Step: 4492, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.214646816253662\n",
      "Step: 4493, Loss: 0.9374459385871887, Accuracy: 0.96875, Computation time: 1.251063346862793\n",
      "Step: 4494, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.1024730205535889\n",
      "Step: 4495, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 0.956519365310669\n",
      "Step: 4496, Loss: 0.9410290122032166, Accuracy: 0.96875, Computation time: 1.481170892715454\n",
      "Step: 4497, Loss: 0.9162137508392334, Accuracy: 1.0, Computation time: 1.217487096786499\n",
      "Step: 4498, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.1396393775939941\n",
      "Step: 4499, Loss: 0.9375631809234619, Accuracy: 0.96875, Computation time: 0.9879426956176758\n",
      "Step: 4500, Loss: 0.9376670718193054, Accuracy: 0.96875, Computation time: 1.4849395751953125\n",
      "Step: 4501, Loss: 0.9186562895774841, Accuracy: 1.0, Computation time: 2.4383323192596436\n",
      "Step: 4502, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.7289328575134277\n",
      "Step: 4503, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 0.969193696975708\n",
      "Step: 4504, Loss: 0.9159711599349976, Accuracy: 1.0, Computation time: 1.6597397327423096\n",
      "Step: 4505, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 1.265838384628296\n",
      "Step: 4506, Loss: 0.9233041405677795, Accuracy: 1.0, Computation time: 2.441793441772461\n",
      "Step: 4507, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.2821662425994873\n",
      "Step: 4508, Loss: 0.9543055891990662, Accuracy: 0.9375, Computation time: 1.657486915588379\n",
      "Step: 4509, Loss: 0.9360232949256897, Accuracy: 0.96875, Computation time: 1.2800796031951904\n",
      "Step: 4510, Loss: 0.9162399768829346, Accuracy: 1.0, Computation time: 1.1507668495178223\n",
      "Step: 4511, Loss: 0.915970504283905, Accuracy: 1.0, Computation time: 1.0584025382995605\n",
      "Step: 4512, Loss: 0.9382026195526123, Accuracy: 0.96875, Computation time: 3.2914140224456787\n",
      "Step: 4513, Loss: 0.9596476554870605, Accuracy: 0.9375, Computation time: 1.2085697650909424\n",
      "Step: 4514, Loss: 0.9159671068191528, Accuracy: 1.0, Computation time: 1.0703961849212646\n",
      "Step: 4515, Loss: 0.9161789417266846, Accuracy: 1.0, Computation time: 1.4411170482635498\n",
      "Step: 4516, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.2741384506225586\n",
      "Step: 4517, Loss: 0.9159213900566101, Accuracy: 1.0, Computation time: 1.2619097232818604\n",
      "Step: 4518, Loss: 0.9367936849594116, Accuracy: 0.96875, Computation time: 1.3304097652435303\n",
      "Step: 4519, Loss: 0.9376875162124634, Accuracy: 0.96875, Computation time: 1.3483386039733887\n",
      "Step: 4520, Loss: 0.9367061853408813, Accuracy: 0.96875, Computation time: 1.143345594406128\n",
      "Step: 4521, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.1202774047851562\n",
      "Step: 4522, Loss: 0.9389685392379761, Accuracy: 0.96875, Computation time: 2.8355140686035156\n",
      "Step: 4523, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 1.0192441940307617\n",
      "Step: 4524, Loss: 0.9375922679901123, Accuracy: 0.96875, Computation time: 1.1052494049072266\n",
      "Step: 4525, Loss: 0.9161429405212402, Accuracy: 1.0, Computation time: 1.0552668571472168\n",
      "Step: 4526, Loss: 0.9162092208862305, Accuracy: 1.0, Computation time: 1.2528479099273682\n",
      "Step: 4527, Loss: 0.9163382649421692, Accuracy: 1.0, Computation time: 1.3925857543945312\n",
      "Step: 4528, Loss: 0.9161145687103271, Accuracy: 1.0, Computation time: 1.2850158214569092\n",
      "Step: 4529, Loss: 0.9160798192024231, Accuracy: 1.0, Computation time: 1.8268351554870605\n",
      "Step: 4530, Loss: 0.9270817041397095, Accuracy: 0.96875, Computation time: 2.025521993637085\n",
      "Step: 4531, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.7722270488739014\n",
      "Step: 4532, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.3592705726623535\n",
      "Step: 4533, Loss: 0.9159377217292786, Accuracy: 1.0, Computation time: 1.3224308490753174\n",
      "Step: 4534, Loss: 0.9159893989562988, Accuracy: 1.0, Computation time: 1.4971261024475098\n",
      "Step: 4535, Loss: 0.9376057982444763, Accuracy: 0.96875, Computation time: 1.4871995449066162\n",
      "Step: 4536, Loss: 0.9186007380485535, Accuracy: 1.0, Computation time: 1.7407641410827637\n",
      "Step: 4537, Loss: 0.9160006046295166, Accuracy: 1.0, Computation time: 1.240234136581421\n",
      "Step: 4538, Loss: 0.9159541726112366, Accuracy: 1.0, Computation time: 1.192356824874878\n",
      "Step: 4539, Loss: 0.9160053730010986, Accuracy: 1.0, Computation time: 1.2731890678405762\n",
      "Step: 4540, Loss: 0.9160540103912354, Accuracy: 1.0, Computation time: 1.3013384342193604\n",
      "Step: 4541, Loss: 0.9159671664237976, Accuracy: 1.0, Computation time: 1.4093568325042725\n",
      "Step: 4542, Loss: 0.915951669216156, Accuracy: 1.0, Computation time: 1.251840591430664\n",
      "Step: 4543, Loss: 0.9188649654388428, Accuracy: 1.0, Computation time: 1.2740280628204346\n",
      "Step: 4544, Loss: 0.9299923181533813, Accuracy: 0.96875, Computation time: 2.0173230171203613\n",
      "Step: 4545, Loss: 0.9166616797447205, Accuracy: 1.0, Computation time: 1.3368501663208008\n",
      "Step: 4546, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 1.1209306716918945\n",
      "Step: 4547, Loss: 0.9160836935043335, Accuracy: 1.0, Computation time: 1.3386919498443604\n",
      "Step: 4548, Loss: 0.916141927242279, Accuracy: 1.0, Computation time: 1.2858805656433105\n",
      "Step: 4549, Loss: 0.9163507223129272, Accuracy: 1.0, Computation time: 1.2718374729156494\n",
      "Step: 4550, Loss: 0.9379932880401611, Accuracy: 0.96875, Computation time: 1.525421380996704\n",
      "Step: 4551, Loss: 0.9379827976226807, Accuracy: 0.96875, Computation time: 1.2320609092712402\n",
      "Step: 4552, Loss: 0.91602623462677, Accuracy: 1.0, Computation time: 1.4307198524475098\n",
      "Step: 4553, Loss: 0.9160194396972656, Accuracy: 1.0, Computation time: 1.306281566619873\n",
      "Step: 4554, Loss: 0.9159682989120483, Accuracy: 1.0, Computation time: 1.5032150745391846\n",
      "Step: 4555, Loss: 0.937781035900116, Accuracy: 0.96875, Computation time: 1.3700995445251465\n",
      "Step: 4556, Loss: 0.9160163998603821, Accuracy: 1.0, Computation time: 1.4844136238098145\n",
      "Step: 4557, Loss: 0.9376658201217651, Accuracy: 0.96875, Computation time: 1.5332105159759521\n",
      "Step: 4558, Loss: 0.9167550802230835, Accuracy: 1.0, Computation time: 1.9582126140594482\n",
      "Step: 4559, Loss: 0.9159613251686096, Accuracy: 1.0, Computation time: 1.1189165115356445\n",
      "Step: 4560, Loss: 0.9243724346160889, Accuracy: 1.0, Computation time: 1.676513671875\n",
      "Step: 4561, Loss: 0.9160485863685608, Accuracy: 1.0, Computation time: 1.6317753791809082\n",
      "Step: 4562, Loss: 0.916138231754303, Accuracy: 1.0, Computation time: 1.4038457870483398\n",
      "Step: 4563, Loss: 0.9174719452857971, Accuracy: 1.0, Computation time: 1.5302543640136719\n",
      "Step: 4564, Loss: 0.9266035556793213, Accuracy: 1.0, Computation time: 3.088214635848999\n",
      "Step: 4565, Loss: 0.9161354899406433, Accuracy: 1.0, Computation time: 1.288588523864746\n",
      "Step: 4566, Loss: 0.9164551496505737, Accuracy: 1.0, Computation time: 1.2648370265960693\n",
      "Step: 4567, Loss: 0.9164985418319702, Accuracy: 1.0, Computation time: 1.3765778541564941\n",
      "Step: 4568, Loss: 0.9163858890533447, Accuracy: 1.0, Computation time: 1.264214277267456\n",
      "Step: 4569, Loss: 0.9161572456359863, Accuracy: 1.0, Computation time: 1.302114486694336\n",
      "Step: 4570, Loss: 0.9160724878311157, Accuracy: 1.0, Computation time: 1.4695544242858887\n",
      "Step: 4571, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 1.359795331954956\n",
      "Step: 4572, Loss: 0.937595546245575, Accuracy: 0.96875, Computation time: 1.276174545288086\n",
      "Step: 4573, Loss: 0.9161405563354492, Accuracy: 1.0, Computation time: 1.1766588687896729\n",
      "Step: 4574, Loss: 0.9357295632362366, Accuracy: 0.96875, Computation time: 1.4471983909606934\n",
      "Step: 4575, Loss: 0.9161415696144104, Accuracy: 1.0, Computation time: 1.282581090927124\n",
      "Step: 4576, Loss: 0.9230033755302429, Accuracy: 1.0, Computation time: 1.4318053722381592\n",
      "Step: 4577, Loss: 0.9161574244499207, Accuracy: 1.0, Computation time: 1.5408704280853271\n",
      "Step: 4578, Loss: 0.9160975217819214, Accuracy: 1.0, Computation time: 1.2266044616699219\n",
      "Step: 4579, Loss: 0.9159711599349976, Accuracy: 1.0, Computation time: 1.3830339908599854\n",
      "Step: 4580, Loss: 0.9159685373306274, Accuracy: 1.0, Computation time: 1.5044605731964111\n",
      "Step: 4581, Loss: 0.9160462617874146, Accuracy: 1.0, Computation time: 1.3154706954956055\n",
      "Step: 4582, Loss: 0.9160853624343872, Accuracy: 1.0, Computation time: 1.4633817672729492\n",
      "Step: 4583, Loss: 0.9159996509552002, Accuracy: 1.0, Computation time: 1.1025364398956299\n",
      "Step: 4584, Loss: 0.9376266598701477, Accuracy: 0.96875, Computation time: 1.358283281326294\n",
      "Step: 4585, Loss: 0.9159810543060303, Accuracy: 1.0, Computation time: 1.2720494270324707\n",
      "########################\n",
      "Test loss: 1.1207891702651978, Test Accuracy_epoch33: 0.703464925289154\n",
      "########################\n",
      "Step: 4586, Loss: 0.9375384449958801, Accuracy: 0.96875, Computation time: 1.1800298690795898\n",
      "Step: 4587, Loss: 0.9377388954162598, Accuracy: 0.96875, Computation time: 1.0153698921203613\n",
      "Step: 4588, Loss: 0.9335371851921082, Accuracy: 0.96875, Computation time: 1.480762004852295\n",
      "Step: 4589, Loss: 0.9159126877784729, Accuracy: 1.0, Computation time: 1.219681739807129\n",
      "Step: 4590, Loss: 0.9159228205680847, Accuracy: 1.0, Computation time: 1.3712844848632812\n",
      "Step: 4591, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.119089126586914\n",
      "Step: 4592, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.5021941661834717\n",
      "Step: 4593, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 1.2498948574066162\n",
      "Step: 4594, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.1125454902648926\n",
      "Step: 4595, Loss: 0.979356050491333, Accuracy: 0.90625, Computation time: 1.911020040512085\n",
      "Step: 4596, Loss: 0.9159743189811707, Accuracy: 1.0, Computation time: 1.278010368347168\n",
      "Step: 4597, Loss: 0.9160366058349609, Accuracy: 1.0, Computation time: 1.2275893688201904\n",
      "Step: 4598, Loss: 0.9159896373748779, Accuracy: 1.0, Computation time: 1.2655842304229736\n",
      "Step: 4599, Loss: 0.9160731434822083, Accuracy: 1.0, Computation time: 1.0800633430480957\n",
      "Step: 4600, Loss: 0.9159573912620544, Accuracy: 1.0, Computation time: 1.2820029258728027\n",
      "Step: 4601, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.2084803581237793\n",
      "Step: 4602, Loss: 0.916000247001648, Accuracy: 1.0, Computation time: 1.5081613063812256\n",
      "Step: 4603, Loss: 0.9164430499076843, Accuracy: 1.0, Computation time: 1.374950647354126\n",
      "Step: 4604, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.239750623703003\n",
      "Step: 4605, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.1188080310821533\n",
      "Step: 4606, Loss: 0.9165346026420593, Accuracy: 1.0, Computation time: 1.907780408859253\n",
      "Step: 4607, Loss: 0.9377402067184448, Accuracy: 0.96875, Computation time: 1.2262089252471924\n",
      "Step: 4608, Loss: 0.9218666553497314, Accuracy: 1.0, Computation time: 1.358788251876831\n",
      "Step: 4609, Loss: 0.9159099459648132, Accuracy: 1.0, Computation time: 1.2409634590148926\n",
      "Step: 4610, Loss: 0.9162262678146362, Accuracy: 1.0, Computation time: 1.3764851093292236\n",
      "Step: 4611, Loss: 0.9373419880867004, Accuracy: 0.96875, Computation time: 1.2910897731781006\n",
      "Step: 4612, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.5231716632843018\n",
      "Step: 4613, Loss: 0.9160904884338379, Accuracy: 1.0, Computation time: 1.184619665145874\n",
      "Step: 4614, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.292978048324585\n",
      "Step: 4615, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.3120479583740234\n",
      "Step: 4616, Loss: 0.916022539138794, Accuracy: 1.0, Computation time: 1.5476758480072021\n",
      "Step: 4617, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.1981518268585205\n",
      "Step: 4618, Loss: 0.928657054901123, Accuracy: 0.96875, Computation time: 1.223581314086914\n",
      "Step: 4619, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 1.1226916313171387\n",
      "Step: 4620, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 1.0583994388580322\n",
      "Step: 4621, Loss: 0.9159747362136841, Accuracy: 1.0, Computation time: 1.9256339073181152\n",
      "Step: 4622, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.189133644104004\n",
      "Step: 4623, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 0.9863073825836182\n",
      "Step: 4624, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.0478336811065674\n",
      "Step: 4625, Loss: 0.9160891771316528, Accuracy: 1.0, Computation time: 1.0721557140350342\n",
      "Step: 4626, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.0724749565124512\n",
      "Step: 4627, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 1.3942360877990723\n",
      "Step: 4628, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.3160598278045654\n",
      "Step: 4629, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 1.2877552509307861\n",
      "Step: 4630, Loss: 0.9583219885826111, Accuracy: 0.9375, Computation time: 1.569838285446167\n",
      "Step: 4631, Loss: 0.9172465205192566, Accuracy: 1.0, Computation time: 1.3097548484802246\n",
      "Step: 4632, Loss: 0.9376015663146973, Accuracy: 0.96875, Computation time: 1.1627333164215088\n",
      "Step: 4633, Loss: 0.9375844597816467, Accuracy: 0.96875, Computation time: 1.457484245300293\n",
      "Step: 4634, Loss: 0.9159545302391052, Accuracy: 1.0, Computation time: 1.1393539905548096\n",
      "Step: 4635, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.2161285877227783\n",
      "Step: 4636, Loss: 0.9159598350524902, Accuracy: 1.0, Computation time: 1.0815234184265137\n",
      "Step: 4637, Loss: 0.9159910678863525, Accuracy: 1.0, Computation time: 1.0574541091918945\n",
      "Step: 4638, Loss: 0.9195652008056641, Accuracy: 1.0, Computation time: 1.7566006183624268\n",
      "Step: 4639, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.542393684387207\n",
      "Step: 4640, Loss: 0.9159585237503052, Accuracy: 1.0, Computation time: 1.2513608932495117\n",
      "Step: 4641, Loss: 0.9182910323143005, Accuracy: 1.0, Computation time: 1.1669237613677979\n",
      "Step: 4642, Loss: 0.9235202670097351, Accuracy: 1.0, Computation time: 1.542067050933838\n",
      "Step: 4643, Loss: 0.9165971279144287, Accuracy: 1.0, Computation time: 1.6818468570709229\n",
      "Step: 4644, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 1.1360225677490234\n",
      "Step: 4645, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.0022783279418945\n",
      "Step: 4646, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.30208158493042\n",
      "Step: 4647, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.2447962760925293\n",
      "Step: 4648, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 1.0827233791351318\n",
      "Step: 4649, Loss: 0.9794037938117981, Accuracy: 0.90625, Computation time: 1.0548059940338135\n",
      "Step: 4650, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 0.926081657409668\n",
      "Step: 4651, Loss: 0.9158936738967896, Accuracy: 1.0, Computation time: 1.0758132934570312\n",
      "Step: 4652, Loss: 0.939142107963562, Accuracy: 0.96875, Computation time: 1.3437421321868896\n",
      "Step: 4653, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 0.9109773635864258\n",
      "Step: 4654, Loss: 0.9160677194595337, Accuracy: 1.0, Computation time: 1.2541143894195557\n",
      "Step: 4655, Loss: 0.9181961417198181, Accuracy: 1.0, Computation time: 1.546377420425415\n",
      "Step: 4656, Loss: 0.9377355575561523, Accuracy: 0.96875, Computation time: 0.9700286388397217\n",
      "Step: 4657, Loss: 0.9162890911102295, Accuracy: 1.0, Computation time: 1.1417410373687744\n",
      "Step: 4658, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 0.9803004264831543\n",
      "Step: 4659, Loss: 0.9160816669464111, Accuracy: 1.0, Computation time: 1.0404047966003418\n",
      "Step: 4660, Loss: 0.9159640669822693, Accuracy: 1.0, Computation time: 1.199199914932251\n",
      "Step: 4661, Loss: 0.9160072207450867, Accuracy: 1.0, Computation time: 1.170774221420288\n",
      "Step: 4662, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 0.9286301136016846\n",
      "Step: 4663, Loss: 0.9167864322662354, Accuracy: 1.0, Computation time: 1.4464125633239746\n",
      "Step: 4664, Loss: 0.9375210404396057, Accuracy: 0.96875, Computation time: 1.2292580604553223\n",
      "Step: 4665, Loss: 0.9159213304519653, Accuracy: 1.0, Computation time: 1.1467363834381104\n",
      "Step: 4666, Loss: 0.9163216352462769, Accuracy: 1.0, Computation time: 1.057694673538208\n",
      "Step: 4667, Loss: 0.9159517288208008, Accuracy: 1.0, Computation time: 1.0027985572814941\n",
      "Step: 4668, Loss: 0.9159900546073914, Accuracy: 1.0, Computation time: 1.3764853477478027\n",
      "Step: 4669, Loss: 0.9373845458030701, Accuracy: 0.96875, Computation time: 0.9587907791137695\n",
      "Step: 4670, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.0822253227233887\n",
      "Step: 4671, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.0355045795440674\n",
      "Step: 4672, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.939516544342041\n",
      "Step: 4673, Loss: 0.9354485273361206, Accuracy: 0.96875, Computation time: 1.5691759586334229\n",
      "Step: 4674, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.2463605403900146\n",
      "Step: 4675, Loss: 0.9372518658638, Accuracy: 0.96875, Computation time: 1.706993818283081\n",
      "Step: 4676, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.0386784076690674\n",
      "Step: 4677, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 1.0633931159973145\n",
      "Step: 4678, Loss: 0.9345523715019226, Accuracy: 0.96875, Computation time: 0.914179801940918\n",
      "Step: 4679, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.3471496105194092\n",
      "Step: 4680, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.0915939807891846\n",
      "Step: 4681, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.3616116046905518\n",
      "Step: 4682, Loss: 0.9159812331199646, Accuracy: 1.0, Computation time: 1.8018734455108643\n",
      "Step: 4683, Loss: 0.9159727692604065, Accuracy: 1.0, Computation time: 1.263519525527954\n",
      "Step: 4684, Loss: 0.9172533750534058, Accuracy: 1.0, Computation time: 1.7194254398345947\n",
      "Step: 4685, Loss: 0.9174222350120544, Accuracy: 1.0, Computation time: 1.3885586261749268\n",
      "Step: 4686, Loss: 0.915948212146759, Accuracy: 1.0, Computation time: 1.6889221668243408\n",
      "Step: 4687, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.445598840713501\n",
      "Step: 4688, Loss: 0.9324061274528503, Accuracy: 0.96875, Computation time: 2.121720552444458\n",
      "Step: 4689, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.212296962738037\n",
      "Step: 4690, Loss: 0.9160385727882385, Accuracy: 1.0, Computation time: 1.4580950736999512\n",
      "Step: 4691, Loss: 0.9159641265869141, Accuracy: 1.0, Computation time: 1.0812628269195557\n",
      "Step: 4692, Loss: 0.9160244464874268, Accuracy: 1.0, Computation time: 1.4405910968780518\n",
      "Step: 4693, Loss: 0.9161170125007629, Accuracy: 1.0, Computation time: 1.465973138809204\n",
      "Step: 4694, Loss: 0.9159647822380066, Accuracy: 1.0, Computation time: 1.156324863433838\n",
      "Step: 4695, Loss: 0.9159519672393799, Accuracy: 1.0, Computation time: 1.2166895866394043\n",
      "Step: 4696, Loss: 0.915973424911499, Accuracy: 1.0, Computation time: 1.1800081729888916\n",
      "Step: 4697, Loss: 0.9375030994415283, Accuracy: 0.96875, Computation time: 1.4679694175720215\n",
      "Step: 4698, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.3036322593688965\n",
      "Step: 4699, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.2343461513519287\n",
      "Step: 4700, Loss: 0.9175479412078857, Accuracy: 1.0, Computation time: 1.8712010383605957\n",
      "Step: 4701, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.268965721130371\n",
      "Step: 4702, Loss: 0.9375690221786499, Accuracy: 0.96875, Computation time: 2.1471385955810547\n",
      "Step: 4703, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 1.566741943359375\n",
      "Step: 4704, Loss: 0.9162271618843079, Accuracy: 1.0, Computation time: 2.3645570278167725\n",
      "Step: 4705, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 1.3115592002868652\n",
      "Step: 4706, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.8557748794555664\n",
      "Step: 4707, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 2.0585672855377197\n",
      "Step: 4708, Loss: 0.9392264485359192, Accuracy: 0.96875, Computation time: 1.475975513458252\n",
      "Step: 4709, Loss: 0.9374857544898987, Accuracy: 0.96875, Computation time: 1.5382192134857178\n",
      "Step: 4710, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.6667582988739014\n",
      "Step: 4711, Loss: 0.9374842643737793, Accuracy: 0.96875, Computation time: 1.3515722751617432\n",
      "Step: 4712, Loss: 0.9373533129692078, Accuracy: 0.96875, Computation time: 1.3472096920013428\n",
      "Step: 4713, Loss: 0.9159573912620544, Accuracy: 1.0, Computation time: 1.63441801071167\n",
      "Step: 4714, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.3861353397369385\n",
      "Step: 4715, Loss: 0.9159427881240845, Accuracy: 1.0, Computation time: 1.9113543033599854\n",
      "Step: 4716, Loss: 0.932685911655426, Accuracy: 0.96875, Computation time: 1.4688093662261963\n",
      "Step: 4717, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.215376853942871\n",
      "Step: 4718, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.8951513767242432\n",
      "Step: 4719, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.6636865139007568\n",
      "Step: 4720, Loss: 0.9159379005432129, Accuracy: 1.0, Computation time: 1.326721429824829\n",
      "Step: 4721, Loss: 0.9375184178352356, Accuracy: 0.96875, Computation time: 1.5112943649291992\n",
      "Step: 4722, Loss: 0.9524251222610474, Accuracy: 0.9375, Computation time: 1.4900357723236084\n",
      "Step: 4723, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.4689760208129883\n",
      "Step: 4724, Loss: 0.9160574674606323, Accuracy: 1.0, Computation time: 1.3295936584472656\n",
      "########################\n",
      "Test loss: 1.1222444772720337, Test Accuracy_epoch34: 0.6970185041427612\n",
      "########################\n",
      "Step: 4725, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.2264034748077393\n",
      "Step: 4726, Loss: 0.9159253239631653, Accuracy: 1.0, Computation time: 1.5084259510040283\n",
      "Step: 4727, Loss: 0.9158900380134583, Accuracy: 1.0, Computation time: 1.4292857646942139\n",
      "Step: 4728, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 1.4207513332366943\n",
      "Step: 4729, Loss: 0.9226549863815308, Accuracy: 1.0, Computation time: 2.4288506507873535\n",
      "Step: 4730, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.5376930236816406\n",
      "Step: 4731, Loss: 0.9164515733718872, Accuracy: 1.0, Computation time: 1.3314309120178223\n",
      "Step: 4732, Loss: 0.915982723236084, Accuracy: 1.0, Computation time: 1.3612074851989746\n",
      "Step: 4733, Loss: 0.9160268902778625, Accuracy: 1.0, Computation time: 1.2741010189056396\n",
      "Step: 4734, Loss: 0.9394568204879761, Accuracy: 0.96875, Computation time: 1.291924238204956\n",
      "Step: 4735, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.2458922863006592\n",
      "Step: 4736, Loss: 0.9162046909332275, Accuracy: 1.0, Computation time: 1.3125402927398682\n",
      "Step: 4737, Loss: 0.9185704588890076, Accuracy: 1.0, Computation time: 1.9357249736785889\n",
      "Step: 4738, Loss: 0.9160012602806091, Accuracy: 1.0, Computation time: 1.4158990383148193\n",
      "Step: 4739, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.2264177799224854\n",
      "Step: 4740, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 1.3777780532836914\n",
      "Step: 4741, Loss: 0.9375089406967163, Accuracy: 0.96875, Computation time: 1.4137237071990967\n",
      "Step: 4742, Loss: 0.9159696102142334, Accuracy: 1.0, Computation time: 1.0958168506622314\n",
      "Step: 4743, Loss: 0.9160864949226379, Accuracy: 1.0, Computation time: 1.3141582012176514\n",
      "Step: 4744, Loss: 0.9159629940986633, Accuracy: 1.0, Computation time: 1.1960835456848145\n",
      "Step: 4745, Loss: 0.9379569292068481, Accuracy: 0.96875, Computation time: 1.7481768131256104\n",
      "Step: 4746, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 1.5856554508209229\n",
      "Step: 4747, Loss: 0.9375779032707214, Accuracy: 0.96875, Computation time: 1.4269211292266846\n",
      "Step: 4748, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 1.3540890216827393\n",
      "Step: 4749, Loss: 0.9159581661224365, Accuracy: 1.0, Computation time: 1.3515195846557617\n",
      "Step: 4750, Loss: 0.9161351919174194, Accuracy: 1.0, Computation time: 1.1643028259277344\n",
      "Step: 4751, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.3479113578796387\n",
      "Step: 4752, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 1.3535518646240234\n",
      "Step: 4753, Loss: 0.9220156073570251, Accuracy: 1.0, Computation time: 1.3892183303833008\n",
      "Step: 4754, Loss: 0.9166802167892456, Accuracy: 1.0, Computation time: 1.6847760677337646\n",
      "Step: 4755, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.3730604648590088\n",
      "Step: 4756, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.391068458557129\n",
      "Step: 4757, Loss: 0.9161106944084167, Accuracy: 1.0, Computation time: 1.3767554759979248\n",
      "Step: 4758, Loss: 0.9160447120666504, Accuracy: 1.0, Computation time: 1.5525877475738525\n",
      "Step: 4759, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.3400371074676514\n",
      "Step: 4760, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.0608673095703125\n",
      "Step: 4761, Loss: 0.9162025451660156, Accuracy: 1.0, Computation time: 2.0683579444885254\n",
      "Step: 4762, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.3264355659484863\n",
      "Step: 4763, Loss: 0.9374917149543762, Accuracy: 0.96875, Computation time: 1.6736102104187012\n",
      "Step: 4764, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.1590702533721924\n",
      "Step: 4765, Loss: 0.9502801895141602, Accuracy: 0.9375, Computation time: 1.2853624820709229\n",
      "Step: 4766, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 1.2777209281921387\n",
      "Step: 4767, Loss: 0.9161130785942078, Accuracy: 1.0, Computation time: 1.4162523746490479\n",
      "Step: 4768, Loss: 0.9160138368606567, Accuracy: 1.0, Computation time: 1.9443976879119873\n",
      "Step: 4769, Loss: 0.9160310626029968, Accuracy: 1.0, Computation time: 1.3323302268981934\n",
      "Step: 4770, Loss: 0.9165939092636108, Accuracy: 1.0, Computation time: 1.5369303226470947\n",
      "Step: 4771, Loss: 0.9368359446525574, Accuracy: 0.96875, Computation time: 1.6033964157104492\n",
      "Step: 4772, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 1.4092483520507812\n",
      "Step: 4773, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.367492914199829\n",
      "Step: 4774, Loss: 0.9159346222877502, Accuracy: 1.0, Computation time: 1.798877239227295\n",
      "Step: 4775, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.0216634273529053\n",
      "Step: 4776, Loss: 0.9161227941513062, Accuracy: 1.0, Computation time: 1.924616813659668\n",
      "Step: 4777, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.3007943630218506\n",
      "Step: 4778, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.1200971603393555\n",
      "Step: 4779, Loss: 0.9176605343818665, Accuracy: 1.0, Computation time: 1.483816146850586\n",
      "Step: 4780, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.4832193851470947\n",
      "Step: 4781, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.657388687133789\n",
      "Step: 4782, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 1.492952823638916\n",
      "Step: 4783, Loss: 0.9161818623542786, Accuracy: 1.0, Computation time: 1.3613932132720947\n",
      "Step: 4784, Loss: 0.9428284168243408, Accuracy: 0.96875, Computation time: 1.919393539428711\n",
      "Step: 4785, Loss: 0.916653573513031, Accuracy: 1.0, Computation time: 1.5033652782440186\n",
      "Step: 4786, Loss: 0.9201882481575012, Accuracy: 1.0, Computation time: 2.092133045196533\n",
      "Step: 4787, Loss: 0.9159477353096008, Accuracy: 1.0, Computation time: 1.5746097564697266\n",
      "Step: 4788, Loss: 0.9159956574440002, Accuracy: 1.0, Computation time: 1.1526174545288086\n",
      "Step: 4789, Loss: 0.9163349270820618, Accuracy: 1.0, Computation time: 1.1472301483154297\n",
      "Step: 4790, Loss: 0.9160749316215515, Accuracy: 1.0, Computation time: 1.6538949012756348\n",
      "Step: 4791, Loss: 0.9159651398658752, Accuracy: 1.0, Computation time: 1.6855816841125488\n",
      "Step: 4792, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.3369476795196533\n",
      "Step: 4793, Loss: 0.916034996509552, Accuracy: 1.0, Computation time: 1.2382283210754395\n",
      "Step: 4794, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 1.352707862854004\n",
      "Step: 4795, Loss: 0.9161613583564758, Accuracy: 1.0, Computation time: 1.3440663814544678\n",
      "Step: 4796, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.4091808795928955\n",
      "Step: 4797, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.7168219089508057\n",
      "Step: 4798, Loss: 0.9307729601860046, Accuracy: 0.96875, Computation time: 2.053030252456665\n",
      "Step: 4799, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 1.64151930809021\n",
      "Step: 4800, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.3531043529510498\n",
      "Step: 4801, Loss: 0.9159679412841797, Accuracy: 1.0, Computation time: 1.4765348434448242\n",
      "Step: 4802, Loss: 0.9159945249557495, Accuracy: 1.0, Computation time: 1.3815937042236328\n",
      "Step: 4803, Loss: 0.9377524256706238, Accuracy: 0.96875, Computation time: 1.8654873371124268\n",
      "Step: 4804, Loss: 0.937678873538971, Accuracy: 0.96875, Computation time: 1.051360845565796\n",
      "Step: 4805, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.648430585861206\n",
      "Step: 4806, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 1.0869081020355225\n",
      "Step: 4807, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.3614966869354248\n",
      "Step: 4808, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.5389738082885742\n",
      "Step: 4809, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.3616623878479004\n",
      "Step: 4810, Loss: 0.9159982204437256, Accuracy: 1.0, Computation time: 1.7610626220703125\n",
      "Step: 4811, Loss: 0.9163784980773926, Accuracy: 1.0, Computation time: 1.4903953075408936\n",
      "Step: 4812, Loss: 0.9162650108337402, Accuracy: 1.0, Computation time: 1.5253722667694092\n",
      "Step: 4813, Loss: 0.9375741481781006, Accuracy: 0.96875, Computation time: 1.4389066696166992\n",
      "Step: 4814, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.5499250888824463\n",
      "Step: 4815, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.1722137928009033\n",
      "Step: 4816, Loss: 0.916179895401001, Accuracy: 1.0, Computation time: 2.2719032764434814\n",
      "Step: 4817, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 1.3974277973175049\n",
      "Step: 4818, Loss: 0.9160498380661011, Accuracy: 1.0, Computation time: 1.9382646083831787\n",
      "Step: 4819, Loss: 0.9377498626708984, Accuracy: 0.96875, Computation time: 1.3515737056732178\n",
      "Step: 4820, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.5713679790496826\n",
      "Step: 4821, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.427408218383789\n",
      "Step: 4822, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 1.2626934051513672\n",
      "Step: 4823, Loss: 0.9395154714584351, Accuracy: 0.96875, Computation time: 1.6699519157409668\n",
      "Step: 4824, Loss: 0.9203259944915771, Accuracy: 1.0, Computation time: 1.8995742797851562\n",
      "Step: 4825, Loss: 0.9375588297843933, Accuracy: 0.96875, Computation time: 1.2782843112945557\n",
      "Step: 4826, Loss: 0.916013777256012, Accuracy: 1.0, Computation time: 1.2704107761383057\n",
      "Step: 4827, Loss: 0.9161285758018494, Accuracy: 1.0, Computation time: 1.395284652709961\n",
      "Step: 4828, Loss: 0.9159661531448364, Accuracy: 1.0, Computation time: 1.1437463760375977\n",
      "Step: 4829, Loss: 0.9160531163215637, Accuracy: 1.0, Computation time: 1.5406863689422607\n",
      "Step: 4830, Loss: 0.915932834148407, Accuracy: 1.0, Computation time: 1.15095853805542\n",
      "Step: 4831, Loss: 0.9377017021179199, Accuracy: 0.96875, Computation time: 1.361527681350708\n",
      "Step: 4832, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 1.55476713180542\n",
      "Step: 4833, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.3832387924194336\n",
      "Step: 4834, Loss: 0.916022777557373, Accuracy: 1.0, Computation time: 1.4770066738128662\n",
      "Step: 4835, Loss: 0.9159457087516785, Accuracy: 1.0, Computation time: 1.0978903770446777\n",
      "Step: 4836, Loss: 0.9380823373794556, Accuracy: 0.96875, Computation time: 1.6120445728302002\n",
      "Step: 4837, Loss: 0.9159027934074402, Accuracy: 1.0, Computation time: 1.4274799823760986\n",
      "Step: 4838, Loss: 0.9370832443237305, Accuracy: 0.96875, Computation time: 1.549046277999878\n",
      "Step: 4839, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.4172098636627197\n",
      "Step: 4840, Loss: 0.9159682989120483, Accuracy: 1.0, Computation time: 1.2412128448486328\n",
      "Step: 4841, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.7951710224151611\n",
      "Step: 4842, Loss: 0.9375237822532654, Accuracy: 0.96875, Computation time: 1.6449434757232666\n",
      "Step: 4843, Loss: 0.9161809682846069, Accuracy: 1.0, Computation time: 1.5082902908325195\n",
      "Step: 4844, Loss: 0.9377738237380981, Accuracy: 0.96875, Computation time: 1.6365242004394531\n",
      "Step: 4845, Loss: 0.9210724830627441, Accuracy: 1.0, Computation time: 1.756958246231079\n",
      "Step: 4846, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.4263110160827637\n",
      "Step: 4847, Loss: 0.9374805092811584, Accuracy: 0.96875, Computation time: 1.5484366416931152\n",
      "Step: 4848, Loss: 0.9159546494483948, Accuracy: 1.0, Computation time: 1.174797773361206\n",
      "Step: 4849, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 1.1595218181610107\n",
      "Step: 4850, Loss: 0.9159759283065796, Accuracy: 1.0, Computation time: 1.3692045211791992\n",
      "Step: 4851, Loss: 0.9375625252723694, Accuracy: 0.96875, Computation time: 1.7074341773986816\n",
      "Step: 4852, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.528996229171753\n",
      "Step: 4853, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 1.3037662506103516\n",
      "Step: 4854, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.4105749130249023\n",
      "Step: 4855, Loss: 0.9472566246986389, Accuracy: 0.9375, Computation time: 1.4051446914672852\n",
      "Step: 4856, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.7721364498138428\n",
      "Step: 4857, Loss: 0.9159528613090515, Accuracy: 1.0, Computation time: 1.2916653156280518\n",
      "Step: 4858, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.4520540237426758\n",
      "Step: 4859, Loss: 0.9159106612205505, Accuracy: 1.0, Computation time: 1.5203444957733154\n",
      "Step: 4860, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.5485877990722656\n",
      "Step: 4861, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.4216580390930176\n",
      "Step: 4862, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.5905535221099854\n",
      "Step: 4863, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.651733636856079\n",
      "########################\n",
      "Test loss: 1.1227130889892578, Test Accuracy_epoch35: 0.7010475397109985\n",
      "########################\n",
      "Step: 4864, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.5915029048919678\n",
      "Step: 4865, Loss: 0.9334920048713684, Accuracy: 0.96875, Computation time: 1.824131727218628\n",
      "Step: 4866, Loss: 0.9590633511543274, Accuracy: 0.9375, Computation time: 1.4455797672271729\n",
      "Step: 4867, Loss: 0.9160016775131226, Accuracy: 1.0, Computation time: 1.599992036819458\n",
      "Step: 4868, Loss: 0.9580024480819702, Accuracy: 0.9375, Computation time: 1.6976332664489746\n",
      "Step: 4869, Loss: 0.9159421920776367, Accuracy: 1.0, Computation time: 1.7691245079040527\n",
      "Step: 4870, Loss: 0.9420035481452942, Accuracy: 0.96875, Computation time: 1.9583525657653809\n",
      "Step: 4871, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.3015594482421875\n",
      "Step: 4872, Loss: 0.9158727526664734, Accuracy: 1.0, Computation time: 1.2881724834442139\n",
      "Step: 4873, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.4529640674591064\n",
      "Step: 4874, Loss: 0.9160728454589844, Accuracy: 1.0, Computation time: 2.0331239700317383\n",
      "Step: 4875, Loss: 0.9159709811210632, Accuracy: 1.0, Computation time: 1.5609753131866455\n",
      "Step: 4876, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 1.495849609375\n",
      "Step: 4877, Loss: 0.9159383773803711, Accuracy: 1.0, Computation time: 1.5279691219329834\n",
      "Step: 4878, Loss: 0.91592937707901, Accuracy: 1.0, Computation time: 1.4910578727722168\n",
      "Step: 4879, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 1.6900227069854736\n",
      "Step: 4880, Loss: 0.9380459785461426, Accuracy: 0.96875, Computation time: 2.076146125793457\n",
      "Step: 4881, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.5242466926574707\n",
      "Step: 4882, Loss: 0.9318471550941467, Accuracy: 0.96875, Computation time: 1.5951130390167236\n",
      "Step: 4883, Loss: 0.9201235771179199, Accuracy: 1.0, Computation time: 1.6790122985839844\n",
      "Step: 4884, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 1.5327584743499756\n",
      "Step: 4885, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 1.3449299335479736\n",
      "Step: 4886, Loss: 0.9159776568412781, Accuracy: 1.0, Computation time: 1.117093801498413\n",
      "Step: 4887, Loss: 0.9159356355667114, Accuracy: 1.0, Computation time: 1.2048659324645996\n",
      "Step: 4888, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.6872363090515137\n",
      "Step: 4889, Loss: 0.9168145656585693, Accuracy: 1.0, Computation time: 2.1578078269958496\n",
      "Step: 4890, Loss: 0.9356472492218018, Accuracy: 0.96875, Computation time: 1.3489196300506592\n",
      "Step: 4891, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 1.355940341949463\n",
      "Step: 4892, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.46278715133667\n",
      "Step: 4893, Loss: 0.9375381469726562, Accuracy: 0.96875, Computation time: 1.4352502822875977\n",
      "Step: 4894, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.2891268730163574\n",
      "Step: 4895, Loss: 0.9162166118621826, Accuracy: 1.0, Computation time: 1.5109126567840576\n",
      "Step: 4896, Loss: 0.9179419279098511, Accuracy: 1.0, Computation time: 1.915477991104126\n",
      "Step: 4897, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.1112208366394043\n",
      "Step: 4898, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.2274644374847412\n",
      "Step: 4899, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 1.0996248722076416\n",
      "Step: 4900, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 1.2748589515686035\n",
      "Step: 4901, Loss: 0.9182963371276855, Accuracy: 1.0, Computation time: 1.8142671585083008\n",
      "Step: 4902, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 0.9962031841278076\n",
      "Step: 4903, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.991654634475708\n",
      "Step: 4904, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.3070366382598877\n",
      "Step: 4905, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 2.0636656284332275\n",
      "Step: 4906, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.6175620555877686\n",
      "Step: 4907, Loss: 0.9375773072242737, Accuracy: 0.96875, Computation time: 1.058945894241333\n",
      "Step: 4908, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.2577974796295166\n",
      "Step: 4909, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.4043283462524414\n",
      "Step: 4910, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.2922248840332031\n",
      "Step: 4911, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.4919042587280273\n",
      "Step: 4912, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.4604225158691406\n",
      "Step: 4913, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 1.1665008068084717\n",
      "Step: 4914, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.109468698501587\n",
      "Step: 4915, Loss: 0.9375309348106384, Accuracy: 0.96875, Computation time: 1.6574795246124268\n",
      "Step: 4916, Loss: 0.9331478476524353, Accuracy: 0.96875, Computation time: 1.7071614265441895\n",
      "Step: 4917, Loss: 0.9173731207847595, Accuracy: 1.0, Computation time: 2.309143304824829\n",
      "Step: 4918, Loss: 0.9372034072875977, Accuracy: 0.96875, Computation time: 1.2067232131958008\n",
      "Step: 4919, Loss: 0.9301422834396362, Accuracy: 0.96875, Computation time: 1.2794122695922852\n",
      "Step: 4920, Loss: 0.9160705804824829, Accuracy: 1.0, Computation time: 2.035433769226074\n",
      "Step: 4921, Loss: 0.915948212146759, Accuracy: 1.0, Computation time: 1.491852045059204\n",
      "Step: 4922, Loss: 0.9161589741706848, Accuracy: 1.0, Computation time: 1.6757559776306152\n",
      "Step: 4923, Loss: 0.9375884532928467, Accuracy: 0.96875, Computation time: 1.4708619117736816\n",
      "Step: 4924, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 1.8293206691741943\n",
      "Step: 4925, Loss: 0.915961742401123, Accuracy: 1.0, Computation time: 1.6506078243255615\n",
      "Step: 4926, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.8671519756317139\n",
      "Step: 4927, Loss: 0.9159758687019348, Accuracy: 1.0, Computation time: 1.4218707084655762\n",
      "Step: 4928, Loss: 0.9159286618232727, Accuracy: 1.0, Computation time: 1.0625028610229492\n",
      "Step: 4929, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.3457341194152832\n",
      "Step: 4930, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 1.2103745937347412\n",
      "Step: 4931, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.1456689834594727\n",
      "Step: 4932, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 1.1521244049072266\n",
      "Step: 4933, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.7799365520477295\n",
      "Step: 4934, Loss: 0.9168798327445984, Accuracy: 1.0, Computation time: 1.2095508575439453\n",
      "Step: 4935, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 1.443171739578247\n",
      "Step: 4936, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.4443776607513428\n",
      "Step: 4937, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.246593713760376\n",
      "Step: 4938, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.2677967548370361\n",
      "Step: 4939, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.323347806930542\n",
      "Step: 4940, Loss: 0.937563955783844, Accuracy: 0.96875, Computation time: 1.4204294681549072\n",
      "Step: 4941, Loss: 0.9163398742675781, Accuracy: 1.0, Computation time: 1.666666030883789\n",
      "Step: 4942, Loss: 0.9375426769256592, Accuracy: 0.96875, Computation time: 1.2020528316497803\n",
      "Step: 4943, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.0722577571868896\n",
      "Step: 4944, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.1035065650939941\n",
      "Step: 4945, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.3634068965911865\n",
      "Step: 4946, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 1.0255863666534424\n",
      "Step: 4947, Loss: 0.9375314116477966, Accuracy: 0.96875, Computation time: 1.2733078002929688\n",
      "Step: 4948, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.572134256362915\n",
      "Step: 4949, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.601637363433838\n",
      "Step: 4950, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.0296425819396973\n",
      "Step: 4951, Loss: 0.9173141121864319, Accuracy: 1.0, Computation time: 1.4657855033874512\n",
      "Step: 4952, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.2688865661621094\n",
      "Step: 4953, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.426271915435791\n",
      "Step: 4954, Loss: 0.9167853593826294, Accuracy: 1.0, Computation time: 1.1786901950836182\n",
      "Step: 4955, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 1.323868989944458\n",
      "Step: 4956, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 1.1893346309661865\n",
      "Step: 4957, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.2536735534667969\n",
      "Step: 4958, Loss: 0.9159197807312012, Accuracy: 1.0, Computation time: 1.4991717338562012\n",
      "Step: 4959, Loss: 0.9170590043067932, Accuracy: 1.0, Computation time: 1.326467752456665\n",
      "Step: 4960, Loss: 0.9159125089645386, Accuracy: 1.0, Computation time: 1.3787610530853271\n",
      "Step: 4961, Loss: 0.9160915017127991, Accuracy: 1.0, Computation time: 1.1960084438323975\n",
      "Step: 4962, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.268258810043335\n",
      "Step: 4963, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.3324413299560547\n",
      "Step: 4964, Loss: 0.9378212094306946, Accuracy: 0.96875, Computation time: 1.4563407897949219\n",
      "Step: 4965, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.3657653331756592\n",
      "Step: 4966, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.170625925064087\n",
      "Step: 4967, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.2441067695617676\n",
      "Step: 4968, Loss: 0.9179084897041321, Accuracy: 1.0, Computation time: 1.6338448524475098\n",
      "Step: 4969, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 1.2661333084106445\n",
      "Step: 4970, Loss: 0.9504119157791138, Accuracy: 0.9375, Computation time: 1.4810431003570557\n",
      "Step: 4971, Loss: 0.9159756898880005, Accuracy: 1.0, Computation time: 1.4119787216186523\n",
      "Step: 4972, Loss: 0.937846839427948, Accuracy: 0.96875, Computation time: 1.8150169849395752\n",
      "Step: 4973, Loss: 0.9375108480453491, Accuracy: 0.96875, Computation time: 1.390577793121338\n",
      "Step: 4974, Loss: 0.915961503982544, Accuracy: 1.0, Computation time: 1.5213086605072021\n",
      "Step: 4975, Loss: 0.9160356521606445, Accuracy: 1.0, Computation time: 1.3293707370758057\n",
      "Step: 4976, Loss: 0.9159519672393799, Accuracy: 1.0, Computation time: 1.4731385707855225\n",
      "Step: 4977, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.1879770755767822\n",
      "Step: 4978, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.6927051544189453\n",
      "Step: 4979, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.2275502681732178\n",
      "Step: 4980, Loss: 0.9159628748893738, Accuracy: 1.0, Computation time: 1.018648624420166\n",
      "Step: 4981, Loss: 0.9161672592163086, Accuracy: 1.0, Computation time: 1.146209716796875\n",
      "Step: 4982, Loss: 0.9159554839134216, Accuracy: 1.0, Computation time: 1.1165838241577148\n",
      "Step: 4983, Loss: 0.9375846982002258, Accuracy: 0.96875, Computation time: 2.2005388736724854\n",
      "Step: 4984, Loss: 0.9158543944358826, Accuracy: 1.0, Computation time: 1.2845242023468018\n",
      "Step: 4985, Loss: 0.9374649524688721, Accuracy: 0.96875, Computation time: 1.0442216396331787\n",
      "Step: 4986, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.1751527786254883\n",
      "Step: 4987, Loss: 0.9191340208053589, Accuracy: 1.0, Computation time: 1.7623939514160156\n",
      "Step: 4988, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.1499242782592773\n",
      "Step: 4989, Loss: 0.9376012682914734, Accuracy: 0.96875, Computation time: 1.2049736976623535\n",
      "Step: 4990, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 1.1734704971313477\n",
      "Step: 4991, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.2682745456695557\n",
      "Step: 4992, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.2229094505310059\n",
      "Step: 4993, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.2000584602355957\n",
      "Step: 4994, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 1.1759986877441406\n",
      "Step: 4995, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.024052619934082\n",
      "Step: 4996, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.166701316833496\n",
      "Step: 4997, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.3838646411895752\n",
      "Step: 4998, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.3631880283355713\n",
      "Step: 4999, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.4349629878997803\n",
      "Step: 5000, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.2264866828918457\n",
      "Step: 5001, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.3192198276519775\n",
      "Step: 5002, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.1795248985290527\n",
      "########################\n",
      "Test loss: 1.1254388093948364, Test Accuracy_epoch36: 0.6937953233718872\n",
      "########################\n",
      "Step: 5003, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.4559712409973145\n",
      "Step: 5004, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.6749663352966309\n",
      "Step: 5005, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 0.9953477382659912\n",
      "Step: 5006, Loss: 0.9159244298934937, Accuracy: 1.0, Computation time: 1.6820929050445557\n",
      "Step: 5007, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.3898744583129883\n",
      "Step: 5008, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 1.3734104633331299\n",
      "Step: 5009, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 1.3718490600585938\n",
      "Step: 5010, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 1.7910752296447754\n",
      "Step: 5011, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.6545956134796143\n",
      "Step: 5012, Loss: 0.9159388542175293, Accuracy: 1.0, Computation time: 1.0930135250091553\n",
      "Step: 5013, Loss: 0.9382808208465576, Accuracy: 0.96875, Computation time: 1.2417781352996826\n",
      "Step: 5014, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.8896021842956543\n",
      "Step: 5015, Loss: 0.9373903274536133, Accuracy: 0.96875, Computation time: 1.777259349822998\n",
      "Step: 5016, Loss: 0.9159294962882996, Accuracy: 1.0, Computation time: 1.6972861289978027\n",
      "Step: 5017, Loss: 0.9363040924072266, Accuracy: 0.96875, Computation time: 1.2779285907745361\n",
      "Step: 5018, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.5446887016296387\n",
      "Step: 5019, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.3813097476959229\n",
      "Step: 5020, Loss: 0.9175025224685669, Accuracy: 1.0, Computation time: 1.8840901851654053\n",
      "Step: 5021, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.2080492973327637\n",
      "Step: 5022, Loss: 0.9166164994239807, Accuracy: 1.0, Computation time: 1.819690227508545\n",
      "Step: 5023, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 1.4269037246704102\n",
      "Step: 5024, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.4731199741363525\n",
      "Step: 5025, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.7400681972503662\n",
      "Step: 5026, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.2358121871948242\n",
      "Step: 5027, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.687964916229248\n",
      "Step: 5028, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.307112455368042\n",
      "Step: 5029, Loss: 0.9322991967201233, Accuracy: 0.96875, Computation time: 2.0463006496429443\n",
      "Step: 5030, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.164412260055542\n",
      "Step: 5031, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.5963852405548096\n",
      "Step: 5032, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 2.237318992614746\n",
      "Step: 5033, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 1.5335962772369385\n",
      "Step: 5034, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 1.2852132320404053\n",
      "Step: 5035, Loss: 0.9159075617790222, Accuracy: 1.0, Computation time: 1.3609788417816162\n",
      "Step: 5036, Loss: 0.9375186562538147, Accuracy: 0.96875, Computation time: 1.6166908740997314\n",
      "Step: 5037, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.3538634777069092\n",
      "Step: 5038, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 0.9725735187530518\n",
      "Step: 5039, Loss: 0.9161544442176819, Accuracy: 1.0, Computation time: 1.3350176811218262\n",
      "Step: 5040, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.2579138278961182\n",
      "Step: 5041, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.383108139038086\n",
      "Step: 5042, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.4117562770843506\n",
      "Step: 5043, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.5737659931182861\n",
      "Step: 5044, Loss: 0.9161519408226013, Accuracy: 1.0, Computation time: 1.1699564456939697\n",
      "Step: 5045, Loss: 0.9383426904678345, Accuracy: 0.96875, Computation time: 1.2852869033813477\n",
      "Step: 5046, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.1366991996765137\n",
      "Step: 5047, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.296492576599121\n",
      "Step: 5048, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.198075771331787\n",
      "Step: 5049, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.2924237251281738\n",
      "Step: 5050, Loss: 0.9160305857658386, Accuracy: 1.0, Computation time: 1.6658308506011963\n",
      "Step: 5051, Loss: 0.9158543944358826, Accuracy: 1.0, Computation time: 1.2019834518432617\n",
      "Step: 5052, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 1.774521827697754\n",
      "Step: 5053, Loss: 0.9376949071884155, Accuracy: 0.96875, Computation time: 1.707484483718872\n",
      "Step: 5054, Loss: 0.936505913734436, Accuracy: 0.96875, Computation time: 1.5495197772979736\n",
      "Step: 5055, Loss: 0.9375112652778625, Accuracy: 0.96875, Computation time: 1.309929370880127\n",
      "Step: 5056, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.3900501728057861\n",
      "Step: 5057, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.1075446605682373\n",
      "Step: 5058, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.180922269821167\n",
      "Step: 5059, Loss: 0.937528133392334, Accuracy: 0.96875, Computation time: 1.6313533782958984\n",
      "Step: 5060, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.5335378646850586\n",
      "Step: 5061, Loss: 0.9591374397277832, Accuracy: 0.9375, Computation time: 1.4915051460266113\n",
      "Step: 5062, Loss: 0.9158425331115723, Accuracy: 1.0, Computation time: 1.2025489807128906\n",
      "Step: 5063, Loss: 0.9374924898147583, Accuracy: 0.96875, Computation time: 1.380500316619873\n",
      "Step: 5064, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 1.2248599529266357\n",
      "Step: 5065, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 0.9907450675964355\n",
      "Step: 5066, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.1369287967681885\n",
      "Step: 5067, Loss: 0.9161144495010376, Accuracy: 1.0, Computation time: 1.4086737632751465\n",
      "Step: 5068, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.4777531623840332\n",
      "Step: 5069, Loss: 0.9259536266326904, Accuracy: 0.96875, Computation time: 1.6609776020050049\n",
      "Step: 5070, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.5661001205444336\n",
      "Step: 5071, Loss: 0.9166094660758972, Accuracy: 1.0, Computation time: 1.554520606994629\n",
      "Step: 5072, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 1.4727187156677246\n",
      "Step: 5073, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 1.4970970153808594\n",
      "Step: 5074, Loss: 0.9161576628684998, Accuracy: 1.0, Computation time: 1.257308006286621\n",
      "Step: 5075, Loss: 0.916711688041687, Accuracy: 1.0, Computation time: 1.4441721439361572\n",
      "Step: 5076, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.1431968212127686\n",
      "Step: 5077, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 1.6504709720611572\n",
      "Step: 5078, Loss: 0.9161036610603333, Accuracy: 1.0, Computation time: 1.417337417602539\n",
      "Step: 5079, Loss: 0.9375242590904236, Accuracy: 0.96875, Computation time: 1.1325256824493408\n",
      "Step: 5080, Loss: 0.937665581703186, Accuracy: 0.96875, Computation time: 1.3407533168792725\n",
      "Step: 5081, Loss: 0.937314510345459, Accuracy: 0.96875, Computation time: 1.9305226802825928\n",
      "Step: 5082, Loss: 0.9159485101699829, Accuracy: 1.0, Computation time: 1.3900871276855469\n",
      "Step: 5083, Loss: 0.9160191416740417, Accuracy: 1.0, Computation time: 1.2077827453613281\n",
      "Step: 5084, Loss: 0.9373078346252441, Accuracy: 0.96875, Computation time: 1.0386145114898682\n",
      "Step: 5085, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.248354434967041\n",
      "Step: 5086, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.3790919780731201\n",
      "Step: 5087, Loss: 0.9284083843231201, Accuracy: 0.96875, Computation time: 1.6012847423553467\n",
      "Step: 5088, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.4068934917449951\n",
      "Step: 5089, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.6531293392181396\n",
      "Step: 5090, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.2856817245483398\n",
      "Step: 5091, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 1.474135398864746\n",
      "Step: 5092, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.4013559818267822\n",
      "Step: 5093, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.6875910758972168\n",
      "Step: 5094, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.4405696392059326\n",
      "Step: 5095, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.3408198356628418\n",
      "Step: 5096, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.4508838653564453\n",
      "Step: 5097, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.1157760620117188\n",
      "Step: 5098, Loss: 0.9163400530815125, Accuracy: 1.0, Computation time: 1.3682715892791748\n",
      "Step: 5099, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.5130622386932373\n",
      "Step: 5100, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.4676027297973633\n",
      "Step: 5101, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.1676075458526611\n",
      "Step: 5102, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.5944161415100098\n",
      "Step: 5103, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 1.2693774700164795\n",
      "Step: 5104, Loss: 0.9375395774841309, Accuracy: 0.96875, Computation time: 1.5270593166351318\n",
      "Step: 5105, Loss: 0.9173634052276611, Accuracy: 1.0, Computation time: 1.3914003372192383\n",
      "Step: 5106, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 1.4726340770721436\n",
      "Step: 5107, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.3737342357635498\n",
      "Step: 5108, Loss: 0.937491238117218, Accuracy: 0.96875, Computation time: 1.9679582118988037\n",
      "Step: 5109, Loss: 0.937556803226471, Accuracy: 0.96875, Computation time: 1.8636419773101807\n",
      "Step: 5110, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 1.334782600402832\n",
      "Step: 5111, Loss: 0.9372942447662354, Accuracy: 0.96875, Computation time: 1.4171168804168701\n",
      "Step: 5112, Loss: 0.9159335494041443, Accuracy: 1.0, Computation time: 1.6429760456085205\n",
      "Step: 5113, Loss: 0.9376737475395203, Accuracy: 0.96875, Computation time: 1.5833113193511963\n",
      "Step: 5114, Loss: 0.9163568615913391, Accuracy: 1.0, Computation time: 1.538869857788086\n",
      "Step: 5115, Loss: 0.9174880385398865, Accuracy: 1.0, Computation time: 1.4192414283752441\n",
      "Step: 5116, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.224482774734497\n",
      "Step: 5117, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.3841652870178223\n",
      "Step: 5118, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.1092052459716797\n",
      "Step: 5119, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.2185518741607666\n",
      "Step: 5120, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 1.4894957542419434\n",
      "Step: 5121, Loss: 0.9205021262168884, Accuracy: 1.0, Computation time: 2.477328062057495\n",
      "Step: 5122, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.8359429836273193\n",
      "Step: 5123, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.4281184673309326\n",
      "Step: 5124, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.6985704898834229\n",
      "Step: 5125, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 1.287766933441162\n",
      "Step: 5126, Loss: 0.9375734329223633, Accuracy: 0.96875, Computation time: 1.77016282081604\n",
      "Step: 5127, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.3864789009094238\n",
      "Step: 5128, Loss: 0.9166028499603271, Accuracy: 1.0, Computation time: 1.1646711826324463\n",
      "Step: 5129, Loss: 0.9158921837806702, Accuracy: 1.0, Computation time: 1.2287518978118896\n",
      "Step: 5130, Loss: 0.9376130700111389, Accuracy: 0.96875, Computation time: 1.213484764099121\n",
      "Step: 5131, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.571329116821289\n",
      "Step: 5132, Loss: 0.91758131980896, Accuracy: 1.0, Computation time: 1.4175989627838135\n",
      "Step: 5133, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 1.436375617980957\n",
      "Step: 5134, Loss: 0.9159626960754395, Accuracy: 1.0, Computation time: 1.5934407711029053\n",
      "Step: 5135, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 1.1563711166381836\n",
      "Step: 5136, Loss: 0.9165924191474915, Accuracy: 1.0, Computation time: 1.8422951698303223\n",
      "Step: 5137, Loss: 0.9174728393554688, Accuracy: 1.0, Computation time: 1.4591267108917236\n",
      "Step: 5138, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.5998859405517578\n",
      "Step: 5139, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.635073184967041\n",
      "Step: 5140, Loss: 0.9375234842300415, Accuracy: 0.96875, Computation time: 1.3797478675842285\n",
      "Step: 5141, Loss: 0.9160164594650269, Accuracy: 1.0, Computation time: 1.566408634185791\n",
      "########################\n",
      "Test loss: 1.124906301498413, Test Accuracy_epoch37: 0.6954069137573242\n",
      "########################\n",
      "Step: 5142, Loss: 0.9160345792770386, Accuracy: 1.0, Computation time: 1.8061821460723877\n",
      "Step: 5143, Loss: 0.9376431703567505, Accuracy: 0.96875, Computation time: 1.5427050590515137\n",
      "Step: 5144, Loss: 0.9424381256103516, Accuracy: 0.96875, Computation time: 1.232858419418335\n",
      "Step: 5145, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.2802557945251465\n",
      "Step: 5146, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.2090632915496826\n",
      "Step: 5147, Loss: 0.9375669956207275, Accuracy: 0.96875, Computation time: 1.6960651874542236\n",
      "Step: 5148, Loss: 0.9159699082374573, Accuracy: 1.0, Computation time: 1.4362051486968994\n",
      "Step: 5149, Loss: 0.9170238971710205, Accuracy: 1.0, Computation time: 1.5392942428588867\n",
      "Step: 5150, Loss: 0.9159543514251709, Accuracy: 1.0, Computation time: 1.2225675582885742\n",
      "Step: 5151, Loss: 0.915959894657135, Accuracy: 1.0, Computation time: 1.4808368682861328\n",
      "Step: 5152, Loss: 0.9377037286758423, Accuracy: 0.96875, Computation time: 1.4955384731292725\n",
      "Step: 5153, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.650268316268921\n",
      "Step: 5154, Loss: 0.9373729825019836, Accuracy: 0.96875, Computation time: 1.3829803466796875\n",
      "Step: 5155, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.5780153274536133\n",
      "Step: 5156, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 1.629455327987671\n",
      "Step: 5157, Loss: 0.9374682903289795, Accuracy: 0.96875, Computation time: 1.3357036113739014\n",
      "Step: 5158, Loss: 0.9159771203994751, Accuracy: 1.0, Computation time: 1.501662254333496\n",
      "Step: 5159, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.2233400344848633\n",
      "Step: 5160, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.5694820880889893\n",
      "Step: 5161, Loss: 0.9351022839546204, Accuracy: 0.96875, Computation time: 1.3896989822387695\n",
      "Step: 5162, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.299198865890503\n",
      "Step: 5163, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.3314335346221924\n",
      "Step: 5164, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.4033095836639404\n",
      "Step: 5165, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.2160723209381104\n",
      "Step: 5166, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 1.6581921577453613\n",
      "Step: 5167, Loss: 0.927189826965332, Accuracy: 0.96875, Computation time: 2.7248733043670654\n",
      "Step: 5168, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.7148990631103516\n",
      "Step: 5169, Loss: 0.9160202741622925, Accuracy: 1.0, Computation time: 1.6271114349365234\n",
      "Step: 5170, Loss: 0.9160109758377075, Accuracy: 1.0, Computation time: 1.5577235221862793\n",
      "Step: 5171, Loss: 0.9160802364349365, Accuracy: 1.0, Computation time: 1.273763656616211\n",
      "Step: 5172, Loss: 0.9161022901535034, Accuracy: 1.0, Computation time: 1.265406608581543\n",
      "Step: 5173, Loss: 0.9160395860671997, Accuracy: 1.0, Computation time: 1.101151466369629\n",
      "Step: 5174, Loss: 0.9160094261169434, Accuracy: 1.0, Computation time: 1.4315767288208008\n",
      "Step: 5175, Loss: 0.9162096977233887, Accuracy: 1.0, Computation time: 1.3765642642974854\n",
      "Step: 5176, Loss: 0.934607982635498, Accuracy: 0.96875, Computation time: 1.9257347583770752\n",
      "Step: 5177, Loss: 0.9161337018013, Accuracy: 1.0, Computation time: 1.3210484981536865\n",
      "Step: 5178, Loss: 0.9172537326812744, Accuracy: 1.0, Computation time: 1.2724504470825195\n",
      "Step: 5179, Loss: 0.9172899127006531, Accuracy: 1.0, Computation time: 1.1072888374328613\n",
      "Step: 5180, Loss: 0.9171923398971558, Accuracy: 1.0, Computation time: 1.4241714477539062\n",
      "Step: 5181, Loss: 0.9169545769691467, Accuracy: 1.0, Computation time: 1.2474849224090576\n",
      "Step: 5182, Loss: 0.9170814156532288, Accuracy: 1.0, Computation time: 1.314554214477539\n",
      "Step: 5183, Loss: 0.9583178758621216, Accuracy: 0.9375, Computation time: 2.2530577182769775\n",
      "Step: 5184, Loss: 0.9257709980010986, Accuracy: 1.0, Computation time: 1.5864160060882568\n",
      "Step: 5185, Loss: 0.9199767708778381, Accuracy: 1.0, Computation time: 1.2727787494659424\n",
      "Step: 5186, Loss: 0.9171039462089539, Accuracy: 1.0, Computation time: 1.4090802669525146\n",
      "Step: 5187, Loss: 0.9441962242126465, Accuracy: 0.96875, Computation time: 1.2990443706512451\n",
      "Step: 5188, Loss: 0.9167358875274658, Accuracy: 1.0, Computation time: 2.1396193504333496\n",
      "Step: 5189, Loss: 0.9163763523101807, Accuracy: 1.0, Computation time: 1.3089215755462646\n",
      "Step: 5190, Loss: 0.9167165160179138, Accuracy: 1.0, Computation time: 1.0234317779541016\n",
      "Step: 5191, Loss: 0.916490375995636, Accuracy: 1.0, Computation time: 1.029094934463501\n",
      "Step: 5192, Loss: 0.9208106398582458, Accuracy: 1.0, Computation time: 1.6575417518615723\n",
      "Step: 5193, Loss: 0.9377883076667786, Accuracy: 0.96875, Computation time: 1.3730878829956055\n",
      "Step: 5194, Loss: 0.9168612957000732, Accuracy: 1.0, Computation time: 1.1166484355926514\n",
      "Step: 5195, Loss: 0.9190291166305542, Accuracy: 1.0, Computation time: 1.4979243278503418\n",
      "Step: 5196, Loss: 0.91932612657547, Accuracy: 1.0, Computation time: 1.811769723892212\n",
      "Step: 5197, Loss: 0.916254997253418, Accuracy: 1.0, Computation time: 1.0616655349731445\n",
      "Step: 5198, Loss: 0.9389675259590149, Accuracy: 0.96875, Computation time: 1.6478137969970703\n",
      "Step: 5199, Loss: 0.9164378046989441, Accuracy: 1.0, Computation time: 1.6034934520721436\n",
      "Step: 5200, Loss: 0.916174590587616, Accuracy: 1.0, Computation time: 1.2483251094818115\n",
      "Step: 5201, Loss: 0.917745053768158, Accuracy: 1.0, Computation time: 1.5415494441986084\n",
      "Step: 5202, Loss: 0.9161513447761536, Accuracy: 1.0, Computation time: 1.1896300315856934\n",
      "Step: 5203, Loss: 0.9166334867477417, Accuracy: 1.0, Computation time: 1.5281848907470703\n",
      "Step: 5204, Loss: 0.9161771535873413, Accuracy: 1.0, Computation time: 1.1450562477111816\n",
      "Step: 5205, Loss: 0.9162318706512451, Accuracy: 1.0, Computation time: 1.1799325942993164\n",
      "Step: 5206, Loss: 0.9164298176765442, Accuracy: 1.0, Computation time: 1.4412810802459717\n",
      "Step: 5207, Loss: 0.935184895992279, Accuracy: 0.96875, Computation time: 1.8008573055267334\n",
      "Step: 5208, Loss: 0.9164643883705139, Accuracy: 1.0, Computation time: 1.392822265625\n",
      "Step: 5209, Loss: 0.9160078763961792, Accuracy: 1.0, Computation time: 1.140503168106079\n",
      "Step: 5210, Loss: 0.9162370562553406, Accuracy: 1.0, Computation time: 1.2384722232818604\n",
      "Step: 5211, Loss: 0.916030764579773, Accuracy: 1.0, Computation time: 1.3317339420318604\n",
      "Step: 5212, Loss: 0.9168448448181152, Accuracy: 1.0, Computation time: 2.1079397201538086\n",
      "Step: 5213, Loss: 0.9160345792770386, Accuracy: 1.0, Computation time: 1.1428143978118896\n",
      "Step: 5214, Loss: 0.9162163734436035, Accuracy: 1.0, Computation time: 1.1290979385375977\n",
      "Step: 5215, Loss: 0.9233944416046143, Accuracy: 1.0, Computation time: 1.5505964756011963\n",
      "Step: 5216, Loss: 0.9163011312484741, Accuracy: 1.0, Computation time: 1.079559564590454\n",
      "Step: 5217, Loss: 0.9161554574966431, Accuracy: 1.0, Computation time: 1.407409906387329\n",
      "Step: 5218, Loss: 0.9242552518844604, Accuracy: 1.0, Computation time: 1.29254150390625\n",
      "Step: 5219, Loss: 0.9372028708457947, Accuracy: 0.96875, Computation time: 1.3447656631469727\n",
      "Step: 5220, Loss: 0.9162208437919617, Accuracy: 1.0, Computation time: 1.0017116069793701\n",
      "Step: 5221, Loss: 0.9163176417350769, Accuracy: 1.0, Computation time: 1.1741487979888916\n",
      "Step: 5222, Loss: 0.9162602424621582, Accuracy: 1.0, Computation time: 1.2111032009124756\n",
      "Step: 5223, Loss: 0.9161539077758789, Accuracy: 1.0, Computation time: 1.437169075012207\n",
      "Step: 5224, Loss: 0.9376526474952698, Accuracy: 0.96875, Computation time: 1.2207796573638916\n",
      "Step: 5225, Loss: 0.9160542488098145, Accuracy: 1.0, Computation time: 1.0230724811553955\n",
      "Step: 5226, Loss: 0.9160755276679993, Accuracy: 1.0, Computation time: 1.0960302352905273\n",
      "Step: 5227, Loss: 0.9168323874473572, Accuracy: 1.0, Computation time: 1.1946806907653809\n",
      "Step: 5228, Loss: 0.9162763357162476, Accuracy: 1.0, Computation time: 1.4088327884674072\n",
      "Step: 5229, Loss: 0.9160945415496826, Accuracy: 1.0, Computation time: 1.0813052654266357\n",
      "Step: 5230, Loss: 0.9299396276473999, Accuracy: 0.96875, Computation time: 1.4691524505615234\n",
      "Step: 5231, Loss: 0.915973424911499, Accuracy: 1.0, Computation time: 1.0969047546386719\n",
      "Step: 5232, Loss: 0.915988564491272, Accuracy: 1.0, Computation time: 1.162337064743042\n",
      "Step: 5233, Loss: 0.9303566813468933, Accuracy: 1.0, Computation time: 1.9556171894073486\n",
      "Step: 5234, Loss: 0.9376986622810364, Accuracy: 0.96875, Computation time: 1.2222745418548584\n",
      "Step: 5235, Loss: 0.9163728952407837, Accuracy: 1.0, Computation time: 1.309523582458496\n",
      "Step: 5236, Loss: 0.916570246219635, Accuracy: 1.0, Computation time: 1.3303601741790771\n",
      "Step: 5237, Loss: 0.9175048470497131, Accuracy: 1.0, Computation time: 1.9487543106079102\n",
      "Step: 5238, Loss: 0.916170060634613, Accuracy: 1.0, Computation time: 1.2889845371246338\n",
      "Step: 5239, Loss: 0.9163473844528198, Accuracy: 1.0, Computation time: 1.2732560634613037\n",
      "Step: 5240, Loss: 0.9162784218788147, Accuracy: 1.0, Computation time: 1.3488190174102783\n",
      "Step: 5241, Loss: 0.9161197543144226, Accuracy: 1.0, Computation time: 1.1471290588378906\n",
      "Step: 5242, Loss: 0.9311918616294861, Accuracy: 0.96875, Computation time: 1.9257993698120117\n",
      "Step: 5243, Loss: 0.9166679382324219, Accuracy: 1.0, Computation time: 1.6634235382080078\n",
      "Step: 5244, Loss: 0.9168602824211121, Accuracy: 1.0, Computation time: 1.6144523620605469\n",
      "Step: 5245, Loss: 0.941550076007843, Accuracy: 0.96875, Computation time: 1.5332601070404053\n",
      "Step: 5246, Loss: 0.916221022605896, Accuracy: 1.0, Computation time: 1.2216448783874512\n",
      "Step: 5247, Loss: 0.9161701202392578, Accuracy: 1.0, Computation time: 1.2843008041381836\n",
      "Step: 5248, Loss: 0.9279868602752686, Accuracy: 0.96875, Computation time: 1.9812581539154053\n",
      "Step: 5249, Loss: 0.9164350032806396, Accuracy: 1.0, Computation time: 0.9900200366973877\n",
      "Step: 5250, Loss: 0.9173598289489746, Accuracy: 1.0, Computation time: 1.2608106136322021\n",
      "Step: 5251, Loss: 0.9579585790634155, Accuracy: 0.9375, Computation time: 1.361609697341919\n",
      "Step: 5252, Loss: 0.9167152643203735, Accuracy: 1.0, Computation time: 1.2989554405212402\n",
      "Step: 5253, Loss: 0.9264714121818542, Accuracy: 0.96875, Computation time: 1.723210334777832\n",
      "Step: 5254, Loss: 0.9162616729736328, Accuracy: 1.0, Computation time: 1.0888752937316895\n",
      "Step: 5255, Loss: 0.9348262548446655, Accuracy: 0.96875, Computation time: 1.6083824634552002\n",
      "Step: 5256, Loss: 0.9406560659408569, Accuracy: 0.96875, Computation time: 1.3553504943847656\n",
      "Step: 5257, Loss: 0.9162248373031616, Accuracy: 1.0, Computation time: 1.3345255851745605\n",
      "Step: 5258, Loss: 0.9167865514755249, Accuracy: 1.0, Computation time: 1.2778704166412354\n",
      "Step: 5259, Loss: 0.9165433049201965, Accuracy: 1.0, Computation time: 1.0448591709136963\n",
      "Step: 5260, Loss: 0.9168798327445984, Accuracy: 1.0, Computation time: 1.3255915641784668\n",
      "Step: 5261, Loss: 0.9164182543754578, Accuracy: 1.0, Computation time: 1.1553142070770264\n",
      "Step: 5262, Loss: 0.9164025783538818, Accuracy: 1.0, Computation time: 1.2054004669189453\n",
      "Step: 5263, Loss: 0.9165428876876831, Accuracy: 1.0, Computation time: 1.245246410369873\n",
      "Step: 5264, Loss: 0.9173012375831604, Accuracy: 1.0, Computation time: 1.1894495487213135\n",
      "Step: 5265, Loss: 0.9223725199699402, Accuracy: 1.0, Computation time: 1.4576756954193115\n",
      "Step: 5266, Loss: 0.9310721158981323, Accuracy: 0.96875, Computation time: 1.3835411071777344\n",
      "Step: 5267, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.0813348293304443\n",
      "Step: 5268, Loss: 0.9162435531616211, Accuracy: 1.0, Computation time: 1.2397215366363525\n",
      "Step: 5269, Loss: 0.9165188074111938, Accuracy: 1.0, Computation time: 1.162144660949707\n",
      "Step: 5270, Loss: 0.9163649082183838, Accuracy: 1.0, Computation time: 1.299767017364502\n",
      "Step: 5271, Loss: 0.9166241884231567, Accuracy: 1.0, Computation time: 1.171149492263794\n",
      "Step: 5272, Loss: 0.9286359548568726, Accuracy: 0.96875, Computation time: 1.3706178665161133\n",
      "Step: 5273, Loss: 0.9163479208946228, Accuracy: 1.0, Computation time: 1.1822905540466309\n",
      "Step: 5274, Loss: 0.9185968041419983, Accuracy: 1.0, Computation time: 1.1556158065795898\n",
      "Step: 5275, Loss: 0.9372267127037048, Accuracy: 0.96875, Computation time: 1.1239116191864014\n",
      "Step: 5276, Loss: 0.9162259101867676, Accuracy: 1.0, Computation time: 1.3601500988006592\n",
      "Step: 5277, Loss: 0.9160603880882263, Accuracy: 1.0, Computation time: 1.0085582733154297\n",
      "Step: 5278, Loss: 0.9377327561378479, Accuracy: 0.96875, Computation time: 1.1431457996368408\n",
      "Step: 5279, Loss: 0.9384343028068542, Accuracy: 0.96875, Computation time: 1.3997628688812256\n",
      "Step: 5280, Loss: 0.9160579442977905, Accuracy: 1.0, Computation time: 1.2444212436676025\n",
      "########################\n",
      "Test loss: 1.123077154159546, Test Accuracy_epoch38: 0.6954069137573242\n",
      "########################\n",
      "Step: 5281, Loss: 0.9162309765815735, Accuracy: 1.0, Computation time: 1.3108124732971191\n",
      "Step: 5282, Loss: 0.9202501773834229, Accuracy: 1.0, Computation time: 1.1561408042907715\n",
      "Step: 5283, Loss: 0.9161568284034729, Accuracy: 1.0, Computation time: 1.2152414321899414\n",
      "Step: 5284, Loss: 0.9165171384811401, Accuracy: 1.0, Computation time: 1.6539320945739746\n",
      "Step: 5285, Loss: 0.9160284399986267, Accuracy: 1.0, Computation time: 1.245424509048462\n",
      "Step: 5286, Loss: 0.9162473082542419, Accuracy: 1.0, Computation time: 1.205996036529541\n",
      "Step: 5287, Loss: 0.9164677858352661, Accuracy: 1.0, Computation time: 1.2015769481658936\n",
      "Step: 5288, Loss: 0.9160094857215881, Accuracy: 1.0, Computation time: 1.1673007011413574\n",
      "Step: 5289, Loss: 0.9159587025642395, Accuracy: 1.0, Computation time: 1.155036449432373\n",
      "Step: 5290, Loss: 0.9161933660507202, Accuracy: 1.0, Computation time: 1.360842227935791\n",
      "Step: 5291, Loss: 0.9163150191307068, Accuracy: 1.0, Computation time: 1.4879748821258545\n",
      "Step: 5292, Loss: 0.9159126877784729, Accuracy: 1.0, Computation time: 1.0938432216644287\n",
      "Step: 5293, Loss: 0.9159756898880005, Accuracy: 1.0, Computation time: 1.03340482711792\n",
      "Step: 5294, Loss: 0.9159930348396301, Accuracy: 1.0, Computation time: 1.4099137783050537\n",
      "Step: 5295, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 1.1738455295562744\n",
      "Step: 5296, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.1192975044250488\n",
      "Step: 5297, Loss: 0.9160388708114624, Accuracy: 1.0, Computation time: 1.807858943939209\n",
      "Step: 5298, Loss: 0.9160221219062805, Accuracy: 1.0, Computation time: 1.205442190170288\n",
      "Step: 5299, Loss: 0.9375816583633423, Accuracy: 0.96875, Computation time: 1.3148088455200195\n",
      "Step: 5300, Loss: 0.9188085198402405, Accuracy: 1.0, Computation time: 1.3677291870117188\n",
      "Step: 5301, Loss: 0.9374927878379822, Accuracy: 0.96875, Computation time: 1.284761667251587\n",
      "Step: 5302, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.0506632328033447\n",
      "Step: 5303, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.2602219581604004\n",
      "Step: 5304, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 1.1340880393981934\n",
      "Step: 5305, Loss: 0.9376856684684753, Accuracy: 0.96875, Computation time: 1.622617483139038\n",
      "Step: 5306, Loss: 0.9159618020057678, Accuracy: 1.0, Computation time: 1.5880675315856934\n",
      "Step: 5307, Loss: 0.9810805916786194, Accuracy: 0.90625, Computation time: 1.0401625633239746\n",
      "Step: 5308, Loss: 0.9159457087516785, Accuracy: 1.0, Computation time: 1.4943277835845947\n",
      "Step: 5309, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.390575885772705\n",
      "Step: 5310, Loss: 0.9160484075546265, Accuracy: 1.0, Computation time: 1.8235642910003662\n",
      "Step: 5311, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.1722800731658936\n",
      "Step: 5312, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 1.1737170219421387\n",
      "Step: 5313, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 1.2116353511810303\n",
      "Step: 5314, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 1.0898032188415527\n",
      "Step: 5315, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 1.2523651123046875\n",
      "Step: 5316, Loss: 0.9232491254806519, Accuracy: 1.0, Computation time: 1.590667724609375\n",
      "Step: 5317, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 0.9562642574310303\n",
      "Step: 5318, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.0793187618255615\n",
      "Step: 5319, Loss: 0.9170407056808472, Accuracy: 1.0, Computation time: 1.7768182754516602\n",
      "Step: 5320, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 1.427558183670044\n",
      "Step: 5321, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.1078879833221436\n",
      "Step: 5322, Loss: 0.91597580909729, Accuracy: 1.0, Computation time: 1.1173045635223389\n",
      "Step: 5323, Loss: 0.9161513447761536, Accuracy: 1.0, Computation time: 1.299752950668335\n",
      "Step: 5324, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.187946081161499\n",
      "Step: 5325, Loss: 0.9369233846664429, Accuracy: 0.96875, Computation time: 1.1254785060882568\n",
      "Step: 5326, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 1.5037541389465332\n",
      "Step: 5327, Loss: 0.9162712693214417, Accuracy: 1.0, Computation time: 1.0899763107299805\n",
      "Step: 5328, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.2062456607818604\n",
      "Step: 5329, Loss: 0.9159253239631653, Accuracy: 1.0, Computation time: 1.127164363861084\n",
      "Step: 5330, Loss: 0.9168432950973511, Accuracy: 1.0, Computation time: 1.1457741260528564\n",
      "Step: 5331, Loss: 0.9376048445701599, Accuracy: 0.96875, Computation time: 1.260084629058838\n",
      "Step: 5332, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.206434965133667\n",
      "Step: 5333, Loss: 0.9160265326499939, Accuracy: 1.0, Computation time: 1.2416160106658936\n",
      "Step: 5334, Loss: 0.9358610510826111, Accuracy: 0.96875, Computation time: 1.4909372329711914\n",
      "Step: 5335, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.137948751449585\n",
      "Step: 5336, Loss: 0.9159932732582092, Accuracy: 1.0, Computation time: 1.167095422744751\n",
      "Step: 5337, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.5243690013885498\n",
      "Step: 5338, Loss: 0.9377270340919495, Accuracy: 0.96875, Computation time: 1.608870506286621\n",
      "Step: 5339, Loss: 0.9159433245658875, Accuracy: 1.0, Computation time: 1.2173564434051514\n",
      "Step: 5340, Loss: 0.9183654189109802, Accuracy: 1.0, Computation time: 1.2428603172302246\n",
      "Step: 5341, Loss: 0.9217551350593567, Accuracy: 1.0, Computation time: 1.3558716773986816\n",
      "Step: 5342, Loss: 0.9159739017486572, Accuracy: 1.0, Computation time: 1.1000230312347412\n",
      "Step: 5343, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.0650131702423096\n",
      "Step: 5344, Loss: 0.9160435795783997, Accuracy: 1.0, Computation time: 1.5474042892456055\n",
      "Step: 5345, Loss: 0.9160147309303284, Accuracy: 1.0, Computation time: 1.7885055541992188\n",
      "Step: 5346, Loss: 0.9159805774688721, Accuracy: 1.0, Computation time: 1.1186213493347168\n",
      "Step: 5347, Loss: 0.9159327745437622, Accuracy: 1.0, Computation time: 1.6188900470733643\n",
      "Step: 5348, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.2988371849060059\n",
      "Step: 5349, Loss: 0.9299343824386597, Accuracy: 0.96875, Computation time: 1.28342866897583\n",
      "Step: 5350, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 1.095992088317871\n",
      "Step: 5351, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.3239450454711914\n",
      "Step: 5352, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 0.9025826454162598\n",
      "Step: 5353, Loss: 0.9159836769104004, Accuracy: 1.0, Computation time: 1.0545940399169922\n",
      "Step: 5354, Loss: 0.9159916639328003, Accuracy: 1.0, Computation time: 1.7516372203826904\n",
      "Step: 5355, Loss: 0.9160799384117126, Accuracy: 1.0, Computation time: 1.512956142425537\n",
      "Step: 5356, Loss: 0.9591860771179199, Accuracy: 0.9375, Computation time: 1.4392814636230469\n",
      "Step: 5357, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.1884784698486328\n",
      "Step: 5358, Loss: 0.915946900844574, Accuracy: 1.0, Computation time: 1.1445996761322021\n",
      "Step: 5359, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 0.9820499420166016\n",
      "Step: 5360, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.3711609840393066\n",
      "Step: 5361, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 1.533576488494873\n",
      "Step: 5362, Loss: 0.9159990549087524, Accuracy: 1.0, Computation time: 1.8281044960021973\n",
      "Step: 5363, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.9090001583099365\n",
      "Step: 5364, Loss: 0.919491708278656, Accuracy: 1.0, Computation time: 1.3665425777435303\n",
      "Step: 5365, Loss: 0.9322336316108704, Accuracy: 0.96875, Computation time: 1.7443690299987793\n",
      "Step: 5366, Loss: 0.9190212488174438, Accuracy: 1.0, Computation time: 1.6610314846038818\n",
      "Step: 5367, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 1.2673168182373047\n",
      "Step: 5368, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.1954894065856934\n",
      "Step: 5369, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.361882209777832\n",
      "Step: 5370, Loss: 0.9159957766532898, Accuracy: 1.0, Computation time: 1.0897071361541748\n",
      "Step: 5371, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.390899419784546\n",
      "Step: 5372, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 1.2995636463165283\n",
      "Step: 5373, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 1.5781972408294678\n",
      "Step: 5374, Loss: 0.9160628318786621, Accuracy: 1.0, Computation time: 1.4596962928771973\n",
      "Step: 5375, Loss: 0.9159457683563232, Accuracy: 1.0, Computation time: 1.1730568408966064\n",
      "Step: 5376, Loss: 0.9160242676734924, Accuracy: 1.0, Computation time: 1.2050724029541016\n",
      "Step: 5377, Loss: 0.936971127986908, Accuracy: 0.96875, Computation time: 1.1464719772338867\n",
      "Step: 5378, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.2779181003570557\n",
      "Step: 5379, Loss: 0.916893482208252, Accuracy: 1.0, Computation time: 1.62611985206604\n",
      "Step: 5380, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.0938396453857422\n",
      "Step: 5381, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 1.129296064376831\n",
      "Step: 5382, Loss: 0.9374723434448242, Accuracy: 0.96875, Computation time: 1.2837498188018799\n",
      "Step: 5383, Loss: 0.9167671799659729, Accuracy: 1.0, Computation time: 1.1932406425476074\n",
      "Step: 5384, Loss: 0.9166241884231567, Accuracy: 1.0, Computation time: 1.5674724578857422\n",
      "Step: 5385, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.4806146621704102\n",
      "Step: 5386, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.5882387161254883\n",
      "Step: 5387, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 1.1202974319458008\n",
      "Step: 5388, Loss: 0.9167107939720154, Accuracy: 1.0, Computation time: 1.2983498573303223\n",
      "Step: 5389, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 1.3808438777923584\n",
      "Step: 5390, Loss: 0.9372957944869995, Accuracy: 0.96875, Computation time: 1.6649236679077148\n",
      "Step: 5391, Loss: 0.916653573513031, Accuracy: 1.0, Computation time: 1.3500268459320068\n",
      "Step: 5392, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.598156452178955\n",
      "Step: 5393, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.2664234638214111\n",
      "Step: 5394, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 1.6191532611846924\n",
      "Step: 5395, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 1.0207486152648926\n",
      "Step: 5396, Loss: 0.9165792465209961, Accuracy: 1.0, Computation time: 1.149139165878296\n",
      "Step: 5397, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.0321717262268066\n",
      "Step: 5398, Loss: 0.9159648418426514, Accuracy: 1.0, Computation time: 1.6023533344268799\n",
      "Step: 5399, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 1.430495262145996\n",
      "Step: 5400, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.394242286682129\n",
      "Step: 5401, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.3542368412017822\n",
      "Step: 5402, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.3942456245422363\n",
      "Step: 5403, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.210160732269287\n",
      "Step: 5404, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.5093183517456055\n",
      "Step: 5405, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.3241322040557861\n",
      "Step: 5406, Loss: 0.9304141402244568, Accuracy: 0.96875, Computation time: 2.3518102169036865\n",
      "Step: 5407, Loss: 0.9374459981918335, Accuracy: 0.96875, Computation time: 1.4946489334106445\n",
      "Step: 5408, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.1832616329193115\n",
      "Step: 5409, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.1288032531738281\n",
      "Step: 5410, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 1.3358323574066162\n",
      "Step: 5411, Loss: 0.937553346157074, Accuracy: 0.96875, Computation time: 1.1516222953796387\n",
      "Step: 5412, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 1.1943416595458984\n",
      "Step: 5413, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.4879755973815918\n",
      "Step: 5414, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.1946063041687012\n",
      "Step: 5415, Loss: 0.9203384518623352, Accuracy: 1.0, Computation time: 1.2806837558746338\n",
      "Step: 5416, Loss: 0.9160957336425781, Accuracy: 1.0, Computation time: 1.2051656246185303\n",
      "Step: 5417, Loss: 0.9160546064376831, Accuracy: 1.0, Computation time: 1.331871747970581\n",
      "Step: 5418, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.123131513595581\n",
      "Step: 5419, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.1289393901824951\n",
      "########################\n",
      "Test loss: 1.1265109777450562, Test Accuracy_epoch39: 0.6937953233718872\n",
      "########################\n",
      "Step: 5420, Loss: 0.9160244464874268, Accuracy: 1.0, Computation time: 1.4455368518829346\n",
      "Step: 5421, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.1306049823760986\n",
      "Step: 5422, Loss: 0.9190747737884521, Accuracy: 1.0, Computation time: 1.2052280902862549\n",
      "Step: 5423, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.1026206016540527\n",
      "Step: 5424, Loss: 0.9186808466911316, Accuracy: 1.0, Computation time: 1.1952202320098877\n",
      "Step: 5425, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.1283471584320068\n",
      "Step: 5426, Loss: 0.9377064108848572, Accuracy: 0.96875, Computation time: 1.487213134765625\n",
      "Step: 5427, Loss: 0.9167411923408508, Accuracy: 1.0, Computation time: 1.1134147644042969\n",
      "Step: 5428, Loss: 0.9376294612884521, Accuracy: 0.96875, Computation time: 1.102618932723999\n",
      "Step: 5429, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.4339861869812012\n",
      "Step: 5430, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.1664032936096191\n",
      "Step: 5431, Loss: 0.959266722202301, Accuracy: 0.9375, Computation time: 1.4208734035491943\n",
      "Step: 5432, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.2779619693756104\n",
      "Step: 5433, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 1.265589952468872\n",
      "Step: 5434, Loss: 0.9375120997428894, Accuracy: 0.96875, Computation time: 1.346541404724121\n",
      "Step: 5435, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.2130746841430664\n",
      "Step: 5436, Loss: 0.9160394668579102, Accuracy: 1.0, Computation time: 1.4690895080566406\n",
      "Step: 5437, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 1.490299940109253\n",
      "Step: 5438, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 1.1241073608398438\n",
      "Step: 5439, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.1665101051330566\n",
      "Step: 5440, Loss: 0.9160001277923584, Accuracy: 1.0, Computation time: 1.2676148414611816\n",
      "Step: 5441, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.1947576999664307\n",
      "Step: 5442, Loss: 0.9167144894599915, Accuracy: 1.0, Computation time: 1.8402082920074463\n",
      "Step: 5443, Loss: 0.9158928394317627, Accuracy: 1.0, Computation time: 1.146460771560669\n",
      "Step: 5444, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.210918664932251\n",
      "Step: 5445, Loss: 0.9253209233283997, Accuracy: 1.0, Computation time: 1.651024580001831\n",
      "Step: 5446, Loss: 0.9210027456283569, Accuracy: 1.0, Computation time: 1.9483604431152344\n",
      "Step: 5447, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 1.075368881225586\n",
      "Step: 5448, Loss: 0.9381349682807922, Accuracy: 0.96875, Computation time: 1.3864045143127441\n",
      "Step: 5449, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.566864252090454\n",
      "Step: 5450, Loss: 0.9159871935844421, Accuracy: 1.0, Computation time: 1.5821433067321777\n",
      "Step: 5451, Loss: 0.9160337448120117, Accuracy: 1.0, Computation time: 1.8693242073059082\n",
      "Step: 5452, Loss: 0.9170686602592468, Accuracy: 1.0, Computation time: 1.2422895431518555\n",
      "Step: 5453, Loss: 0.9160946607589722, Accuracy: 1.0, Computation time: 1.4034442901611328\n",
      "Step: 5454, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 1.0004308223724365\n",
      "Step: 5455, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.3164124488830566\n",
      "Step: 5456, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.228407859802246\n",
      "Step: 5457, Loss: 0.9287682771682739, Accuracy: 0.96875, Computation time: 1.7079060077667236\n",
      "Step: 5458, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.2163949012756348\n",
      "Step: 5459, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.4226248264312744\n",
      "Step: 5460, Loss: 0.9159713387489319, Accuracy: 1.0, Computation time: 2.0272581577301025\n",
      "Step: 5461, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.1497783660888672\n",
      "Step: 5462, Loss: 0.9159504175186157, Accuracy: 1.0, Computation time: 1.2589826583862305\n",
      "Step: 5463, Loss: 0.9223247766494751, Accuracy: 1.0, Computation time: 1.1156089305877686\n",
      "Step: 5464, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.328263282775879\n",
      "Step: 5465, Loss: 0.9592322111129761, Accuracy: 0.9375, Computation time: 1.2845652103424072\n",
      "Step: 5466, Loss: 0.936985194683075, Accuracy: 0.96875, Computation time: 1.3811206817626953\n",
      "Step: 5467, Loss: 0.9207466244697571, Accuracy: 1.0, Computation time: 1.9235806465148926\n",
      "Step: 5468, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 1.0957982540130615\n",
      "Step: 5469, Loss: 0.9164153933525085, Accuracy: 1.0, Computation time: 2.1080191135406494\n",
      "Step: 5470, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 1.4634954929351807\n",
      "Step: 5471, Loss: 0.9159291386604309, Accuracy: 1.0, Computation time: 1.017293930053711\n",
      "Step: 5472, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 1.31801176071167\n",
      "Step: 5473, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 1.0723395347595215\n",
      "Step: 5474, Loss: 0.9373728632926941, Accuracy: 0.96875, Computation time: 1.7184834480285645\n",
      "Step: 5475, Loss: 0.9376201033592224, Accuracy: 0.96875, Computation time: 1.2207188606262207\n",
      "Step: 5476, Loss: 0.9159225225448608, Accuracy: 1.0, Computation time: 1.1849212646484375\n",
      "Step: 5477, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.5664572715759277\n",
      "Step: 5478, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.0298948287963867\n",
      "Step: 5479, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.098663330078125\n",
      "Step: 5480, Loss: 0.9375203251838684, Accuracy: 0.96875, Computation time: 1.2239229679107666\n",
      "Step: 5481, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 0.9464132785797119\n",
      "Step: 5482, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.378774642944336\n",
      "Step: 5483, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.1317627429962158\n",
      "Step: 5484, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.211270809173584\n",
      "Step: 5485, Loss: 0.9374672174453735, Accuracy: 0.96875, Computation time: 1.2733502388000488\n",
      "Step: 5486, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.207979679107666\n",
      "Step: 5487, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.1847400665283203\n",
      "Step: 5488, Loss: 0.9374783039093018, Accuracy: 0.96875, Computation time: 1.6108195781707764\n",
      "Step: 5489, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.5183639526367188\n",
      "Step: 5490, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.198315143585205\n",
      "Step: 5491, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.375175952911377\n",
      "Step: 5492, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.17777681350708\n",
      "Step: 5493, Loss: 0.9177919626235962, Accuracy: 1.0, Computation time: 1.4808807373046875\n",
      "Step: 5494, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.13944411277771\n",
      "Step: 5495, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.0360229015350342\n",
      "Step: 5496, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 1.1466288566589355\n",
      "Step: 5497, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.547032356262207\n",
      "Step: 5498, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.640946388244629\n",
      "Step: 5499, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.1849162578582764\n",
      "Step: 5500, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.168194055557251\n",
      "Step: 5501, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 0.9476938247680664\n",
      "Step: 5502, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.3206167221069336\n",
      "Step: 5503, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.7074167728424072\n",
      "Step: 5504, Loss: 0.9159907698631287, Accuracy: 1.0, Computation time: 1.852675199508667\n",
      "Step: 5505, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.2635633945465088\n",
      "Step: 5506, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.29119873046875\n",
      "Step: 5507, Loss: 0.9376547932624817, Accuracy: 0.96875, Computation time: 1.2005155086517334\n",
      "Step: 5508, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.1969242095947266\n",
      "Step: 5509, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 1.429051399230957\n",
      "Step: 5510, Loss: 0.9391619563102722, Accuracy: 0.96875, Computation time: 1.4327850341796875\n",
      "Step: 5511, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.4768149852752686\n",
      "Step: 5512, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.0977697372436523\n",
      "Step: 5513, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.1678566932678223\n",
      "Step: 5514, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.5372140407562256\n",
      "Step: 5515, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.5603938102722168\n",
      "Step: 5516, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.428553819656372\n",
      "Step: 5517, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.451512098312378\n",
      "Step: 5518, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.1094307899475098\n",
      "Step: 5519, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.3280346393585205\n",
      "Step: 5520, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.252000331878662\n",
      "Step: 5521, Loss: 0.9358012080192566, Accuracy: 0.96875, Computation time: 1.0797371864318848\n",
      "Step: 5522, Loss: 0.9162961840629578, Accuracy: 1.0, Computation time: 1.185694932937622\n",
      "Step: 5523, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 0.85654616355896\n",
      "Step: 5524, Loss: 0.9376600384712219, Accuracy: 0.96875, Computation time: 1.5766382217407227\n",
      "Step: 5525, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.2233798503875732\n",
      "Step: 5526, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.1638562679290771\n",
      "Step: 5527, Loss: 0.9358924031257629, Accuracy: 0.96875, Computation time: 1.1622309684753418\n",
      "Step: 5528, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.0696675777435303\n",
      "Step: 5529, Loss: 0.9161216020584106, Accuracy: 1.0, Computation time: 1.258702039718628\n",
      "Step: 5530, Loss: 0.9375209808349609, Accuracy: 0.96875, Computation time: 1.0539684295654297\n",
      "Step: 5531, Loss: 0.9273380041122437, Accuracy: 0.96875, Computation time: 1.956742286682129\n",
      "Step: 5532, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.1420857906341553\n",
      "Step: 5533, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.466155767440796\n",
      "Step: 5534, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.1535181999206543\n",
      "Step: 5535, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 1.223963975906372\n",
      "Step: 5536, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.048706293106079\n",
      "Step: 5537, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.0076043605804443\n",
      "Step: 5538, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.280116081237793\n",
      "Step: 5539, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.7942450046539307\n",
      "Step: 5540, Loss: 0.9375007152557373, Accuracy: 0.96875, Computation time: 1.015552043914795\n",
      "Step: 5541, Loss: 0.9158425331115723, Accuracy: 1.0, Computation time: 1.2401390075683594\n",
      "Step: 5542, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.1693170070648193\n",
      "Step: 5543, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.2616376876831055\n",
      "Step: 5544, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.9836657047271729\n",
      "Step: 5545, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.4828181266784668\n",
      "Step: 5546, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.5718042850494385\n",
      "Step: 5547, Loss: 0.9170196056365967, Accuracy: 1.0, Computation time: 1.3610897064208984\n",
      "Step: 5548, Loss: 0.9174894094467163, Accuracy: 1.0, Computation time: 2.0948610305786133\n",
      "Step: 5549, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 1.5213308334350586\n",
      "Step: 5550, Loss: 0.9373193383216858, Accuracy: 0.96875, Computation time: 1.3947064876556396\n",
      "Step: 5551, Loss: 0.9160864949226379, Accuracy: 1.0, Computation time: 1.3954660892486572\n",
      "Step: 5552, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.412536859512329\n",
      "Step: 5553, Loss: 0.9374696612358093, Accuracy: 0.96875, Computation time: 1.4404866695404053\n",
      "Step: 5554, Loss: 0.9375946521759033, Accuracy: 0.96875, Computation time: 1.1031935214996338\n",
      "Step: 5555, Loss: 0.9159880876541138, Accuracy: 1.0, Computation time: 1.5185449123382568\n",
      "Step: 5556, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.2088077068328857\n",
      "Step: 5557, Loss: 0.9373993873596191, Accuracy: 0.96875, Computation time: 1.4106543064117432\n",
      "Step: 5558, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.1141371726989746\n",
      "########################\n",
      "Test loss: 1.1232740879058838, Test Accuracy_epoch40: 0.6970185041427612\n",
      "########################\n",
      "Step: 5559, Loss: 0.9373860359191895, Accuracy: 0.96875, Computation time: 1.4899652004241943\n",
      "Step: 5560, Loss: 0.9354691505432129, Accuracy: 0.96875, Computation time: 1.3608624935150146\n",
      "Step: 5561, Loss: 0.937556803226471, Accuracy: 0.96875, Computation time: 1.4906954765319824\n",
      "Step: 5562, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.1394555568695068\n",
      "Step: 5563, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.0157859325408936\n",
      "Step: 5564, Loss: 0.9159470200538635, Accuracy: 1.0, Computation time: 1.1558864116668701\n",
      "Step: 5565, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 1.143730878829956\n",
      "Step: 5566, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 1.2619633674621582\n",
      "Step: 5567, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.2458455562591553\n",
      "Step: 5568, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 0.9713692665100098\n",
      "Step: 5569, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.2383546829223633\n",
      "Step: 5570, Loss: 0.9369769096374512, Accuracy: 0.96875, Computation time: 1.1000218391418457\n",
      "Step: 5571, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.6524500846862793\n",
      "Step: 5572, Loss: 0.9159724712371826, Accuracy: 1.0, Computation time: 1.3188502788543701\n",
      "Step: 5573, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.079204797744751\n",
      "Step: 5574, Loss: 0.938124418258667, Accuracy: 0.96875, Computation time: 1.519648551940918\n",
      "Step: 5575, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.4387028217315674\n",
      "Step: 5576, Loss: 0.9375412464141846, Accuracy: 0.96875, Computation time: 1.5597517490386963\n",
      "Step: 5577, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 1.084932565689087\n",
      "Step: 5578, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 1.609891653060913\n",
      "Step: 5579, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.1088173389434814\n",
      "Step: 5580, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.6910417079925537\n",
      "Step: 5581, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.3677704334259033\n",
      "Step: 5582, Loss: 0.9169384241104126, Accuracy: 1.0, Computation time: 1.602813482284546\n",
      "Step: 5583, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.3031868934631348\n",
      "Step: 5584, Loss: 0.9372591972351074, Accuracy: 0.96875, Computation time: 1.28525710105896\n",
      "Step: 5585, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.2730135917663574\n",
      "Step: 5586, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.1430222988128662\n",
      "Step: 5587, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 0.9961087703704834\n",
      "Step: 5588, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.3117587566375732\n",
      "Step: 5589, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 1.2270147800445557\n",
      "Step: 5590, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.9562649726867676\n",
      "Step: 5591, Loss: 0.9158470034599304, Accuracy: 1.0, Computation time: 0.9930877685546875\n",
      "Step: 5592, Loss: 0.9163608551025391, Accuracy: 1.0, Computation time: 1.5865588188171387\n",
      "Step: 5593, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.5393009185791016\n",
      "Step: 5594, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.204751968383789\n",
      "Step: 5595, Loss: 0.9171262383460999, Accuracy: 1.0, Computation time: 1.171140193939209\n",
      "Step: 5596, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.1555252075195312\n",
      "Step: 5597, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 1.1595661640167236\n",
      "Step: 5598, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.228118896484375\n",
      "Step: 5599, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.0765585899353027\n",
      "Step: 5600, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 0.9385910034179688\n",
      "Step: 5601, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.196157455444336\n",
      "Step: 5602, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.157628059387207\n",
      "Step: 5603, Loss: 0.9159510135650635, Accuracy: 1.0, Computation time: 1.1398372650146484\n",
      "Step: 5604, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.3643114566802979\n",
      "Step: 5605, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.4706416130065918\n",
      "Step: 5606, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 1.6528069972991943\n",
      "Step: 5607, Loss: 0.9158347845077515, Accuracy: 1.0, Computation time: 0.8680949211120605\n",
      "Step: 5608, Loss: 0.9158360362052917, Accuracy: 1.0, Computation time: 1.0967326164245605\n",
      "Step: 5609, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.1876707077026367\n",
      "Step: 5610, Loss: 0.91641765832901, Accuracy: 1.0, Computation time: 1.4469213485717773\n",
      "Step: 5611, Loss: 0.9375325441360474, Accuracy: 0.96875, Computation time: 1.226182222366333\n",
      "Step: 5612, Loss: 0.937577486038208, Accuracy: 0.96875, Computation time: 1.1204109191894531\n",
      "Step: 5613, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 1.2091681957244873\n",
      "Step: 5614, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.1545522212982178\n",
      "Step: 5615, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 1.2327990531921387\n",
      "Step: 5616, Loss: 0.937507152557373, Accuracy: 0.96875, Computation time: 1.2652308940887451\n",
      "Step: 5617, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.1220674514770508\n",
      "Step: 5618, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.568373680114746\n",
      "Step: 5619, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.1482558250427246\n",
      "Step: 5620, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.115720510482788\n",
      "Step: 5621, Loss: 0.9159694910049438, Accuracy: 1.0, Computation time: 1.111222743988037\n",
      "Step: 5622, Loss: 0.9200136065483093, Accuracy: 1.0, Computation time: 2.425023317337036\n",
      "Step: 5623, Loss: 0.9189993143081665, Accuracy: 1.0, Computation time: 1.5125913619995117\n",
      "Step: 5624, Loss: 0.9375255107879639, Accuracy: 0.96875, Computation time: 1.193974494934082\n",
      "Step: 5625, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.2540256977081299\n",
      "Step: 5626, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.0997745990753174\n",
      "Step: 5627, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.4368791580200195\n",
      "Step: 5628, Loss: 0.9161151051521301, Accuracy: 1.0, Computation time: 1.0531139373779297\n",
      "Step: 5629, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.9736604690551758\n",
      "Step: 5630, Loss: 0.9375486373901367, Accuracy: 0.96875, Computation time: 1.631988525390625\n",
      "Step: 5631, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 1.36580491065979\n",
      "Step: 5632, Loss: 0.9506642818450928, Accuracy: 0.9375, Computation time: 2.3763647079467773\n",
      "Step: 5633, Loss: 0.9370869994163513, Accuracy: 0.96875, Computation time: 1.6837732791900635\n",
      "Step: 5634, Loss: 0.9376112818717957, Accuracy: 0.96875, Computation time: 1.3371236324310303\n",
      "Step: 5635, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.1365485191345215\n",
      "Step: 5636, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 1.3591957092285156\n",
      "Step: 5637, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.546633005142212\n",
      "Step: 5638, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.4650301933288574\n",
      "Step: 5639, Loss: 0.9375820159912109, Accuracy: 0.96875, Computation time: 1.4963185787200928\n",
      "Step: 5640, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.1372320652008057\n",
      "Step: 5641, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.433379888534546\n",
      "Step: 5642, Loss: 0.9170725345611572, Accuracy: 1.0, Computation time: 2.1699109077453613\n",
      "Step: 5643, Loss: 0.918497622013092, Accuracy: 1.0, Computation time: 3.105844736099243\n",
      "Step: 5644, Loss: 0.9160844087600708, Accuracy: 1.0, Computation time: 1.0323991775512695\n",
      "Step: 5645, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.1369338035583496\n",
      "Step: 5646, Loss: 0.9379132986068726, Accuracy: 0.96875, Computation time: 1.2185380458831787\n",
      "Step: 5647, Loss: 0.91599041223526, Accuracy: 1.0, Computation time: 1.0757455825805664\n",
      "Step: 5648, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 1.5886321067810059\n",
      "Step: 5649, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.8020105361938477\n",
      "Step: 5650, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.3813583850860596\n",
      "Step: 5651, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 1.1084773540496826\n",
      "Step: 5652, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.2062287330627441\n",
      "Step: 5653, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.7202191352844238\n",
      "Step: 5654, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.9536795616149902\n",
      "Step: 5655, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.2935130596160889\n",
      "Step: 5656, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.4539251327514648\n",
      "Step: 5657, Loss: 0.915917694568634, Accuracy: 1.0, Computation time: 1.836538553237915\n",
      "Step: 5658, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 1.6550710201263428\n",
      "Step: 5659, Loss: 0.9353762269020081, Accuracy: 0.96875, Computation time: 1.4132165908813477\n",
      "Step: 5660, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.6586284637451172\n",
      "Step: 5661, Loss: 0.916061282157898, Accuracy: 1.0, Computation time: 2.195979118347168\n",
      "Step: 5662, Loss: 0.9318253993988037, Accuracy: 0.96875, Computation time: 2.313426971435547\n",
      "Step: 5663, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.269573450088501\n",
      "Step: 5664, Loss: 0.9374999403953552, Accuracy: 0.96875, Computation time: 1.1314795017242432\n",
      "Step: 5665, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 0.9991726875305176\n",
      "Step: 5666, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.2291998863220215\n",
      "Step: 5667, Loss: 0.9197812080383301, Accuracy: 1.0, Computation time: 1.7478508949279785\n",
      "Step: 5668, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.5279183387756348\n",
      "Step: 5669, Loss: 0.9159730672836304, Accuracy: 1.0, Computation time: 1.4678168296813965\n",
      "Step: 5670, Loss: 0.9165290594100952, Accuracy: 1.0, Computation time: 1.3026573657989502\n",
      "Step: 5671, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 0.9223728179931641\n",
      "Step: 5672, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.2323274612426758\n",
      "Step: 5673, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 0.9860234260559082\n",
      "Step: 5674, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 1.3473420143127441\n",
      "Step: 5675, Loss: 0.9158729910850525, Accuracy: 1.0, Computation time: 1.5505526065826416\n",
      "Step: 5676, Loss: 0.9254006743431091, Accuracy: 0.96875, Computation time: 1.4318945407867432\n",
      "Step: 5677, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.385845422744751\n",
      "Step: 5678, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.2545502185821533\n",
      "Step: 5679, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.2967529296875\n",
      "Step: 5680, Loss: 0.9162341952323914, Accuracy: 1.0, Computation time: 1.3720693588256836\n",
      "Step: 5681, Loss: 0.9373806715011597, Accuracy: 0.96875, Computation time: 1.2636675834655762\n",
      "Step: 5682, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.331657886505127\n",
      "Step: 5683, Loss: 0.9158975481987, Accuracy: 1.0, Computation time: 1.459822416305542\n",
      "Step: 5684, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.139434576034546\n",
      "Step: 5685, Loss: 0.9371750950813293, Accuracy: 0.96875, Computation time: 1.4591915607452393\n",
      "Step: 5686, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.3294482231140137\n",
      "Step: 5687, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.261946439743042\n",
      "Step: 5688, Loss: 0.9169664978981018, Accuracy: 1.0, Computation time: 1.559633493423462\n",
      "Step: 5689, Loss: 0.9365425109863281, Accuracy: 0.96875, Computation time: 1.5918102264404297\n",
      "Step: 5690, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 1.3351645469665527\n",
      "Step: 5691, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.0530364513397217\n",
      "Step: 5692, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.6233158111572266\n",
      "Step: 5693, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.3099102973937988\n",
      "Step: 5694, Loss: 0.9375220537185669, Accuracy: 0.96875, Computation time: 1.3862273693084717\n",
      "Step: 5695, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.5833320617675781\n",
      "Step: 5696, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.1510767936706543\n",
      "Step: 5697, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.0143351554870605\n",
      "########################\n",
      "Test loss: 1.1265149116516113, Test Accuracy_epoch41: 0.6921837329864502\n",
      "########################\n",
      "Step: 5698, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.1339364051818848\n",
      "Step: 5699, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.463545560836792\n",
      "Step: 5700, Loss: 0.9160010814666748, Accuracy: 1.0, Computation time: 1.6069278717041016\n",
      "Step: 5701, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.431325912475586\n",
      "Step: 5702, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.1911861896514893\n",
      "Step: 5703, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.292060136795044\n",
      "Step: 5704, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.1360070705413818\n",
      "Step: 5705, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.9102234840393066\n",
      "Step: 5706, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.3421475887298584\n",
      "Step: 5707, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.4728856086730957\n",
      "Step: 5708, Loss: 0.9375191330909729, Accuracy: 0.96875, Computation time: 1.4521212577819824\n",
      "Step: 5709, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.0722477436065674\n",
      "Step: 5710, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.3955082893371582\n",
      "Step: 5711, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.216322898864746\n",
      "Step: 5712, Loss: 0.9375090003013611, Accuracy: 0.96875, Computation time: 1.4018652439117432\n",
      "Step: 5713, Loss: 0.9159740805625916, Accuracy: 1.0, Computation time: 1.1541848182678223\n",
      "Step: 5714, Loss: 0.9162545204162598, Accuracy: 1.0, Computation time: 1.3676681518554688\n",
      "Step: 5715, Loss: 0.915909469127655, Accuracy: 1.0, Computation time: 1.4553005695343018\n",
      "Step: 5716, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.2652959823608398\n",
      "Step: 5717, Loss: 0.9158470034599304, Accuracy: 1.0, Computation time: 1.3077287673950195\n",
      "Step: 5718, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.665902853012085\n",
      "Step: 5719, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.274366855621338\n",
      "Step: 5720, Loss: 0.9376949071884155, Accuracy: 0.96875, Computation time: 1.1841719150543213\n",
      "Step: 5721, Loss: 0.9444071650505066, Accuracy: 0.96875, Computation time: 1.7676427364349365\n",
      "Step: 5722, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.4170441627502441\n",
      "Step: 5723, Loss: 0.9375670552253723, Accuracy: 0.96875, Computation time: 1.3297924995422363\n",
      "Step: 5724, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 1.3313093185424805\n",
      "Step: 5725, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.2570273876190186\n",
      "Step: 5726, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.1735434532165527\n",
      "Step: 5727, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.1577622890472412\n",
      "Step: 5728, Loss: 0.9182233214378357, Accuracy: 1.0, Computation time: 2.126413345336914\n",
      "Step: 5729, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.1865382194519043\n",
      "Step: 5730, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.1688251495361328\n",
      "Step: 5731, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.19907546043396\n",
      "Step: 5732, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.5177357196807861\n",
      "Step: 5733, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.1688480377197266\n",
      "Step: 5734, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.350914716720581\n",
      "Step: 5735, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.019977331161499\n",
      "Step: 5736, Loss: 0.9158967137336731, Accuracy: 1.0, Computation time: 1.4813053607940674\n",
      "Step: 5737, Loss: 0.9337826371192932, Accuracy: 0.96875, Computation time: 1.2973756790161133\n",
      "Step: 5738, Loss: 0.9159562587738037, Accuracy: 1.0, Computation time: 1.5322067737579346\n",
      "Step: 5739, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.2799043655395508\n",
      "Step: 5740, Loss: 0.916256844997406, Accuracy: 1.0, Computation time: 1.3135149478912354\n",
      "Step: 5741, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.320998191833496\n",
      "Step: 5742, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.4742207527160645\n",
      "Step: 5743, Loss: 0.9159305691719055, Accuracy: 1.0, Computation time: 1.3431928157806396\n",
      "Step: 5744, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.3035860061645508\n",
      "Step: 5745, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.5698881149291992\n",
      "Step: 5746, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.569284200668335\n",
      "Step: 5747, Loss: 0.9472420811653137, Accuracy: 0.9375, Computation time: 2.154252052307129\n",
      "Step: 5748, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.3533973693847656\n",
      "Step: 5749, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.2049169540405273\n",
      "Step: 5750, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.0703144073486328\n",
      "Step: 5751, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.4542675018310547\n",
      "Step: 5752, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 1.3677923679351807\n",
      "Step: 5753, Loss: 0.9165123701095581, Accuracy: 1.0, Computation time: 1.2653388977050781\n",
      "Step: 5754, Loss: 0.916232168674469, Accuracy: 1.0, Computation time: 1.3321824073791504\n",
      "Step: 5755, Loss: 0.9162768721580505, Accuracy: 1.0, Computation time: 1.1196000576019287\n",
      "Step: 5756, Loss: 0.9159919023513794, Accuracy: 1.0, Computation time: 1.2311491966247559\n",
      "Step: 5757, Loss: 0.9374098777770996, Accuracy: 0.96875, Computation time: 1.4499902725219727\n",
      "Step: 5758, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.1402101516723633\n",
      "Step: 5759, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 1.3572638034820557\n",
      "Step: 5760, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.4053304195404053\n",
      "Step: 5761, Loss: 0.9375667572021484, Accuracy: 0.96875, Computation time: 1.4808952808380127\n",
      "Step: 5762, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 1.5247676372528076\n",
      "Step: 5763, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.4649267196655273\n",
      "Step: 5764, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.5510540008544922\n",
      "Step: 5765, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.2266697883605957\n",
      "Step: 5766, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.2625863552093506\n",
      "Step: 5767, Loss: 0.9159355759620667, Accuracy: 1.0, Computation time: 1.675354242324829\n",
      "Step: 5768, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 2.03140926361084\n",
      "Step: 5769, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 1.552464246749878\n",
      "Step: 5770, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.3348803520202637\n",
      "Step: 5771, Loss: 0.9158341288566589, Accuracy: 1.0, Computation time: 1.2140052318572998\n",
      "Step: 5772, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.3546497821807861\n",
      "Step: 5773, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.2330944538116455\n",
      "Step: 5774, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.4174492359161377\n",
      "Step: 5775, Loss: 0.9171740412712097, Accuracy: 1.0, Computation time: 1.691776990890503\n",
      "Step: 5776, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.389819860458374\n",
      "Step: 5777, Loss: 0.9376176595687866, Accuracy: 0.96875, Computation time: 1.3229565620422363\n",
      "Step: 5778, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 1.3550183773040771\n",
      "Step: 5779, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.2634708881378174\n",
      "Step: 5780, Loss: 0.9240638017654419, Accuracy: 1.0, Computation time: 1.9127871990203857\n",
      "Step: 5781, Loss: 0.9158541560173035, Accuracy: 1.0, Computation time: 1.561647891998291\n",
      "Step: 5782, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.268974781036377\n",
      "Step: 5783, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.5458686351776123\n",
      "Step: 5784, Loss: 0.9375890493392944, Accuracy: 0.96875, Computation time: 1.6081321239471436\n",
      "Step: 5785, Loss: 0.9159016013145447, Accuracy: 1.0, Computation time: 1.2599995136260986\n",
      "Step: 5786, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 1.377573013305664\n",
      "Step: 5787, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 0.9601027965545654\n",
      "Step: 5788, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.392420768737793\n",
      "Step: 5789, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.2699854373931885\n",
      "Step: 5790, Loss: 0.9375824928283691, Accuracy: 0.96875, Computation time: 1.7945640087127686\n",
      "Step: 5791, Loss: 0.9160213470458984, Accuracy: 1.0, Computation time: 1.297553300857544\n",
      "Step: 5792, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 1.16697096824646\n",
      "Step: 5793, Loss: 0.9159027934074402, Accuracy: 1.0, Computation time: 1.3153982162475586\n",
      "Step: 5794, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.4098026752471924\n",
      "Step: 5795, Loss: 0.9164957404136658, Accuracy: 1.0, Computation time: 2.084840774536133\n",
      "Step: 5796, Loss: 0.937527060508728, Accuracy: 0.96875, Computation time: 1.168475866317749\n",
      "Step: 5797, Loss: 0.9376509189605713, Accuracy: 0.96875, Computation time: 1.507314682006836\n",
      "Step: 5798, Loss: 0.9376478791236877, Accuracy: 0.96875, Computation time: 1.816427230834961\n",
      "Step: 5799, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.3450050354003906\n",
      "Step: 5800, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.626302719116211\n",
      "Step: 5801, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 1.1840927600860596\n",
      "Step: 5802, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.3432743549346924\n",
      "Step: 5803, Loss: 0.9184535145759583, Accuracy: 1.0, Computation time: 1.558973789215088\n",
      "Step: 5804, Loss: 0.9332114458084106, Accuracy: 0.96875, Computation time: 1.2975683212280273\n",
      "Step: 5805, Loss: 0.9591190814971924, Accuracy: 0.9375, Computation time: 1.5918679237365723\n",
      "Step: 5806, Loss: 0.9165180921554565, Accuracy: 1.0, Computation time: 1.8293852806091309\n",
      "Step: 5807, Loss: 0.9159267544746399, Accuracy: 1.0, Computation time: 1.3753092288970947\n",
      "Step: 5808, Loss: 0.9159369468688965, Accuracy: 1.0, Computation time: 1.49688720703125\n",
      "Step: 5809, Loss: 0.9159464836120605, Accuracy: 1.0, Computation time: 1.2598075866699219\n",
      "Step: 5810, Loss: 0.9159241914749146, Accuracy: 1.0, Computation time: 1.4286346435546875\n",
      "Step: 5811, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 1.159135341644287\n",
      "Step: 5812, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.2748377323150635\n",
      "Step: 5813, Loss: 0.937483012676239, Accuracy: 0.96875, Computation time: 1.3960466384887695\n",
      "Step: 5814, Loss: 0.9375579953193665, Accuracy: 0.96875, Computation time: 1.2463133335113525\n",
      "Step: 5815, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.5492026805877686\n",
      "Step: 5816, Loss: 0.9366629123687744, Accuracy: 0.96875, Computation time: 2.204376459121704\n",
      "Step: 5817, Loss: 0.9168200492858887, Accuracy: 1.0, Computation time: 2.015669345855713\n",
      "Step: 5818, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.1921796798706055\n",
      "Step: 5819, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.3586151599884033\n",
      "Step: 5820, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.5805532932281494\n",
      "Step: 5821, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.324385166168213\n",
      "Step: 5822, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.346329689025879\n",
      "Step: 5823, Loss: 0.9162939190864563, Accuracy: 1.0, Computation time: 1.59812593460083\n",
      "Step: 5824, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 1.7497689723968506\n",
      "Step: 5825, Loss: 0.9158390760421753, Accuracy: 1.0, Computation time: 1.6884734630584717\n",
      "Step: 5826, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 1.7428796291351318\n",
      "Step: 5827, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.2724709510803223\n",
      "Step: 5828, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.4589805603027344\n",
      "Step: 5829, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.6506145000457764\n",
      "Step: 5830, Loss: 0.9584663510322571, Accuracy: 0.9375, Computation time: 1.3457715511322021\n",
      "Step: 5831, Loss: 0.9161978363990784, Accuracy: 1.0, Computation time: 1.6198058128356934\n",
      "Step: 5832, Loss: 0.9374789595603943, Accuracy: 0.96875, Computation time: 1.6934192180633545\n",
      "Step: 5833, Loss: 0.9159519672393799, Accuracy: 1.0, Computation time: 1.471299409866333\n",
      "Step: 5834, Loss: 0.9376770853996277, Accuracy: 0.96875, Computation time: 1.7182338237762451\n",
      "Step: 5835, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.4190752506256104\n",
      "Step: 5836, Loss: 0.9159473776817322, Accuracy: 1.0, Computation time: 1.2510640621185303\n",
      "########################\n",
      "Test loss: 1.1242575645446777, Test Accuracy_epoch42: 0.6978243589401245\n",
      "########################\n",
      "Step: 5837, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 1.3843181133270264\n",
      "Step: 5838, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.403559923171997\n",
      "Step: 5839, Loss: 0.916096568107605, Accuracy: 1.0, Computation time: 1.3426122665405273\n",
      "Step: 5840, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.3867614269256592\n",
      "Step: 5841, Loss: 0.9159042239189148, Accuracy: 1.0, Computation time: 1.3669869899749756\n",
      "Step: 5842, Loss: 0.9160191416740417, Accuracy: 1.0, Computation time: 1.3451719284057617\n",
      "Step: 5843, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.3061401844024658\n",
      "Step: 5844, Loss: 0.9160664677619934, Accuracy: 1.0, Computation time: 1.376861333847046\n",
      "Step: 5845, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 2.036630868911743\n",
      "Step: 5846, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.8960871696472168\n",
      "Step: 5847, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.6498446464538574\n",
      "Step: 5848, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 2.2565245628356934\n",
      "Step: 5849, Loss: 0.9375263452529907, Accuracy: 0.96875, Computation time: 1.4247019290924072\n",
      "Step: 5850, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.501142978668213\n",
      "Step: 5851, Loss: 0.9166189432144165, Accuracy: 1.0, Computation time: 1.6170086860656738\n",
      "Step: 5852, Loss: 0.9346742033958435, Accuracy: 0.96875, Computation time: 1.7742815017700195\n",
      "Step: 5853, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.5370242595672607\n",
      "Step: 5854, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 1.3373548984527588\n",
      "Step: 5855, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 1.9370722770690918\n",
      "Step: 5856, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.3567781448364258\n",
      "Step: 5857, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.4034759998321533\n",
      "Step: 5858, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.882267951965332\n",
      "Step: 5859, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.3912861347198486\n",
      "Step: 5860, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.2721190452575684\n",
      "Step: 5861, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 1.5121781826019287\n",
      "Step: 5862, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.3588578701019287\n",
      "Step: 5863, Loss: 0.9216993451118469, Accuracy: 1.0, Computation time: 1.6160564422607422\n",
      "Step: 5864, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 2.394378662109375\n",
      "Step: 5865, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 1.5386757850646973\n",
      "Step: 5866, Loss: 0.9159087538719177, Accuracy: 1.0, Computation time: 1.0873446464538574\n",
      "Step: 5867, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.3623712062835693\n",
      "Step: 5868, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.2760050296783447\n",
      "Step: 5869, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 1.2248730659484863\n",
      "Step: 5870, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.3714697360992432\n",
      "Step: 5871, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.2097883224487305\n",
      "Step: 5872, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.2879467010498047\n",
      "Step: 5873, Loss: 0.9375388622283936, Accuracy: 0.96875, Computation time: 1.5573701858520508\n",
      "Step: 5874, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.220832109451294\n",
      "Step: 5875, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.5013401508331299\n",
      "Step: 5876, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.244802474975586\n",
      "Step: 5877, Loss: 0.9179509878158569, Accuracy: 1.0, Computation time: 2.075791597366333\n",
      "Step: 5878, Loss: 0.9159457087516785, Accuracy: 1.0, Computation time: 1.308483362197876\n",
      "Step: 5879, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.324047565460205\n",
      "Step: 5880, Loss: 0.9187167882919312, Accuracy: 1.0, Computation time: 1.1750400066375732\n",
      "Step: 5881, Loss: 0.9159770011901855, Accuracy: 1.0, Computation time: 1.422956943511963\n",
      "Step: 5882, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.2860889434814453\n",
      "Step: 5883, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.3766565322875977\n",
      "Step: 5884, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 1.1820111274719238\n",
      "Step: 5885, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.3289210796356201\n",
      "Step: 5886, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.3028018474578857\n",
      "Step: 5887, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.1600942611694336\n",
      "Step: 5888, Loss: 0.9374160766601562, Accuracy: 0.96875, Computation time: 1.6057264804840088\n",
      "Step: 5889, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.087996006011963\n",
      "Step: 5890, Loss: 0.9167526960372925, Accuracy: 1.0, Computation time: 1.7921998500823975\n",
      "Step: 5891, Loss: 0.9161302447319031, Accuracy: 1.0, Computation time: 1.4273402690887451\n",
      "Step: 5892, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.3978655338287354\n",
      "Step: 5893, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.1422741413116455\n",
      "Step: 5894, Loss: 0.9370377659797668, Accuracy: 0.96875, Computation time: 1.2642102241516113\n",
      "Step: 5895, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.5252478122711182\n",
      "Step: 5896, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.4284017086029053\n",
      "Step: 5897, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.5390655994415283\n",
      "Step: 5898, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.275681734085083\n",
      "Step: 5899, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 1.455317735671997\n",
      "Step: 5900, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.5914745330810547\n",
      "Step: 5901, Loss: 0.9159920811653137, Accuracy: 1.0, Computation time: 1.4515221118927002\n",
      "Step: 5902, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.4200019836425781\n",
      "Step: 5903, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.541215419769287\n",
      "Step: 5904, Loss: 0.9162247776985168, Accuracy: 1.0, Computation time: 1.4228487014770508\n",
      "Step: 5905, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.1335854530334473\n",
      "Step: 5906, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.5169174671173096\n",
      "Step: 5907, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.5678198337554932\n",
      "Step: 5908, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 1.6384894847869873\n",
      "Step: 5909, Loss: 0.9160327911376953, Accuracy: 1.0, Computation time: 1.23248291015625\n",
      "Step: 5910, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.2388889789581299\n",
      "Step: 5911, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.4633290767669678\n",
      "Step: 5912, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.449864149093628\n",
      "Step: 5913, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.3407559394836426\n",
      "Step: 5914, Loss: 0.9377114176750183, Accuracy: 0.96875, Computation time: 1.3619229793548584\n",
      "Step: 5915, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.4348042011260986\n",
      "Step: 5916, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 1.4670922756195068\n",
      "Step: 5917, Loss: 0.9373931288719177, Accuracy: 0.96875, Computation time: 1.6876747608184814\n",
      "Step: 5918, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.7776153087615967\n",
      "Step: 5919, Loss: 0.9159124493598938, Accuracy: 1.0, Computation time: 1.669780969619751\n",
      "Step: 5920, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.1415841579437256\n",
      "Step: 5921, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 1.5696258544921875\n",
      "Step: 5922, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 1.2046267986297607\n",
      "Step: 5923, Loss: 0.9158366322517395, Accuracy: 1.0, Computation time: 1.1901235580444336\n",
      "Step: 5924, Loss: 0.9158350229263306, Accuracy: 1.0, Computation time: 1.1051373481750488\n",
      "Step: 5925, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.3448209762573242\n",
      "Step: 5926, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.2139232158660889\n",
      "Step: 5927, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.8598337173461914\n",
      "Step: 5928, Loss: 0.9158400297164917, Accuracy: 1.0, Computation time: 1.412013053894043\n",
      "Step: 5929, Loss: 0.9165140986442566, Accuracy: 1.0, Computation time: 1.3163199424743652\n",
      "Step: 5930, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.5404176712036133\n",
      "Step: 5931, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 1.7832741737365723\n",
      "Step: 5932, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.7366242408752441\n",
      "Step: 5933, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.762958288192749\n",
      "Step: 5934, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.3644132614135742\n",
      "Step: 5935, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 1.3520948886871338\n",
      "Step: 5936, Loss: 0.9160596132278442, Accuracy: 1.0, Computation time: 1.9255292415618896\n",
      "Step: 5937, Loss: 0.9160239696502686, Accuracy: 1.0, Computation time: 1.6645188331604004\n",
      "Step: 5938, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.3526947498321533\n",
      "Step: 5939, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.3744723796844482\n",
      "Step: 5940, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 1.4495482444763184\n",
      "Step: 5941, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 1.4818637371063232\n",
      "Step: 5942, Loss: 0.9383933544158936, Accuracy: 0.96875, Computation time: 1.2057037353515625\n",
      "Step: 5943, Loss: 0.9160574078559875, Accuracy: 1.0, Computation time: 1.7103931903839111\n",
      "Step: 5944, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.435297966003418\n",
      "Step: 5945, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 1.846172571182251\n",
      "Step: 5946, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.644101858139038\n",
      "Step: 5947, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.6696734428405762\n",
      "Step: 5948, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.131685733795166\n",
      "Step: 5949, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.5592470169067383\n",
      "Step: 5950, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.2851896286010742\n",
      "Step: 5951, Loss: 0.9376478791236877, Accuracy: 0.96875, Computation time: 1.6459369659423828\n",
      "Step: 5952, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.2808327674865723\n",
      "Step: 5953, Loss: 0.9620218873023987, Accuracy: 0.9375, Computation time: 1.5369460582733154\n",
      "Step: 5954, Loss: 0.9375332593917847, Accuracy: 0.96875, Computation time: 1.373824119567871\n",
      "Step: 5955, Loss: 0.9246617555618286, Accuracy: 1.0, Computation time: 1.4312853813171387\n",
      "Step: 5956, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.3739650249481201\n",
      "Step: 5957, Loss: 0.9159338474273682, Accuracy: 1.0, Computation time: 1.3192272186279297\n",
      "Step: 5958, Loss: 0.9376144409179688, Accuracy: 0.96875, Computation time: 1.3917741775512695\n",
      "Step: 5959, Loss: 0.9160344004631042, Accuracy: 1.0, Computation time: 1.257047414779663\n",
      "Step: 5960, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 1.426124095916748\n",
      "Step: 5961, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 0.8225932121276855\n",
      "Step: 5962, Loss: 0.9160173535346985, Accuracy: 1.0, Computation time: 1.1431632041931152\n",
      "Step: 5963, Loss: 0.9376157522201538, Accuracy: 0.96875, Computation time: 1.3075344562530518\n",
      "Step: 5964, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.2469098567962646\n",
      "Step: 5965, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.5647180080413818\n",
      "Step: 5966, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.380223274230957\n",
      "Step: 5967, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.1539936065673828\n",
      "Step: 5968, Loss: 0.9376177787780762, Accuracy: 0.96875, Computation time: 1.6531805992126465\n",
      "Step: 5969, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.7447013854980469\n",
      "Step: 5970, Loss: 0.9171590805053711, Accuracy: 1.0, Computation time: 1.5294315814971924\n",
      "Step: 5971, Loss: 0.9160692095756531, Accuracy: 1.0, Computation time: 1.1185004711151123\n",
      "Step: 5972, Loss: 0.9160658121109009, Accuracy: 1.0, Computation time: 1.1471269130706787\n",
      "Step: 5973, Loss: 0.9376265406608582, Accuracy: 0.96875, Computation time: 2.3902857303619385\n",
      "Step: 5974, Loss: 0.9573285579681396, Accuracy: 0.9375, Computation time: 2.2643909454345703\n",
      "Step: 5975, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 1.2251932621002197\n",
      "########################\n",
      "Test loss: 1.1267484426498413, Test Accuracy_epoch43: 0.6937953233718872\n",
      "########################\n",
      "Step: 5976, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.7019376754760742\n",
      "Step: 5977, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.3122472763061523\n",
      "Step: 5978, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 1.8342187404632568\n",
      "Step: 5979, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.1192083358764648\n",
      "Step: 5980, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.0695383548736572\n",
      "Step: 5981, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 1.3341906070709229\n",
      "Step: 5982, Loss: 0.9159846305847168, Accuracy: 1.0, Computation time: 1.5346424579620361\n",
      "Step: 5983, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.1318750381469727\n",
      "Step: 5984, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.1668446063995361\n",
      "Step: 5985, Loss: 0.9158372282981873, Accuracy: 1.0, Computation time: 1.1977040767669678\n",
      "Step: 5986, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.5475530624389648\n",
      "Step: 5987, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 1.1196370124816895\n",
      "Step: 5988, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 1.2498490810394287\n",
      "Step: 5989, Loss: 0.9369608163833618, Accuracy: 0.96875, Computation time: 1.4098155498504639\n",
      "Step: 5990, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.358548641204834\n",
      "Step: 5991, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 1.3929400444030762\n",
      "Step: 5992, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.0869505405426025\n",
      "Step: 5993, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.495873212814331\n",
      "Step: 5994, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.4292123317718506\n",
      "Step: 5995, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 1.226675033569336\n",
      "Step: 5996, Loss: 0.9158384799957275, Accuracy: 1.0, Computation time: 1.0652849674224854\n",
      "Step: 5997, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 1.0883145332336426\n",
      "Step: 5998, Loss: 0.9158420562744141, Accuracy: 1.0, Computation time: 1.0977137088775635\n",
      "Step: 5999, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 1.1064891815185547\n",
      "Step: 6000, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.4578804969787598\n",
      "Step: 6001, Loss: 0.9359411597251892, Accuracy: 0.96875, Computation time: 1.196232795715332\n",
      "Step: 6002, Loss: 0.9376139640808105, Accuracy: 0.96875, Computation time: 1.1885826587677002\n",
      "Step: 6003, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 1.7519502639770508\n",
      "Step: 6004, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.2868762016296387\n",
      "Step: 6005, Loss: 0.9383368492126465, Accuracy: 0.96875, Computation time: 1.270641565322876\n",
      "Step: 6006, Loss: 0.9375482797622681, Accuracy: 0.96875, Computation time: 1.1858203411102295\n",
      "Step: 6007, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.3391249179840088\n",
      "Step: 6008, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.0093462467193604\n",
      "Step: 6009, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 1.2559378147125244\n",
      "Step: 6010, Loss: 0.9159446358680725, Accuracy: 1.0, Computation time: 1.1247365474700928\n",
      "Step: 6011, Loss: 0.9330462217330933, Accuracy: 0.96875, Computation time: 2.0349552631378174\n",
      "Step: 6012, Loss: 0.9385047554969788, Accuracy: 0.96875, Computation time: 1.626934289932251\n",
      "Step: 6013, Loss: 0.9377351403236389, Accuracy: 0.96875, Computation time: 1.0937747955322266\n",
      "Step: 6014, Loss: 0.9159755706787109, Accuracy: 1.0, Computation time: 1.3282711505889893\n",
      "Step: 6015, Loss: 0.9159574508666992, Accuracy: 1.0, Computation time: 1.0913546085357666\n",
      "Step: 6016, Loss: 0.915985107421875, Accuracy: 1.0, Computation time: 1.490934133529663\n",
      "Step: 6017, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 1.1504297256469727\n",
      "Step: 6018, Loss: 0.9159483909606934, Accuracy: 1.0, Computation time: 1.3782315254211426\n",
      "Step: 6019, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 2.1892638206481934\n",
      "Step: 6020, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.4206922054290771\n",
      "Step: 6021, Loss: 0.9376108646392822, Accuracy: 0.96875, Computation time: 1.3143255710601807\n",
      "Step: 6022, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 2.1597740650177\n",
      "Step: 6023, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.4044489860534668\n",
      "Step: 6024, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.252185583114624\n",
      "Step: 6025, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 1.365549087524414\n",
      "Step: 6026, Loss: 0.9159700870513916, Accuracy: 1.0, Computation time: 1.5635521411895752\n",
      "Step: 6027, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.5551612377166748\n",
      "Step: 6028, Loss: 0.9355425834655762, Accuracy: 0.96875, Computation time: 2.1876227855682373\n",
      "Step: 6029, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.5025355815887451\n",
      "Step: 6030, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.3937418460845947\n",
      "Step: 6031, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 1.327833890914917\n",
      "Step: 6032, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.4366304874420166\n",
      "Step: 6033, Loss: 0.9161260724067688, Accuracy: 1.0, Computation time: 1.2187721729278564\n",
      "Step: 6034, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.6067845821380615\n",
      "Step: 6035, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.3046355247497559\n",
      "Step: 6036, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.4603822231292725\n",
      "Step: 6037, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.3958594799041748\n",
      "Step: 6038, Loss: 0.9376049041748047, Accuracy: 0.96875, Computation time: 1.6821460723876953\n",
      "Step: 6039, Loss: 0.940302848815918, Accuracy: 0.96875, Computation time: 2.764256477355957\n",
      "Step: 6040, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 2.083750009536743\n",
      "Step: 6041, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.1575708389282227\n",
      "Step: 6042, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 1.3775148391723633\n",
      "Step: 6043, Loss: 0.9159448146820068, Accuracy: 1.0, Computation time: 1.4374239444732666\n",
      "Step: 6044, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 2.1898033618927\n",
      "Step: 6045, Loss: 0.9158975481987, Accuracy: 1.0, Computation time: 1.4014043807983398\n",
      "Step: 6046, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.3981072902679443\n",
      "Step: 6047, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.137361764907837\n",
      "Step: 6048, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.5188157558441162\n",
      "Step: 6049, Loss: 0.9376408457756042, Accuracy: 0.96875, Computation time: 1.6846518516540527\n",
      "Step: 6050, Loss: 0.9158381819725037, Accuracy: 1.0, Computation time: 1.0662977695465088\n",
      "Step: 6051, Loss: 0.9371508955955505, Accuracy: 0.96875, Computation time: 1.2074308395385742\n",
      "Step: 6052, Loss: 0.9160034656524658, Accuracy: 1.0, Computation time: 1.4486196041107178\n",
      "Step: 6053, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.1527113914489746\n",
      "Step: 6054, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 2.3139376640319824\n",
      "Step: 6055, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.3900110721588135\n",
      "Step: 6056, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.4378294944763184\n",
      "Step: 6057, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.3667776584625244\n",
      "Step: 6058, Loss: 0.91603684425354, Accuracy: 1.0, Computation time: 1.3438904285430908\n",
      "Step: 6059, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 1.4671554565429688\n",
      "Step: 6060, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.78373384475708\n",
      "Step: 6061, Loss: 0.9158428907394409, Accuracy: 1.0, Computation time: 1.2952516078948975\n",
      "Step: 6062, Loss: 0.9163433909416199, Accuracy: 1.0, Computation time: 1.5147521495819092\n",
      "Step: 6063, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.2840509414672852\n",
      "Step: 6064, Loss: 0.9212923049926758, Accuracy: 1.0, Computation time: 1.3658201694488525\n",
      "Step: 6065, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 1.687040090560913\n",
      "Step: 6066, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.2669498920440674\n",
      "Step: 6067, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 1.5128483772277832\n",
      "Step: 6068, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.4730489253997803\n",
      "Step: 6069, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.5195481777191162\n",
      "Step: 6070, Loss: 0.9375133514404297, Accuracy: 0.96875, Computation time: 1.4582059383392334\n",
      "Step: 6071, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.6218454837799072\n",
      "Step: 6072, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.699742078781128\n",
      "Step: 6073, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 1.632852554321289\n",
      "Step: 6074, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.5675139427185059\n",
      "Step: 6075, Loss: 0.9171115756034851, Accuracy: 1.0, Computation time: 1.5808916091918945\n",
      "Step: 6076, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.4802641868591309\n",
      "Step: 6077, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.5202279090881348\n",
      "Step: 6078, Loss: 0.9372307658195496, Accuracy: 0.96875, Computation time: 2.0489721298217773\n",
      "Step: 6079, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.6713552474975586\n",
      "Step: 6080, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.2177255153656006\n",
      "Step: 6081, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 1.021566390991211\n",
      "Step: 6082, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.5937879085540771\n",
      "Step: 6083, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 1.5252478122711182\n",
      "Step: 6084, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.4476935863494873\n",
      "Step: 6085, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.4302737712860107\n",
      "Step: 6086, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.4612514972686768\n",
      "Step: 6087, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.455209732055664\n",
      "Step: 6088, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.5286178588867188\n",
      "Step: 6089, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.4491522312164307\n",
      "Step: 6090, Loss: 0.9328531622886658, Accuracy: 0.96875, Computation time: 1.5792741775512695\n",
      "Step: 6091, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.9912185668945312\n",
      "Step: 6092, Loss: 0.9158980250358582, Accuracy: 1.0, Computation time: 1.8153903484344482\n",
      "Step: 6093, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.5521581172943115\n",
      "Step: 6094, Loss: 0.9375877380371094, Accuracy: 0.96875, Computation time: 1.2008099555969238\n",
      "Step: 6095, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.2383768558502197\n",
      "Step: 6096, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 1.5286259651184082\n",
      "Step: 6097, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.0598595142364502\n",
      "Step: 6098, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.1233136653900146\n",
      "Step: 6099, Loss: 0.9158998727798462, Accuracy: 1.0, Computation time: 1.2598810195922852\n",
      "Step: 6100, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.441561222076416\n",
      "Step: 6101, Loss: 0.9375758171081543, Accuracy: 0.96875, Computation time: 1.352177381515503\n",
      "Step: 6102, Loss: 0.9168684482574463, Accuracy: 1.0, Computation time: 1.46974778175354\n",
      "Step: 6103, Loss: 0.9375081658363342, Accuracy: 0.96875, Computation time: 1.1915876865386963\n",
      "Step: 6104, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.1073651313781738\n",
      "Step: 6105, Loss: 0.9160187840461731, Accuracy: 1.0, Computation time: 1.2162134647369385\n",
      "Step: 6106, Loss: 0.9375762343406677, Accuracy: 0.96875, Computation time: 1.0744221210479736\n",
      "Step: 6107, Loss: 0.9173918962478638, Accuracy: 1.0, Computation time: 1.5257835388183594\n",
      "Step: 6108, Loss: 0.9159241318702698, Accuracy: 1.0, Computation time: 1.2551672458648682\n",
      "Step: 6109, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 1.4025483131408691\n",
      "Step: 6110, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.3272321224212646\n",
      "Step: 6111, Loss: 0.9374107718467712, Accuracy: 0.96875, Computation time: 1.4604570865631104\n",
      "Step: 6112, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 1.1851434707641602\n",
      "Step: 6113, Loss: 0.9161449670791626, Accuracy: 1.0, Computation time: 1.1893281936645508\n",
      "Step: 6114, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.0754714012145996\n",
      "########################\n",
      "Test loss: 1.126305341720581, Test Accuracy_epoch44: 0.6946011185646057\n",
      "########################\n",
      "Step: 6115, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.5984740257263184\n",
      "Step: 6116, Loss: 0.9160302877426147, Accuracy: 1.0, Computation time: 1.113048791885376\n",
      "Step: 6117, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.3648552894592285\n",
      "Step: 6118, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.2669901847839355\n",
      "Step: 6119, Loss: 0.9375079870223999, Accuracy: 0.96875, Computation time: 1.1639091968536377\n",
      "Step: 6120, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.4435279369354248\n",
      "Step: 6121, Loss: 0.9375390410423279, Accuracy: 0.96875, Computation time: 1.3348476886749268\n",
      "Step: 6122, Loss: 0.9375048875808716, Accuracy: 0.96875, Computation time: 1.5534625053405762\n",
      "Step: 6123, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 0.9844176769256592\n",
      "Step: 6124, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 1.6979291439056396\n",
      "Step: 6125, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.5333237648010254\n",
      "Step: 6126, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.3173041343688965\n",
      "Step: 6127, Loss: 0.9368805289268494, Accuracy: 0.96875, Computation time: 2.277766466140747\n",
      "Step: 6128, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.0952060222625732\n",
      "Step: 6129, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.097715139389038\n",
      "Step: 6130, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 1.3109290599822998\n",
      "Step: 6131, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.7892863750457764\n",
      "Step: 6132, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.4165294170379639\n",
      "Step: 6133, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.4671783447265625\n",
      "Step: 6134, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.366901159286499\n",
      "Step: 6135, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.2963345050811768\n",
      "Step: 6136, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.3232860565185547\n",
      "Step: 6137, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.472141981124878\n",
      "Step: 6138, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 1.4048552513122559\n",
      "Step: 6139, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.110107660293579\n",
      "Step: 6140, Loss: 0.9158357977867126, Accuracy: 1.0, Computation time: 1.3558626174926758\n",
      "Step: 6141, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.729968786239624\n",
      "Step: 6142, Loss: 0.9158369898796082, Accuracy: 1.0, Computation time: 1.234705924987793\n",
      "Step: 6143, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 1.1309328079223633\n",
      "Step: 6144, Loss: 0.9268685579299927, Accuracy: 0.96875, Computation time: 1.759340524673462\n",
      "Step: 6145, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 1.875452995300293\n",
      "Step: 6146, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 1.19581937789917\n",
      "Step: 6147, Loss: 0.9376257061958313, Accuracy: 0.96875, Computation time: 1.3305199146270752\n",
      "Step: 6148, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 1.145519495010376\n",
      "Step: 6149, Loss: 0.9341166615486145, Accuracy: 0.96875, Computation time: 1.2082011699676514\n",
      "Step: 6150, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.0473206043243408\n",
      "Step: 6151, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.2339847087860107\n",
      "Step: 6152, Loss: 0.9361127614974976, Accuracy: 0.96875, Computation time: 1.6362450122833252\n",
      "Step: 6153, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 0.9924771785736084\n",
      "Step: 6154, Loss: 0.9377259016036987, Accuracy: 0.96875, Computation time: 1.105820894241333\n",
      "Step: 6155, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 0.8518688678741455\n",
      "Step: 6156, Loss: 0.9373956322669983, Accuracy: 0.96875, Computation time: 1.1573257446289062\n",
      "Step: 6157, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.0195887088775635\n",
      "Step: 6158, Loss: 0.9159428477287292, Accuracy: 1.0, Computation time: 1.343088150024414\n",
      "Step: 6159, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.0601320266723633\n",
      "Step: 6160, Loss: 0.9574076533317566, Accuracy: 0.9375, Computation time: 1.7907376289367676\n",
      "Step: 6161, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.617110252380371\n",
      "Step: 6162, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 0.9310240745544434\n",
      "Step: 6163, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 0.877622127532959\n",
      "Step: 6164, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.0202298164367676\n",
      "Step: 6165, Loss: 0.9230388402938843, Accuracy: 1.0, Computation time: 1.2009482383728027\n",
      "Step: 6166, Loss: 0.915939211845398, Accuracy: 1.0, Computation time: 1.2913696765899658\n",
      "Step: 6167, Loss: 0.9159776568412781, Accuracy: 1.0, Computation time: 1.090691328048706\n",
      "Step: 6168, Loss: 0.9173537492752075, Accuracy: 1.0, Computation time: 1.2579185962677002\n",
      "Step: 6169, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 0.9223577976226807\n",
      "Step: 6170, Loss: 0.915944516658783, Accuracy: 1.0, Computation time: 1.0988411903381348\n",
      "Step: 6171, Loss: 0.9159889817237854, Accuracy: 1.0, Computation time: 0.9922101497650146\n",
      "Step: 6172, Loss: 0.9159515500068665, Accuracy: 1.0, Computation time: 1.5742089748382568\n",
      "Step: 6173, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.2749919891357422\n",
      "Step: 6174, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.1530838012695312\n",
      "Step: 6175, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.4301729202270508\n",
      "Step: 6176, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.5508489608764648\n",
      "Step: 6177, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.2189221382141113\n",
      "Step: 6178, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.2420032024383545\n",
      "Step: 6179, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.1579368114471436\n",
      "Step: 6180, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.0915451049804688\n",
      "Step: 6181, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.0117087364196777\n",
      "Step: 6182, Loss: 0.9159094095230103, Accuracy: 1.0, Computation time: 1.457031488418579\n",
      "Step: 6183, Loss: 0.9160166382789612, Accuracy: 1.0, Computation time: 1.5577991008758545\n",
      "Step: 6184, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 0.968085765838623\n",
      "Step: 6185, Loss: 0.9375617504119873, Accuracy: 0.96875, Computation time: 1.098621129989624\n",
      "Step: 6186, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.4539668560028076\n",
      "Step: 6187, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.1278927326202393\n",
      "Step: 6188, Loss: 0.9158397316932678, Accuracy: 1.0, Computation time: 0.9917049407958984\n",
      "Step: 6189, Loss: 0.9376307129859924, Accuracy: 0.96875, Computation time: 1.7304670810699463\n",
      "Step: 6190, Loss: 0.9159900546073914, Accuracy: 1.0, Computation time: 1.0619146823883057\n",
      "Step: 6191, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.4703779220581055\n",
      "Step: 6192, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 0.9693460464477539\n",
      "Step: 6193, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.2734105587005615\n",
      "Step: 6194, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.3056366443634033\n",
      "Step: 6195, Loss: 0.9374790787696838, Accuracy: 0.96875, Computation time: 1.186643123626709\n",
      "Step: 6196, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.480595588684082\n",
      "Step: 6197, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.3031513690948486\n",
      "Step: 6198, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.9102535247802734\n",
      "Step: 6199, Loss: 0.9407515525817871, Accuracy: 0.96875, Computation time: 1.5604233741760254\n",
      "Step: 6200, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 0.8431580066680908\n",
      "Step: 6201, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.6102213859558105\n",
      "Step: 6202, Loss: 0.9160706400871277, Accuracy: 1.0, Computation time: 1.2508318424224854\n",
      "Step: 6203, Loss: 0.9159232378005981, Accuracy: 1.0, Computation time: 0.9760005474090576\n",
      "Step: 6204, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 0.9862439632415771\n",
      "Step: 6205, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.1138205528259277\n",
      "Step: 6206, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.397552490234375\n",
      "Step: 6207, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.406968116760254\n",
      "Step: 6208, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 0.9768178462982178\n",
      "Step: 6209, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.7332499027252197\n",
      "Step: 6210, Loss: 0.9373548030853271, Accuracy: 0.96875, Computation time: 1.8286998271942139\n",
      "Step: 6211, Loss: 0.9160080552101135, Accuracy: 1.0, Computation time: 1.7602083683013916\n",
      "Step: 6212, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.011092185974121\n",
      "Step: 6213, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.1832656860351562\n",
      "Step: 6214, Loss: 0.9373785257339478, Accuracy: 0.96875, Computation time: 1.2438278198242188\n",
      "Step: 6215, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.0001976490020752\n",
      "Step: 6216, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.2965106964111328\n",
      "Step: 6217, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 1.2559902667999268\n",
      "Step: 6218, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.6512455940246582\n",
      "Step: 6219, Loss: 0.9556761980056763, Accuracy: 0.9375, Computation time: 1.4095110893249512\n",
      "Step: 6220, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 1.5107288360595703\n",
      "Step: 6221, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.5911283493041992\n",
      "Step: 6222, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.224069595336914\n",
      "Step: 6223, Loss: 0.9158763289451599, Accuracy: 1.0, Computation time: 1.1437253952026367\n",
      "Step: 6224, Loss: 0.9212262630462646, Accuracy: 1.0, Computation time: 2.9333434104919434\n",
      "Step: 6225, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 1.0100147724151611\n",
      "Step: 6226, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.23384428024292\n",
      "Step: 6227, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 1.1466419696807861\n",
      "Step: 6228, Loss: 0.9160149693489075, Accuracy: 1.0, Computation time: 1.184119701385498\n",
      "Step: 6229, Loss: 0.9160436391830444, Accuracy: 1.0, Computation time: 1.6432499885559082\n",
      "Step: 6230, Loss: 0.915930449962616, Accuracy: 1.0, Computation time: 1.1892845630645752\n",
      "Step: 6231, Loss: 0.9159391522407532, Accuracy: 1.0, Computation time: 1.0673794746398926\n",
      "Step: 6232, Loss: 0.9159005284309387, Accuracy: 1.0, Computation time: 1.7098708152770996\n",
      "Step: 6233, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.231614112854004\n",
      "Step: 6234, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.379629373550415\n",
      "Step: 6235, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 1.389441967010498\n",
      "Step: 6236, Loss: 0.9375778436660767, Accuracy: 0.96875, Computation time: 1.368316888809204\n",
      "Step: 6237, Loss: 0.9242438673973083, Accuracy: 1.0, Computation time: 2.9115331172943115\n",
      "Step: 6238, Loss: 0.9159699082374573, Accuracy: 1.0, Computation time: 1.1384100914001465\n",
      "Step: 6239, Loss: 0.9161188006401062, Accuracy: 1.0, Computation time: 1.145880937576294\n",
      "Step: 6240, Loss: 0.9161498546600342, Accuracy: 1.0, Computation time: 1.254692792892456\n",
      "Step: 6241, Loss: 0.9394519329071045, Accuracy: 0.96875, Computation time: 1.691457986831665\n",
      "Step: 6242, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.089740514755249\n",
      "Step: 6243, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.1331639289855957\n",
      "Step: 6244, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.1400938034057617\n",
      "Step: 6245, Loss: 0.9186815023422241, Accuracy: 1.0, Computation time: 2.016662120819092\n",
      "Step: 6246, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 1.1504158973693848\n",
      "Step: 6247, Loss: 0.9161002039909363, Accuracy: 1.0, Computation time: 1.10563325881958\n",
      "Step: 6248, Loss: 0.9160704016685486, Accuracy: 1.0, Computation time: 1.1613743305206299\n",
      "Step: 6249, Loss: 0.9178435802459717, Accuracy: 1.0, Computation time: 1.543750286102295\n",
      "Step: 6250, Loss: 0.937309741973877, Accuracy: 0.96875, Computation time: 2.2191483974456787\n",
      "Step: 6251, Loss: 0.9282733201980591, Accuracy: 0.96875, Computation time: 1.8043725490570068\n",
      "Step: 6252, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.2703924179077148\n",
      "Step: 6253, Loss: 0.9159857034683228, Accuracy: 1.0, Computation time: 1.4508388042449951\n",
      "########################\n",
      "Test loss: 1.1258344650268555, Test Accuracy_epoch45: 0.6946011185646057\n",
      "########################\n",
      "Step: 6254, Loss: 0.93779057264328, Accuracy: 0.96875, Computation time: 1.3876817226409912\n",
      "Step: 6255, Loss: 0.938064455986023, Accuracy: 0.96875, Computation time: 1.4445490837097168\n",
      "Step: 6256, Loss: 0.9163472652435303, Accuracy: 1.0, Computation time: 1.0169837474822998\n",
      "Step: 6257, Loss: 0.9161539077758789, Accuracy: 1.0, Computation time: 1.4169881343841553\n",
      "Step: 6258, Loss: 0.9162362217903137, Accuracy: 1.0, Computation time: 1.0110948085784912\n",
      "Step: 6259, Loss: 0.9160231351852417, Accuracy: 1.0, Computation time: 1.176386833190918\n",
      "Step: 6260, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.4702215194702148\n",
      "Step: 6261, Loss: 0.9171425104141235, Accuracy: 1.0, Computation time: 2.6598894596099854\n",
      "Step: 6262, Loss: 0.916145920753479, Accuracy: 1.0, Computation time: 1.1346020698547363\n",
      "Step: 6263, Loss: 0.9160497784614563, Accuracy: 1.0, Computation time: 1.195011854171753\n",
      "Step: 6264, Loss: 0.9324519038200378, Accuracy: 0.96875, Computation time: 1.7250001430511475\n",
      "Step: 6265, Loss: 0.9159598350524902, Accuracy: 1.0, Computation time: 1.0826454162597656\n",
      "Step: 6266, Loss: 0.9159197211265564, Accuracy: 1.0, Computation time: 1.2403078079223633\n",
      "Step: 6267, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 1.016859531402588\n",
      "Step: 6268, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.0359687805175781\n",
      "Step: 6269, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.3080813884735107\n",
      "Step: 6270, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.0840814113616943\n",
      "Step: 6271, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 1.7096836566925049\n",
      "Step: 6272, Loss: 0.9375759959220886, Accuracy: 0.96875, Computation time: 1.2519972324371338\n",
      "Step: 6273, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 1.037846326828003\n",
      "Step: 6274, Loss: 0.9158900380134583, Accuracy: 1.0, Computation time: 1.247239112854004\n",
      "Step: 6275, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 0.9903275966644287\n",
      "Step: 6276, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.0309619903564453\n",
      "Step: 6277, Loss: 0.937416136264801, Accuracy: 0.96875, Computation time: 1.309084415435791\n",
      "Step: 6278, Loss: 0.947501540184021, Accuracy: 0.9375, Computation time: 1.4170193672180176\n",
      "Step: 6279, Loss: 0.9159058332443237, Accuracy: 1.0, Computation time: 1.122408390045166\n",
      "Step: 6280, Loss: 0.9159459471702576, Accuracy: 1.0, Computation time: 1.0348150730133057\n",
      "Step: 6281, Loss: 0.9588622450828552, Accuracy: 0.9375, Computation time: 1.4512267112731934\n",
      "Step: 6282, Loss: 0.9159599542617798, Accuracy: 1.0, Computation time: 1.3462541103363037\n",
      "Step: 6283, Loss: 0.9159790277481079, Accuracy: 1.0, Computation time: 0.9063742160797119\n",
      "Step: 6284, Loss: 0.9160512089729309, Accuracy: 1.0, Computation time: 1.111053705215454\n",
      "Step: 6285, Loss: 0.9159722328186035, Accuracy: 1.0, Computation time: 1.3193354606628418\n",
      "Step: 6286, Loss: 0.9375891089439392, Accuracy: 0.96875, Computation time: 1.0449919700622559\n",
      "Step: 6287, Loss: 0.9165762662887573, Accuracy: 1.0, Computation time: 2.033160448074341\n",
      "Step: 6288, Loss: 0.9159132242202759, Accuracy: 1.0, Computation time: 0.8864750862121582\n",
      "Step: 6289, Loss: 0.93691486120224, Accuracy: 0.96875, Computation time: 1.323777198791504\n",
      "Step: 6290, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 0.9982337951660156\n",
      "Step: 6291, Loss: 0.9159824848175049, Accuracy: 1.0, Computation time: 1.054405927658081\n",
      "Step: 6292, Loss: 0.9160305261611938, Accuracy: 1.0, Computation time: 0.9836575984954834\n",
      "Step: 6293, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 1.1113975048065186\n",
      "Step: 6294, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.0202476978302002\n",
      "Step: 6295, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.1547467708587646\n",
      "Step: 6296, Loss: 0.9375297427177429, Accuracy: 0.96875, Computation time: 0.9279956817626953\n",
      "Step: 6297, Loss: 0.9374845027923584, Accuracy: 0.96875, Computation time: 1.422504186630249\n",
      "Step: 6298, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.9817569255828857\n",
      "Step: 6299, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 0.8462977409362793\n",
      "Step: 6300, Loss: 0.917724072933197, Accuracy: 1.0, Computation time: 1.8002333641052246\n",
      "Step: 6301, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 0.9316825866699219\n",
      "Step: 6302, Loss: 0.9159228205680847, Accuracy: 1.0, Computation time: 0.9342997074127197\n",
      "Step: 6303, Loss: 0.9160241484642029, Accuracy: 1.0, Computation time: 1.1646161079406738\n",
      "Step: 6304, Loss: 0.9346711039543152, Accuracy: 0.96875, Computation time: 1.4372901916503906\n",
      "Step: 6305, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 1.2082719802856445\n",
      "Step: 6306, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.3160219192504883\n",
      "Step: 6307, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.3626251220703125\n",
      "Step: 6308, Loss: 0.9159075617790222, Accuracy: 1.0, Computation time: 1.1721408367156982\n",
      "Step: 6309, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 1.172013521194458\n",
      "Step: 6310, Loss: 0.9565849304199219, Accuracy: 0.9375, Computation time: 1.8163955211639404\n",
      "Step: 6311, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.2647805213928223\n",
      "Step: 6312, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 1.1483142375946045\n",
      "Step: 6313, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 1.0236365795135498\n",
      "Step: 6314, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 1.2647581100463867\n",
      "Step: 6315, Loss: 0.9376307725906372, Accuracy: 0.96875, Computation time: 1.2042605876922607\n",
      "Step: 6316, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 1.1952884197235107\n",
      "Step: 6317, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 1.0988249778747559\n",
      "Step: 6318, Loss: 0.915949285030365, Accuracy: 1.0, Computation time: 1.332458257675171\n",
      "Step: 6319, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 1.4280989170074463\n",
      "Step: 6320, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.870485782623291\n",
      "Step: 6321, Loss: 0.9238299131393433, Accuracy: 1.0, Computation time: 1.3969402313232422\n",
      "Step: 6322, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.3649730682373047\n",
      "Step: 6323, Loss: 0.9373444318771362, Accuracy: 0.96875, Computation time: 1.3357017040252686\n",
      "Step: 6324, Loss: 0.9375871419906616, Accuracy: 0.96875, Computation time: 1.7296369075775146\n",
      "Step: 6325, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 0.96051025390625\n",
      "Step: 6326, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 0.9963939189910889\n",
      "Step: 6327, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.6168291568756104\n",
      "Step: 6328, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.2277638912200928\n",
      "Step: 6329, Loss: 0.9379804134368896, Accuracy: 0.96875, Computation time: 1.0324976444244385\n",
      "Step: 6330, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.1210198402404785\n",
      "Step: 6331, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.2119407653808594\n",
      "Step: 6332, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.040215015411377\n",
      "Step: 6333, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.0229582786560059\n",
      "Step: 6334, Loss: 0.9174103736877441, Accuracy: 1.0, Computation time: 1.4812748432159424\n",
      "Step: 6335, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.6408238410949707\n",
      "Step: 6336, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.1922740936279297\n",
      "Step: 6337, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 1.2182879447937012\n",
      "Step: 6338, Loss: 0.9159272313117981, Accuracy: 1.0, Computation time: 1.3650257587432861\n",
      "Step: 6339, Loss: 0.9161052107810974, Accuracy: 1.0, Computation time: 1.2805633544921875\n",
      "Step: 6340, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 0.9907166957855225\n",
      "Step: 6341, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 0.9331283569335938\n",
      "Step: 6342, Loss: 0.9175980687141418, Accuracy: 1.0, Computation time: 1.1933174133300781\n",
      "Step: 6343, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.1428825855255127\n",
      "Step: 6344, Loss: 0.9375019073486328, Accuracy: 0.96875, Computation time: 1.5322675704956055\n",
      "Step: 6345, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 1.1647019386291504\n",
      "Step: 6346, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 1.256446361541748\n",
      "Step: 6347, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.1709098815917969\n",
      "Step: 6348, Loss: 0.9374386072158813, Accuracy: 0.96875, Computation time: 1.063180685043335\n",
      "Step: 6349, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.1568644046783447\n",
      "Step: 6350, Loss: 0.9402098655700684, Accuracy: 0.96875, Computation time: 1.7312819957733154\n",
      "Step: 6351, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.281447172164917\n",
      "Step: 6352, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 0.8955175876617432\n",
      "Step: 6353, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 0.852567195892334\n",
      "Step: 6354, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 1.0256834030151367\n",
      "Step: 6355, Loss: 0.915938138961792, Accuracy: 1.0, Computation time: 1.2620551586151123\n",
      "Step: 6356, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.2006969451904297\n",
      "Step: 6357, Loss: 0.9178804159164429, Accuracy: 1.0, Computation time: 1.863128900527954\n",
      "Step: 6358, Loss: 0.9376044869422913, Accuracy: 0.96875, Computation time: 0.8763113021850586\n",
      "Step: 6359, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.3618638515472412\n",
      "Step: 6360, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.0104200839996338\n",
      "Step: 6361, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.1083571910858154\n",
      "Step: 6362, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.2561225891113281\n",
      "Step: 6363, Loss: 0.9158852696418762, Accuracy: 1.0, Computation time: 1.6593408584594727\n",
      "Step: 6364, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.3383522033691406\n",
      "Step: 6365, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.9070229530334473\n",
      "Step: 6366, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.9831020832061768\n",
      "Step: 6367, Loss: 0.9371857643127441, Accuracy: 0.96875, Computation time: 1.0311129093170166\n",
      "Step: 6368, Loss: 0.9375083446502686, Accuracy: 0.96875, Computation time: 1.161562442779541\n",
      "Step: 6369, Loss: 0.9161713123321533, Accuracy: 1.0, Computation time: 1.3041138648986816\n",
      "Step: 6370, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.2567415237426758\n",
      "Step: 6371, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.4227066040039062\n",
      "Step: 6372, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.3620190620422363\n",
      "Step: 6373, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.1199212074279785\n",
      "Step: 6374, Loss: 0.9158470034599304, Accuracy: 1.0, Computation time: 1.4387900829315186\n",
      "Step: 6375, Loss: 0.9375663995742798, Accuracy: 0.96875, Computation time: 1.2297065258026123\n",
      "Step: 6376, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.5921661853790283\n",
      "Step: 6377, Loss: 0.9375351071357727, Accuracy: 0.96875, Computation time: 1.4483768939971924\n",
      "Step: 6378, Loss: 0.915833592414856, Accuracy: 1.0, Computation time: 1.4068078994750977\n",
      "Step: 6379, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 1.126993179321289\n",
      "Step: 6380, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.2287776470184326\n",
      "Step: 6381, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.1350922584533691\n",
      "Step: 6382, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.7787680625915527\n",
      "Step: 6383, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 1.3950345516204834\n",
      "Step: 6384, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.6941165924072266\n",
      "Step: 6385, Loss: 0.9388327598571777, Accuracy: 0.96875, Computation time: 1.7189137935638428\n",
      "Step: 6386, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.5883491039276123\n",
      "Step: 6387, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.5619986057281494\n",
      "Step: 6388, Loss: 0.9161309599876404, Accuracy: 1.0, Computation time: 1.4675860404968262\n",
      "Step: 6389, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.2040956020355225\n",
      "Step: 6390, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.3948280811309814\n",
      "Step: 6391, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 1.2697193622589111\n",
      "Step: 6392, Loss: 0.9375, Accuracy: 0.96875, Computation time: 1.3686487674713135\n",
      "########################\n",
      "Test loss: 1.1262913942337036, Test Accuracy_epoch46: 0.6921837329864502\n",
      "########################\n",
      "Step: 6393, Loss: 0.9375918507575989, Accuracy: 0.96875, Computation time: 1.3057284355163574\n",
      "Step: 6394, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.359018325805664\n",
      "Step: 6395, Loss: 0.9159621596336365, Accuracy: 1.0, Computation time: 1.332932949066162\n",
      "Step: 6396, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.3639557361602783\n",
      "Step: 6397, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.1516170501708984\n",
      "Step: 6398, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 1.512263536453247\n",
      "Step: 6399, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.7571957111358643\n",
      "Step: 6400, Loss: 0.9357913732528687, Accuracy: 0.96875, Computation time: 1.404130220413208\n",
      "Step: 6401, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.3551738262176514\n",
      "Step: 6402, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.3304650783538818\n",
      "Step: 6403, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.4195995330810547\n",
      "Step: 6404, Loss: 0.9375597834587097, Accuracy: 0.96875, Computation time: 1.184431552886963\n",
      "Step: 6405, Loss: 0.930754542350769, Accuracy: 0.96875, Computation time: 1.3477487564086914\n",
      "Step: 6406, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.2664880752563477\n",
      "Step: 6407, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.4967372417449951\n",
      "Step: 6408, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.1301288604736328\n",
      "Step: 6409, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.1201376914978027\n",
      "Step: 6410, Loss: 0.9349673390388489, Accuracy: 0.96875, Computation time: 1.4140651226043701\n",
      "Step: 6411, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.1223416328430176\n",
      "Step: 6412, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.2369906902313232\n",
      "Step: 6413, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 1.095578670501709\n",
      "Step: 6414, Loss: 0.9196292757987976, Accuracy: 1.0, Computation time: 1.1428964138031006\n",
      "Step: 6415, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.3773460388183594\n",
      "Step: 6416, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 0.8918740749359131\n",
      "Step: 6417, Loss: 0.9375741481781006, Accuracy: 0.96875, Computation time: 1.259789228439331\n",
      "Step: 6418, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 0.9778778553009033\n",
      "Step: 6419, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 0.9145433902740479\n",
      "Step: 6420, Loss: 0.9250962734222412, Accuracy: 1.0, Computation time: 1.6709914207458496\n",
      "Step: 6421, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.5916388034820557\n",
      "Step: 6422, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 1.172919511795044\n",
      "Step: 6423, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.0400261878967285\n",
      "Step: 6424, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.2846617698669434\n",
      "Step: 6425, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 1.5178704261779785\n",
      "Step: 6426, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.1197664737701416\n",
      "Step: 6427, Loss: 0.9399538040161133, Accuracy: 0.96875, Computation time: 1.2823009490966797\n",
      "Step: 6428, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 1.6986327171325684\n",
      "Step: 6429, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.287424087524414\n",
      "Step: 6430, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.6055960655212402\n",
      "Step: 6431, Loss: 0.9374489784240723, Accuracy: 0.96875, Computation time: 1.1568348407745361\n",
      "Step: 6432, Loss: 0.9159197807312012, Accuracy: 1.0, Computation time: 1.507080078125\n",
      "Step: 6433, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.182729721069336\n",
      "Step: 6434, Loss: 0.9166433811187744, Accuracy: 1.0, Computation time: 1.227914571762085\n",
      "Step: 6435, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.0637516975402832\n",
      "Step: 6436, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 1.2252247333526611\n",
      "Step: 6437, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.1525788307189941\n",
      "Step: 6438, Loss: 0.9375103116035461, Accuracy: 0.96875, Computation time: 1.2389447689056396\n",
      "Step: 6439, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 1.608142614364624\n",
      "Step: 6440, Loss: 0.9158362746238708, Accuracy: 1.0, Computation time: 1.6533548831939697\n",
      "Step: 6441, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 1.2364118099212646\n",
      "Step: 6442, Loss: 0.9160942435264587, Accuracy: 1.0, Computation time: 1.302046775817871\n",
      "Step: 6443, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.5580403804779053\n",
      "Step: 6444, Loss: 0.9375259876251221, Accuracy: 0.96875, Computation time: 1.4399487972259521\n",
      "Step: 6445, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.4241056442260742\n",
      "Step: 6446, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.3294010162353516\n",
      "Step: 6447, Loss: 0.9375326037406921, Accuracy: 0.96875, Computation time: 1.2580265998840332\n",
      "Step: 6448, Loss: 0.916822075843811, Accuracy: 1.0, Computation time: 1.2824201583862305\n",
      "Step: 6449, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.2317984104156494\n",
      "Step: 6450, Loss: 0.9173034429550171, Accuracy: 1.0, Computation time: 1.1906664371490479\n",
      "Step: 6451, Loss: 0.9158356189727783, Accuracy: 1.0, Computation time: 1.5875706672668457\n",
      "Step: 6452, Loss: 0.9589986801147461, Accuracy: 0.9375, Computation time: 1.4335517883300781\n",
      "Step: 6453, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.396376371383667\n",
      "Step: 6454, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.8881430625915527\n",
      "Step: 6455, Loss: 0.9375960826873779, Accuracy: 0.96875, Computation time: 1.7523784637451172\n",
      "Step: 6456, Loss: 0.9357714056968689, Accuracy: 0.96875, Computation time: 1.6180150508880615\n",
      "Step: 6457, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.4214158058166504\n",
      "Step: 6458, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.1959903240203857\n",
      "Step: 6459, Loss: 0.928449273109436, Accuracy: 0.96875, Computation time: 2.445128917694092\n",
      "Step: 6460, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.988985300064087\n",
      "Step: 6461, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.1933977603912354\n",
      "Step: 6462, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.233940839767456\n",
      "Step: 6463, Loss: 0.9159471988677979, Accuracy: 1.0, Computation time: 1.2040715217590332\n",
      "Step: 6464, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.3230321407318115\n",
      "Step: 6465, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 1.1755120754241943\n",
      "Step: 6466, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.271083116531372\n",
      "Step: 6467, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 1.3209943771362305\n",
      "Step: 6468, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.4500350952148438\n",
      "Step: 6469, Loss: 0.9373447895050049, Accuracy: 0.96875, Computation time: 1.102088212966919\n",
      "Step: 6470, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.0261337757110596\n",
      "Step: 6471, Loss: 0.915931761264801, Accuracy: 1.0, Computation time: 1.5405144691467285\n",
      "Step: 6472, Loss: 0.9375045299530029, Accuracy: 0.96875, Computation time: 1.0150673389434814\n",
      "Step: 6473, Loss: 0.9159373044967651, Accuracy: 1.0, Computation time: 1.1256442070007324\n",
      "Step: 6474, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 1.1093099117279053\n",
      "Step: 6475, Loss: 0.9373717308044434, Accuracy: 0.96875, Computation time: 1.3838798999786377\n",
      "Step: 6476, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.2573950290679932\n",
      "Step: 6477, Loss: 0.9159278273582458, Accuracy: 1.0, Computation time: 1.4014675617218018\n",
      "Step: 6478, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 0.9432141780853271\n",
      "Step: 6479, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.1482727527618408\n",
      "Step: 6480, Loss: 0.9373902678489685, Accuracy: 0.96875, Computation time: 1.3153510093688965\n",
      "Step: 6481, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 0.9979643821716309\n",
      "Step: 6482, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 0.9497532844543457\n",
      "Step: 6483, Loss: 0.9584775567054749, Accuracy: 0.9375, Computation time: 1.3411004543304443\n",
      "Step: 6484, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.1734130382537842\n",
      "Step: 6485, Loss: 0.9166491031646729, Accuracy: 1.0, Computation time: 1.249549388885498\n",
      "Step: 6486, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 1.0450031757354736\n",
      "Step: 6487, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.3493099212646484\n",
      "Step: 6488, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.0976157188415527\n",
      "Step: 6489, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.1697399616241455\n",
      "Step: 6490, Loss: 0.9375469088554382, Accuracy: 0.96875, Computation time: 1.3152110576629639\n",
      "Step: 6491, Loss: 0.9334942698478699, Accuracy: 0.96875, Computation time: 1.113173007965088\n",
      "Step: 6492, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.1532914638519287\n",
      "Step: 6493, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.273714303970337\n",
      "Step: 6494, Loss: 0.9376004934310913, Accuracy: 0.96875, Computation time: 1.4111132621765137\n",
      "Step: 6495, Loss: 0.9159141778945923, Accuracy: 1.0, Computation time: 1.242480993270874\n",
      "Step: 6496, Loss: 0.9377213716506958, Accuracy: 0.96875, Computation time: 1.3523259162902832\n",
      "Step: 6497, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 1.0127012729644775\n",
      "Step: 6498, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 1.1152498722076416\n",
      "Step: 6499, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.159538745880127\n",
      "Step: 6500, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.0481865406036377\n",
      "Step: 6501, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.6785411834716797\n",
      "Step: 6502, Loss: 0.9168752431869507, Accuracy: 1.0, Computation time: 1.3819448947906494\n",
      "Step: 6503, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.2647547721862793\n",
      "Step: 6504, Loss: 0.9375672340393066, Accuracy: 0.96875, Computation time: 1.3594260215759277\n",
      "Step: 6505, Loss: 0.9162135124206543, Accuracy: 1.0, Computation time: 1.0255451202392578\n",
      "Step: 6506, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 0.9047386646270752\n",
      "Step: 6507, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.0363399982452393\n",
      "Step: 6508, Loss: 0.9374881982803345, Accuracy: 0.96875, Computation time: 1.011317253112793\n",
      "Step: 6509, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 0.9181404113769531\n",
      "Step: 6510, Loss: 0.925177276134491, Accuracy: 0.96875, Computation time: 1.444772720336914\n",
      "Step: 6511, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 0.9984512329101562\n",
      "Step: 6512, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.587320327758789\n",
      "Step: 6513, Loss: 0.9173167943954468, Accuracy: 1.0, Computation time: 0.9815065860748291\n",
      "Step: 6514, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.0614938735961914\n",
      "Step: 6515, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 1.1650004386901855\n",
      "Step: 6516, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 1.1741266250610352\n",
      "Step: 6517, Loss: 0.9158521294593811, Accuracy: 1.0, Computation time: 1.2556967735290527\n",
      "Step: 6518, Loss: 0.918397843837738, Accuracy: 1.0, Computation time: 1.2853763103485107\n",
      "Step: 6519, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 1.2246761322021484\n",
      "Step: 6520, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.9075555801391602\n",
      "Step: 6521, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.411886215209961\n",
      "Step: 6522, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.2245569229125977\n",
      "Step: 6523, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.014742136001587\n",
      "Step: 6524, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.0702691078186035\n",
      "Step: 6525, Loss: 0.9162092208862305, Accuracy: 1.0, Computation time: 1.7614493370056152\n",
      "Step: 6526, Loss: 0.9160425066947937, Accuracy: 1.0, Computation time: 1.3503930568695068\n",
      "Step: 6527, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.0870518684387207\n",
      "Step: 6528, Loss: 0.9169602990150452, Accuracy: 1.0, Computation time: 1.3093085289001465\n",
      "Step: 6529, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 2.23740291595459\n",
      "Step: 6530, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 0.9664595127105713\n",
      "Step: 6531, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.9380989074707031\n",
      "########################\n",
      "Test loss: 1.1257400512695312, Test Accuracy_epoch47: 0.6962127089500427\n",
      "########################\n",
      "Step: 6532, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.0822417736053467\n",
      "Step: 6533, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.12776517868042\n",
      "Step: 6534, Loss: 0.9160358905792236, Accuracy: 1.0, Computation time: 1.3184316158294678\n",
      "Step: 6535, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.2930471897125244\n",
      "Step: 6536, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 1.1557605266571045\n",
      "Step: 6537, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.3655211925506592\n",
      "Step: 6538, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.5629539489746094\n",
      "Step: 6539, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 1.3741660118103027\n",
      "Step: 6540, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.8876504898071289\n",
      "Step: 6541, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.3992054462432861\n",
      "Step: 6542, Loss: 0.9158353209495544, Accuracy: 1.0, Computation time: 0.9840772151947021\n",
      "Step: 6543, Loss: 0.9160953164100647, Accuracy: 1.0, Computation time: 1.082097053527832\n",
      "Step: 6544, Loss: 0.9158341288566589, Accuracy: 1.0, Computation time: 1.120894193649292\n",
      "Step: 6545, Loss: 0.9375112652778625, Accuracy: 0.96875, Computation time: 1.0914161205291748\n",
      "Step: 6546, Loss: 0.9158434271812439, Accuracy: 1.0, Computation time: 0.9557545185089111\n",
      "Step: 6547, Loss: 0.9158420562744141, Accuracy: 1.0, Computation time: 0.9327311515808105\n",
      "Step: 6548, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.0492115020751953\n",
      "Step: 6549, Loss: 0.9375303983688354, Accuracy: 0.96875, Computation time: 1.0541930198669434\n",
      "Step: 6550, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 2.2985565662384033\n",
      "Step: 6551, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.1109373569488525\n",
      "Step: 6552, Loss: 0.9171128273010254, Accuracy: 1.0, Computation time: 1.3469171524047852\n",
      "Step: 6553, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.022170066833496\n",
      "Step: 6554, Loss: 0.9168037176132202, Accuracy: 1.0, Computation time: 1.0236289501190186\n",
      "Step: 6555, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.3694531917572021\n",
      "Step: 6556, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.048414707183838\n",
      "Step: 6557, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.0725808143615723\n",
      "Step: 6558, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.1042249202728271\n",
      "Step: 6559, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 1.2316389083862305\n",
      "Step: 6560, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.6470789909362793\n",
      "Step: 6561, Loss: 0.9373700618743896, Accuracy: 0.96875, Computation time: 1.1564056873321533\n",
      "Step: 6562, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.3071670532226562\n",
      "Step: 6563, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.2019166946411133\n",
      "Step: 6564, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.276207685470581\n",
      "Step: 6565, Loss: 0.9174538850784302, Accuracy: 1.0, Computation time: 1.9945054054260254\n",
      "Step: 6566, Loss: 0.9375709891319275, Accuracy: 0.96875, Computation time: 1.1255607604980469\n",
      "Step: 6567, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.1759891510009766\n",
      "Step: 6568, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.2280912399291992\n",
      "Step: 6569, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.1281497478485107\n",
      "Step: 6570, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.3341257572174072\n",
      "Step: 6571, Loss: 0.9375196099281311, Accuracy: 0.96875, Computation time: 1.1902399063110352\n",
      "Step: 6572, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.3596022129058838\n",
      "Step: 6573, Loss: 0.9158470034599304, Accuracy: 1.0, Computation time: 1.6508593559265137\n",
      "Step: 6574, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.4241304397583008\n",
      "Step: 6575, Loss: 0.9359468817710876, Accuracy: 0.96875, Computation time: 1.7190577983856201\n",
      "Step: 6576, Loss: 0.9171529412269592, Accuracy: 1.0, Computation time: 1.381234884262085\n",
      "Step: 6577, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.4967060089111328\n",
      "Step: 6578, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 1.3925809860229492\n",
      "Step: 6579, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.204054594039917\n",
      "Step: 6580, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.3690898418426514\n",
      "Step: 6581, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.4221289157867432\n",
      "Step: 6582, Loss: 0.9160034656524658, Accuracy: 1.0, Computation time: 1.2860007286071777\n",
      "Step: 6583, Loss: 0.9158425331115723, Accuracy: 1.0, Computation time: 1.1046075820922852\n",
      "Step: 6584, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.3228049278259277\n",
      "Step: 6585, Loss: 0.9158368110656738, Accuracy: 1.0, Computation time: 1.3053371906280518\n",
      "Step: 6586, Loss: 0.9158397316932678, Accuracy: 1.0, Computation time: 1.2739903926849365\n",
      "Step: 6587, Loss: 0.9374540448188782, Accuracy: 0.96875, Computation time: 1.447007179260254\n",
      "Step: 6588, Loss: 0.9183143973350525, Accuracy: 1.0, Computation time: 2.0716166496276855\n",
      "Step: 6589, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.3164422512054443\n",
      "Step: 6590, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.0544822216033936\n",
      "Step: 6591, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 1.299628734588623\n",
      "Step: 6592, Loss: 0.9162302613258362, Accuracy: 1.0, Computation time: 1.3706779479980469\n",
      "Step: 6593, Loss: 0.9374799728393555, Accuracy: 0.96875, Computation time: 1.266404390335083\n",
      "Step: 6594, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.2192442417144775\n",
      "Step: 6595, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.7298915386199951\n",
      "Step: 6596, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.3955628871917725\n",
      "Step: 6597, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 1.3937311172485352\n",
      "Step: 6598, Loss: 0.9372012615203857, Accuracy: 0.96875, Computation time: 1.5957446098327637\n",
      "Step: 6599, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.5648961067199707\n",
      "Step: 6600, Loss: 0.916412353515625, Accuracy: 1.0, Computation time: 1.4397480487823486\n",
      "Step: 6601, Loss: 0.9375783801078796, Accuracy: 0.96875, Computation time: 1.143589735031128\n",
      "Step: 6602, Loss: 0.9158894419670105, Accuracy: 1.0, Computation time: 1.6103522777557373\n",
      "Step: 6603, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.154275894165039\n",
      "Step: 6604, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.3324856758117676\n",
      "Step: 6605, Loss: 0.9368256330490112, Accuracy: 0.96875, Computation time: 1.3801822662353516\n",
      "Step: 6606, Loss: 0.9158448576927185, Accuracy: 1.0, Computation time: 1.1718647480010986\n",
      "Step: 6607, Loss: 0.9375080466270447, Accuracy: 0.96875, Computation time: 1.2207117080688477\n",
      "Step: 6608, Loss: 0.937534511089325, Accuracy: 0.96875, Computation time: 1.071183204650879\n",
      "Step: 6609, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.120647668838501\n",
      "Step: 6610, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 1.3877859115600586\n",
      "Step: 6611, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.2585501670837402\n",
      "Step: 6612, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.4053378105163574\n",
      "Step: 6613, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.1718380451202393\n",
      "Step: 6614, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.7174901962280273\n",
      "Step: 6615, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 1.442321538925171\n",
      "Step: 6616, Loss: 0.932431697845459, Accuracy: 0.96875, Computation time: 1.3121299743652344\n",
      "Step: 6617, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.1960949897766113\n",
      "Step: 6618, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 0.9782371520996094\n",
      "Step: 6619, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 0.9631233215332031\n",
      "Step: 6620, Loss: 0.9374063611030579, Accuracy: 0.96875, Computation time: 1.1797728538513184\n",
      "Step: 6621, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.421050786972046\n",
      "Step: 6622, Loss: 0.9375597834587097, Accuracy: 0.96875, Computation time: 1.071976900100708\n",
      "Step: 6623, Loss: 0.9167740345001221, Accuracy: 1.0, Computation time: 1.3670885562896729\n",
      "Step: 6624, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.8043866157531738\n",
      "Step: 6625, Loss: 0.9159446358680725, Accuracy: 1.0, Computation time: 1.0820820331573486\n",
      "Step: 6626, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.3438189029693604\n",
      "Step: 6627, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.1619141101837158\n",
      "Step: 6628, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 0.9733390808105469\n",
      "Step: 6629, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.3342835903167725\n",
      "Step: 6630, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.2620501518249512\n",
      "Step: 6631, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 0.9578044414520264\n",
      "Step: 6632, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.109781265258789\n",
      "Step: 6633, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.119969129562378\n",
      "Step: 6634, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.10557222366333\n",
      "Step: 6635, Loss: 0.9369816184043884, Accuracy: 0.96875, Computation time: 1.3389747142791748\n",
      "Step: 6636, Loss: 0.937523603439331, Accuracy: 0.96875, Computation time: 1.3511290550231934\n",
      "Step: 6637, Loss: 0.9368705749511719, Accuracy: 0.96875, Computation time: 1.2207140922546387\n",
      "Step: 6638, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.1151070594787598\n",
      "Step: 6639, Loss: 0.9159451127052307, Accuracy: 1.0, Computation time: 1.287426233291626\n",
      "Step: 6640, Loss: 0.9167472720146179, Accuracy: 1.0, Computation time: 2.0644354820251465\n",
      "Step: 6641, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.2171611785888672\n",
      "Step: 6642, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 0.9018237590789795\n",
      "Step: 6643, Loss: 0.9158341884613037, Accuracy: 1.0, Computation time: 1.054189682006836\n",
      "Step: 6644, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.1251976490020752\n",
      "Step: 6645, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.0876209735870361\n",
      "Step: 6646, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.1133239269256592\n",
      "Step: 6647, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.5810351371765137\n",
      "Step: 6648, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.0941333770751953\n",
      "Step: 6649, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.0731704235076904\n",
      "Step: 6650, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.1152338981628418\n",
      "Step: 6651, Loss: 0.9178364872932434, Accuracy: 1.0, Computation time: 1.5069313049316406\n",
      "Step: 6652, Loss: 0.9168984889984131, Accuracy: 1.0, Computation time: 1.1321651935577393\n",
      "Step: 6653, Loss: 0.9170149564743042, Accuracy: 1.0, Computation time: 1.7791306972503662\n",
      "Step: 6654, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.19366455078125\n",
      "Step: 6655, Loss: 0.9163389205932617, Accuracy: 1.0, Computation time: 1.1843671798706055\n",
      "Step: 6656, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.4504873752593994\n",
      "Step: 6657, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 1.5140743255615234\n",
      "Step: 6658, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.3161790370941162\n",
      "Step: 6659, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.371291160583496\n",
      "Step: 6660, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 1.2308015823364258\n",
      "Step: 6661, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.3735630512237549\n",
      "Step: 6662, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.401068925857544\n",
      "Step: 6663, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.3517756462097168\n",
      "Step: 6664, Loss: 0.9158393740653992, Accuracy: 1.0, Computation time: 1.270958662033081\n",
      "Step: 6665, Loss: 0.9375019073486328, Accuracy: 0.96875, Computation time: 1.6388459205627441\n",
      "Step: 6666, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.5948164463043213\n",
      "Step: 6667, Loss: 0.9374379515647888, Accuracy: 0.96875, Computation time: 1.511213779449463\n",
      "Step: 6668, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.3279035091400146\n",
      "Step: 6669, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.0801172256469727\n",
      "########################\n",
      "Test loss: 1.1258814334869385, Test Accuracy_epoch48: 0.6929895281791687\n",
      "########################\n",
      "Step: 6670, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.1775884628295898\n",
      "Step: 6671, Loss: 0.9160112738609314, Accuracy: 1.0, Computation time: 1.7646677494049072\n",
      "Step: 6672, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.6953999996185303\n",
      "Step: 6673, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.263390064239502\n",
      "Step: 6674, Loss: 0.9160275459289551, Accuracy: 1.0, Computation time: 1.183429479598999\n",
      "Step: 6675, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.4278249740600586\n",
      "Step: 6676, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.1658599376678467\n",
      "Step: 6677, Loss: 0.9375895261764526, Accuracy: 0.96875, Computation time: 1.393833875656128\n",
      "Step: 6678, Loss: 0.9375821352005005, Accuracy: 0.96875, Computation time: 1.3683462142944336\n",
      "Step: 6679, Loss: 0.9375357627868652, Accuracy: 0.96875, Computation time: 1.2114262580871582\n",
      "Step: 6680, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.1131455898284912\n",
      "Step: 6681, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.4773006439208984\n",
      "Step: 6682, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.419569492340088\n",
      "Step: 6683, Loss: 0.931729793548584, Accuracy: 0.96875, Computation time: 2.0285375118255615\n",
      "Step: 6684, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 1.4193103313446045\n",
      "Step: 6685, Loss: 0.9159099459648132, Accuracy: 1.0, Computation time: 1.3643059730529785\n",
      "Step: 6686, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.1615161895751953\n",
      "Step: 6687, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.2143454551696777\n",
      "Step: 6688, Loss: 0.9162673950195312, Accuracy: 1.0, Computation time: 1.2541241645812988\n",
      "Step: 6689, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.0331766605377197\n",
      "Step: 6690, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 1.2787582874298096\n",
      "Step: 6691, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 1.0236093997955322\n",
      "Step: 6692, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.1525211334228516\n",
      "Step: 6693, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.7145075798034668\n",
      "Step: 6694, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.550635814666748\n",
      "Step: 6695, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.4990208148956299\n",
      "Step: 6696, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.440342664718628\n",
      "Step: 6697, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 1.2254188060760498\n",
      "Step: 6698, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.7261316776275635\n",
      "Step: 6699, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.439279317855835\n",
      "Step: 6700, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.758789300918579\n",
      "Step: 6701, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.4843406677246094\n",
      "Step: 6702, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.1494884490966797\n",
      "Step: 6703, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 1.3952949047088623\n",
      "Step: 6704, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.3045740127563477\n",
      "Step: 6705, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.4965801239013672\n",
      "Step: 6706, Loss: 0.9158340692520142, Accuracy: 1.0, Computation time: 1.1729493141174316\n",
      "Step: 6707, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.1192736625671387\n",
      "Step: 6708, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.8150734901428223\n",
      "Step: 6709, Loss: 0.9375138878822327, Accuracy: 0.96875, Computation time: 1.464876651763916\n",
      "Step: 6710, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.2366814613342285\n",
      "Step: 6711, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.33371901512146\n",
      "Step: 6712, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.2204253673553467\n",
      "Step: 6713, Loss: 0.9169679880142212, Accuracy: 1.0, Computation time: 1.6458876132965088\n",
      "Step: 6714, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.483182430267334\n",
      "Step: 6715, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.5082337856292725\n",
      "Step: 6716, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 2.0837464332580566\n",
      "Step: 6717, Loss: 0.9158441424369812, Accuracy: 1.0, Computation time: 1.449686050415039\n",
      "Step: 6718, Loss: 0.9161184430122375, Accuracy: 1.0, Computation time: 1.485551118850708\n",
      "Step: 6719, Loss: 0.9374615550041199, Accuracy: 0.96875, Computation time: 1.4862897396087646\n",
      "Step: 6720, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 1.1333582401275635\n",
      "Step: 6721, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.1219725608825684\n",
      "Step: 6722, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 2.360384941101074\n",
      "Step: 6723, Loss: 0.9375211000442505, Accuracy: 0.96875, Computation time: 1.3219823837280273\n",
      "Step: 6724, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.4409468173980713\n",
      "Step: 6725, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.329789638519287\n",
      "Step: 6726, Loss: 0.917515754699707, Accuracy: 1.0, Computation time: 1.2758967876434326\n",
      "Step: 6727, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.542099952697754\n",
      "Step: 6728, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.497258186340332\n",
      "Step: 6729, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 1.3723008632659912\n",
      "Step: 6730, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.5269780158996582\n",
      "Step: 6731, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.3201789855957031\n",
      "Step: 6732, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 1.3463001251220703\n",
      "Step: 6733, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.3493397235870361\n",
      "Step: 6734, Loss: 0.915838897228241, Accuracy: 1.0, Computation time: 1.1343436241149902\n",
      "Step: 6735, Loss: 0.915834367275238, Accuracy: 1.0, Computation time: 1.3140766620635986\n",
      "Step: 6736, Loss: 0.9375869631767273, Accuracy: 0.96875, Computation time: 1.2970342636108398\n",
      "Step: 6737, Loss: 0.9158371686935425, Accuracy: 1.0, Computation time: 1.162846565246582\n",
      "Step: 6738, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.5314934253692627\n",
      "Step: 6739, Loss: 0.9375184178352356, Accuracy: 0.96875, Computation time: 1.2719542980194092\n",
      "Step: 6740, Loss: 0.9158401489257812, Accuracy: 1.0, Computation time: 1.27154541015625\n",
      "Step: 6741, Loss: 0.9166983962059021, Accuracy: 1.0, Computation time: 1.3218753337860107\n",
      "Step: 6742, Loss: 0.9311031103134155, Accuracy: 0.96875, Computation time: 1.6437170505523682\n",
      "Step: 6743, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.1933653354644775\n",
      "Step: 6744, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.6208374500274658\n",
      "Step: 6745, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.2128252983093262\n",
      "Step: 6746, Loss: 0.9375143647193909, Accuracy: 0.96875, Computation time: 1.5186707973480225\n",
      "Step: 6747, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.3441905975341797\n",
      "Step: 6748, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 1.5416584014892578\n",
      "Step: 6749, Loss: 0.915838897228241, Accuracy: 1.0, Computation time: 1.3232438564300537\n",
      "Step: 6750, Loss: 0.9158394932746887, Accuracy: 1.0, Computation time: 1.600794792175293\n",
      "Step: 6751, Loss: 0.9164239168167114, Accuracy: 1.0, Computation time: 1.4468967914581299\n",
      "Step: 6752, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.7278804779052734\n",
      "Step: 6753, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.157198429107666\n",
      "Step: 6754, Loss: 0.933422327041626, Accuracy: 0.96875, Computation time: 1.785978078842163\n",
      "Step: 6755, Loss: 0.9162169694900513, Accuracy: 1.0, Computation time: 1.237009048461914\n",
      "Step: 6756, Loss: 0.9375700354576111, Accuracy: 0.96875, Computation time: 1.3649687767028809\n",
      "Step: 6757, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.7501513957977295\n",
      "Step: 6758, Loss: 0.9161232113838196, Accuracy: 1.0, Computation time: 1.2580804824829102\n",
      "Step: 6759, Loss: 0.9526002407073975, Accuracy: 0.9375, Computation time: 2.139263391494751\n",
      "Step: 6760, Loss: 0.9375131130218506, Accuracy: 0.96875, Computation time: 1.0946967601776123\n",
      "Step: 6761, Loss: 0.9534717202186584, Accuracy: 0.9375, Computation time: 1.2995121479034424\n",
      "Step: 6762, Loss: 0.9182946681976318, Accuracy: 1.0, Computation time: 1.9913434982299805\n",
      "Step: 6763, Loss: 0.9160149097442627, Accuracy: 1.0, Computation time: 1.0164809226989746\n",
      "Step: 6764, Loss: 0.915932297706604, Accuracy: 1.0, Computation time: 1.033700942993164\n",
      "Step: 6765, Loss: 0.9159942269325256, Accuracy: 1.0, Computation time: 1.0722906589508057\n",
      "Step: 6766, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 0.987781286239624\n",
      "Step: 6767, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.181504726409912\n",
      "Step: 6768, Loss: 0.9375630617141724, Accuracy: 0.96875, Computation time: 1.1391818523406982\n",
      "Step: 6769, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.5914804935455322\n",
      "Step: 6770, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.175158977508545\n",
      "Step: 6771, Loss: 0.9193184971809387, Accuracy: 1.0, Computation time: 1.617480993270874\n",
      "Step: 6772, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 1.1009223461151123\n",
      "Step: 6773, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.0121636390686035\n",
      "Step: 6774, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 1.3905785083770752\n",
      "Step: 6775, Loss: 0.916447639465332, Accuracy: 1.0, Computation time: 1.3855764865875244\n",
      "Step: 6776, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.1400642395019531\n",
      "Step: 6777, Loss: 0.9160675406455994, Accuracy: 1.0, Computation time: 1.277071475982666\n",
      "Step: 6778, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.275665044784546\n",
      "Step: 6779, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.2641148567199707\n",
      "Step: 6780, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.4877524375915527\n",
      "Step: 6781, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.309699535369873\n",
      "Step: 6782, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.288987398147583\n",
      "Step: 6783, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.1771011352539062\n",
      "Step: 6784, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.2420318126678467\n",
      "Step: 6785, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.0866022109985352\n",
      "Step: 6786, Loss: 0.9158395528793335, Accuracy: 1.0, Computation time: 1.005929708480835\n",
      "Step: 6787, Loss: 0.9375290274620056, Accuracy: 0.96875, Computation time: 0.8955409526824951\n",
      "Step: 6788, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 1.050346851348877\n",
      "Step: 6789, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.066307544708252\n",
      "Step: 6790, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 0.9277386665344238\n",
      "Step: 6791, Loss: 0.9230809211730957, Accuracy: 1.0, Computation time: 1.7516756057739258\n",
      "Step: 6792, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.1053078174591064\n",
      "Step: 6793, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.486757755279541\n",
      "Step: 6794, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 1.5406649112701416\n",
      "Step: 6795, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 0.9818601608276367\n",
      "Step: 6796, Loss: 0.9160261750221252, Accuracy: 1.0, Computation time: 1.125807762145996\n",
      "Step: 6797, Loss: 0.9159553647041321, Accuracy: 1.0, Computation time: 1.3751657009124756\n",
      "Step: 6798, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 1.0632648468017578\n",
      "Step: 6799, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.10438871383667\n",
      "Step: 6800, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.2061717510223389\n",
      "Step: 6801, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 1.2752726078033447\n",
      "Step: 6802, Loss: 0.9369262456893921, Accuracy: 0.96875, Computation time: 1.7770352363586426\n",
      "Step: 6803, Loss: 0.9374695420265198, Accuracy: 0.96875, Computation time: 1.3130455017089844\n",
      "Step: 6804, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.1676483154296875\n",
      "Step: 6805, Loss: 0.9592896699905396, Accuracy: 0.9375, Computation time: 1.6039142608642578\n",
      "Step: 6806, Loss: 0.9376294612884521, Accuracy: 0.96875, Computation time: 1.5009193420410156\n",
      "Step: 6807, Loss: 0.9372319579124451, Accuracy: 0.96875, Computation time: 1.5927784442901611\n",
      "Step: 6808, Loss: 0.9162346124649048, Accuracy: 1.0, Computation time: 1.9008917808532715\n",
      "########################\n",
      "Test loss: 1.1242796182632446, Test Accuracy_epoch49: 0.7010475397109985\n",
      "########################\n",
      "Step: 6809, Loss: 0.9160570502281189, Accuracy: 1.0, Computation time: 1.3489751815795898\n",
      "Step: 6810, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 1.4769518375396729\n",
      "Step: 6811, Loss: 0.9161062836647034, Accuracy: 1.0, Computation time: 1.5594911575317383\n",
      "Step: 6812, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.4372999668121338\n",
      "Step: 6813, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.3810441493988037\n",
      "Step: 6814, Loss: 0.9159957766532898, Accuracy: 1.0, Computation time: 1.1845223903656006\n",
      "Step: 6815, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.161689043045044\n",
      "Step: 6816, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.3179378509521484\n",
      "Step: 6817, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.2060604095458984\n",
      "Step: 6818, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.172104835510254\n",
      "Step: 6819, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.2826004028320312\n",
      "Step: 6820, Loss: 0.9327131509780884, Accuracy: 0.96875, Computation time: 1.865983486175537\n",
      "Step: 6821, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.1683266162872314\n",
      "Step: 6822, Loss: 0.933505117893219, Accuracy: 0.96875, Computation time: 2.515230655670166\n",
      "Step: 6823, Loss: 0.9159941673278809, Accuracy: 1.0, Computation time: 1.3271064758300781\n",
      "Step: 6824, Loss: 0.9159876704216003, Accuracy: 1.0, Computation time: 1.5556867122650146\n",
      "Step: 6825, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.1237797737121582\n",
      "Step: 6826, Loss: 0.9359791278839111, Accuracy: 0.96875, Computation time: 1.7650489807128906\n",
      "Step: 6827, Loss: 0.9159929156303406, Accuracy: 1.0, Computation time: 1.3773601055145264\n",
      "Step: 6828, Loss: 0.9159770607948303, Accuracy: 1.0, Computation time: 0.9687542915344238\n",
      "Step: 6829, Loss: 0.9378926753997803, Accuracy: 0.96875, Computation time: 1.2782695293426514\n",
      "Step: 6830, Loss: 0.9160937070846558, Accuracy: 1.0, Computation time: 1.1249659061431885\n",
      "Step: 6831, Loss: 0.91611248254776, Accuracy: 1.0, Computation time: 1.0405831336975098\n",
      "Step: 6832, Loss: 0.916056752204895, Accuracy: 1.0, Computation time: 1.0442023277282715\n",
      "Step: 6833, Loss: 0.9382396936416626, Accuracy: 0.96875, Computation time: 1.0787103176116943\n",
      "Step: 6834, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 0.9912166595458984\n",
      "Step: 6835, Loss: 0.9159480929374695, Accuracy: 1.0, Computation time: 1.0494203567504883\n",
      "Step: 6836, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.1433582305908203\n",
      "Step: 6837, Loss: 0.9161283373832703, Accuracy: 1.0, Computation time: 1.3647046089172363\n",
      "Step: 6838, Loss: 0.9439064860343933, Accuracy: 0.96875, Computation time: 1.448662519454956\n",
      "Step: 6839, Loss: 0.9161289930343628, Accuracy: 1.0, Computation time: 1.0013701915740967\n",
      "Step: 6840, Loss: 0.9512348175048828, Accuracy: 0.9375, Computation time: 1.6094698905944824\n",
      "Step: 6841, Loss: 0.9163725972175598, Accuracy: 1.0, Computation time: 1.050983190536499\n",
      "Step: 6842, Loss: 0.9167621731758118, Accuracy: 1.0, Computation time: 1.3287672996520996\n",
      "Step: 6843, Loss: 0.9168230295181274, Accuracy: 1.0, Computation time: 1.1517822742462158\n",
      "Step: 6844, Loss: 0.9170286655426025, Accuracy: 1.0, Computation time: 0.9448676109313965\n",
      "Step: 6845, Loss: 0.9175288081169128, Accuracy: 1.0, Computation time: 1.1625428199768066\n",
      "Step: 6846, Loss: 0.9379572868347168, Accuracy: 0.96875, Computation time: 1.4822003841400146\n",
      "Step: 6847, Loss: 0.9376971125602722, Accuracy: 0.96875, Computation time: 1.2848284244537354\n",
      "Step: 6848, Loss: 0.9194222688674927, Accuracy: 1.0, Computation time: 1.3266422748565674\n",
      "Step: 6849, Loss: 0.9585176706314087, Accuracy: 0.9375, Computation time: 1.1986901760101318\n",
      "Step: 6850, Loss: 0.937189519405365, Accuracy: 0.96875, Computation time: 1.0059854984283447\n",
      "Step: 6851, Loss: 0.9381073713302612, Accuracy: 0.96875, Computation time: 1.0621516704559326\n",
      "Step: 6852, Loss: 0.917199432849884, Accuracy: 1.0, Computation time: 1.148942232131958\n",
      "Step: 6853, Loss: 0.916512668132782, Accuracy: 1.0, Computation time: 0.9489147663116455\n",
      "Step: 6854, Loss: 0.9380722045898438, Accuracy: 0.96875, Computation time: 1.6077985763549805\n",
      "Step: 6855, Loss: 0.916057288646698, Accuracy: 1.0, Computation time: 1.1856095790863037\n",
      "Step: 6856, Loss: 0.916029691696167, Accuracy: 1.0, Computation time: 1.050356388092041\n",
      "Step: 6857, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 1.3213918209075928\n",
      "Step: 6858, Loss: 0.9259171485900879, Accuracy: 0.96875, Computation time: 1.516071081161499\n",
      "Step: 6859, Loss: 0.9367663860321045, Accuracy: 0.96875, Computation time: 1.7730708122253418\n",
      "Step: 6860, Loss: 0.9377891421318054, Accuracy: 0.96875, Computation time: 1.0942096710205078\n",
      "Step: 6861, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 0.9967441558837891\n",
      "Step: 6862, Loss: 0.920052707195282, Accuracy: 1.0, Computation time: 1.1001431941986084\n",
      "Step: 6863, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.150763988494873\n",
      "Step: 6864, Loss: 0.9160845279693604, Accuracy: 1.0, Computation time: 1.3191249370574951\n",
      "Step: 6865, Loss: 0.9159232378005981, Accuracy: 1.0, Computation time: 0.9081881046295166\n",
      "Step: 6866, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 0.8470823764801025\n",
      "Step: 6867, Loss: 0.9167367815971375, Accuracy: 1.0, Computation time: 1.3765308856964111\n",
      "Step: 6868, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 1.001593828201294\n",
      "Step: 6869, Loss: 0.9197697639465332, Accuracy: 1.0, Computation time: 1.3815586566925049\n",
      "Step: 6870, Loss: 0.9160740971565247, Accuracy: 1.0, Computation time: 1.2600014209747314\n",
      "Step: 6871, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 0.8695268630981445\n",
      "Step: 6872, Loss: 0.9172078967094421, Accuracy: 1.0, Computation time: 1.4842736721038818\n",
      "Step: 6873, Loss: 0.9159957766532898, Accuracy: 1.0, Computation time: 1.21174955368042\n",
      "Step: 6874, Loss: 0.9376011490821838, Accuracy: 0.96875, Computation time: 1.1406629085540771\n",
      "Step: 6875, Loss: 0.9161465764045715, Accuracy: 1.0, Computation time: 1.2482845783233643\n",
      "Step: 6876, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.2441177368164062\n",
      "Step: 6877, Loss: 0.9175792932510376, Accuracy: 1.0, Computation time: 1.3884053230285645\n",
      "Step: 6878, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 0.9807243347167969\n",
      "Step: 6879, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 0.8740329742431641\n",
      "Step: 6880, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.0352661609649658\n",
      "Step: 6881, Loss: 0.9158763289451599, Accuracy: 1.0, Computation time: 1.1136724948883057\n",
      "Step: 6882, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 0.9300990104675293\n",
      "Step: 6883, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.3229615688323975\n",
      "Step: 6884, Loss: 0.9227350950241089, Accuracy: 1.0, Computation time: 1.3461613655090332\n",
      "Step: 6885, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.0907080173492432\n",
      "Step: 6886, Loss: 0.9374877214431763, Accuracy: 0.96875, Computation time: 1.2059507369995117\n",
      "Step: 6887, Loss: 0.9373586177825928, Accuracy: 0.96875, Computation time: 1.0271804332733154\n",
      "Step: 6888, Loss: 0.9159511923789978, Accuracy: 1.0, Computation time: 1.1948094367980957\n",
      "Step: 6889, Loss: 0.9173347353935242, Accuracy: 1.0, Computation time: 1.8023874759674072\n",
      "Step: 6890, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.2794153690338135\n",
      "Step: 6891, Loss: 0.9159683585166931, Accuracy: 1.0, Computation time: 1.1265404224395752\n",
      "Step: 6892, Loss: 0.916361391544342, Accuracy: 1.0, Computation time: 0.9525165557861328\n",
      "Step: 6893, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.3274049758911133\n",
      "Step: 6894, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.3054709434509277\n",
      "Step: 6895, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.0573670864105225\n",
      "Step: 6896, Loss: 0.9276113510131836, Accuracy: 0.96875, Computation time: 1.1193923950195312\n",
      "Step: 6897, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 0.9476258754730225\n",
      "Step: 6898, Loss: 0.915920615196228, Accuracy: 1.0, Computation time: 1.0574254989624023\n",
      "Step: 6899, Loss: 0.9159594774246216, Accuracy: 1.0, Computation time: 1.4021413326263428\n",
      "Step: 6900, Loss: 0.9159895777702332, Accuracy: 1.0, Computation time: 1.3385677337646484\n",
      "Step: 6901, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 1.3838930130004883\n",
      "Step: 6902, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 1.1088433265686035\n",
      "Step: 6903, Loss: 0.9161034822463989, Accuracy: 1.0, Computation time: 1.0172617435455322\n",
      "Step: 6904, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.0533525943756104\n",
      "Step: 6905, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 1.337892770767212\n",
      "Step: 6906, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 0.9964306354522705\n",
      "Step: 6907, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.0017130374908447\n",
      "Step: 6908, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.11863112449646\n",
      "Step: 6909, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.1236698627471924\n",
      "Step: 6910, Loss: 0.9179109930992126, Accuracy: 1.0, Computation time: 1.3053898811340332\n",
      "Step: 6911, Loss: 0.915917694568634, Accuracy: 1.0, Computation time: 1.3513116836547852\n",
      "Step: 6912, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 0.9359390735626221\n",
      "Step: 6913, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 0.8753666877746582\n",
      "Step: 6914, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.1028954982757568\n",
      "Step: 6915, Loss: 0.9374412894248962, Accuracy: 0.96875, Computation time: 1.011768102645874\n",
      "Step: 6916, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 0.9369680881500244\n",
      "Step: 6917, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.1623191833496094\n",
      "Step: 6918, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 1.0024220943450928\n",
      "Step: 6919, Loss: 0.9367890357971191, Accuracy: 0.96875, Computation time: 1.4281926155090332\n",
      "Step: 6920, Loss: 0.9158401489257812, Accuracy: 1.0, Computation time: 0.9754188060760498\n",
      "Step: 6921, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.2555618286132812\n",
      "Step: 6922, Loss: 0.9375309348106384, Accuracy: 0.96875, Computation time: 1.1632580757141113\n",
      "Step: 6923, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 1.3133692741394043\n",
      "Step: 6924, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.7955613136291504\n",
      "Step: 6925, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 0.8976502418518066\n",
      "Step: 6926, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.020481824874878\n",
      "Step: 6927, Loss: 0.9375622272491455, Accuracy: 0.96875, Computation time: 1.0401034355163574\n",
      "Step: 6928, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.1506314277648926\n",
      "Step: 6929, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.0249898433685303\n",
      "Step: 6930, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 0.8082189559936523\n",
      "Step: 6931, Loss: 0.9163356423377991, Accuracy: 1.0, Computation time: 1.0306742191314697\n",
      "Step: 6932, Loss: 0.9162456393241882, Accuracy: 1.0, Computation time: 1.4396882057189941\n",
      "Step: 6933, Loss: 0.9372470378875732, Accuracy: 0.96875, Computation time: 1.1575405597686768\n",
      "Step: 6934, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 0.8977172374725342\n",
      "Step: 6935, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 0.8836257457733154\n",
      "Step: 6936, Loss: 0.9159510731697083, Accuracy: 1.0, Computation time: 0.9244611263275146\n",
      "Step: 6937, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 0.9195296764373779\n",
      "Step: 6938, Loss: 0.9172203540802002, Accuracy: 1.0, Computation time: 0.9097704887390137\n",
      "Step: 6939, Loss: 0.9160001873970032, Accuracy: 1.0, Computation time: 1.2521319389343262\n",
      "Step: 6940, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.035646915435791\n",
      "Step: 6941, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.178128957748413\n",
      "Step: 6942, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.091041088104248\n",
      "Step: 6943, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 1.2113163471221924\n",
      "Step: 6944, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.1682465076446533\n",
      "Step: 6945, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.1125893592834473\n",
      "Step: 6946, Loss: 0.937553346157074, Accuracy: 0.96875, Computation time: 1.0965404510498047\n",
      "Step: 6947, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.2637646198272705\n",
      "########################\n",
      "Test loss: 1.1241086721420288, Test Accuracy_epoch50: 0.6994359493255615\n",
      "########################\n",
      "Step: 6948, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 0.8524906635284424\n",
      "Step: 6949, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 0.9294004440307617\n",
      "Step: 6950, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.2325878143310547\n",
      "Step: 6951, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.0383896827697754\n",
      "Step: 6952, Loss: 0.9159477949142456, Accuracy: 1.0, Computation time: 1.1477761268615723\n",
      "Step: 6953, Loss: 0.916366457939148, Accuracy: 1.0, Computation time: 0.9399409294128418\n",
      "Step: 6954, Loss: 0.9267892837524414, Accuracy: 0.96875, Computation time: 1.5654003620147705\n",
      "Step: 6955, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.178421974182129\n",
      "Step: 6956, Loss: 0.9160371422767639, Accuracy: 1.0, Computation time: 1.2467386722564697\n",
      "Step: 6957, Loss: 0.9266077876091003, Accuracy: 0.96875, Computation time: 1.3938195705413818\n",
      "Step: 6958, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 0.8306729793548584\n",
      "Step: 6959, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 0.8911056518554688\n",
      "Step: 6960, Loss: 0.9207875728607178, Accuracy: 1.0, Computation time: 1.5773816108703613\n",
      "Step: 6961, Loss: 0.9376740455627441, Accuracy: 0.96875, Computation time: 1.0660209655761719\n",
      "Step: 6962, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 1.227658987045288\n",
      "Step: 6963, Loss: 0.91590815782547, Accuracy: 1.0, Computation time: 0.9700222015380859\n",
      "Step: 6964, Loss: 0.9166690707206726, Accuracy: 1.0, Computation time: 0.962897539138794\n",
      "Step: 6965, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.161264181137085\n",
      "Step: 6966, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.2873823642730713\n",
      "Step: 6967, Loss: 0.9375820159912109, Accuracy: 0.96875, Computation time: 1.0282762050628662\n",
      "Step: 6968, Loss: 0.9160630702972412, Accuracy: 1.0, Computation time: 1.2756671905517578\n",
      "Step: 6969, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 0.9508087635040283\n",
      "Step: 6970, Loss: 0.937590479850769, Accuracy: 0.96875, Computation time: 1.1612260341644287\n",
      "Step: 6971, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 1.2164134979248047\n",
      "Step: 6972, Loss: 0.9377446174621582, Accuracy: 0.96875, Computation time: 0.9036300182342529\n",
      "Step: 6973, Loss: 0.915925920009613, Accuracy: 1.0, Computation time: 0.9474742412567139\n",
      "Step: 6974, Loss: 0.9167064428329468, Accuracy: 1.0, Computation time: 1.2012147903442383\n",
      "Step: 6975, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.019749641418457\n",
      "Step: 6976, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 0.8948149681091309\n",
      "Step: 6977, Loss: 0.9164310693740845, Accuracy: 1.0, Computation time: 1.1519300937652588\n",
      "Step: 6978, Loss: 0.9159100651741028, Accuracy: 1.0, Computation time: 0.9499292373657227\n",
      "Step: 6979, Loss: 0.916172981262207, Accuracy: 1.0, Computation time: 1.2509815692901611\n",
      "Step: 6980, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 0.9616296291351318\n",
      "Step: 6981, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 0.8934159278869629\n",
      "Step: 6982, Loss: 0.9158948063850403, Accuracy: 1.0, Computation time: 1.1567606925964355\n",
      "Step: 6983, Loss: 0.9375041127204895, Accuracy: 0.96875, Computation time: 1.3680157661437988\n",
      "Step: 6984, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.0770516395568848\n",
      "Step: 6985, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 0.872779369354248\n",
      "Step: 6986, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.2549726963043213\n",
      "Step: 6987, Loss: 0.9328670501708984, Accuracy: 0.96875, Computation time: 1.655416488647461\n",
      "Step: 6988, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 0.9702138900756836\n",
      "Step: 6989, Loss: 0.9163195490837097, Accuracy: 1.0, Computation time: 1.4588642120361328\n",
      "Step: 6990, Loss: 0.9161359071731567, Accuracy: 1.0, Computation time: 1.061262845993042\n",
      "Step: 6991, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.1059625148773193\n",
      "Step: 6992, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.0395019054412842\n",
      "Step: 6993, Loss: 0.9159577488899231, Accuracy: 1.0, Computation time: 0.8805477619171143\n",
      "Step: 6994, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 0.8197722434997559\n",
      "Step: 6995, Loss: 0.9160946607589722, Accuracy: 1.0, Computation time: 0.9920156002044678\n",
      "Step: 6996, Loss: 0.9376562237739563, Accuracy: 0.96875, Computation time: 1.007476806640625\n",
      "Step: 6997, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 0.827167272567749\n",
      "Step: 6998, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 1.1617488861083984\n",
      "Step: 6999, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.029768943786621\n",
      "Step: 7000, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.3216824531555176\n",
      "Step: 7001, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.0066909790039062\n",
      "Step: 7002, Loss: 0.9159412980079651, Accuracy: 1.0, Computation time: 1.1726503372192383\n",
      "Step: 7003, Loss: 0.9363753795623779, Accuracy: 0.96875, Computation time: 1.1298975944519043\n",
      "Step: 7004, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.0729753971099854\n",
      "Step: 7005, Loss: 0.9374231100082397, Accuracy: 0.96875, Computation time: 1.4107749462127686\n",
      "Step: 7006, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 0.8999135494232178\n",
      "Step: 7007, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 0.9531066417694092\n",
      "Step: 7008, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 1.1345798969268799\n",
      "Step: 7009, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 0.9892034530639648\n",
      "Step: 7010, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.5354468822479248\n",
      "Step: 7011, Loss: 0.916202187538147, Accuracy: 1.0, Computation time: 1.2011902332305908\n",
      "Step: 7012, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.2937328815460205\n",
      "Step: 7013, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 0.940070629119873\n",
      "Step: 7014, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 0.984548807144165\n",
      "Step: 7015, Loss: 0.9374752044677734, Accuracy: 0.96875, Computation time: 0.8329143524169922\n",
      "Step: 7016, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 0.913395881652832\n",
      "Step: 7017, Loss: 0.9593517780303955, Accuracy: 0.9375, Computation time: 1.5043389797210693\n",
      "Step: 7018, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 1.0139727592468262\n",
      "Step: 7019, Loss: 0.9160673022270203, Accuracy: 1.0, Computation time: 1.1397900581359863\n",
      "Step: 7020, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 0.9018797874450684\n",
      "Step: 7021, Loss: 0.9172264933586121, Accuracy: 1.0, Computation time: 1.1541717052459717\n",
      "Step: 7022, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 0.9297511577606201\n",
      "Step: 7023, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 0.9400463104248047\n",
      "Step: 7024, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 0.8676354885101318\n",
      "Step: 7025, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.222247838973999\n",
      "Step: 7026, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 1.1832818984985352\n",
      "Step: 7027, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 1.0595290660858154\n",
      "Step: 7028, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 0.9985885620117188\n",
      "Step: 7029, Loss: 0.9168280959129333, Accuracy: 1.0, Computation time: 0.9743819236755371\n",
      "Step: 7030, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 0.8855869770050049\n",
      "Step: 7031, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 1.046034574508667\n",
      "Step: 7032, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 1.447793960571289\n",
      "Step: 7033, Loss: 0.9352007508277893, Accuracy: 0.96875, Computation time: 2.179720401763916\n",
      "Step: 7034, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 0.9613368511199951\n",
      "Step: 7035, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.1014180183410645\n",
      "Step: 7036, Loss: 0.9164701104164124, Accuracy: 1.0, Computation time: 0.9840774536132812\n",
      "Step: 7037, Loss: 0.9364425539970398, Accuracy: 0.96875, Computation time: 0.8682126998901367\n",
      "Step: 7038, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 0.9616591930389404\n",
      "Step: 7039, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 0.847614049911499\n",
      "Step: 7040, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 0.9130384922027588\n",
      "Step: 7041, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 0.9351787567138672\n",
      "Step: 7042, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 0.8239686489105225\n",
      "Step: 7043, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.1718478202819824\n",
      "Step: 7044, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.089245319366455\n",
      "Step: 7045, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 0.9076480865478516\n",
      "Step: 7046, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.041844367980957\n",
      "Step: 7047, Loss: 0.9159946441650391, Accuracy: 1.0, Computation time: 1.449110984802246\n",
      "Step: 7048, Loss: 0.9158390164375305, Accuracy: 1.0, Computation time: 1.1984074115753174\n",
      "Step: 7049, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 0.9349477291107178\n",
      "Step: 7050, Loss: 0.934920072555542, Accuracy: 0.96875, Computation time: 1.3981068134307861\n",
      "Step: 7051, Loss: 0.9374544620513916, Accuracy: 0.96875, Computation time: 1.0075504779815674\n",
      "Step: 7052, Loss: 0.9184845089912415, Accuracy: 1.0, Computation time: 1.2781078815460205\n",
      "Step: 7053, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.1947836875915527\n",
      "Step: 7054, Loss: 0.9364758729934692, Accuracy: 0.96875, Computation time: 1.1908020973205566\n",
      "Step: 7055, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.2230312824249268\n",
      "Step: 7056, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 0.8192412853240967\n",
      "Step: 7057, Loss: 0.9375417828559875, Accuracy: 0.96875, Computation time: 1.131725549697876\n",
      "Step: 7058, Loss: 0.9162085652351379, Accuracy: 1.0, Computation time: 1.3392703533172607\n",
      "Step: 7059, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.1563739776611328\n",
      "Step: 7060, Loss: 0.9162135720252991, Accuracy: 1.0, Computation time: 0.8994724750518799\n",
      "Step: 7061, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 0.8802580833435059\n",
      "Step: 7062, Loss: 0.917065441608429, Accuracy: 1.0, Computation time: 1.4421403408050537\n",
      "Step: 7063, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.0455961227416992\n",
      "Step: 7064, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 0.9431591033935547\n",
      "Step: 7065, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 0.9694099426269531\n",
      "Step: 7066, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 0.9281594753265381\n",
      "Step: 7067, Loss: 0.9374979734420776, Accuracy: 0.96875, Computation time: 1.145125389099121\n",
      "Step: 7068, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.0115652084350586\n",
      "Step: 7069, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.2721161842346191\n",
      "Step: 7070, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 0.966437578201294\n",
      "Step: 7071, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 0.9691295623779297\n",
      "Step: 7072, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 0.9551293849945068\n",
      "Step: 7073, Loss: 0.9159852266311646, Accuracy: 1.0, Computation time: 1.0946288108825684\n",
      "Step: 7074, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 0.9345376491546631\n",
      "Step: 7075, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 0.9104762077331543\n",
      "Step: 7076, Loss: 0.9228148460388184, Accuracy: 1.0, Computation time: 1.5536792278289795\n",
      "Step: 7077, Loss: 0.9372487664222717, Accuracy: 0.96875, Computation time: 0.980053186416626\n",
      "Step: 7078, Loss: 0.937580943107605, Accuracy: 0.96875, Computation time: 1.6749866008758545\n",
      "Step: 7079, Loss: 0.9160862565040588, Accuracy: 1.0, Computation time: 1.4313621520996094\n",
      "Step: 7080, Loss: 0.9159542322158813, Accuracy: 1.0, Computation time: 1.0025665760040283\n",
      "Step: 7081, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 0.9669389724731445\n",
      "Step: 7082, Loss: 0.9159602522850037, Accuracy: 1.0, Computation time: 1.089101791381836\n",
      "Step: 7083, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.1339993476867676\n",
      "Step: 7084, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.118468999862671\n",
      "Step: 7085, Loss: 0.9158384799957275, Accuracy: 1.0, Computation time: 1.0452985763549805\n",
      "Test loss: 1.1263257265090942, Test Accuracy: 0.6921837329864502\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6ca08-4360-4d7b-b294-12dab499f1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
