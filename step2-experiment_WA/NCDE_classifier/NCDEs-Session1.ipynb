{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7955e4a3-b704-4077-9383-884fdc18d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7357cc7b-5041-46df-ae9b-e7a918331e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c19f25-e914-4d92-8319-2123ec714272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4446, 256, 768) (4446,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last2, wav2vec_last3, wav2vec_last4, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last2,label_last3,label_last4,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34500eb2-dcfd-4da7-b69d-fe201c34d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2d8459-5146-4a3b-824e-042acfcb65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f08cd37-6fa1-42fa-bca1-173bbd70d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        #先将数据流降维再放入模型中训练\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0)  \n",
    "            pred_var=prediction.var(axis=0)   \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)    \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae2ed2a-da32-4f23-a5ba-06d2717cac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c05e2d9c-a3eb-4ddf-9b15-dc92e77615fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last1], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last1\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33fc7a49-1a46-4ecc-8ee0-da4dfc3b399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58961b2b-1a70-4639-b9d4-4dabfe45d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97111cc9-7aab-4e16-b2f1-4395681bf4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4446,\n",
    "    dataset_test_size=1085,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=3123,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        total_acc = 0\n",
    "        total_num = total_size\n",
    "        predicted_class = jnp.argmax(pred, axis=1)\n",
    "        total_acc += jnp.sum(predicted_class == label_i)\n",
    "        return total_acc / total_num\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "715c386c-faa1-431f-857d-9b8285568997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.3574368953704834, Accuracy: 0.40625, Computation time: 13.672141075134277\n",
      "Step: 1, Loss: 1.389399766921997, Accuracy: 0.3125, Computation time: 2.548259735107422\n",
      "Step: 2, Loss: 1.4624747037887573, Accuracy: 0.21875, Computation time: 3.6503207683563232\n",
      "Step: 3, Loss: 1.4107763767242432, Accuracy: 0.28125, Computation time: 2.8645737171173096\n",
      "Step: 4, Loss: 1.36627995967865, Accuracy: 0.34375, Computation time: 3.180358648300171\n",
      "Step: 5, Loss: 1.3559569120407104, Accuracy: 0.34375, Computation time: 2.2108097076416016\n",
      "Step: 6, Loss: 1.3461146354675293, Accuracy: 0.3125, Computation time: 2.9865097999572754\n",
      "Step: 7, Loss: 1.3569376468658447, Accuracy: 0.28125, Computation time: 3.172100782394409\n",
      "Step: 8, Loss: 1.426077127456665, Accuracy: 0.34375, Computation time: 2.770467758178711\n",
      "Step: 9, Loss: 1.397111415863037, Accuracy: 0.21875, Computation time: 3.54663348197937\n",
      "Step: 10, Loss: 1.3997483253479004, Accuracy: 0.1875, Computation time: 2.47177791595459\n",
      "Step: 11, Loss: 1.3190815448760986, Accuracy: 0.59375, Computation time: 2.9958319664001465\n",
      "Step: 12, Loss: 1.3471108675003052, Accuracy: 0.46875, Computation time: 2.212134599685669\n",
      "Step: 13, Loss: 1.2859625816345215, Accuracy: 0.625, Computation time: 2.9117815494537354\n",
      "Step: 14, Loss: 1.2731837034225464, Accuracy: 0.5625, Computation time: 2.495330572128296\n",
      "Step: 15, Loss: 1.2934978008270264, Accuracy: 0.46875, Computation time: 2.7129697799682617\n",
      "Step: 16, Loss: 1.2582383155822754, Accuracy: 0.53125, Computation time: 3.5178520679473877\n",
      "Step: 17, Loss: 1.196176528930664, Accuracy: 0.5625, Computation time: 2.7863106727600098\n",
      "Step: 18, Loss: 1.1600611209869385, Accuracy: 0.71875, Computation time: 3.105471611022949\n",
      "Step: 19, Loss: 1.1204224824905396, Accuracy: 0.8125, Computation time: 2.882967948913574\n",
      "Step: 20, Loss: 1.1007070541381836, Accuracy: 0.84375, Computation time: 2.5379767417907715\n",
      "Step: 21, Loss: 1.0973775386810303, Accuracy: 0.78125, Computation time: 2.939206123352051\n",
      "Step: 22, Loss: 1.0832915306091309, Accuracy: 0.8125, Computation time: 3.1119821071624756\n",
      "Step: 23, Loss: 1.121735692024231, Accuracy: 0.75, Computation time: 3.0463006496429443\n",
      "Step: 24, Loss: 1.1034296751022339, Accuracy: 0.78125, Computation time: 3.626267910003662\n",
      "Step: 25, Loss: 1.08480703830719, Accuracy: 0.78125, Computation time: 2.957852602005005\n",
      "Step: 26, Loss: 1.0598869323730469, Accuracy: 0.84375, Computation time: 3.3242604732513428\n",
      "Step: 27, Loss: 1.0585829019546509, Accuracy: 0.84375, Computation time: 2.6994435787200928\n",
      "Step: 28, Loss: 1.0148823261260986, Accuracy: 0.84375, Computation time: 2.5697121620178223\n",
      "Step: 29, Loss: 1.0023020505905151, Accuracy: 1.0, Computation time: 3.2489142417907715\n",
      "Step: 30, Loss: 0.9983096718788147, Accuracy: 0.96875, Computation time: 2.9906115531921387\n",
      "Step: 31, Loss: 0.9507766962051392, Accuracy: 1.0, Computation time: 2.9459869861602783\n",
      "Step: 32, Loss: 0.9735416173934937, Accuracy: 1.0, Computation time: 2.792785167694092\n",
      "Step: 33, Loss: 0.9631102085113525, Accuracy: 1.0, Computation time: 2.7075817584991455\n",
      "Step: 34, Loss: 0.9804647564888, Accuracy: 0.96875, Computation time: 2.3994879722595215\n",
      "Step: 35, Loss: 0.9793194532394409, Accuracy: 0.96875, Computation time: 2.3774044513702393\n",
      "Step: 36, Loss: 1.0034046173095703, Accuracy: 0.9375, Computation time: 2.31856632232666\n",
      "Step: 37, Loss: 0.9381362199783325, Accuracy: 1.0, Computation time: 2.660848379135132\n",
      "Step: 38, Loss: 0.9657726883888245, Accuracy: 0.96875, Computation time: 3.0309338569641113\n",
      "Step: 39, Loss: 0.9417976140975952, Accuracy: 1.0, Computation time: 2.6800637245178223\n",
      "Step: 40, Loss: 0.9360401034355164, Accuracy: 1.0, Computation time: 2.6285696029663086\n",
      "Step: 41, Loss: 0.9404844045639038, Accuracy: 1.0, Computation time: 2.4020285606384277\n",
      "Step: 42, Loss: 0.9825052618980408, Accuracy: 0.9375, Computation time: 2.6327953338623047\n",
      "Step: 43, Loss: 0.9328851103782654, Accuracy: 1.0, Computation time: 2.331033945083618\n",
      "Step: 44, Loss: 0.9350407123565674, Accuracy: 0.96875, Computation time: 2.6749231815338135\n",
      "Step: 45, Loss: 0.943236768245697, Accuracy: 0.96875, Computation time: 2.102386474609375\n",
      "Step: 46, Loss: 0.92330002784729, Accuracy: 1.0, Computation time: 2.782860517501831\n",
      "Step: 47, Loss: 0.9191298484802246, Accuracy: 1.0, Computation time: 2.1746766567230225\n",
      "Step: 48, Loss: 0.9221315979957581, Accuracy: 1.0, Computation time: 2.383350133895874\n",
      "Step: 49, Loss: 0.9196119904518127, Accuracy: 1.0, Computation time: 2.241971731185913\n",
      "Step: 50, Loss: 0.9200030565261841, Accuracy: 1.0, Computation time: 2.6814560890197754\n",
      "Step: 51, Loss: 0.9202761650085449, Accuracy: 1.0, Computation time: 2.4543240070343018\n",
      "Step: 52, Loss: 0.942375898361206, Accuracy: 0.96875, Computation time: 2.1915504932403564\n",
      "Step: 53, Loss: 0.9201856851577759, Accuracy: 1.0, Computation time: 2.157256841659546\n",
      "Step: 54, Loss: 0.9454914927482605, Accuracy: 0.96875, Computation time: 2.80639386177063\n",
      "Step: 55, Loss: 0.9417743682861328, Accuracy: 0.96875, Computation time: 2.8388636112213135\n",
      "Step: 56, Loss: 0.9366319179534912, Accuracy: 0.96875, Computation time: 2.1761393547058105\n",
      "Step: 57, Loss: 0.9340515732765198, Accuracy: 0.96875, Computation time: 2.7971394062042236\n",
      "Step: 58, Loss: 0.9379502534866333, Accuracy: 0.96875, Computation time: 2.936041831970215\n",
      "Step: 59, Loss: 0.9216283559799194, Accuracy: 1.0, Computation time: 2.323875904083252\n",
      "Step: 60, Loss: 0.9280642867088318, Accuracy: 1.0, Computation time: 2.6347227096557617\n",
      "Step: 61, Loss: 0.920509934425354, Accuracy: 1.0, Computation time: 2.303611993789673\n",
      "Step: 62, Loss: 0.9208253622055054, Accuracy: 1.0, Computation time: 2.466770648956299\n",
      "Step: 63, Loss: 0.9195684194564819, Accuracy: 1.0, Computation time: 2.2917263507843018\n",
      "Step: 64, Loss: 0.9377004504203796, Accuracy: 0.96875, Computation time: 2.6190590858459473\n",
      "Step: 65, Loss: 0.9315805435180664, Accuracy: 0.96875, Computation time: 2.2951831817626953\n",
      "Step: 66, Loss: 0.9177493453025818, Accuracy: 1.0, Computation time: 2.5737717151641846\n",
      "Step: 67, Loss: 0.9241523146629333, Accuracy: 1.0, Computation time: 2.3567519187927246\n",
      "Step: 68, Loss: 0.9211273193359375, Accuracy: 1.0, Computation time: 2.1664047241210938\n",
      "Step: 69, Loss: 0.9192118644714355, Accuracy: 1.0, Computation time: 2.706707239151001\n",
      "Step: 70, Loss: 0.9381060600280762, Accuracy: 0.96875, Computation time: 2.5026986598968506\n",
      "Step: 71, Loss: 0.9238969087600708, Accuracy: 1.0, Computation time: 2.6196584701538086\n",
      "Step: 72, Loss: 0.9416359066963196, Accuracy: 0.96875, Computation time: 2.2367706298828125\n",
      "Step: 73, Loss: 0.9408447742462158, Accuracy: 0.96875, Computation time: 2.1310572624206543\n",
      "Step: 74, Loss: 0.9210770130157471, Accuracy: 1.0, Computation time: 2.3662257194519043\n",
      "Step: 75, Loss: 0.9232504367828369, Accuracy: 1.0, Computation time: 2.325167417526245\n",
      "Step: 76, Loss: 0.9480641484260559, Accuracy: 0.9375, Computation time: 2.957008123397827\n",
      "Step: 77, Loss: 0.9178540706634521, Accuracy: 1.0, Computation time: 2.3317666053771973\n",
      "Step: 78, Loss: 0.9300370812416077, Accuracy: 0.96875, Computation time: 2.171895742416382\n",
      "Step: 79, Loss: 0.9183946251869202, Accuracy: 1.0, Computation time: 2.2897918224334717\n",
      "Step: 80, Loss: 0.9426757097244263, Accuracy: 0.96875, Computation time: 2.7225213050842285\n",
      "Step: 81, Loss: 0.918839156627655, Accuracy: 1.0, Computation time: 2.4206652641296387\n",
      "Step: 82, Loss: 0.9200361967086792, Accuracy: 1.0, Computation time: 1.8839225769042969\n",
      "Step: 83, Loss: 0.9185929894447327, Accuracy: 1.0, Computation time: 2.3280768394470215\n",
      "Step: 84, Loss: 0.9342225790023804, Accuracy: 0.96875, Computation time: 2.147965431213379\n",
      "Step: 85, Loss: 0.917555034160614, Accuracy: 1.0, Computation time: 2.2087013721466064\n",
      "Step: 86, Loss: 0.9187864065170288, Accuracy: 1.0, Computation time: 1.9572014808654785\n",
      "Step: 87, Loss: 0.9180236458778381, Accuracy: 1.0, Computation time: 2.4577348232269287\n",
      "Step: 88, Loss: 0.9173084497451782, Accuracy: 1.0, Computation time: 2.170642614364624\n",
      "Step: 89, Loss: 0.9172757267951965, Accuracy: 1.0, Computation time: 1.8743581771850586\n",
      "Step: 90, Loss: 0.9213628768920898, Accuracy: 1.0, Computation time: 2.2535815238952637\n",
      "Step: 91, Loss: 0.9225666522979736, Accuracy: 1.0, Computation time: 2.7523045539855957\n",
      "Step: 92, Loss: 0.9177590608596802, Accuracy: 1.0, Computation time: 2.391374111175537\n",
      "Step: 93, Loss: 0.9349545240402222, Accuracy: 0.96875, Computation time: 2.71930193901062\n",
      "Step: 94, Loss: 0.9188663363456726, Accuracy: 1.0, Computation time: 2.8502399921417236\n",
      "Step: 95, Loss: 0.9179089665412903, Accuracy: 1.0, Computation time: 2.3190901279449463\n",
      "Step: 96, Loss: 0.9391458034515381, Accuracy: 0.96875, Computation time: 2.3597936630249023\n",
      "Step: 97, Loss: 0.917096734046936, Accuracy: 1.0, Computation time: 2.5067875385284424\n",
      "Step: 98, Loss: 0.917107343673706, Accuracy: 1.0, Computation time: 2.2077531814575195\n",
      "Step: 99, Loss: 0.9169571399688721, Accuracy: 1.0, Computation time: 2.677459716796875\n",
      "Step: 100, Loss: 0.9170451760292053, Accuracy: 1.0, Computation time: 1.9665801525115967\n",
      "Step: 101, Loss: 0.9177089929580688, Accuracy: 1.0, Computation time: 2.5400583744049072\n",
      "Step: 102, Loss: 0.9174951910972595, Accuracy: 1.0, Computation time: 2.257561206817627\n",
      "Step: 103, Loss: 0.9193900227546692, Accuracy: 1.0, Computation time: 2.364856243133545\n",
      "Step: 104, Loss: 0.9165539741516113, Accuracy: 1.0, Computation time: 1.8883905410766602\n",
      "Step: 105, Loss: 0.9172396659851074, Accuracy: 1.0, Computation time: 2.069023609161377\n",
      "Step: 106, Loss: 0.9171141386032104, Accuracy: 1.0, Computation time: 2.6881840229034424\n",
      "Step: 107, Loss: 0.9359240531921387, Accuracy: 0.96875, Computation time: 2.0026021003723145\n",
      "Step: 108, Loss: 0.9171351790428162, Accuracy: 1.0, Computation time: 2.8561179637908936\n",
      "Step: 109, Loss: 0.9166303873062134, Accuracy: 1.0, Computation time: 2.4523162841796875\n",
      "Step: 110, Loss: 0.9334380626678467, Accuracy: 0.96875, Computation time: 2.957874059677124\n",
      "Step: 111, Loss: 0.9184324145317078, Accuracy: 1.0, Computation time: 2.6614387035369873\n",
      "Step: 112, Loss: 0.9238434433937073, Accuracy: 1.0, Computation time: 2.0306155681610107\n",
      "Step: 113, Loss: 0.9168034791946411, Accuracy: 1.0, Computation time: 2.411238431930542\n",
      "Step: 114, Loss: 0.9165011644363403, Accuracy: 1.0, Computation time: 2.3061840534210205\n",
      "Step: 115, Loss: 0.9337019920349121, Accuracy: 0.96875, Computation time: 2.514004707336426\n",
      "Step: 116, Loss: 0.9180662631988525, Accuracy: 1.0, Computation time: 2.145160675048828\n",
      "Step: 117, Loss: 0.9174877405166626, Accuracy: 1.0, Computation time: 2.682011842727661\n",
      "Step: 118, Loss: 0.9248589277267456, Accuracy: 1.0, Computation time: 2.4455490112304688\n",
      "Step: 119, Loss: 0.9176241159439087, Accuracy: 1.0, Computation time: 2.334176778793335\n",
      "Step: 120, Loss: 0.9176480174064636, Accuracy: 1.0, Computation time: 1.969904899597168\n",
      "Step: 121, Loss: 0.9170194864273071, Accuracy: 1.0, Computation time: 2.3959662914276123\n",
      "Step: 122, Loss: 0.917956531047821, Accuracy: 1.0, Computation time: 2.2646822929382324\n",
      "Step: 123, Loss: 0.9168112277984619, Accuracy: 1.0, Computation time: 2.3061037063598633\n",
      "Step: 124, Loss: 0.9391181468963623, Accuracy: 0.96875, Computation time: 2.2646453380584717\n",
      "Step: 125, Loss: 0.9209602475166321, Accuracy: 1.0, Computation time: 2.936534881591797\n",
      "Step: 126, Loss: 0.936671257019043, Accuracy: 0.96875, Computation time: 2.0890471935272217\n",
      "Step: 127, Loss: 0.9163170456886292, Accuracy: 1.0, Computation time: 2.566706895828247\n",
      "Step: 128, Loss: 0.9244007468223572, Accuracy: 1.0, Computation time: 2.105778217315674\n",
      "Step: 129, Loss: 0.9192389845848083, Accuracy: 1.0, Computation time: 2.427783966064453\n",
      "Step: 130, Loss: 0.9168672561645508, Accuracy: 1.0, Computation time: 2.2244086265563965\n",
      "Step: 131, Loss: 0.9167339205741882, Accuracy: 1.0, Computation time: 2.58416485786438\n",
      "Step: 132, Loss: 0.933431088924408, Accuracy: 0.96875, Computation time: 2.214681386947632\n",
      "Step: 133, Loss: 0.917747974395752, Accuracy: 1.0, Computation time: 2.4447531700134277\n",
      "Step: 134, Loss: 0.9169660806655884, Accuracy: 1.0, Computation time: 2.166877269744873\n",
      "Step: 135, Loss: 0.9165505766868591, Accuracy: 1.0, Computation time: 2.8311655521392822\n",
      "Step: 136, Loss: 0.9408872127532959, Accuracy: 0.96875, Computation time: 2.121443271636963\n",
      "Step: 137, Loss: 0.9176300764083862, Accuracy: 1.0, Computation time: 2.642239570617676\n",
      "Step: 138, Loss: 0.9163408279418945, Accuracy: 1.0, Computation time: 2.5577874183654785\n",
      "Step: 139, Loss: 0.9371657371520996, Accuracy: 0.96875, Computation time: 2.2039072513580322\n",
      "########################\n",
      "Test loss: 1.1176577806472778, Test Accuracy_epoch1: 0.7069124579429626\n",
      "########################\n",
      "Step: 140, Loss: 0.9389593601226807, Accuracy: 0.96875, Computation time: 2.624053716659546\n",
      "Step: 141, Loss: 0.9174026250839233, Accuracy: 1.0, Computation time: 2.552791118621826\n",
      "Step: 142, Loss: 0.916236162185669, Accuracy: 1.0, Computation time: 2.8789522647857666\n",
      "Step: 143, Loss: 0.917566180229187, Accuracy: 1.0, Computation time: 2.5768754482269287\n",
      "Step: 144, Loss: 0.9167314171791077, Accuracy: 1.0, Computation time: 2.0013506412506104\n",
      "Step: 145, Loss: 0.9189633727073669, Accuracy: 1.0, Computation time: 2.378129243850708\n",
      "Step: 146, Loss: 0.9167531728744507, Accuracy: 1.0, Computation time: 2.276531934738159\n",
      "Step: 147, Loss: 0.9282270073890686, Accuracy: 1.0, Computation time: 2.326322317123413\n",
      "Step: 148, Loss: 0.9163686037063599, Accuracy: 1.0, Computation time: 2.4227635860443115\n",
      "Step: 149, Loss: 0.9171298742294312, Accuracy: 1.0, Computation time: 2.7949178218841553\n",
      "Step: 150, Loss: 0.9167207479476929, Accuracy: 1.0, Computation time: 2.6649422645568848\n",
      "Step: 151, Loss: 0.9164210557937622, Accuracy: 1.0, Computation time: 2.028761625289917\n",
      "Step: 152, Loss: 0.9212549924850464, Accuracy: 1.0, Computation time: 2.8482296466827393\n",
      "Step: 153, Loss: 0.9181985259056091, Accuracy: 1.0, Computation time: 2.4119679927825928\n",
      "Step: 154, Loss: 0.9181668162345886, Accuracy: 1.0, Computation time: 2.221107244491577\n",
      "Step: 155, Loss: 0.9396467208862305, Accuracy: 0.96875, Computation time: 2.6021971702575684\n",
      "Step: 156, Loss: 0.916278600692749, Accuracy: 1.0, Computation time: 2.8165934085845947\n",
      "Step: 157, Loss: 0.9360233545303345, Accuracy: 0.96875, Computation time: 2.6557095050811768\n",
      "Step: 158, Loss: 0.9270447492599487, Accuracy: 0.96875, Computation time: 3.0834262371063232\n",
      "Step: 159, Loss: 0.9205166697502136, Accuracy: 1.0, Computation time: 2.509767532348633\n",
      "Step: 160, Loss: 0.9167395234107971, Accuracy: 1.0, Computation time: 2.4479243755340576\n",
      "Step: 161, Loss: 0.916528046131134, Accuracy: 1.0, Computation time: 2.467672348022461\n",
      "Step: 162, Loss: 0.9211193919181824, Accuracy: 1.0, Computation time: 2.4927873611450195\n",
      "Step: 163, Loss: 0.9241288900375366, Accuracy: 1.0, Computation time: 2.628572702407837\n",
      "Step: 164, Loss: 0.9366121888160706, Accuracy: 0.96875, Computation time: 2.61458420753479\n",
      "Step: 165, Loss: 0.9172354340553284, Accuracy: 1.0, Computation time: 1.9153225421905518\n",
      "Step: 166, Loss: 0.916348397731781, Accuracy: 1.0, Computation time: 2.574678897857666\n",
      "Step: 167, Loss: 0.9167129397392273, Accuracy: 1.0, Computation time: 2.0713560581207275\n",
      "Step: 168, Loss: 0.9337769746780396, Accuracy: 0.96875, Computation time: 2.3685529232025146\n",
      "Step: 169, Loss: 0.9167618155479431, Accuracy: 1.0, Computation time: 2.0737011432647705\n",
      "Step: 170, Loss: 0.9295629262924194, Accuracy: 0.96875, Computation time: 2.2388570308685303\n",
      "Step: 171, Loss: 0.937269389629364, Accuracy: 0.96875, Computation time: 2.1552512645721436\n",
      "Step: 172, Loss: 0.9168524742126465, Accuracy: 1.0, Computation time: 2.262547492980957\n",
      "Step: 173, Loss: 0.9162865281105042, Accuracy: 1.0, Computation time: 2.988786458969116\n",
      "Step: 174, Loss: 0.916971743106842, Accuracy: 1.0, Computation time: 2.2920947074890137\n",
      "Step: 175, Loss: 0.9169382452964783, Accuracy: 1.0, Computation time: 2.0127627849578857\n",
      "Step: 176, Loss: 0.9165993332862854, Accuracy: 1.0, Computation time: 2.28320574760437\n",
      "Step: 177, Loss: 0.9174528121948242, Accuracy: 1.0, Computation time: 2.047887086868286\n",
      "Step: 178, Loss: 0.937181293964386, Accuracy: 0.96875, Computation time: 2.10976505279541\n",
      "Step: 179, Loss: 0.9167197942733765, Accuracy: 1.0, Computation time: 2.0438451766967773\n",
      "Step: 180, Loss: 0.916515052318573, Accuracy: 1.0, Computation time: 1.9383809566497803\n",
      "Step: 181, Loss: 0.9171019196510315, Accuracy: 1.0, Computation time: 2.205787181854248\n",
      "Step: 182, Loss: 0.9355571866035461, Accuracy: 0.96875, Computation time: 2.4196348190307617\n",
      "Step: 183, Loss: 0.9165192246437073, Accuracy: 1.0, Computation time: 2.4610986709594727\n",
      "Step: 184, Loss: 0.9180899858474731, Accuracy: 1.0, Computation time: 2.185987710952759\n",
      "Step: 185, Loss: 0.9169225096702576, Accuracy: 1.0, Computation time: 2.200266122817993\n",
      "Step: 186, Loss: 0.9164271354675293, Accuracy: 1.0, Computation time: 2.2116076946258545\n",
      "Step: 187, Loss: 0.924017071723938, Accuracy: 1.0, Computation time: 2.3918344974517822\n",
      "Step: 188, Loss: 0.9174180030822754, Accuracy: 1.0, Computation time: 2.183551788330078\n",
      "Step: 189, Loss: 0.9364453554153442, Accuracy: 0.96875, Computation time: 2.5501317977905273\n",
      "Step: 190, Loss: 0.9168879389762878, Accuracy: 1.0, Computation time: 2.144282102584839\n",
      "Step: 191, Loss: 0.9439743757247925, Accuracy: 0.96875, Computation time: 2.186542272567749\n",
      "Step: 192, Loss: 0.9300046563148499, Accuracy: 0.96875, Computation time: 2.5045695304870605\n",
      "Step: 193, Loss: 0.9167363047599792, Accuracy: 1.0, Computation time: 2.3939783573150635\n",
      "Step: 194, Loss: 0.9163421392440796, Accuracy: 1.0, Computation time: 2.1690571308135986\n",
      "Step: 195, Loss: 0.916846513748169, Accuracy: 1.0, Computation time: 2.3718044757843018\n",
      "Step: 196, Loss: 0.9161088466644287, Accuracy: 1.0, Computation time: 1.8744251728057861\n",
      "Step: 197, Loss: 0.9208664894104004, Accuracy: 1.0, Computation time: 2.24926495552063\n",
      "Step: 198, Loss: 0.9163902997970581, Accuracy: 1.0, Computation time: 2.2265753746032715\n",
      "Step: 199, Loss: 0.9172303080558777, Accuracy: 1.0, Computation time: 2.332695960998535\n",
      "Step: 200, Loss: 0.917860209941864, Accuracy: 1.0, Computation time: 2.220391035079956\n",
      "Step: 201, Loss: 0.916283369064331, Accuracy: 1.0, Computation time: 1.9992003440856934\n",
      "Step: 202, Loss: 0.9163156151771545, Accuracy: 1.0, Computation time: 1.9929232597351074\n",
      "Step: 203, Loss: 0.9355308413505554, Accuracy: 0.96875, Computation time: 2.113548517227173\n",
      "Step: 204, Loss: 0.9160898923873901, Accuracy: 1.0, Computation time: 2.040217876434326\n",
      "Step: 205, Loss: 0.9161195158958435, Accuracy: 1.0, Computation time: 2.384114980697632\n",
      "Step: 206, Loss: 0.9216098785400391, Accuracy: 1.0, Computation time: 2.9555037021636963\n",
      "Step: 207, Loss: 0.9405231475830078, Accuracy: 0.96875, Computation time: 2.334744453430176\n",
      "Step: 208, Loss: 0.9165164828300476, Accuracy: 1.0, Computation time: 2.2007977962493896\n",
      "Step: 209, Loss: 0.9471980333328247, Accuracy: 0.9375, Computation time: 2.207484006881714\n",
      "Step: 210, Loss: 0.9622634649276733, Accuracy: 0.9375, Computation time: 2.169369697570801\n",
      "Step: 211, Loss: 0.9164655804634094, Accuracy: 1.0, Computation time: 2.3472650051116943\n",
      "Step: 212, Loss: 0.9172749519348145, Accuracy: 1.0, Computation time: 2.061413288116455\n",
      "Step: 213, Loss: 0.9380118250846863, Accuracy: 0.96875, Computation time: 2.44785737991333\n",
      "Step: 214, Loss: 0.9394806027412415, Accuracy: 0.96875, Computation time: 2.291861057281494\n",
      "Step: 215, Loss: 0.9185669422149658, Accuracy: 1.0, Computation time: 2.2700119018554688\n",
      "Step: 216, Loss: 0.9203420877456665, Accuracy: 1.0, Computation time: 2.171488046646118\n",
      "Step: 217, Loss: 0.917013943195343, Accuracy: 1.0, Computation time: 2.4195005893707275\n",
      "Step: 218, Loss: 0.9285612106323242, Accuracy: 0.96875, Computation time: 2.5457911491394043\n",
      "Step: 219, Loss: 0.9375213384628296, Accuracy: 0.96875, Computation time: 2.7263925075531006\n",
      "Step: 220, Loss: 0.9171842932701111, Accuracy: 1.0, Computation time: 2.8834786415100098\n",
      "Step: 221, Loss: 0.9166819453239441, Accuracy: 1.0, Computation time: 1.9477930068969727\n",
      "Step: 222, Loss: 0.9175757169723511, Accuracy: 1.0, Computation time: 1.9985926151275635\n",
      "Step: 223, Loss: 0.9169331789016724, Accuracy: 1.0, Computation time: 2.8590455055236816\n",
      "Step: 224, Loss: 0.9163194894790649, Accuracy: 1.0, Computation time: 1.9521491527557373\n",
      "Step: 225, Loss: 0.9182298183441162, Accuracy: 1.0, Computation time: 2.2075982093811035\n",
      "Step: 226, Loss: 0.934334397315979, Accuracy: 0.96875, Computation time: 2.3349764347076416\n",
      "Step: 227, Loss: 0.920669674873352, Accuracy: 1.0, Computation time: 2.9017624855041504\n",
      "Step: 228, Loss: 0.9166897535324097, Accuracy: 1.0, Computation time: 2.615218162536621\n",
      "Step: 229, Loss: 0.9171913862228394, Accuracy: 1.0, Computation time: 2.305197238922119\n",
      "Step: 230, Loss: 0.9191632270812988, Accuracy: 1.0, Computation time: 2.010857343673706\n",
      "Step: 231, Loss: 0.9174598455429077, Accuracy: 1.0, Computation time: 2.536339282989502\n",
      "Step: 232, Loss: 0.9372260570526123, Accuracy: 0.96875, Computation time: 2.497634172439575\n",
      "Step: 233, Loss: 0.9171857833862305, Accuracy: 1.0, Computation time: 2.013856887817383\n",
      "Step: 234, Loss: 0.9165894985198975, Accuracy: 1.0, Computation time: 2.361781120300293\n",
      "Step: 235, Loss: 0.9164093732833862, Accuracy: 1.0, Computation time: 2.237276077270508\n",
      "Step: 236, Loss: 0.9172893166542053, Accuracy: 1.0, Computation time: 2.340116262435913\n",
      "Step: 237, Loss: 0.916490375995636, Accuracy: 1.0, Computation time: 2.519906997680664\n",
      "Step: 238, Loss: 0.9181825518608093, Accuracy: 1.0, Computation time: 2.279287815093994\n",
      "Step: 239, Loss: 0.9243461489677429, Accuracy: 1.0, Computation time: 2.3373429775238037\n",
      "Step: 240, Loss: 0.919060230255127, Accuracy: 1.0, Computation time: 2.3572189807891846\n",
      "Step: 241, Loss: 0.9163699150085449, Accuracy: 1.0, Computation time: 2.1395106315612793\n",
      "Step: 242, Loss: 0.9169169068336487, Accuracy: 1.0, Computation time: 2.1468186378479004\n",
      "Step: 243, Loss: 0.9174882769584656, Accuracy: 1.0, Computation time: 1.9865691661834717\n",
      "Step: 244, Loss: 0.9180852174758911, Accuracy: 1.0, Computation time: 2.2738447189331055\n",
      "Step: 245, Loss: 0.9197322130203247, Accuracy: 1.0, Computation time: 2.463127851486206\n",
      "Step: 246, Loss: 0.920558750629425, Accuracy: 1.0, Computation time: 2.0001697540283203\n",
      "Step: 247, Loss: 0.9167192578315735, Accuracy: 1.0, Computation time: 2.7897093296051025\n",
      "Step: 248, Loss: 0.9420367479324341, Accuracy: 0.96875, Computation time: 2.4728784561157227\n",
      "Step: 249, Loss: 0.9163362979888916, Accuracy: 1.0, Computation time: 2.0541558265686035\n",
      "Step: 250, Loss: 0.9172772765159607, Accuracy: 1.0, Computation time: 2.1681289672851562\n",
      "Step: 251, Loss: 0.9328088164329529, Accuracy: 0.96875, Computation time: 2.0163843631744385\n",
      "Step: 252, Loss: 0.9164571762084961, Accuracy: 1.0, Computation time: 1.956719160079956\n",
      "Step: 253, Loss: 0.9163833856582642, Accuracy: 1.0, Computation time: 2.1229379177093506\n",
      "Step: 254, Loss: 0.9163371920585632, Accuracy: 1.0, Computation time: 1.9937753677368164\n",
      "Step: 255, Loss: 0.9172357320785522, Accuracy: 1.0, Computation time: 1.9346580505371094\n",
      "Step: 256, Loss: 0.9365477561950684, Accuracy: 0.96875, Computation time: 2.4990005493164062\n",
      "Step: 257, Loss: 0.9226971864700317, Accuracy: 1.0, Computation time: 1.8989179134368896\n",
      "Step: 258, Loss: 0.9174225926399231, Accuracy: 1.0, Computation time: 2.5927371978759766\n",
      "Step: 259, Loss: 0.9164043664932251, Accuracy: 1.0, Computation time: 1.9652140140533447\n",
      "Step: 260, Loss: 0.9340569376945496, Accuracy: 0.96875, Computation time: 2.1461939811706543\n",
      "Step: 261, Loss: 0.9381895661354065, Accuracy: 0.96875, Computation time: 2.6540729999542236\n",
      "Step: 262, Loss: 0.9194344878196716, Accuracy: 1.0, Computation time: 2.121507406234741\n",
      "Step: 263, Loss: 0.9393099546432495, Accuracy: 0.96875, Computation time: 2.342834711074829\n",
      "Step: 264, Loss: 0.9379354119300842, Accuracy: 0.96875, Computation time: 2.1126749515533447\n",
      "Step: 265, Loss: 0.9167470335960388, Accuracy: 1.0, Computation time: 2.1090381145477295\n",
      "Step: 266, Loss: 0.9162434339523315, Accuracy: 1.0, Computation time: 2.0791831016540527\n",
      "Step: 267, Loss: 0.9384418725967407, Accuracy: 0.96875, Computation time: 2.432546377182007\n",
      "Step: 268, Loss: 0.9193262457847595, Accuracy: 1.0, Computation time: 2.4317550659179688\n",
      "Step: 269, Loss: 0.9165618419647217, Accuracy: 1.0, Computation time: 2.253659963607788\n",
      "Step: 270, Loss: 0.917630136013031, Accuracy: 1.0, Computation time: 2.2520554065704346\n",
      "Step: 271, Loss: 0.9168399572372437, Accuracy: 1.0, Computation time: 2.5953621864318848\n",
      "Step: 272, Loss: 0.9165623188018799, Accuracy: 1.0, Computation time: 2.625880479812622\n",
      "Step: 273, Loss: 0.9171857833862305, Accuracy: 1.0, Computation time: 2.1389451026916504\n",
      "Step: 274, Loss: 0.9166138768196106, Accuracy: 1.0, Computation time: 2.0571398735046387\n",
      "Step: 275, Loss: 0.9169110655784607, Accuracy: 1.0, Computation time: 2.227926254272461\n",
      "Step: 276, Loss: 0.9167382717132568, Accuracy: 1.0, Computation time: 2.2908568382263184\n",
      "Step: 277, Loss: 0.9175881147384644, Accuracy: 1.0, Computation time: 2.012122392654419\n",
      "Step: 278, Loss: 0.92044597864151, Accuracy: 1.0, Computation time: 2.2511391639709473\n",
      "########################\n",
      "Test loss: 1.132011890411377, Test Accuracy_epoch2: 0.6838709712028503\n",
      "########################\n",
      "Step: 279, Loss: 0.9313662648200989, Accuracy: 0.96875, Computation time: 2.284290313720703\n",
      "Step: 280, Loss: 0.9164904356002808, Accuracy: 1.0, Computation time: 2.384146213531494\n",
      "Step: 281, Loss: 0.9170693159103394, Accuracy: 1.0, Computation time: 1.8112223148345947\n",
      "Step: 282, Loss: 0.9174978137016296, Accuracy: 1.0, Computation time: 2.230496644973755\n",
      "Step: 283, Loss: 0.9162424802780151, Accuracy: 1.0, Computation time: 2.1791298389434814\n",
      "Step: 284, Loss: 0.9349819421768188, Accuracy: 0.96875, Computation time: 2.317408800125122\n",
      "Step: 285, Loss: 0.9165807366371155, Accuracy: 1.0, Computation time: 2.1186180114746094\n",
      "Step: 286, Loss: 0.9188344478607178, Accuracy: 1.0, Computation time: 2.3293211460113525\n",
      "Step: 287, Loss: 0.9240735769271851, Accuracy: 1.0, Computation time: 2.335402488708496\n",
      "Step: 288, Loss: 0.9163802862167358, Accuracy: 1.0, Computation time: 1.9756639003753662\n",
      "Step: 289, Loss: 0.9167351126670837, Accuracy: 1.0, Computation time: 1.999147891998291\n",
      "Step: 290, Loss: 0.9162357449531555, Accuracy: 1.0, Computation time: 1.8382046222686768\n",
      "Step: 291, Loss: 0.9177671670913696, Accuracy: 1.0, Computation time: 2.479964017868042\n",
      "Step: 292, Loss: 0.9163543581962585, Accuracy: 1.0, Computation time: 2.2419791221618652\n",
      "Step: 293, Loss: 0.9161390066146851, Accuracy: 1.0, Computation time: 2.1650331020355225\n",
      "Step: 294, Loss: 0.916207492351532, Accuracy: 1.0, Computation time: 2.162665367126465\n",
      "Step: 295, Loss: 0.9162695407867432, Accuracy: 1.0, Computation time: 2.211007833480835\n",
      "Step: 296, Loss: 0.9176648855209351, Accuracy: 1.0, Computation time: 2.0542831420898438\n",
      "Step: 297, Loss: 0.9361836910247803, Accuracy: 0.96875, Computation time: 2.501633405685425\n",
      "Step: 298, Loss: 0.9196639657020569, Accuracy: 1.0, Computation time: 2.4213004112243652\n",
      "Step: 299, Loss: 0.9187272191047668, Accuracy: 1.0, Computation time: 2.0577890872955322\n",
      "Step: 300, Loss: 0.9163884520530701, Accuracy: 1.0, Computation time: 2.142329692840576\n",
      "Step: 301, Loss: 0.9162312150001526, Accuracy: 1.0, Computation time: 2.0090842247009277\n",
      "Step: 302, Loss: 0.9163435101509094, Accuracy: 1.0, Computation time: 1.9154677391052246\n",
      "Step: 303, Loss: 0.9161113500595093, Accuracy: 1.0, Computation time: 2.0851802825927734\n",
      "Step: 304, Loss: 0.9163442254066467, Accuracy: 1.0, Computation time: 2.1491968631744385\n",
      "Step: 305, Loss: 0.9379369616508484, Accuracy: 0.96875, Computation time: 1.9804410934448242\n",
      "Step: 306, Loss: 0.9162337183952332, Accuracy: 1.0, Computation time: 2.5238077640533447\n",
      "Step: 307, Loss: 0.9223862290382385, Accuracy: 1.0, Computation time: 2.082301378250122\n",
      "Step: 308, Loss: 0.9164061546325684, Accuracy: 1.0, Computation time: 2.0679707527160645\n",
      "Step: 309, Loss: 0.9161533117294312, Accuracy: 1.0, Computation time: 2.5709800720214844\n",
      "Step: 310, Loss: 0.9378540515899658, Accuracy: 0.96875, Computation time: 2.2219464778900146\n",
      "Step: 311, Loss: 0.9160400629043579, Accuracy: 1.0, Computation time: 2.0631911754608154\n",
      "Step: 312, Loss: 0.9276850819587708, Accuracy: 0.96875, Computation time: 2.257397174835205\n",
      "Step: 313, Loss: 0.9358709454536438, Accuracy: 0.96875, Computation time: 2.423874616622925\n",
      "Step: 314, Loss: 0.9389361143112183, Accuracy: 0.96875, Computation time: 1.8723726272583008\n",
      "Step: 315, Loss: 0.9162718057632446, Accuracy: 1.0, Computation time: 2.209754228591919\n",
      "Step: 316, Loss: 0.9277911186218262, Accuracy: 0.96875, Computation time: 2.3739686012268066\n",
      "Step: 317, Loss: 0.9164807200431824, Accuracy: 1.0, Computation time: 1.9468092918395996\n",
      "Step: 318, Loss: 0.9174900650978088, Accuracy: 1.0, Computation time: 1.9272658824920654\n",
      "Step: 319, Loss: 0.9162628650665283, Accuracy: 1.0, Computation time: 1.9754984378814697\n",
      "Step: 320, Loss: 0.9378381371498108, Accuracy: 0.96875, Computation time: 2.4439828395843506\n",
      "Step: 321, Loss: 0.9191970825195312, Accuracy: 1.0, Computation time: 2.0881268978118896\n",
      "Step: 322, Loss: 0.9178833961486816, Accuracy: 1.0, Computation time: 2.160115957260132\n",
      "Step: 323, Loss: 0.9208454489707947, Accuracy: 1.0, Computation time: 2.05795955657959\n",
      "Step: 324, Loss: 0.9163581132888794, Accuracy: 1.0, Computation time: 2.285397529602051\n",
      "Step: 325, Loss: 0.9193373322486877, Accuracy: 1.0, Computation time: 2.0053763389587402\n",
      "Step: 326, Loss: 0.9171597361564636, Accuracy: 1.0, Computation time: 1.8142671585083008\n",
      "Step: 327, Loss: 0.9204009771347046, Accuracy: 1.0, Computation time: 2.535696268081665\n",
      "Step: 328, Loss: 0.9161978960037231, Accuracy: 1.0, Computation time: 1.9842817783355713\n",
      "Step: 329, Loss: 0.9164093136787415, Accuracy: 1.0, Computation time: 2.2767162322998047\n",
      "Step: 330, Loss: 0.9166870713233948, Accuracy: 1.0, Computation time: 2.0750627517700195\n",
      "Step: 331, Loss: 0.9165515303611755, Accuracy: 1.0, Computation time: 2.304838180541992\n",
      "Step: 332, Loss: 0.9195466041564941, Accuracy: 1.0, Computation time: 2.219224214553833\n",
      "Step: 333, Loss: 0.9167202711105347, Accuracy: 1.0, Computation time: 2.3228323459625244\n",
      "Step: 334, Loss: 0.920095682144165, Accuracy: 1.0, Computation time: 2.2189457416534424\n",
      "Step: 335, Loss: 0.916265070438385, Accuracy: 1.0, Computation time: 2.2281711101531982\n",
      "Step: 336, Loss: 0.9341089725494385, Accuracy: 0.96875, Computation time: 2.1089892387390137\n",
      "Step: 337, Loss: 0.9161942601203918, Accuracy: 1.0, Computation time: 2.6682779788970947\n",
      "Step: 338, Loss: 0.9161092638969421, Accuracy: 1.0, Computation time: 2.2897696495056152\n",
      "Step: 339, Loss: 0.9295063018798828, Accuracy: 0.96875, Computation time: 2.170600175857544\n",
      "Step: 340, Loss: 0.9174166321754456, Accuracy: 1.0, Computation time: 2.3041534423828125\n",
      "Step: 341, Loss: 0.9379721879959106, Accuracy: 0.96875, Computation time: 2.3251447677612305\n",
      "Step: 342, Loss: 0.9167172312736511, Accuracy: 1.0, Computation time: 2.1513516902923584\n",
      "Step: 343, Loss: 0.9164738059043884, Accuracy: 1.0, Computation time: 2.1789591312408447\n",
      "Step: 344, Loss: 0.9162498712539673, Accuracy: 1.0, Computation time: 2.34651255607605\n",
      "Step: 345, Loss: 0.9160807728767395, Accuracy: 1.0, Computation time: 1.985961675643921\n",
      "Step: 346, Loss: 0.9164428114891052, Accuracy: 1.0, Computation time: 1.9791920185089111\n",
      "Step: 347, Loss: 0.9377825260162354, Accuracy: 0.96875, Computation time: 2.094048023223877\n",
      "Step: 348, Loss: 0.9160982370376587, Accuracy: 1.0, Computation time: 1.9928123950958252\n",
      "Step: 349, Loss: 0.9233259558677673, Accuracy: 1.0, Computation time: 2.441420793533325\n",
      "Step: 350, Loss: 0.9172008633613586, Accuracy: 1.0, Computation time: 1.8804059028625488\n",
      "Step: 351, Loss: 0.9160587191581726, Accuracy: 1.0, Computation time: 1.9750392436981201\n",
      "Step: 352, Loss: 0.9192873239517212, Accuracy: 1.0, Computation time: 2.090069055557251\n",
      "Step: 353, Loss: 0.9161709547042847, Accuracy: 1.0, Computation time: 2.164470911026001\n",
      "Step: 354, Loss: 0.9160689115524292, Accuracy: 1.0, Computation time: 2.2591097354888916\n",
      "Step: 355, Loss: 0.916073739528656, Accuracy: 1.0, Computation time: 1.964831829071045\n",
      "Step: 356, Loss: 0.9162668585777283, Accuracy: 1.0, Computation time: 2.3842482566833496\n",
      "Step: 357, Loss: 0.9163366556167603, Accuracy: 1.0, Computation time: 2.201490640640259\n",
      "Step: 358, Loss: 0.937974750995636, Accuracy: 0.96875, Computation time: 2.0026416778564453\n",
      "Step: 359, Loss: 0.9194649457931519, Accuracy: 1.0, Computation time: 2.230862855911255\n",
      "Step: 360, Loss: 0.9160909652709961, Accuracy: 1.0, Computation time: 2.2412660121917725\n",
      "Step: 361, Loss: 0.9163606762886047, Accuracy: 1.0, Computation time: 2.4382824897766113\n",
      "Step: 362, Loss: 0.9236770868301392, Accuracy: 1.0, Computation time: 2.1927149295806885\n",
      "Step: 363, Loss: 0.9162713289260864, Accuracy: 1.0, Computation time: 2.3276069164276123\n",
      "Step: 364, Loss: 0.9238742589950562, Accuracy: 1.0, Computation time: 2.2227392196655273\n",
      "Step: 365, Loss: 0.9298697113990784, Accuracy: 0.96875, Computation time: 2.15421724319458\n",
      "Step: 366, Loss: 0.9174016118049622, Accuracy: 1.0, Computation time: 2.2004754543304443\n",
      "Step: 367, Loss: 0.9162752032279968, Accuracy: 1.0, Computation time: 2.3424293994903564\n",
      "Step: 368, Loss: 0.9162143468856812, Accuracy: 1.0, Computation time: 2.378309726715088\n",
      "Step: 369, Loss: 0.9537975192070007, Accuracy: 0.9375, Computation time: 2.6444222927093506\n",
      "Step: 370, Loss: 0.9377997517585754, Accuracy: 0.96875, Computation time: 2.3391857147216797\n",
      "Step: 371, Loss: 0.9161859750747681, Accuracy: 1.0, Computation time: 2.138686180114746\n",
      "Step: 372, Loss: 0.9388867616653442, Accuracy: 0.96875, Computation time: 2.3735239505767822\n",
      "Step: 373, Loss: 0.9164348840713501, Accuracy: 1.0, Computation time: 2.3122262954711914\n",
      "Step: 374, Loss: 0.9164279103279114, Accuracy: 1.0, Computation time: 2.2184979915618896\n",
      "Step: 375, Loss: 0.9165175557136536, Accuracy: 1.0, Computation time: 2.2478208541870117\n",
      "Step: 376, Loss: 0.9365299940109253, Accuracy: 0.96875, Computation time: 2.3697516918182373\n",
      "Step: 377, Loss: 0.9277648329734802, Accuracy: 0.96875, Computation time: 2.341871500015259\n",
      "Step: 378, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 1.9098477363586426\n",
      "Step: 379, Loss: 0.9205725789070129, Accuracy: 1.0, Computation time: 2.8042349815368652\n",
      "Step: 380, Loss: 0.9164801239967346, Accuracy: 1.0, Computation time: 2.1426665782928467\n",
      "Step: 381, Loss: 0.9163413047790527, Accuracy: 1.0, Computation time: 2.3483731746673584\n",
      "Step: 382, Loss: 0.9167346358299255, Accuracy: 1.0, Computation time: 2.4568991661071777\n",
      "Step: 383, Loss: 0.9177504777908325, Accuracy: 1.0, Computation time: 2.0785861015319824\n",
      "Step: 384, Loss: 0.9363117814064026, Accuracy: 0.96875, Computation time: 2.6851463317871094\n",
      "Step: 385, Loss: 0.9212990403175354, Accuracy: 1.0, Computation time: 1.8933591842651367\n",
      "Step: 386, Loss: 0.9371233582496643, Accuracy: 0.96875, Computation time: 2.087475061416626\n",
      "Step: 387, Loss: 0.9168344140052795, Accuracy: 1.0, Computation time: 2.041868209838867\n",
      "Step: 388, Loss: 0.9214039444923401, Accuracy: 1.0, Computation time: 2.373002529144287\n",
      "Step: 389, Loss: 0.9383209347724915, Accuracy: 0.96875, Computation time: 2.255120277404785\n",
      "Step: 390, Loss: 0.9175668358802795, Accuracy: 1.0, Computation time: 1.8564655780792236\n",
      "Step: 391, Loss: 0.9399818778038025, Accuracy: 0.96875, Computation time: 2.454360246658325\n",
      "Step: 392, Loss: 0.9166014790534973, Accuracy: 1.0, Computation time: 2.1354055404663086\n",
      "Step: 393, Loss: 0.9164108037948608, Accuracy: 1.0, Computation time: 2.457779884338379\n",
      "Step: 394, Loss: 0.9172307252883911, Accuracy: 1.0, Computation time: 2.050567150115967\n",
      "Step: 395, Loss: 0.945747971534729, Accuracy: 0.96875, Computation time: 2.1709654331207275\n",
      "Step: 396, Loss: 0.9179823994636536, Accuracy: 1.0, Computation time: 2.23358154296875\n",
      "Step: 397, Loss: 0.9172566533088684, Accuracy: 1.0, Computation time: 2.205277681350708\n",
      "Step: 398, Loss: 0.9203285574913025, Accuracy: 1.0, Computation time: 2.199352741241455\n",
      "Step: 399, Loss: 0.9387761354446411, Accuracy: 0.96875, Computation time: 2.23732328414917\n",
      "Step: 400, Loss: 0.9268922209739685, Accuracy: 0.96875, Computation time: 2.00156569480896\n",
      "Step: 401, Loss: 0.916439414024353, Accuracy: 1.0, Computation time: 2.175477981567383\n",
      "Step: 402, Loss: 0.9316869974136353, Accuracy: 0.96875, Computation time: 1.9166171550750732\n",
      "Step: 403, Loss: 0.9160564541816711, Accuracy: 1.0, Computation time: 2.779613494873047\n",
      "Step: 404, Loss: 0.9168145060539246, Accuracy: 1.0, Computation time: 1.9074184894561768\n",
      "Step: 405, Loss: 0.9162968397140503, Accuracy: 1.0, Computation time: 2.6046266555786133\n",
      "Step: 406, Loss: 0.9162144064903259, Accuracy: 1.0, Computation time: 2.2266428470611572\n",
      "Step: 407, Loss: 0.9162638187408447, Accuracy: 1.0, Computation time: 1.8926072120666504\n",
      "Step: 408, Loss: 0.916016697883606, Accuracy: 1.0, Computation time: 1.910743236541748\n",
      "Step: 409, Loss: 0.916164219379425, Accuracy: 1.0, Computation time: 1.9331064224243164\n",
      "Step: 410, Loss: 0.9376897811889648, Accuracy: 0.96875, Computation time: 2.085005760192871\n",
      "Step: 411, Loss: 0.9177765250205994, Accuracy: 1.0, Computation time: 2.169471263885498\n",
      "Step: 412, Loss: 0.9348721504211426, Accuracy: 0.96875, Computation time: 1.926269769668579\n",
      "Step: 413, Loss: 0.9169915914535522, Accuracy: 1.0, Computation time: 1.902099609375\n",
      "Step: 414, Loss: 0.9166083335876465, Accuracy: 1.0, Computation time: 2.32446551322937\n",
      "Step: 415, Loss: 0.9163704514503479, Accuracy: 1.0, Computation time: 2.0303430557250977\n",
      "Step: 416, Loss: 0.9377450942993164, Accuracy: 0.96875, Computation time: 2.325364589691162\n",
      "Step: 417, Loss: 0.9165676236152649, Accuracy: 1.0, Computation time: 2.1471903324127197\n",
      "########################\n",
      "Test loss: 1.1170907020568848, Test Accuracy_epoch3: 0.7096773982048035\n",
      "########################\n",
      "Step: 418, Loss: 0.9161105155944824, Accuracy: 1.0, Computation time: 2.4173743724823\n",
      "Step: 419, Loss: 0.916388750076294, Accuracy: 1.0, Computation time: 2.523869037628174\n",
      "Step: 420, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 2.162485122680664\n",
      "Step: 421, Loss: 0.9163543581962585, Accuracy: 1.0, Computation time: 3.1287474632263184\n",
      "Step: 422, Loss: 0.916136622428894, Accuracy: 1.0, Computation time: 2.349609136581421\n",
      "Step: 423, Loss: 0.9160670042037964, Accuracy: 1.0, Computation time: 1.9933855533599854\n",
      "Step: 424, Loss: 0.91608065366745, Accuracy: 1.0, Computation time: 2.4171090126037598\n",
      "Step: 425, Loss: 0.9450787305831909, Accuracy: 0.9375, Computation time: 1.9032795429229736\n",
      "Step: 426, Loss: 0.9162584543228149, Accuracy: 1.0, Computation time: 2.5417110919952393\n",
      "Step: 427, Loss: 0.9159807562828064, Accuracy: 1.0, Computation time: 2.1327056884765625\n",
      "Step: 428, Loss: 0.9180478453636169, Accuracy: 1.0, Computation time: 2.3889505863189697\n",
      "Step: 429, Loss: 0.9162572026252747, Accuracy: 1.0, Computation time: 2.0372073650360107\n",
      "Step: 430, Loss: 0.9377704858779907, Accuracy: 0.96875, Computation time: 1.7810728549957275\n",
      "Step: 431, Loss: 0.9589013457298279, Accuracy: 0.9375, Computation time: 1.9940450191497803\n",
      "Step: 432, Loss: 0.9258779883384705, Accuracy: 0.96875, Computation time: 2.0780656337738037\n",
      "Step: 433, Loss: 0.925234854221344, Accuracy: 1.0, Computation time: 2.283484697341919\n",
      "Step: 434, Loss: 0.9159891605377197, Accuracy: 1.0, Computation time: 2.0829625129699707\n",
      "Step: 435, Loss: 0.9159795641899109, Accuracy: 1.0, Computation time: 2.142240285873413\n",
      "Step: 436, Loss: 0.9172865152359009, Accuracy: 1.0, Computation time: 2.0806941986083984\n",
      "Step: 437, Loss: 0.9163144826889038, Accuracy: 1.0, Computation time: 2.1768667697906494\n",
      "Step: 438, Loss: 0.9375985264778137, Accuracy: 0.96875, Computation time: 1.8187627792358398\n",
      "Step: 439, Loss: 0.9160634279251099, Accuracy: 1.0, Computation time: 2.60587215423584\n",
      "Step: 440, Loss: 0.9160786867141724, Accuracy: 1.0, Computation time: 2.256265640258789\n",
      "Step: 441, Loss: 0.9160280823707581, Accuracy: 1.0, Computation time: 2.163809061050415\n",
      "Step: 442, Loss: 0.9175621271133423, Accuracy: 1.0, Computation time: 2.5776188373565674\n",
      "Step: 443, Loss: 0.9167391061782837, Accuracy: 1.0, Computation time: 1.9779303073883057\n",
      "Step: 444, Loss: 0.9189867973327637, Accuracy: 1.0, Computation time: 2.5554349422454834\n",
      "Step: 445, Loss: 0.91656494140625, Accuracy: 1.0, Computation time: 1.8394017219543457\n",
      "Step: 446, Loss: 0.9166682958602905, Accuracy: 1.0, Computation time: 2.1826765537261963\n",
      "Step: 447, Loss: 0.9198580980300903, Accuracy: 1.0, Computation time: 1.8729515075683594\n",
      "Step: 448, Loss: 0.9160414338111877, Accuracy: 1.0, Computation time: 1.937629222869873\n",
      "Step: 449, Loss: 0.9162193536758423, Accuracy: 1.0, Computation time: 2.1735026836395264\n",
      "Step: 450, Loss: 0.9172332882881165, Accuracy: 1.0, Computation time: 2.3846805095672607\n",
      "Step: 451, Loss: 0.9225166440010071, Accuracy: 1.0, Computation time: 2.082151174545288\n",
      "Step: 452, Loss: 0.9237591028213501, Accuracy: 1.0, Computation time: 1.9065096378326416\n",
      "Step: 453, Loss: 0.9163680076599121, Accuracy: 1.0, Computation time: 2.3108880519866943\n",
      "Step: 454, Loss: 0.9162963628768921, Accuracy: 1.0, Computation time: 2.2784998416900635\n",
      "Step: 455, Loss: 0.9162842035293579, Accuracy: 1.0, Computation time: 2.569538116455078\n",
      "Step: 456, Loss: 0.9374778270721436, Accuracy: 0.96875, Computation time: 2.136749267578125\n",
      "Step: 457, Loss: 0.9378321766853333, Accuracy: 0.96875, Computation time: 2.7209510803222656\n",
      "Step: 458, Loss: 0.9566428065299988, Accuracy: 0.9375, Computation time: 2.4467859268188477\n",
      "Step: 459, Loss: 0.9160208106040955, Accuracy: 1.0, Computation time: 2.5206422805786133\n",
      "Step: 460, Loss: 0.9355969429016113, Accuracy: 0.96875, Computation time: 2.474503993988037\n",
      "Step: 461, Loss: 0.9161442518234253, Accuracy: 1.0, Computation time: 2.6318306922912598\n",
      "Step: 462, Loss: 0.9338147640228271, Accuracy: 0.96875, Computation time: 2.258916139602661\n",
      "Step: 463, Loss: 0.9163873195648193, Accuracy: 1.0, Computation time: 2.163818359375\n",
      "Step: 464, Loss: 0.9161889553070068, Accuracy: 1.0, Computation time: 1.7565882205963135\n",
      "Step: 465, Loss: 0.916299045085907, Accuracy: 1.0, Computation time: 2.16904354095459\n",
      "Step: 466, Loss: 0.9162024855613708, Accuracy: 1.0, Computation time: 2.226109027862549\n",
      "Step: 467, Loss: 0.9163230657577515, Accuracy: 1.0, Computation time: 2.0113372802734375\n",
      "Step: 468, Loss: 0.9163128137588501, Accuracy: 1.0, Computation time: 2.2809765338897705\n",
      "Step: 469, Loss: 0.917831540107727, Accuracy: 1.0, Computation time: 2.266416549682617\n",
      "Step: 470, Loss: 0.9161282777786255, Accuracy: 1.0, Computation time: 2.2936182022094727\n",
      "Step: 471, Loss: 0.9258447885513306, Accuracy: 1.0, Computation time: 2.4750454425811768\n",
      "Step: 472, Loss: 0.9160979390144348, Accuracy: 1.0, Computation time: 1.9112796783447266\n",
      "Step: 473, Loss: 0.9160977602005005, Accuracy: 1.0, Computation time: 2.1428515911102295\n",
      "Step: 474, Loss: 0.9165115356445312, Accuracy: 1.0, Computation time: 2.1245994567871094\n",
      "Step: 475, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 2.1604807376861572\n",
      "Step: 476, Loss: 0.9163097143173218, Accuracy: 1.0, Computation time: 1.8326244354248047\n",
      "Step: 477, Loss: 0.9164521098136902, Accuracy: 1.0, Computation time: 2.4635534286499023\n",
      "Step: 478, Loss: 0.9308351874351501, Accuracy: 0.96875, Computation time: 1.6650550365447998\n",
      "Step: 479, Loss: 0.9164859652519226, Accuracy: 1.0, Computation time: 2.097421169281006\n",
      "Step: 480, Loss: 0.9299055933952332, Accuracy: 0.96875, Computation time: 2.248542070388794\n",
      "Step: 481, Loss: 0.9160816669464111, Accuracy: 1.0, Computation time: 2.109527111053467\n",
      "Step: 482, Loss: 0.9161966443061829, Accuracy: 1.0, Computation time: 2.0592422485351562\n",
      "Step: 483, Loss: 0.9179319739341736, Accuracy: 1.0, Computation time: 1.932563304901123\n",
      "Step: 484, Loss: 0.9168074727058411, Accuracy: 1.0, Computation time: 2.115889072418213\n",
      "Step: 485, Loss: 0.9161983132362366, Accuracy: 1.0, Computation time: 1.7322165966033936\n",
      "Step: 486, Loss: 0.9595168232917786, Accuracy: 0.9375, Computation time: 2.6691882610321045\n",
      "Step: 487, Loss: 0.9188375473022461, Accuracy: 1.0, Computation time: 2.3181746006011963\n",
      "Step: 488, Loss: 0.9163510799407959, Accuracy: 1.0, Computation time: 2.202146291732788\n",
      "Step: 489, Loss: 0.9161731004714966, Accuracy: 1.0, Computation time: 2.3168163299560547\n",
      "Step: 490, Loss: 0.9162474274635315, Accuracy: 1.0, Computation time: 2.0279924869537354\n",
      "Step: 491, Loss: 0.9162083268165588, Accuracy: 1.0, Computation time: 2.3512184619903564\n",
      "Step: 492, Loss: 0.9166679382324219, Accuracy: 1.0, Computation time: 2.256528615951538\n",
      "Step: 493, Loss: 0.9172562956809998, Accuracy: 1.0, Computation time: 2.5379509925842285\n",
      "Step: 494, Loss: 0.9161081910133362, Accuracy: 1.0, Computation time: 2.5841591358184814\n",
      "Step: 495, Loss: 0.9160932898521423, Accuracy: 1.0, Computation time: 2.366530656814575\n",
      "Step: 496, Loss: 0.9160882830619812, Accuracy: 1.0, Computation time: 1.9037067890167236\n",
      "Step: 497, Loss: 0.9387323260307312, Accuracy: 0.96875, Computation time: 2.299849510192871\n",
      "Step: 498, Loss: 0.9165241718292236, Accuracy: 1.0, Computation time: 2.1880910396575928\n",
      "Step: 499, Loss: 0.9161251187324524, Accuracy: 1.0, Computation time: 2.043405055999756\n",
      "Step: 500, Loss: 0.9160590767860413, Accuracy: 1.0, Computation time: 2.143047571182251\n",
      "Step: 501, Loss: 0.9377507567405701, Accuracy: 0.96875, Computation time: 2.1000216007232666\n",
      "Step: 502, Loss: 0.9167906045913696, Accuracy: 1.0, Computation time: 1.8673272132873535\n",
      "Step: 503, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.8577368259429932\n",
      "Step: 504, Loss: 0.9597837328910828, Accuracy: 0.9375, Computation time: 1.7818143367767334\n",
      "Step: 505, Loss: 0.9348821640014648, Accuracy: 0.96875, Computation time: 2.0661540031433105\n",
      "Step: 506, Loss: 0.916023313999176, Accuracy: 1.0, Computation time: 1.729459524154663\n",
      "Step: 507, Loss: 0.915996789932251, Accuracy: 1.0, Computation time: 1.922985553741455\n",
      "Step: 508, Loss: 0.9166603088378906, Accuracy: 1.0, Computation time: 1.6882591247558594\n",
      "Step: 509, Loss: 0.9363939762115479, Accuracy: 0.96875, Computation time: 1.979569911956787\n",
      "Step: 510, Loss: 0.9161290526390076, Accuracy: 1.0, Computation time: 2.3173108100891113\n",
      "Step: 511, Loss: 0.9161580801010132, Accuracy: 1.0, Computation time: 2.1041555404663086\n",
      "Step: 512, Loss: 0.9189181327819824, Accuracy: 1.0, Computation time: 1.7991771697998047\n",
      "Step: 513, Loss: 0.9161779880523682, Accuracy: 1.0, Computation time: 1.790574073791504\n",
      "Step: 514, Loss: 0.9296188354492188, Accuracy: 0.96875, Computation time: 2.040104389190674\n",
      "Step: 515, Loss: 0.9165596961975098, Accuracy: 1.0, Computation time: 2.157884120941162\n",
      "Step: 516, Loss: 0.9162881374359131, Accuracy: 1.0, Computation time: 1.9503397941589355\n",
      "Step: 517, Loss: 0.9163222908973694, Accuracy: 1.0, Computation time: 2.051581382751465\n",
      "Step: 518, Loss: 0.9164549708366394, Accuracy: 1.0, Computation time: 1.9951601028442383\n",
      "Step: 519, Loss: 0.9164595603942871, Accuracy: 1.0, Computation time: 2.1789867877960205\n",
      "Step: 520, Loss: 0.9166828989982605, Accuracy: 1.0, Computation time: 2.227116584777832\n",
      "Step: 521, Loss: 0.9159896969795227, Accuracy: 1.0, Computation time: 2.365230083465576\n",
      "Step: 522, Loss: 0.91673743724823, Accuracy: 1.0, Computation time: 1.5669455528259277\n",
      "Step: 523, Loss: 0.9163293838500977, Accuracy: 1.0, Computation time: 1.978133201599121\n",
      "Step: 524, Loss: 0.91651850938797, Accuracy: 1.0, Computation time: 2.4434866905212402\n",
      "Step: 525, Loss: 0.919995903968811, Accuracy: 1.0, Computation time: 2.154137372970581\n",
      "Step: 526, Loss: 0.9161812663078308, Accuracy: 1.0, Computation time: 2.1573379039764404\n",
      "Step: 527, Loss: 0.9243884682655334, Accuracy: 1.0, Computation time: 1.8252801895141602\n",
      "Step: 528, Loss: 0.9491444230079651, Accuracy: 0.9375, Computation time: 1.8640265464782715\n",
      "Step: 529, Loss: 0.9161497950553894, Accuracy: 1.0, Computation time: 1.9063992500305176\n",
      "Step: 530, Loss: 0.9183982610702515, Accuracy: 1.0, Computation time: 2.0060856342315674\n",
      "Step: 531, Loss: 0.9164699912071228, Accuracy: 1.0, Computation time: 2.247389793395996\n",
      "Step: 532, Loss: 0.9166122078895569, Accuracy: 1.0, Computation time: 2.027541160583496\n",
      "Step: 533, Loss: 0.9167342185974121, Accuracy: 1.0, Computation time: 2.0054354667663574\n",
      "Step: 534, Loss: 0.9174233078956604, Accuracy: 1.0, Computation time: 1.7769181728363037\n",
      "Step: 535, Loss: 0.9184162616729736, Accuracy: 1.0, Computation time: 2.121107816696167\n",
      "Step: 536, Loss: 0.9160405397415161, Accuracy: 1.0, Computation time: 1.5660295486450195\n",
      "Step: 537, Loss: 0.9159988760948181, Accuracy: 1.0, Computation time: 2.059514284133911\n",
      "Step: 538, Loss: 0.9167672991752625, Accuracy: 1.0, Computation time: 2.0119690895080566\n",
      "Step: 539, Loss: 0.9165884256362915, Accuracy: 1.0, Computation time: 2.322512149810791\n",
      "Step: 540, Loss: 0.9239193797111511, Accuracy: 1.0, Computation time: 2.0905492305755615\n",
      "Step: 541, Loss: 0.9270859956741333, Accuracy: 0.96875, Computation time: 2.049236297607422\n",
      "Step: 542, Loss: 0.9241029024124146, Accuracy: 1.0, Computation time: 2.0286355018615723\n",
      "Step: 543, Loss: 0.9164304137229919, Accuracy: 1.0, Computation time: 1.9977171421051025\n",
      "Step: 544, Loss: 0.9356560707092285, Accuracy: 0.96875, Computation time: 1.9429845809936523\n",
      "Step: 545, Loss: 0.9168484210968018, Accuracy: 1.0, Computation time: 2.1935653686523438\n",
      "Step: 546, Loss: 0.916317880153656, Accuracy: 1.0, Computation time: 1.6897716522216797\n",
      "Step: 547, Loss: 0.9175935983657837, Accuracy: 1.0, Computation time: 2.1380178928375244\n",
      "Step: 548, Loss: 0.9165058732032776, Accuracy: 1.0, Computation time: 1.7905092239379883\n",
      "Step: 549, Loss: 0.9167565703392029, Accuracy: 1.0, Computation time: 2.1032350063323975\n",
      "Step: 550, Loss: 0.9165157079696655, Accuracy: 1.0, Computation time: 1.7970664501190186\n",
      "Step: 551, Loss: 0.9161472320556641, Accuracy: 1.0, Computation time: 2.1522514820098877\n",
      "Step: 552, Loss: 0.9175668954849243, Accuracy: 1.0, Computation time: 2.431415557861328\n",
      "Step: 553, Loss: 0.9161609411239624, Accuracy: 1.0, Computation time: 1.7977352142333984\n",
      "Step: 554, Loss: 0.9163839221000671, Accuracy: 1.0, Computation time: 2.218862771987915\n",
      "Step: 555, Loss: 0.9164699912071228, Accuracy: 1.0, Computation time: 2.2131807804107666\n",
      "Step: 556, Loss: 0.9162489771842957, Accuracy: 1.0, Computation time: 2.082327365875244\n",
      "########################\n",
      "Test loss: 1.1173666715621948, Test Accuracy_epoch4: 0.7087557911872864\n",
      "########################\n",
      "Step: 557, Loss: 0.916168749332428, Accuracy: 1.0, Computation time: 2.230132579803467\n",
      "Step: 558, Loss: 0.9160236120223999, Accuracy: 1.0, Computation time: 1.732421636581421\n",
      "Step: 559, Loss: 0.9162974953651428, Accuracy: 1.0, Computation time: 2.0863516330718994\n",
      "Step: 560, Loss: 0.9162843823432922, Accuracy: 1.0, Computation time: 1.9493980407714844\n",
      "Step: 561, Loss: 0.91612309217453, Accuracy: 1.0, Computation time: 2.738189458847046\n",
      "Step: 562, Loss: 0.9160159826278687, Accuracy: 1.0, Computation time: 1.734867811203003\n",
      "Step: 563, Loss: 0.916720986366272, Accuracy: 1.0, Computation time: 1.9159622192382812\n",
      "Step: 564, Loss: 0.9161167144775391, Accuracy: 1.0, Computation time: 1.7610485553741455\n",
      "Step: 565, Loss: 0.9175740480422974, Accuracy: 1.0, Computation time: 2.1778547763824463\n",
      "Step: 566, Loss: 0.9160442352294922, Accuracy: 1.0, Computation time: 2.109938383102417\n",
      "Step: 567, Loss: 0.9162550568580627, Accuracy: 1.0, Computation time: 1.7218263149261475\n",
      "Step: 568, Loss: 0.935081958770752, Accuracy: 0.96875, Computation time: 2.054213047027588\n",
      "Step: 569, Loss: 0.9176047444343567, Accuracy: 1.0, Computation time: 1.8829305171966553\n",
      "Step: 570, Loss: 0.9164606332778931, Accuracy: 1.0, Computation time: 2.2208809852600098\n",
      "Step: 571, Loss: 0.936431884765625, Accuracy: 0.96875, Computation time: 2.822113037109375\n",
      "Step: 572, Loss: 0.9161994457244873, Accuracy: 1.0, Computation time: 2.6588549613952637\n",
      "Step: 573, Loss: 0.9160314202308655, Accuracy: 1.0, Computation time: 1.450085163116455\n",
      "Step: 574, Loss: 0.9232407212257385, Accuracy: 1.0, Computation time: 2.0920121669769287\n",
      "Step: 575, Loss: 0.9169725775718689, Accuracy: 1.0, Computation time: 2.015010118484497\n",
      "Step: 576, Loss: 0.918304979801178, Accuracy: 1.0, Computation time: 1.7538275718688965\n",
      "Step: 577, Loss: 0.9163244366645813, Accuracy: 1.0, Computation time: 1.776808738708496\n",
      "Step: 578, Loss: 0.9168732762336731, Accuracy: 1.0, Computation time: 2.0318071842193604\n",
      "Step: 579, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 1.7825677394866943\n",
      "Step: 580, Loss: 0.9160314798355103, Accuracy: 1.0, Computation time: 1.696925401687622\n",
      "Step: 581, Loss: 0.9160081148147583, Accuracy: 1.0, Computation time: 2.0754854679107666\n",
      "Step: 582, Loss: 0.9160481691360474, Accuracy: 1.0, Computation time: 1.7588443756103516\n",
      "Step: 583, Loss: 0.9161423444747925, Accuracy: 1.0, Computation time: 1.9763386249542236\n",
      "Step: 584, Loss: 0.918393611907959, Accuracy: 1.0, Computation time: 2.139172077178955\n",
      "Step: 585, Loss: 0.9183812141418457, Accuracy: 1.0, Computation time: 2.185270309448242\n",
      "Step: 586, Loss: 0.9374938011169434, Accuracy: 0.96875, Computation time: 1.9833612442016602\n",
      "Step: 587, Loss: 0.9159560203552246, Accuracy: 1.0, Computation time: 1.7281763553619385\n",
      "Step: 588, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.5679469108581543\n",
      "Step: 589, Loss: 0.9379001259803772, Accuracy: 0.96875, Computation time: 2.07544207572937\n",
      "Step: 590, Loss: 0.9161084890365601, Accuracy: 1.0, Computation time: 2.4256670475006104\n",
      "Step: 591, Loss: 0.9309935569763184, Accuracy: 0.96875, Computation time: 1.9460127353668213\n",
      "Step: 592, Loss: 0.9159784317016602, Accuracy: 1.0, Computation time: 1.8853273391723633\n",
      "Step: 593, Loss: 0.9161895513534546, Accuracy: 1.0, Computation time: 1.9800300598144531\n",
      "Step: 594, Loss: 0.9161673188209534, Accuracy: 1.0, Computation time: 1.742325782775879\n",
      "Step: 595, Loss: 0.9372814297676086, Accuracy: 0.96875, Computation time: 2.050220251083374\n",
      "Step: 596, Loss: 0.9185724854469299, Accuracy: 1.0, Computation time: 2.048306465148926\n",
      "Step: 597, Loss: 0.9160109758377075, Accuracy: 1.0, Computation time: 2.0050055980682373\n",
      "Step: 598, Loss: 0.9176791310310364, Accuracy: 1.0, Computation time: 2.4058637619018555\n",
      "Step: 599, Loss: 0.9177409410476685, Accuracy: 1.0, Computation time: 1.9628210067749023\n",
      "Step: 600, Loss: 0.9565235376358032, Accuracy: 0.9375, Computation time: 2.11914324760437\n",
      "Step: 601, Loss: 0.9161787033081055, Accuracy: 1.0, Computation time: 1.8669548034667969\n",
      "Step: 602, Loss: 0.916269838809967, Accuracy: 1.0, Computation time: 2.06310772895813\n",
      "Step: 603, Loss: 0.9396564960479736, Accuracy: 0.96875, Computation time: 2.2800025939941406\n",
      "Step: 604, Loss: 0.9163400530815125, Accuracy: 1.0, Computation time: 1.9562435150146484\n",
      "Step: 605, Loss: 0.9162736535072327, Accuracy: 1.0, Computation time: 1.7120473384857178\n",
      "Step: 606, Loss: 0.9378598928451538, Accuracy: 0.96875, Computation time: 1.8682475090026855\n",
      "Step: 607, Loss: 0.9377933144569397, Accuracy: 0.96875, Computation time: 2.285682439804077\n",
      "Step: 608, Loss: 0.9161518812179565, Accuracy: 1.0, Computation time: 1.9459757804870605\n",
      "Step: 609, Loss: 0.9371957182884216, Accuracy: 0.96875, Computation time: 1.8047852516174316\n",
      "Step: 610, Loss: 0.9161741733551025, Accuracy: 1.0, Computation time: 2.145414113998413\n",
      "Step: 611, Loss: 0.9162189960479736, Accuracy: 1.0, Computation time: 1.855147123336792\n",
      "Step: 612, Loss: 0.9422820806503296, Accuracy: 0.96875, Computation time: 1.8473396301269531\n",
      "Step: 613, Loss: 0.9163329601287842, Accuracy: 1.0, Computation time: 1.9703104496002197\n",
      "Step: 614, Loss: 0.9163643717765808, Accuracy: 1.0, Computation time: 2.0117268562316895\n",
      "Step: 615, Loss: 0.9184942245483398, Accuracy: 1.0, Computation time: 1.9225685596466064\n",
      "Step: 616, Loss: 0.9160270690917969, Accuracy: 1.0, Computation time: 1.9528050422668457\n",
      "Step: 617, Loss: 0.9159971475601196, Accuracy: 1.0, Computation time: 1.9579548835754395\n",
      "Step: 618, Loss: 0.9181945323944092, Accuracy: 1.0, Computation time: 2.0075483322143555\n",
      "Step: 619, Loss: 0.9159860610961914, Accuracy: 1.0, Computation time: 2.325037717819214\n",
      "Step: 620, Loss: 0.9161850810050964, Accuracy: 1.0, Computation time: 2.1464035511016846\n",
      "Step: 621, Loss: 0.9160566329956055, Accuracy: 1.0, Computation time: 2.3215560913085938\n",
      "Step: 622, Loss: 0.9330521821975708, Accuracy: 0.96875, Computation time: 2.134331703186035\n",
      "Step: 623, Loss: 0.938956081867218, Accuracy: 0.96875, Computation time: 2.336608648300171\n",
      "Step: 624, Loss: 0.9290673136711121, Accuracy: 0.96875, Computation time: 2.2034337520599365\n",
      "Step: 625, Loss: 0.9179033041000366, Accuracy: 1.0, Computation time: 2.21069073677063\n",
      "Step: 626, Loss: 0.9160246849060059, Accuracy: 1.0, Computation time: 2.2080094814300537\n",
      "Step: 627, Loss: 0.9336896538734436, Accuracy: 0.96875, Computation time: 2.1594812870025635\n",
      "Step: 628, Loss: 0.9174415469169617, Accuracy: 1.0, Computation time: 2.298962354660034\n",
      "Step: 629, Loss: 0.9166252017021179, Accuracy: 1.0, Computation time: 1.6397018432617188\n",
      "Step: 630, Loss: 0.9196566939353943, Accuracy: 1.0, Computation time: 2.626438856124878\n",
      "Step: 631, Loss: 0.9165616631507874, Accuracy: 1.0, Computation time: 1.9995577335357666\n",
      "Step: 632, Loss: 0.9167531728744507, Accuracy: 1.0, Computation time: 1.8935766220092773\n",
      "Step: 633, Loss: 0.9233633279800415, Accuracy: 1.0, Computation time: 2.1726577281951904\n",
      "Step: 634, Loss: 0.9380952715873718, Accuracy: 0.96875, Computation time: 1.8221979141235352\n",
      "Step: 635, Loss: 0.9160892367362976, Accuracy: 1.0, Computation time: 1.776371717453003\n",
      "Step: 636, Loss: 0.9165908694267273, Accuracy: 1.0, Computation time: 2.130995035171509\n",
      "Step: 637, Loss: 0.937936007976532, Accuracy: 0.96875, Computation time: 1.8320865631103516\n",
      "Step: 638, Loss: 0.9162802696228027, Accuracy: 1.0, Computation time: 1.9586327075958252\n",
      "Step: 639, Loss: 0.9160361289978027, Accuracy: 1.0, Computation time: 1.995436668395996\n",
      "Step: 640, Loss: 0.9160850048065186, Accuracy: 1.0, Computation time: 1.6518065929412842\n",
      "Step: 641, Loss: 0.9159913659095764, Accuracy: 1.0, Computation time: 1.917961835861206\n",
      "Step: 642, Loss: 0.9161967039108276, Accuracy: 1.0, Computation time: 1.5745182037353516\n",
      "Step: 643, Loss: 0.9172029495239258, Accuracy: 1.0, Computation time: 2.0422582626342773\n",
      "Step: 644, Loss: 0.9160451292991638, Accuracy: 1.0, Computation time: 1.9031996726989746\n",
      "Step: 645, Loss: 0.9160991907119751, Accuracy: 1.0, Computation time: 1.825547456741333\n",
      "Step: 646, Loss: 0.9162107706069946, Accuracy: 1.0, Computation time: 1.655890703201294\n",
      "Step: 647, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 1.6166493892669678\n",
      "Step: 648, Loss: 0.9159408807754517, Accuracy: 1.0, Computation time: 2.2195780277252197\n",
      "Step: 649, Loss: 0.9303146600723267, Accuracy: 0.96875, Computation time: 1.9337413311004639\n",
      "Step: 650, Loss: 0.9372857213020325, Accuracy: 0.96875, Computation time: 2.343602418899536\n",
      "Step: 651, Loss: 0.9160135984420776, Accuracy: 1.0, Computation time: 1.5440726280212402\n",
      "Step: 652, Loss: 0.9161766767501831, Accuracy: 1.0, Computation time: 1.9559745788574219\n",
      "Step: 653, Loss: 0.9406277537345886, Accuracy: 0.96875, Computation time: 2.5249218940734863\n",
      "Step: 654, Loss: 0.915949285030365, Accuracy: 1.0, Computation time: 1.7894058227539062\n",
      "Step: 655, Loss: 0.9160218834877014, Accuracy: 1.0, Computation time: 1.7024974822998047\n",
      "Step: 656, Loss: 0.9161725044250488, Accuracy: 1.0, Computation time: 2.038912773132324\n",
      "Step: 657, Loss: 0.9366129636764526, Accuracy: 0.96875, Computation time: 1.7889182567596436\n",
      "Step: 658, Loss: 0.9159132242202759, Accuracy: 1.0, Computation time: 1.6159298419952393\n",
      "Step: 659, Loss: 0.915958046913147, Accuracy: 1.0, Computation time: 1.423647403717041\n",
      "Step: 660, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 1.9944250583648682\n",
      "Step: 661, Loss: 0.9163180589675903, Accuracy: 1.0, Computation time: 1.411285638809204\n",
      "Step: 662, Loss: 0.916802167892456, Accuracy: 1.0, Computation time: 1.8949992656707764\n",
      "Step: 663, Loss: 0.9161182045936584, Accuracy: 1.0, Computation time: 2.1181070804595947\n",
      "Step: 664, Loss: 0.9259253144264221, Accuracy: 0.96875, Computation time: 1.9760890007019043\n",
      "Step: 665, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 1.706052303314209\n",
      "Step: 666, Loss: 0.9183923602104187, Accuracy: 1.0, Computation time: 1.754608392715454\n",
      "Step: 667, Loss: 0.9292250275611877, Accuracy: 0.96875, Computation time: 1.777780532836914\n",
      "Step: 668, Loss: 0.9178693294525146, Accuracy: 1.0, Computation time: 1.4963998794555664\n",
      "Step: 669, Loss: 0.9162387251853943, Accuracy: 1.0, Computation time: 1.3242721557617188\n",
      "Step: 670, Loss: 0.9164754748344421, Accuracy: 1.0, Computation time: 1.5144543647766113\n",
      "Step: 671, Loss: 0.9161531925201416, Accuracy: 1.0, Computation time: 1.564359426498413\n",
      "Step: 672, Loss: 0.916468620300293, Accuracy: 1.0, Computation time: 1.790696144104004\n",
      "Step: 673, Loss: 0.9165937304496765, Accuracy: 1.0, Computation time: 1.7718408107757568\n",
      "Step: 674, Loss: 0.9165722727775574, Accuracy: 1.0, Computation time: 1.885267972946167\n",
      "Step: 675, Loss: 0.9160546064376831, Accuracy: 1.0, Computation time: 1.6790943145751953\n",
      "Step: 676, Loss: 0.9159931540489197, Accuracy: 1.0, Computation time: 1.6091299057006836\n",
      "Step: 677, Loss: 0.9159759283065796, Accuracy: 1.0, Computation time: 1.5310559272766113\n",
      "Step: 678, Loss: 0.9161392450332642, Accuracy: 1.0, Computation time: 1.7385647296905518\n",
      "Step: 679, Loss: 0.9242257475852966, Accuracy: 1.0, Computation time: 1.560610294342041\n",
      "Step: 680, Loss: 0.9168171882629395, Accuracy: 1.0, Computation time: 1.7962448596954346\n",
      "Step: 681, Loss: 0.934920608997345, Accuracy: 0.96875, Computation time: 2.1026556491851807\n",
      "Step: 682, Loss: 0.9161202311515808, Accuracy: 1.0, Computation time: 1.8516426086425781\n",
      "Step: 683, Loss: 0.954544186592102, Accuracy: 0.9375, Computation time: 1.851379156112671\n",
      "Step: 684, Loss: 0.9359409809112549, Accuracy: 0.96875, Computation time: 2.1068289279937744\n",
      "Step: 685, Loss: 0.9375634789466858, Accuracy: 0.96875, Computation time: 2.483337163925171\n",
      "Step: 686, Loss: 0.9189985394477844, Accuracy: 1.0, Computation time: 1.9553649425506592\n",
      "Step: 687, Loss: 0.9164389967918396, Accuracy: 1.0, Computation time: 1.9718761444091797\n",
      "Step: 688, Loss: 0.916895866394043, Accuracy: 1.0, Computation time: 1.971949815750122\n",
      "Step: 689, Loss: 0.9164438247680664, Accuracy: 1.0, Computation time: 1.792175054550171\n",
      "Step: 690, Loss: 0.916284441947937, Accuracy: 1.0, Computation time: 2.0461864471435547\n",
      "Step: 691, Loss: 0.9163619875907898, Accuracy: 1.0, Computation time: 2.1569981575012207\n",
      "Step: 692, Loss: 0.9164387583732605, Accuracy: 1.0, Computation time: 2.1119353771209717\n",
      "Step: 693, Loss: 0.9335678219795227, Accuracy: 0.96875, Computation time: 2.256727933883667\n",
      "Step: 694, Loss: 0.9159614443778992, Accuracy: 1.0, Computation time: 1.6910748481750488\n",
      "Step: 695, Loss: 0.915958046913147, Accuracy: 1.0, Computation time: 1.9334046840667725\n",
      "########################\n",
      "Test loss: 1.1240209341049194, Test Accuracy_epoch5: 0.6940092444419861\n",
      "########################\n",
      "Step: 696, Loss: 0.9162608981132507, Accuracy: 1.0, Computation time: 1.9196219444274902\n",
      "Step: 697, Loss: 0.9159837365150452, Accuracy: 1.0, Computation time: 1.8882625102996826\n",
      "Step: 698, Loss: 0.9161218404769897, Accuracy: 1.0, Computation time: 1.7781269550323486\n",
      "Step: 699, Loss: 0.916022777557373, Accuracy: 1.0, Computation time: 1.819462776184082\n",
      "Step: 700, Loss: 0.9160289764404297, Accuracy: 1.0, Computation time: 1.7898075580596924\n",
      "Step: 701, Loss: 0.9352049231529236, Accuracy: 0.96875, Computation time: 1.8192312717437744\n",
      "Step: 702, Loss: 0.9191205501556396, Accuracy: 1.0, Computation time: 1.942277431488037\n",
      "Step: 703, Loss: 0.9263160824775696, Accuracy: 0.96875, Computation time: 2.191636562347412\n",
      "Step: 704, Loss: 0.9159393906593323, Accuracy: 1.0, Computation time: 1.6583166122436523\n",
      "Step: 705, Loss: 0.9160018563270569, Accuracy: 1.0, Computation time: 1.7609641551971436\n",
      "Step: 706, Loss: 0.921358048915863, Accuracy: 1.0, Computation time: 2.9528167247772217\n",
      "Step: 707, Loss: 0.916265070438385, Accuracy: 1.0, Computation time: 2.0152649879455566\n",
      "Step: 708, Loss: 0.9161856770515442, Accuracy: 1.0, Computation time: 2.125483989715576\n",
      "Step: 709, Loss: 0.916901171207428, Accuracy: 1.0, Computation time: 1.7150111198425293\n",
      "Step: 710, Loss: 0.9175397753715515, Accuracy: 1.0, Computation time: 2.0378293991088867\n",
      "Step: 711, Loss: 0.9164843559265137, Accuracy: 1.0, Computation time: 2.2461180686950684\n",
      "Step: 712, Loss: 0.9161410927772522, Accuracy: 1.0, Computation time: 1.6379246711730957\n",
      "Step: 713, Loss: 0.9160432815551758, Accuracy: 1.0, Computation time: 2.3716940879821777\n",
      "Step: 714, Loss: 0.9163723587989807, Accuracy: 1.0, Computation time: 1.725745677947998\n",
      "Step: 715, Loss: 0.9588346481323242, Accuracy: 0.9375, Computation time: 1.8686256408691406\n",
      "Step: 716, Loss: 0.9377725124359131, Accuracy: 0.96875, Computation time: 1.9554462432861328\n",
      "Step: 717, Loss: 0.9292076826095581, Accuracy: 0.96875, Computation time: 1.937960147857666\n",
      "Step: 718, Loss: 0.9202121496200562, Accuracy: 1.0, Computation time: 1.948751449584961\n",
      "Step: 719, Loss: 0.91621994972229, Accuracy: 1.0, Computation time: 1.9736530780792236\n",
      "Step: 720, Loss: 0.9164386987686157, Accuracy: 1.0, Computation time: 2.4137465953826904\n",
      "Step: 721, Loss: 0.9343348741531372, Accuracy: 0.96875, Computation time: 2.2169783115386963\n",
      "Step: 722, Loss: 0.9162488579750061, Accuracy: 1.0, Computation time: 1.8753156661987305\n",
      "Step: 723, Loss: 0.9377949833869934, Accuracy: 0.96875, Computation time: 1.890282154083252\n",
      "Step: 724, Loss: 0.9188211560249329, Accuracy: 1.0, Computation time: 2.221231698989868\n",
      "Step: 725, Loss: 0.9159893989562988, Accuracy: 1.0, Computation time: 2.099968910217285\n",
      "Step: 726, Loss: 0.9159602522850037, Accuracy: 1.0, Computation time: 1.7008497714996338\n",
      "Step: 727, Loss: 0.9413937330245972, Accuracy: 0.96875, Computation time: 2.1020047664642334\n",
      "Step: 728, Loss: 0.9379370212554932, Accuracy: 0.96875, Computation time: 2.053020715713501\n",
      "Step: 729, Loss: 0.9160119295120239, Accuracy: 1.0, Computation time: 1.7695882320404053\n",
      "Step: 730, Loss: 0.9159968495368958, Accuracy: 1.0, Computation time: 1.4293420314788818\n",
      "Step: 731, Loss: 0.9161481857299805, Accuracy: 1.0, Computation time: 1.7155749797821045\n",
      "Step: 732, Loss: 0.9160805940628052, Accuracy: 1.0, Computation time: 2.3648598194122314\n",
      "Step: 733, Loss: 0.916165828704834, Accuracy: 1.0, Computation time: 2.116229295730591\n",
      "Step: 734, Loss: 0.9160503149032593, Accuracy: 1.0, Computation time: 2.2123615741729736\n",
      "Step: 735, Loss: 0.9373076558113098, Accuracy: 0.96875, Computation time: 2.1535377502441406\n",
      "Step: 736, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 2.1356709003448486\n",
      "Step: 737, Loss: 0.916003406047821, Accuracy: 1.0, Computation time: 2.3320443630218506\n",
      "Step: 738, Loss: 0.9160341620445251, Accuracy: 1.0, Computation time: 1.8381648063659668\n",
      "Step: 739, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.8410425186157227\n",
      "Step: 740, Loss: 0.9358943700790405, Accuracy: 0.96875, Computation time: 1.720644235610962\n",
      "Step: 741, Loss: 0.9348635673522949, Accuracy: 0.96875, Computation time: 1.9747045040130615\n",
      "Step: 742, Loss: 0.9161507487297058, Accuracy: 1.0, Computation time: 2.1205060482025146\n",
      "Step: 743, Loss: 0.9159991145133972, Accuracy: 1.0, Computation time: 1.797067403793335\n",
      "Step: 744, Loss: 0.9161720871925354, Accuracy: 1.0, Computation time: 1.9672152996063232\n",
      "Step: 745, Loss: 0.9165654182434082, Accuracy: 1.0, Computation time: 2.6048288345336914\n",
      "Step: 746, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.5820047855377197\n",
      "Step: 747, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 1.7063884735107422\n",
      "Step: 748, Loss: 0.9168533682823181, Accuracy: 1.0, Computation time: 2.290085792541504\n",
      "Step: 749, Loss: 0.9160574078559875, Accuracy: 1.0, Computation time: 1.8140513896942139\n",
      "Step: 750, Loss: 0.9159809350967407, Accuracy: 1.0, Computation time: 1.740569829940796\n",
      "Step: 751, Loss: 0.9379759430885315, Accuracy: 0.96875, Computation time: 1.9644808769226074\n",
      "Step: 752, Loss: 0.9159374833106995, Accuracy: 1.0, Computation time: 1.9538917541503906\n",
      "Step: 753, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 2.3704776763916016\n",
      "Step: 754, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.7196762561798096\n",
      "Step: 755, Loss: 0.9159670472145081, Accuracy: 1.0, Computation time: 1.5687487125396729\n",
      "Step: 756, Loss: 0.9170931577682495, Accuracy: 1.0, Computation time: 1.8220338821411133\n",
      "Step: 757, Loss: 0.9160500764846802, Accuracy: 1.0, Computation time: 1.8372299671173096\n",
      "Step: 758, Loss: 0.9164983630180359, Accuracy: 1.0, Computation time: 2.0162606239318848\n",
      "Step: 759, Loss: 0.9162700772285461, Accuracy: 1.0, Computation time: 1.7806134223937988\n",
      "Step: 760, Loss: 0.932910680770874, Accuracy: 0.96875, Computation time: 2.0786256790161133\n",
      "Step: 761, Loss: 0.9240254759788513, Accuracy: 1.0, Computation time: 2.123858690261841\n",
      "Step: 762, Loss: 0.9162179827690125, Accuracy: 1.0, Computation time: 1.691387414932251\n",
      "Step: 763, Loss: 0.9380090236663818, Accuracy: 0.96875, Computation time: 1.6550929546356201\n",
      "Step: 764, Loss: 0.9232813715934753, Accuracy: 1.0, Computation time: 2.1410775184631348\n",
      "Step: 765, Loss: 0.9165967702865601, Accuracy: 1.0, Computation time: 1.899993658065796\n",
      "Step: 766, Loss: 0.9172247052192688, Accuracy: 1.0, Computation time: 2.0718419551849365\n",
      "Step: 767, Loss: 0.9164289236068726, Accuracy: 1.0, Computation time: 1.7646949291229248\n",
      "Step: 768, Loss: 0.9164170622825623, Accuracy: 1.0, Computation time: 1.6574883460998535\n",
      "Step: 769, Loss: 0.9175477027893066, Accuracy: 1.0, Computation time: 2.3926079273223877\n",
      "Step: 770, Loss: 0.916168212890625, Accuracy: 1.0, Computation time: 2.343473196029663\n",
      "Step: 771, Loss: 0.9161652326583862, Accuracy: 1.0, Computation time: 1.7260193824768066\n",
      "Step: 772, Loss: 0.9162273406982422, Accuracy: 1.0, Computation time: 2.0805420875549316\n",
      "Step: 773, Loss: 0.9162448048591614, Accuracy: 1.0, Computation time: 1.9172379970550537\n",
      "Step: 774, Loss: 0.9360849857330322, Accuracy: 0.96875, Computation time: 2.2726941108703613\n",
      "Step: 775, Loss: 0.9202136397361755, Accuracy: 1.0, Computation time: 2.114612579345703\n",
      "Step: 776, Loss: 0.9161214828491211, Accuracy: 1.0, Computation time: 1.8317863941192627\n",
      "Step: 777, Loss: 0.9162828326225281, Accuracy: 1.0, Computation time: 2.00687575340271\n",
      "Step: 778, Loss: 0.9162740707397461, Accuracy: 1.0, Computation time: 2.22686505317688\n",
      "Step: 779, Loss: 0.9161463975906372, Accuracy: 1.0, Computation time: 2.1821506023406982\n",
      "Step: 780, Loss: 0.9162724018096924, Accuracy: 1.0, Computation time: 2.0875158309936523\n",
      "Step: 781, Loss: 0.9367854595184326, Accuracy: 0.96875, Computation time: 1.7043650150299072\n",
      "Step: 782, Loss: 0.9164426922798157, Accuracy: 1.0, Computation time: 2.1719372272491455\n",
      "Step: 783, Loss: 0.9168527722358704, Accuracy: 1.0, Computation time: 2.8272526264190674\n",
      "Step: 784, Loss: 0.9160525798797607, Accuracy: 1.0, Computation time: 1.7712857723236084\n",
      "Step: 785, Loss: 0.9163617491722107, Accuracy: 1.0, Computation time: 1.7727961540222168\n",
      "Step: 786, Loss: 0.9165120124816895, Accuracy: 1.0, Computation time: 1.8299732208251953\n",
      "Step: 787, Loss: 0.9162794947624207, Accuracy: 1.0, Computation time: 1.8344008922576904\n",
      "Step: 788, Loss: 0.9162921905517578, Accuracy: 1.0, Computation time: 1.9424898624420166\n",
      "Step: 789, Loss: 0.9161311388015747, Accuracy: 1.0, Computation time: 2.0974831581115723\n",
      "Step: 790, Loss: 0.916082501411438, Accuracy: 1.0, Computation time: 2.422790050506592\n",
      "Step: 791, Loss: 0.9160748720169067, Accuracy: 1.0, Computation time: 2.18896746635437\n",
      "Step: 792, Loss: 0.9159497022628784, Accuracy: 1.0, Computation time: 1.8374450206756592\n",
      "Step: 793, Loss: 0.9161921739578247, Accuracy: 1.0, Computation time: 1.7004976272583008\n",
      "Step: 794, Loss: 0.9162693023681641, Accuracy: 1.0, Computation time: 2.2622389793395996\n",
      "Step: 795, Loss: 0.9164832830429077, Accuracy: 1.0, Computation time: 2.2401461601257324\n",
      "Step: 796, Loss: 0.9294726252555847, Accuracy: 0.96875, Computation time: 2.384258985519409\n",
      "Step: 797, Loss: 0.9377602934837341, Accuracy: 0.96875, Computation time: 2.0472941398620605\n",
      "Step: 798, Loss: 0.9159579873085022, Accuracy: 1.0, Computation time: 1.5524516105651855\n",
      "Step: 799, Loss: 0.9371283650398254, Accuracy: 0.96875, Computation time: 2.275204658508301\n",
      "Step: 800, Loss: 0.9159952402114868, Accuracy: 1.0, Computation time: 1.5263841152191162\n",
      "Step: 801, Loss: 0.9371129870414734, Accuracy: 0.96875, Computation time: 2.1602895259857178\n",
      "Step: 802, Loss: 0.9165291786193848, Accuracy: 1.0, Computation time: 1.6988487243652344\n",
      "Step: 803, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.8404622077941895\n",
      "Step: 804, Loss: 0.9184005260467529, Accuracy: 1.0, Computation time: 2.1185898780822754\n",
      "Step: 805, Loss: 0.9173224568367004, Accuracy: 1.0, Computation time: 1.9261219501495361\n",
      "Step: 806, Loss: 0.937930703163147, Accuracy: 0.96875, Computation time: 1.880807638168335\n",
      "Step: 807, Loss: 0.9175440073013306, Accuracy: 1.0, Computation time: 1.981186866760254\n",
      "Step: 808, Loss: 0.9160909652709961, Accuracy: 1.0, Computation time: 2.1054656505584717\n",
      "Step: 809, Loss: 0.9169269800186157, Accuracy: 1.0, Computation time: 1.9949073791503906\n",
      "Step: 810, Loss: 0.9427290558815002, Accuracy: 0.96875, Computation time: 1.7659151554107666\n",
      "Step: 811, Loss: 0.9381070733070374, Accuracy: 0.96875, Computation time: 2.003328323364258\n",
      "Step: 812, Loss: 0.9160416722297668, Accuracy: 1.0, Computation time: 1.8153858184814453\n",
      "Step: 813, Loss: 0.9169727563858032, Accuracy: 1.0, Computation time: 2.0719008445739746\n",
      "Step: 814, Loss: 0.9161304235458374, Accuracy: 1.0, Computation time: 1.602660894393921\n",
      "Step: 815, Loss: 0.916145920753479, Accuracy: 1.0, Computation time: 2.502997636795044\n",
      "Step: 816, Loss: 0.9159348011016846, Accuracy: 1.0, Computation time: 1.946838140487671\n",
      "Step: 817, Loss: 0.9161514043807983, Accuracy: 1.0, Computation time: 1.954883337020874\n",
      "Step: 818, Loss: 0.9161384105682373, Accuracy: 1.0, Computation time: 2.0392940044403076\n",
      "Step: 819, Loss: 0.9174010157585144, Accuracy: 1.0, Computation time: 2.4482553005218506\n",
      "Step: 820, Loss: 0.9160115122795105, Accuracy: 1.0, Computation time: 2.0193729400634766\n",
      "Step: 821, Loss: 0.9160140752792358, Accuracy: 1.0, Computation time: 2.0725722312927246\n",
      "Step: 822, Loss: 0.9159508347511292, Accuracy: 1.0, Computation time: 2.1011133193969727\n",
      "Step: 823, Loss: 0.9161783456802368, Accuracy: 1.0, Computation time: 2.239602565765381\n",
      "Step: 824, Loss: 0.9389688968658447, Accuracy: 0.96875, Computation time: 2.3692426681518555\n",
      "Step: 825, Loss: 0.9160351753234863, Accuracy: 1.0, Computation time: 2.1984643936157227\n",
      "Step: 826, Loss: 0.91607266664505, Accuracy: 1.0, Computation time: 2.6016640663146973\n",
      "Step: 827, Loss: 0.916537880897522, Accuracy: 1.0, Computation time: 2.062753915786743\n",
      "Step: 828, Loss: 0.916032075881958, Accuracy: 1.0, Computation time: 2.206071138381958\n",
      "Step: 829, Loss: 0.9159740209579468, Accuracy: 1.0, Computation time: 2.3717405796051025\n",
      "Step: 830, Loss: 0.9159770011901855, Accuracy: 1.0, Computation time: 1.9085578918457031\n",
      "Step: 831, Loss: 0.928519070148468, Accuracy: 0.96875, Computation time: 2.152608633041382\n",
      "Step: 832, Loss: 0.9199697375297546, Accuracy: 1.0, Computation time: 2.044729709625244\n",
      "Step: 833, Loss: 0.9160566329956055, Accuracy: 1.0, Computation time: 2.024714708328247\n",
      "Step: 834, Loss: 0.9375019073486328, Accuracy: 0.96875, Computation time: 1.8694558143615723\n",
      "########################\n",
      "Test loss: 1.1223331689834595, Test Accuracy_epoch6: 0.6995391845703125\n",
      "########################\n",
      "Step: 835, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 2.0026695728302\n",
      "Step: 836, Loss: 0.9356131553649902, Accuracy: 0.96875, Computation time: 2.131300210952759\n",
      "Step: 837, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 2.4865381717681885\n",
      "Step: 838, Loss: 0.9160595536231995, Accuracy: 1.0, Computation time: 2.1617953777313232\n",
      "Step: 839, Loss: 0.9160689115524292, Accuracy: 1.0, Computation time: 2.3716063499450684\n",
      "Step: 840, Loss: 0.9377857446670532, Accuracy: 0.96875, Computation time: 2.239110231399536\n",
      "Step: 841, Loss: 0.9161099195480347, Accuracy: 1.0, Computation time: 2.0491085052490234\n",
      "Step: 842, Loss: 0.91595059633255, Accuracy: 1.0, Computation time: 2.168168544769287\n",
      "Step: 843, Loss: 0.9172160029411316, Accuracy: 1.0, Computation time: 2.108818769454956\n",
      "Step: 844, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 2.374835252761841\n",
      "Step: 845, Loss: 0.9178975224494934, Accuracy: 1.0, Computation time: 2.2555320262908936\n",
      "Step: 846, Loss: 0.9160901308059692, Accuracy: 1.0, Computation time: 2.085015296936035\n",
      "Step: 847, Loss: 0.9356422424316406, Accuracy: 0.96875, Computation time: 2.086620807647705\n",
      "Step: 848, Loss: 0.9165269732475281, Accuracy: 1.0, Computation time: 2.171151638031006\n",
      "Step: 849, Loss: 0.9366025924682617, Accuracy: 0.96875, Computation time: 2.122328042984009\n",
      "Step: 850, Loss: 0.9259159564971924, Accuracy: 0.96875, Computation time: 1.8502206802368164\n",
      "Step: 851, Loss: 0.9161779284477234, Accuracy: 1.0, Computation time: 1.9771006107330322\n",
      "Step: 852, Loss: 0.9162588715553284, Accuracy: 1.0, Computation time: 1.8151109218597412\n",
      "Step: 853, Loss: 0.9162304997444153, Accuracy: 1.0, Computation time: 1.664431095123291\n",
      "Step: 854, Loss: 0.9161578416824341, Accuracy: 1.0, Computation time: 2.0371906757354736\n",
      "Step: 855, Loss: 0.9161213040351868, Accuracy: 1.0, Computation time: 1.9837944507598877\n",
      "Step: 856, Loss: 0.9301552176475525, Accuracy: 0.96875, Computation time: 2.19284725189209\n",
      "Step: 857, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 1.9171350002288818\n",
      "Step: 858, Loss: 0.9162207841873169, Accuracy: 1.0, Computation time: 1.5790417194366455\n",
      "Step: 859, Loss: 0.9160991311073303, Accuracy: 1.0, Computation time: 1.751199722290039\n",
      "Step: 860, Loss: 0.9163535833358765, Accuracy: 1.0, Computation time: 2.394986629486084\n",
      "Step: 861, Loss: 0.9160750508308411, Accuracy: 1.0, Computation time: 1.8344027996063232\n",
      "Step: 862, Loss: 0.9160332083702087, Accuracy: 1.0, Computation time: 1.7629270553588867\n",
      "Step: 863, Loss: 0.9159886240959167, Accuracy: 1.0, Computation time: 1.7129182815551758\n",
      "Step: 864, Loss: 0.9383441805839539, Accuracy: 0.96875, Computation time: 2.1989948749542236\n",
      "Step: 865, Loss: 0.9160338640213013, Accuracy: 1.0, Computation time: 1.8108389377593994\n",
      "Step: 866, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 1.8769676685333252\n",
      "Step: 867, Loss: 0.9167059063911438, Accuracy: 1.0, Computation time: 1.720679521560669\n",
      "Step: 868, Loss: 0.9160555005073547, Accuracy: 1.0, Computation time: 2.113978862762451\n",
      "Step: 869, Loss: 0.9345303177833557, Accuracy: 0.96875, Computation time: 2.072223424911499\n",
      "Step: 870, Loss: 0.9178153276443481, Accuracy: 1.0, Computation time: 2.0798180103302\n",
      "Step: 871, Loss: 0.9159349203109741, Accuracy: 1.0, Computation time: 2.1440420150756836\n",
      "Step: 872, Loss: 0.9161664843559265, Accuracy: 1.0, Computation time: 1.5883994102478027\n",
      "Step: 873, Loss: 0.9198431968688965, Accuracy: 1.0, Computation time: 1.8091731071472168\n",
      "Step: 874, Loss: 0.9379686117172241, Accuracy: 0.96875, Computation time: 1.7659387588500977\n",
      "Step: 875, Loss: 0.9161872863769531, Accuracy: 1.0, Computation time: 1.7757985591888428\n",
      "Step: 876, Loss: 0.9159530997276306, Accuracy: 1.0, Computation time: 1.9090068340301514\n",
      "Step: 877, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.9284887313842773\n",
      "Step: 878, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 2.0940582752227783\n",
      "Step: 879, Loss: 0.9160850048065186, Accuracy: 1.0, Computation time: 1.910898208618164\n",
      "Step: 880, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.7597687244415283\n",
      "Step: 881, Loss: 0.9159793853759766, Accuracy: 1.0, Computation time: 1.971012830734253\n",
      "Step: 882, Loss: 0.9162346720695496, Accuracy: 1.0, Computation time: 1.7801916599273682\n",
      "Step: 883, Loss: 0.9159795641899109, Accuracy: 1.0, Computation time: 1.772458553314209\n",
      "Step: 884, Loss: 0.9162024259567261, Accuracy: 1.0, Computation time: 1.9707999229431152\n",
      "Step: 885, Loss: 0.9377809166908264, Accuracy: 0.96875, Computation time: 2.4794633388519287\n",
      "Step: 886, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 1.9393904209136963\n",
      "Step: 887, Loss: 0.9176596999168396, Accuracy: 1.0, Computation time: 2.709982395172119\n",
      "Step: 888, Loss: 0.9185057878494263, Accuracy: 1.0, Computation time: 1.958953619003296\n",
      "Step: 889, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 2.080294609069824\n",
      "Step: 890, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.763923168182373\n",
      "Step: 891, Loss: 0.9159810543060303, Accuracy: 1.0, Computation time: 1.7786779403686523\n",
      "Step: 892, Loss: 0.9159454107284546, Accuracy: 1.0, Computation time: 1.9162054061889648\n",
      "Step: 893, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 1.9761359691619873\n",
      "Step: 894, Loss: 0.9436425566673279, Accuracy: 0.96875, Computation time: 2.1415340900421143\n",
      "Step: 895, Loss: 0.9170406460762024, Accuracy: 1.0, Computation time: 1.8798871040344238\n",
      "Step: 896, Loss: 0.9160317778587341, Accuracy: 1.0, Computation time: 1.945004940032959\n",
      "Step: 897, Loss: 0.9161369204521179, Accuracy: 1.0, Computation time: 1.707348108291626\n",
      "Step: 898, Loss: 0.9160796403884888, Accuracy: 1.0, Computation time: 2.2702786922454834\n",
      "Step: 899, Loss: 0.9172359108924866, Accuracy: 1.0, Computation time: 1.9848980903625488\n",
      "Step: 900, Loss: 0.9579271674156189, Accuracy: 0.9375, Computation time: 2.384009599685669\n",
      "Step: 901, Loss: 0.9178309440612793, Accuracy: 1.0, Computation time: 1.9681477546691895\n",
      "Step: 902, Loss: 0.9502416849136353, Accuracy: 0.9375, Computation time: 1.6664354801177979\n",
      "Step: 903, Loss: 0.9161673188209534, Accuracy: 1.0, Computation time: 2.053831100463867\n",
      "Step: 904, Loss: 0.916256308555603, Accuracy: 1.0, Computation time: 2.318431854248047\n",
      "Step: 905, Loss: 0.9173250198364258, Accuracy: 1.0, Computation time: 2.6202239990234375\n",
      "Step: 906, Loss: 0.9160876274108887, Accuracy: 1.0, Computation time: 2.0604937076568604\n",
      "Step: 907, Loss: 0.9162734746932983, Accuracy: 1.0, Computation time: 1.9670565128326416\n",
      "Step: 908, Loss: 0.9161095023155212, Accuracy: 1.0, Computation time: 2.3149561882019043\n",
      "Step: 909, Loss: 0.9159377217292786, Accuracy: 1.0, Computation time: 1.8282251358032227\n",
      "Step: 910, Loss: 0.9161449074745178, Accuracy: 1.0, Computation time: 1.9714915752410889\n",
      "Step: 911, Loss: 0.9159181714057922, Accuracy: 1.0, Computation time: 1.7979044914245605\n",
      "Step: 912, Loss: 0.9159598350524902, Accuracy: 1.0, Computation time: 1.680112600326538\n",
      "Step: 913, Loss: 0.917174756526947, Accuracy: 1.0, Computation time: 2.4832465648651123\n",
      "Step: 914, Loss: 0.916087806224823, Accuracy: 1.0, Computation time: 2.515446186065674\n",
      "Step: 915, Loss: 0.9160902500152588, Accuracy: 1.0, Computation time: 1.5866682529449463\n",
      "Step: 916, Loss: 0.9162091016769409, Accuracy: 1.0, Computation time: 1.540116548538208\n",
      "Step: 917, Loss: 0.9159899950027466, Accuracy: 1.0, Computation time: 1.5952715873718262\n",
      "Step: 918, Loss: 0.9160078763961792, Accuracy: 1.0, Computation time: 1.6936695575714111\n",
      "Step: 919, Loss: 0.9379838705062866, Accuracy: 0.96875, Computation time: 1.7679729461669922\n",
      "Step: 920, Loss: 0.9378623962402344, Accuracy: 0.96875, Computation time: 1.5611109733581543\n",
      "Step: 921, Loss: 0.9160103797912598, Accuracy: 1.0, Computation time: 1.7795391082763672\n",
      "Step: 922, Loss: 0.9355257749557495, Accuracy: 0.96875, Computation time: 2.081191062927246\n",
      "Step: 923, Loss: 0.9385213255882263, Accuracy: 0.96875, Computation time: 2.2719364166259766\n",
      "Step: 924, Loss: 0.9385098218917847, Accuracy: 0.96875, Computation time: 1.835256814956665\n",
      "Step: 925, Loss: 0.9165357351303101, Accuracy: 1.0, Computation time: 2.4484927654266357\n",
      "Step: 926, Loss: 0.9370417594909668, Accuracy: 0.96875, Computation time: 1.7960951328277588\n",
      "Step: 927, Loss: 0.9163119792938232, Accuracy: 1.0, Computation time: 1.534860372543335\n",
      "Step: 928, Loss: 0.947425365447998, Accuracy: 0.9375, Computation time: 2.2733864784240723\n",
      "Step: 929, Loss: 0.9161499738693237, Accuracy: 1.0, Computation time: 1.5295062065124512\n",
      "Step: 930, Loss: 0.9161788821220398, Accuracy: 1.0, Computation time: 1.548224687576294\n",
      "Step: 931, Loss: 0.9162393808364868, Accuracy: 1.0, Computation time: 1.681950330734253\n",
      "Step: 932, Loss: 0.9378292560577393, Accuracy: 0.96875, Computation time: 1.7833969593048096\n",
      "Step: 933, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.7417373657226562\n",
      "Step: 934, Loss: 0.918476402759552, Accuracy: 1.0, Computation time: 1.5519556999206543\n",
      "Step: 935, Loss: 0.9159543514251709, Accuracy: 1.0, Computation time: 1.8427917957305908\n",
      "Step: 936, Loss: 0.9180647730827332, Accuracy: 1.0, Computation time: 1.7199316024780273\n",
      "Step: 937, Loss: 0.9314702153205872, Accuracy: 0.96875, Computation time: 1.8477609157562256\n",
      "Step: 938, Loss: 0.9162730574607849, Accuracy: 1.0, Computation time: 2.033466100692749\n",
      "Step: 939, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.9737379550933838\n",
      "Step: 940, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 1.9349265098571777\n",
      "Step: 941, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.4991858005523682\n",
      "Step: 942, Loss: 0.9162274599075317, Accuracy: 1.0, Computation time: 1.7733345031738281\n",
      "Step: 943, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.737778663635254\n",
      "Step: 944, Loss: 0.9159999489784241, Accuracy: 1.0, Computation time: 1.8749911785125732\n",
      "Step: 945, Loss: 0.9163307547569275, Accuracy: 1.0, Computation time: 1.8017332553863525\n",
      "Step: 946, Loss: 0.9159168601036072, Accuracy: 1.0, Computation time: 1.9778971672058105\n",
      "Step: 947, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 1.5526022911071777\n",
      "Step: 948, Loss: 0.9158921837806702, Accuracy: 1.0, Computation time: 1.927943229675293\n",
      "Step: 949, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.7593247890472412\n",
      "Step: 950, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 1.8930449485778809\n",
      "Step: 951, Loss: 0.9376663565635681, Accuracy: 0.96875, Computation time: 1.574115514755249\n",
      "Step: 952, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.684861183166504\n",
      "Step: 953, Loss: 0.9387054443359375, Accuracy: 0.96875, Computation time: 1.5858144760131836\n",
      "Step: 954, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.9227705001831055\n",
      "Step: 955, Loss: 0.9369209408760071, Accuracy: 0.96875, Computation time: 1.9047329425811768\n",
      "Step: 956, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.876112461090088\n",
      "Step: 957, Loss: 0.9159672856330872, Accuracy: 1.0, Computation time: 1.440678358078003\n",
      "Step: 958, Loss: 0.9320651292800903, Accuracy: 0.96875, Computation time: 2.3799026012420654\n",
      "Step: 959, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.945897102355957\n",
      "Step: 960, Loss: 0.9162784814834595, Accuracy: 1.0, Computation time: 1.6947588920593262\n",
      "Step: 961, Loss: 0.9231342077255249, Accuracy: 1.0, Computation time: 1.8764195442199707\n",
      "Step: 962, Loss: 0.9160447716712952, Accuracy: 1.0, Computation time: 1.8528039455413818\n",
      "Step: 963, Loss: 0.9162588715553284, Accuracy: 1.0, Computation time: 1.5085339546203613\n",
      "Step: 964, Loss: 0.9161877632141113, Accuracy: 1.0, Computation time: 1.989438772201538\n",
      "Step: 965, Loss: 0.9382007122039795, Accuracy: 0.96875, Computation time: 1.6132500171661377\n",
      "Step: 966, Loss: 0.9160048365592957, Accuracy: 1.0, Computation time: 1.4210693836212158\n",
      "Step: 967, Loss: 0.916052520275116, Accuracy: 1.0, Computation time: 1.889336109161377\n",
      "Step: 968, Loss: 0.9165846705436707, Accuracy: 1.0, Computation time: 1.8924517631530762\n",
      "Step: 969, Loss: 0.9160195589065552, Accuracy: 1.0, Computation time: 2.0640947818756104\n",
      "Step: 970, Loss: 0.9160130023956299, Accuracy: 1.0, Computation time: 1.9713099002838135\n",
      "Step: 971, Loss: 0.9160025715827942, Accuracy: 1.0, Computation time: 1.666550874710083\n",
      "Step: 972, Loss: 0.9161903858184814, Accuracy: 1.0, Computation time: 2.238154172897339\n",
      "Step: 973, Loss: 0.9160766005516052, Accuracy: 1.0, Computation time: 2.208749294281006\n",
      "########################\n",
      "Test loss: 1.1284866333007812, Test Accuracy_epoch7: 0.6894009113311768\n",
      "########################\n",
      "Step: 974, Loss: 0.9334035515785217, Accuracy: 0.96875, Computation time: 2.312264919281006\n",
      "Step: 975, Loss: 0.9301184415817261, Accuracy: 0.96875, Computation time: 2.0615265369415283\n",
      "Step: 976, Loss: 0.9159876108169556, Accuracy: 1.0, Computation time: 1.9185600280761719\n",
      "Step: 977, Loss: 0.9287174940109253, Accuracy: 0.96875, Computation time: 2.054037570953369\n",
      "Step: 978, Loss: 0.9161043763160706, Accuracy: 1.0, Computation time: 1.60109281539917\n",
      "Step: 979, Loss: 0.9188288450241089, Accuracy: 1.0, Computation time: 1.8831446170806885\n",
      "Step: 980, Loss: 0.9378693103790283, Accuracy: 0.96875, Computation time: 2.3110427856445312\n",
      "Step: 981, Loss: 0.9167743921279907, Accuracy: 1.0, Computation time: 2.036620855331421\n",
      "Step: 982, Loss: 0.9160348176956177, Accuracy: 1.0, Computation time: 1.5450975894927979\n",
      "Step: 983, Loss: 0.9160155653953552, Accuracy: 1.0, Computation time: 1.6768567562103271\n",
      "Step: 984, Loss: 0.9161872267723083, Accuracy: 1.0, Computation time: 2.1023457050323486\n",
      "Step: 985, Loss: 0.9246732592582703, Accuracy: 1.0, Computation time: 2.241398334503174\n",
      "Step: 986, Loss: 0.9163433909416199, Accuracy: 1.0, Computation time: 2.210242748260498\n",
      "Step: 987, Loss: 0.9181098341941833, Accuracy: 1.0, Computation time: 1.5751011371612549\n",
      "Step: 988, Loss: 0.9210211038589478, Accuracy: 1.0, Computation time: 2.0984795093536377\n",
      "Step: 989, Loss: 0.9159528613090515, Accuracy: 1.0, Computation time: 1.5353591442108154\n",
      "Step: 990, Loss: 0.93752121925354, Accuracy: 0.96875, Computation time: 1.978919267654419\n",
      "Step: 991, Loss: 0.9160791039466858, Accuracy: 1.0, Computation time: 1.9568486213684082\n",
      "Step: 992, Loss: 0.9160943627357483, Accuracy: 1.0, Computation time: 1.7196309566497803\n",
      "Step: 993, Loss: 0.9159932732582092, Accuracy: 1.0, Computation time: 1.9306972026824951\n",
      "Step: 994, Loss: 0.9377961754798889, Accuracy: 0.96875, Computation time: 1.913780927658081\n",
      "Step: 995, Loss: 0.9377667903900146, Accuracy: 0.96875, Computation time: 1.7105128765106201\n",
      "Step: 996, Loss: 0.9175371527671814, Accuracy: 1.0, Computation time: 2.6469640731811523\n",
      "Step: 997, Loss: 0.9160988926887512, Accuracy: 1.0, Computation time: 2.004556655883789\n",
      "Step: 998, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.9727520942687988\n",
      "Step: 999, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 1.5150601863861084\n",
      "Step: 1000, Loss: 0.9164236783981323, Accuracy: 1.0, Computation time: 1.5054843425750732\n",
      "Step: 1001, Loss: 0.9432659149169922, Accuracy: 0.96875, Computation time: 2.137359619140625\n",
      "Step: 1002, Loss: 0.9165024161338806, Accuracy: 1.0, Computation time: 1.6749460697174072\n",
      "Step: 1003, Loss: 0.9160280823707581, Accuracy: 1.0, Computation time: 1.8507423400878906\n",
      "Step: 1004, Loss: 0.9160440564155579, Accuracy: 1.0, Computation time: 2.2177574634552\n",
      "Step: 1005, Loss: 0.9161659479141235, Accuracy: 1.0, Computation time: 1.997419834136963\n",
      "Step: 1006, Loss: 0.9159466624259949, Accuracy: 1.0, Computation time: 1.7691936492919922\n",
      "Step: 1007, Loss: 0.9263460636138916, Accuracy: 0.96875, Computation time: 1.7597274780273438\n",
      "Step: 1008, Loss: 0.9168704152107239, Accuracy: 1.0, Computation time: 1.5285704135894775\n",
      "Step: 1009, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 1.704993724822998\n",
      "Step: 1010, Loss: 0.9161513447761536, Accuracy: 1.0, Computation time: 2.1707022190093994\n",
      "Step: 1011, Loss: 0.9159703254699707, Accuracy: 1.0, Computation time: 1.7771821022033691\n",
      "Step: 1012, Loss: 0.916087806224823, Accuracy: 1.0, Computation time: 1.9928936958312988\n",
      "Step: 1013, Loss: 0.9161405563354492, Accuracy: 1.0, Computation time: 2.0969181060791016\n",
      "Step: 1014, Loss: 0.9380254149436951, Accuracy: 0.96875, Computation time: 1.7151579856872559\n",
      "Step: 1015, Loss: 0.9159443378448486, Accuracy: 1.0, Computation time: 1.9082729816436768\n",
      "Step: 1016, Loss: 0.9357376098632812, Accuracy: 0.96875, Computation time: 1.6634001731872559\n",
      "Step: 1017, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 1.4538860321044922\n",
      "Step: 1018, Loss: 0.9160178899765015, Accuracy: 1.0, Computation time: 1.9323372840881348\n",
      "Step: 1019, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 1.6386055946350098\n",
      "Step: 1020, Loss: 0.9159368276596069, Accuracy: 1.0, Computation time: 1.430736780166626\n",
      "Step: 1021, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 1.9843871593475342\n",
      "Step: 1022, Loss: 0.9160333275794983, Accuracy: 1.0, Computation time: 1.6129062175750732\n",
      "Step: 1023, Loss: 0.9159122109413147, Accuracy: 1.0, Computation time: 1.510321855545044\n",
      "Step: 1024, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 2.251305103302002\n",
      "Step: 1025, Loss: 0.9159267544746399, Accuracy: 1.0, Computation time: 2.2687008380889893\n",
      "Step: 1026, Loss: 0.9165539145469666, Accuracy: 1.0, Computation time: 1.7371001243591309\n",
      "Step: 1027, Loss: 0.9170825481414795, Accuracy: 1.0, Computation time: 1.9320173263549805\n",
      "Step: 1028, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 1.6593806743621826\n",
      "Step: 1029, Loss: 0.9375749230384827, Accuracy: 0.96875, Computation time: 1.6294641494750977\n",
      "Step: 1030, Loss: 0.9160966277122498, Accuracy: 1.0, Computation time: 1.6422839164733887\n",
      "Step: 1031, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 1.499101161956787\n",
      "Step: 1032, Loss: 0.9161550402641296, Accuracy: 1.0, Computation time: 1.7067058086395264\n",
      "Step: 1033, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.5655779838562012\n",
      "Step: 1034, Loss: 0.9162204265594482, Accuracy: 1.0, Computation time: 2.0950489044189453\n",
      "Step: 1035, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 1.6385290622711182\n",
      "Step: 1036, Loss: 0.9159932136535645, Accuracy: 1.0, Computation time: 1.937912940979004\n",
      "Step: 1037, Loss: 0.9354254603385925, Accuracy: 0.96875, Computation time: 1.8748888969421387\n",
      "Step: 1038, Loss: 0.9160158634185791, Accuracy: 1.0, Computation time: 2.386509418487549\n",
      "Step: 1039, Loss: 0.9159876108169556, Accuracy: 1.0, Computation time: 1.8434803485870361\n",
      "Step: 1040, Loss: 0.9227095246315002, Accuracy: 1.0, Computation time: 2.1259098052978516\n",
      "Step: 1041, Loss: 0.916676938533783, Accuracy: 1.0, Computation time: 2.3431475162506104\n",
      "Step: 1042, Loss: 0.9220189452171326, Accuracy: 1.0, Computation time: 1.7073886394500732\n",
      "Step: 1043, Loss: 0.916007936000824, Accuracy: 1.0, Computation time: 1.587813138961792\n",
      "Step: 1044, Loss: 0.9216755628585815, Accuracy: 1.0, Computation time: 2.030080795288086\n",
      "Step: 1045, Loss: 0.9366646409034729, Accuracy: 0.96875, Computation time: 1.7316749095916748\n",
      "Step: 1046, Loss: 0.9161086678504944, Accuracy: 1.0, Computation time: 1.7403411865234375\n",
      "Step: 1047, Loss: 0.9376845359802246, Accuracy: 0.96875, Computation time: 1.892807960510254\n",
      "Step: 1048, Loss: 0.9161164164543152, Accuracy: 1.0, Computation time: 1.8464982509613037\n",
      "Step: 1049, Loss: 0.9160602688789368, Accuracy: 1.0, Computation time: 2.0982391834259033\n",
      "Step: 1050, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 1.8394551277160645\n",
      "Step: 1051, Loss: 0.9159641265869141, Accuracy: 1.0, Computation time: 1.928236722946167\n",
      "Step: 1052, Loss: 0.9160247445106506, Accuracy: 1.0, Computation time: 2.5707550048828125\n",
      "Step: 1053, Loss: 0.9359664916992188, Accuracy: 0.96875, Computation time: 1.7914533615112305\n",
      "Step: 1054, Loss: 0.9190706014633179, Accuracy: 1.0, Computation time: 1.9257252216339111\n",
      "Step: 1055, Loss: 0.9424514770507812, Accuracy: 0.96875, Computation time: 2.3059000968933105\n",
      "Step: 1056, Loss: 0.9369851350784302, Accuracy: 0.96875, Computation time: 1.8719048500061035\n",
      "Step: 1057, Loss: 0.916023850440979, Accuracy: 1.0, Computation time: 2.028505325317383\n",
      "Step: 1058, Loss: 0.9377681612968445, Accuracy: 0.96875, Computation time: 1.9363963603973389\n",
      "Step: 1059, Loss: 0.9164508581161499, Accuracy: 1.0, Computation time: 1.8916280269622803\n",
      "Step: 1060, Loss: 0.9163104295730591, Accuracy: 1.0, Computation time: 1.8167362213134766\n",
      "Step: 1061, Loss: 0.9160645604133606, Accuracy: 1.0, Computation time: 1.959791660308838\n",
      "Step: 1062, Loss: 0.9188705682754517, Accuracy: 1.0, Computation time: 1.7500553131103516\n",
      "Step: 1063, Loss: 0.9322488307952881, Accuracy: 0.96875, Computation time: 2.100839376449585\n",
      "Step: 1064, Loss: 0.9159849882125854, Accuracy: 1.0, Computation time: 2.102308750152588\n",
      "Step: 1065, Loss: 0.9163615703582764, Accuracy: 1.0, Computation time: 2.3207693099975586\n",
      "Step: 1066, Loss: 0.9161888957023621, Accuracy: 1.0, Computation time: 1.7719616889953613\n",
      "Step: 1067, Loss: 0.9162259697914124, Accuracy: 1.0, Computation time: 1.9196884632110596\n",
      "Step: 1068, Loss: 0.9161169528961182, Accuracy: 1.0, Computation time: 2.2264106273651123\n",
      "Step: 1069, Loss: 0.9160958528518677, Accuracy: 1.0, Computation time: 1.9945242404937744\n",
      "Step: 1070, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 1.8709628582000732\n",
      "Step: 1071, Loss: 0.9162887930870056, Accuracy: 1.0, Computation time: 1.871506690979004\n",
      "Step: 1072, Loss: 0.9159452319145203, Accuracy: 1.0, Computation time: 1.9959840774536133\n",
      "Step: 1073, Loss: 0.9169259071350098, Accuracy: 1.0, Computation time: 2.1172873973846436\n",
      "Step: 1074, Loss: 0.9159025549888611, Accuracy: 1.0, Computation time: 1.6348705291748047\n",
      "Step: 1075, Loss: 0.9162952303886414, Accuracy: 1.0, Computation time: 1.9629170894622803\n",
      "Step: 1076, Loss: 0.9159808158874512, Accuracy: 1.0, Computation time: 1.5423071384429932\n",
      "Step: 1077, Loss: 0.9161484837532043, Accuracy: 1.0, Computation time: 1.8453209400177002\n",
      "Step: 1078, Loss: 0.9160611033439636, Accuracy: 1.0, Computation time: 1.8173599243164062\n",
      "Step: 1079, Loss: 0.9159696102142334, Accuracy: 1.0, Computation time: 2.069009304046631\n",
      "Step: 1080, Loss: 0.9160154461860657, Accuracy: 1.0, Computation time: 1.4477622509002686\n",
      "Step: 1081, Loss: 0.916222870349884, Accuracy: 1.0, Computation time: 1.9543333053588867\n",
      "Step: 1082, Loss: 0.91597580909729, Accuracy: 1.0, Computation time: 1.608626127243042\n",
      "Step: 1083, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 1.7874085903167725\n",
      "Step: 1084, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 1.6089892387390137\n",
      "Step: 1085, Loss: 0.9219700694084167, Accuracy: 1.0, Computation time: 2.064091205596924\n",
      "Step: 1086, Loss: 0.9209045767784119, Accuracy: 1.0, Computation time: 2.285752296447754\n",
      "Step: 1087, Loss: 0.9170096516609192, Accuracy: 1.0, Computation time: 2.3122122287750244\n",
      "Step: 1088, Loss: 0.9160359501838684, Accuracy: 1.0, Computation time: 2.316777467727661\n",
      "Step: 1089, Loss: 0.9161286354064941, Accuracy: 1.0, Computation time: 2.010446786880493\n",
      "Step: 1090, Loss: 0.929333508014679, Accuracy: 0.96875, Computation time: 1.8581397533416748\n",
      "Step: 1091, Loss: 0.9172440767288208, Accuracy: 1.0, Computation time: 1.872936487197876\n",
      "Step: 1092, Loss: 0.9593537449836731, Accuracy: 0.9375, Computation time: 1.837498664855957\n",
      "Step: 1093, Loss: 0.9380887150764465, Accuracy: 0.96875, Computation time: 2.04531192779541\n",
      "Step: 1094, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 2.2954671382904053\n",
      "Step: 1095, Loss: 0.9159842133522034, Accuracy: 1.0, Computation time: 2.0843567848205566\n",
      "Step: 1096, Loss: 0.9162119030952454, Accuracy: 1.0, Computation time: 2.030710458755493\n",
      "Step: 1097, Loss: 0.9159573316574097, Accuracy: 1.0, Computation time: 2.1182069778442383\n",
      "Step: 1098, Loss: 0.9160681962966919, Accuracy: 1.0, Computation time: 2.300443410873413\n",
      "Step: 1099, Loss: 0.9160032868385315, Accuracy: 1.0, Computation time: 1.9210572242736816\n",
      "Step: 1100, Loss: 0.9159873723983765, Accuracy: 1.0, Computation time: 2.208218812942505\n",
      "Step: 1101, Loss: 0.9377347826957703, Accuracy: 0.96875, Computation time: 1.9797453880310059\n",
      "Step: 1102, Loss: 0.9160369634628296, Accuracy: 1.0, Computation time: 1.9297950267791748\n",
      "Step: 1103, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.6831936836242676\n",
      "Step: 1104, Loss: 0.9160535931587219, Accuracy: 1.0, Computation time: 2.265158176422119\n",
      "Step: 1105, Loss: 0.9168812036514282, Accuracy: 1.0, Computation time: 2.286616325378418\n",
      "Step: 1106, Loss: 0.9160277843475342, Accuracy: 1.0, Computation time: 1.9248793125152588\n",
      "Step: 1107, Loss: 0.9192816615104675, Accuracy: 1.0, Computation time: 2.2345125675201416\n",
      "Step: 1108, Loss: 0.9365162253379822, Accuracy: 0.96875, Computation time: 1.9710063934326172\n",
      "Step: 1109, Loss: 0.9165080785751343, Accuracy: 1.0, Computation time: 2.1265342235565186\n",
      "Step: 1110, Loss: 0.9166166186332703, Accuracy: 1.0, Computation time: 2.6466660499572754\n",
      "Step: 1111, Loss: 0.9383897185325623, Accuracy: 0.96875, Computation time: 2.0021440982818604\n",
      "Step: 1112, Loss: 0.9176913499832153, Accuracy: 1.0, Computation time: 1.9594597816467285\n",
      "########################\n",
      "Test loss: 1.1156378984451294, Test Accuracy_epoch8: 0.7133640646934509\n",
      "########################\n",
      "Step: 1113, Loss: 0.916486382484436, Accuracy: 1.0, Computation time: 1.9891364574432373\n",
      "Step: 1114, Loss: 0.9160048961639404, Accuracy: 1.0, Computation time: 2.058164358139038\n",
      "Step: 1115, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 1.6207947731018066\n",
      "Step: 1116, Loss: 0.9180428385734558, Accuracy: 1.0, Computation time: 2.0852839946746826\n",
      "Step: 1117, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.8651487827301025\n",
      "Step: 1118, Loss: 0.916171133518219, Accuracy: 1.0, Computation time: 1.6944422721862793\n",
      "Step: 1119, Loss: 0.9163685441017151, Accuracy: 1.0, Computation time: 1.5728249549865723\n",
      "Step: 1120, Loss: 0.916103720664978, Accuracy: 1.0, Computation time: 1.4430558681488037\n",
      "Step: 1121, Loss: 0.9172265529632568, Accuracy: 1.0, Computation time: 2.185835123062134\n",
      "Step: 1122, Loss: 0.9160683751106262, Accuracy: 1.0, Computation time: 1.5253326892852783\n",
      "Step: 1123, Loss: 0.9160465002059937, Accuracy: 1.0, Computation time: 1.9119253158569336\n",
      "Step: 1124, Loss: 0.9162023663520813, Accuracy: 1.0, Computation time: 1.7538738250732422\n",
      "Step: 1125, Loss: 0.9159561991691589, Accuracy: 1.0, Computation time: 1.8884057998657227\n",
      "Step: 1126, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 1.6857683658599854\n",
      "Step: 1127, Loss: 0.9159725904464722, Accuracy: 1.0, Computation time: 1.9036273956298828\n",
      "Step: 1128, Loss: 0.9161144495010376, Accuracy: 1.0, Computation time: 1.9079387187957764\n",
      "Step: 1129, Loss: 0.937755823135376, Accuracy: 0.96875, Computation time: 1.7619633674621582\n",
      "Step: 1130, Loss: 0.9166336059570312, Accuracy: 1.0, Computation time: 2.0312705039978027\n",
      "Step: 1131, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.6904785633087158\n",
      "Step: 1132, Loss: 0.937991201877594, Accuracy: 0.96875, Computation time: 2.190948724746704\n",
      "Step: 1133, Loss: 0.9159988164901733, Accuracy: 1.0, Computation time: 1.8377504348754883\n",
      "Step: 1134, Loss: 0.9160178899765015, Accuracy: 1.0, Computation time: 2.1931982040405273\n",
      "Step: 1135, Loss: 0.9170145392417908, Accuracy: 1.0, Computation time: 1.7846319675445557\n",
      "Step: 1136, Loss: 0.9159761667251587, Accuracy: 1.0, Computation time: 1.6369423866271973\n",
      "Step: 1137, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 1.9311656951904297\n",
      "Step: 1138, Loss: 0.9159516096115112, Accuracy: 1.0, Computation time: 1.6940178871154785\n",
      "Step: 1139, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 2.2497341632843018\n",
      "Step: 1140, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.7796368598937988\n",
      "Step: 1141, Loss: 0.9160356521606445, Accuracy: 1.0, Computation time: 2.0621960163116455\n",
      "Step: 1142, Loss: 0.9377652406692505, Accuracy: 0.96875, Computation time: 1.85471510887146\n",
      "Step: 1143, Loss: 0.9162325263023376, Accuracy: 1.0, Computation time: 1.4425253868103027\n",
      "Step: 1144, Loss: 0.939978301525116, Accuracy: 0.96875, Computation time: 2.188509941101074\n",
      "Step: 1145, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 2.3876662254333496\n",
      "Step: 1146, Loss: 0.9161765575408936, Accuracy: 1.0, Computation time: 2.265587329864502\n",
      "Step: 1147, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.370703935623169\n",
      "Step: 1148, Loss: 0.9159609079360962, Accuracy: 1.0, Computation time: 1.8505754470825195\n",
      "Step: 1149, Loss: 0.9375564455986023, Accuracy: 0.96875, Computation time: 1.638300895690918\n",
      "Step: 1150, Loss: 0.9162118434906006, Accuracy: 1.0, Computation time: 1.7878222465515137\n",
      "Step: 1151, Loss: 0.9369170069694519, Accuracy: 0.96875, Computation time: 1.8454878330230713\n",
      "Step: 1152, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.779806137084961\n",
      "Step: 1153, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 1.9354474544525146\n",
      "Step: 1154, Loss: 0.9164412021636963, Accuracy: 1.0, Computation time: 1.6103792190551758\n",
      "Step: 1155, Loss: 0.91819828748703, Accuracy: 1.0, Computation time: 2.1202170848846436\n",
      "Step: 1156, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 1.6299028396606445\n",
      "Step: 1157, Loss: 0.915922999382019, Accuracy: 1.0, Computation time: 1.7772917747497559\n",
      "Step: 1158, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 1.964975118637085\n",
      "Step: 1159, Loss: 0.915959894657135, Accuracy: 1.0, Computation time: 1.6301121711730957\n",
      "Step: 1160, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 2.170707941055298\n",
      "Step: 1161, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.9639153480529785\n",
      "Step: 1162, Loss: 0.9172496795654297, Accuracy: 1.0, Computation time: 2.145338296890259\n",
      "Step: 1163, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 2.0510518550872803\n",
      "Step: 1164, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 1.4956486225128174\n",
      "Step: 1165, Loss: 0.9378930330276489, Accuracy: 0.96875, Computation time: 2.571021795272827\n",
      "Step: 1166, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.99432373046875\n",
      "Step: 1167, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 2.1755223274230957\n",
      "Step: 1168, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.7974157333374023\n",
      "Step: 1169, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 2.422030448913574\n",
      "Step: 1170, Loss: 0.9159772992134094, Accuracy: 1.0, Computation time: 2.0819883346557617\n",
      "Step: 1171, Loss: 0.9159506559371948, Accuracy: 1.0, Computation time: 2.3357725143432617\n",
      "Step: 1172, Loss: 0.9373534917831421, Accuracy: 0.96875, Computation time: 2.151637554168701\n",
      "Step: 1173, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.987861156463623\n",
      "Step: 1174, Loss: 0.9159070253372192, Accuracy: 1.0, Computation time: 2.0463104248046875\n",
      "Step: 1175, Loss: 0.9161522388458252, Accuracy: 1.0, Computation time: 2.4233908653259277\n",
      "Step: 1176, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 2.531768560409546\n",
      "Step: 1177, Loss: 0.9362926483154297, Accuracy: 0.96875, Computation time: 2.496952533721924\n",
      "Step: 1178, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 2.1760611534118652\n",
      "Step: 1179, Loss: 0.9162768125534058, Accuracy: 1.0, Computation time: 2.3547415733337402\n",
      "Step: 1180, Loss: 0.9160245656967163, Accuracy: 1.0, Computation time: 2.2635295391082764\n",
      "Step: 1181, Loss: 0.9250569343566895, Accuracy: 1.0, Computation time: 2.4230310916900635\n",
      "Step: 1182, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 2.324209451675415\n",
      "Step: 1183, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 2.0421292781829834\n",
      "Step: 1184, Loss: 0.9160003066062927, Accuracy: 1.0, Computation time: 2.070267915725708\n",
      "Step: 1185, Loss: 0.9380606412887573, Accuracy: 0.96875, Computation time: 1.9560320377349854\n",
      "Step: 1186, Loss: 0.9195435047149658, Accuracy: 1.0, Computation time: 2.094752311706543\n",
      "Step: 1187, Loss: 0.9159407615661621, Accuracy: 1.0, Computation time: 2.1308867931365967\n",
      "Step: 1188, Loss: 0.9184550046920776, Accuracy: 1.0, Computation time: 2.0942959785461426\n",
      "Step: 1189, Loss: 0.9170309901237488, Accuracy: 1.0, Computation time: 1.9406378269195557\n",
      "Step: 1190, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 1.851794719696045\n",
      "Step: 1191, Loss: 0.9165822267532349, Accuracy: 1.0, Computation time: 2.0597195625305176\n",
      "Step: 1192, Loss: 0.9160366058349609, Accuracy: 1.0, Computation time: 2.0894315242767334\n",
      "Step: 1193, Loss: 0.9378070831298828, Accuracy: 0.96875, Computation time: 1.8927946090698242\n",
      "Step: 1194, Loss: 0.9161639213562012, Accuracy: 1.0, Computation time: 2.0170180797576904\n",
      "Step: 1195, Loss: 0.9161627292633057, Accuracy: 1.0, Computation time: 2.3605916500091553\n",
      "Step: 1196, Loss: 0.9347272515296936, Accuracy: 0.96875, Computation time: 2.004873514175415\n",
      "Step: 1197, Loss: 0.9161732792854309, Accuracy: 1.0, Computation time: 2.069028377532959\n",
      "Step: 1198, Loss: 0.9159908890724182, Accuracy: 1.0, Computation time: 1.9291553497314453\n",
      "Step: 1199, Loss: 0.916153073310852, Accuracy: 1.0, Computation time: 1.9442343711853027\n",
      "Step: 1200, Loss: 0.9160303473472595, Accuracy: 1.0, Computation time: 2.046969175338745\n",
      "Step: 1201, Loss: 0.91720050573349, Accuracy: 1.0, Computation time: 1.8185279369354248\n",
      "Step: 1202, Loss: 0.9159801602363586, Accuracy: 1.0, Computation time: 2.3564674854278564\n",
      "Step: 1203, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 2.471940040588379\n",
      "Step: 1204, Loss: 0.9159169793128967, Accuracy: 1.0, Computation time: 2.0360910892486572\n",
      "Step: 1205, Loss: 0.9162794947624207, Accuracy: 1.0, Computation time: 2.0120272636413574\n",
      "Step: 1206, Loss: 0.9159480929374695, Accuracy: 1.0, Computation time: 2.010340452194214\n",
      "Step: 1207, Loss: 0.9161735773086548, Accuracy: 1.0, Computation time: 2.1197195053100586\n",
      "Step: 1208, Loss: 0.9167605638504028, Accuracy: 1.0, Computation time: 2.3443541526794434\n",
      "Step: 1209, Loss: 0.9296215772628784, Accuracy: 0.96875, Computation time: 2.262387990951538\n",
      "Step: 1210, Loss: 0.916751503944397, Accuracy: 1.0, Computation time: 2.5826821327209473\n",
      "Step: 1211, Loss: 0.9160497188568115, Accuracy: 1.0, Computation time: 2.3023159503936768\n",
      "Step: 1212, Loss: 0.9515536427497864, Accuracy: 0.9375, Computation time: 2.309736967086792\n",
      "Step: 1213, Loss: 0.9252343773841858, Accuracy: 1.0, Computation time: 2.344604015350342\n",
      "Step: 1214, Loss: 0.9162553548812866, Accuracy: 1.0, Computation time: 2.222125291824341\n",
      "Step: 1215, Loss: 0.9161362648010254, Accuracy: 1.0, Computation time: 1.8836231231689453\n",
      "Step: 1216, Loss: 0.9166094660758972, Accuracy: 1.0, Computation time: 2.4922220706939697\n",
      "Step: 1217, Loss: 0.9186468720436096, Accuracy: 1.0, Computation time: 1.9509596824645996\n",
      "Step: 1218, Loss: 0.9171631932258606, Accuracy: 1.0, Computation time: 2.3492379188537598\n",
      "Step: 1219, Loss: 0.9167983531951904, Accuracy: 1.0, Computation time: 1.7535488605499268\n",
      "Step: 1220, Loss: 0.91668301820755, Accuracy: 1.0, Computation time: 2.2260937690734863\n",
      "Step: 1221, Loss: 0.9162512421607971, Accuracy: 1.0, Computation time: 1.8052000999450684\n",
      "Step: 1222, Loss: 0.9355862736701965, Accuracy: 0.96875, Computation time: 2.061769723892212\n",
      "Step: 1223, Loss: 0.9486105442047119, Accuracy: 0.9375, Computation time: 2.01475191116333\n",
      "Step: 1224, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.8174018859863281\n",
      "Step: 1225, Loss: 0.916307270526886, Accuracy: 1.0, Computation time: 1.8767485618591309\n",
      "Step: 1226, Loss: 0.9272542595863342, Accuracy: 0.96875, Computation time: 2.074387550354004\n",
      "Step: 1227, Loss: 0.9163413047790527, Accuracy: 1.0, Computation time: 1.6396369934082031\n",
      "Step: 1228, Loss: 0.9323079586029053, Accuracy: 0.96875, Computation time: 2.523090362548828\n",
      "Step: 1229, Loss: 0.9162826538085938, Accuracy: 1.0, Computation time: 1.765152931213379\n",
      "Step: 1230, Loss: 0.9163074493408203, Accuracy: 1.0, Computation time: 1.6119108200073242\n",
      "Step: 1231, Loss: 0.9162822961807251, Accuracy: 1.0, Computation time: 2.0149190425872803\n",
      "Step: 1232, Loss: 0.9161215424537659, Accuracy: 1.0, Computation time: 1.9618115425109863\n",
      "Step: 1233, Loss: 0.937538743019104, Accuracy: 0.96875, Computation time: 1.7106478214263916\n",
      "Step: 1234, Loss: 0.9160805344581604, Accuracy: 1.0, Computation time: 1.9700005054473877\n",
      "Step: 1235, Loss: 0.937637448310852, Accuracy: 0.96875, Computation time: 1.7439672946929932\n",
      "Step: 1236, Loss: 0.9159787893295288, Accuracy: 1.0, Computation time: 1.7263565063476562\n",
      "Step: 1237, Loss: 0.9169726371765137, Accuracy: 1.0, Computation time: 1.9992339611053467\n",
      "Step: 1238, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 1.9994182586669922\n",
      "Step: 1239, Loss: 0.9433453679084778, Accuracy: 0.96875, Computation time: 1.6302013397216797\n",
      "Step: 1240, Loss: 0.9160286784172058, Accuracy: 1.0, Computation time: 1.7770979404449463\n",
      "Step: 1241, Loss: 0.9159739017486572, Accuracy: 1.0, Computation time: 1.4928841590881348\n",
      "Step: 1242, Loss: 0.916045069694519, Accuracy: 1.0, Computation time: 1.4595367908477783\n",
      "Step: 1243, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.5968434810638428\n",
      "Step: 1244, Loss: 0.9159988164901733, Accuracy: 1.0, Computation time: 1.5581789016723633\n",
      "Step: 1245, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.6292569637298584\n",
      "Step: 1246, Loss: 0.9159882068634033, Accuracy: 1.0, Computation time: 1.588257074356079\n",
      "Step: 1247, Loss: 0.9290143847465515, Accuracy: 0.96875, Computation time: 2.0698177814483643\n",
      "Step: 1248, Loss: 0.916025698184967, Accuracy: 1.0, Computation time: 1.8721699714660645\n",
      "Step: 1249, Loss: 0.9160551428794861, Accuracy: 1.0, Computation time: 3.0224874019622803\n",
      "Step: 1250, Loss: 0.9160829782485962, Accuracy: 1.0, Computation time: 1.6546261310577393\n",
      "Step: 1251, Loss: 0.9161203503608704, Accuracy: 1.0, Computation time: 1.630035400390625\n",
      "########################\n",
      "Test loss: 1.1143444776535034, Test Accuracy_epoch9: 0.7124423980712891\n",
      "########################\n",
      "Step: 1252, Loss: 0.9160247445106506, Accuracy: 1.0, Computation time: 1.6864900588989258\n",
      "Step: 1253, Loss: 0.9193142652511597, Accuracy: 1.0, Computation time: 2.132052421569824\n",
      "Step: 1254, Loss: 0.91604083776474, Accuracy: 1.0, Computation time: 1.6505775451660156\n",
      "Step: 1255, Loss: 0.9159390330314636, Accuracy: 1.0, Computation time: 1.6913704872131348\n",
      "Step: 1256, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 1.785243272781372\n",
      "Step: 1257, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.4588649272918701\n",
      "Step: 1258, Loss: 0.9159087538719177, Accuracy: 1.0, Computation time: 1.3594729900360107\n",
      "Step: 1259, Loss: 0.9159724712371826, Accuracy: 1.0, Computation time: 1.6848227977752686\n",
      "Step: 1260, Loss: 0.9158980250358582, Accuracy: 1.0, Computation time: 1.9123811721801758\n",
      "Step: 1261, Loss: 0.9159100651741028, Accuracy: 1.0, Computation time: 1.465423822402954\n",
      "Step: 1262, Loss: 0.9161291122436523, Accuracy: 1.0, Computation time: 1.8965120315551758\n",
      "Step: 1263, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.6577320098876953\n",
      "Step: 1264, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 1.5416102409362793\n",
      "Step: 1265, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.7803797721862793\n",
      "Step: 1266, Loss: 0.9160887598991394, Accuracy: 1.0, Computation time: 1.9970018863677979\n",
      "Step: 1267, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.4304049015045166\n",
      "Step: 1268, Loss: 0.9164432287216187, Accuracy: 1.0, Computation time: 1.6846857070922852\n",
      "Step: 1269, Loss: 0.9374855756759644, Accuracy: 0.96875, Computation time: 1.7285773754119873\n",
      "Step: 1270, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.8311700820922852\n",
      "Step: 1271, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.4970519542694092\n",
      "Step: 1272, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.6148509979248047\n",
      "Step: 1273, Loss: 0.9160871505737305, Accuracy: 1.0, Computation time: 1.8917639255523682\n",
      "Step: 1274, Loss: 0.9290517568588257, Accuracy: 0.96875, Computation time: 1.961862325668335\n",
      "Step: 1275, Loss: 0.9375766515731812, Accuracy: 0.96875, Computation time: 1.6355969905853271\n",
      "Step: 1276, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.638650894165039\n",
      "Step: 1277, Loss: 0.9161805510520935, Accuracy: 1.0, Computation time: 1.7346405982971191\n",
      "Step: 1278, Loss: 0.9162681698799133, Accuracy: 1.0, Computation time: 1.7177104949951172\n",
      "Step: 1279, Loss: 0.916059672832489, Accuracy: 1.0, Computation time: 1.455489158630371\n",
      "Step: 1280, Loss: 0.9370906949043274, Accuracy: 0.96875, Computation time: 1.5446703433990479\n",
      "Step: 1281, Loss: 0.9160233736038208, Accuracy: 1.0, Computation time: 1.638366460800171\n",
      "Step: 1282, Loss: 0.937599241733551, Accuracy: 0.96875, Computation time: 1.6405103206634521\n",
      "Step: 1283, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.6357972621917725\n",
      "Step: 1284, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.6126062870025635\n",
      "Step: 1285, Loss: 0.9224688410758972, Accuracy: 1.0, Computation time: 1.7294673919677734\n",
      "Step: 1286, Loss: 0.9159289002418518, Accuracy: 1.0, Computation time: 1.8869454860687256\n",
      "Step: 1287, Loss: 0.9362857341766357, Accuracy: 0.96875, Computation time: 1.5223643779754639\n",
      "Step: 1288, Loss: 0.9312402606010437, Accuracy: 0.96875, Computation time: 1.5836706161499023\n",
      "Step: 1289, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 1.7575418949127197\n",
      "Step: 1290, Loss: 0.9324550032615662, Accuracy: 0.96875, Computation time: 1.6131091117858887\n",
      "Step: 1291, Loss: 0.9374351501464844, Accuracy: 0.96875, Computation time: 1.6232223510742188\n",
      "Step: 1292, Loss: 0.9160610437393188, Accuracy: 1.0, Computation time: 1.3132541179656982\n",
      "Step: 1293, Loss: 0.9161125421524048, Accuracy: 1.0, Computation time: 1.5140354633331299\n",
      "Step: 1294, Loss: 0.9160057306289673, Accuracy: 1.0, Computation time: 1.6446874141693115\n",
      "Step: 1295, Loss: 0.9162436127662659, Accuracy: 1.0, Computation time: 1.62428617477417\n",
      "Step: 1296, Loss: 0.9370649456977844, Accuracy: 0.96875, Computation time: 1.5976831912994385\n",
      "Step: 1297, Loss: 0.9168241024017334, Accuracy: 1.0, Computation time: 2.040357828140259\n",
      "Step: 1298, Loss: 0.9190548658370972, Accuracy: 1.0, Computation time: 1.676788330078125\n",
      "Step: 1299, Loss: 0.9191047549247742, Accuracy: 1.0, Computation time: 1.6210806369781494\n",
      "Step: 1300, Loss: 0.9161104559898376, Accuracy: 1.0, Computation time: 1.438126564025879\n",
      "Step: 1301, Loss: 0.937783420085907, Accuracy: 0.96875, Computation time: 1.631701946258545\n",
      "Step: 1302, Loss: 0.916405200958252, Accuracy: 1.0, Computation time: 1.7766146659851074\n",
      "Step: 1303, Loss: 0.9164648056030273, Accuracy: 1.0, Computation time: 1.4196975231170654\n",
      "Step: 1304, Loss: 0.91636723279953, Accuracy: 1.0, Computation time: 1.6242098808288574\n",
      "Step: 1305, Loss: 0.9159922003746033, Accuracy: 1.0, Computation time: 1.5473952293395996\n",
      "Step: 1306, Loss: 0.9562800526618958, Accuracy: 0.9375, Computation time: 1.683441400527954\n",
      "Step: 1307, Loss: 0.9159638285636902, Accuracy: 1.0, Computation time: 1.7867634296417236\n",
      "Step: 1308, Loss: 0.9167965650558472, Accuracy: 1.0, Computation time: 1.829066276550293\n",
      "Step: 1309, Loss: 0.9159972071647644, Accuracy: 1.0, Computation time: 1.7544231414794922\n",
      "Step: 1310, Loss: 0.91607266664505, Accuracy: 1.0, Computation time: 1.618135929107666\n",
      "Step: 1311, Loss: 0.9161888360977173, Accuracy: 1.0, Computation time: 1.8949027061462402\n",
      "Step: 1312, Loss: 0.9166547060012817, Accuracy: 1.0, Computation time: 1.8242709636688232\n",
      "Step: 1313, Loss: 0.9160685539245605, Accuracy: 1.0, Computation time: 1.7298378944396973\n",
      "Step: 1314, Loss: 0.9160524606704712, Accuracy: 1.0, Computation time: 2.0803120136260986\n",
      "Step: 1315, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.8089179992675781\n",
      "Step: 1316, Loss: 0.9159162640571594, Accuracy: 1.0, Computation time: 1.586277961730957\n",
      "Step: 1317, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.7854819297790527\n",
      "Step: 1318, Loss: 0.916066586971283, Accuracy: 1.0, Computation time: 1.9725337028503418\n",
      "Step: 1319, Loss: 0.9159687757492065, Accuracy: 1.0, Computation time: 2.054676055908203\n",
      "Step: 1320, Loss: 0.9374741911888123, Accuracy: 0.96875, Computation time: 1.8862354755401611\n",
      "Step: 1321, Loss: 0.9173458218574524, Accuracy: 1.0, Computation time: 1.637932538986206\n",
      "Step: 1322, Loss: 0.9388157725334167, Accuracy: 0.96875, Computation time: 1.7911639213562012\n",
      "Step: 1323, Loss: 0.9365133047103882, Accuracy: 0.96875, Computation time: 1.962186336517334\n",
      "Step: 1324, Loss: 0.9279249310493469, Accuracy: 0.96875, Computation time: 2.0572023391723633\n",
      "Step: 1325, Loss: 0.9160007238388062, Accuracy: 1.0, Computation time: 1.5941731929779053\n",
      "Step: 1326, Loss: 0.9161109924316406, Accuracy: 1.0, Computation time: 1.939239263534546\n",
      "Step: 1327, Loss: 0.9161271452903748, Accuracy: 1.0, Computation time: 2.0986833572387695\n",
      "Step: 1328, Loss: 0.9169120788574219, Accuracy: 1.0, Computation time: 2.336846351623535\n",
      "Step: 1329, Loss: 0.916001558303833, Accuracy: 1.0, Computation time: 1.9711353778839111\n",
      "Step: 1330, Loss: 0.9371234774589539, Accuracy: 0.96875, Computation time: 1.9295861721038818\n",
      "Step: 1331, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 1.644517183303833\n",
      "Step: 1332, Loss: 0.9160211086273193, Accuracy: 1.0, Computation time: 1.753194808959961\n",
      "Step: 1333, Loss: 0.9160678386688232, Accuracy: 1.0, Computation time: 1.8619508743286133\n",
      "Step: 1334, Loss: 0.9162065386772156, Accuracy: 1.0, Computation time: 2.0372965335845947\n",
      "Step: 1335, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 1.578902006149292\n",
      "Step: 1336, Loss: 0.9161663055419922, Accuracy: 1.0, Computation time: 1.9037389755249023\n",
      "Step: 1337, Loss: 0.9160367250442505, Accuracy: 1.0, Computation time: 2.140878200531006\n",
      "Step: 1338, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.7442457675933838\n",
      "Step: 1339, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.7326946258544922\n",
      "Step: 1340, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 1.68882155418396\n",
      "Step: 1341, Loss: 0.9370698928833008, Accuracy: 0.96875, Computation time: 2.374154567718506\n",
      "Step: 1342, Loss: 0.9230085611343384, Accuracy: 1.0, Computation time: 1.7957289218902588\n",
      "Step: 1343, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 1.5476751327514648\n",
      "Step: 1344, Loss: 0.9161313772201538, Accuracy: 1.0, Computation time: 1.7505898475646973\n",
      "Step: 1345, Loss: 0.9159265756607056, Accuracy: 1.0, Computation time: 1.4608242511749268\n",
      "Step: 1346, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 1.736931324005127\n",
      "Step: 1347, Loss: 0.9160082936286926, Accuracy: 1.0, Computation time: 1.7926170825958252\n",
      "Step: 1348, Loss: 0.9165046215057373, Accuracy: 1.0, Computation time: 2.2627828121185303\n",
      "Step: 1349, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.5762643814086914\n",
      "Step: 1350, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 1.5263588428497314\n",
      "Step: 1351, Loss: 0.9185463786125183, Accuracy: 1.0, Computation time: 2.091221570968628\n",
      "Step: 1352, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.8777189254760742\n",
      "Step: 1353, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.8508121967315674\n",
      "Step: 1354, Loss: 0.9162022471427917, Accuracy: 1.0, Computation time: 2.066173553466797\n",
      "Step: 1355, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 2.0050103664398193\n",
      "Step: 1356, Loss: 0.9162921905517578, Accuracy: 1.0, Computation time: 2.048248291015625\n",
      "Step: 1357, Loss: 0.916700541973114, Accuracy: 1.0, Computation time: 2.161963701248169\n",
      "Step: 1358, Loss: 0.9256906509399414, Accuracy: 0.96875, Computation time: 2.1368839740753174\n",
      "Step: 1359, Loss: 0.9376005530357361, Accuracy: 0.96875, Computation time: 1.998295783996582\n",
      "Step: 1360, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.494452714920044\n",
      "Step: 1361, Loss: 0.9159843325614929, Accuracy: 1.0, Computation time: 1.7041127681732178\n",
      "Step: 1362, Loss: 0.9161729216575623, Accuracy: 1.0, Computation time: 1.993237018585205\n",
      "Step: 1363, Loss: 0.9558895230293274, Accuracy: 0.9375, Computation time: 1.7750632762908936\n",
      "Step: 1364, Loss: 0.9177582263946533, Accuracy: 1.0, Computation time: 2.3763530254364014\n",
      "Step: 1365, Loss: 0.9190280437469482, Accuracy: 1.0, Computation time: 1.8749399185180664\n",
      "Step: 1366, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.5813934803009033\n",
      "Step: 1367, Loss: 0.9377474784851074, Accuracy: 0.96875, Computation time: 1.8257355690002441\n",
      "Step: 1368, Loss: 0.916023313999176, Accuracy: 1.0, Computation time: 1.6068458557128906\n",
      "Step: 1369, Loss: 0.937588632106781, Accuracy: 0.96875, Computation time: 1.5700187683105469\n",
      "Step: 1370, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 1.4666953086853027\n",
      "Step: 1371, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.4609007835388184\n",
      "Step: 1372, Loss: 0.916133463382721, Accuracy: 1.0, Computation time: 1.8660058975219727\n",
      "Step: 1373, Loss: 0.9381949305534363, Accuracy: 0.96875, Computation time: 1.6540896892547607\n",
      "Step: 1374, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 1.938647747039795\n",
      "Step: 1375, Loss: 0.9159871339797974, Accuracy: 1.0, Computation time: 1.8123695850372314\n",
      "Step: 1376, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.8264148235321045\n",
      "Step: 1377, Loss: 0.9161364436149597, Accuracy: 1.0, Computation time: 1.817086935043335\n",
      "Step: 1378, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.8747532367706299\n",
      "Step: 1379, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.7258853912353516\n",
      "Step: 1380, Loss: 0.9160093665122986, Accuracy: 1.0, Computation time: 1.90248441696167\n",
      "Step: 1381, Loss: 0.9373876452445984, Accuracy: 0.96875, Computation time: 1.9713165760040283\n",
      "Step: 1382, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.7758915424346924\n",
      "Step: 1383, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.5354914665222168\n",
      "Step: 1384, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 1.651179313659668\n",
      "Step: 1385, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.9036133289337158\n",
      "Step: 1386, Loss: 0.9193310737609863, Accuracy: 1.0, Computation time: 2.214090347290039\n",
      "Step: 1387, Loss: 0.93479323387146, Accuracy: 0.96875, Computation time: 1.8391876220703125\n",
      "Step: 1388, Loss: 0.9160242676734924, Accuracy: 1.0, Computation time: 1.5294854640960693\n",
      "Step: 1389, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.6309292316436768\n",
      "Step: 1390, Loss: 0.91597580909729, Accuracy: 1.0, Computation time: 1.6589820384979248\n",
      "########################\n",
      "Test loss: 1.131352186203003, Test Accuracy_epoch10: 0.6884792447090149\n",
      "########################\n",
      "Step: 1391, Loss: 0.9159587025642395, Accuracy: 1.0, Computation time: 1.7525622844696045\n",
      "Step: 1392, Loss: 0.9373946189880371, Accuracy: 0.96875, Computation time: 1.8759260177612305\n",
      "Step: 1393, Loss: 0.9160029888153076, Accuracy: 1.0, Computation time: 1.917388677597046\n",
      "Step: 1394, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 1.8812906742095947\n",
      "Step: 1395, Loss: 0.9163287878036499, Accuracy: 1.0, Computation time: 1.9365880489349365\n",
      "Step: 1396, Loss: 0.9173676371574402, Accuracy: 1.0, Computation time: 1.5936644077301025\n",
      "Step: 1397, Loss: 0.9169880747795105, Accuracy: 1.0, Computation time: 2.0226638317108154\n",
      "Step: 1398, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.9475266933441162\n",
      "Step: 1399, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 2.095097780227661\n",
      "Step: 1400, Loss: 0.9366325736045837, Accuracy: 0.96875, Computation time: 2.5362911224365234\n",
      "Step: 1401, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.5865256786346436\n",
      "Step: 1402, Loss: 0.9376943111419678, Accuracy: 0.96875, Computation time: 2.0120773315429688\n",
      "Step: 1403, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.7742154598236084\n",
      "Step: 1404, Loss: 0.9399036169052124, Accuracy: 0.96875, Computation time: 2.2474865913391113\n",
      "Step: 1405, Loss: 0.9160931706428528, Accuracy: 1.0, Computation time: 2.567608594894409\n",
      "Step: 1406, Loss: 0.9160095453262329, Accuracy: 1.0, Computation time: 2.188955068588257\n",
      "Step: 1407, Loss: 0.9160492420196533, Accuracy: 1.0, Computation time: 2.00219988822937\n",
      "Step: 1408, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 1.9464142322540283\n",
      "Step: 1409, Loss: 0.9159536957740784, Accuracy: 1.0, Computation time: 1.7489275932312012\n",
      "Step: 1410, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 1.8313522338867188\n",
      "Step: 1411, Loss: 0.9378402829170227, Accuracy: 0.96875, Computation time: 2.0755574703216553\n",
      "Step: 1412, Loss: 0.9161751866340637, Accuracy: 1.0, Computation time: 1.5812976360321045\n",
      "Step: 1413, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 1.8590505123138428\n",
      "Step: 1414, Loss: 0.959587574005127, Accuracy: 0.9375, Computation time: 1.900310754776001\n",
      "Step: 1415, Loss: 0.9159800410270691, Accuracy: 1.0, Computation time: 2.140695810317993\n",
      "Step: 1416, Loss: 0.9160282015800476, Accuracy: 1.0, Computation time: 1.8283991813659668\n",
      "Step: 1417, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 2.2560667991638184\n",
      "Step: 1418, Loss: 0.9159300327301025, Accuracy: 1.0, Computation time: 1.6568224430084229\n",
      "Step: 1419, Loss: 0.9434890151023865, Accuracy: 0.96875, Computation time: 1.762768030166626\n",
      "Step: 1420, Loss: 0.9553855657577515, Accuracy: 0.9375, Computation time: 1.9394676685333252\n",
      "Step: 1421, Loss: 0.9375836253166199, Accuracy: 0.96875, Computation time: 1.7150259017944336\n",
      "Step: 1422, Loss: 0.9163790345191956, Accuracy: 1.0, Computation time: 2.006669521331787\n",
      "Step: 1423, Loss: 0.9161020517349243, Accuracy: 1.0, Computation time: 1.7588648796081543\n",
      "Step: 1424, Loss: 0.9162960648536682, Accuracy: 1.0, Computation time: 1.9909281730651855\n",
      "Step: 1425, Loss: 0.9160919189453125, Accuracy: 1.0, Computation time: 2.0472681522369385\n",
      "Step: 1426, Loss: 0.9160807132720947, Accuracy: 1.0, Computation time: 1.67586350440979\n",
      "Step: 1427, Loss: 0.9159379601478577, Accuracy: 1.0, Computation time: 1.989508867263794\n",
      "Step: 1428, Loss: 0.9166028499603271, Accuracy: 1.0, Computation time: 1.6272051334381104\n",
      "Step: 1429, Loss: 0.9163240790367126, Accuracy: 1.0, Computation time: 1.7848188877105713\n",
      "Step: 1430, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.6232032775878906\n",
      "Step: 1431, Loss: 0.9159797430038452, Accuracy: 1.0, Computation time: 2.319655418395996\n",
      "Step: 1432, Loss: 0.9159570336341858, Accuracy: 1.0, Computation time: 2.0379772186279297\n",
      "Step: 1433, Loss: 0.9235388040542603, Accuracy: 1.0, Computation time: 2.244148015975952\n",
      "Step: 1434, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 1.7222015857696533\n",
      "Step: 1435, Loss: 0.9159956574440002, Accuracy: 1.0, Computation time: 1.982079267501831\n",
      "Step: 1436, Loss: 0.9356285333633423, Accuracy: 0.96875, Computation time: 1.779850959777832\n",
      "Step: 1437, Loss: 0.9159965515136719, Accuracy: 1.0, Computation time: 1.762934923171997\n",
      "Step: 1438, Loss: 0.9167007207870483, Accuracy: 1.0, Computation time: 2.0515942573547363\n",
      "Step: 1439, Loss: 0.933425784111023, Accuracy: 0.96875, Computation time: 1.89387845993042\n",
      "Step: 1440, Loss: 0.9161123037338257, Accuracy: 1.0, Computation time: 2.013453483581543\n",
      "Step: 1441, Loss: 0.9159656167030334, Accuracy: 1.0, Computation time: 2.2984368801116943\n",
      "Step: 1442, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 1.9938504695892334\n",
      "Step: 1443, Loss: 0.9162360429763794, Accuracy: 1.0, Computation time: 2.375497579574585\n",
      "Step: 1444, Loss: 0.9356932044029236, Accuracy: 0.96875, Computation time: 1.7864251136779785\n",
      "Step: 1445, Loss: 0.9359471797943115, Accuracy: 0.96875, Computation time: 2.141883373260498\n",
      "Step: 1446, Loss: 0.9159864187240601, Accuracy: 1.0, Computation time: 2.211728811264038\n",
      "Step: 1447, Loss: 0.9375479221343994, Accuracy: 0.96875, Computation time: 1.7046830654144287\n",
      "Step: 1448, Loss: 0.9160311222076416, Accuracy: 1.0, Computation time: 2.0982258319854736\n",
      "Step: 1449, Loss: 0.9159955382347107, Accuracy: 1.0, Computation time: 2.08023738861084\n",
      "Step: 1450, Loss: 0.9160242676734924, Accuracy: 1.0, Computation time: 1.9617602825164795\n",
      "Step: 1451, Loss: 0.9161210060119629, Accuracy: 1.0, Computation time: 2.34770131111145\n",
      "Step: 1452, Loss: 0.916050374507904, Accuracy: 1.0, Computation time: 2.1759889125823975\n",
      "Step: 1453, Loss: 0.916402280330658, Accuracy: 1.0, Computation time: 2.2055904865264893\n",
      "Step: 1454, Loss: 0.9194756150245667, Accuracy: 1.0, Computation time: 1.946552038192749\n",
      "Step: 1455, Loss: 0.9160084128379822, Accuracy: 1.0, Computation time: 1.7891144752502441\n",
      "Step: 1456, Loss: 0.9160591959953308, Accuracy: 1.0, Computation time: 2.4964985847473145\n",
      "Step: 1457, Loss: 0.9174137115478516, Accuracy: 1.0, Computation time: 2.0295095443725586\n",
      "Step: 1458, Loss: 0.9513716101646423, Accuracy: 0.9375, Computation time: 3.0324716567993164\n",
      "Step: 1459, Loss: 0.91602623462677, Accuracy: 1.0, Computation time: 2.2398922443389893\n",
      "Step: 1460, Loss: 0.9159900546073914, Accuracy: 1.0, Computation time: 2.4933807849884033\n",
      "Step: 1461, Loss: 0.9161918759346008, Accuracy: 1.0, Computation time: 1.9547970294952393\n",
      "Step: 1462, Loss: 0.937616765499115, Accuracy: 0.96875, Computation time: 2.044025421142578\n",
      "Step: 1463, Loss: 0.9162266254425049, Accuracy: 1.0, Computation time: 2.4784743785858154\n",
      "Step: 1464, Loss: 0.9167908430099487, Accuracy: 1.0, Computation time: 2.230336904525757\n",
      "Step: 1465, Loss: 0.9160401821136475, Accuracy: 1.0, Computation time: 1.7026965618133545\n",
      "Step: 1466, Loss: 0.9161361455917358, Accuracy: 1.0, Computation time: 1.8958044052124023\n",
      "Step: 1467, Loss: 0.9160929322242737, Accuracy: 1.0, Computation time: 2.020766258239746\n",
      "Step: 1468, Loss: 0.9160079956054688, Accuracy: 1.0, Computation time: 1.9970602989196777\n",
      "Step: 1469, Loss: 0.9161296486854553, Accuracy: 1.0, Computation time: 2.1108012199401855\n",
      "Step: 1470, Loss: 0.9340939521789551, Accuracy: 0.96875, Computation time: 3.4895966053009033\n",
      "Step: 1471, Loss: 0.916027307510376, Accuracy: 1.0, Computation time: 2.0669443607330322\n",
      "Step: 1472, Loss: 0.9377114772796631, Accuracy: 0.96875, Computation time: 1.9883615970611572\n",
      "Step: 1473, Loss: 0.9166339039802551, Accuracy: 1.0, Computation time: 2.1537227630615234\n",
      "Step: 1474, Loss: 0.916024386882782, Accuracy: 1.0, Computation time: 2.1805036067962646\n",
      "Step: 1475, Loss: 0.9162060022354126, Accuracy: 1.0, Computation time: 2.3671329021453857\n",
      "Step: 1476, Loss: 0.9379554390907288, Accuracy: 0.96875, Computation time: 2.5610084533691406\n",
      "Step: 1477, Loss: 0.9160496592521667, Accuracy: 1.0, Computation time: 2.0824756622314453\n",
      "Step: 1478, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 2.0900819301605225\n",
      "Step: 1479, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 2.3178749084472656\n",
      "Step: 1480, Loss: 0.9159601926803589, Accuracy: 1.0, Computation time: 1.9049756526947021\n",
      "Step: 1481, Loss: 0.9162023067474365, Accuracy: 1.0, Computation time: 2.127845287322998\n",
      "Step: 1482, Loss: 0.9159517288208008, Accuracy: 1.0, Computation time: 2.2340798377990723\n",
      "Step: 1483, Loss: 0.921917200088501, Accuracy: 1.0, Computation time: 2.3156421184539795\n",
      "Step: 1484, Loss: 0.9162248969078064, Accuracy: 1.0, Computation time: 2.318075656890869\n",
      "Step: 1485, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 1.9990172386169434\n",
      "Step: 1486, Loss: 0.9160477519035339, Accuracy: 1.0, Computation time: 2.2133231163024902\n",
      "Step: 1487, Loss: 0.9204704165458679, Accuracy: 1.0, Computation time: 2.4111721515655518\n",
      "Step: 1488, Loss: 0.9192317724227905, Accuracy: 1.0, Computation time: 2.2278826236724854\n",
      "Step: 1489, Loss: 0.9162574410438538, Accuracy: 1.0, Computation time: 2.005415916442871\n",
      "Step: 1490, Loss: 0.9164048433303833, Accuracy: 1.0, Computation time: 1.9385249614715576\n",
      "Step: 1491, Loss: 0.9164914488792419, Accuracy: 1.0, Computation time: 2.273958921432495\n",
      "Step: 1492, Loss: 0.9161792993545532, Accuracy: 1.0, Computation time: 2.142760753631592\n",
      "Step: 1493, Loss: 0.9163821339607239, Accuracy: 1.0, Computation time: 1.6423511505126953\n",
      "Step: 1494, Loss: 0.916340708732605, Accuracy: 1.0, Computation time: 1.829641580581665\n",
      "Step: 1495, Loss: 0.9160973429679871, Accuracy: 1.0, Computation time: 1.7038066387176514\n",
      "Step: 1496, Loss: 0.9160518646240234, Accuracy: 1.0, Computation time: 2.2048983573913574\n",
      "Step: 1497, Loss: 0.9161421656608582, Accuracy: 1.0, Computation time: 1.892273187637329\n",
      "Step: 1498, Loss: 0.9160153865814209, Accuracy: 1.0, Computation time: 1.6287469863891602\n",
      "Step: 1499, Loss: 0.9160211086273193, Accuracy: 1.0, Computation time: 1.4994993209838867\n",
      "Step: 1500, Loss: 0.9160119891166687, Accuracy: 1.0, Computation time: 1.7447679042816162\n",
      "Step: 1501, Loss: 0.9160076379776001, Accuracy: 1.0, Computation time: 1.5640079975128174\n",
      "Step: 1502, Loss: 0.9159820675849915, Accuracy: 1.0, Computation time: 1.7026605606079102\n",
      "Step: 1503, Loss: 0.9159906506538391, Accuracy: 1.0, Computation time: 1.8118643760681152\n",
      "Step: 1504, Loss: 0.9159571528434753, Accuracy: 1.0, Computation time: 1.7735919952392578\n",
      "Step: 1505, Loss: 0.9159396886825562, Accuracy: 1.0, Computation time: 1.6867849826812744\n",
      "Step: 1506, Loss: 0.918199896812439, Accuracy: 1.0, Computation time: 2.223174810409546\n",
      "Step: 1507, Loss: 0.915901780128479, Accuracy: 1.0, Computation time: 1.7915539741516113\n",
      "Step: 1508, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.5667383670806885\n",
      "Step: 1509, Loss: 0.9159308671951294, Accuracy: 1.0, Computation time: 1.757883071899414\n",
      "Step: 1510, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.9438679218292236\n",
      "Step: 1511, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 1.625765323638916\n",
      "Step: 1512, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.8101255893707275\n",
      "Step: 1513, Loss: 0.9159459471702576, Accuracy: 1.0, Computation time: 1.8065333366394043\n",
      "Step: 1514, Loss: 0.9334163665771484, Accuracy: 0.96875, Computation time: 1.84993314743042\n",
      "Step: 1515, Loss: 0.9160138368606567, Accuracy: 1.0, Computation time: 2.0686285495758057\n",
      "Step: 1516, Loss: 0.9162194132804871, Accuracy: 1.0, Computation time: 1.946594476699829\n",
      "Step: 1517, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 1.7508306503295898\n",
      "Step: 1518, Loss: 0.9358214139938354, Accuracy: 0.96875, Computation time: 1.6291563510894775\n",
      "Step: 1519, Loss: 0.9376070499420166, Accuracy: 0.96875, Computation time: 1.5125198364257812\n",
      "Step: 1520, Loss: 0.9159389734268188, Accuracy: 1.0, Computation time: 1.815610408782959\n",
      "Step: 1521, Loss: 0.9163856506347656, Accuracy: 1.0, Computation time: 1.8128106594085693\n",
      "Step: 1522, Loss: 0.9160756468772888, Accuracy: 1.0, Computation time: 1.7689998149871826\n",
      "Step: 1523, Loss: 0.9246944785118103, Accuracy: 1.0, Computation time: 2.2264652252197266\n",
      "Step: 1524, Loss: 0.9376024603843689, Accuracy: 0.96875, Computation time: 2.1084482669830322\n",
      "Step: 1525, Loss: 0.9159373044967651, Accuracy: 1.0, Computation time: 1.7808382511138916\n",
      "Step: 1526, Loss: 0.9377487897872925, Accuracy: 0.96875, Computation time: 2.0453011989593506\n",
      "Step: 1527, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.9144794940948486\n",
      "Step: 1528, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.912442684173584\n",
      "Step: 1529, Loss: 0.9158843159675598, Accuracy: 1.0, Computation time: 2.020205497741699\n",
      "########################\n",
      "Test loss: 1.1305632591247559, Test Accuracy_epoch11: 0.6838709712028503\n",
      "########################\n",
      "Step: 1530, Loss: 0.9161304831504822, Accuracy: 1.0, Computation time: 1.9467689990997314\n",
      "Step: 1531, Loss: 0.9348868727684021, Accuracy: 0.96875, Computation time: 1.8194327354431152\n",
      "Step: 1532, Loss: 0.9159448146820068, Accuracy: 1.0, Computation time: 1.6821844577789307\n",
      "Step: 1533, Loss: 0.9266379475593567, Accuracy: 0.96875, Computation time: 2.5063211917877197\n",
      "Step: 1534, Loss: 0.9163509607315063, Accuracy: 1.0, Computation time: 2.3278353214263916\n",
      "Step: 1535, Loss: 0.9161543846130371, Accuracy: 1.0, Computation time: 1.7031660079956055\n",
      "Step: 1536, Loss: 0.9376120567321777, Accuracy: 0.96875, Computation time: 1.867654800415039\n",
      "Step: 1537, Loss: 0.9372978210449219, Accuracy: 0.96875, Computation time: 2.5035481452941895\n",
      "Step: 1538, Loss: 0.9160082936286926, Accuracy: 1.0, Computation time: 2.065195083618164\n",
      "Step: 1539, Loss: 0.9160367250442505, Accuracy: 1.0, Computation time: 2.0835015773773193\n",
      "Step: 1540, Loss: 0.9167042970657349, Accuracy: 1.0, Computation time: 1.9021565914154053\n",
      "Step: 1541, Loss: 0.9159610867500305, Accuracy: 1.0, Computation time: 2.0570225715637207\n",
      "Step: 1542, Loss: 0.9179761409759521, Accuracy: 1.0, Computation time: 1.820594310760498\n",
      "Step: 1543, Loss: 0.9161243438720703, Accuracy: 1.0, Computation time: 2.017242670059204\n",
      "Step: 1544, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.9507713317871094\n",
      "Step: 1545, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 1.8274857997894287\n",
      "Step: 1546, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.9148495197296143\n",
      "Step: 1547, Loss: 0.9162277579307556, Accuracy: 1.0, Computation time: 2.147839069366455\n",
      "Step: 1548, Loss: 0.9376270174980164, Accuracy: 0.96875, Computation time: 2.039205312728882\n",
      "Step: 1549, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.8405661582946777\n",
      "Step: 1550, Loss: 0.9376233816146851, Accuracy: 0.96875, Computation time: 1.91683030128479\n",
      "Step: 1551, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 2.2960875034332275\n",
      "Step: 1552, Loss: 0.9160857200622559, Accuracy: 1.0, Computation time: 2.0075812339782715\n",
      "Step: 1553, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 1.9091222286224365\n",
      "Step: 1554, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 1.8857476711273193\n",
      "Step: 1555, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.8681581020355225\n",
      "Step: 1556, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 1.7347705364227295\n",
      "Step: 1557, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 2.1169307231903076\n",
      "Step: 1558, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 2.2050294876098633\n",
      "Step: 1559, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 2.229567289352417\n",
      "Step: 1560, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.9738121032714844\n",
      "Step: 1561, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.9451072216033936\n",
      "Step: 1562, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 2.002553701400757\n",
      "Step: 1563, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.7969281673431396\n",
      "Step: 1564, Loss: 0.918839693069458, Accuracy: 1.0, Computation time: 2.1898348331451416\n",
      "Step: 1565, Loss: 0.9375714659690857, Accuracy: 0.96875, Computation time: 1.7679142951965332\n",
      "Step: 1566, Loss: 0.9160258173942566, Accuracy: 1.0, Computation time: 2.3583340644836426\n",
      "Step: 1567, Loss: 0.9159974455833435, Accuracy: 1.0, Computation time: 1.6539866924285889\n",
      "Step: 1568, Loss: 0.9327986836433411, Accuracy: 0.96875, Computation time: 2.3024325370788574\n",
      "Step: 1569, Loss: 0.9159314036369324, Accuracy: 1.0, Computation time: 1.8338756561279297\n",
      "Step: 1570, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.840515375137329\n",
      "Step: 1571, Loss: 0.916200578212738, Accuracy: 1.0, Computation time: 2.160606622695923\n",
      "Step: 1572, Loss: 0.9159387350082397, Accuracy: 1.0, Computation time: 1.7692303657531738\n",
      "Step: 1573, Loss: 0.9160680770874023, Accuracy: 1.0, Computation time: 2.2802176475524902\n",
      "Step: 1574, Loss: 0.9160422086715698, Accuracy: 1.0, Computation time: 1.5484619140625\n",
      "Step: 1575, Loss: 0.9160194396972656, Accuracy: 1.0, Computation time: 2.158996105194092\n",
      "Step: 1576, Loss: 0.9159528613090515, Accuracy: 1.0, Computation time: 2.074342727661133\n",
      "Step: 1577, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.833460807800293\n",
      "Step: 1578, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 2.051619291305542\n",
      "Step: 1579, Loss: 0.9373965859413147, Accuracy: 0.96875, Computation time: 2.038645029067993\n",
      "Step: 1580, Loss: 0.9159916639328003, Accuracy: 1.0, Computation time: 2.3597192764282227\n",
      "Step: 1581, Loss: 0.9159025549888611, Accuracy: 1.0, Computation time: 1.5784034729003906\n",
      "Step: 1582, Loss: 0.9159554839134216, Accuracy: 1.0, Computation time: 1.69706392288208\n",
      "Step: 1583, Loss: 0.9374959468841553, Accuracy: 0.96875, Computation time: 1.6489455699920654\n",
      "Step: 1584, Loss: 0.9159839749336243, Accuracy: 1.0, Computation time: 1.9894330501556396\n",
      "Step: 1585, Loss: 0.924369215965271, Accuracy: 1.0, Computation time: 2.044571876525879\n",
      "Step: 1586, Loss: 0.9168508648872375, Accuracy: 1.0, Computation time: 1.6868877410888672\n",
      "Step: 1587, Loss: 0.9169183969497681, Accuracy: 1.0, Computation time: 2.2306411266326904\n",
      "Step: 1588, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 2.421750068664551\n",
      "Step: 1589, Loss: 0.9161107540130615, Accuracy: 1.0, Computation time: 2.1882152557373047\n",
      "Step: 1590, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 1.9732108116149902\n",
      "Step: 1591, Loss: 0.9160244464874268, Accuracy: 1.0, Computation time: 2.0468568801879883\n",
      "Step: 1592, Loss: 0.9159139394760132, Accuracy: 1.0, Computation time: 1.5766220092773438\n",
      "Step: 1593, Loss: 0.9168443083763123, Accuracy: 1.0, Computation time: 2.2246458530426025\n",
      "Step: 1594, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 1.922424554824829\n",
      "Step: 1595, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 1.8777568340301514\n",
      "Step: 1596, Loss: 0.915890634059906, Accuracy: 1.0, Computation time: 1.8411128520965576\n",
      "Step: 1597, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.9478528499603271\n",
      "Step: 1598, Loss: 0.9163230061531067, Accuracy: 1.0, Computation time: 2.225668430328369\n",
      "Step: 1599, Loss: 0.9368640780448914, Accuracy: 0.96875, Computation time: 2.0278658866882324\n",
      "Step: 1600, Loss: 0.9168916344642639, Accuracy: 1.0, Computation time: 1.930696964263916\n",
      "Step: 1601, Loss: 0.915920615196228, Accuracy: 1.0, Computation time: 1.7545015811920166\n",
      "Step: 1602, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.724879264831543\n",
      "Step: 1603, Loss: 0.9261161684989929, Accuracy: 0.96875, Computation time: 1.941763162612915\n",
      "Step: 1604, Loss: 0.927121639251709, Accuracy: 0.96875, Computation time: 2.215970277786255\n",
      "Step: 1605, Loss: 0.9161557555198669, Accuracy: 1.0, Computation time: 2.0478079319000244\n",
      "Step: 1606, Loss: 0.9163854122161865, Accuracy: 1.0, Computation time: 1.8108415603637695\n",
      "Step: 1607, Loss: 0.9163268208503723, Accuracy: 1.0, Computation time: 1.7399380207061768\n",
      "Step: 1608, Loss: 0.9163556694984436, Accuracy: 1.0, Computation time: 2.092578172683716\n",
      "Step: 1609, Loss: 0.9160903096199036, Accuracy: 1.0, Computation time: 2.041543483734131\n",
      "Step: 1610, Loss: 0.9161121249198914, Accuracy: 1.0, Computation time: 1.9051074981689453\n",
      "Step: 1611, Loss: 0.91599041223526, Accuracy: 1.0, Computation time: 1.9212517738342285\n",
      "Step: 1612, Loss: 0.9178706407546997, Accuracy: 1.0, Computation time: 1.6546170711517334\n",
      "Step: 1613, Loss: 0.9162910580635071, Accuracy: 1.0, Computation time: 1.8519139289855957\n",
      "Step: 1614, Loss: 0.9160443544387817, Accuracy: 1.0, Computation time: 1.6352429389953613\n",
      "Step: 1615, Loss: 0.9162104725837708, Accuracy: 1.0, Computation time: 2.2601583003997803\n",
      "Step: 1616, Loss: 0.9366162419319153, Accuracy: 0.96875, Computation time: 1.7923903465270996\n",
      "Step: 1617, Loss: 0.9160752296447754, Accuracy: 1.0, Computation time: 2.028491497039795\n",
      "Step: 1618, Loss: 0.9375658631324768, Accuracy: 0.96875, Computation time: 2.688242197036743\n",
      "Step: 1619, Loss: 0.9169901609420776, Accuracy: 1.0, Computation time: 1.705010175704956\n",
      "Step: 1620, Loss: 0.9285925626754761, Accuracy: 0.96875, Computation time: 1.631230115890503\n",
      "Step: 1621, Loss: 0.9374446272850037, Accuracy: 0.96875, Computation time: 2.012432336807251\n",
      "Step: 1622, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.576120138168335\n",
      "Step: 1623, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.5488839149475098\n",
      "Step: 1624, Loss: 0.9160531163215637, Accuracy: 1.0, Computation time: 1.4035778045654297\n",
      "Step: 1625, Loss: 0.9159989356994629, Accuracy: 1.0, Computation time: 1.5779216289520264\n",
      "Step: 1626, Loss: 0.9376763701438904, Accuracy: 0.96875, Computation time: 1.631798505783081\n",
      "Step: 1627, Loss: 0.9161081910133362, Accuracy: 1.0, Computation time: 1.8292667865753174\n",
      "Step: 1628, Loss: 0.9166788458824158, Accuracy: 1.0, Computation time: 1.6435441970825195\n",
      "Step: 1629, Loss: 0.9159749150276184, Accuracy: 1.0, Computation time: 1.7182209491729736\n",
      "Step: 1630, Loss: 0.9160254001617432, Accuracy: 1.0, Computation time: 1.5896549224853516\n",
      "Step: 1631, Loss: 0.937337338924408, Accuracy: 0.96875, Computation time: 1.9381141662597656\n",
      "Step: 1632, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.4319798946380615\n",
      "Step: 1633, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 1.78975248336792\n",
      "Step: 1634, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.4324896335601807\n",
      "Step: 1635, Loss: 0.9177027344703674, Accuracy: 1.0, Computation time: 1.5318059921264648\n",
      "Step: 1636, Loss: 0.9354107975959778, Accuracy: 0.96875, Computation time: 1.9296340942382812\n",
      "Step: 1637, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.4775655269622803\n",
      "Step: 1638, Loss: 0.9160834550857544, Accuracy: 1.0, Computation time: 1.555030107498169\n",
      "Step: 1639, Loss: 0.9160272479057312, Accuracy: 1.0, Computation time: 1.7782888412475586\n",
      "Step: 1640, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.5008952617645264\n",
      "Step: 1641, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.9340136051177979\n",
      "Step: 1642, Loss: 0.9162415862083435, Accuracy: 1.0, Computation time: 2.085237741470337\n",
      "Step: 1643, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.4789676666259766\n",
      "Step: 1644, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 1.3795671463012695\n",
      "Step: 1645, Loss: 0.9159452319145203, Accuracy: 1.0, Computation time: 1.6253743171691895\n",
      "Step: 1646, Loss: 0.9214535355567932, Accuracy: 1.0, Computation time: 2.577885389328003\n",
      "Step: 1647, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 1.4883368015289307\n",
      "Step: 1648, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.697143316268921\n",
      "Step: 1649, Loss: 0.9419426321983337, Accuracy: 0.96875, Computation time: 2.6141269207000732\n",
      "Step: 1650, Loss: 0.9376404881477356, Accuracy: 0.96875, Computation time: 1.6673965454101562\n",
      "Step: 1651, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.8732218742370605\n",
      "Step: 1652, Loss: 0.9159592986106873, Accuracy: 1.0, Computation time: 1.7181408405303955\n",
      "Step: 1653, Loss: 0.9164546728134155, Accuracy: 1.0, Computation time: 1.8047211170196533\n",
      "Step: 1654, Loss: 0.9160147905349731, Accuracy: 1.0, Computation time: 1.8761346340179443\n",
      "Step: 1655, Loss: 0.9159275889396667, Accuracy: 1.0, Computation time: 1.445746898651123\n",
      "Step: 1656, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.4233427047729492\n",
      "Step: 1657, Loss: 0.9373872876167297, Accuracy: 0.96875, Computation time: 1.9770152568817139\n",
      "Step: 1658, Loss: 0.9159599542617798, Accuracy: 1.0, Computation time: 1.7138712406158447\n",
      "Step: 1659, Loss: 0.9159565567970276, Accuracy: 1.0, Computation time: 1.3070201873779297\n",
      "Step: 1660, Loss: 0.9159456491470337, Accuracy: 1.0, Computation time: 1.6845901012420654\n",
      "Step: 1661, Loss: 0.9369706511497498, Accuracy: 0.96875, Computation time: 1.4932994842529297\n",
      "Step: 1662, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.5488452911376953\n",
      "Step: 1663, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.310856819152832\n",
      "Step: 1664, Loss: 0.9377560615539551, Accuracy: 0.96875, Computation time: 1.688403844833374\n",
      "Step: 1665, Loss: 0.9362683892250061, Accuracy: 0.96875, Computation time: 2.142956256866455\n",
      "Step: 1666, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 1.519878625869751\n",
      "Step: 1667, Loss: 0.9162387251853943, Accuracy: 1.0, Computation time: 1.7479956150054932\n",
      "Step: 1668, Loss: 0.9160065650939941, Accuracy: 1.0, Computation time: 1.3572947978973389\n",
      "########################\n",
      "Test loss: 1.1292439699172974, Test Accuracy_epoch12: 0.6903225779533386\n",
      "########################\n",
      "Step: 1669, Loss: 0.916421115398407, Accuracy: 1.0, Computation time: 1.8829872608184814\n",
      "Step: 1670, Loss: 0.9159107804298401, Accuracy: 1.0, Computation time: 1.688962459564209\n",
      "Step: 1671, Loss: 0.9178680181503296, Accuracy: 1.0, Computation time: 2.616027593612671\n",
      "Step: 1672, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.5190083980560303\n",
      "Step: 1673, Loss: 0.9173128604888916, Accuracy: 1.0, Computation time: 2.267045497894287\n",
      "Step: 1674, Loss: 0.9377384185791016, Accuracy: 0.96875, Computation time: 1.946847915649414\n",
      "Step: 1675, Loss: 0.9161508083343506, Accuracy: 1.0, Computation time: 1.7053699493408203\n",
      "Step: 1676, Loss: 0.9161702990531921, Accuracy: 1.0, Computation time: 1.4166195392608643\n",
      "Step: 1677, Loss: 0.937751054763794, Accuracy: 0.96875, Computation time: 1.5733435153961182\n",
      "Step: 1678, Loss: 0.9159622192382812, Accuracy: 1.0, Computation time: 1.614734172821045\n",
      "Step: 1679, Loss: 0.9378058910369873, Accuracy: 0.96875, Computation time: 1.4668190479278564\n",
      "Step: 1680, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.3245799541473389\n",
      "Step: 1681, Loss: 0.9219224452972412, Accuracy: 1.0, Computation time: 2.074538230895996\n",
      "Step: 1682, Loss: 0.9163756370544434, Accuracy: 1.0, Computation time: 1.700376033782959\n",
      "Step: 1683, Loss: 0.9159501194953918, Accuracy: 1.0, Computation time: 1.4894170761108398\n",
      "Step: 1684, Loss: 0.9159805178642273, Accuracy: 1.0, Computation time: 1.5933926105499268\n",
      "Step: 1685, Loss: 0.9159517288208008, Accuracy: 1.0, Computation time: 1.8051629066467285\n",
      "Step: 1686, Loss: 0.915930449962616, Accuracy: 1.0, Computation time: 1.4921998977661133\n",
      "Step: 1687, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.4999356269836426\n",
      "Step: 1688, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 1.9975616931915283\n",
      "Step: 1689, Loss: 0.9166232347488403, Accuracy: 1.0, Computation time: 1.654872179031372\n",
      "Step: 1690, Loss: 0.9160942435264587, Accuracy: 1.0, Computation time: 1.6895158290863037\n",
      "Step: 1691, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.296311616897583\n",
      "Step: 1692, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.7083256244659424\n",
      "Step: 1693, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 1.4908692836761475\n",
      "Step: 1694, Loss: 0.937629222869873, Accuracy: 0.96875, Computation time: 1.4603421688079834\n",
      "Step: 1695, Loss: 0.9224826693534851, Accuracy: 1.0, Computation time: 2.017043352127075\n",
      "Step: 1696, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.958616018295288\n",
      "Step: 1697, Loss: 0.9159241318702698, Accuracy: 1.0, Computation time: 2.189141273498535\n",
      "Step: 1698, Loss: 0.935951292514801, Accuracy: 0.96875, Computation time: 2.1802544593811035\n",
      "Step: 1699, Loss: 0.9347578883171082, Accuracy: 0.96875, Computation time: 1.441880702972412\n",
      "Step: 1700, Loss: 0.9387012124061584, Accuracy: 0.96875, Computation time: 1.7430593967437744\n",
      "Step: 1701, Loss: 0.9160178303718567, Accuracy: 1.0, Computation time: 1.418107271194458\n",
      "Step: 1702, Loss: 0.9159261584281921, Accuracy: 1.0, Computation time: 1.4195120334625244\n",
      "Step: 1703, Loss: 0.9177784323692322, Accuracy: 1.0, Computation time: 1.7950241565704346\n",
      "Step: 1704, Loss: 0.9165070056915283, Accuracy: 1.0, Computation time: 1.9916656017303467\n",
      "Step: 1705, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.4018011093139648\n",
      "Step: 1706, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 1.771008014678955\n",
      "Step: 1707, Loss: 0.91590815782547, Accuracy: 1.0, Computation time: 1.6293830871582031\n",
      "Step: 1708, Loss: 0.9371139407157898, Accuracy: 0.96875, Computation time: 1.7909154891967773\n",
      "Step: 1709, Loss: 0.9382826089859009, Accuracy: 0.96875, Computation time: 1.5220134258270264\n",
      "Step: 1710, Loss: 0.9377518892288208, Accuracy: 0.96875, Computation time: 1.3544878959655762\n",
      "Step: 1711, Loss: 0.91911780834198, Accuracy: 1.0, Computation time: 2.113898754119873\n",
      "Step: 1712, Loss: 0.9160798788070679, Accuracy: 1.0, Computation time: 1.701216459274292\n",
      "Step: 1713, Loss: 0.9159995317459106, Accuracy: 1.0, Computation time: 1.4411308765411377\n",
      "Step: 1714, Loss: 0.9168019890785217, Accuracy: 1.0, Computation time: 1.8652982711791992\n",
      "Step: 1715, Loss: 0.9160701632499695, Accuracy: 1.0, Computation time: 1.7233538627624512\n",
      "Step: 1716, Loss: 0.9159681797027588, Accuracy: 1.0, Computation time: 1.63958740234375\n",
      "Step: 1717, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 1.7933964729309082\n",
      "Step: 1718, Loss: 0.9159846305847168, Accuracy: 1.0, Computation time: 1.4422991275787354\n",
      "Step: 1719, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.5873456001281738\n",
      "Step: 1720, Loss: 0.9168888330459595, Accuracy: 1.0, Computation time: 2.1280460357666016\n",
      "Step: 1721, Loss: 0.9366220831871033, Accuracy: 0.96875, Computation time: 1.6112277507781982\n",
      "Step: 1722, Loss: 0.9376376867294312, Accuracy: 0.96875, Computation time: 1.9844727516174316\n",
      "Step: 1723, Loss: 0.9163531064987183, Accuracy: 1.0, Computation time: 1.5927927494049072\n",
      "Step: 1724, Loss: 0.9160131812095642, Accuracy: 1.0, Computation time: 2.2464747428894043\n",
      "Step: 1725, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.466193675994873\n",
      "Step: 1726, Loss: 0.9159817695617676, Accuracy: 1.0, Computation time: 1.2522404193878174\n",
      "Step: 1727, Loss: 0.9161804914474487, Accuracy: 1.0, Computation time: 1.5940930843353271\n",
      "Step: 1728, Loss: 0.9160564541816711, Accuracy: 1.0, Computation time: 1.335278034210205\n",
      "Step: 1729, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.7995598316192627\n",
      "Step: 1730, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.7161376476287842\n",
      "Step: 1731, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.6441278457641602\n",
      "Step: 1732, Loss: 0.9174283742904663, Accuracy: 1.0, Computation time: 1.8015718460083008\n",
      "Step: 1733, Loss: 0.9166926741600037, Accuracy: 1.0, Computation time: 1.6724128723144531\n",
      "Step: 1734, Loss: 0.9377641677856445, Accuracy: 0.96875, Computation time: 1.7756521701812744\n",
      "Step: 1735, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 2.037243366241455\n",
      "Step: 1736, Loss: 0.9190866947174072, Accuracy: 1.0, Computation time: 2.1385602951049805\n",
      "Step: 1737, Loss: 0.9159331321716309, Accuracy: 1.0, Computation time: 1.4448919296264648\n",
      "Step: 1738, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.8133320808410645\n",
      "Step: 1739, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.2627263069152832\n",
      "Step: 1740, Loss: 0.9162163734436035, Accuracy: 1.0, Computation time: 1.5949153900146484\n",
      "Step: 1741, Loss: 0.9160096645355225, Accuracy: 1.0, Computation time: 1.9236650466918945\n",
      "Step: 1742, Loss: 0.9181190133094788, Accuracy: 1.0, Computation time: 1.9503183364868164\n",
      "Step: 1743, Loss: 0.9159372448921204, Accuracy: 1.0, Computation time: 1.2018933296203613\n",
      "Step: 1744, Loss: 0.9160557985305786, Accuracy: 1.0, Computation time: 1.3458633422851562\n",
      "Step: 1745, Loss: 0.9166346192359924, Accuracy: 1.0, Computation time: 1.5073356628417969\n",
      "Step: 1746, Loss: 0.9165784120559692, Accuracy: 1.0, Computation time: 1.9220507144927979\n",
      "Step: 1747, Loss: 0.9159853458404541, Accuracy: 1.0, Computation time: 2.0906875133514404\n",
      "Step: 1748, Loss: 0.9160647988319397, Accuracy: 1.0, Computation time: 1.747518539428711\n",
      "Step: 1749, Loss: 0.9160866141319275, Accuracy: 1.0, Computation time: 1.4190495014190674\n",
      "Step: 1750, Loss: 0.9162886142730713, Accuracy: 1.0, Computation time: 1.62860107421875\n",
      "Step: 1751, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.6032485961914062\n",
      "Step: 1752, Loss: 0.9159327745437622, Accuracy: 1.0, Computation time: 1.5436370372772217\n",
      "Step: 1753, Loss: 0.9158998727798462, Accuracy: 1.0, Computation time: 1.6490890979766846\n",
      "Step: 1754, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.9741201400756836\n",
      "Step: 1755, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 1.9070031642913818\n",
      "Step: 1756, Loss: 0.93731689453125, Accuracy: 0.96875, Computation time: 1.6775455474853516\n",
      "Step: 1757, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.2785789966583252\n",
      "Step: 1758, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.5644891262054443\n",
      "Step: 1759, Loss: 0.9159969687461853, Accuracy: 1.0, Computation time: 1.2499780654907227\n",
      "Step: 1760, Loss: 0.91600102186203, Accuracy: 1.0, Computation time: 1.770354986190796\n",
      "Step: 1761, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 1.489628553390503\n",
      "Step: 1762, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 1.4813652038574219\n",
      "Step: 1763, Loss: 0.9162494540214539, Accuracy: 1.0, Computation time: 1.496527910232544\n",
      "Step: 1764, Loss: 0.9160714745521545, Accuracy: 1.0, Computation time: 1.5362441539764404\n",
      "Step: 1765, Loss: 0.9176306128501892, Accuracy: 1.0, Computation time: 2.123790979385376\n",
      "Step: 1766, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.7769691944122314\n",
      "Step: 1767, Loss: 0.9356864094734192, Accuracy: 0.96875, Computation time: 1.5527887344360352\n",
      "Step: 1768, Loss: 0.9159373044967651, Accuracy: 1.0, Computation time: 1.5259382724761963\n",
      "Step: 1769, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 1.3546793460845947\n",
      "Step: 1770, Loss: 0.9252468347549438, Accuracy: 1.0, Computation time: 2.001406669616699\n",
      "Step: 1771, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 1.7352209091186523\n",
      "Step: 1772, Loss: 0.9163325428962708, Accuracy: 1.0, Computation time: 1.8420672416687012\n",
      "Step: 1773, Loss: 0.9160840511322021, Accuracy: 1.0, Computation time: 1.5563247203826904\n",
      "Step: 1774, Loss: 0.9384782910346985, Accuracy: 0.96875, Computation time: 2.247999668121338\n",
      "Step: 1775, Loss: 0.9395184516906738, Accuracy: 0.96875, Computation time: 1.879713773727417\n",
      "Step: 1776, Loss: 0.9190734028816223, Accuracy: 1.0, Computation time: 1.943094253540039\n",
      "Step: 1777, Loss: 0.916225790977478, Accuracy: 1.0, Computation time: 2.112098217010498\n",
      "Step: 1778, Loss: 0.9351865649223328, Accuracy: 0.96875, Computation time: 1.8552260398864746\n",
      "Step: 1779, Loss: 0.916205883026123, Accuracy: 1.0, Computation time: 1.9387874603271484\n",
      "Step: 1780, Loss: 0.9160477519035339, Accuracy: 1.0, Computation time: 1.7066826820373535\n",
      "Step: 1781, Loss: 0.916256844997406, Accuracy: 1.0, Computation time: 2.049001455307007\n",
      "Step: 1782, Loss: 0.9160154461860657, Accuracy: 1.0, Computation time: 1.9350919723510742\n",
      "Step: 1783, Loss: 0.9159649014472961, Accuracy: 1.0, Computation time: 1.9725921154022217\n",
      "Step: 1784, Loss: 0.9160590767860413, Accuracy: 1.0, Computation time: 1.8819606304168701\n",
      "Step: 1785, Loss: 0.9163593649864197, Accuracy: 1.0, Computation time: 2.2712109088897705\n",
      "Step: 1786, Loss: 0.916069507598877, Accuracy: 1.0, Computation time: 2.460817575454712\n",
      "Step: 1787, Loss: 0.9339041113853455, Accuracy: 0.96875, Computation time: 2.2765979766845703\n",
      "Step: 1788, Loss: 0.9160515666007996, Accuracy: 1.0, Computation time: 1.9981558322906494\n",
      "Step: 1789, Loss: 0.9160007238388062, Accuracy: 1.0, Computation time: 2.279634475708008\n",
      "Step: 1790, Loss: 0.9159577488899231, Accuracy: 1.0, Computation time: 2.0983242988586426\n",
      "Step: 1791, Loss: 0.9382238984107971, Accuracy: 0.96875, Computation time: 1.9519925117492676\n",
      "Step: 1792, Loss: 0.9161400198936462, Accuracy: 1.0, Computation time: 1.7483208179473877\n",
      "Step: 1793, Loss: 0.9377395510673523, Accuracy: 0.96875, Computation time: 2.0400819778442383\n",
      "Step: 1794, Loss: 0.9159380197525024, Accuracy: 1.0, Computation time: 2.229130506515503\n",
      "Step: 1795, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 2.4058563709259033\n",
      "Step: 1796, Loss: 0.916154146194458, Accuracy: 1.0, Computation time: 2.1120009422302246\n",
      "Step: 1797, Loss: 0.9167240858078003, Accuracy: 1.0, Computation time: 2.0437052249908447\n",
      "Step: 1798, Loss: 0.9376083612442017, Accuracy: 0.96875, Computation time: 1.8556787967681885\n",
      "Step: 1799, Loss: 0.917121946811676, Accuracy: 1.0, Computation time: 2.08794903755188\n",
      "Step: 1800, Loss: 0.9375489950180054, Accuracy: 0.96875, Computation time: 2.1168570518493652\n",
      "Step: 1801, Loss: 0.9160823225975037, Accuracy: 1.0, Computation time: 2.1344871520996094\n",
      "Step: 1802, Loss: 0.9160518646240234, Accuracy: 1.0, Computation time: 1.9710962772369385\n",
      "Step: 1803, Loss: 0.9161364436149597, Accuracy: 1.0, Computation time: 1.8989708423614502\n",
      "Step: 1804, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.9752626419067383\n",
      "Step: 1805, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 2.051232099533081\n",
      "Step: 1806, Loss: 0.9376507997512817, Accuracy: 0.96875, Computation time: 2.0658469200134277\n",
      "Step: 1807, Loss: 0.9159696698188782, Accuracy: 1.0, Computation time: 2.0437350273132324\n",
      "########################\n",
      "Test loss: 1.1306178569793701, Test Accuracy_epoch13: 0.6866359710693359\n",
      "########################\n",
      "Step: 1808, Loss: 0.9164336323738098, Accuracy: 1.0, Computation time: 2.331526279449463\n",
      "Step: 1809, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.840179204940796\n",
      "Step: 1810, Loss: 0.9159514904022217, Accuracy: 1.0, Computation time: 2.2020506858825684\n",
      "Step: 1811, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 2.249924421310425\n",
      "Step: 1812, Loss: 0.915948212146759, Accuracy: 1.0, Computation time: 2.136263847351074\n",
      "Step: 1813, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 2.1523525714874268\n",
      "Step: 1814, Loss: 0.9159090518951416, Accuracy: 1.0, Computation time: 2.0749666690826416\n",
      "Step: 1815, Loss: 0.9160861372947693, Accuracy: 1.0, Computation time: 2.4306998252868652\n",
      "Step: 1816, Loss: 0.9391263723373413, Accuracy: 0.96875, Computation time: 2.02266788482666\n",
      "Step: 1817, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 2.0155141353607178\n",
      "Step: 1818, Loss: 0.9357576370239258, Accuracy: 0.96875, Computation time: 2.2137439250946045\n",
      "Step: 1819, Loss: 0.9161893725395203, Accuracy: 1.0, Computation time: 1.9605748653411865\n",
      "Step: 1820, Loss: 0.9253364205360413, Accuracy: 1.0, Computation time: 2.0748698711395264\n",
      "Step: 1821, Loss: 0.9160149693489075, Accuracy: 1.0, Computation time: 1.9234724044799805\n",
      "Step: 1822, Loss: 0.9163489937782288, Accuracy: 1.0, Computation time: 2.276705265045166\n",
      "Step: 1823, Loss: 0.9160517454147339, Accuracy: 1.0, Computation time: 2.141470193862915\n",
      "Step: 1824, Loss: 0.916012167930603, Accuracy: 1.0, Computation time: 2.2095108032226562\n",
      "Step: 1825, Loss: 0.916077196598053, Accuracy: 1.0, Computation time: 1.9579343795776367\n",
      "Step: 1826, Loss: 0.9160222411155701, Accuracy: 1.0, Computation time: 2.6732351779937744\n",
      "Step: 1827, Loss: 0.9162584543228149, Accuracy: 1.0, Computation time: 2.091723918914795\n",
      "Step: 1828, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 2.208803653717041\n",
      "Step: 1829, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 1.7822847366333008\n",
      "Step: 1830, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 1.6071155071258545\n",
      "Step: 1831, Loss: 0.9162819385528564, Accuracy: 1.0, Computation time: 1.7654993534088135\n",
      "Step: 1832, Loss: 0.9159621000289917, Accuracy: 1.0, Computation time: 1.708326816558838\n",
      "Step: 1833, Loss: 0.915992259979248, Accuracy: 1.0, Computation time: 2.2249436378479004\n",
      "Step: 1834, Loss: 0.9160811305046082, Accuracy: 1.0, Computation time: 1.7106585502624512\n",
      "Step: 1835, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.8126826286315918\n",
      "Step: 1836, Loss: 0.9159241914749146, Accuracy: 1.0, Computation time: 1.5715644359588623\n",
      "Step: 1837, Loss: 0.9159550070762634, Accuracy: 1.0, Computation time: 2.197561264038086\n",
      "Step: 1838, Loss: 0.9161251783370972, Accuracy: 1.0, Computation time: 1.7822353839874268\n",
      "Step: 1839, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.7698919773101807\n",
      "Step: 1840, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 2.201042890548706\n",
      "Step: 1841, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 2.1995480060577393\n",
      "Step: 1842, Loss: 0.9225580096244812, Accuracy: 1.0, Computation time: 2.1334142684936523\n",
      "Step: 1843, Loss: 0.916976809501648, Accuracy: 1.0, Computation time: 2.019357919692993\n",
      "Step: 1844, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 2.2393202781677246\n",
      "Step: 1845, Loss: 0.9366751313209534, Accuracy: 0.96875, Computation time: 2.4568779468536377\n",
      "Step: 1846, Loss: 0.9159087538719177, Accuracy: 1.0, Computation time: 1.9660162925720215\n",
      "Step: 1847, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 1.7982828617095947\n",
      "Step: 1848, Loss: 0.9159542322158813, Accuracy: 1.0, Computation time: 2.27069354057312\n",
      "Step: 1849, Loss: 0.9376193284988403, Accuracy: 0.96875, Computation time: 1.9053778648376465\n",
      "Step: 1850, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 1.670818567276001\n",
      "Step: 1851, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 2.129880428314209\n",
      "Step: 1852, Loss: 0.9159727692604065, Accuracy: 1.0, Computation time: 1.939652681350708\n",
      "Step: 1853, Loss: 0.9160603284835815, Accuracy: 1.0, Computation time: 1.6104066371917725\n",
      "Step: 1854, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 2.148430347442627\n",
      "Step: 1855, Loss: 0.931820273399353, Accuracy: 0.96875, Computation time: 1.872039794921875\n",
      "Step: 1856, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.8100709915161133\n",
      "Step: 1857, Loss: 0.9375929832458496, Accuracy: 0.96875, Computation time: 2.2921831607818604\n",
      "Step: 1858, Loss: 0.9159224629402161, Accuracy: 1.0, Computation time: 1.9230372905731201\n",
      "Step: 1859, Loss: 0.9159631133079529, Accuracy: 1.0, Computation time: 1.7241110801696777\n",
      "Step: 1860, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.704813003540039\n",
      "Step: 1861, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 2.2552220821380615\n",
      "Step: 1862, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 1.9195377826690674\n",
      "Step: 1863, Loss: 0.9311305284500122, Accuracy: 0.96875, Computation time: 2.3387222290039062\n",
      "Step: 1864, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.6418392658233643\n",
      "Step: 1865, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.7928662300109863\n",
      "Step: 1866, Loss: 0.9371398687362671, Accuracy: 0.96875, Computation time: 2.4043145179748535\n",
      "Step: 1867, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.928013563156128\n",
      "Step: 1868, Loss: 0.9344678521156311, Accuracy: 0.96875, Computation time: 1.929419994354248\n",
      "Step: 1869, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.6434803009033203\n",
      "Step: 1870, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.8534963130950928\n",
      "Step: 1871, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.7859957218170166\n",
      "Step: 1872, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 2.1649513244628906\n",
      "Step: 1873, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 1.5834944248199463\n",
      "Step: 1874, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 2.1126792430877686\n",
      "Step: 1875, Loss: 0.9159504771232605, Accuracy: 1.0, Computation time: 2.106879711151123\n",
      "Step: 1876, Loss: 0.9374818801879883, Accuracy: 0.96875, Computation time: 2.0614280700683594\n",
      "Step: 1877, Loss: 0.9165599346160889, Accuracy: 1.0, Computation time: 2.5709993839263916\n",
      "Step: 1878, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 2.0326664447784424\n",
      "Step: 1879, Loss: 0.9160702228546143, Accuracy: 1.0, Computation time: 1.6871321201324463\n",
      "Step: 1880, Loss: 0.938441276550293, Accuracy: 0.96875, Computation time: 1.9957726001739502\n",
      "Step: 1881, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 2.18772554397583\n",
      "Step: 1882, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 1.9240009784698486\n",
      "Step: 1883, Loss: 0.9160566926002502, Accuracy: 1.0, Computation time: 1.8030993938446045\n",
      "Step: 1884, Loss: 0.9377787709236145, Accuracy: 0.96875, Computation time: 1.7318873405456543\n",
      "Step: 1885, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.911696195602417\n",
      "Step: 1886, Loss: 0.9170341491699219, Accuracy: 1.0, Computation time: 1.9888577461242676\n",
      "Step: 1887, Loss: 0.9213190078735352, Accuracy: 1.0, Computation time: 2.283691883087158\n",
      "Step: 1888, Loss: 0.9159519672393799, Accuracy: 1.0, Computation time: 1.8388328552246094\n",
      "Step: 1889, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 2.30523419380188\n",
      "Step: 1890, Loss: 0.916127622127533, Accuracy: 1.0, Computation time: 1.990452766418457\n",
      "Step: 1891, Loss: 0.9159663319587708, Accuracy: 1.0, Computation time: 1.6346499919891357\n",
      "Step: 1892, Loss: 0.9160050749778748, Accuracy: 1.0, Computation time: 1.836745023727417\n",
      "Step: 1893, Loss: 0.9325315952301025, Accuracy: 0.96875, Computation time: 1.723402500152588\n",
      "Step: 1894, Loss: 0.9160827994346619, Accuracy: 1.0, Computation time: 1.7804584503173828\n",
      "Step: 1895, Loss: 0.9161756634712219, Accuracy: 1.0, Computation time: 1.7227685451507568\n",
      "Step: 1896, Loss: 0.9162822365760803, Accuracy: 1.0, Computation time: 1.5942668914794922\n",
      "Step: 1897, Loss: 0.9160856604576111, Accuracy: 1.0, Computation time: 1.8849358558654785\n",
      "Step: 1898, Loss: 0.9362949728965759, Accuracy: 0.96875, Computation time: 1.8315019607543945\n",
      "Step: 1899, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 2.0297040939331055\n",
      "Step: 1900, Loss: 0.9167153239250183, Accuracy: 1.0, Computation time: 2.4256930351257324\n",
      "Step: 1901, Loss: 0.9161152839660645, Accuracy: 1.0, Computation time: 2.2318785190582275\n",
      "Step: 1902, Loss: 0.9179849624633789, Accuracy: 1.0, Computation time: 2.0266916751861572\n",
      "Step: 1903, Loss: 0.9377801418304443, Accuracy: 0.96875, Computation time: 2.0296761989593506\n",
      "Step: 1904, Loss: 0.9162824749946594, Accuracy: 1.0, Computation time: 1.4404535293579102\n",
      "Step: 1905, Loss: 0.9164304137229919, Accuracy: 1.0, Computation time: 2.0542943477630615\n",
      "Step: 1906, Loss: 0.9243287444114685, Accuracy: 1.0, Computation time: 1.8083462715148926\n",
      "Step: 1907, Loss: 0.916124701499939, Accuracy: 1.0, Computation time: 2.046966314315796\n",
      "Step: 1908, Loss: 0.9167838096618652, Accuracy: 1.0, Computation time: 1.8916051387786865\n",
      "Step: 1909, Loss: 0.9167816042900085, Accuracy: 1.0, Computation time: 2.8396799564361572\n",
      "Step: 1910, Loss: 0.941530168056488, Accuracy: 0.96875, Computation time: 2.1978485584259033\n",
      "Step: 1911, Loss: 0.9184595346450806, Accuracy: 1.0, Computation time: 3.0467538833618164\n",
      "Step: 1912, Loss: 0.9160374402999878, Accuracy: 1.0, Computation time: 2.067471504211426\n",
      "Step: 1913, Loss: 0.9160860776901245, Accuracy: 1.0, Computation time: 2.081716775894165\n",
      "Step: 1914, Loss: 0.9160371422767639, Accuracy: 1.0, Computation time: 2.7510805130004883\n",
      "Step: 1915, Loss: 0.9159790873527527, Accuracy: 1.0, Computation time: 2.3022406101226807\n",
      "Step: 1916, Loss: 0.9377465844154358, Accuracy: 0.96875, Computation time: 2.082650661468506\n",
      "Step: 1917, Loss: 0.915955662727356, Accuracy: 1.0, Computation time: 1.9833042621612549\n",
      "Step: 1918, Loss: 0.9159767031669617, Accuracy: 1.0, Computation time: 1.8001492023468018\n",
      "Step: 1919, Loss: 0.9159860014915466, Accuracy: 1.0, Computation time: 1.7281827926635742\n",
      "Step: 1920, Loss: 0.9159746766090393, Accuracy: 1.0, Computation time: 1.8578784465789795\n",
      "Step: 1921, Loss: 0.9160326719284058, Accuracy: 1.0, Computation time: 1.833463191986084\n",
      "Step: 1922, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 1.9750220775604248\n",
      "Step: 1923, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.687312126159668\n",
      "Step: 1924, Loss: 0.9442533850669861, Accuracy: 0.96875, Computation time: 1.970888614654541\n",
      "Step: 1925, Loss: 0.9159489870071411, Accuracy: 1.0, Computation time: 1.8225994110107422\n",
      "Step: 1926, Loss: 0.9160247445106506, Accuracy: 1.0, Computation time: 2.23418927192688\n",
      "Step: 1927, Loss: 0.915890634059906, Accuracy: 1.0, Computation time: 1.7960631847381592\n",
      "Step: 1928, Loss: 0.9161317348480225, Accuracy: 1.0, Computation time: 1.8267009258270264\n",
      "Step: 1929, Loss: 0.9160149097442627, Accuracy: 1.0, Computation time: 1.6518616676330566\n",
      "Step: 1930, Loss: 0.9160187840461731, Accuracy: 1.0, Computation time: 2.0258052349090576\n",
      "Step: 1931, Loss: 0.9159773588180542, Accuracy: 1.0, Computation time: 1.8168995380401611\n",
      "Step: 1932, Loss: 0.9159753322601318, Accuracy: 1.0, Computation time: 1.6328344345092773\n",
      "Step: 1933, Loss: 0.916924238204956, Accuracy: 1.0, Computation time: 2.1162314414978027\n",
      "Step: 1934, Loss: 0.9284242391586304, Accuracy: 0.96875, Computation time: 1.869035243988037\n",
      "Step: 1935, Loss: 0.9160763621330261, Accuracy: 1.0, Computation time: 1.7055914402008057\n",
      "Step: 1936, Loss: 0.9160912036895752, Accuracy: 1.0, Computation time: 1.976306676864624\n",
      "Step: 1937, Loss: 0.9161713719367981, Accuracy: 1.0, Computation time: 1.7059834003448486\n",
      "Step: 1938, Loss: 0.9160478115081787, Accuracy: 1.0, Computation time: 1.882591724395752\n",
      "Step: 1939, Loss: 0.9161047339439392, Accuracy: 1.0, Computation time: 1.5113916397094727\n",
      "Step: 1940, Loss: 0.916106641292572, Accuracy: 1.0, Computation time: 1.8277812004089355\n",
      "Step: 1941, Loss: 0.9377800226211548, Accuracy: 0.96875, Computation time: 1.7831292152404785\n",
      "Step: 1942, Loss: 0.9160357713699341, Accuracy: 1.0, Computation time: 1.8167271614074707\n",
      "Step: 1943, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 1.464355707168579\n",
      "Step: 1944, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.9557862281799316\n",
      "Step: 1945, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 1.8599369525909424\n",
      "Step: 1946, Loss: 0.9159877896308899, Accuracy: 1.0, Computation time: 2.274763584136963\n",
      "########################\n",
      "Test loss: 1.1128549575805664, Test Accuracy_epoch14: 0.7142857313156128\n",
      "########################\n",
      "Step: 1947, Loss: 0.9160090088844299, Accuracy: 1.0, Computation time: 1.6945178508758545\n",
      "Step: 1948, Loss: 0.9162229299545288, Accuracy: 1.0, Computation time: 1.8139748573303223\n",
      "Step: 1949, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.442617654800415\n",
      "Step: 1950, Loss: 0.9160242676734924, Accuracy: 1.0, Computation time: 1.736577033996582\n",
      "Step: 1951, Loss: 0.9159353971481323, Accuracy: 1.0, Computation time: 1.60878324508667\n",
      "Step: 1952, Loss: 0.9365081787109375, Accuracy: 0.96875, Computation time: 1.8067502975463867\n",
      "Step: 1953, Loss: 0.9159711003303528, Accuracy: 1.0, Computation time: 1.5353679656982422\n",
      "Step: 1954, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 2.016925811767578\n",
      "Step: 1955, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.7335495948791504\n",
      "Step: 1956, Loss: 0.915998101234436, Accuracy: 1.0, Computation time: 2.092749834060669\n",
      "Step: 1957, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.659574031829834\n",
      "Step: 1958, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.6662657260894775\n",
      "Step: 1959, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.585200309753418\n",
      "Step: 1960, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.4647281169891357\n",
      "Step: 1961, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.6606099605560303\n",
      "Step: 1962, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 2.2229092121124268\n",
      "Step: 1963, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 2.2519264221191406\n",
      "Step: 1964, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 1.6455812454223633\n",
      "Step: 1965, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.516523838043213\n",
      "Step: 1966, Loss: 0.937760591506958, Accuracy: 0.96875, Computation time: 1.840759515762329\n",
      "Step: 1967, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 1.4341676235198975\n",
      "Step: 1968, Loss: 0.9375333786010742, Accuracy: 0.96875, Computation time: 1.9546263217926025\n",
      "Step: 1969, Loss: 0.9403526782989502, Accuracy: 0.96875, Computation time: 1.5900697708129883\n",
      "Step: 1970, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.594123125076294\n",
      "Step: 1971, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 2.0477705001831055\n",
      "Step: 1972, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.4681673049926758\n",
      "Step: 1973, Loss: 0.9159784913063049, Accuracy: 1.0, Computation time: 1.8377420902252197\n",
      "Step: 1974, Loss: 0.9378924369812012, Accuracy: 0.96875, Computation time: 1.3907771110534668\n",
      "Step: 1975, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 1.5178313255310059\n",
      "Step: 1976, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.7949719429016113\n",
      "Step: 1977, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 2.0899717807769775\n",
      "Step: 1978, Loss: 0.9180538654327393, Accuracy: 1.0, Computation time: 1.5818595886230469\n",
      "Step: 1979, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.65861177444458\n",
      "Step: 1980, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.382964849472046\n",
      "Step: 1981, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.7276368141174316\n",
      "Step: 1982, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 1.3991777896881104\n",
      "Step: 1983, Loss: 0.9376661777496338, Accuracy: 0.96875, Computation time: 1.5510759353637695\n",
      "Step: 1984, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 2.076514720916748\n",
      "Step: 1985, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.6629705429077148\n",
      "Step: 1986, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.7528297901153564\n",
      "Step: 1987, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 1.6347177028656006\n",
      "Step: 1988, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.6291930675506592\n",
      "Step: 1989, Loss: 0.916064441204071, Accuracy: 1.0, Computation time: 2.0827155113220215\n",
      "Step: 1990, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.7212388515472412\n",
      "Step: 1991, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.6793310642242432\n",
      "Step: 1992, Loss: 0.9378359913825989, Accuracy: 0.96875, Computation time: 1.604686975479126\n",
      "Step: 1993, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.5868091583251953\n",
      "Step: 1994, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.7596087455749512\n",
      "Step: 1995, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 2.1983835697174072\n",
      "Step: 1996, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.497061014175415\n",
      "Step: 1997, Loss: 0.9158888459205627, Accuracy: 1.0, Computation time: 1.8367435932159424\n",
      "Step: 1998, Loss: 0.9319906234741211, Accuracy: 0.96875, Computation time: 2.0704784393310547\n",
      "Step: 1999, Loss: 0.9160153865814209, Accuracy: 1.0, Computation time: 1.4609363079071045\n",
      "Step: 2000, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.9011759757995605\n",
      "Step: 2001, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.6424682140350342\n",
      "Step: 2002, Loss: 0.9164412021636963, Accuracy: 1.0, Computation time: 1.7636075019836426\n",
      "Step: 2003, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.8509535789489746\n",
      "Step: 2004, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.6860108375549316\n",
      "Step: 2005, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.7724859714508057\n",
      "Step: 2006, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 1.201507329940796\n",
      "Step: 2007, Loss: 0.9372498989105225, Accuracy: 0.96875, Computation time: 1.607975721359253\n",
      "Step: 2008, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.5724010467529297\n",
      "Step: 2009, Loss: 0.9173097014427185, Accuracy: 1.0, Computation time: 1.801724910736084\n",
      "Step: 2010, Loss: 0.9158712029457092, Accuracy: 1.0, Computation time: 1.7372384071350098\n",
      "Step: 2011, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.6493806838989258\n",
      "Step: 2012, Loss: 0.9159483909606934, Accuracy: 1.0, Computation time: 1.7832980155944824\n",
      "Step: 2013, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.7741751670837402\n",
      "Step: 2014, Loss: 0.9234225153923035, Accuracy: 1.0, Computation time: 1.8197815418243408\n",
      "Step: 2015, Loss: 0.9159573912620544, Accuracy: 1.0, Computation time: 1.7737557888031006\n",
      "Step: 2016, Loss: 0.9174273014068604, Accuracy: 1.0, Computation time: 1.9798774719238281\n",
      "Step: 2017, Loss: 0.9370564818382263, Accuracy: 0.96875, Computation time: 1.9331653118133545\n",
      "Step: 2018, Loss: 0.915932297706604, Accuracy: 1.0, Computation time: 2.3098957538604736\n",
      "Step: 2019, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 1.5521581172943115\n",
      "Step: 2020, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 2.0972323417663574\n",
      "Step: 2021, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 2.19861102104187\n",
      "Step: 2022, Loss: 0.9180725812911987, Accuracy: 1.0, Computation time: 1.7747166156768799\n",
      "Step: 2023, Loss: 0.9159244298934937, Accuracy: 1.0, Computation time: 1.6397583484649658\n",
      "Step: 2024, Loss: 0.9159571528434753, Accuracy: 1.0, Computation time: 1.6621100902557373\n",
      "Step: 2025, Loss: 0.916618287563324, Accuracy: 1.0, Computation time: 2.513249635696411\n",
      "Step: 2026, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 1.701634407043457\n",
      "Step: 2027, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 1.7537529468536377\n",
      "Step: 2028, Loss: 0.9161204695701599, Accuracy: 1.0, Computation time: 1.8635358810424805\n",
      "Step: 2029, Loss: 0.9216248989105225, Accuracy: 1.0, Computation time: 2.096619129180908\n",
      "Step: 2030, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.7076303958892822\n",
      "Step: 2031, Loss: 0.9375492334365845, Accuracy: 0.96875, Computation time: 1.636885166168213\n",
      "Step: 2032, Loss: 0.9372999668121338, Accuracy: 0.96875, Computation time: 2.2765543460845947\n",
      "Step: 2033, Loss: 0.9159544110298157, Accuracy: 1.0, Computation time: 1.9776999950408936\n",
      "Step: 2034, Loss: 0.9219868183135986, Accuracy: 1.0, Computation time: 1.8475825786590576\n",
      "Step: 2035, Loss: 0.9321740865707397, Accuracy: 0.96875, Computation time: 1.8588218688964844\n",
      "Step: 2036, Loss: 0.9378143548965454, Accuracy: 0.96875, Computation time: 1.7158851623535156\n",
      "Step: 2037, Loss: 0.93764328956604, Accuracy: 0.96875, Computation time: 1.7242648601531982\n",
      "Step: 2038, Loss: 0.917891800403595, Accuracy: 1.0, Computation time: 2.1048240661621094\n",
      "Step: 2039, Loss: 0.9168140292167664, Accuracy: 1.0, Computation time: 1.9881296157836914\n",
      "Step: 2040, Loss: 0.9161632657051086, Accuracy: 1.0, Computation time: 2.068533182144165\n",
      "Step: 2041, Loss: 0.9375579953193665, Accuracy: 0.96875, Computation time: 2.0874526500701904\n",
      "Step: 2042, Loss: 0.9160844683647156, Accuracy: 1.0, Computation time: 2.0543057918548584\n",
      "Step: 2043, Loss: 0.9376266002655029, Accuracy: 0.96875, Computation time: 1.9769999980926514\n",
      "Step: 2044, Loss: 0.9161376953125, Accuracy: 1.0, Computation time: 2.080059766769409\n",
      "Step: 2045, Loss: 0.9161401391029358, Accuracy: 1.0, Computation time: 1.9499614238739014\n",
      "Step: 2046, Loss: 0.9167641997337341, Accuracy: 1.0, Computation time: 2.4556379318237305\n",
      "Step: 2047, Loss: 0.9160984754562378, Accuracy: 1.0, Computation time: 2.204231023788452\n",
      "Step: 2048, Loss: 0.9160618185997009, Accuracy: 1.0, Computation time: 2.058291435241699\n",
      "Step: 2049, Loss: 0.9161087870597839, Accuracy: 1.0, Computation time: 2.0091569423675537\n",
      "Step: 2050, Loss: 0.9160610437393188, Accuracy: 1.0, Computation time: 2.5369815826416016\n",
      "Step: 2051, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 2.172085762023926\n",
      "Step: 2052, Loss: 0.9166895747184753, Accuracy: 1.0, Computation time: 2.253779888153076\n",
      "Step: 2053, Loss: 0.9160915017127991, Accuracy: 1.0, Computation time: 1.9833433628082275\n",
      "Step: 2054, Loss: 0.933851420879364, Accuracy: 0.96875, Computation time: 1.8861205577850342\n",
      "Step: 2055, Loss: 0.937256932258606, Accuracy: 0.96875, Computation time: 2.5499660968780518\n",
      "Step: 2056, Loss: 0.9161480665206909, Accuracy: 1.0, Computation time: 2.2197868824005127\n",
      "Step: 2057, Loss: 0.9379337430000305, Accuracy: 0.96875, Computation time: 2.3361265659332275\n",
      "Step: 2058, Loss: 0.9160038828849792, Accuracy: 1.0, Computation time: 2.5083999633789062\n",
      "Step: 2059, Loss: 0.9267098307609558, Accuracy: 0.96875, Computation time: 2.1188952922821045\n",
      "Step: 2060, Loss: 0.9160051941871643, Accuracy: 1.0, Computation time: 2.275163412094116\n",
      "Step: 2061, Loss: 0.9161661267280579, Accuracy: 1.0, Computation time: 2.013795852661133\n",
      "Step: 2062, Loss: 0.9162018895149231, Accuracy: 1.0, Computation time: 2.3918774127960205\n",
      "Step: 2063, Loss: 0.9161272048950195, Accuracy: 1.0, Computation time: 2.397637128829956\n",
      "Step: 2064, Loss: 0.9377854466438293, Accuracy: 0.96875, Computation time: 1.9045190811157227\n",
      "Step: 2065, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 2.14365553855896\n",
      "Step: 2066, Loss: 0.9160277843475342, Accuracy: 1.0, Computation time: 1.9874002933502197\n",
      "Step: 2067, Loss: 0.9159213304519653, Accuracy: 1.0, Computation time: 1.6517434120178223\n",
      "Step: 2068, Loss: 0.9159990549087524, Accuracy: 1.0, Computation time: 2.4930102825164795\n",
      "Step: 2069, Loss: 0.9159671664237976, Accuracy: 1.0, Computation time: 2.1364145278930664\n",
      "Step: 2070, Loss: 0.9379487037658691, Accuracy: 0.96875, Computation time: 2.574375629425049\n",
      "Step: 2071, Loss: 0.9378094673156738, Accuracy: 0.96875, Computation time: 2.0141117572784424\n",
      "Step: 2072, Loss: 0.9170324802398682, Accuracy: 1.0, Computation time: 2.058312177658081\n",
      "Step: 2073, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.823194980621338\n",
      "Step: 2074, Loss: 0.915953516960144, Accuracy: 1.0, Computation time: 1.9615395069122314\n",
      "Step: 2075, Loss: 0.9375819563865662, Accuracy: 0.96875, Computation time: 2.3583383560180664\n",
      "Step: 2076, Loss: 0.9159705638885498, Accuracy: 1.0, Computation time: 2.143807888031006\n",
      "Step: 2077, Loss: 0.9179325103759766, Accuracy: 1.0, Computation time: 1.9952292442321777\n",
      "Step: 2078, Loss: 0.9160465598106384, Accuracy: 1.0, Computation time: 2.2743074893951416\n",
      "Step: 2079, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.7944843769073486\n",
      "Step: 2080, Loss: 0.9376549124717712, Accuracy: 0.96875, Computation time: 2.2555017471313477\n",
      "Step: 2081, Loss: 0.9246166348457336, Accuracy: 1.0, Computation time: 2.3911166191101074\n",
      "Step: 2082, Loss: 0.9165173768997192, Accuracy: 1.0, Computation time: 1.975468397140503\n",
      "Step: 2083, Loss: 0.9159685969352722, Accuracy: 1.0, Computation time: 1.6327040195465088\n",
      "Step: 2084, Loss: 0.9179635643959045, Accuracy: 1.0, Computation time: 1.9968855381011963\n",
      "Step: 2085, Loss: 0.9159446954727173, Accuracy: 1.0, Computation time: 1.9113972187042236\n",
      "########################\n",
      "Test loss: 1.1167675256729126, Test Accuracy_epoch15: 0.7105990648269653\n",
      "########################\n",
      "Step: 2086, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.5627059936523438\n",
      "Step: 2087, Loss: 0.9159610271453857, Accuracy: 1.0, Computation time: 1.6548855304718018\n",
      "Step: 2088, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.802480936050415\n",
      "Step: 2089, Loss: 0.9161545038223267, Accuracy: 1.0, Computation time: 1.4709384441375732\n",
      "Step: 2090, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 1.6070854663848877\n",
      "Step: 2091, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.4602367877960205\n",
      "Step: 2092, Loss: 0.9164456129074097, Accuracy: 1.0, Computation time: 1.785367488861084\n",
      "Step: 2093, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 1.8104987144470215\n",
      "Step: 2094, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.781123399734497\n",
      "Step: 2095, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.466897964477539\n",
      "Step: 2096, Loss: 0.9211958050727844, Accuracy: 1.0, Computation time: 1.51539945602417\n",
      "Step: 2097, Loss: 0.9159721732139587, Accuracy: 1.0, Computation time: 1.7265191078186035\n",
      "Step: 2098, Loss: 0.9169142246246338, Accuracy: 1.0, Computation time: 1.8713154792785645\n",
      "Step: 2099, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.8764674663543701\n",
      "Step: 2100, Loss: 0.9363142251968384, Accuracy: 0.96875, Computation time: 2.804535150527954\n",
      "Step: 2101, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.905653476715088\n",
      "Step: 2102, Loss: 0.9161075353622437, Accuracy: 1.0, Computation time: 1.7864618301391602\n",
      "Step: 2103, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.5961875915527344\n",
      "Step: 2104, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 1.7002851963043213\n",
      "Step: 2105, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.72579026222229\n",
      "Step: 2106, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 2.063328504562378\n",
      "Step: 2107, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.6167271137237549\n",
      "Step: 2108, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.7694907188415527\n",
      "Step: 2109, Loss: 0.9362854957580566, Accuracy: 0.96875, Computation time: 1.8181164264678955\n",
      "Step: 2110, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.5783765316009521\n",
      "Step: 2111, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.941361665725708\n",
      "Step: 2112, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.715193510055542\n",
      "Step: 2113, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.4762482643127441\n",
      "Step: 2114, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 2.030273199081421\n",
      "Step: 2115, Loss: 0.9160538911819458, Accuracy: 1.0, Computation time: 1.4830915927886963\n",
      "Step: 2116, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.5339713096618652\n",
      "Step: 2117, Loss: 0.9169062376022339, Accuracy: 1.0, Computation time: 1.9807133674621582\n",
      "Step: 2118, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 1.5207207202911377\n",
      "Step: 2119, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.7281911373138428\n",
      "Step: 2120, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.5319077968597412\n",
      "Step: 2121, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.6823797225952148\n",
      "Step: 2122, Loss: 0.9159387350082397, Accuracy: 1.0, Computation time: 1.512664794921875\n",
      "Step: 2123, Loss: 0.9178329110145569, Accuracy: 1.0, Computation time: 1.6834673881530762\n",
      "Step: 2124, Loss: 0.9168745279312134, Accuracy: 1.0, Computation time: 1.8082852363586426\n",
      "Step: 2125, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.5525333881378174\n",
      "Step: 2126, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 2.041008710861206\n",
      "Step: 2127, Loss: 0.9158967137336731, Accuracy: 1.0, Computation time: 1.519178867340088\n",
      "Step: 2128, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.6385219097137451\n",
      "Step: 2129, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 1.4224839210510254\n",
      "Step: 2130, Loss: 0.9159368872642517, Accuracy: 1.0, Computation time: 1.265406847000122\n",
      "Step: 2131, Loss: 0.9377673268318176, Accuracy: 0.96875, Computation time: 1.6539249420166016\n",
      "Step: 2132, Loss: 0.9370831847190857, Accuracy: 0.96875, Computation time: 1.6669132709503174\n",
      "Step: 2133, Loss: 0.9375495314598083, Accuracy: 0.96875, Computation time: 1.444730520248413\n",
      "Step: 2134, Loss: 0.9159056544303894, Accuracy: 1.0, Computation time: 1.2135860919952393\n",
      "Step: 2135, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.6893501281738281\n",
      "Step: 2136, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 1.7149031162261963\n",
      "Step: 2137, Loss: 0.9159830212593079, Accuracy: 1.0, Computation time: 1.4012410640716553\n",
      "Step: 2138, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.6242656707763672\n",
      "Step: 2139, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 1.535599708557129\n",
      "Step: 2140, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.5990865230560303\n",
      "Step: 2141, Loss: 0.9167097210884094, Accuracy: 1.0, Computation time: 1.4519267082214355\n",
      "Step: 2142, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.50522780418396\n",
      "Step: 2143, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.6693103313446045\n",
      "Step: 2144, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.6466386318206787\n",
      "Step: 2145, Loss: 0.9316626787185669, Accuracy: 0.96875, Computation time: 1.6818077564239502\n",
      "Step: 2146, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.5362026691436768\n",
      "Step: 2147, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.2861168384552002\n",
      "Step: 2148, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.6177031993865967\n",
      "Step: 2149, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.64505934715271\n",
      "Step: 2150, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.645115613937378\n",
      "Step: 2151, Loss: 0.9160388708114624, Accuracy: 1.0, Computation time: 1.4557104110717773\n",
      "Step: 2152, Loss: 0.9160348773002625, Accuracy: 1.0, Computation time: 1.9163219928741455\n",
      "Step: 2153, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.2947285175323486\n",
      "Step: 2154, Loss: 0.9379397630691528, Accuracy: 0.96875, Computation time: 1.780430555343628\n",
      "Step: 2155, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.5867149829864502\n",
      "Step: 2156, Loss: 0.9183673858642578, Accuracy: 1.0, Computation time: 2.0788726806640625\n",
      "Step: 2157, Loss: 0.9173333048820496, Accuracy: 1.0, Computation time: 1.9629132747650146\n",
      "Step: 2158, Loss: 0.9171998500823975, Accuracy: 1.0, Computation time: 2.213735580444336\n",
      "Step: 2159, Loss: 0.9361329674720764, Accuracy: 0.96875, Computation time: 1.7305946350097656\n",
      "Step: 2160, Loss: 0.936221182346344, Accuracy: 0.96875, Computation time: 1.726886510848999\n",
      "Step: 2161, Loss: 0.9166437983512878, Accuracy: 1.0, Computation time: 1.9193964004516602\n",
      "Step: 2162, Loss: 0.9159616827964783, Accuracy: 1.0, Computation time: 1.7321486473083496\n",
      "Step: 2163, Loss: 0.9160085916519165, Accuracy: 1.0, Computation time: 1.6606168746948242\n",
      "Step: 2164, Loss: 0.9591846466064453, Accuracy: 0.9375, Computation time: 1.834838628768921\n",
      "Step: 2165, Loss: 0.9162317514419556, Accuracy: 1.0, Computation time: 1.3742234706878662\n",
      "Step: 2166, Loss: 0.9160305261611938, Accuracy: 1.0, Computation time: 1.552736520767212\n",
      "Step: 2167, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 1.6938703060150146\n",
      "Step: 2168, Loss: 0.9376943707466125, Accuracy: 0.96875, Computation time: 1.8946418762207031\n",
      "Step: 2169, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 1.5207939147949219\n",
      "Step: 2170, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.4582602977752686\n",
      "Step: 2171, Loss: 0.9163845777511597, Accuracy: 1.0, Computation time: 1.694732427597046\n",
      "Step: 2172, Loss: 0.9160630106925964, Accuracy: 1.0, Computation time: 1.835136890411377\n",
      "Step: 2173, Loss: 0.9160671234130859, Accuracy: 1.0, Computation time: 2.3021435737609863\n",
      "Step: 2174, Loss: 0.9160957932472229, Accuracy: 1.0, Computation time: 1.8289344310760498\n",
      "Step: 2175, Loss: 0.9159544110298157, Accuracy: 1.0, Computation time: 1.6911876201629639\n",
      "Step: 2176, Loss: 0.9163787961006165, Accuracy: 1.0, Computation time: 2.0205700397491455\n",
      "Step: 2177, Loss: 0.9171894192695618, Accuracy: 1.0, Computation time: 1.7956337928771973\n",
      "Step: 2178, Loss: 0.9159436225891113, Accuracy: 1.0, Computation time: 1.6762123107910156\n",
      "Step: 2179, Loss: 0.9195922613143921, Accuracy: 1.0, Computation time: 2.766771078109741\n",
      "Step: 2180, Loss: 0.9159824252128601, Accuracy: 1.0, Computation time: 1.7482144832611084\n",
      "Step: 2181, Loss: 0.9199179410934448, Accuracy: 1.0, Computation time: 2.1083731651306152\n",
      "Step: 2182, Loss: 0.9167366027832031, Accuracy: 1.0, Computation time: 1.76680588722229\n",
      "Step: 2183, Loss: 0.9180076718330383, Accuracy: 1.0, Computation time: 1.6928095817565918\n",
      "Step: 2184, Loss: 0.9163523316383362, Accuracy: 1.0, Computation time: 1.86098051071167\n",
      "Step: 2185, Loss: 0.9164494276046753, Accuracy: 1.0, Computation time: 1.6429688930511475\n",
      "Step: 2186, Loss: 0.9161568880081177, Accuracy: 1.0, Computation time: 1.546741247177124\n",
      "Step: 2187, Loss: 0.916077733039856, Accuracy: 1.0, Computation time: 2.2643139362335205\n",
      "Step: 2188, Loss: 0.9160487055778503, Accuracy: 1.0, Computation time: 1.7501516342163086\n",
      "Step: 2189, Loss: 0.9159390330314636, Accuracy: 1.0, Computation time: 2.141685724258423\n",
      "Step: 2190, Loss: 0.9159122109413147, Accuracy: 1.0, Computation time: 1.5634586811065674\n",
      "Step: 2191, Loss: 0.9159947037696838, Accuracy: 1.0, Computation time: 1.980950117111206\n",
      "Step: 2192, Loss: 0.9363255500793457, Accuracy: 0.96875, Computation time: 1.875448226928711\n",
      "Step: 2193, Loss: 0.9163039922714233, Accuracy: 1.0, Computation time: 1.565871000289917\n",
      "Step: 2194, Loss: 0.9161496758460999, Accuracy: 1.0, Computation time: 1.8407154083251953\n",
      "Step: 2195, Loss: 0.9160854816436768, Accuracy: 1.0, Computation time: 2.13020920753479\n",
      "Step: 2196, Loss: 0.9161866307258606, Accuracy: 1.0, Computation time: 1.8078153133392334\n",
      "Step: 2197, Loss: 0.9160411953926086, Accuracy: 1.0, Computation time: 1.733175277709961\n",
      "Step: 2198, Loss: 0.9165583848953247, Accuracy: 1.0, Computation time: 1.6340367794036865\n",
      "Step: 2199, Loss: 0.9526806473731995, Accuracy: 0.9375, Computation time: 2.21478271484375\n",
      "Step: 2200, Loss: 0.9165025353431702, Accuracy: 1.0, Computation time: 1.9241859912872314\n",
      "Step: 2201, Loss: 0.9168214201927185, Accuracy: 1.0, Computation time: 1.7150776386260986\n",
      "Step: 2202, Loss: 0.9161279201507568, Accuracy: 1.0, Computation time: 1.8183603286743164\n",
      "Step: 2203, Loss: 0.9161410927772522, Accuracy: 1.0, Computation time: 1.9190499782562256\n",
      "Step: 2204, Loss: 0.9165903925895691, Accuracy: 1.0, Computation time: 1.9811134338378906\n",
      "Step: 2205, Loss: 0.9161932468414307, Accuracy: 1.0, Computation time: 1.8969621658325195\n",
      "Step: 2206, Loss: 0.9162091612815857, Accuracy: 1.0, Computation time: 1.5933353900909424\n",
      "Step: 2207, Loss: 0.9166490435600281, Accuracy: 1.0, Computation time: 1.9107122421264648\n",
      "Step: 2208, Loss: 0.9379306435585022, Accuracy: 0.96875, Computation time: 1.8685331344604492\n",
      "Step: 2209, Loss: 0.9345596432685852, Accuracy: 0.96875, Computation time: 1.6801562309265137\n",
      "Step: 2210, Loss: 0.943393349647522, Accuracy: 0.9375, Computation time: 2.5288004875183105\n",
      "Step: 2211, Loss: 0.9379449486732483, Accuracy: 0.96875, Computation time: 2.1357572078704834\n",
      "Step: 2212, Loss: 0.9164859652519226, Accuracy: 1.0, Computation time: 2.429594039916992\n",
      "Step: 2213, Loss: 0.9162102341651917, Accuracy: 1.0, Computation time: 2.1837637424468994\n",
      "Step: 2214, Loss: 0.916272759437561, Accuracy: 1.0, Computation time: 1.9189460277557373\n",
      "Step: 2215, Loss: 0.9160879254341125, Accuracy: 1.0, Computation time: 2.1650614738464355\n",
      "Step: 2216, Loss: 0.9159768223762512, Accuracy: 1.0, Computation time: 2.1931087970733643\n",
      "Step: 2217, Loss: 0.9167554378509521, Accuracy: 1.0, Computation time: 2.179621696472168\n",
      "Step: 2218, Loss: 0.9159983396530151, Accuracy: 1.0, Computation time: 2.1406471729278564\n",
      "Step: 2219, Loss: 0.9159703254699707, Accuracy: 1.0, Computation time: 2.1925294399261475\n",
      "Step: 2220, Loss: 0.9164626598358154, Accuracy: 1.0, Computation time: 2.226109504699707\n",
      "Step: 2221, Loss: 0.9297546744346619, Accuracy: 0.96875, Computation time: 2.2364463806152344\n",
      "Step: 2222, Loss: 0.9159136414527893, Accuracy: 1.0, Computation time: 2.085902214050293\n",
      "Step: 2223, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.9896321296691895\n",
      "########################\n",
      "Test loss: 1.1189128160476685, Test Accuracy_epoch16: 0.704147458076477\n",
      "########################\n",
      "Step: 2224, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.947375774383545\n",
      "Step: 2225, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 2.1208746433258057\n",
      "Step: 2226, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 2.03275728225708\n",
      "Step: 2227, Loss: 0.9159439206123352, Accuracy: 1.0, Computation time: 2.0609846115112305\n",
      "Step: 2228, Loss: 0.9213654398918152, Accuracy: 1.0, Computation time: 2.1423094272613525\n",
      "Step: 2229, Loss: 0.9160839319229126, Accuracy: 1.0, Computation time: 1.7558698654174805\n",
      "Step: 2230, Loss: 0.9159505367279053, Accuracy: 1.0, Computation time: 2.3010597229003906\n",
      "Step: 2231, Loss: 0.9205531477928162, Accuracy: 1.0, Computation time: 1.933988332748413\n",
      "Step: 2232, Loss: 0.9159644246101379, Accuracy: 1.0, Computation time: 1.7683250904083252\n",
      "Step: 2233, Loss: 0.9160633087158203, Accuracy: 1.0, Computation time: 1.8371984958648682\n",
      "Step: 2234, Loss: 0.9188414216041565, Accuracy: 1.0, Computation time: 2.3556501865386963\n",
      "Step: 2235, Loss: 0.9161335229873657, Accuracy: 1.0, Computation time: 2.1401381492614746\n",
      "Step: 2236, Loss: 0.9160799980163574, Accuracy: 1.0, Computation time: 1.643061637878418\n",
      "Step: 2237, Loss: 0.9162852764129639, Accuracy: 1.0, Computation time: 1.6292736530303955\n",
      "Step: 2238, Loss: 0.9161713123321533, Accuracy: 1.0, Computation time: 1.617691993713379\n",
      "Step: 2239, Loss: 0.9166423678398132, Accuracy: 1.0, Computation time: 1.6280584335327148\n",
      "Step: 2240, Loss: 0.9161824584007263, Accuracy: 1.0, Computation time: 2.107329845428467\n",
      "Step: 2241, Loss: 0.9268597364425659, Accuracy: 0.96875, Computation time: 1.9833126068115234\n",
      "Step: 2242, Loss: 0.9172301888465881, Accuracy: 1.0, Computation time: 1.9032819271087646\n",
      "Step: 2243, Loss: 0.9159511923789978, Accuracy: 1.0, Computation time: 1.9922239780426025\n",
      "Step: 2244, Loss: 0.915957510471344, Accuracy: 1.0, Computation time: 1.8604469299316406\n",
      "Step: 2245, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 2.193187952041626\n",
      "Step: 2246, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.7015933990478516\n",
      "Step: 2247, Loss: 0.9159606695175171, Accuracy: 1.0, Computation time: 2.1050240993499756\n",
      "Step: 2248, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 1.841947317123413\n",
      "Step: 2249, Loss: 0.9170734286308289, Accuracy: 1.0, Computation time: 1.9040675163269043\n",
      "Step: 2250, Loss: 0.9160045981407166, Accuracy: 1.0, Computation time: 1.585604190826416\n",
      "Step: 2251, Loss: 0.9159671664237976, Accuracy: 1.0, Computation time: 1.6218299865722656\n",
      "Step: 2252, Loss: 0.9379734396934509, Accuracy: 0.96875, Computation time: 1.8014540672302246\n",
      "Step: 2253, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 1.6172730922698975\n",
      "Step: 2254, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 1.4258074760437012\n",
      "Step: 2255, Loss: 0.9160138964653015, Accuracy: 1.0, Computation time: 1.7853033542633057\n",
      "Step: 2256, Loss: 0.94227135181427, Accuracy: 0.96875, Computation time: 3.293689727783203\n",
      "Step: 2257, Loss: 0.9160544872283936, Accuracy: 1.0, Computation time: 1.7198731899261475\n",
      "Step: 2258, Loss: 0.9163927435874939, Accuracy: 1.0, Computation time: 1.799811601638794\n",
      "Step: 2259, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 1.6263060569763184\n",
      "Step: 2260, Loss: 0.9379660487174988, Accuracy: 0.96875, Computation time: 1.7607767581939697\n",
      "Step: 2261, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 1.4937283992767334\n",
      "Step: 2262, Loss: 0.9178921580314636, Accuracy: 1.0, Computation time: 2.010648727416992\n",
      "Step: 2263, Loss: 0.9161033630371094, Accuracy: 1.0, Computation time: 1.996034860610962\n",
      "Step: 2264, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 2.10632061958313\n",
      "Step: 2265, Loss: 0.934125542640686, Accuracy: 0.96875, Computation time: 1.9835658073425293\n",
      "Step: 2266, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.9356119632720947\n",
      "Step: 2267, Loss: 0.9159390330314636, Accuracy: 1.0, Computation time: 2.1357040405273438\n",
      "Step: 2268, Loss: 0.9170633554458618, Accuracy: 1.0, Computation time: 1.8675878047943115\n",
      "Step: 2269, Loss: 0.9287685751914978, Accuracy: 0.96875, Computation time: 1.7761869430541992\n",
      "Step: 2270, Loss: 0.9159513115882874, Accuracy: 1.0, Computation time: 1.759587049484253\n",
      "Step: 2271, Loss: 0.9159760475158691, Accuracy: 1.0, Computation time: 1.5859375\n",
      "Step: 2272, Loss: 0.9160022139549255, Accuracy: 1.0, Computation time: 1.8860547542572021\n",
      "Step: 2273, Loss: 0.9375703930854797, Accuracy: 0.96875, Computation time: 1.8473634719848633\n",
      "Step: 2274, Loss: 0.9159924387931824, Accuracy: 1.0, Computation time: 1.6142289638519287\n",
      "Step: 2275, Loss: 0.9160231351852417, Accuracy: 1.0, Computation time: 1.5579943656921387\n",
      "Step: 2276, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.5260181427001953\n",
      "Step: 2277, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.9274024963378906\n",
      "Step: 2278, Loss: 0.915961503982544, Accuracy: 1.0, Computation time: 1.7043581008911133\n",
      "Step: 2279, Loss: 0.921187162399292, Accuracy: 1.0, Computation time: 2.6370654106140137\n",
      "Step: 2280, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.7362637519836426\n",
      "Step: 2281, Loss: 0.9181675910949707, Accuracy: 1.0, Computation time: 2.005603551864624\n",
      "Step: 2282, Loss: 0.916066586971283, Accuracy: 1.0, Computation time: 1.9695391654968262\n",
      "Step: 2283, Loss: 0.9161077737808228, Accuracy: 1.0, Computation time: 1.7300000190734863\n",
      "Step: 2284, Loss: 0.9159762859344482, Accuracy: 1.0, Computation time: 1.4864838123321533\n",
      "Step: 2285, Loss: 0.9159873127937317, Accuracy: 1.0, Computation time: 2.0913076400756836\n",
      "Step: 2286, Loss: 0.916005551815033, Accuracy: 1.0, Computation time: 1.7195730209350586\n",
      "Step: 2287, Loss: 0.9159922003746033, Accuracy: 1.0, Computation time: 1.787278413772583\n",
      "Step: 2288, Loss: 0.9161098003387451, Accuracy: 1.0, Computation time: 1.7776927947998047\n",
      "Step: 2289, Loss: 0.9159600734710693, Accuracy: 1.0, Computation time: 2.146059036254883\n",
      "Step: 2290, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.641657829284668\n",
      "Step: 2291, Loss: 0.9355819821357727, Accuracy: 0.96875, Computation time: 2.368487596511841\n",
      "Step: 2292, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 1.7395298480987549\n",
      "Step: 2293, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 1.6651053428649902\n",
      "Step: 2294, Loss: 0.9376283288002014, Accuracy: 0.96875, Computation time: 1.8107585906982422\n",
      "Step: 2295, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 1.6427056789398193\n",
      "Step: 2296, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.7273170948028564\n",
      "Step: 2297, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 1.7024750709533691\n",
      "Step: 2298, Loss: 0.9161239266395569, Accuracy: 1.0, Computation time: 1.674346923828125\n",
      "Step: 2299, Loss: 0.9159292578697205, Accuracy: 1.0, Computation time: 1.3300831317901611\n",
      "Step: 2300, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 1.8571021556854248\n",
      "Step: 2301, Loss: 0.9160135984420776, Accuracy: 1.0, Computation time: 1.8955533504486084\n",
      "Step: 2302, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 1.581162452697754\n",
      "Step: 2303, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.7720491886138916\n",
      "Step: 2304, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.6303472518920898\n",
      "Step: 2305, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.7276825904846191\n",
      "Step: 2306, Loss: 0.9343503713607788, Accuracy: 0.96875, Computation time: 2.35567045211792\n",
      "Step: 2307, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 2.1402370929718018\n",
      "Step: 2308, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.7986128330230713\n",
      "Step: 2309, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.6732814311981201\n",
      "Step: 2310, Loss: 0.9376530647277832, Accuracy: 0.96875, Computation time: 1.962146520614624\n",
      "Step: 2311, Loss: 0.9160433411598206, Accuracy: 1.0, Computation time: 2.008984327316284\n",
      "Step: 2312, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.5935513973236084\n",
      "Step: 2313, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 1.661574125289917\n",
      "Step: 2314, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.9672117233276367\n",
      "Step: 2315, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.4641079902648926\n",
      "Step: 2316, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 2.022273302078247\n",
      "Step: 2317, Loss: 0.9378502368927002, Accuracy: 0.96875, Computation time: 1.6729495525360107\n",
      "Step: 2318, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.8229272365570068\n",
      "Step: 2319, Loss: 0.9361116886138916, Accuracy: 0.96875, Computation time: 1.7559406757354736\n",
      "Step: 2320, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.748727798461914\n",
      "Step: 2321, Loss: 0.9547414779663086, Accuracy: 0.9375, Computation time: 2.4460582733154297\n",
      "Step: 2322, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 2.207714557647705\n",
      "Step: 2323, Loss: 0.9376201629638672, Accuracy: 0.96875, Computation time: 1.7041268348693848\n",
      "Step: 2324, Loss: 0.9160342812538147, Accuracy: 1.0, Computation time: 1.638230800628662\n",
      "Step: 2325, Loss: 0.915949821472168, Accuracy: 1.0, Computation time: 1.5527851581573486\n",
      "Step: 2326, Loss: 0.9159950613975525, Accuracy: 1.0, Computation time: 1.8551478385925293\n",
      "Step: 2327, Loss: 0.9160380959510803, Accuracy: 1.0, Computation time: 2.01027512550354\n",
      "Step: 2328, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 1.777541160583496\n",
      "Step: 2329, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.4305956363677979\n",
      "Step: 2330, Loss: 0.916262686252594, Accuracy: 1.0, Computation time: 1.633458137512207\n",
      "Step: 2331, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 1.5926334857940674\n",
      "Step: 2332, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.8004589080810547\n",
      "Step: 2333, Loss: 0.9166666865348816, Accuracy: 1.0, Computation time: 1.6627233028411865\n",
      "Step: 2334, Loss: 0.9312631487846375, Accuracy: 0.96875, Computation time: 2.318911075592041\n",
      "Step: 2335, Loss: 0.9270631670951843, Accuracy: 0.96875, Computation time: 1.7818255424499512\n",
      "Step: 2336, Loss: 0.9163954257965088, Accuracy: 1.0, Computation time: 1.384186029434204\n",
      "Step: 2337, Loss: 0.9179035425186157, Accuracy: 1.0, Computation time: 2.0892210006713867\n",
      "Step: 2338, Loss: 0.9369145631790161, Accuracy: 0.96875, Computation time: 2.1506240367889404\n",
      "Step: 2339, Loss: 0.916096031665802, Accuracy: 1.0, Computation time: 1.6556789875030518\n",
      "Step: 2340, Loss: 0.9165432453155518, Accuracy: 1.0, Computation time: 1.674278736114502\n",
      "Step: 2341, Loss: 0.9160590767860413, Accuracy: 1.0, Computation time: 1.8009138107299805\n",
      "Step: 2342, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.6992483139038086\n",
      "Step: 2343, Loss: 0.915931761264801, Accuracy: 1.0, Computation time: 1.9687309265136719\n",
      "Step: 2344, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 1.7024707794189453\n",
      "Step: 2345, Loss: 0.9164391160011292, Accuracy: 1.0, Computation time: 2.1452298164367676\n",
      "Step: 2346, Loss: 0.9160877466201782, Accuracy: 1.0, Computation time: 1.9930222034454346\n",
      "Step: 2347, Loss: 0.9160400629043579, Accuracy: 1.0, Computation time: 1.8273565769195557\n",
      "Step: 2348, Loss: 0.9161835312843323, Accuracy: 1.0, Computation time: 1.5696301460266113\n",
      "Step: 2349, Loss: 0.9160710573196411, Accuracy: 1.0, Computation time: 1.906696081161499\n",
      "Step: 2350, Loss: 0.9159812331199646, Accuracy: 1.0, Computation time: 1.5503973960876465\n",
      "Step: 2351, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 1.9343476295471191\n",
      "Step: 2352, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 1.8645470142364502\n",
      "Step: 2353, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 2.342923402786255\n",
      "Step: 2354, Loss: 0.9160677790641785, Accuracy: 1.0, Computation time: 1.937150239944458\n",
      "Step: 2355, Loss: 0.9377254843711853, Accuracy: 0.96875, Computation time: 1.591346025466919\n",
      "Step: 2356, Loss: 0.9160864353179932, Accuracy: 1.0, Computation time: 2.0717968940734863\n",
      "Step: 2357, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.7634632587432861\n",
      "Step: 2358, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 2.053351640701294\n",
      "Step: 2359, Loss: 0.9174761772155762, Accuracy: 1.0, Computation time: 1.8752970695495605\n",
      "Step: 2360, Loss: 0.9161860942840576, Accuracy: 1.0, Computation time: 1.7602910995483398\n",
      "Step: 2361, Loss: 0.9160397052764893, Accuracy: 1.0, Computation time: 1.8073415756225586\n",
      "Step: 2362, Loss: 0.916760265827179, Accuracy: 1.0, Computation time: 1.4981553554534912\n",
      "########################\n",
      "Test loss: 1.1188008785247803, Test Accuracy_epoch17: 0.7032257914543152\n",
      "########################\n",
      "Step: 2363, Loss: 0.9166193604469299, Accuracy: 1.0, Computation time: 1.9089314937591553\n",
      "Step: 2364, Loss: 0.9160985350608826, Accuracy: 1.0, Computation time: 2.102430820465088\n",
      "Step: 2365, Loss: 0.9159952998161316, Accuracy: 1.0, Computation time: 1.6224279403686523\n",
      "Step: 2366, Loss: 0.9376487731933594, Accuracy: 0.96875, Computation time: 1.5790002346038818\n",
      "Step: 2367, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.6881186962127686\n",
      "Step: 2368, Loss: 0.9159321784973145, Accuracy: 1.0, Computation time: 1.6928749084472656\n",
      "Step: 2369, Loss: 0.9160445332527161, Accuracy: 1.0, Computation time: 1.6796271800994873\n",
      "Step: 2370, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 1.7301599979400635\n",
      "Step: 2371, Loss: 0.9159848093986511, Accuracy: 1.0, Computation time: 2.0318074226379395\n",
      "Step: 2372, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 1.7432670593261719\n",
      "Step: 2373, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.9067773818969727\n",
      "Step: 2374, Loss: 0.9365253448486328, Accuracy: 0.96875, Computation time: 1.9923906326293945\n",
      "Step: 2375, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 2.108156442642212\n",
      "Step: 2376, Loss: 0.9212161302566528, Accuracy: 1.0, Computation time: 1.8343970775604248\n",
      "Step: 2377, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 1.5245168209075928\n",
      "Step: 2378, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 2.0196452140808105\n",
      "Step: 2379, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.5223681926727295\n",
      "Step: 2380, Loss: 0.9377114772796631, Accuracy: 0.96875, Computation time: 1.6631102561950684\n",
      "Step: 2381, Loss: 0.9159915447235107, Accuracy: 1.0, Computation time: 1.3440213203430176\n",
      "Step: 2382, Loss: 0.9372394680976868, Accuracy: 0.96875, Computation time: 1.755608320236206\n",
      "Step: 2383, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 1.7500500679016113\n",
      "Step: 2384, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.4769577980041504\n",
      "Step: 2385, Loss: 0.9183225631713867, Accuracy: 1.0, Computation time: 2.0720181465148926\n",
      "Step: 2386, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 1.1627416610717773\n",
      "Step: 2387, Loss: 0.9159499406814575, Accuracy: 1.0, Computation time: 2.058140754699707\n",
      "Step: 2388, Loss: 0.9159793853759766, Accuracy: 1.0, Computation time: 1.760324239730835\n",
      "Step: 2389, Loss: 0.9162577986717224, Accuracy: 1.0, Computation time: 1.6952769756317139\n",
      "Step: 2390, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 1.7133948802947998\n",
      "Step: 2391, Loss: 0.9378079175949097, Accuracy: 0.96875, Computation time: 1.847550868988037\n",
      "Step: 2392, Loss: 0.9369487166404724, Accuracy: 0.96875, Computation time: 1.5212652683258057\n",
      "Step: 2393, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.2379283905029297\n",
      "Step: 2394, Loss: 0.9159703254699707, Accuracy: 1.0, Computation time: 1.8028852939605713\n",
      "Step: 2395, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.668311357498169\n",
      "Step: 2396, Loss: 0.9167428016662598, Accuracy: 1.0, Computation time: 1.533980369567871\n",
      "Step: 2397, Loss: 0.9536206722259521, Accuracy: 0.9375, Computation time: 2.090176582336426\n",
      "Step: 2398, Loss: 0.9159289002418518, Accuracy: 1.0, Computation time: 1.4604976177215576\n",
      "Step: 2399, Loss: 0.9159179329872131, Accuracy: 1.0, Computation time: 1.642228364944458\n",
      "Step: 2400, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.6101338863372803\n",
      "Step: 2401, Loss: 0.9159098863601685, Accuracy: 1.0, Computation time: 1.6714770793914795\n",
      "Step: 2402, Loss: 0.9372262954711914, Accuracy: 0.96875, Computation time: 1.618565559387207\n",
      "Step: 2403, Loss: 0.9174759387969971, Accuracy: 1.0, Computation time: 1.9071173667907715\n",
      "Step: 2404, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.3198997974395752\n",
      "Step: 2405, Loss: 0.9159023761749268, Accuracy: 1.0, Computation time: 1.4346978664398193\n",
      "Step: 2406, Loss: 0.9377390146255493, Accuracy: 0.96875, Computation time: 1.4856352806091309\n",
      "Step: 2407, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 1.533564567565918\n",
      "Step: 2408, Loss: 0.9173252582550049, Accuracy: 1.0, Computation time: 1.5330350399017334\n",
      "Step: 2409, Loss: 0.9160365462303162, Accuracy: 1.0, Computation time: 1.6947340965270996\n",
      "Step: 2410, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.0973384380340576\n",
      "Step: 2411, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.845573902130127\n",
      "Step: 2412, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.5562307834625244\n",
      "Step: 2413, Loss: 0.9163762927055359, Accuracy: 1.0, Computation time: 1.6941568851470947\n",
      "Step: 2414, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.431791067123413\n",
      "Step: 2415, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 1.611964464187622\n",
      "Step: 2416, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.3659648895263672\n",
      "Step: 2417, Loss: 0.9160494208335876, Accuracy: 1.0, Computation time: 1.5007150173187256\n",
      "Step: 2418, Loss: 0.9316183924674988, Accuracy: 0.96875, Computation time: 1.7888009548187256\n",
      "Step: 2419, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.4902253150939941\n",
      "Step: 2420, Loss: 0.9344446063041687, Accuracy: 0.96875, Computation time: 2.144535541534424\n",
      "Step: 2421, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.4098219871520996\n",
      "Step: 2422, Loss: 0.9159480929374695, Accuracy: 1.0, Computation time: 1.6320676803588867\n",
      "Step: 2423, Loss: 0.9375889301300049, Accuracy: 0.96875, Computation time: 1.7263939380645752\n",
      "Step: 2424, Loss: 0.9161056280136108, Accuracy: 1.0, Computation time: 1.8225932121276855\n",
      "Step: 2425, Loss: 0.9159427881240845, Accuracy: 1.0, Computation time: 1.511565923690796\n",
      "Step: 2426, Loss: 0.9175536036491394, Accuracy: 1.0, Computation time: 1.7673742771148682\n",
      "Step: 2427, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.3287396430969238\n",
      "Step: 2428, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.740990400314331\n",
      "Step: 2429, Loss: 0.9168257117271423, Accuracy: 1.0, Computation time: 1.5347387790679932\n",
      "Step: 2430, Loss: 0.91590815782547, Accuracy: 1.0, Computation time: 1.5465552806854248\n",
      "Step: 2431, Loss: 0.9168609380722046, Accuracy: 1.0, Computation time: 1.7794735431671143\n",
      "Step: 2432, Loss: 0.9159700870513916, Accuracy: 1.0, Computation time: 1.5312750339508057\n",
      "Step: 2433, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.2431843280792236\n",
      "Step: 2434, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 1.600114345550537\n",
      "Step: 2435, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 1.7419037818908691\n",
      "Step: 2436, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.448638916015625\n",
      "Step: 2437, Loss: 0.9338434934616089, Accuracy: 0.96875, Computation time: 1.6423397064208984\n",
      "Step: 2438, Loss: 0.9165175557136536, Accuracy: 1.0, Computation time: 1.5471267700195312\n",
      "Step: 2439, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.7744412422180176\n",
      "Step: 2440, Loss: 0.9376866221427917, Accuracy: 0.96875, Computation time: 1.585000991821289\n",
      "Step: 2441, Loss: 0.9231859445571899, Accuracy: 1.0, Computation time: 1.7312805652618408\n",
      "Step: 2442, Loss: 0.9160460233688354, Accuracy: 1.0, Computation time: 1.4910781383514404\n",
      "Step: 2443, Loss: 0.9160441160202026, Accuracy: 1.0, Computation time: 1.4296553134918213\n",
      "Step: 2444, Loss: 0.9159744381904602, Accuracy: 1.0, Computation time: 1.3778748512268066\n",
      "Step: 2445, Loss: 0.9160261750221252, Accuracy: 1.0, Computation time: 1.5286056995391846\n",
      "Step: 2446, Loss: 0.9160458445549011, Accuracy: 1.0, Computation time: 1.4356975555419922\n",
      "Step: 2447, Loss: 0.9160565137863159, Accuracy: 1.0, Computation time: 1.4080393314361572\n",
      "Step: 2448, Loss: 0.9160313606262207, Accuracy: 1.0, Computation time: 1.5940163135528564\n",
      "Step: 2449, Loss: 0.9159824848175049, Accuracy: 1.0, Computation time: 1.7259681224822998\n",
      "Step: 2450, Loss: 0.9159734845161438, Accuracy: 1.0, Computation time: 1.6565959453582764\n",
      "Step: 2451, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 1.6227869987487793\n",
      "Step: 2452, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.7795255184173584\n",
      "Step: 2453, Loss: 0.9171960353851318, Accuracy: 1.0, Computation time: 1.7160258293151855\n",
      "Step: 2454, Loss: 0.9346175193786621, Accuracy: 0.96875, Computation time: 1.4764404296875\n",
      "Step: 2455, Loss: 0.9159464836120605, Accuracy: 1.0, Computation time: 1.8115286827087402\n",
      "Step: 2456, Loss: 0.9160156846046448, Accuracy: 1.0, Computation time: 1.5872302055358887\n",
      "Step: 2457, Loss: 0.9161180257797241, Accuracy: 1.0, Computation time: 1.799046277999878\n",
      "Step: 2458, Loss: 0.916033923625946, Accuracy: 1.0, Computation time: 1.3293325901031494\n",
      "Step: 2459, Loss: 0.9160535335540771, Accuracy: 1.0, Computation time: 1.6433815956115723\n",
      "Step: 2460, Loss: 0.9161281585693359, Accuracy: 1.0, Computation time: 1.8718841075897217\n",
      "Step: 2461, Loss: 0.9168083667755127, Accuracy: 1.0, Computation time: 1.9754390716552734\n",
      "Step: 2462, Loss: 0.9580814838409424, Accuracy: 0.9375, Computation time: 1.6575016975402832\n",
      "Step: 2463, Loss: 0.916857123374939, Accuracy: 1.0, Computation time: 2.126641035079956\n",
      "Step: 2464, Loss: 0.9163086414337158, Accuracy: 1.0, Computation time: 1.5917651653289795\n",
      "Step: 2465, Loss: 0.916887104511261, Accuracy: 1.0, Computation time: 2.0708179473876953\n",
      "Step: 2466, Loss: 0.9165031313896179, Accuracy: 1.0, Computation time: 1.740861177444458\n",
      "Step: 2467, Loss: 0.9163490533828735, Accuracy: 1.0, Computation time: 2.793978691101074\n",
      "Step: 2468, Loss: 0.9377827048301697, Accuracy: 0.96875, Computation time: 1.518716812133789\n",
      "Step: 2469, Loss: 0.9160280823707581, Accuracy: 1.0, Computation time: 1.3957912921905518\n",
      "Step: 2470, Loss: 0.9160975217819214, Accuracy: 1.0, Computation time: 1.386289358139038\n",
      "Step: 2471, Loss: 0.9159988164901733, Accuracy: 1.0, Computation time: 1.5537991523742676\n",
      "Step: 2472, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.6373918056488037\n",
      "Step: 2473, Loss: 0.959480345249176, Accuracy: 0.9375, Computation time: 1.6538419723510742\n",
      "Step: 2474, Loss: 0.9160804152488708, Accuracy: 1.0, Computation time: 1.7132306098937988\n",
      "Step: 2475, Loss: 0.9160013794898987, Accuracy: 1.0, Computation time: 1.5034449100494385\n",
      "Step: 2476, Loss: 0.915934145450592, Accuracy: 1.0, Computation time: 1.4469163417816162\n",
      "Step: 2477, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.2757759094238281\n",
      "Step: 2478, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.9090497493743896\n",
      "Step: 2479, Loss: 0.9160754084587097, Accuracy: 1.0, Computation time: 1.543635368347168\n",
      "Step: 2480, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 1.327272891998291\n",
      "Step: 2481, Loss: 0.9184063673019409, Accuracy: 1.0, Computation time: 1.528487205505371\n",
      "Step: 2482, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 1.210864782333374\n",
      "Step: 2483, Loss: 0.9309960007667542, Accuracy: 0.96875, Computation time: 2.176346778869629\n",
      "Step: 2484, Loss: 0.9163498878479004, Accuracy: 1.0, Computation time: 2.1412980556488037\n",
      "Step: 2485, Loss: 0.9160429239273071, Accuracy: 1.0, Computation time: 1.492811679840088\n",
      "Step: 2486, Loss: 0.9159805774688721, Accuracy: 1.0, Computation time: 1.4401533603668213\n",
      "Step: 2487, Loss: 0.9378169178962708, Accuracy: 0.96875, Computation time: 1.4526493549346924\n",
      "Step: 2488, Loss: 0.9429244995117188, Accuracy: 0.96875, Computation time: 1.8008267879486084\n",
      "Step: 2489, Loss: 0.9219687581062317, Accuracy: 1.0, Computation time: 1.8692152500152588\n",
      "Step: 2490, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.6209197044372559\n",
      "Step: 2491, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.3012659549713135\n",
      "Step: 2492, Loss: 0.9160736203193665, Accuracy: 1.0, Computation time: 1.4182116985321045\n",
      "Step: 2493, Loss: 0.9160826206207275, Accuracy: 1.0, Computation time: 1.2078661918640137\n",
      "Step: 2494, Loss: 0.9162384271621704, Accuracy: 1.0, Computation time: 1.293731927871704\n",
      "Step: 2495, Loss: 0.9380772113800049, Accuracy: 0.96875, Computation time: 1.2742962837219238\n",
      "Step: 2496, Loss: 0.9161202907562256, Accuracy: 1.0, Computation time: 1.444716215133667\n",
      "Step: 2497, Loss: 0.9160388708114624, Accuracy: 1.0, Computation time: 1.3422658443450928\n",
      "Step: 2498, Loss: 0.9160148501396179, Accuracy: 1.0, Computation time: 1.1539599895477295\n",
      "Step: 2499, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.2316815853118896\n",
      "Step: 2500, Loss: 0.9176117181777954, Accuracy: 1.0, Computation time: 1.6412336826324463\n",
      "Step: 2501, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.3625156879425049\n",
      "########################\n",
      "Test loss: 1.1146507263183594, Test Accuracy_epoch18: 0.7115207314491272\n",
      "########################\n",
      "Step: 2502, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.2653820514678955\n",
      "Step: 2503, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 2.2607548236846924\n",
      "Step: 2504, Loss: 0.9160280227661133, Accuracy: 1.0, Computation time: 1.3972442150115967\n",
      "Step: 2505, Loss: 0.9162268042564392, Accuracy: 1.0, Computation time: 1.276684045791626\n",
      "Step: 2506, Loss: 0.9163478016853333, Accuracy: 1.0, Computation time: 1.6310689449310303\n",
      "Step: 2507, Loss: 0.9159379601478577, Accuracy: 1.0, Computation time: 1.272810935974121\n",
      "Step: 2508, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 1.5148015022277832\n",
      "Step: 2509, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 1.2881815433502197\n",
      "Step: 2510, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.3252832889556885\n",
      "Step: 2511, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.6305489540100098\n",
      "Step: 2512, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.4254558086395264\n",
      "Step: 2513, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 1.4423291683197021\n",
      "Step: 2514, Loss: 0.9161808490753174, Accuracy: 1.0, Computation time: 1.6826837062835693\n",
      "Step: 2515, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.1810505390167236\n",
      "Step: 2516, Loss: 0.9159121513366699, Accuracy: 1.0, Computation time: 1.509911298751831\n",
      "Step: 2517, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.6115803718566895\n",
      "Step: 2518, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.4001696109771729\n",
      "Step: 2519, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.1775310039520264\n",
      "Step: 2520, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.4577784538269043\n",
      "Step: 2521, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.3790898323059082\n",
      "Step: 2522, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.7293810844421387\n",
      "Step: 2523, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.5246100425720215\n",
      "Step: 2524, Loss: 0.9348835349082947, Accuracy: 0.96875, Computation time: 2.3968546390533447\n",
      "Step: 2525, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.7208442687988281\n",
      "Step: 2526, Loss: 0.9160443544387817, Accuracy: 1.0, Computation time: 1.5456559658050537\n",
      "Step: 2527, Loss: 0.9451358318328857, Accuracy: 0.96875, Computation time: 1.7852933406829834\n",
      "Step: 2528, Loss: 0.9376248121261597, Accuracy: 0.96875, Computation time: 1.927673101425171\n",
      "Step: 2529, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.9896321296691895\n",
      "Step: 2530, Loss: 0.9375460743904114, Accuracy: 0.96875, Computation time: 1.6704137325286865\n",
      "Step: 2531, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.9636406898498535\n",
      "Step: 2532, Loss: 0.9159474968910217, Accuracy: 1.0, Computation time: 1.8550989627838135\n",
      "Step: 2533, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 1.3714711666107178\n",
      "Step: 2534, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 1.5097973346710205\n",
      "Step: 2535, Loss: 0.9361491203308105, Accuracy: 0.96875, Computation time: 1.3275597095489502\n",
      "Step: 2536, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.5097544193267822\n",
      "Step: 2537, Loss: 0.9162797331809998, Accuracy: 1.0, Computation time: 1.9221961498260498\n",
      "Step: 2538, Loss: 0.9361929297447205, Accuracy: 0.96875, Computation time: 1.914628028869629\n",
      "Step: 2539, Loss: 0.9166061878204346, Accuracy: 1.0, Computation time: 1.7642858028411865\n",
      "Step: 2540, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.488290548324585\n",
      "Step: 2541, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 2.0585925579071045\n",
      "Step: 2542, Loss: 0.9160122871398926, Accuracy: 1.0, Computation time: 1.7577521800994873\n",
      "Step: 2543, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.3439314365386963\n",
      "Step: 2544, Loss: 0.9373770952224731, Accuracy: 0.96875, Computation time: 1.513883113861084\n",
      "Step: 2545, Loss: 0.9352105259895325, Accuracy: 0.96875, Computation time: 1.613461971282959\n",
      "Step: 2546, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.659426212310791\n",
      "Step: 2547, Loss: 0.9216351509094238, Accuracy: 1.0, Computation time: 1.7454352378845215\n",
      "Step: 2548, Loss: 0.9172399640083313, Accuracy: 1.0, Computation time: 1.9477760791778564\n",
      "Step: 2549, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 1.4567246437072754\n",
      "Step: 2550, Loss: 0.9163718819618225, Accuracy: 1.0, Computation time: 1.3577227592468262\n",
      "Step: 2551, Loss: 0.9161925911903381, Accuracy: 1.0, Computation time: 1.7538414001464844\n",
      "Step: 2552, Loss: 0.9377554655075073, Accuracy: 0.96875, Computation time: 1.3163294792175293\n",
      "Step: 2553, Loss: 0.9160594344139099, Accuracy: 1.0, Computation time: 1.4027810096740723\n",
      "Step: 2554, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.1557331085205078\n",
      "Step: 2555, Loss: 0.9187211990356445, Accuracy: 1.0, Computation time: 1.7856638431549072\n",
      "Step: 2556, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.4282925128936768\n",
      "Step: 2557, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.5597772598266602\n",
      "Step: 2558, Loss: 0.9376668930053711, Accuracy: 0.96875, Computation time: 1.1890170574188232\n",
      "Step: 2559, Loss: 0.9160289168357849, Accuracy: 1.0, Computation time: 1.3187587261199951\n",
      "Step: 2560, Loss: 0.9296608567237854, Accuracy: 0.96875, Computation time: 1.716068983078003\n",
      "Step: 2561, Loss: 0.9161544442176819, Accuracy: 1.0, Computation time: 1.5308306217193604\n",
      "Step: 2562, Loss: 0.9159569144248962, Accuracy: 1.0, Computation time: 1.4706525802612305\n",
      "Step: 2563, Loss: 0.9159826040267944, Accuracy: 1.0, Computation time: 1.5793697834014893\n",
      "Step: 2564, Loss: 0.9159759879112244, Accuracy: 1.0, Computation time: 1.6548795700073242\n",
      "Step: 2565, Loss: 0.9159960746765137, Accuracy: 1.0, Computation time: 1.7693755626678467\n",
      "Step: 2566, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 1.4070684909820557\n",
      "Step: 2567, Loss: 0.9160186648368835, Accuracy: 1.0, Computation time: 1.5153377056121826\n",
      "Step: 2568, Loss: 0.9164876937866211, Accuracy: 1.0, Computation time: 1.8269522190093994\n",
      "Step: 2569, Loss: 0.9159495234489441, Accuracy: 1.0, Computation time: 1.4672746658325195\n",
      "Step: 2570, Loss: 0.9164025187492371, Accuracy: 1.0, Computation time: 1.6081209182739258\n",
      "Step: 2571, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 1.5717825889587402\n",
      "Step: 2572, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 1.2677371501922607\n",
      "Step: 2573, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 1.1746506690979004\n",
      "Step: 2574, Loss: 0.9214567542076111, Accuracy: 1.0, Computation time: 1.7510757446289062\n",
      "Step: 2575, Loss: 0.9160500168800354, Accuracy: 1.0, Computation time: 2.0699548721313477\n",
      "Step: 2576, Loss: 0.9375858306884766, Accuracy: 0.96875, Computation time: 1.681746006011963\n",
      "Step: 2577, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 1.2259764671325684\n",
      "Step: 2578, Loss: 0.9160623550415039, Accuracy: 1.0, Computation time: 1.4196434020996094\n",
      "Step: 2579, Loss: 0.9159850478172302, Accuracy: 1.0, Computation time: 1.4279980659484863\n",
      "Step: 2580, Loss: 0.9160294532775879, Accuracy: 1.0, Computation time: 1.8635365962982178\n",
      "Step: 2581, Loss: 0.9160782694816589, Accuracy: 1.0, Computation time: 1.7328338623046875\n",
      "Step: 2582, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.541236162185669\n",
      "Step: 2583, Loss: 0.9159942865371704, Accuracy: 1.0, Computation time: 1.7928407192230225\n",
      "Step: 2584, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 1.6309239864349365\n",
      "Step: 2585, Loss: 0.9160553216934204, Accuracy: 1.0, Computation time: 1.814115285873413\n",
      "Step: 2586, Loss: 0.9161286354064941, Accuracy: 1.0, Computation time: 1.3688609600067139\n",
      "Step: 2587, Loss: 0.9160608649253845, Accuracy: 1.0, Computation time: 1.4163613319396973\n",
      "Step: 2588, Loss: 0.9162880182266235, Accuracy: 1.0, Computation time: 1.5406837463378906\n",
      "Step: 2589, Loss: 0.9160712361335754, Accuracy: 1.0, Computation time: 1.5035393238067627\n",
      "Step: 2590, Loss: 0.9163035750389099, Accuracy: 1.0, Computation time: 1.6615984439849854\n",
      "Step: 2591, Loss: 0.9372861385345459, Accuracy: 0.96875, Computation time: 1.7250936031341553\n",
      "Step: 2592, Loss: 0.9160353541374207, Accuracy: 1.0, Computation time: 1.9795384407043457\n",
      "Step: 2593, Loss: 0.9161384105682373, Accuracy: 1.0, Computation time: 2.0321462154388428\n",
      "Step: 2594, Loss: 0.9159116744995117, Accuracy: 1.0, Computation time: 1.9097161293029785\n",
      "Step: 2595, Loss: 0.918448805809021, Accuracy: 1.0, Computation time: 2.123337745666504\n",
      "Step: 2596, Loss: 0.9160023927688599, Accuracy: 1.0, Computation time: 1.6702330112457275\n",
      "Step: 2597, Loss: 0.9159302115440369, Accuracy: 1.0, Computation time: 1.679762601852417\n",
      "Step: 2598, Loss: 0.916343092918396, Accuracy: 1.0, Computation time: 1.8548870086669922\n",
      "Step: 2599, Loss: 0.9159996509552002, Accuracy: 1.0, Computation time: 1.4899265766143799\n",
      "Step: 2600, Loss: 0.9160711765289307, Accuracy: 1.0, Computation time: 1.4922943115234375\n",
      "Step: 2601, Loss: 0.9232078194618225, Accuracy: 1.0, Computation time: 1.747300386428833\n",
      "Step: 2602, Loss: 0.91621333360672, Accuracy: 1.0, Computation time: 1.2910022735595703\n",
      "Step: 2603, Loss: 0.9184015989303589, Accuracy: 1.0, Computation time: 2.12821626663208\n",
      "Step: 2604, Loss: 0.916399359703064, Accuracy: 1.0, Computation time: 1.406907081604004\n",
      "Step: 2605, Loss: 0.9161436557769775, Accuracy: 1.0, Computation time: 1.4862785339355469\n",
      "Step: 2606, Loss: 0.9161439538002014, Accuracy: 1.0, Computation time: 1.2645556926727295\n",
      "Step: 2607, Loss: 0.9164813160896301, Accuracy: 1.0, Computation time: 1.7828986644744873\n",
      "Step: 2608, Loss: 0.9159178137779236, Accuracy: 1.0, Computation time: 1.4121088981628418\n",
      "Step: 2609, Loss: 0.916201651096344, Accuracy: 1.0, Computation time: 1.9293324947357178\n",
      "Step: 2610, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 1.2707066535949707\n",
      "Step: 2611, Loss: 0.9376876354217529, Accuracy: 0.96875, Computation time: 1.3253285884857178\n",
      "Step: 2612, Loss: 0.9161461591720581, Accuracy: 1.0, Computation time: 1.6543622016906738\n",
      "Step: 2613, Loss: 0.9393576383590698, Accuracy: 0.96875, Computation time: 1.338998794555664\n",
      "Step: 2614, Loss: 0.9160703420639038, Accuracy: 1.0, Computation time: 1.080397129058838\n",
      "Step: 2615, Loss: 0.9162242412567139, Accuracy: 1.0, Computation time: 1.3343653678894043\n",
      "Step: 2616, Loss: 0.9160727262496948, Accuracy: 1.0, Computation time: 1.2719619274139404\n",
      "Step: 2617, Loss: 0.938101589679718, Accuracy: 0.96875, Computation time: 1.1703155040740967\n",
      "Step: 2618, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 1.1735668182373047\n",
      "Step: 2619, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.2234864234924316\n",
      "Step: 2620, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.1286747455596924\n",
      "Step: 2621, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.0770297050476074\n",
      "Step: 2622, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 1.1946113109588623\n",
      "Step: 2623, Loss: 0.9160534739494324, Accuracy: 1.0, Computation time: 1.3246278762817383\n",
      "Step: 2624, Loss: 0.9187313318252563, Accuracy: 1.0, Computation time: 1.8766090869903564\n",
      "Step: 2625, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.2047796249389648\n",
      "Step: 2626, Loss: 0.9160071611404419, Accuracy: 1.0, Computation time: 1.332033395767212\n",
      "Step: 2627, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.4047789573669434\n",
      "Step: 2628, Loss: 0.9159139394760132, Accuracy: 1.0, Computation time: 1.0739266872406006\n",
      "Step: 2629, Loss: 0.9167339205741882, Accuracy: 1.0, Computation time: 1.2067899703979492\n",
      "Step: 2630, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.5190343856811523\n",
      "Step: 2631, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 1.1775622367858887\n",
      "Step: 2632, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.4289391040802002\n",
      "Step: 2633, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.409423828125\n",
      "Step: 2634, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.4391133785247803\n",
      "Step: 2635, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.4144904613494873\n",
      "Step: 2636, Loss: 0.9444189667701721, Accuracy: 0.96875, Computation time: 2.1261444091796875\n",
      "Step: 2637, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.2315123081207275\n",
      "Step: 2638, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.6475038528442383\n",
      "Step: 2639, Loss: 0.9374418258666992, Accuracy: 0.96875, Computation time: 1.4078340530395508\n",
      "Step: 2640, Loss: 0.9336739778518677, Accuracy: 0.96875, Computation time: 1.4777188301086426\n",
      "########################\n",
      "Test loss: 1.1163384914398193, Test Accuracy_epoch19: 0.7087557911872864\n",
      "########################\n",
      "Step: 2641, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.3084776401519775\n",
      "Step: 2642, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 1.6462383270263672\n",
      "Step: 2643, Loss: 0.9159473180770874, Accuracy: 1.0, Computation time: 1.4900834560394287\n",
      "Step: 2644, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 2.094186544418335\n",
      "Step: 2645, Loss: 0.9392865896224976, Accuracy: 0.96875, Computation time: 1.3226137161254883\n",
      "Step: 2646, Loss: 0.9233679175376892, Accuracy: 1.0, Computation time: 3.7731053829193115\n",
      "Step: 2647, Loss: 0.9376630187034607, Accuracy: 0.96875, Computation time: 1.7164673805236816\n",
      "Step: 2648, Loss: 0.9357687830924988, Accuracy: 0.96875, Computation time: 1.744251012802124\n",
      "Step: 2649, Loss: 0.9161953330039978, Accuracy: 1.0, Computation time: 1.4635460376739502\n",
      "Step: 2650, Loss: 0.9273214936256409, Accuracy: 0.96875, Computation time: 1.8140599727630615\n",
      "Step: 2651, Loss: 0.9163047671318054, Accuracy: 1.0, Computation time: 1.5012269020080566\n",
      "Step: 2652, Loss: 0.9166907668113708, Accuracy: 1.0, Computation time: 1.526646375656128\n",
      "Step: 2653, Loss: 0.9385300874710083, Accuracy: 0.96875, Computation time: 1.5529747009277344\n",
      "Step: 2654, Loss: 0.9164663553237915, Accuracy: 1.0, Computation time: 1.6288247108459473\n",
      "Step: 2655, Loss: 0.9177688956260681, Accuracy: 1.0, Computation time: 1.9946935176849365\n",
      "Step: 2656, Loss: 0.9160295724868774, Accuracy: 1.0, Computation time: 1.557447910308838\n",
      "Step: 2657, Loss: 0.9426226615905762, Accuracy: 0.96875, Computation time: 2.5040643215179443\n",
      "Step: 2658, Loss: 0.9163962602615356, Accuracy: 1.0, Computation time: 1.5274105072021484\n",
      "Step: 2659, Loss: 0.9165865778923035, Accuracy: 1.0, Computation time: 1.4381279945373535\n",
      "Step: 2660, Loss: 0.9166101217269897, Accuracy: 1.0, Computation time: 1.5814850330352783\n",
      "Step: 2661, Loss: 0.9164672493934631, Accuracy: 1.0, Computation time: 1.6254901885986328\n",
      "Step: 2662, Loss: 0.9162223935127258, Accuracy: 1.0, Computation time: 1.5017237663269043\n",
      "Step: 2663, Loss: 0.916527271270752, Accuracy: 1.0, Computation time: 1.583148717880249\n",
      "Step: 2664, Loss: 0.9164755940437317, Accuracy: 1.0, Computation time: 1.4025676250457764\n",
      "Step: 2665, Loss: 0.9421142339706421, Accuracy: 0.96875, Computation time: 2.1391661167144775\n",
      "Step: 2666, Loss: 0.9379199743270874, Accuracy: 0.96875, Computation time: 2.095883369445801\n",
      "Step: 2667, Loss: 0.916197657585144, Accuracy: 1.0, Computation time: 1.6294887065887451\n",
      "Step: 2668, Loss: 0.9164477586746216, Accuracy: 1.0, Computation time: 1.318955421447754\n",
      "Step: 2669, Loss: 0.9163564443588257, Accuracy: 1.0, Computation time: 1.4164421558380127\n",
      "Step: 2670, Loss: 0.9170446991920471, Accuracy: 1.0, Computation time: 1.5723700523376465\n",
      "Step: 2671, Loss: 0.9162533283233643, Accuracy: 1.0, Computation time: 1.4424307346343994\n",
      "Step: 2672, Loss: 0.9163296818733215, Accuracy: 1.0, Computation time: 1.5503158569335938\n",
      "Step: 2673, Loss: 0.9213815927505493, Accuracy: 1.0, Computation time: 1.6698195934295654\n",
      "Step: 2674, Loss: 0.9181793928146362, Accuracy: 1.0, Computation time: 2.244516372680664\n",
      "Step: 2675, Loss: 0.9161990284919739, Accuracy: 1.0, Computation time: 1.6408169269561768\n",
      "Step: 2676, Loss: 0.9177436232566833, Accuracy: 1.0, Computation time: 1.3413617610931396\n",
      "Step: 2677, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.5336356163024902\n",
      "Step: 2678, Loss: 0.9163920879364014, Accuracy: 1.0, Computation time: 1.4608252048492432\n",
      "Step: 2679, Loss: 0.9368833899497986, Accuracy: 0.96875, Computation time: 2.347231864929199\n",
      "Step: 2680, Loss: 0.9287667274475098, Accuracy: 0.96875, Computation time: 1.8971378803253174\n",
      "Step: 2681, Loss: 0.9196035861968994, Accuracy: 1.0, Computation time: 1.535146713256836\n",
      "Step: 2682, Loss: 0.9245008230209351, Accuracy: 1.0, Computation time: 1.4621176719665527\n",
      "Step: 2683, Loss: 0.9161006212234497, Accuracy: 1.0, Computation time: 1.7223167419433594\n",
      "Step: 2684, Loss: 0.9162131547927856, Accuracy: 1.0, Computation time: 1.726841926574707\n",
      "Step: 2685, Loss: 0.9162098169326782, Accuracy: 1.0, Computation time: 1.606248378753662\n",
      "Step: 2686, Loss: 0.9163445830345154, Accuracy: 1.0, Computation time: 1.4314217567443848\n",
      "Step: 2687, Loss: 0.9160892367362976, Accuracy: 1.0, Computation time: 1.6032524108886719\n",
      "Step: 2688, Loss: 0.9160576462745667, Accuracy: 1.0, Computation time: 2.082430839538574\n",
      "Step: 2689, Loss: 0.9303579330444336, Accuracy: 0.96875, Computation time: 1.8026528358459473\n",
      "Step: 2690, Loss: 0.9160302877426147, Accuracy: 1.0, Computation time: 1.5659587383270264\n",
      "Step: 2691, Loss: 0.9160560369491577, Accuracy: 1.0, Computation time: 1.4948081970214844\n",
      "Step: 2692, Loss: 0.9160987138748169, Accuracy: 1.0, Computation time: 2.0631167888641357\n",
      "Step: 2693, Loss: 0.9374330639839172, Accuracy: 0.96875, Computation time: 1.8021705150604248\n",
      "Step: 2694, Loss: 0.9162606000900269, Accuracy: 1.0, Computation time: 1.9501733779907227\n",
      "Step: 2695, Loss: 0.9163997769355774, Accuracy: 1.0, Computation time: 2.217780590057373\n",
      "Step: 2696, Loss: 0.9162060022354126, Accuracy: 1.0, Computation time: 2.1129496097564697\n",
      "Step: 2697, Loss: 0.9161171317100525, Accuracy: 1.0, Computation time: 1.7771518230438232\n",
      "Step: 2698, Loss: 0.9373919367790222, Accuracy: 0.96875, Computation time: 1.5695269107818604\n",
      "Step: 2699, Loss: 0.9164258241653442, Accuracy: 1.0, Computation time: 2.2164011001586914\n",
      "Step: 2700, Loss: 0.9160892963409424, Accuracy: 1.0, Computation time: 2.178663969039917\n",
      "Step: 2701, Loss: 0.9163122773170471, Accuracy: 1.0, Computation time: 2.121635675430298\n",
      "Step: 2702, Loss: 0.9383167028427124, Accuracy: 0.96875, Computation time: 1.8243317604064941\n",
      "Step: 2703, Loss: 0.9161200523376465, Accuracy: 1.0, Computation time: 1.8124268054962158\n",
      "Step: 2704, Loss: 0.9161528944969177, Accuracy: 1.0, Computation time: 1.835723638534546\n",
      "Step: 2705, Loss: 0.9161841869354248, Accuracy: 1.0, Computation time: 1.8534538745880127\n",
      "Step: 2706, Loss: 0.9297218322753906, Accuracy: 0.96875, Computation time: 1.9891281127929688\n",
      "Step: 2707, Loss: 0.9160846471786499, Accuracy: 1.0, Computation time: 1.7600796222686768\n",
      "Step: 2708, Loss: 0.9364069104194641, Accuracy: 0.96875, Computation time: 1.936835765838623\n",
      "Step: 2709, Loss: 0.9161971211433411, Accuracy: 1.0, Computation time: 2.3984415531158447\n",
      "Step: 2710, Loss: 0.9161933660507202, Accuracy: 1.0, Computation time: 1.6517338752746582\n",
      "Step: 2711, Loss: 0.9161527156829834, Accuracy: 1.0, Computation time: 2.2208242416381836\n",
      "Step: 2712, Loss: 0.9160553216934204, Accuracy: 1.0, Computation time: 2.047490119934082\n",
      "Step: 2713, Loss: 0.9160159230232239, Accuracy: 1.0, Computation time: 1.8691058158874512\n",
      "Step: 2714, Loss: 0.9160323143005371, Accuracy: 1.0, Computation time: 2.528182029724121\n",
      "Step: 2715, Loss: 0.9259027242660522, Accuracy: 0.96875, Computation time: 2.78316593170166\n",
      "Step: 2716, Loss: 0.9163001775741577, Accuracy: 1.0, Computation time: 1.5565013885498047\n",
      "Step: 2717, Loss: 0.9378731846809387, Accuracy: 0.96875, Computation time: 1.7961351871490479\n",
      "Step: 2718, Loss: 0.9161624908447266, Accuracy: 1.0, Computation time: 1.987717628479004\n",
      "Step: 2719, Loss: 0.9171072840690613, Accuracy: 1.0, Computation time: 1.8529548645019531\n",
      "Step: 2720, Loss: 0.9162392020225525, Accuracy: 1.0, Computation time: 1.5731310844421387\n",
      "Step: 2721, Loss: 0.9160080552101135, Accuracy: 1.0, Computation time: 1.3838496208190918\n",
      "Step: 2722, Loss: 0.916215717792511, Accuracy: 1.0, Computation time: 1.8312671184539795\n",
      "Step: 2723, Loss: 0.9161345958709717, Accuracy: 1.0, Computation time: 1.5512280464172363\n",
      "Step: 2724, Loss: 0.916069746017456, Accuracy: 1.0, Computation time: 1.571580410003662\n",
      "Step: 2725, Loss: 0.9205970764160156, Accuracy: 1.0, Computation time: 1.5113465785980225\n",
      "Step: 2726, Loss: 0.9310976266860962, Accuracy: 0.96875, Computation time: 1.6631760597229004\n",
      "Step: 2727, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 1.7917497158050537\n",
      "Step: 2728, Loss: 0.9160635471343994, Accuracy: 1.0, Computation time: 1.5823605060577393\n",
      "Step: 2729, Loss: 0.9170954823493958, Accuracy: 1.0, Computation time: 1.8225178718566895\n",
      "Step: 2730, Loss: 0.91641765832901, Accuracy: 1.0, Computation time: 1.5915288925170898\n",
      "Step: 2731, Loss: 0.9168753027915955, Accuracy: 1.0, Computation time: 1.506699800491333\n",
      "Step: 2732, Loss: 0.9160680174827576, Accuracy: 1.0, Computation time: 2.1535463333129883\n",
      "Step: 2733, Loss: 0.9160394668579102, Accuracy: 1.0, Computation time: 2.1564548015594482\n",
      "Step: 2734, Loss: 0.9380529522895813, Accuracy: 0.96875, Computation time: 1.3253793716430664\n",
      "Step: 2735, Loss: 0.9162002205848694, Accuracy: 1.0, Computation time: 1.188612461090088\n",
      "Step: 2736, Loss: 0.9205384850502014, Accuracy: 1.0, Computation time: 1.6156277656555176\n",
      "Step: 2737, Loss: 0.916312038898468, Accuracy: 1.0, Computation time: 1.391279935836792\n",
      "Step: 2738, Loss: 0.9162043929100037, Accuracy: 1.0, Computation time: 1.603252649307251\n",
      "Step: 2739, Loss: 0.9165676832199097, Accuracy: 1.0, Computation time: 1.3856983184814453\n",
      "Step: 2740, Loss: 0.9247032403945923, Accuracy: 1.0, Computation time: 1.6070222854614258\n",
      "Step: 2741, Loss: 0.9199681878089905, Accuracy: 1.0, Computation time: 2.20609450340271\n",
      "Step: 2742, Loss: 0.9379035830497742, Accuracy: 0.96875, Computation time: 1.6979167461395264\n",
      "Step: 2743, Loss: 0.937425434589386, Accuracy: 0.96875, Computation time: 1.4796850681304932\n",
      "Step: 2744, Loss: 0.9164576530456543, Accuracy: 1.0, Computation time: 1.6226122379302979\n",
      "Step: 2745, Loss: 0.9249976873397827, Accuracy: 1.0, Computation time: 1.9104995727539062\n",
      "Step: 2746, Loss: 0.9160056710243225, Accuracy: 1.0, Computation time: 1.4379091262817383\n",
      "Step: 2747, Loss: 0.9159670472145081, Accuracy: 1.0, Computation time: 1.441220760345459\n",
      "Step: 2748, Loss: 0.9371438026428223, Accuracy: 0.96875, Computation time: 1.589460849761963\n",
      "Step: 2749, Loss: 0.9161779284477234, Accuracy: 1.0, Computation time: 1.8293631076812744\n",
      "Step: 2750, Loss: 0.9163795709609985, Accuracy: 1.0, Computation time: 1.8080852031707764\n",
      "Step: 2751, Loss: 0.916311502456665, Accuracy: 1.0, Computation time: 2.0440025329589844\n",
      "Step: 2752, Loss: 0.9161489009857178, Accuracy: 1.0, Computation time: 1.976975917816162\n",
      "Step: 2753, Loss: 0.9161611199378967, Accuracy: 1.0, Computation time: 1.382542610168457\n",
      "Step: 2754, Loss: 0.931654155254364, Accuracy: 0.96875, Computation time: 1.75632643699646\n",
      "Step: 2755, Loss: 0.9159836173057556, Accuracy: 1.0, Computation time: 1.4513969421386719\n",
      "Step: 2756, Loss: 0.9160684943199158, Accuracy: 1.0, Computation time: 1.7707974910736084\n",
      "Step: 2757, Loss: 0.9162485599517822, Accuracy: 1.0, Computation time: 1.523496150970459\n",
      "Step: 2758, Loss: 0.916111171245575, Accuracy: 1.0, Computation time: 1.5552818775177002\n",
      "Step: 2759, Loss: 0.9160052537918091, Accuracy: 1.0, Computation time: 1.6860809326171875\n",
      "Step: 2760, Loss: 0.9160100221633911, Accuracy: 1.0, Computation time: 1.6269032955169678\n",
      "Step: 2761, Loss: 0.9160003066062927, Accuracy: 1.0, Computation time: 1.509678602218628\n",
      "Step: 2762, Loss: 0.9160587191581726, Accuracy: 1.0, Computation time: 1.3109643459320068\n",
      "Step: 2763, Loss: 0.9160219430923462, Accuracy: 1.0, Computation time: 1.9273300170898438\n",
      "Step: 2764, Loss: 0.9364510178565979, Accuracy: 0.96875, Computation time: 1.8623125553131104\n",
      "Step: 2765, Loss: 0.9159095883369446, Accuracy: 1.0, Computation time: 1.6727533340454102\n",
      "Step: 2766, Loss: 0.9159592986106873, Accuracy: 1.0, Computation time: 1.8464152812957764\n",
      "Step: 2767, Loss: 0.9318936467170715, Accuracy: 0.96875, Computation time: 1.802133321762085\n",
      "Step: 2768, Loss: 0.9186587333679199, Accuracy: 1.0, Computation time: 2.087710380554199\n",
      "Step: 2769, Loss: 0.9159910678863525, Accuracy: 1.0, Computation time: 1.626478672027588\n",
      "Step: 2770, Loss: 0.9159938097000122, Accuracy: 1.0, Computation time: 1.5708270072937012\n",
      "Step: 2771, Loss: 0.916018009185791, Accuracy: 1.0, Computation time: 1.4004075527191162\n",
      "Step: 2772, Loss: 0.9162919521331787, Accuracy: 1.0, Computation time: 1.293607473373413\n",
      "Step: 2773, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.5534577369689941\n",
      "Step: 2774, Loss: 0.9594637751579285, Accuracy: 0.9375, Computation time: 1.337846279144287\n",
      "Step: 2775, Loss: 0.9325867295265198, Accuracy: 0.96875, Computation time: 1.7088615894317627\n",
      "Step: 2776, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.333641767501831\n",
      "Step: 2777, Loss: 0.9160751700401306, Accuracy: 1.0, Computation time: 1.4022111892700195\n",
      "Step: 2778, Loss: 0.916203498840332, Accuracy: 1.0, Computation time: 1.3271214962005615\n",
      "Step: 2779, Loss: 0.9162667393684387, Accuracy: 1.0, Computation time: 1.520383596420288\n",
      "########################\n",
      "Test loss: 1.114999532699585, Test Accuracy_epoch20: 0.7069124579429626\n",
      "########################\n",
      "Step: 2780, Loss: 0.9160944819450378, Accuracy: 1.0, Computation time: 1.1765601634979248\n",
      "Step: 2781, Loss: 0.934618353843689, Accuracy: 0.96875, Computation time: 1.761183500289917\n",
      "Step: 2782, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.5436522960662842\n",
      "Step: 2783, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 1.2747337818145752\n",
      "Step: 2784, Loss: 0.9263402819633484, Accuracy: 0.96875, Computation time: 1.4950532913208008\n",
      "Step: 2785, Loss: 0.9158989787101746, Accuracy: 1.0, Computation time: 1.2657747268676758\n",
      "Step: 2786, Loss: 0.923378586769104, Accuracy: 1.0, Computation time: 1.7128841876983643\n",
      "Step: 2787, Loss: 0.9162399768829346, Accuracy: 1.0, Computation time: 1.4480223655700684\n",
      "Step: 2788, Loss: 0.9159850478172302, Accuracy: 1.0, Computation time: 1.7165687084197998\n",
      "Step: 2789, Loss: 0.9160290360450745, Accuracy: 1.0, Computation time: 1.5885601043701172\n",
      "Step: 2790, Loss: 0.9161803126335144, Accuracy: 1.0, Computation time: 1.4621074199676514\n",
      "Step: 2791, Loss: 0.9271261096000671, Accuracy: 0.96875, Computation time: 1.2608098983764648\n",
      "Step: 2792, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 1.4058480262756348\n",
      "Step: 2793, Loss: 0.9162288904190063, Accuracy: 1.0, Computation time: 1.585386037826538\n",
      "Step: 2794, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.424501657485962\n",
      "Step: 2795, Loss: 0.9159122109413147, Accuracy: 1.0, Computation time: 1.450495719909668\n",
      "Step: 2796, Loss: 0.9160107970237732, Accuracy: 1.0, Computation time: 1.6165695190429688\n",
      "Step: 2797, Loss: 0.9375989437103271, Accuracy: 0.96875, Computation time: 1.693434476852417\n",
      "Step: 2798, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.7020928859710693\n",
      "Step: 2799, Loss: 0.916101336479187, Accuracy: 1.0, Computation time: 1.5923354625701904\n",
      "Step: 2800, Loss: 0.9159528017044067, Accuracy: 1.0, Computation time: 1.417210578918457\n",
      "Step: 2801, Loss: 0.9402222037315369, Accuracy: 0.96875, Computation time: 1.6154532432556152\n",
      "Step: 2802, Loss: 0.9160028100013733, Accuracy: 1.0, Computation time: 1.6483573913574219\n",
      "Step: 2803, Loss: 0.9396783113479614, Accuracy: 0.96875, Computation time: 1.627476453781128\n",
      "Step: 2804, Loss: 0.9159456491470337, Accuracy: 1.0, Computation time: 1.7377381324768066\n",
      "Step: 2805, Loss: 0.9371898174285889, Accuracy: 0.96875, Computation time: 1.9152956008911133\n",
      "Step: 2806, Loss: 0.9160311222076416, Accuracy: 1.0, Computation time: 2.19028902053833\n",
      "Step: 2807, Loss: 0.915998637676239, Accuracy: 1.0, Computation time: 1.5815222263336182\n",
      "Step: 2808, Loss: 0.915977954864502, Accuracy: 1.0, Computation time: 1.4159634113311768\n",
      "Step: 2809, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 1.5644476413726807\n",
      "Step: 2810, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 1.5951650142669678\n",
      "Step: 2811, Loss: 0.9159761667251587, Accuracy: 1.0, Computation time: 1.603649377822876\n",
      "Step: 2812, Loss: 0.9159151911735535, Accuracy: 1.0, Computation time: 1.571131944656372\n",
      "Step: 2813, Loss: 0.9159103631973267, Accuracy: 1.0, Computation time: 1.7870404720306396\n",
      "Step: 2814, Loss: 0.9363664388656616, Accuracy: 0.96875, Computation time: 1.6618235111236572\n",
      "Step: 2815, Loss: 0.9160272479057312, Accuracy: 1.0, Computation time: 1.525313138961792\n",
      "Step: 2816, Loss: 0.9375418424606323, Accuracy: 0.96875, Computation time: 1.840332269668579\n",
      "Step: 2817, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.587552547454834\n",
      "Step: 2818, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.6755778789520264\n",
      "Step: 2819, Loss: 0.931555449962616, Accuracy: 0.96875, Computation time: 1.5772080421447754\n",
      "Step: 2820, Loss: 0.9161078333854675, Accuracy: 1.0, Computation time: 1.5463242530822754\n",
      "Step: 2821, Loss: 0.9322741627693176, Accuracy: 0.96875, Computation time: 1.7299036979675293\n",
      "Step: 2822, Loss: 0.9161244034767151, Accuracy: 1.0, Computation time: 1.4711525440216064\n",
      "Step: 2823, Loss: 0.91606605052948, Accuracy: 1.0, Computation time: 1.6709637641906738\n",
      "Step: 2824, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 1.5075109004974365\n",
      "Step: 2825, Loss: 0.9159334897994995, Accuracy: 1.0, Computation time: 1.4469363689422607\n",
      "Step: 2826, Loss: 0.9174864888191223, Accuracy: 1.0, Computation time: 1.8667778968811035\n",
      "Step: 2827, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 1.661158561706543\n",
      "Step: 2828, Loss: 0.9323437809944153, Accuracy: 0.96875, Computation time: 1.4984679222106934\n",
      "Step: 2829, Loss: 0.9161534905433655, Accuracy: 1.0, Computation time: 1.7931067943572998\n",
      "Step: 2830, Loss: 0.9378221035003662, Accuracy: 0.96875, Computation time: 1.4226386547088623\n",
      "Step: 2831, Loss: 0.9208319187164307, Accuracy: 1.0, Computation time: 1.953568458557129\n",
      "Step: 2832, Loss: 0.9163737297058105, Accuracy: 1.0, Computation time: 1.6089515686035156\n",
      "Step: 2833, Loss: 0.918138861656189, Accuracy: 1.0, Computation time: 1.5620753765106201\n",
      "Step: 2834, Loss: 0.9161019921302795, Accuracy: 1.0, Computation time: 1.6212279796600342\n",
      "Step: 2835, Loss: 0.91615891456604, Accuracy: 1.0, Computation time: 2.121034860610962\n",
      "Step: 2836, Loss: 0.9160404801368713, Accuracy: 1.0, Computation time: 1.4275085926055908\n",
      "Step: 2837, Loss: 0.9159761667251587, Accuracy: 1.0, Computation time: 1.4728879928588867\n",
      "Step: 2838, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 1.294341802597046\n",
      "Step: 2839, Loss: 0.9162430763244629, Accuracy: 1.0, Computation time: 1.5783746242523193\n",
      "Step: 2840, Loss: 0.9161771535873413, Accuracy: 1.0, Computation time: 1.6810753345489502\n",
      "Step: 2841, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 1.4518351554870605\n",
      "Step: 2842, Loss: 0.9160364270210266, Accuracy: 1.0, Computation time: 1.3305590152740479\n",
      "Step: 2843, Loss: 0.9170660376548767, Accuracy: 1.0, Computation time: 1.7282061576843262\n",
      "Step: 2844, Loss: 0.916027843952179, Accuracy: 1.0, Computation time: 1.3137385845184326\n",
      "Step: 2845, Loss: 0.91640305519104, Accuracy: 1.0, Computation time: 1.6166858673095703\n",
      "Step: 2846, Loss: 0.9350457191467285, Accuracy: 0.96875, Computation time: 1.413799524307251\n",
      "Step: 2847, Loss: 0.9169169068336487, Accuracy: 1.0, Computation time: 1.7508835792541504\n",
      "Step: 2848, Loss: 0.9190715551376343, Accuracy: 1.0, Computation time: 1.2646236419677734\n",
      "Step: 2849, Loss: 0.9159365296363831, Accuracy: 1.0, Computation time: 1.7378895282745361\n",
      "Step: 2850, Loss: 0.9160784482955933, Accuracy: 1.0, Computation time: 1.3549463748931885\n",
      "Step: 2851, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.4269778728485107\n",
      "Step: 2852, Loss: 0.9161849021911621, Accuracy: 1.0, Computation time: 1.4233896732330322\n",
      "Step: 2853, Loss: 0.9160325527191162, Accuracy: 1.0, Computation time: 1.3771235942840576\n",
      "Step: 2854, Loss: 0.9161392450332642, Accuracy: 1.0, Computation time: 1.6167972087860107\n",
      "Step: 2855, Loss: 0.9352340698242188, Accuracy: 0.96875, Computation time: 1.4148290157318115\n",
      "Step: 2856, Loss: 0.9161151051521301, Accuracy: 1.0, Computation time: 1.0240066051483154\n",
      "Step: 2857, Loss: 0.9160334467887878, Accuracy: 1.0, Computation time: 1.3244104385375977\n",
      "Step: 2858, Loss: 0.9496616721153259, Accuracy: 0.9375, Computation time: 1.5500645637512207\n",
      "Step: 2859, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.1226165294647217\n",
      "Step: 2860, Loss: 0.9377300143241882, Accuracy: 0.96875, Computation time: 1.1723248958587646\n",
      "Step: 2861, Loss: 0.915976881980896, Accuracy: 1.0, Computation time: 1.159111499786377\n",
      "Step: 2862, Loss: 0.9160360097885132, Accuracy: 1.0, Computation time: 1.7928788661956787\n",
      "Step: 2863, Loss: 0.9161087870597839, Accuracy: 1.0, Computation time: 1.2381858825683594\n",
      "Step: 2864, Loss: 0.9159708619117737, Accuracy: 1.0, Computation time: 1.6294846534729004\n",
      "Step: 2865, Loss: 0.9160522222518921, Accuracy: 1.0, Computation time: 1.3881378173828125\n",
      "Step: 2866, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.3721692562103271\n",
      "Step: 2867, Loss: 0.9280433654785156, Accuracy: 0.96875, Computation time: 1.7983675003051758\n",
      "Step: 2868, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.377793312072754\n",
      "Step: 2869, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.4606263637542725\n",
      "Step: 2870, Loss: 0.9247274994850159, Accuracy: 1.0, Computation time: 1.1399567127227783\n",
      "Step: 2871, Loss: 0.9159921407699585, Accuracy: 1.0, Computation time: 1.2836167812347412\n",
      "Step: 2872, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 1.2971491813659668\n",
      "Step: 2873, Loss: 0.9160741567611694, Accuracy: 1.0, Computation time: 1.1876418590545654\n",
      "Step: 2874, Loss: 0.9166578650474548, Accuracy: 1.0, Computation time: 1.4470953941345215\n",
      "Step: 2875, Loss: 0.9160375595092773, Accuracy: 1.0, Computation time: 1.4118890762329102\n",
      "Step: 2876, Loss: 0.9263703227043152, Accuracy: 0.96875, Computation time: 1.334242343902588\n",
      "Step: 2877, Loss: 0.9159665703773499, Accuracy: 1.0, Computation time: 1.5078496932983398\n",
      "Step: 2878, Loss: 0.9164698719978333, Accuracy: 1.0, Computation time: 1.2483527660369873\n",
      "Step: 2879, Loss: 0.9162195324897766, Accuracy: 1.0, Computation time: 1.5935792922973633\n",
      "Step: 2880, Loss: 0.9281256794929504, Accuracy: 0.96875, Computation time: 1.6345224380493164\n",
      "Step: 2881, Loss: 0.9159657955169678, Accuracy: 1.0, Computation time: 1.215071678161621\n",
      "Step: 2882, Loss: 0.9174460172653198, Accuracy: 1.0, Computation time: 1.3416097164154053\n",
      "Step: 2883, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.4422616958618164\n",
      "Step: 2884, Loss: 0.9159005284309387, Accuracy: 1.0, Computation time: 1.3464856147766113\n",
      "Step: 2885, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.3928401470184326\n",
      "Step: 2886, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.3683056831359863\n",
      "Step: 2887, Loss: 0.9161583185195923, Accuracy: 1.0, Computation time: 1.5313055515289307\n",
      "Step: 2888, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.512861728668213\n",
      "Step: 2889, Loss: 0.9165419936180115, Accuracy: 1.0, Computation time: 1.626985788345337\n",
      "Step: 2890, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.2903916835784912\n",
      "Step: 2891, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.537978172302246\n",
      "Step: 2892, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.3668553829193115\n",
      "Step: 2893, Loss: 0.9159436225891113, Accuracy: 1.0, Computation time: 1.8877289295196533\n",
      "Step: 2894, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.6265130043029785\n",
      "Step: 2895, Loss: 0.937466561794281, Accuracy: 0.96875, Computation time: 1.2689752578735352\n",
      "Step: 2896, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.4676809310913086\n",
      "Step: 2897, Loss: 0.9159992337226868, Accuracy: 1.0, Computation time: 1.263566017150879\n",
      "Step: 2898, Loss: 0.936768651008606, Accuracy: 0.96875, Computation time: 1.8304874897003174\n",
      "Step: 2899, Loss: 0.9332605600357056, Accuracy: 0.96875, Computation time: 1.2852678298950195\n",
      "Step: 2900, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.6080706119537354\n",
      "Step: 2901, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.7977545261383057\n",
      "Step: 2902, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.4464659690856934\n",
      "Step: 2903, Loss: 0.9159647822380066, Accuracy: 1.0, Computation time: 1.6231153011322021\n",
      "Step: 2904, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.8022186756134033\n",
      "Step: 2905, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 1.4317402839660645\n",
      "Step: 2906, Loss: 0.9159762263298035, Accuracy: 1.0, Computation time: 1.4205641746520996\n",
      "Step: 2907, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.3434979915618896\n",
      "Step: 2908, Loss: 0.9160036444664001, Accuracy: 1.0, Computation time: 1.3434548377990723\n",
      "Step: 2909, Loss: 0.9359442591667175, Accuracy: 0.96875, Computation time: 1.4361176490783691\n",
      "Step: 2910, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.2682042121887207\n",
      "Step: 2911, Loss: 0.9375620484352112, Accuracy: 0.96875, Computation time: 1.4309756755828857\n",
      "Step: 2912, Loss: 0.9159548878669739, Accuracy: 1.0, Computation time: 2.0536139011383057\n",
      "Step: 2913, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.4081580638885498\n",
      "Step: 2914, Loss: 0.9159596562385559, Accuracy: 1.0, Computation time: 1.4208698272705078\n",
      "Step: 2915, Loss: 0.9160453677177429, Accuracy: 1.0, Computation time: 1.3266403675079346\n",
      "Step: 2916, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.672532320022583\n",
      "Step: 2917, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 1.3339731693267822\n",
      "Step: 2918, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.6217682361602783\n",
      "########################\n",
      "Test loss: 1.1122092008590698, Test Accuracy_epoch21: 0.7179723381996155\n",
      "########################\n",
      "Step: 2919, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.4476416110992432\n",
      "Step: 2920, Loss: 0.9164211750030518, Accuracy: 1.0, Computation time: 1.4204530715942383\n",
      "Step: 2921, Loss: 0.9376055002212524, Accuracy: 0.96875, Computation time: 1.3785040378570557\n",
      "Step: 2922, Loss: 0.9167592525482178, Accuracy: 1.0, Computation time: 1.8040189743041992\n",
      "Step: 2923, Loss: 0.9162042737007141, Accuracy: 1.0, Computation time: 1.7418665885925293\n",
      "Step: 2924, Loss: 0.9170023202896118, Accuracy: 1.0, Computation time: 1.326261043548584\n",
      "Step: 2925, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.3707845211029053\n",
      "Step: 2926, Loss: 0.9159513711929321, Accuracy: 1.0, Computation time: 1.022538423538208\n",
      "Step: 2927, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.6396386623382568\n",
      "Step: 2928, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.5197668075561523\n",
      "Step: 2929, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.0577960014343262\n",
      "Step: 2930, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.4731225967407227\n",
      "Step: 2931, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.9965546131134033\n",
      "Step: 2932, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.258345603942871\n",
      "Step: 2933, Loss: 0.9179983735084534, Accuracy: 1.0, Computation time: 2.101926565170288\n",
      "Step: 2934, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.5324082374572754\n",
      "Step: 2935, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.250230073928833\n",
      "Step: 2936, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.5206539630889893\n",
      "Step: 2937, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.352759599685669\n",
      "Step: 2938, Loss: 0.937588095664978, Accuracy: 0.96875, Computation time: 1.2240958213806152\n",
      "Step: 2939, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 1.515526294708252\n",
      "Step: 2940, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.6064417362213135\n",
      "Step: 2941, Loss: 0.9362025260925293, Accuracy: 0.96875, Computation time: 1.3910751342773438\n",
      "Step: 2942, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.5707454681396484\n",
      "Step: 2943, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.5948233604431152\n",
      "Step: 2944, Loss: 0.915938675403595, Accuracy: 1.0, Computation time: 1.4594402313232422\n",
      "Step: 2945, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.8739161491394043\n",
      "Step: 2946, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.732548475265503\n",
      "Step: 2947, Loss: 0.9160335659980774, Accuracy: 1.0, Computation time: 1.717344045639038\n",
      "Step: 2948, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.557760238647461\n",
      "Step: 2949, Loss: 0.9182285666465759, Accuracy: 1.0, Computation time: 1.6477832794189453\n",
      "Step: 2950, Loss: 0.9161223769187927, Accuracy: 1.0, Computation time: 1.6903610229492188\n",
      "Step: 2951, Loss: 0.9359927773475647, Accuracy: 0.96875, Computation time: 1.2929911613464355\n",
      "Step: 2952, Loss: 0.9176556468009949, Accuracy: 1.0, Computation time: 1.47261643409729\n",
      "Step: 2953, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.841294527053833\n",
      "Step: 2954, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.6905641555786133\n",
      "Step: 2955, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 1.9842395782470703\n",
      "Step: 2956, Loss: 0.9160326719284058, Accuracy: 1.0, Computation time: 1.5431289672851562\n",
      "Step: 2957, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.405761480331421\n",
      "Step: 2958, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.2677888870239258\n",
      "Step: 2959, Loss: 0.9160629510879517, Accuracy: 1.0, Computation time: 1.0676321983337402\n",
      "Step: 2960, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.5496935844421387\n",
      "Step: 2961, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.3925068378448486\n",
      "Step: 2962, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.3203151226043701\n",
      "Step: 2963, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.4324049949645996\n",
      "Step: 2964, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.7675483226776123\n",
      "Step: 2965, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.5450880527496338\n",
      "Step: 2966, Loss: 0.9198744893074036, Accuracy: 1.0, Computation time: 1.554225206375122\n",
      "Step: 2967, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.6622073650360107\n",
      "Step: 2968, Loss: 0.9186400771141052, Accuracy: 1.0, Computation time: 2.0971388816833496\n",
      "Step: 2969, Loss: 0.9159334301948547, Accuracy: 1.0, Computation time: 1.2201261520385742\n",
      "Step: 2970, Loss: 0.9159500002861023, Accuracy: 1.0, Computation time: 1.8266847133636475\n",
      "Step: 2971, Loss: 0.9159409999847412, Accuracy: 1.0, Computation time: 1.1978435516357422\n",
      "Step: 2972, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.827580213546753\n",
      "Step: 2973, Loss: 0.9378901720046997, Accuracy: 0.96875, Computation time: 1.5384576320648193\n",
      "Step: 2974, Loss: 0.915936291217804, Accuracy: 1.0, Computation time: 1.4293982982635498\n",
      "Step: 2975, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.5628480911254883\n",
      "Step: 2976, Loss: 0.9159500598907471, Accuracy: 1.0, Computation time: 1.7641844749450684\n",
      "Step: 2977, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.4419376850128174\n",
      "Step: 2978, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.290137767791748\n",
      "Step: 2979, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.805086612701416\n",
      "Step: 2980, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.4020397663116455\n",
      "Step: 2981, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.7863543033599854\n",
      "Step: 2982, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 1.5018129348754883\n",
      "Step: 2983, Loss: 0.9165149927139282, Accuracy: 1.0, Computation time: 1.5792334079742432\n",
      "Step: 2984, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.679478406906128\n",
      "Step: 2985, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.4741849899291992\n",
      "Step: 2986, Loss: 0.9375371932983398, Accuracy: 0.96875, Computation time: 1.7942354679107666\n",
      "Step: 2987, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.6352133750915527\n",
      "Step: 2988, Loss: 0.9161252975463867, Accuracy: 1.0, Computation time: 1.7447435855865479\n",
      "Step: 2989, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.153764247894287\n",
      "Step: 2990, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.3662447929382324\n",
      "Step: 2991, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.4703896045684814\n",
      "Step: 2992, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.4099693298339844\n",
      "Step: 2993, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.7962546348571777\n",
      "Step: 2994, Loss: 0.9376550316810608, Accuracy: 0.96875, Computation time: 1.402977705001831\n",
      "Step: 2995, Loss: 0.9178952574729919, Accuracy: 1.0, Computation time: 1.9641470909118652\n",
      "Step: 2996, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.2641875743865967\n",
      "Step: 2997, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.2813429832458496\n",
      "Step: 2998, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.7826793193817139\n",
      "Step: 2999, Loss: 0.9165084362030029, Accuracy: 1.0, Computation time: 1.8724572658538818\n",
      "Step: 3000, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.4231064319610596\n",
      "Step: 3001, Loss: 0.9372743964195251, Accuracy: 0.96875, Computation time: 1.2381854057312012\n",
      "Step: 3002, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 2.0403201580047607\n",
      "Step: 3003, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.5303101539611816\n",
      "Step: 3004, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.4354124069213867\n",
      "Step: 3005, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.4309792518615723\n",
      "Step: 3006, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.6822142601013184\n",
      "Step: 3007, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.8011474609375\n",
      "Step: 3008, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.3857996463775635\n",
      "Step: 3009, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.3519530296325684\n",
      "Step: 3010, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 1.570894718170166\n",
      "Step: 3011, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 1.974069356918335\n",
      "Step: 3012, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.5052027702331543\n",
      "Step: 3013, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.7511849403381348\n",
      "Step: 3014, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.2842376232147217\n",
      "Step: 3015, Loss: 0.9160045385360718, Accuracy: 1.0, Computation time: 1.8895790576934814\n",
      "Step: 3016, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.200737714767456\n",
      "Step: 3017, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.6347644329071045\n",
      "Step: 3018, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.3010854721069336\n",
      "Step: 3019, Loss: 0.9374760389328003, Accuracy: 0.96875, Computation time: 1.226966381072998\n",
      "Step: 3020, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.4355483055114746\n",
      "Step: 3021, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.2319951057434082\n",
      "Step: 3022, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.3439197540283203\n",
      "Step: 3023, Loss: 0.9366909861564636, Accuracy: 0.96875, Computation time: 1.553642988204956\n",
      "Step: 3024, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.7233490943908691\n",
      "Step: 3025, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.4800405502319336\n",
      "Step: 3026, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.4225597381591797\n",
      "Step: 3027, Loss: 0.915882408618927, Accuracy: 1.0, Computation time: 1.55983304977417\n",
      "Step: 3028, Loss: 0.93218594789505, Accuracy: 0.96875, Computation time: 1.655841588973999\n",
      "Step: 3029, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 1.377540111541748\n",
      "Step: 3030, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.639153242111206\n",
      "Step: 3031, Loss: 0.9159338474273682, Accuracy: 1.0, Computation time: 1.6866939067840576\n",
      "Step: 3032, Loss: 0.9376528859138489, Accuracy: 0.96875, Computation time: 1.3112883567810059\n",
      "Step: 3033, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 1.478865146636963\n",
      "Step: 3034, Loss: 0.916808009147644, Accuracy: 1.0, Computation time: 1.6799426078796387\n",
      "Step: 3035, Loss: 0.9173792600631714, Accuracy: 1.0, Computation time: 1.6343348026275635\n",
      "Step: 3036, Loss: 0.9175474643707275, Accuracy: 1.0, Computation time: 1.59013032913208\n",
      "Step: 3037, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.2923007011413574\n",
      "Step: 3038, Loss: 0.9377953410148621, Accuracy: 0.96875, Computation time: 1.4287025928497314\n",
      "Step: 3039, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.3054187297821045\n",
      "Step: 3040, Loss: 0.9159126877784729, Accuracy: 1.0, Computation time: 1.9338901042938232\n",
      "Step: 3041, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 1.5653979778289795\n",
      "Step: 3042, Loss: 0.9192684888839722, Accuracy: 1.0, Computation time: 2.0743417739868164\n",
      "Step: 3043, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.416609764099121\n",
      "Step: 3044, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.3252654075622559\n",
      "Step: 3045, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.5317983627319336\n",
      "Step: 3046, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.896212100982666\n",
      "Step: 3047, Loss: 0.9160472750663757, Accuracy: 1.0, Computation time: 1.774212121963501\n",
      "Step: 3048, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.4696879386901855\n",
      "Step: 3049, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.7442047595977783\n",
      "Step: 3050, Loss: 0.916002631187439, Accuracy: 1.0, Computation time: 1.7336502075195312\n",
      "Step: 3051, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 2.2314932346343994\n",
      "Step: 3052, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.5302214622497559\n",
      "Step: 3053, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.7988359928131104\n",
      "Step: 3054, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.8610670566558838\n",
      "Step: 3055, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 2.044041156768799\n",
      "Step: 3056, Loss: 0.9171656370162964, Accuracy: 1.0, Computation time: 2.07999324798584\n",
      "Step: 3057, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.8905682563781738\n",
      "########################\n",
      "Test loss: 1.1309053897857666, Test Accuracy_epoch22: 0.6847926378250122\n",
      "########################\n",
      "Step: 3058, Loss: 0.918917179107666, Accuracy: 1.0, Computation time: 2.2651240825653076\n",
      "Step: 3059, Loss: 0.937863290309906, Accuracy: 0.96875, Computation time: 1.8683717250823975\n",
      "Step: 3060, Loss: 0.9160594940185547, Accuracy: 1.0, Computation time: 1.6680223941802979\n",
      "Step: 3061, Loss: 0.9161524772644043, Accuracy: 1.0, Computation time: 1.5670709609985352\n",
      "Step: 3062, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.650305986404419\n",
      "Step: 3063, Loss: 0.9159670472145081, Accuracy: 1.0, Computation time: 1.7935285568237305\n",
      "Step: 3064, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 1.7915589809417725\n",
      "Step: 3065, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.6425888538360596\n",
      "Step: 3066, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.5886571407318115\n",
      "Step: 3067, Loss: 0.9159601926803589, Accuracy: 1.0, Computation time: 1.6303198337554932\n",
      "Step: 3068, Loss: 0.9159616231918335, Accuracy: 1.0, Computation time: 1.7751209735870361\n",
      "Step: 3069, Loss: 0.9208763837814331, Accuracy: 1.0, Computation time: 2.046457529067993\n",
      "Step: 3070, Loss: 0.9162039160728455, Accuracy: 1.0, Computation time: 1.8436918258666992\n",
      "Step: 3071, Loss: 0.9357941746711731, Accuracy: 0.96875, Computation time: 1.8472709655761719\n",
      "Step: 3072, Loss: 0.9258346557617188, Accuracy: 0.96875, Computation time: 2.6323049068450928\n",
      "Step: 3073, Loss: 0.9160046577453613, Accuracy: 1.0, Computation time: 2.0256547927856445\n",
      "Step: 3074, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 2.139739513397217\n",
      "Step: 3075, Loss: 0.9159330725669861, Accuracy: 1.0, Computation time: 1.3966493606567383\n",
      "Step: 3076, Loss: 0.9159538149833679, Accuracy: 1.0, Computation time: 1.4872581958770752\n",
      "Step: 3077, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 1.9851510524749756\n",
      "Step: 3078, Loss: 0.9159232378005981, Accuracy: 1.0, Computation time: 2.001066207885742\n",
      "Step: 3079, Loss: 0.9159240126609802, Accuracy: 1.0, Computation time: 1.6760778427124023\n",
      "Step: 3080, Loss: 0.9159478545188904, Accuracy: 1.0, Computation time: 1.8815746307373047\n",
      "Step: 3081, Loss: 0.9160311222076416, Accuracy: 1.0, Computation time: 2.240785598754883\n",
      "Step: 3082, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 2.0355472564697266\n",
      "Step: 3083, Loss: 0.9375975131988525, Accuracy: 0.96875, Computation time: 1.7785944938659668\n",
      "Step: 3084, Loss: 0.916121780872345, Accuracy: 1.0, Computation time: 2.0775463581085205\n",
      "Step: 3085, Loss: 0.9159911870956421, Accuracy: 1.0, Computation time: 1.876603603363037\n",
      "Step: 3086, Loss: 0.9163655042648315, Accuracy: 1.0, Computation time: 2.108534574508667\n",
      "Step: 3087, Loss: 0.9159104228019714, Accuracy: 1.0, Computation time: 1.8866283893585205\n",
      "Step: 3088, Loss: 0.9161871075630188, Accuracy: 1.0, Computation time: 2.4753968715667725\n",
      "Step: 3089, Loss: 0.9336869716644287, Accuracy: 0.96875, Computation time: 2.1351540088653564\n",
      "Step: 3090, Loss: 0.9160451292991638, Accuracy: 1.0, Computation time: 1.9726471900939941\n",
      "Step: 3091, Loss: 0.9160723686218262, Accuracy: 1.0, Computation time: 1.9246702194213867\n",
      "Step: 3092, Loss: 0.9161813855171204, Accuracy: 1.0, Computation time: 2.1429147720336914\n",
      "Step: 3093, Loss: 0.916256844997406, Accuracy: 1.0, Computation time: 1.802091121673584\n",
      "Step: 3094, Loss: 0.9227406978607178, Accuracy: 1.0, Computation time: 1.570124864578247\n",
      "Step: 3095, Loss: 0.9159757494926453, Accuracy: 1.0, Computation time: 1.5439493656158447\n",
      "Step: 3096, Loss: 0.9160651564598083, Accuracy: 1.0, Computation time: 1.809229850769043\n",
      "Step: 3097, Loss: 0.9160897135734558, Accuracy: 1.0, Computation time: 1.648909091949463\n",
      "Step: 3098, Loss: 0.9373698234558105, Accuracy: 0.96875, Computation time: 1.5866451263427734\n",
      "Step: 3099, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.395228385925293\n",
      "Step: 3100, Loss: 0.9159562587738037, Accuracy: 1.0, Computation time: 1.5494277477264404\n",
      "Step: 3101, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.4727237224578857\n",
      "Step: 3102, Loss: 0.91657555103302, Accuracy: 1.0, Computation time: 1.9964277744293213\n",
      "Step: 3103, Loss: 0.9270444512367249, Accuracy: 0.96875, Computation time: 2.0322952270507812\n",
      "Step: 3104, Loss: 0.9160235524177551, Accuracy: 1.0, Computation time: 1.5581471920013428\n",
      "Step: 3105, Loss: 0.91618412733078, Accuracy: 1.0, Computation time: 0.9880714416503906\n",
      "Step: 3106, Loss: 0.9165032505989075, Accuracy: 1.0, Computation time: 1.295921802520752\n",
      "Step: 3107, Loss: 0.9166496992111206, Accuracy: 1.0, Computation time: 1.1901464462280273\n",
      "Step: 3108, Loss: 0.9164103865623474, Accuracy: 1.0, Computation time: 1.744126796722412\n",
      "Step: 3109, Loss: 0.9342208504676819, Accuracy: 0.96875, Computation time: 2.068681001663208\n",
      "Step: 3110, Loss: 0.9162577390670776, Accuracy: 1.0, Computation time: 1.600872278213501\n",
      "Step: 3111, Loss: 0.9163681268692017, Accuracy: 1.0, Computation time: 1.4187490940093994\n",
      "Step: 3112, Loss: 0.9161551594734192, Accuracy: 1.0, Computation time: 1.406109094619751\n",
      "Step: 3113, Loss: 0.9162796139717102, Accuracy: 1.0, Computation time: 1.222459316253662\n",
      "Step: 3114, Loss: 0.9163778424263, Accuracy: 1.0, Computation time: 1.3770318031311035\n",
      "Step: 3115, Loss: 0.9161726832389832, Accuracy: 1.0, Computation time: 1.1680481433868408\n",
      "Step: 3116, Loss: 0.9159482717514038, Accuracy: 1.0, Computation time: 1.331902027130127\n",
      "Step: 3117, Loss: 0.9380321502685547, Accuracy: 0.96875, Computation time: 2.0387661457061768\n",
      "Step: 3118, Loss: 0.9160675406455994, Accuracy: 1.0, Computation time: 1.1772096157073975\n",
      "Step: 3119, Loss: 0.9161893129348755, Accuracy: 1.0, Computation time: 1.3000450134277344\n",
      "Step: 3120, Loss: 0.9169501662254333, Accuracy: 1.0, Computation time: 1.9228296279907227\n",
      "Step: 3121, Loss: 0.9163182973861694, Accuracy: 1.0, Computation time: 1.1491014957427979\n",
      "Step: 3122, Loss: 0.9165359735488892, Accuracy: 1.0, Computation time: 1.7249255180358887\n",
      "Step: 3123, Loss: 0.9161417484283447, Accuracy: 1.0, Computation time: 1.3348157405853271\n",
      "Step: 3124, Loss: 0.9162425994873047, Accuracy: 1.0, Computation time: 1.3486180305480957\n",
      "Step: 3125, Loss: 0.9168089628219604, Accuracy: 1.0, Computation time: 1.201301097869873\n",
      "Step: 3126, Loss: 0.9203329682350159, Accuracy: 1.0, Computation time: 1.5069410800933838\n",
      "Step: 3127, Loss: 0.9393162131309509, Accuracy: 0.96875, Computation time: 1.1982336044311523\n",
      "Step: 3128, Loss: 0.9379082322120667, Accuracy: 0.96875, Computation time: 1.4397428035736084\n",
      "Step: 3129, Loss: 0.9161074161529541, Accuracy: 1.0, Computation time: 1.497746229171753\n",
      "Step: 3130, Loss: 0.9162432551383972, Accuracy: 1.0, Computation time: 1.58188796043396\n",
      "Step: 3131, Loss: 0.9161753058433533, Accuracy: 1.0, Computation time: 1.3425910472869873\n",
      "Step: 3132, Loss: 0.9162365198135376, Accuracy: 1.0, Computation time: 1.1786508560180664\n",
      "Step: 3133, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 1.2117290496826172\n",
      "Step: 3134, Loss: 0.9166481494903564, Accuracy: 1.0, Computation time: 1.3889656066894531\n",
      "Step: 3135, Loss: 0.9159263968467712, Accuracy: 1.0, Computation time: 1.2605705261230469\n",
      "Step: 3136, Loss: 0.9204868078231812, Accuracy: 1.0, Computation time: 1.5923490524291992\n",
      "Step: 3137, Loss: 0.9160468578338623, Accuracy: 1.0, Computation time: 1.2231926918029785\n",
      "Step: 3138, Loss: 0.9160237908363342, Accuracy: 1.0, Computation time: 1.6380672454833984\n",
      "Step: 3139, Loss: 0.9169048070907593, Accuracy: 1.0, Computation time: 1.6945035457611084\n",
      "Step: 3140, Loss: 0.9160927534103394, Accuracy: 1.0, Computation time: 1.219698190689087\n",
      "Step: 3141, Loss: 0.9160731434822083, Accuracy: 1.0, Computation time: 1.3376569747924805\n",
      "Step: 3142, Loss: 0.9159836769104004, Accuracy: 1.0, Computation time: 1.271956205368042\n",
      "Step: 3143, Loss: 0.9343708753585815, Accuracy: 0.96875, Computation time: 1.2389576435089111\n",
      "Step: 3144, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.1736268997192383\n",
      "Step: 3145, Loss: 0.9159291386604309, Accuracy: 1.0, Computation time: 1.4657669067382812\n",
      "Step: 3146, Loss: 0.9249278903007507, Accuracy: 1.0, Computation time: 1.243267297744751\n",
      "Step: 3147, Loss: 0.9161180853843689, Accuracy: 1.0, Computation time: 1.2020900249481201\n",
      "Step: 3148, Loss: 0.9161907434463501, Accuracy: 1.0, Computation time: 1.332751989364624\n",
      "Step: 3149, Loss: 0.9160425662994385, Accuracy: 1.0, Computation time: 1.4626266956329346\n",
      "Step: 3150, Loss: 0.9162591695785522, Accuracy: 1.0, Computation time: 1.4772932529449463\n",
      "Step: 3151, Loss: 0.9160169959068298, Accuracy: 1.0, Computation time: 1.6312954425811768\n",
      "Step: 3152, Loss: 0.9161842465400696, Accuracy: 1.0, Computation time: 1.2106907367706299\n",
      "Step: 3153, Loss: 0.9159818291664124, Accuracy: 1.0, Computation time: 1.1797897815704346\n",
      "Step: 3154, Loss: 0.9160025715827942, Accuracy: 1.0, Computation time: 1.6505553722381592\n",
      "Step: 3155, Loss: 0.9373445510864258, Accuracy: 0.96875, Computation time: 1.2178337574005127\n",
      "Step: 3156, Loss: 0.9160342216491699, Accuracy: 1.0, Computation time: 1.1520459651947021\n",
      "Step: 3157, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.2183387279510498\n",
      "Step: 3158, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.2704851627349854\n",
      "Step: 3159, Loss: 0.9167961478233337, Accuracy: 1.0, Computation time: 1.328399658203125\n",
      "Step: 3160, Loss: 0.9167017936706543, Accuracy: 1.0, Computation time: 1.4703636169433594\n",
      "Step: 3161, Loss: 0.9459221959114075, Accuracy: 0.96875, Computation time: 1.8912739753723145\n",
      "Step: 3162, Loss: 0.9166750907897949, Accuracy: 1.0, Computation time: 1.6122398376464844\n",
      "Step: 3163, Loss: 0.938999354839325, Accuracy: 0.96875, Computation time: 1.298703908920288\n",
      "Step: 3164, Loss: 0.9159983396530151, Accuracy: 1.0, Computation time: 1.2196552753448486\n",
      "Step: 3165, Loss: 0.9163675308227539, Accuracy: 1.0, Computation time: 2.165205717086792\n",
      "Step: 3166, Loss: 0.9161116480827332, Accuracy: 1.0, Computation time: 1.3289086818695068\n",
      "Step: 3167, Loss: 0.9162524938583374, Accuracy: 1.0, Computation time: 1.736415147781372\n",
      "Step: 3168, Loss: 0.9160146117210388, Accuracy: 1.0, Computation time: 1.18507981300354\n",
      "Step: 3169, Loss: 0.916050374507904, Accuracy: 1.0, Computation time: 1.214353322982788\n",
      "Step: 3170, Loss: 0.9160093665122986, Accuracy: 1.0, Computation time: 1.3555538654327393\n",
      "Step: 3171, Loss: 0.9472498893737793, Accuracy: 0.96875, Computation time: 2.100041627883911\n",
      "Step: 3172, Loss: 0.9387504458427429, Accuracy: 0.96875, Computation time: 1.3346638679504395\n",
      "Step: 3173, Loss: 0.9380491375923157, Accuracy: 0.96875, Computation time: 1.4247188568115234\n",
      "Step: 3174, Loss: 0.9165015816688538, Accuracy: 1.0, Computation time: 1.3786380290985107\n",
      "Step: 3175, Loss: 0.916398823261261, Accuracy: 1.0, Computation time: 1.4132130146026611\n",
      "Step: 3176, Loss: 0.9239164590835571, Accuracy: 1.0, Computation time: 1.8993525505065918\n",
      "Step: 3177, Loss: 0.9171773195266724, Accuracy: 1.0, Computation time: 2.0945723056793213\n",
      "Step: 3178, Loss: 0.9178455471992493, Accuracy: 1.0, Computation time: 1.4261753559112549\n",
      "Step: 3179, Loss: 0.9166735410690308, Accuracy: 1.0, Computation time: 1.56105637550354\n",
      "Step: 3180, Loss: 0.9381544589996338, Accuracy: 0.96875, Computation time: 2.1027684211730957\n",
      "Step: 3181, Loss: 0.916166365146637, Accuracy: 1.0, Computation time: 1.1809146404266357\n",
      "Step: 3182, Loss: 0.9165062308311462, Accuracy: 1.0, Computation time: 1.4872832298278809\n",
      "Step: 3183, Loss: 0.9160820245742798, Accuracy: 1.0, Computation time: 1.3694219589233398\n",
      "Step: 3184, Loss: 0.916754424571991, Accuracy: 1.0, Computation time: 1.3148376941680908\n",
      "Step: 3185, Loss: 0.9161390066146851, Accuracy: 1.0, Computation time: 1.510077953338623\n",
      "Step: 3186, Loss: 0.9169831275939941, Accuracy: 1.0, Computation time: 2.11446475982666\n",
      "Step: 3187, Loss: 0.9376847147941589, Accuracy: 0.96875, Computation time: 1.2096619606018066\n",
      "Step: 3188, Loss: 0.9160819053649902, Accuracy: 1.0, Computation time: 1.445723295211792\n",
      "Step: 3189, Loss: 0.9187113642692566, Accuracy: 1.0, Computation time: 2.0315182209014893\n",
      "Step: 3190, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 1.5765368938446045\n",
      "Step: 3191, Loss: 0.9160376787185669, Accuracy: 1.0, Computation time: 1.1622352600097656\n",
      "Step: 3192, Loss: 0.9162229299545288, Accuracy: 1.0, Computation time: 1.263103723526001\n",
      "Step: 3193, Loss: 0.9160495400428772, Accuracy: 1.0, Computation time: 1.1022872924804688\n",
      "Step: 3194, Loss: 0.9161681532859802, Accuracy: 1.0, Computation time: 1.3237767219543457\n",
      "Step: 3195, Loss: 0.9171223044395447, Accuracy: 1.0, Computation time: 1.6876921653747559\n",
      "Step: 3196, Loss: 0.9159744381904602, Accuracy: 1.0, Computation time: 1.2759394645690918\n",
      "########################\n",
      "Test loss: 1.1221290826797485, Test Accuracy_epoch23: 0.6995391845703125\n",
      "########################\n",
      "Step: 3197, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 1.5055034160614014\n",
      "Step: 3198, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 1.237105369567871\n",
      "Step: 3199, Loss: 0.9159217476844788, Accuracy: 1.0, Computation time: 1.4716846942901611\n",
      "Step: 3200, Loss: 0.9161424040794373, Accuracy: 1.0, Computation time: 1.314906358718872\n",
      "Step: 3201, Loss: 0.9161450862884521, Accuracy: 1.0, Computation time: 1.1902592182159424\n",
      "Step: 3202, Loss: 0.9160658121109009, Accuracy: 1.0, Computation time: 2.178478956222534\n",
      "Step: 3203, Loss: 0.9159879684448242, Accuracy: 1.0, Computation time: 1.374898910522461\n",
      "Step: 3204, Loss: 0.9402539134025574, Accuracy: 0.96875, Computation time: 1.5137767791748047\n",
      "Step: 3205, Loss: 0.9386801719665527, Accuracy: 0.96875, Computation time: 1.898092269897461\n",
      "Step: 3206, Loss: 0.9160122871398926, Accuracy: 1.0, Computation time: 1.5295932292938232\n",
      "Step: 3207, Loss: 0.9163200855255127, Accuracy: 1.0, Computation time: 1.3787107467651367\n",
      "Step: 3208, Loss: 0.9161099195480347, Accuracy: 1.0, Computation time: 1.4860215187072754\n",
      "Step: 3209, Loss: 0.9395086169242859, Accuracy: 0.96875, Computation time: 1.2625064849853516\n",
      "Step: 3210, Loss: 0.9159933924674988, Accuracy: 1.0, Computation time: 1.268136978149414\n",
      "Step: 3211, Loss: 0.9171448349952698, Accuracy: 1.0, Computation time: 1.3266918659210205\n",
      "Step: 3212, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.5116302967071533\n",
      "Step: 3213, Loss: 0.9159868955612183, Accuracy: 1.0, Computation time: 1.7340378761291504\n",
      "Step: 3214, Loss: 0.9162038564682007, Accuracy: 1.0, Computation time: 1.3848097324371338\n",
      "Step: 3215, Loss: 0.9161251187324524, Accuracy: 1.0, Computation time: 1.1932072639465332\n",
      "Step: 3216, Loss: 0.9160524606704712, Accuracy: 1.0, Computation time: 1.4110596179962158\n",
      "Step: 3217, Loss: 0.9160696268081665, Accuracy: 1.0, Computation time: 1.4189929962158203\n",
      "Step: 3218, Loss: 0.915958046913147, Accuracy: 1.0, Computation time: 1.4716682434082031\n",
      "Step: 3219, Loss: 0.9159058332443237, Accuracy: 1.0, Computation time: 1.471369743347168\n",
      "Step: 3220, Loss: 0.9159302115440369, Accuracy: 1.0, Computation time: 1.2229540348052979\n",
      "Step: 3221, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 1.434023380279541\n",
      "Step: 3222, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.277538776397705\n",
      "Step: 3223, Loss: 0.9376494288444519, Accuracy: 0.96875, Computation time: 1.2741549015045166\n",
      "Step: 3224, Loss: 0.9160025119781494, Accuracy: 1.0, Computation time: 1.4198622703552246\n",
      "Step: 3225, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.5448412895202637\n",
      "Step: 3226, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.4944934844970703\n",
      "Step: 3227, Loss: 0.937741219997406, Accuracy: 0.96875, Computation time: 1.4134397506713867\n",
      "Step: 3228, Loss: 0.9373484253883362, Accuracy: 0.96875, Computation time: 1.2613945007324219\n",
      "Step: 3229, Loss: 0.9160062670707703, Accuracy: 1.0, Computation time: 1.672072410583496\n",
      "Step: 3230, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.3582820892333984\n",
      "Step: 3231, Loss: 0.9160128235816956, Accuracy: 1.0, Computation time: 1.4361011981964111\n",
      "Step: 3232, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 1.5554964542388916\n",
      "Step: 3233, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.5564467906951904\n",
      "Step: 3234, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.412421703338623\n",
      "Step: 3235, Loss: 0.9159427285194397, Accuracy: 1.0, Computation time: 1.4093620777130127\n",
      "Step: 3236, Loss: 0.9162124991416931, Accuracy: 1.0, Computation time: 1.8946020603179932\n",
      "Step: 3237, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.5248947143554688\n",
      "Step: 3238, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.400247573852539\n",
      "Step: 3239, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.5570547580718994\n",
      "Step: 3240, Loss: 0.9162203073501587, Accuracy: 1.0, Computation time: 1.474273443222046\n",
      "Step: 3241, Loss: 0.9369843006134033, Accuracy: 0.96875, Computation time: 1.4816644191741943\n",
      "Step: 3242, Loss: 0.9172877073287964, Accuracy: 1.0, Computation time: 2.5308494567871094\n",
      "Step: 3243, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.9661118984222412\n",
      "Step: 3244, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.360684871673584\n",
      "Step: 3245, Loss: 0.9162580370903015, Accuracy: 1.0, Computation time: 1.7003297805786133\n",
      "Step: 3246, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.717531442642212\n",
      "Step: 3247, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.6243038177490234\n",
      "Step: 3248, Loss: 0.9165181517601013, Accuracy: 1.0, Computation time: 1.0810301303863525\n",
      "Step: 3249, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.6093838214874268\n",
      "Step: 3250, Loss: 0.9163955450057983, Accuracy: 1.0, Computation time: 1.608537197113037\n",
      "Step: 3251, Loss: 0.915961503982544, Accuracy: 1.0, Computation time: 1.4096038341522217\n",
      "Step: 3252, Loss: 0.9159624576568604, Accuracy: 1.0, Computation time: 1.434732437133789\n",
      "Step: 3253, Loss: 0.9350237846374512, Accuracy: 0.96875, Computation time: 3.0824763774871826\n",
      "Step: 3254, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 1.2427418231964111\n",
      "Step: 3255, Loss: 0.9372613430023193, Accuracy: 0.96875, Computation time: 1.4131355285644531\n",
      "Step: 3256, Loss: 0.9317036867141724, Accuracy: 0.96875, Computation time: 1.715881586074829\n",
      "Step: 3257, Loss: 0.9160867929458618, Accuracy: 1.0, Computation time: 1.625575304031372\n",
      "Step: 3258, Loss: 0.9166355133056641, Accuracy: 1.0, Computation time: 1.4464735984802246\n",
      "Step: 3259, Loss: 0.9164091348648071, Accuracy: 1.0, Computation time: 1.7319145202636719\n",
      "Step: 3260, Loss: 0.9169961810112, Accuracy: 1.0, Computation time: 1.6228601932525635\n",
      "Step: 3261, Loss: 0.9162765145301819, Accuracy: 1.0, Computation time: 1.7599945068359375\n",
      "Step: 3262, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.9492177963256836\n",
      "Step: 3263, Loss: 0.9159743189811707, Accuracy: 1.0, Computation time: 1.8845326900482178\n",
      "Step: 3264, Loss: 0.9164790511131287, Accuracy: 1.0, Computation time: 2.1964285373687744\n",
      "Step: 3265, Loss: 0.9375548958778381, Accuracy: 0.96875, Computation time: 1.59527587890625\n",
      "Step: 3266, Loss: 0.9398325681686401, Accuracy: 0.96875, Computation time: 2.203777313232422\n",
      "Step: 3267, Loss: 0.9167328476905823, Accuracy: 1.0, Computation time: 2.3887734413146973\n",
      "Step: 3268, Loss: 0.916340708732605, Accuracy: 1.0, Computation time: 1.7484102249145508\n",
      "Step: 3269, Loss: 0.9165221452713013, Accuracy: 1.0, Computation time: 2.103635787963867\n",
      "Step: 3270, Loss: 0.9169058203697205, Accuracy: 1.0, Computation time: 2.304628849029541\n",
      "Step: 3271, Loss: 0.9337818026542664, Accuracy: 0.96875, Computation time: 2.2898380756378174\n",
      "Step: 3272, Loss: 0.9160916209220886, Accuracy: 1.0, Computation time: 2.5015130043029785\n",
      "Step: 3273, Loss: 0.9194251298904419, Accuracy: 1.0, Computation time: 1.8773188591003418\n",
      "Step: 3274, Loss: 0.9170576333999634, Accuracy: 1.0, Computation time: 2.034609794616699\n",
      "Step: 3275, Loss: 0.9161426424980164, Accuracy: 1.0, Computation time: 2.1476991176605225\n",
      "Step: 3276, Loss: 0.9161285758018494, Accuracy: 1.0, Computation time: 1.6615564823150635\n",
      "Step: 3277, Loss: 0.9160329103469849, Accuracy: 1.0, Computation time: 1.449582576751709\n",
      "Step: 3278, Loss: 0.91621333360672, Accuracy: 1.0, Computation time: 1.7353320121765137\n",
      "Step: 3279, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 1.6853749752044678\n",
      "Step: 3280, Loss: 0.9376168847084045, Accuracy: 0.96875, Computation time: 1.678062915802002\n",
      "Step: 3281, Loss: 0.9160212278366089, Accuracy: 1.0, Computation time: 1.9899122714996338\n",
      "Step: 3282, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 1.7273120880126953\n",
      "Step: 3283, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 1.309640884399414\n",
      "Step: 3284, Loss: 0.9159626960754395, Accuracy: 1.0, Computation time: 1.448617935180664\n",
      "Step: 3285, Loss: 0.9379388689994812, Accuracy: 0.96875, Computation time: 1.7918691635131836\n",
      "Step: 3286, Loss: 0.9159637689590454, Accuracy: 1.0, Computation time: 1.5157463550567627\n",
      "Step: 3287, Loss: 0.9159674644470215, Accuracy: 1.0, Computation time: 1.4045000076293945\n",
      "Step: 3288, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.4953804016113281\n",
      "Step: 3289, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.8236863613128662\n",
      "Step: 3290, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 1.4727962017059326\n",
      "Step: 3291, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.8007757663726807\n",
      "Step: 3292, Loss: 0.9160189032554626, Accuracy: 1.0, Computation time: 1.5436198711395264\n",
      "Step: 3293, Loss: 0.9160589575767517, Accuracy: 1.0, Computation time: 2.1178793907165527\n",
      "Step: 3294, Loss: 0.9341263175010681, Accuracy: 0.96875, Computation time: 2.0210275650024414\n",
      "Step: 3295, Loss: 0.9375795125961304, Accuracy: 0.96875, Computation time: 1.4664864540100098\n",
      "Step: 3296, Loss: 0.9176297783851624, Accuracy: 1.0, Computation time: 2.1998226642608643\n",
      "Step: 3297, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 2.1320602893829346\n",
      "Step: 3298, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 1.3784027099609375\n",
      "Step: 3299, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.743208408355713\n",
      "Step: 3300, Loss: 0.91595458984375, Accuracy: 1.0, Computation time: 1.185396671295166\n",
      "Step: 3301, Loss: 0.9159376621246338, Accuracy: 1.0, Computation time: 1.6383681297302246\n",
      "Step: 3302, Loss: 0.9372685551643372, Accuracy: 0.96875, Computation time: 1.2373683452606201\n",
      "Step: 3303, Loss: 0.9160262942314148, Accuracy: 1.0, Computation time: 1.4385020732879639\n",
      "Step: 3304, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.216942310333252\n",
      "Step: 3305, Loss: 0.9159056544303894, Accuracy: 1.0, Computation time: 1.5984716415405273\n",
      "Step: 3306, Loss: 0.9162303805351257, Accuracy: 1.0, Computation time: 1.4021971225738525\n",
      "Step: 3307, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.5081119537353516\n",
      "Step: 3308, Loss: 0.9160330295562744, Accuracy: 1.0, Computation time: 1.6991214752197266\n",
      "Step: 3309, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.5764553546905518\n",
      "Step: 3310, Loss: 0.9379159212112427, Accuracy: 0.96875, Computation time: 1.465728759765625\n",
      "Step: 3311, Loss: 0.9362328052520752, Accuracy: 0.96875, Computation time: 1.7435071468353271\n",
      "Step: 3312, Loss: 0.915951132774353, Accuracy: 1.0, Computation time: 1.8649401664733887\n",
      "Step: 3313, Loss: 0.9159446954727173, Accuracy: 1.0, Computation time: 1.678431749343872\n",
      "Step: 3314, Loss: 0.9160085320472717, Accuracy: 1.0, Computation time: 1.6445670127868652\n",
      "Step: 3315, Loss: 0.916557788848877, Accuracy: 1.0, Computation time: 1.5097806453704834\n",
      "Step: 3316, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.5016093254089355\n",
      "Step: 3317, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.1936180591583252\n",
      "Step: 3318, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.4816110134124756\n",
      "Step: 3319, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 1.423405408859253\n",
      "Step: 3320, Loss: 0.9160981178283691, Accuracy: 1.0, Computation time: 1.4654817581176758\n",
      "Step: 3321, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.3880813121795654\n",
      "Step: 3322, Loss: 0.9379454851150513, Accuracy: 0.96875, Computation time: 1.772883653640747\n",
      "Step: 3323, Loss: 0.9377411007881165, Accuracy: 0.96875, Computation time: 1.648266315460205\n",
      "Step: 3324, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 1.2477304935455322\n",
      "Step: 3325, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.3968217372894287\n",
      "Step: 3326, Loss: 0.9161608219146729, Accuracy: 1.0, Computation time: 1.6837389469146729\n",
      "Step: 3327, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.2989885807037354\n",
      "Step: 3328, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.4510540962219238\n",
      "Step: 3329, Loss: 0.9367120862007141, Accuracy: 0.96875, Computation time: 1.702848196029663\n",
      "Step: 3330, Loss: 0.9376013278961182, Accuracy: 0.96875, Computation time: 1.367525339126587\n",
      "Step: 3331, Loss: 0.9160428643226624, Accuracy: 1.0, Computation time: 1.2654883861541748\n",
      "Step: 3332, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.1803076267242432\n",
      "Step: 3333, Loss: 0.9161189198493958, Accuracy: 1.0, Computation time: 1.6458985805511475\n",
      "Step: 3334, Loss: 0.9374646544456482, Accuracy: 0.96875, Computation time: 1.5379812717437744\n",
      "Step: 3335, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.1923034191131592\n",
      "########################\n",
      "Test loss: 1.1223171949386597, Test Accuracy_epoch24: 0.7032257914543152\n",
      "########################\n",
      "Step: 3336, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.5045204162597656\n",
      "Step: 3337, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.2211711406707764\n",
      "Step: 3338, Loss: 0.9159961342811584, Accuracy: 1.0, Computation time: 1.3408598899841309\n",
      "Step: 3339, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.423407793045044\n",
      "Step: 3340, Loss: 0.9166752696037292, Accuracy: 1.0, Computation time: 1.6982572078704834\n",
      "Step: 3341, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.3340106010437012\n",
      "Step: 3342, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 1.168226957321167\n",
      "Step: 3343, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.5291070938110352\n",
      "Step: 3344, Loss: 0.9169921278953552, Accuracy: 1.0, Computation time: 1.7553930282592773\n",
      "Step: 3345, Loss: 0.9165872931480408, Accuracy: 1.0, Computation time: 2.178852081298828\n",
      "Step: 3346, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 0.9663329124450684\n",
      "Step: 3347, Loss: 0.9159346222877502, Accuracy: 1.0, Computation time: 1.258833408355713\n",
      "Step: 3348, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.4445226192474365\n",
      "Step: 3349, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.4386019706726074\n",
      "Step: 3350, Loss: 0.9169837832450867, Accuracy: 1.0, Computation time: 1.3094520568847656\n",
      "Step: 3351, Loss: 0.9160906076431274, Accuracy: 1.0, Computation time: 1.7938852310180664\n",
      "Step: 3352, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.489025592803955\n",
      "Step: 3353, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.6546764373779297\n",
      "Step: 3354, Loss: 0.9159172773361206, Accuracy: 1.0, Computation time: 1.7657876014709473\n",
      "Step: 3355, Loss: 0.9165910482406616, Accuracy: 1.0, Computation time: 1.5247948169708252\n",
      "Step: 3356, Loss: 0.9363661408424377, Accuracy: 0.96875, Computation time: 1.2868037223815918\n",
      "Step: 3357, Loss: 0.9362547397613525, Accuracy: 0.96875, Computation time: 1.2848730087280273\n",
      "Step: 3358, Loss: 0.9159090518951416, Accuracy: 1.0, Computation time: 1.3537850379943848\n",
      "Step: 3359, Loss: 0.9375316500663757, Accuracy: 0.96875, Computation time: 2.161069869995117\n",
      "Step: 3360, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 1.4280116558074951\n",
      "Step: 3361, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 1.6190831661224365\n",
      "Step: 3362, Loss: 0.9180527925491333, Accuracy: 1.0, Computation time: 2.135977268218994\n",
      "Step: 3363, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.6082026958465576\n",
      "Step: 3364, Loss: 0.9160592555999756, Accuracy: 1.0, Computation time: 1.6154193878173828\n",
      "Step: 3365, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.604954719543457\n",
      "Step: 3366, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 1.2933976650238037\n",
      "Step: 3367, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 1.496164083480835\n",
      "Step: 3368, Loss: 0.9158875346183777, Accuracy: 1.0, Computation time: 1.5314910411834717\n",
      "Step: 3369, Loss: 0.9214906096458435, Accuracy: 1.0, Computation time: 1.921217679977417\n",
      "Step: 3370, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.4224588871002197\n",
      "Step: 3371, Loss: 0.9160266518592834, Accuracy: 1.0, Computation time: 1.7193176746368408\n",
      "Step: 3372, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 1.5581471920013428\n",
      "Step: 3373, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.432831048965454\n",
      "Step: 3374, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.6526062488555908\n",
      "Step: 3375, Loss: 0.9218989014625549, Accuracy: 1.0, Computation time: 1.771265983581543\n",
      "Step: 3376, Loss: 0.9159684777259827, Accuracy: 1.0, Computation time: 1.5459294319152832\n",
      "Step: 3377, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.656252145767212\n",
      "Step: 3378, Loss: 0.9376566410064697, Accuracy: 0.96875, Computation time: 1.45766282081604\n",
      "Step: 3379, Loss: 0.9405139088630676, Accuracy: 0.96875, Computation time: 2.0254669189453125\n",
      "Step: 3380, Loss: 0.916129469871521, Accuracy: 1.0, Computation time: 1.5842399597167969\n",
      "Step: 3381, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.7132844924926758\n",
      "Step: 3382, Loss: 0.9160160422325134, Accuracy: 1.0, Computation time: 2.2158730030059814\n",
      "Step: 3383, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 1.3214828968048096\n",
      "Step: 3384, Loss: 0.916790246963501, Accuracy: 1.0, Computation time: 1.7001333236694336\n",
      "Step: 3385, Loss: 0.937812328338623, Accuracy: 0.96875, Computation time: 1.4194221496582031\n",
      "Step: 3386, Loss: 0.9160398244857788, Accuracy: 1.0, Computation time: 1.6272742748260498\n",
      "Step: 3387, Loss: 0.9159521460533142, Accuracy: 1.0, Computation time: 1.4885592460632324\n",
      "Step: 3388, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 1.1577167510986328\n",
      "Step: 3389, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.4525072574615479\n",
      "Step: 3390, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.4483976364135742\n",
      "Step: 3391, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 1.610626459121704\n",
      "Step: 3392, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.5901696681976318\n",
      "Step: 3393, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 1.6050314903259277\n",
      "Step: 3394, Loss: 0.9160848259925842, Accuracy: 1.0, Computation time: 1.6529738903045654\n",
      "Step: 3395, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.5872361660003662\n",
      "Step: 3396, Loss: 0.9248268604278564, Accuracy: 1.0, Computation time: 2.033815383911133\n",
      "Step: 3397, Loss: 0.9159213304519653, Accuracy: 1.0, Computation time: 1.4253628253936768\n",
      "Step: 3398, Loss: 0.9160231947898865, Accuracy: 1.0, Computation time: 1.60410737991333\n",
      "Step: 3399, Loss: 0.938075840473175, Accuracy: 0.96875, Computation time: 2.0444495677948\n",
      "Step: 3400, Loss: 0.916071355342865, Accuracy: 1.0, Computation time: 1.2046198844909668\n",
      "Step: 3401, Loss: 0.9365351796150208, Accuracy: 0.96875, Computation time: 1.5067405700683594\n",
      "Step: 3402, Loss: 0.9160068035125732, Accuracy: 1.0, Computation time: 1.2122845649719238\n",
      "Step: 3403, Loss: 0.9173465967178345, Accuracy: 1.0, Computation time: 1.5294692516326904\n",
      "Step: 3404, Loss: 0.9159548878669739, Accuracy: 1.0, Computation time: 1.3644590377807617\n",
      "Step: 3405, Loss: 0.915996253490448, Accuracy: 1.0, Computation time: 1.1701903343200684\n",
      "Step: 3406, Loss: 0.9173054695129395, Accuracy: 1.0, Computation time: 1.3810653686523438\n",
      "Step: 3407, Loss: 0.9380431175231934, Accuracy: 0.96875, Computation time: 1.4273450374603271\n",
      "Step: 3408, Loss: 0.9367345571517944, Accuracy: 0.96875, Computation time: 2.0674479007720947\n",
      "Step: 3409, Loss: 0.9190051555633545, Accuracy: 1.0, Computation time: 1.0962071418762207\n",
      "Step: 3410, Loss: 0.9414650201797485, Accuracy: 0.96875, Computation time: 1.9255352020263672\n",
      "Step: 3411, Loss: 0.9163417816162109, Accuracy: 1.0, Computation time: 1.2614388465881348\n",
      "Step: 3412, Loss: 0.9381198287010193, Accuracy: 0.96875, Computation time: 0.9734501838684082\n",
      "Step: 3413, Loss: 0.9165664911270142, Accuracy: 1.0, Computation time: 1.0357322692871094\n",
      "Step: 3414, Loss: 0.9164202213287354, Accuracy: 1.0, Computation time: 1.2342443466186523\n",
      "Step: 3415, Loss: 0.9163113236427307, Accuracy: 1.0, Computation time: 1.1836531162261963\n",
      "Step: 3416, Loss: 0.9377834796905518, Accuracy: 0.96875, Computation time: 1.3303086757659912\n",
      "Step: 3417, Loss: 0.9163707494735718, Accuracy: 1.0, Computation time: 1.8909971714019775\n",
      "Step: 3418, Loss: 0.9169985055923462, Accuracy: 1.0, Computation time: 1.4232890605926514\n",
      "Step: 3419, Loss: 0.9161955118179321, Accuracy: 1.0, Computation time: 1.084754467010498\n",
      "Step: 3420, Loss: 0.9165270328521729, Accuracy: 1.0, Computation time: 1.501601219177246\n",
      "Step: 3421, Loss: 0.9160817265510559, Accuracy: 1.0, Computation time: 1.1686201095581055\n",
      "Step: 3422, Loss: 0.9373437166213989, Accuracy: 0.96875, Computation time: 1.252896785736084\n",
      "Step: 3423, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 0.9604909420013428\n",
      "Step: 3424, Loss: 0.9162225723266602, Accuracy: 1.0, Computation time: 0.9681165218353271\n",
      "Step: 3425, Loss: 0.9160723090171814, Accuracy: 1.0, Computation time: 1.0861473083496094\n",
      "Step: 3426, Loss: 0.915989339351654, Accuracy: 1.0, Computation time: 1.1685965061187744\n",
      "Step: 3427, Loss: 0.9162010550498962, Accuracy: 1.0, Computation time: 1.4205574989318848\n",
      "Step: 3428, Loss: 0.9159749746322632, Accuracy: 1.0, Computation time: 0.9296753406524658\n",
      "Step: 3429, Loss: 0.9160231351852417, Accuracy: 1.0, Computation time: 0.9460816383361816\n",
      "Step: 3430, Loss: 0.9160323739051819, Accuracy: 1.0, Computation time: 1.1144559383392334\n",
      "Step: 3431, Loss: 0.9171719551086426, Accuracy: 1.0, Computation time: 0.8860292434692383\n",
      "Step: 3432, Loss: 0.9161655902862549, Accuracy: 1.0, Computation time: 1.2550601959228516\n",
      "Step: 3433, Loss: 0.9159700274467468, Accuracy: 1.0, Computation time: 1.2003753185272217\n",
      "Step: 3434, Loss: 0.9382678270339966, Accuracy: 0.96875, Computation time: 1.8333113193511963\n",
      "Step: 3435, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.3894989490509033\n",
      "Step: 3436, Loss: 0.9159358143806458, Accuracy: 1.0, Computation time: 1.1348838806152344\n",
      "Step: 3437, Loss: 0.918400764465332, Accuracy: 1.0, Computation time: 1.3956272602081299\n",
      "Step: 3438, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.0383093357086182\n",
      "Step: 3439, Loss: 0.9160078763961792, Accuracy: 1.0, Computation time: 1.0098817348480225\n",
      "Step: 3440, Loss: 0.9329485893249512, Accuracy: 0.96875, Computation time: 0.9714946746826172\n",
      "Step: 3441, Loss: 0.9159736037254333, Accuracy: 1.0, Computation time: 1.081012487411499\n",
      "Step: 3442, Loss: 0.9159644842147827, Accuracy: 1.0, Computation time: 0.9929001331329346\n",
      "Step: 3443, Loss: 0.9159897565841675, Accuracy: 1.0, Computation time: 1.2480731010437012\n",
      "Step: 3444, Loss: 0.9165021181106567, Accuracy: 1.0, Computation time: 1.092566967010498\n",
      "Step: 3445, Loss: 0.9159893989562988, Accuracy: 1.0, Computation time: 1.1444003582000732\n",
      "Step: 3446, Loss: 0.9159912467002869, Accuracy: 1.0, Computation time: 1.0908019542694092\n",
      "Step: 3447, Loss: 0.9159822463989258, Accuracy: 1.0, Computation time: 0.994265079498291\n",
      "Step: 3448, Loss: 0.9159889221191406, Accuracy: 1.0, Computation time: 1.03910231590271\n",
      "Step: 3449, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 0.8806941509246826\n",
      "Step: 3450, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.4258003234863281\n",
      "Step: 3451, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.3339083194732666\n",
      "Step: 3452, Loss: 0.9159247279167175, Accuracy: 1.0, Computation time: 1.3151381015777588\n",
      "Step: 3453, Loss: 0.9162103533744812, Accuracy: 1.0, Computation time: 1.0325253009796143\n",
      "Step: 3454, Loss: 0.9159983396530151, Accuracy: 1.0, Computation time: 1.3773207664489746\n",
      "Step: 3455, Loss: 0.9159078001976013, Accuracy: 1.0, Computation time: 0.9605362415313721\n",
      "Step: 3456, Loss: 0.9375424385070801, Accuracy: 0.96875, Computation time: 1.231334924697876\n",
      "Step: 3457, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 1.1916241645812988\n",
      "Step: 3458, Loss: 0.9159113764762878, Accuracy: 1.0, Computation time: 1.002180814743042\n",
      "Step: 3459, Loss: 0.9376099109649658, Accuracy: 0.96875, Computation time: 0.9844369888305664\n",
      "Step: 3460, Loss: 0.9190626740455627, Accuracy: 1.0, Computation time: 1.1445281505584717\n",
      "Step: 3461, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.1734530925750732\n",
      "Step: 3462, Loss: 0.937646210193634, Accuracy: 0.96875, Computation time: 1.0369503498077393\n",
      "Step: 3463, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 1.11842679977417\n",
      "Step: 3464, Loss: 0.928380012512207, Accuracy: 0.96875, Computation time: 1.1849870681762695\n",
      "Step: 3465, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 1.1127829551696777\n",
      "Step: 3466, Loss: 0.9348927140235901, Accuracy: 0.96875, Computation time: 1.3255946636199951\n",
      "Step: 3467, Loss: 0.9168304800987244, Accuracy: 1.0, Computation time: 1.197462797164917\n",
      "Step: 3468, Loss: 0.9162057042121887, Accuracy: 1.0, Computation time: 1.0750679969787598\n",
      "Step: 3469, Loss: 0.9160189628601074, Accuracy: 1.0, Computation time: 1.2241439819335938\n",
      "Step: 3470, Loss: 0.9160981774330139, Accuracy: 1.0, Computation time: 1.0322680473327637\n",
      "Step: 3471, Loss: 0.9183148741722107, Accuracy: 1.0, Computation time: 1.583827018737793\n",
      "Step: 3472, Loss: 0.9165404438972473, Accuracy: 1.0, Computation time: 1.2297918796539307\n",
      "Step: 3473, Loss: 0.9160156846046448, Accuracy: 1.0, Computation time: 1.285761833190918\n",
      "Step: 3474, Loss: 0.9375291466712952, Accuracy: 0.96875, Computation time: 0.8281846046447754\n",
      "########################\n",
      "Test loss: 1.1173255443572998, Test Accuracy_epoch25: 0.7115207314491272\n",
      "########################\n",
      "Step: 3475, Loss: 0.9160597920417786, Accuracy: 1.0, Computation time: 1.1010661125183105\n",
      "Step: 3476, Loss: 0.9161797761917114, Accuracy: 1.0, Computation time: 2.05047345161438\n",
      "Step: 3477, Loss: 0.9349210262298584, Accuracy: 0.96875, Computation time: 1.321826457977295\n",
      "Step: 3478, Loss: 0.916054904460907, Accuracy: 1.0, Computation time: 1.025191068649292\n",
      "Step: 3479, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.0347187519073486\n",
      "Step: 3480, Loss: 0.916008472442627, Accuracy: 1.0, Computation time: 1.0459990501403809\n",
      "Step: 3481, Loss: 0.916409432888031, Accuracy: 1.0, Computation time: 0.8528451919555664\n",
      "Step: 3482, Loss: 0.9160751700401306, Accuracy: 1.0, Computation time: 1.0633878707885742\n",
      "Step: 3483, Loss: 0.9377016425132751, Accuracy: 0.96875, Computation time: 1.0587451457977295\n",
      "Step: 3484, Loss: 0.9160465002059937, Accuracy: 1.0, Computation time: 0.9642131328582764\n",
      "Step: 3485, Loss: 0.9160933494567871, Accuracy: 1.0, Computation time: 1.1421990394592285\n",
      "Step: 3486, Loss: 0.9161261916160583, Accuracy: 1.0, Computation time: 1.0799477100372314\n",
      "Step: 3487, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 1.1665360927581787\n",
      "Step: 3488, Loss: 0.9160431623458862, Accuracy: 1.0, Computation time: 1.1863243579864502\n",
      "Step: 3489, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 0.930368185043335\n",
      "Step: 3490, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.0430693626403809\n",
      "Step: 3491, Loss: 0.9159889221191406, Accuracy: 1.0, Computation time: 1.101043462753296\n",
      "Step: 3492, Loss: 0.9186369180679321, Accuracy: 1.0, Computation time: 1.3639562129974365\n",
      "Step: 3493, Loss: 0.9160950779914856, Accuracy: 1.0, Computation time: 1.3663902282714844\n",
      "Step: 3494, Loss: 0.9341530203819275, Accuracy: 0.96875, Computation time: 1.4275150299072266\n",
      "Step: 3495, Loss: 0.9160100817680359, Accuracy: 1.0, Computation time: 1.3272984027862549\n",
      "Step: 3496, Loss: 0.9160057902336121, Accuracy: 1.0, Computation time: 1.0837337970733643\n",
      "Step: 3497, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.1086843013763428\n",
      "Step: 3498, Loss: 0.9173259735107422, Accuracy: 1.0, Computation time: 0.9789068698883057\n",
      "Step: 3499, Loss: 0.915981113910675, Accuracy: 1.0, Computation time: 0.9180104732513428\n",
      "Step: 3500, Loss: 0.9159477949142456, Accuracy: 1.0, Computation time: 0.7889246940612793\n",
      "Step: 3501, Loss: 0.9162303805351257, Accuracy: 1.0, Computation time: 1.003441333770752\n",
      "Step: 3502, Loss: 0.9164056777954102, Accuracy: 1.0, Computation time: 0.978621244430542\n",
      "Step: 3503, Loss: 0.9167051315307617, Accuracy: 1.0, Computation time: 0.9784636497497559\n",
      "Step: 3504, Loss: 0.9160060286521912, Accuracy: 1.0, Computation time: 1.1959447860717773\n",
      "Step: 3505, Loss: 0.915948212146759, Accuracy: 1.0, Computation time: 1.0152239799499512\n",
      "Step: 3506, Loss: 0.9163352847099304, Accuracy: 1.0, Computation time: 1.0078163146972656\n",
      "Step: 3507, Loss: 0.9204755425453186, Accuracy: 1.0, Computation time: 1.0300946235656738\n",
      "Step: 3508, Loss: 0.9160767197608948, Accuracy: 1.0, Computation time: 1.160224199295044\n",
      "Step: 3509, Loss: 0.9162428975105286, Accuracy: 1.0, Computation time: 1.3658933639526367\n",
      "Step: 3510, Loss: 0.9161924719810486, Accuracy: 1.0, Computation time: 1.1299359798431396\n",
      "Step: 3511, Loss: 0.9161331057548523, Accuracy: 1.0, Computation time: 1.6555109024047852\n",
      "Step: 3512, Loss: 0.9159934520721436, Accuracy: 1.0, Computation time: 1.196237564086914\n",
      "Step: 3513, Loss: 0.9160652756690979, Accuracy: 1.0, Computation time: 1.0397732257843018\n",
      "Step: 3514, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.0240130424499512\n",
      "Step: 3515, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 0.9295947551727295\n",
      "Step: 3516, Loss: 0.9159463047981262, Accuracy: 1.0, Computation time: 0.8303649425506592\n",
      "Step: 3517, Loss: 0.9202865362167358, Accuracy: 1.0, Computation time: 1.210597038269043\n",
      "Step: 3518, Loss: 0.9160064458847046, Accuracy: 1.0, Computation time: 1.1546626091003418\n",
      "Step: 3519, Loss: 0.9378587603569031, Accuracy: 0.96875, Computation time: 1.7012560367584229\n",
      "Step: 3520, Loss: 0.9570262432098389, Accuracy: 0.9375, Computation time: 1.0625030994415283\n",
      "Step: 3521, Loss: 0.9166662693023682, Accuracy: 1.0, Computation time: 1.3435163497924805\n",
      "Step: 3522, Loss: 0.915946364402771, Accuracy: 1.0, Computation time: 1.187635898590088\n",
      "Step: 3523, Loss: 0.9160290956497192, Accuracy: 1.0, Computation time: 1.4856703281402588\n",
      "Step: 3524, Loss: 0.9378261566162109, Accuracy: 0.96875, Computation time: 1.2176766395568848\n",
      "Step: 3525, Loss: 0.9253883361816406, Accuracy: 1.0, Computation time: 1.3704354763031006\n",
      "Step: 3526, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 1.6710331439971924\n",
      "Step: 3527, Loss: 0.9160985946655273, Accuracy: 1.0, Computation time: 1.4771840572357178\n",
      "Step: 3528, Loss: 0.916111409664154, Accuracy: 1.0, Computation time: 0.8534250259399414\n",
      "Step: 3529, Loss: 0.9160364270210266, Accuracy: 1.0, Computation time: 0.8757355213165283\n",
      "Step: 3530, Loss: 0.9161936640739441, Accuracy: 1.0, Computation time: 1.1809508800506592\n",
      "Step: 3531, Loss: 0.9367983341217041, Accuracy: 0.96875, Computation time: 1.1632051467895508\n",
      "Step: 3532, Loss: 0.9384382963180542, Accuracy: 0.96875, Computation time: 1.2971453666687012\n",
      "Step: 3533, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 1.1231961250305176\n",
      "Step: 3534, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 0.90618896484375\n",
      "Step: 3535, Loss: 0.9160072803497314, Accuracy: 1.0, Computation time: 0.9678335189819336\n",
      "Step: 3536, Loss: 0.9159913659095764, Accuracy: 1.0, Computation time: 1.10426926612854\n",
      "Step: 3537, Loss: 0.9159544110298157, Accuracy: 1.0, Computation time: 0.9319884777069092\n",
      "Step: 3538, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 0.9533090591430664\n",
      "Step: 3539, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.02252197265625\n",
      "Step: 3540, Loss: 0.916365921497345, Accuracy: 1.0, Computation time: 1.098083257675171\n",
      "Step: 3541, Loss: 0.9160019755363464, Accuracy: 1.0, Computation time: 0.9514455795288086\n",
      "Step: 3542, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.2084167003631592\n",
      "Step: 3543, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 0.8698360919952393\n",
      "Step: 3544, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.0094327926635742\n",
      "Step: 3545, Loss: 0.9335377216339111, Accuracy: 0.96875, Computation time: 1.0736253261566162\n",
      "Step: 3546, Loss: 0.9241309762001038, Accuracy: 1.0, Computation time: 1.3571324348449707\n",
      "Step: 3547, Loss: 0.9159467220306396, Accuracy: 1.0, Computation time: 1.2943522930145264\n",
      "Step: 3548, Loss: 0.9376766681671143, Accuracy: 0.96875, Computation time: 0.9908983707427979\n",
      "Step: 3549, Loss: 0.9161426424980164, Accuracy: 1.0, Computation time: 0.9936532974243164\n",
      "Step: 3550, Loss: 0.9161128997802734, Accuracy: 1.0, Computation time: 0.95499587059021\n",
      "Step: 3551, Loss: 0.9196552634239197, Accuracy: 1.0, Computation time: 1.1981720924377441\n",
      "Step: 3552, Loss: 0.9161503314971924, Accuracy: 1.0, Computation time: 1.0326628684997559\n",
      "Step: 3553, Loss: 0.9591278433799744, Accuracy: 0.9375, Computation time: 1.090815544128418\n",
      "Step: 3554, Loss: 0.9161944389343262, Accuracy: 1.0, Computation time: 1.2101528644561768\n",
      "Step: 3555, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 1.1983847618103027\n",
      "Step: 3556, Loss: 0.9162378907203674, Accuracy: 1.0, Computation time: 1.0615150928497314\n",
      "Step: 3557, Loss: 0.9160594344139099, Accuracy: 1.0, Computation time: 1.2051737308502197\n",
      "Step: 3558, Loss: 0.9160333275794983, Accuracy: 1.0, Computation time: 1.332961082458496\n",
      "Step: 3559, Loss: 0.9378507137298584, Accuracy: 0.96875, Computation time: 1.0213706493377686\n",
      "Step: 3560, Loss: 0.9160729646682739, Accuracy: 1.0, Computation time: 1.1239404678344727\n",
      "Step: 3561, Loss: 0.9231359958648682, Accuracy: 1.0, Computation time: 1.3691260814666748\n",
      "Step: 3562, Loss: 0.9159813523292542, Accuracy: 1.0, Computation time: 1.2231996059417725\n",
      "Step: 3563, Loss: 0.9187451004981995, Accuracy: 1.0, Computation time: 1.329380750656128\n",
      "Step: 3564, Loss: 0.9159746766090393, Accuracy: 1.0, Computation time: 1.1766550540924072\n",
      "Step: 3565, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 1.0704078674316406\n",
      "Step: 3566, Loss: 0.9159661531448364, Accuracy: 1.0, Computation time: 1.2606017589569092\n",
      "Step: 3567, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 1.130490779876709\n",
      "Step: 3568, Loss: 0.9177353978157043, Accuracy: 1.0, Computation time: 1.450348138809204\n",
      "Step: 3569, Loss: 0.9161315560340881, Accuracy: 1.0, Computation time: 1.1739270687103271\n",
      "Step: 3570, Loss: 0.9160874485969543, Accuracy: 1.0, Computation time: 1.252739429473877\n",
      "Step: 3571, Loss: 0.9165781140327454, Accuracy: 1.0, Computation time: 1.1022069454193115\n",
      "Step: 3572, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.0317299365997314\n",
      "Step: 3573, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 1.2908942699432373\n",
      "Step: 3574, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 1.1752967834472656\n",
      "Step: 3575, Loss: 0.9159865379333496, Accuracy: 1.0, Computation time: 1.6109051704406738\n",
      "Step: 3576, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.6790821552276611\n",
      "Step: 3577, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.2897841930389404\n",
      "Step: 3578, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.168015956878662\n",
      "Step: 3579, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.1923205852508545\n",
      "Step: 3580, Loss: 0.9159880876541138, Accuracy: 1.0, Computation time: 1.1154215335845947\n",
      "Step: 3581, Loss: 0.9161478877067566, Accuracy: 1.0, Computation time: 1.3120806217193604\n",
      "Step: 3582, Loss: 0.9179471135139465, Accuracy: 1.0, Computation time: 1.208188533782959\n",
      "Step: 3583, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.312636375427246\n",
      "Step: 3584, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.632411003112793\n",
      "Step: 3585, Loss: 0.9160889983177185, Accuracy: 1.0, Computation time: 1.3144512176513672\n",
      "Step: 3586, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.3839774131774902\n",
      "Step: 3587, Loss: 0.9160184860229492, Accuracy: 1.0, Computation time: 1.0612010955810547\n",
      "Step: 3588, Loss: 0.9159801602363586, Accuracy: 1.0, Computation time: 1.338385820388794\n",
      "Step: 3589, Loss: 0.9291249513626099, Accuracy: 0.96875, Computation time: 1.3712999820709229\n",
      "Step: 3590, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 1.148362159729004\n",
      "Step: 3591, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.214081048965454\n",
      "Step: 3592, Loss: 0.9162974953651428, Accuracy: 1.0, Computation time: 1.273303747177124\n",
      "Step: 3593, Loss: 0.9162615537643433, Accuracy: 1.0, Computation time: 1.2690742015838623\n",
      "Step: 3594, Loss: 0.916078507900238, Accuracy: 1.0, Computation time: 1.1377136707305908\n",
      "Step: 3595, Loss: 0.9375771880149841, Accuracy: 0.96875, Computation time: 1.1170167922973633\n",
      "Step: 3596, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 1.4213738441467285\n",
      "Step: 3597, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.2361674308776855\n",
      "Step: 3598, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.0534741878509521\n",
      "Step: 3599, Loss: 0.9344357848167419, Accuracy: 0.96875, Computation time: 1.987865924835205\n",
      "Step: 3600, Loss: 0.9379198551177979, Accuracy: 0.96875, Computation time: 1.113048791885376\n",
      "Step: 3601, Loss: 0.9160676002502441, Accuracy: 1.0, Computation time: 1.227950096130371\n",
      "Step: 3602, Loss: 0.916182279586792, Accuracy: 1.0, Computation time: 1.100280523300171\n",
      "Step: 3603, Loss: 0.9161245822906494, Accuracy: 1.0, Computation time: 1.063847541809082\n",
      "Step: 3604, Loss: 0.9162077307701111, Accuracy: 1.0, Computation time: 1.4923059940338135\n",
      "Step: 3605, Loss: 0.9161496162414551, Accuracy: 1.0, Computation time: 1.3983912467956543\n",
      "Step: 3606, Loss: 0.9160268306732178, Accuracy: 1.0, Computation time: 1.303100347518921\n",
      "Step: 3607, Loss: 0.9160235524177551, Accuracy: 1.0, Computation time: 0.9015862941741943\n",
      "Step: 3608, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 0.911552906036377\n",
      "Step: 3609, Loss: 0.916201114654541, Accuracy: 1.0, Computation time: 1.198960542678833\n",
      "Step: 3610, Loss: 0.9159619808197021, Accuracy: 1.0, Computation time: 1.021489143371582\n",
      "Step: 3611, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 0.9907755851745605\n",
      "Step: 3612, Loss: 0.9160403609275818, Accuracy: 1.0, Computation time: 1.1673283576965332\n",
      "Step: 3613, Loss: 0.9418450593948364, Accuracy: 0.96875, Computation time: 1.966646671295166\n",
      "########################\n",
      "Test loss: 1.1266252994537354, Test Accuracy_epoch26: 0.6930875778198242\n",
      "########################\n",
      "Step: 3614, Loss: 0.9159103631973267, Accuracy: 1.0, Computation time: 0.9141952991485596\n",
      "Step: 3615, Loss: 0.9160043001174927, Accuracy: 1.0, Computation time: 1.0400996208190918\n",
      "Step: 3616, Loss: 0.9160758852958679, Accuracy: 1.0, Computation time: 0.9255936145782471\n",
      "Step: 3617, Loss: 0.924087405204773, Accuracy: 1.0, Computation time: 1.6886928081512451\n",
      "Step: 3618, Loss: 0.9159968495368958, Accuracy: 1.0, Computation time: 1.027174949645996\n",
      "Step: 3619, Loss: 0.9159652590751648, Accuracy: 1.0, Computation time: 0.9828684329986572\n",
      "Step: 3620, Loss: 0.9160574674606323, Accuracy: 1.0, Computation time: 0.9524219036102295\n",
      "Step: 3621, Loss: 0.9160267114639282, Accuracy: 1.0, Computation time: 1.0283324718475342\n",
      "Step: 3622, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 0.9205577373504639\n",
      "Step: 3623, Loss: 0.9367250204086304, Accuracy: 0.96875, Computation time: 1.0689747333526611\n",
      "Step: 3624, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.252802848815918\n",
      "Step: 3625, Loss: 0.9159801602363586, Accuracy: 1.0, Computation time: 1.0148112773895264\n",
      "Step: 3626, Loss: 0.9163199663162231, Accuracy: 1.0, Computation time: 1.1121745109558105\n",
      "Step: 3627, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.6842782497406006\n",
      "Step: 3628, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 1.024461030960083\n",
      "Step: 3629, Loss: 0.9159555435180664, Accuracy: 1.0, Computation time: 1.1344707012176514\n",
      "Step: 3630, Loss: 0.9159294962882996, Accuracy: 1.0, Computation time: 1.0341424942016602\n",
      "Step: 3631, Loss: 0.915945291519165, Accuracy: 1.0, Computation time: 0.9656939506530762\n",
      "Step: 3632, Loss: 0.916012704372406, Accuracy: 1.0, Computation time: 1.452122688293457\n",
      "Step: 3633, Loss: 0.9357879757881165, Accuracy: 0.96875, Computation time: 0.9538803100585938\n",
      "Step: 3634, Loss: 0.9159517288208008, Accuracy: 1.0, Computation time: 1.0624487400054932\n",
      "Step: 3635, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 0.8968660831451416\n",
      "Step: 3636, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 0.8852188587188721\n",
      "Step: 3637, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 0.9392054080963135\n",
      "Step: 3638, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 0.9647078514099121\n",
      "Step: 3639, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 0.8743889331817627\n",
      "Step: 3640, Loss: 0.9159777760505676, Accuracy: 1.0, Computation time: 1.042297124862671\n",
      "Step: 3641, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 0.9824235439300537\n",
      "Step: 3642, Loss: 0.9159680604934692, Accuracy: 1.0, Computation time: 1.0565481185913086\n",
      "Step: 3643, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 0.8255634307861328\n",
      "Step: 3644, Loss: 0.916123628616333, Accuracy: 1.0, Computation time: 0.942577600479126\n",
      "Step: 3645, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.0841948986053467\n",
      "Step: 3646, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 0.8891780376434326\n",
      "Step: 3647, Loss: 0.9166279435157776, Accuracy: 1.0, Computation time: 1.0844988822937012\n",
      "Step: 3648, Loss: 0.9162782430648804, Accuracy: 1.0, Computation time: 1.0128471851348877\n",
      "Step: 3649, Loss: 0.9299736022949219, Accuracy: 0.96875, Computation time: 1.0573837757110596\n",
      "Step: 3650, Loss: 0.9180524945259094, Accuracy: 1.0, Computation time: 1.0449936389923096\n",
      "Step: 3651, Loss: 0.9159406423568726, Accuracy: 1.0, Computation time: 1.1540491580963135\n",
      "Step: 3652, Loss: 0.915976881980896, Accuracy: 1.0, Computation time: 1.1001179218292236\n",
      "Step: 3653, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 1.0256497859954834\n",
      "Step: 3654, Loss: 0.9186562895774841, Accuracy: 1.0, Computation time: 1.2034010887145996\n",
      "Step: 3655, Loss: 0.9159576892852783, Accuracy: 1.0, Computation time: 0.847463846206665\n",
      "Step: 3656, Loss: 0.9159107804298401, Accuracy: 1.0, Computation time: 1.1117937564849854\n",
      "Step: 3657, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 0.9114115238189697\n",
      "Step: 3658, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 0.9379315376281738\n",
      "Step: 3659, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 0.8570659160614014\n",
      "Step: 3660, Loss: 0.9372607469558716, Accuracy: 0.96875, Computation time: 0.8726129531860352\n",
      "Step: 3661, Loss: 0.9159687757492065, Accuracy: 1.0, Computation time: 0.9505739212036133\n",
      "Step: 3662, Loss: 0.9159329533576965, Accuracy: 1.0, Computation time: 0.9116201400756836\n",
      "Step: 3663, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 1.0041539669036865\n",
      "Step: 3664, Loss: 0.9167689085006714, Accuracy: 1.0, Computation time: 1.1663806438446045\n",
      "Step: 3665, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 0.9609723091125488\n",
      "Step: 3666, Loss: 0.918133020401001, Accuracy: 1.0, Computation time: 1.1899182796478271\n",
      "Step: 3667, Loss: 0.9160849452018738, Accuracy: 1.0, Computation time: 0.8053772449493408\n",
      "Step: 3668, Loss: 0.915909469127655, Accuracy: 1.0, Computation time: 0.921088457107544\n",
      "Step: 3669, Loss: 0.9581186771392822, Accuracy: 0.9375, Computation time: 0.9761972427368164\n",
      "Step: 3670, Loss: 0.9256587028503418, Accuracy: 0.96875, Computation time: 1.323512077331543\n",
      "Step: 3671, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 0.8692333698272705\n",
      "Step: 3672, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 0.9272181987762451\n",
      "Step: 3673, Loss: 0.9159780144691467, Accuracy: 1.0, Computation time: 1.0833680629730225\n",
      "Step: 3674, Loss: 0.9375298619270325, Accuracy: 0.96875, Computation time: 1.0396959781646729\n",
      "Step: 3675, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 1.0299508571624756\n",
      "Step: 3676, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 0.9757909774780273\n",
      "Step: 3677, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.085106611251831\n",
      "Step: 3678, Loss: 0.9327673316001892, Accuracy: 0.96875, Computation time: 1.1585462093353271\n",
      "Step: 3679, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 0.9994966983795166\n",
      "Step: 3680, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 0.9151484966278076\n",
      "Step: 3681, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.1060740947723389\n",
      "Step: 3682, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.1437592506408691\n",
      "Step: 3683, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 0.9234218597412109\n",
      "Step: 3684, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 0.8677811622619629\n",
      "Step: 3685, Loss: 0.9376856684684753, Accuracy: 0.96875, Computation time: 1.1015326976776123\n",
      "Step: 3686, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.0878307819366455\n",
      "Step: 3687, Loss: 0.9161733388900757, Accuracy: 1.0, Computation time: 1.0518078804016113\n",
      "Step: 3688, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.057389259338379\n",
      "Step: 3689, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 0.9882833957672119\n",
      "Step: 3690, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 0.9120290279388428\n",
      "Step: 3691, Loss: 0.9194993376731873, Accuracy: 1.0, Computation time: 1.1726171970367432\n",
      "Step: 3692, Loss: 0.9160001873970032, Accuracy: 1.0, Computation time: 1.0399541854858398\n",
      "Step: 3693, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.2245404720306396\n",
      "Step: 3694, Loss: 0.9232540130615234, Accuracy: 1.0, Computation time: 1.1885383129119873\n",
      "Step: 3695, Loss: 0.9371562600135803, Accuracy: 0.96875, Computation time: 1.7729029655456543\n",
      "Step: 3696, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 1.0934820175170898\n",
      "Step: 3697, Loss: 0.9160107374191284, Accuracy: 1.0, Computation time: 1.1905481815338135\n",
      "Step: 3698, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.1714823246002197\n",
      "Step: 3699, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.0034711360931396\n",
      "Step: 3700, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 0.9982929229736328\n",
      "Step: 3701, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.269195795059204\n",
      "Step: 3702, Loss: 0.9181197285652161, Accuracy: 1.0, Computation time: 1.1323821544647217\n",
      "Step: 3703, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.161745309829712\n",
      "Step: 3704, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 1.2131285667419434\n",
      "Step: 3705, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 0.9657032489776611\n",
      "Step: 3706, Loss: 0.9375816583633423, Accuracy: 0.96875, Computation time: 1.119041919708252\n",
      "Step: 3707, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 0.9889793395996094\n",
      "Step: 3708, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 0.9383091926574707\n",
      "Step: 3709, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 0.9724268913269043\n",
      "Step: 3710, Loss: 0.9600649476051331, Accuracy: 0.9375, Computation time: 1.4570283889770508\n",
      "Step: 3711, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.216407060623169\n",
      "Step: 3712, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.0813114643096924\n",
      "Step: 3713, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.376471757888794\n",
      "Step: 3714, Loss: 0.9159383773803711, Accuracy: 1.0, Computation time: 1.357952356338501\n",
      "Step: 3715, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 1.113037347793579\n",
      "Step: 3716, Loss: 0.9374893307685852, Accuracy: 0.96875, Computation time: 1.2998838424682617\n",
      "Step: 3717, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.04085111618042\n",
      "Step: 3718, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.4136443138122559\n",
      "Step: 3719, Loss: 0.9158753156661987, Accuracy: 1.0, Computation time: 1.091597080230713\n",
      "Step: 3720, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 0.9465141296386719\n",
      "Step: 3721, Loss: 0.9158543944358826, Accuracy: 1.0, Computation time: 1.2979974746704102\n",
      "Step: 3722, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.1431331634521484\n",
      "Step: 3723, Loss: 0.9158852696418762, Accuracy: 1.0, Computation time: 1.3236732482910156\n",
      "Step: 3724, Loss: 0.9160838723182678, Accuracy: 1.0, Computation time: 1.2017745971679688\n",
      "Step: 3725, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 0.9935939311981201\n",
      "Step: 3726, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.299163818359375\n",
      "Step: 3727, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.150036334991455\n",
      "Step: 3728, Loss: 0.9158875346183777, Accuracy: 1.0, Computation time: 1.1559669971466064\n",
      "Step: 3729, Loss: 0.9361401200294495, Accuracy: 0.96875, Computation time: 1.3918485641479492\n",
      "Step: 3730, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.1908252239227295\n",
      "Step: 3731, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 1.5846328735351562\n",
      "Step: 3732, Loss: 0.9160141348838806, Accuracy: 1.0, Computation time: 1.1711833477020264\n",
      "Step: 3733, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 1.5916087627410889\n",
      "Step: 3734, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 1.186748743057251\n",
      "Step: 3735, Loss: 0.9164294600486755, Accuracy: 1.0, Computation time: 1.0683331489562988\n",
      "Step: 3736, Loss: 0.9159913659095764, Accuracy: 1.0, Computation time: 1.285999059677124\n",
      "Step: 3737, Loss: 0.9283368587493896, Accuracy: 0.96875, Computation time: 1.3296372890472412\n",
      "Step: 3738, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.1317253112792969\n",
      "Step: 3739, Loss: 0.9159939289093018, Accuracy: 1.0, Computation time: 0.9968409538269043\n",
      "Step: 3740, Loss: 0.9160637259483337, Accuracy: 1.0, Computation time: 0.947396993637085\n",
      "Step: 3741, Loss: 0.9423832893371582, Accuracy: 0.96875, Computation time: 1.5395350456237793\n",
      "Step: 3742, Loss: 0.9160639643669128, Accuracy: 1.0, Computation time: 1.062795877456665\n",
      "Step: 3743, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 1.2118113040924072\n",
      "Step: 3744, Loss: 0.915952205657959, Accuracy: 1.0, Computation time: 1.1220054626464844\n",
      "Step: 3745, Loss: 0.915946900844574, Accuracy: 1.0, Computation time: 0.9230921268463135\n",
      "Step: 3746, Loss: 0.9374839067459106, Accuracy: 0.96875, Computation time: 1.0907604694366455\n",
      "Step: 3747, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.2125585079193115\n",
      "Step: 3748, Loss: 0.9159741997718811, Accuracy: 1.0, Computation time: 1.2331645488739014\n",
      "Step: 3749, Loss: 0.9158998727798462, Accuracy: 1.0, Computation time: 0.932391881942749\n",
      "Step: 3750, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 0.974759578704834\n",
      "Step: 3751, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.1594882011413574\n",
      "Step: 3752, Loss: 0.9164453148841858, Accuracy: 1.0, Computation time: 1.3505632877349854\n",
      "########################\n",
      "Test loss: 1.1156690120697021, Test Accuracy_epoch27: 0.7115207314491272\n",
      "########################\n",
      "Step: 3753, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 0.8886330127716064\n",
      "Step: 3754, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.0138158798217773\n",
      "Step: 3755, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.1588377952575684\n",
      "Step: 3756, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.0571918487548828\n",
      "Step: 3757, Loss: 0.9293684363365173, Accuracy: 0.96875, Computation time: 1.2414214611053467\n",
      "Step: 3758, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 0.9071981906890869\n",
      "Step: 3759, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 0.9570949077606201\n",
      "Step: 3760, Loss: 0.9376804232597351, Accuracy: 0.96875, Computation time: 1.0526254177093506\n",
      "Step: 3761, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 1.2981021404266357\n",
      "Step: 3762, Loss: 0.9159464836120605, Accuracy: 1.0, Computation time: 1.027156114578247\n",
      "Step: 3763, Loss: 0.9159781336784363, Accuracy: 1.0, Computation time: 1.0504424571990967\n",
      "Step: 3764, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.144000768661499\n",
      "Step: 3765, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.0791411399841309\n",
      "Step: 3766, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 0.8845155239105225\n",
      "Step: 3767, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 1.0714702606201172\n",
      "Step: 3768, Loss: 0.9377020001411438, Accuracy: 0.96875, Computation time: 1.2232933044433594\n",
      "Step: 3769, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 0.9350216388702393\n",
      "Step: 3770, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.2155280113220215\n",
      "Step: 3771, Loss: 0.936197817325592, Accuracy: 0.96875, Computation time: 1.185535192489624\n",
      "Step: 3772, Loss: 0.916185736656189, Accuracy: 1.0, Computation time: 1.0904276371002197\n",
      "Step: 3773, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.1973183155059814\n",
      "Step: 3774, Loss: 0.915909469127655, Accuracy: 1.0, Computation time: 1.1391358375549316\n",
      "Step: 3775, Loss: 0.9159376621246338, Accuracy: 1.0, Computation time: 1.5399417877197266\n",
      "Step: 3776, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 0.9732034206390381\n",
      "Step: 3777, Loss: 0.9160783886909485, Accuracy: 1.0, Computation time: 1.1219260692596436\n",
      "Step: 3778, Loss: 0.9158667922019958, Accuracy: 1.0, Computation time: 1.1032538414001465\n",
      "Step: 3779, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 1.1882386207580566\n",
      "Step: 3780, Loss: 0.9584551453590393, Accuracy: 0.9375, Computation time: 1.5180401802062988\n",
      "Step: 3781, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.338043451309204\n",
      "Step: 3782, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.381464958190918\n",
      "Step: 3783, Loss: 0.9159523248672485, Accuracy: 1.0, Computation time: 1.293440818786621\n",
      "Step: 3784, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 0.9713230133056641\n",
      "Step: 3785, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.127526044845581\n",
      "Step: 3786, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.9946019649505615\n",
      "Step: 3787, Loss: 0.9379410147666931, Accuracy: 0.96875, Computation time: 1.0508387088775635\n",
      "Step: 3788, Loss: 0.9373717904090881, Accuracy: 0.96875, Computation time: 1.5658044815063477\n",
      "Step: 3789, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 0.9605422019958496\n",
      "Step: 3790, Loss: 0.9158441424369812, Accuracy: 1.0, Computation time: 1.3281660079956055\n",
      "Step: 3791, Loss: 0.9158603549003601, Accuracy: 1.0, Computation time: 1.5623011589050293\n",
      "Step: 3792, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.582900047302246\n",
      "Step: 3793, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.464878797531128\n",
      "Step: 3794, Loss: 0.9365112781524658, Accuracy: 0.96875, Computation time: 1.3889412879943848\n",
      "Step: 3795, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.2748761177062988\n",
      "Step: 3796, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.3343439102172852\n",
      "Step: 3797, Loss: 0.9373912811279297, Accuracy: 0.96875, Computation time: 1.9691483974456787\n",
      "Step: 3798, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.3020477294921875\n",
      "Step: 3799, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.3474822044372559\n",
      "Step: 3800, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 1.5078253746032715\n",
      "Step: 3801, Loss: 0.9161825180053711, Accuracy: 1.0, Computation time: 1.2713050842285156\n",
      "Step: 3802, Loss: 0.916182816028595, Accuracy: 1.0, Computation time: 1.2233011722564697\n",
      "Step: 3803, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.63442063331604\n",
      "Step: 3804, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.4038937091827393\n",
      "Step: 3805, Loss: 0.9158755540847778, Accuracy: 1.0, Computation time: 1.0606250762939453\n",
      "Step: 3806, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.334242582321167\n",
      "Step: 3807, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 1.1262457370758057\n",
      "Step: 3808, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 1.2308385372161865\n",
      "Step: 3809, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.0574672222137451\n",
      "Step: 3810, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.0429117679595947\n",
      "Step: 3811, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 0.9548115730285645\n",
      "Step: 3812, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 1.4621891975402832\n",
      "Step: 3813, Loss: 0.936884880065918, Accuracy: 0.96875, Computation time: 1.766387701034546\n",
      "Step: 3814, Loss: 0.9592514634132385, Accuracy: 0.9375, Computation time: 1.4351296424865723\n",
      "Step: 3815, Loss: 0.9376229047775269, Accuracy: 0.96875, Computation time: 1.1914193630218506\n",
      "Step: 3816, Loss: 0.9164934754371643, Accuracy: 1.0, Computation time: 1.143338918685913\n",
      "Step: 3817, Loss: 0.9165470600128174, Accuracy: 1.0, Computation time: 1.1811344623565674\n",
      "Step: 3818, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.4722073078155518\n",
      "Step: 3819, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.0836615562438965\n",
      "Step: 3820, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.1533408164978027\n",
      "Step: 3821, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.11763596534729\n",
      "Step: 3822, Loss: 0.9182630777359009, Accuracy: 1.0, Computation time: 1.2737364768981934\n",
      "Step: 3823, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.4263076782226562\n",
      "Step: 3824, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 1.0975072383880615\n",
      "Step: 3825, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.3419196605682373\n",
      "Step: 3826, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.1294713020324707\n",
      "Step: 3827, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.495638370513916\n",
      "Step: 3828, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.118218183517456\n",
      "Step: 3829, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.1271586418151855\n",
      "Step: 3830, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.486499309539795\n",
      "Step: 3831, Loss: 0.9160881638526917, Accuracy: 1.0, Computation time: 1.373218297958374\n",
      "Step: 3832, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.650434970855713\n",
      "Step: 3833, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 1.4230384826660156\n",
      "Step: 3834, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.1861381530761719\n",
      "Step: 3835, Loss: 0.9171563982963562, Accuracy: 1.0, Computation time: 1.3751435279846191\n",
      "Step: 3836, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.5538573265075684\n",
      "Step: 3837, Loss: 0.937874972820282, Accuracy: 0.96875, Computation time: 1.370375633239746\n",
      "Step: 3838, Loss: 0.9159594178199768, Accuracy: 1.0, Computation time: 1.1888389587402344\n",
      "Step: 3839, Loss: 0.9344269633293152, Accuracy: 0.96875, Computation time: 1.65147066116333\n",
      "Step: 3840, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.6444404125213623\n",
      "Step: 3841, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 1.3500068187713623\n",
      "Step: 3842, Loss: 0.9342955350875854, Accuracy: 0.96875, Computation time: 1.2707791328430176\n",
      "Step: 3843, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 0.9372000694274902\n",
      "Step: 3844, Loss: 0.9370632171630859, Accuracy: 0.96875, Computation time: 1.3134033679962158\n",
      "Step: 3845, Loss: 0.9160037040710449, Accuracy: 1.0, Computation time: 1.8332033157348633\n",
      "Step: 3846, Loss: 0.9159145951271057, Accuracy: 1.0, Computation time: 1.068845510482788\n",
      "Step: 3847, Loss: 0.935440182685852, Accuracy: 0.96875, Computation time: 1.4183251857757568\n",
      "Step: 3848, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 0.973193883895874\n",
      "Step: 3849, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.3766238689422607\n",
      "Step: 3850, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.0910060405731201\n",
      "Step: 3851, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.256899356842041\n",
      "Step: 3852, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.4041955471038818\n",
      "Step: 3853, Loss: 0.915919303894043, Accuracy: 1.0, Computation time: 1.5176057815551758\n",
      "Step: 3854, Loss: 0.9168752431869507, Accuracy: 1.0, Computation time: 1.1942665576934814\n",
      "Step: 3855, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 1.1279895305633545\n",
      "Step: 3856, Loss: 0.9377639293670654, Accuracy: 0.96875, Computation time: 1.1512408256530762\n",
      "Step: 3857, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.0830607414245605\n",
      "Step: 3858, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.1133849620819092\n",
      "Step: 3859, Loss: 0.9168928861618042, Accuracy: 1.0, Computation time: 1.1369726657867432\n",
      "Step: 3860, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.1152229309082031\n",
      "Step: 3861, Loss: 0.9329108595848083, Accuracy: 0.96875, Computation time: 1.2524263858795166\n",
      "Step: 3862, Loss: 0.916374921798706, Accuracy: 1.0, Computation time: 1.3233561515808105\n",
      "Step: 3863, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 1.0701014995574951\n",
      "Step: 3864, Loss: 0.9162110686302185, Accuracy: 1.0, Computation time: 1.2663640975952148\n",
      "Step: 3865, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.4395942687988281\n",
      "Step: 3866, Loss: 0.9160104990005493, Accuracy: 1.0, Computation time: 0.9739105701446533\n",
      "Step: 3867, Loss: 0.925710916519165, Accuracy: 0.96875, Computation time: 1.6104581356048584\n",
      "Step: 3868, Loss: 0.9161562323570251, Accuracy: 1.0, Computation time: 1.2545466423034668\n",
      "Step: 3869, Loss: 0.9159697890281677, Accuracy: 1.0, Computation time: 1.2541449069976807\n",
      "Step: 3870, Loss: 0.9159831404685974, Accuracy: 1.0, Computation time: 1.3940625190734863\n",
      "Step: 3871, Loss: 0.9377672672271729, Accuracy: 0.96875, Computation time: 1.1984517574310303\n",
      "Step: 3872, Loss: 0.9159506559371948, Accuracy: 1.0, Computation time: 1.2353134155273438\n",
      "Step: 3873, Loss: 0.9159916639328003, Accuracy: 1.0, Computation time: 1.3522047996520996\n",
      "Step: 3874, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 1.1865453720092773\n",
      "Step: 3875, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 1.483781337738037\n",
      "Step: 3876, Loss: 0.9160568714141846, Accuracy: 1.0, Computation time: 1.3444764614105225\n",
      "Step: 3877, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.0438246726989746\n",
      "Step: 3878, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 0.990032434463501\n",
      "Step: 3879, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 1.3149042129516602\n",
      "Step: 3880, Loss: 0.9159631133079529, Accuracy: 1.0, Computation time: 1.267806053161621\n",
      "Step: 3881, Loss: 0.9159518480300903, Accuracy: 1.0, Computation time: 1.180086374282837\n",
      "Step: 3882, Loss: 0.9285891652107239, Accuracy: 0.96875, Computation time: 1.174933671951294\n",
      "Step: 3883, Loss: 0.9160465598106384, Accuracy: 1.0, Computation time: 1.8432464599609375\n",
      "Step: 3884, Loss: 0.9324322938919067, Accuracy: 0.96875, Computation time: 1.4370496273040771\n",
      "Step: 3885, Loss: 0.9160547852516174, Accuracy: 1.0, Computation time: 1.335648775100708\n",
      "Step: 3886, Loss: 0.916546642780304, Accuracy: 1.0, Computation time: 1.208221197128296\n",
      "Step: 3887, Loss: 0.9161047339439392, Accuracy: 1.0, Computation time: 1.2726500034332275\n",
      "Step: 3888, Loss: 0.9164907336235046, Accuracy: 1.0, Computation time: 0.9549999237060547\n",
      "Step: 3889, Loss: 0.915980875492096, Accuracy: 1.0, Computation time: 1.010970115661621\n",
      "Step: 3890, Loss: 0.9159199595451355, Accuracy: 1.0, Computation time: 1.3258233070373535\n",
      "Step: 3891, Loss: 0.9169918894767761, Accuracy: 1.0, Computation time: 1.1901016235351562\n",
      "########################\n",
      "Test loss: 1.1265733242034912, Test Accuracy_epoch28: 0.6940092444419861\n",
      "########################\n",
      "Step: 3892, Loss: 0.9163743257522583, Accuracy: 1.0, Computation time: 1.3181641101837158\n",
      "Step: 3893, Loss: 0.9161093235015869, Accuracy: 1.0, Computation time: 1.128598928451538\n",
      "Step: 3894, Loss: 0.9164835214614868, Accuracy: 1.0, Computation time: 1.4821062088012695\n",
      "Step: 3895, Loss: 0.916498601436615, Accuracy: 1.0, Computation time: 1.2863032817840576\n",
      "Step: 3896, Loss: 0.9161978363990784, Accuracy: 1.0, Computation time: 1.1577115058898926\n",
      "Step: 3897, Loss: 0.9377198219299316, Accuracy: 0.96875, Computation time: 1.2197680473327637\n",
      "Step: 3898, Loss: 0.9159691333770752, Accuracy: 1.0, Computation time: 1.4454548358917236\n",
      "Step: 3899, Loss: 0.916133463382721, Accuracy: 1.0, Computation time: 1.1897766590118408\n",
      "Step: 3900, Loss: 0.9161301255226135, Accuracy: 1.0, Computation time: 1.1392197608947754\n",
      "Step: 3901, Loss: 0.9160850048065186, Accuracy: 1.0, Computation time: 1.2904002666473389\n",
      "Step: 3902, Loss: 0.937957227230072, Accuracy: 0.96875, Computation time: 1.506192922592163\n",
      "Step: 3903, Loss: 0.9163663387298584, Accuracy: 1.0, Computation time: 1.239537239074707\n",
      "Step: 3904, Loss: 0.9159952402114868, Accuracy: 1.0, Computation time: 1.2887423038482666\n",
      "Step: 3905, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.3875877857208252\n",
      "Step: 3906, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.2409870624542236\n",
      "Step: 3907, Loss: 0.9166154861450195, Accuracy: 1.0, Computation time: 1.059715986251831\n",
      "Step: 3908, Loss: 0.916071355342865, Accuracy: 1.0, Computation time: 1.0712499618530273\n",
      "Step: 3909, Loss: 0.9377825260162354, Accuracy: 0.96875, Computation time: 1.0309398174285889\n",
      "Step: 3910, Loss: 0.9159231185913086, Accuracy: 1.0, Computation time: 1.1139156818389893\n",
      "Step: 3911, Loss: 0.9160351753234863, Accuracy: 1.0, Computation time: 0.9658732414245605\n",
      "Step: 3912, Loss: 0.9160720109939575, Accuracy: 1.0, Computation time: 1.147202730178833\n",
      "Step: 3913, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 0.999544620513916\n",
      "Step: 3914, Loss: 0.9161593914031982, Accuracy: 1.0, Computation time: 1.0705888271331787\n",
      "Step: 3915, Loss: 0.9159197807312012, Accuracy: 1.0, Computation time: 1.1634068489074707\n",
      "Step: 3916, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 0.9946110248565674\n",
      "Step: 3917, Loss: 0.9161394834518433, Accuracy: 1.0, Computation time: 1.4320545196533203\n",
      "Step: 3918, Loss: 0.9159244894981384, Accuracy: 1.0, Computation time: 1.1620397567749023\n",
      "Step: 3919, Loss: 0.9160146117210388, Accuracy: 1.0, Computation time: 1.0593738555908203\n",
      "Step: 3920, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 1.1096997261047363\n",
      "Step: 3921, Loss: 0.9205663800239563, Accuracy: 1.0, Computation time: 1.08345627784729\n",
      "Step: 3922, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 0.9382259845733643\n",
      "Step: 3923, Loss: 0.9162445664405823, Accuracy: 1.0, Computation time: 1.1588199138641357\n",
      "Step: 3924, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 1.2578442096710205\n",
      "Step: 3925, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 1.193242073059082\n",
      "Step: 3926, Loss: 0.9226066470146179, Accuracy: 1.0, Computation time: 1.460010051727295\n",
      "Step: 3927, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 1.5196201801300049\n",
      "Step: 3928, Loss: 0.9161044955253601, Accuracy: 1.0, Computation time: 1.5450589656829834\n",
      "Step: 3929, Loss: 0.9160072207450867, Accuracy: 1.0, Computation time: 1.3215961456298828\n",
      "Step: 3930, Loss: 0.9159854054450989, Accuracy: 1.0, Computation time: 1.052919626235962\n",
      "Step: 3931, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.5094671249389648\n",
      "Step: 3932, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.254237413406372\n",
      "Step: 3933, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.1823945045471191\n",
      "Step: 3934, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.1536586284637451\n",
      "Step: 3935, Loss: 0.9159711599349976, Accuracy: 1.0, Computation time: 1.3316192626953125\n",
      "Step: 3936, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.9585568904876709\n",
      "Step: 3937, Loss: 0.9268483519554138, Accuracy: 0.96875, Computation time: 1.8162868022918701\n",
      "Step: 3938, Loss: 0.9376567006111145, Accuracy: 0.96875, Computation time: 1.3335826396942139\n",
      "Step: 3939, Loss: 0.9160287976264954, Accuracy: 1.0, Computation time: 1.2780001163482666\n",
      "Step: 3940, Loss: 0.9160000681877136, Accuracy: 1.0, Computation time: 1.4504585266113281\n",
      "Step: 3941, Loss: 0.9161114692687988, Accuracy: 1.0, Computation time: 1.0688426494598389\n",
      "Step: 3942, Loss: 0.9381044507026672, Accuracy: 0.96875, Computation time: 2.1556429862976074\n",
      "Step: 3943, Loss: 0.9378753900527954, Accuracy: 0.96875, Computation time: 1.422924280166626\n",
      "Step: 3944, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.1027617454528809\n",
      "Step: 3945, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.097005844116211\n",
      "Step: 3946, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 1.0363996028900146\n",
      "Step: 3947, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.3816330432891846\n",
      "Step: 3948, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 0.9730596542358398\n",
      "Step: 3949, Loss: 0.9186670184135437, Accuracy: 1.0, Computation time: 1.346738338470459\n",
      "Step: 3950, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 1.3414945602416992\n",
      "Step: 3951, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.0033149719238281\n",
      "Step: 3952, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.2576634883880615\n",
      "Step: 3953, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.578988790512085\n",
      "Step: 3954, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.1782221794128418\n",
      "Step: 3955, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.2947108745574951\n",
      "Step: 3956, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.1338293552398682\n",
      "Step: 3957, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 0.853564977645874\n",
      "Step: 3958, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 1.6377027034759521\n",
      "Step: 3959, Loss: 0.9173914790153503, Accuracy: 1.0, Computation time: 1.3171424865722656\n",
      "Step: 3960, Loss: 0.9191226959228516, Accuracy: 1.0, Computation time: 1.1476268768310547\n",
      "Step: 3961, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 1.27376127243042\n",
      "Step: 3962, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.1618192195892334\n",
      "Step: 3963, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 1.219959020614624\n",
      "Step: 3964, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 1.1222712993621826\n",
      "Step: 3965, Loss: 0.9363064765930176, Accuracy: 0.96875, Computation time: 1.2362117767333984\n",
      "Step: 3966, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 1.0473923683166504\n",
      "Step: 3967, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 1.1568889617919922\n",
      "Step: 3968, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.2685983180999756\n",
      "Step: 3969, Loss: 0.937236487865448, Accuracy: 0.96875, Computation time: 1.2285754680633545\n",
      "Step: 3970, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 0.9805862903594971\n",
      "Step: 3971, Loss: 0.9158843159675598, Accuracy: 1.0, Computation time: 0.9529433250427246\n",
      "Step: 3972, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.0117785930633545\n",
      "Step: 3973, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.3429527282714844\n",
      "Step: 3974, Loss: 0.9246100187301636, Accuracy: 1.0, Computation time: 1.5140693187713623\n",
      "Step: 3975, Loss: 0.9373824596405029, Accuracy: 0.96875, Computation time: 1.3112373352050781\n",
      "Step: 3976, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.137345790863037\n",
      "Step: 3977, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.0676467418670654\n",
      "Step: 3978, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.1405646800994873\n",
      "Step: 3979, Loss: 0.9159955382347107, Accuracy: 1.0, Computation time: 1.2513368129730225\n",
      "Step: 3980, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 1.021167278289795\n",
      "Step: 3981, Loss: 0.9158755540847778, Accuracy: 1.0, Computation time: 1.1140773296356201\n",
      "Step: 3982, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 1.3109393119812012\n",
      "Step: 3983, Loss: 0.9374154806137085, Accuracy: 0.96875, Computation time: 1.061694622039795\n",
      "Step: 3984, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 0.9566483497619629\n",
      "Step: 3985, Loss: 0.916598916053772, Accuracy: 1.0, Computation time: 1.1022353172302246\n",
      "Step: 3986, Loss: 0.9165733456611633, Accuracy: 1.0, Computation time: 1.4855244159698486\n",
      "Step: 3987, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 0.9920873641967773\n",
      "Step: 3988, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.2199361324310303\n",
      "Step: 3989, Loss: 0.9375620484352112, Accuracy: 0.96875, Computation time: 1.3561789989471436\n",
      "Step: 3990, Loss: 0.917264997959137, Accuracy: 1.0, Computation time: 1.2310125827789307\n",
      "Step: 3991, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 1.1956040859222412\n",
      "Step: 3992, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 0.9657704830169678\n",
      "Step: 3993, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 0.9246699810028076\n",
      "Step: 3994, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 1.2472665309906006\n",
      "Step: 3995, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.273850440979004\n",
      "Step: 3996, Loss: 0.9376157522201538, Accuracy: 0.96875, Computation time: 1.3420560359954834\n",
      "Step: 3997, Loss: 0.9160449504852295, Accuracy: 1.0, Computation time: 1.0946714878082275\n",
      "Step: 3998, Loss: 0.9375588297843933, Accuracy: 0.96875, Computation time: 1.1192517280578613\n",
      "Step: 3999, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.2024848461151123\n",
      "Step: 4000, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 0.9598538875579834\n",
      "Step: 4001, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.2006680965423584\n",
      "Step: 4002, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.164186954498291\n",
      "Step: 4003, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 0.9468979835510254\n",
      "Step: 4004, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.0093796253204346\n",
      "Step: 4005, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.4063074588775635\n",
      "Step: 4006, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.9230999946594238\n",
      "Step: 4007, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.1971712112426758\n",
      "Step: 4008, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.5689723491668701\n",
      "Step: 4009, Loss: 0.916282594203949, Accuracy: 1.0, Computation time: 0.9892928600311279\n",
      "Step: 4010, Loss: 0.9374512434005737, Accuracy: 0.96875, Computation time: 1.414942741394043\n",
      "Step: 4011, Loss: 0.9373682737350464, Accuracy: 0.96875, Computation time: 1.1707897186279297\n",
      "Step: 4012, Loss: 0.9166041016578674, Accuracy: 1.0, Computation time: 1.1388602256774902\n",
      "Step: 4013, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 0.9687690734863281\n",
      "Step: 4014, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.296964406967163\n",
      "Step: 4015, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 1.4472346305847168\n",
      "Step: 4016, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.5650463104248047\n",
      "Step: 4017, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.2478773593902588\n",
      "Step: 4018, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.1441075801849365\n",
      "Step: 4019, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 0.97334885597229\n",
      "Step: 4020, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 0.8594436645507812\n",
      "Step: 4021, Loss: 0.9375643730163574, Accuracy: 0.96875, Computation time: 0.8863933086395264\n",
      "Step: 4022, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 0.8728089332580566\n",
      "Step: 4023, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.1669459342956543\n",
      "Step: 4024, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 1.0644888877868652\n",
      "Step: 4025, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.272819995880127\n",
      "Step: 4026, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.132680892944336\n",
      "Step: 4027, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.348785161972046\n",
      "Step: 4028, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 0.9157154560089111\n",
      "Step: 4029, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.0797147750854492\n",
      "Step: 4030, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 1.4053537845611572\n",
      "########################\n",
      "Test loss: 1.1161247491836548, Test Accuracy_epoch29: 0.7105990648269653\n",
      "########################\n",
      "Step: 4031, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.023711919784546\n",
      "Step: 4032, Loss: 0.9158371686935425, Accuracy: 1.0, Computation time: 1.0359506607055664\n",
      "Step: 4033, Loss: 0.9159570336341858, Accuracy: 1.0, Computation time: 1.0746240615844727\n",
      "Step: 4034, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.4297218322753906\n",
      "Step: 4035, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.0713398456573486\n",
      "Step: 4036, Loss: 0.9375371932983398, Accuracy: 0.96875, Computation time: 0.9385356903076172\n",
      "Step: 4037, Loss: 0.9192057251930237, Accuracy: 1.0, Computation time: 1.1996686458587646\n",
      "Step: 4038, Loss: 0.9159172177314758, Accuracy: 1.0, Computation time: 0.9911060333251953\n",
      "Step: 4039, Loss: 0.9159275889396667, Accuracy: 1.0, Computation time: 1.2290616035461426\n",
      "Step: 4040, Loss: 0.9160592555999756, Accuracy: 1.0, Computation time: 1.2717761993408203\n",
      "Step: 4041, Loss: 0.9160063862800598, Accuracy: 1.0, Computation time: 1.111337661743164\n",
      "Step: 4042, Loss: 0.9159452319145203, Accuracy: 1.0, Computation time: 1.1151340007781982\n",
      "Step: 4043, Loss: 0.9159908294677734, Accuracy: 1.0, Computation time: 0.967710018157959\n",
      "Step: 4044, Loss: 0.9353551268577576, Accuracy: 0.96875, Computation time: 1.3987269401550293\n",
      "Step: 4045, Loss: 0.9375839829444885, Accuracy: 0.96875, Computation time: 1.0005202293395996\n",
      "Step: 4046, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.1353867053985596\n",
      "Step: 4047, Loss: 0.9159443378448486, Accuracy: 1.0, Computation time: 1.1848888397216797\n",
      "Step: 4048, Loss: 0.9265924692153931, Accuracy: 0.96875, Computation time: 1.6835715770721436\n",
      "Step: 4049, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 0.7887117862701416\n",
      "Step: 4050, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.6153020858764648\n",
      "Step: 4051, Loss: 0.9159008264541626, Accuracy: 1.0, Computation time: 0.9795911312103271\n",
      "Step: 4052, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 0.9341497421264648\n",
      "Step: 4053, Loss: 0.937614917755127, Accuracy: 0.96875, Computation time: 1.4406917095184326\n",
      "Step: 4054, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 0.9453451633453369\n",
      "Step: 4055, Loss: 0.937811553478241, Accuracy: 0.96875, Computation time: 1.3158001899719238\n",
      "Step: 4056, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 1.4145169258117676\n",
      "Step: 4057, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.3086881637573242\n",
      "Step: 4058, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 0.9112873077392578\n",
      "Step: 4059, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 1.4745972156524658\n",
      "Step: 4060, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.1770470142364502\n",
      "Step: 4061, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.014378309249878\n",
      "Step: 4062, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.2986781597137451\n",
      "Step: 4063, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.1770942211151123\n",
      "Step: 4064, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.498171329498291\n",
      "Step: 4065, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.3151843547821045\n",
      "Step: 4066, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.4315235614776611\n",
      "Step: 4067, Loss: 0.9357838034629822, Accuracy: 0.96875, Computation time: 1.5906040668487549\n",
      "Step: 4068, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.17704176902771\n",
      "Step: 4069, Loss: 0.9376471042633057, Accuracy: 0.96875, Computation time: 1.268181324005127\n",
      "Step: 4070, Loss: 0.9160029888153076, Accuracy: 1.0, Computation time: 1.0931248664855957\n",
      "Step: 4071, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 1.5030248165130615\n",
      "Step: 4072, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 1.1092603206634521\n",
      "Step: 4073, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.106443166732788\n",
      "Step: 4074, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.193444013595581\n",
      "Step: 4075, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.2583036422729492\n",
      "Step: 4076, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.5185022354125977\n",
      "Step: 4077, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.1068089008331299\n",
      "Step: 4078, Loss: 0.918109655380249, Accuracy: 1.0, Computation time: 1.3396954536437988\n",
      "Step: 4079, Loss: 0.9187018871307373, Accuracy: 1.0, Computation time: 1.3065955638885498\n",
      "Step: 4080, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.3759264945983887\n",
      "Step: 4081, Loss: 0.9358874559402466, Accuracy: 0.96875, Computation time: 1.2258567810058594\n",
      "Step: 4082, Loss: 0.9163117408752441, Accuracy: 1.0, Computation time: 1.3667943477630615\n",
      "Step: 4083, Loss: 0.9160677194595337, Accuracy: 1.0, Computation time: 1.8003511428833008\n",
      "Step: 4084, Loss: 0.916013777256012, Accuracy: 1.0, Computation time: 1.3165171146392822\n",
      "Step: 4085, Loss: 0.91609787940979, Accuracy: 1.0, Computation time: 1.3343291282653809\n",
      "Step: 4086, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.4277191162109375\n",
      "Step: 4087, Loss: 0.937171459197998, Accuracy: 0.96875, Computation time: 1.7896738052368164\n",
      "Step: 4088, Loss: 0.9298105239868164, Accuracy: 0.96875, Computation time: 1.6734302043914795\n",
      "Step: 4089, Loss: 0.919119656085968, Accuracy: 1.0, Computation time: 1.755802869796753\n",
      "Step: 4090, Loss: 0.9160234928131104, Accuracy: 1.0, Computation time: 1.2303190231323242\n",
      "Step: 4091, Loss: 0.916054368019104, Accuracy: 1.0, Computation time: 1.2986912727355957\n",
      "Step: 4092, Loss: 0.9160026907920837, Accuracy: 1.0, Computation time: 1.408754825592041\n",
      "Step: 4093, Loss: 0.9160940647125244, Accuracy: 1.0, Computation time: 0.9759020805358887\n",
      "Step: 4094, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 0.8962366580963135\n",
      "Step: 4095, Loss: 0.9159544110298157, Accuracy: 1.0, Computation time: 1.2801151275634766\n",
      "Step: 4096, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 1.5933890342712402\n",
      "Step: 4097, Loss: 0.915917694568634, Accuracy: 1.0, Computation time: 1.3050341606140137\n",
      "Step: 4098, Loss: 0.9184622764587402, Accuracy: 1.0, Computation time: 1.3691041469573975\n",
      "Step: 4099, Loss: 0.9160082936286926, Accuracy: 1.0, Computation time: 1.503483533859253\n",
      "Step: 4100, Loss: 0.9159801006317139, Accuracy: 1.0, Computation time: 1.4623699188232422\n",
      "Step: 4101, Loss: 0.9159553647041321, Accuracy: 1.0, Computation time: 1.1799960136413574\n",
      "Step: 4102, Loss: 0.93767911195755, Accuracy: 0.96875, Computation time: 1.4365715980529785\n",
      "Step: 4103, Loss: 0.920610785484314, Accuracy: 1.0, Computation time: 1.7154731750488281\n",
      "Step: 4104, Loss: 0.9161387085914612, Accuracy: 1.0, Computation time: 1.8183014392852783\n",
      "Step: 4105, Loss: 0.9375845789909363, Accuracy: 0.96875, Computation time: 1.904891014099121\n",
      "Step: 4106, Loss: 0.9160326719284058, Accuracy: 1.0, Computation time: 1.3034687042236328\n",
      "Step: 4107, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.2905819416046143\n",
      "Step: 4108, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 1.3396356105804443\n",
      "Step: 4109, Loss: 0.915955662727356, Accuracy: 1.0, Computation time: 1.4886035919189453\n",
      "Step: 4110, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.1680870056152344\n",
      "Step: 4111, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 1.4386632442474365\n",
      "Step: 4112, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 0.9589662551879883\n",
      "Step: 4113, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 1.2840306758880615\n",
      "Step: 4114, Loss: 0.9159232378005981, Accuracy: 1.0, Computation time: 1.147040843963623\n",
      "Step: 4115, Loss: 0.9164551496505737, Accuracy: 1.0, Computation time: 1.4455499649047852\n",
      "Step: 4116, Loss: 0.9158980250358582, Accuracy: 1.0, Computation time: 1.030555248260498\n",
      "Step: 4117, Loss: 0.9373388290405273, Accuracy: 0.96875, Computation time: 1.5401837825775146\n",
      "Step: 4118, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.5860042572021484\n",
      "Step: 4119, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 1.178983449935913\n",
      "Step: 4120, Loss: 0.916046679019928, Accuracy: 1.0, Computation time: 1.388340950012207\n",
      "Step: 4121, Loss: 0.916163444519043, Accuracy: 1.0, Computation time: 1.4498326778411865\n",
      "Step: 4122, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.1793134212493896\n",
      "Step: 4123, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.9254560470581055\n",
      "Step: 4124, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 1.2788968086242676\n",
      "Step: 4125, Loss: 0.9378901124000549, Accuracy: 0.96875, Computation time: 1.6693880558013916\n",
      "Step: 4126, Loss: 0.9170432090759277, Accuracy: 1.0, Computation time: 1.9069113731384277\n",
      "Step: 4127, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.3750600814819336\n",
      "Step: 4128, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 1.6592638492584229\n",
      "Step: 4129, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.586108922958374\n",
      "Step: 4130, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 1.5098567008972168\n",
      "Step: 4131, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.5527582168579102\n",
      "Step: 4132, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 1.5495936870574951\n",
      "Step: 4133, Loss: 0.9260141253471375, Accuracy: 0.96875, Computation time: 1.7302675247192383\n",
      "Step: 4134, Loss: 0.9375120997428894, Accuracy: 0.96875, Computation time: 1.2805612087249756\n",
      "Step: 4135, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.238621473312378\n",
      "Step: 4136, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.334352970123291\n",
      "Step: 4137, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.3510358333587646\n",
      "Step: 4138, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.2860829830169678\n",
      "Step: 4139, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 1.0967481136322021\n",
      "Step: 4140, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.3427417278289795\n",
      "Step: 4141, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.455552339553833\n",
      "Step: 4142, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.0784759521484375\n",
      "Step: 4143, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.4098992347717285\n",
      "Step: 4144, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.0417449474334717\n",
      "Step: 4145, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.5098817348480225\n",
      "Step: 4146, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.3117690086364746\n",
      "Step: 4147, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.1879558563232422\n",
      "Step: 4148, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.2334325313568115\n",
      "Step: 4149, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.1536214351654053\n",
      "Step: 4150, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 1.1013984680175781\n",
      "Step: 4151, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 0.9852867126464844\n",
      "Step: 4152, Loss: 0.9160420298576355, Accuracy: 1.0, Computation time: 1.0433316230773926\n",
      "Step: 4153, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.3172831535339355\n",
      "Step: 4154, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.2328379154205322\n",
      "Step: 4155, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.279902696609497\n",
      "Step: 4156, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.4695591926574707\n",
      "Step: 4157, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.0942058563232422\n",
      "Step: 4158, Loss: 0.9569385051727295, Accuracy: 0.9375, Computation time: 1.9929289817810059\n",
      "Step: 4159, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.345367431640625\n",
      "Step: 4160, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.1344008445739746\n",
      "Step: 4161, Loss: 0.9159465432167053, Accuracy: 1.0, Computation time: 1.241990089416504\n",
      "Step: 4162, Loss: 0.9218429923057556, Accuracy: 1.0, Computation time: 2.063342809677124\n",
      "Step: 4163, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.262052059173584\n",
      "Step: 4164, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.2422716617584229\n",
      "Step: 4165, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.3262887001037598\n",
      "Step: 4166, Loss: 0.9158727526664734, Accuracy: 1.0, Computation time: 2.164888381958008\n",
      "Step: 4167, Loss: 0.9191852807998657, Accuracy: 1.0, Computation time: 1.433784008026123\n",
      "Step: 4168, Loss: 0.9159356951713562, Accuracy: 1.0, Computation time: 1.2205891609191895\n",
      "Step: 4169, Loss: 0.9160648584365845, Accuracy: 1.0, Computation time: 1.0713651180267334\n",
      "########################\n",
      "Test loss: 1.1199837923049927, Test Accuracy_epoch30: 0.704147458076477\n",
      "########################\n",
      "Step: 4170, Loss: 0.9162696599960327, Accuracy: 1.0, Computation time: 1.5606555938720703\n",
      "Step: 4171, Loss: 0.9364493489265442, Accuracy: 0.96875, Computation time: 1.4885478019714355\n",
      "Step: 4172, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.1870696544647217\n",
      "Step: 4173, Loss: 0.9160943031311035, Accuracy: 1.0, Computation time: 1.3831887245178223\n",
      "Step: 4174, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 1.2674181461334229\n",
      "Step: 4175, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.3788659572601318\n",
      "Step: 4176, Loss: 0.9160619974136353, Accuracy: 1.0, Computation time: 1.5880944728851318\n",
      "Step: 4177, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.511016845703125\n",
      "Step: 4178, Loss: 0.937616765499115, Accuracy: 0.96875, Computation time: 1.5800929069519043\n",
      "Step: 4179, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.451747179031372\n",
      "Step: 4180, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 1.2989189624786377\n",
      "Step: 4181, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.3704020977020264\n",
      "Step: 4182, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.1949877738952637\n",
      "Step: 4183, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.203629493713379\n",
      "Step: 4184, Loss: 0.9375404119491577, Accuracy: 0.96875, Computation time: 1.0943152904510498\n",
      "Step: 4185, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.1891815662384033\n",
      "Step: 4186, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.0792043209075928\n",
      "Step: 4187, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.0927677154541016\n",
      "Step: 4188, Loss: 0.9159204363822937, Accuracy: 1.0, Computation time: 1.3788840770721436\n",
      "Step: 4189, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 0.8466684818267822\n",
      "Step: 4190, Loss: 0.9375637769699097, Accuracy: 0.96875, Computation time: 1.1230759620666504\n",
      "Step: 4191, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.122194528579712\n",
      "Step: 4192, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.5673282146453857\n",
      "Step: 4193, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.5804755687713623\n",
      "Step: 4194, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.4272973537445068\n",
      "Step: 4195, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 1.111482858657837\n",
      "Step: 4196, Loss: 0.9164101481437683, Accuracy: 1.0, Computation time: 1.260838270187378\n",
      "Step: 4197, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.434171199798584\n",
      "Step: 4198, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.0708982944488525\n",
      "Step: 4199, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.2163894176483154\n",
      "Step: 4200, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.9990272521972656\n",
      "Step: 4201, Loss: 0.9373226761817932, Accuracy: 0.96875, Computation time: 1.148949384689331\n",
      "Step: 4202, Loss: 0.9158434271812439, Accuracy: 1.0, Computation time: 1.326444149017334\n",
      "Step: 4203, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.0528371334075928\n",
      "Step: 4204, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.5855233669281006\n",
      "Step: 4205, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.4032917022705078\n",
      "Step: 4206, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.1100177764892578\n",
      "Step: 4207, Loss: 0.9361236095428467, Accuracy: 0.96875, Computation time: 1.1463699340820312\n",
      "Step: 4208, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 0.9845819473266602\n",
      "Step: 4209, Loss: 0.9159356951713562, Accuracy: 1.0, Computation time: 1.0818510055541992\n",
      "Step: 4210, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.2786331176757812\n",
      "Step: 4211, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.4002864360809326\n",
      "Step: 4212, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.304931402206421\n",
      "Step: 4213, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.0082626342773438\n",
      "Step: 4214, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.2043957710266113\n",
      "Step: 4215, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.3079397678375244\n",
      "Step: 4216, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.3423254489898682\n",
      "Step: 4217, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.0408217906951904\n",
      "Step: 4218, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 0.8916380405426025\n",
      "Step: 4219, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.3319549560546875\n",
      "Step: 4220, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.108717918395996\n",
      "Step: 4221, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.2711904048919678\n",
      "Step: 4222, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.2318048477172852\n",
      "Step: 4223, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.2201645374298096\n",
      "Step: 4224, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.27913236618042\n",
      "Step: 4225, Loss: 0.9375239014625549, Accuracy: 0.96875, Computation time: 0.934593677520752\n",
      "Step: 4226, Loss: 0.9161015152931213, Accuracy: 1.0, Computation time: 1.3353278636932373\n",
      "Step: 4227, Loss: 0.916103184223175, Accuracy: 1.0, Computation time: 1.2647838592529297\n",
      "Step: 4228, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.3978466987609863\n",
      "Step: 4229, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 1.1602680683135986\n",
      "Step: 4230, Loss: 0.9365364909172058, Accuracy: 0.96875, Computation time: 1.1496727466583252\n",
      "Step: 4231, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 0.9325857162475586\n",
      "Step: 4232, Loss: 0.9375491142272949, Accuracy: 0.96875, Computation time: 0.9796769618988037\n",
      "Step: 4233, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.133659839630127\n",
      "Step: 4234, Loss: 0.936613917350769, Accuracy: 0.96875, Computation time: 1.3438234329223633\n",
      "Step: 4235, Loss: 0.9163066148757935, Accuracy: 1.0, Computation time: 1.2198634147644043\n",
      "Step: 4236, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.0956156253814697\n",
      "Step: 4237, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.2910029888153076\n",
      "Step: 4238, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 0.8836948871612549\n",
      "Step: 4239, Loss: 0.9322749376296997, Accuracy: 0.96875, Computation time: 1.538011074066162\n",
      "Step: 4240, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.205674409866333\n",
      "Step: 4241, Loss: 0.9159542322158813, Accuracy: 1.0, Computation time: 1.1895978450775146\n",
      "Step: 4242, Loss: 0.9160171747207642, Accuracy: 1.0, Computation time: 1.474470615386963\n",
      "Step: 4243, Loss: 0.9377110004425049, Accuracy: 0.96875, Computation time: 1.3110802173614502\n",
      "Step: 4244, Loss: 0.9160544276237488, Accuracy: 1.0, Computation time: 1.3129568099975586\n",
      "Step: 4245, Loss: 0.9160215258598328, Accuracy: 1.0, Computation time: 1.9093718528747559\n",
      "Step: 4246, Loss: 0.9159664511680603, Accuracy: 1.0, Computation time: 1.9858651161193848\n",
      "Step: 4247, Loss: 0.9163507223129272, Accuracy: 1.0, Computation time: 1.8933181762695312\n",
      "Step: 4248, Loss: 0.9232628345489502, Accuracy: 1.0, Computation time: 1.211855411529541\n",
      "Step: 4249, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.6004106998443604\n",
      "Step: 4250, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.2739877700805664\n",
      "Step: 4251, Loss: 0.9161102175712585, Accuracy: 1.0, Computation time: 1.094782829284668\n",
      "Step: 4252, Loss: 0.9160751700401306, Accuracy: 1.0, Computation time: 1.2096235752105713\n",
      "Step: 4253, Loss: 0.9161535501480103, Accuracy: 1.0, Computation time: 1.2958052158355713\n",
      "Step: 4254, Loss: 0.9161078929901123, Accuracy: 1.0, Computation time: 1.248314619064331\n",
      "Step: 4255, Loss: 0.9519428610801697, Accuracy: 0.9375, Computation time: 1.258742332458496\n",
      "Step: 4256, Loss: 0.9159748554229736, Accuracy: 1.0, Computation time: 1.3521757125854492\n",
      "Step: 4257, Loss: 0.9159882664680481, Accuracy: 1.0, Computation time: 1.30950927734375\n",
      "Step: 4258, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.3987479209899902\n",
      "Step: 4259, Loss: 0.9159964919090271, Accuracy: 1.0, Computation time: 1.019700288772583\n",
      "Step: 4260, Loss: 0.9159573316574097, Accuracy: 1.0, Computation time: 1.2671542167663574\n",
      "Step: 4261, Loss: 0.9162836670875549, Accuracy: 1.0, Computation time: 1.284926176071167\n",
      "Step: 4262, Loss: 0.916218101978302, Accuracy: 1.0, Computation time: 1.5253686904907227\n",
      "Step: 4263, Loss: 0.9374284148216248, Accuracy: 0.96875, Computation time: 1.280705451965332\n",
      "Step: 4264, Loss: 0.9159168601036072, Accuracy: 1.0, Computation time: 1.1792669296264648\n",
      "Step: 4265, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.3751611709594727\n",
      "Step: 4266, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.502152681350708\n",
      "Step: 4267, Loss: 0.9192536473274231, Accuracy: 1.0, Computation time: 1.7301719188690186\n",
      "Step: 4268, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.6275007724761963\n",
      "Step: 4269, Loss: 0.9375467300415039, Accuracy: 0.96875, Computation time: 1.3711755275726318\n",
      "Step: 4270, Loss: 0.9375961422920227, Accuracy: 0.96875, Computation time: 1.4710185527801514\n",
      "Step: 4271, Loss: 0.9159507155418396, Accuracy: 1.0, Computation time: 1.5045526027679443\n",
      "Step: 4272, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.3033201694488525\n",
      "Step: 4273, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.3089628219604492\n",
      "Step: 4274, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.3512272834777832\n",
      "Step: 4275, Loss: 0.9486851692199707, Accuracy: 0.9375, Computation time: 1.4734506607055664\n",
      "Step: 4276, Loss: 0.9159202575683594, Accuracy: 1.0, Computation time: 1.3023741245269775\n",
      "Step: 4277, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 1.1884095668792725\n",
      "Step: 4278, Loss: 0.9324620962142944, Accuracy: 0.96875, Computation time: 1.725707769393921\n",
      "Step: 4279, Loss: 0.9160155057907104, Accuracy: 1.0, Computation time: 1.0884578227996826\n",
      "Step: 4280, Loss: 0.9160091876983643, Accuracy: 1.0, Computation time: 1.5355145931243896\n",
      "Step: 4281, Loss: 0.9159896969795227, Accuracy: 1.0, Computation time: 1.4009215831756592\n",
      "Step: 4282, Loss: 0.915938675403595, Accuracy: 1.0, Computation time: 1.1442956924438477\n",
      "Step: 4283, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.452291488647461\n",
      "Step: 4284, Loss: 0.9159078001976013, Accuracy: 1.0, Computation time: 1.2859225273132324\n",
      "Step: 4285, Loss: 0.9160279631614685, Accuracy: 1.0, Computation time: 1.414294958114624\n",
      "Step: 4286, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 1.3889057636260986\n",
      "Step: 4287, Loss: 0.9171543121337891, Accuracy: 1.0, Computation time: 1.2605059146881104\n",
      "Step: 4288, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.3014352321624756\n",
      "Step: 4289, Loss: 0.916122317314148, Accuracy: 1.0, Computation time: 1.2869057655334473\n",
      "Step: 4290, Loss: 0.9159145951271057, Accuracy: 1.0, Computation time: 1.4941952228546143\n",
      "Step: 4291, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.3024249076843262\n",
      "Step: 4292, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.3654518127441406\n",
      "Step: 4293, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.1766605377197266\n",
      "Step: 4294, Loss: 0.9160093069076538, Accuracy: 1.0, Computation time: 1.4477829933166504\n",
      "Step: 4295, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.6077663898468018\n",
      "Step: 4296, Loss: 0.9378855228424072, Accuracy: 0.96875, Computation time: 1.2769742012023926\n",
      "Step: 4297, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.5065064430236816\n",
      "Step: 4298, Loss: 0.9275475144386292, Accuracy: 0.96875, Computation time: 1.5438041687011719\n",
      "Step: 4299, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 1.5011911392211914\n",
      "Step: 4300, Loss: 0.9417179822921753, Accuracy: 0.96875, Computation time: 1.937828779220581\n",
      "Step: 4301, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 1.2934787273406982\n",
      "Step: 4302, Loss: 0.9159061312675476, Accuracy: 1.0, Computation time: 1.5308215618133545\n",
      "Step: 4303, Loss: 0.9178996682167053, Accuracy: 1.0, Computation time: 2.033938407897949\n",
      "Step: 4304, Loss: 0.9160000085830688, Accuracy: 1.0, Computation time: 1.5553336143493652\n",
      "Step: 4305, Loss: 0.9160135388374329, Accuracy: 1.0, Computation time: 1.3562066555023193\n",
      "Step: 4306, Loss: 0.9362543225288391, Accuracy: 0.96875, Computation time: 1.5866737365722656\n",
      "Step: 4307, Loss: 0.9161567687988281, Accuracy: 1.0, Computation time: 1.7394061088562012\n",
      "Step: 4308, Loss: 0.9161399006843567, Accuracy: 1.0, Computation time: 1.2484033107757568\n",
      "########################\n",
      "Test loss: 1.1337320804595947, Test Accuracy_epoch31: 0.6829493045806885\n",
      "########################\n",
      "Step: 4309, Loss: 0.9160348176956177, Accuracy: 1.0, Computation time: 1.6898765563964844\n",
      "Step: 4310, Loss: 0.9160093069076538, Accuracy: 1.0, Computation time: 1.7616102695465088\n",
      "Step: 4311, Loss: 0.9159692525863647, Accuracy: 1.0, Computation time: 1.6315464973449707\n",
      "Step: 4312, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 1.7019987106323242\n",
      "Step: 4313, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.1238250732421875\n",
      "Step: 4314, Loss: 0.9345577955245972, Accuracy: 0.96875, Computation time: 1.51092529296875\n",
      "Step: 4315, Loss: 0.9159379005432129, Accuracy: 1.0, Computation time: 1.8580918312072754\n",
      "Step: 4316, Loss: 0.9372652769088745, Accuracy: 0.96875, Computation time: 1.4159362316131592\n",
      "Step: 4317, Loss: 0.9169031977653503, Accuracy: 1.0, Computation time: 1.6069865226745605\n",
      "Step: 4318, Loss: 0.9160312414169312, Accuracy: 1.0, Computation time: 1.3752670288085938\n",
      "Step: 4319, Loss: 0.9159762263298035, Accuracy: 1.0, Computation time: 1.7366342544555664\n",
      "Step: 4320, Loss: 0.9375110268592834, Accuracy: 0.96875, Computation time: 1.438713788986206\n",
      "Step: 4321, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.4362330436706543\n",
      "Step: 4322, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 0.9873716831207275\n",
      "Step: 4323, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.379493236541748\n",
      "Step: 4324, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 1.1308705806732178\n",
      "Step: 4325, Loss: 0.9159900546073914, Accuracy: 1.0, Computation time: 1.5308620929718018\n",
      "Step: 4326, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.2101051807403564\n",
      "Step: 4327, Loss: 0.9159674644470215, Accuracy: 1.0, Computation time: 0.9367225170135498\n",
      "Step: 4328, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.1564862728118896\n",
      "Step: 4329, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.2851676940917969\n",
      "Step: 4330, Loss: 0.9159404039382935, Accuracy: 1.0, Computation time: 0.9618844985961914\n",
      "Step: 4331, Loss: 0.9375675916671753, Accuracy: 0.96875, Computation time: 1.2934534549713135\n",
      "Step: 4332, Loss: 0.9375351071357727, Accuracy: 0.96875, Computation time: 1.1665618419647217\n",
      "Step: 4333, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 1.0784227848052979\n",
      "Step: 4334, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 1.1991939544677734\n",
      "Step: 4335, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.0831503868103027\n",
      "Step: 4336, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.6420247554779053\n",
      "Step: 4337, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.2305822372436523\n",
      "Step: 4338, Loss: 0.9161189198493958, Accuracy: 1.0, Computation time: 1.4645302295684814\n",
      "Step: 4339, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.0899298191070557\n",
      "Step: 4340, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 1.126225471496582\n",
      "Step: 4341, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 0.9894437789916992\n",
      "Step: 4342, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.3084568977355957\n",
      "Step: 4343, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.2300994396209717\n",
      "Step: 4344, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.0918681621551514\n",
      "Step: 4345, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.6588873863220215\n",
      "Step: 4346, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.2644462585449219\n",
      "Step: 4347, Loss: 0.916045606136322, Accuracy: 1.0, Computation time: 1.091329574584961\n",
      "Step: 4348, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.1776773929595947\n",
      "Step: 4349, Loss: 0.936464250087738, Accuracy: 0.96875, Computation time: 1.5321805477142334\n",
      "Step: 4350, Loss: 0.9166380167007446, Accuracy: 1.0, Computation time: 1.7810814380645752\n",
      "Step: 4351, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 2.2893905639648438\n",
      "Step: 4352, Loss: 0.9165302515029907, Accuracy: 1.0, Computation time: 1.6473884582519531\n",
      "Step: 4353, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 1.0320513248443604\n",
      "Step: 4354, Loss: 0.9376230835914612, Accuracy: 0.96875, Computation time: 1.2835335731506348\n",
      "Step: 4355, Loss: 0.9532946348190308, Accuracy: 0.9375, Computation time: 1.04896879196167\n",
      "Step: 4356, Loss: 0.9171908497810364, Accuracy: 1.0, Computation time: 1.3402683734893799\n",
      "Step: 4357, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 0.9394323825836182\n",
      "Step: 4358, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.0323445796966553\n",
      "Step: 4359, Loss: 0.9169900417327881, Accuracy: 1.0, Computation time: 1.3126442432403564\n",
      "Step: 4360, Loss: 0.9161041975021362, Accuracy: 1.0, Computation time: 1.4970464706420898\n",
      "Step: 4361, Loss: 0.9159572124481201, Accuracy: 1.0, Computation time: 0.8618552684783936\n",
      "Step: 4362, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.0385398864746094\n",
      "Step: 4363, Loss: 0.9159172177314758, Accuracy: 1.0, Computation time: 1.1431849002838135\n",
      "Step: 4364, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.280243158340454\n",
      "Step: 4365, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.0055816173553467\n",
      "Step: 4366, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 1.0716230869293213\n",
      "Step: 4367, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 0.996596097946167\n",
      "Step: 4368, Loss: 0.9163979291915894, Accuracy: 1.0, Computation time: 1.4071478843688965\n",
      "Step: 4369, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.007643699645996\n",
      "Step: 4370, Loss: 0.9374492168426514, Accuracy: 0.96875, Computation time: 1.2751734256744385\n",
      "Step: 4371, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.069122314453125\n",
      "Step: 4372, Loss: 0.9160424470901489, Accuracy: 1.0, Computation time: 1.3793599605560303\n",
      "Step: 4373, Loss: 0.9159131646156311, Accuracy: 1.0, Computation time: 0.9574992656707764\n",
      "Step: 4374, Loss: 0.9374760389328003, Accuracy: 0.96875, Computation time: 1.1170976161956787\n",
      "Step: 4375, Loss: 0.9376451373100281, Accuracy: 0.96875, Computation time: 1.233215093612671\n",
      "Step: 4376, Loss: 0.9162454009056091, Accuracy: 1.0, Computation time: 1.2157275676727295\n",
      "Step: 4377, Loss: 0.9305623173713684, Accuracy: 0.96875, Computation time: 0.9681868553161621\n",
      "Step: 4378, Loss: 0.915903627872467, Accuracy: 1.0, Computation time: 0.9178893566131592\n",
      "Step: 4379, Loss: 0.9159631133079529, Accuracy: 1.0, Computation time: 1.068242073059082\n",
      "Step: 4380, Loss: 0.9160239696502686, Accuracy: 1.0, Computation time: 1.0661821365356445\n",
      "Step: 4381, Loss: 0.9159877300262451, Accuracy: 1.0, Computation time: 1.0654118061065674\n",
      "Step: 4382, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 1.0039489269256592\n",
      "Step: 4383, Loss: 0.915911078453064, Accuracy: 1.0, Computation time: 0.9990768432617188\n",
      "Step: 4384, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.3561654090881348\n",
      "Step: 4385, Loss: 0.9373782277107239, Accuracy: 0.96875, Computation time: 0.9984738826751709\n",
      "Step: 4386, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.1155006885528564\n",
      "Step: 4387, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 0.9445829391479492\n",
      "Step: 4388, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 0.922684907913208\n",
      "Step: 4389, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.0256390571594238\n",
      "Step: 4390, Loss: 0.9375059604644775, Accuracy: 0.96875, Computation time: 0.8641636371612549\n",
      "Step: 4391, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.3501925468444824\n",
      "Step: 4392, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 1.1225275993347168\n",
      "Step: 4393, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.1058158874511719\n",
      "Step: 4394, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 1.114715337753296\n",
      "Step: 4395, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.036816120147705\n",
      "Step: 4396, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.116236686706543\n",
      "Step: 4397, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.1859142780303955\n",
      "Step: 4398, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.0706868171691895\n",
      "Step: 4399, Loss: 0.9161767959594727, Accuracy: 1.0, Computation time: 1.3897407054901123\n",
      "Step: 4400, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 0.9682726860046387\n",
      "Step: 4401, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 0.9621002674102783\n",
      "Step: 4402, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 0.9724712371826172\n",
      "Step: 4403, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.8902902603149414\n",
      "Step: 4404, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.4488136768341064\n",
      "Step: 4405, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 0.998136043548584\n",
      "Step: 4406, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 1.0465290546417236\n",
      "Step: 4407, Loss: 0.9365443587303162, Accuracy: 0.96875, Computation time: 1.6671512126922607\n",
      "Step: 4408, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.0432710647583008\n",
      "Step: 4409, Loss: 0.9365679621696472, Accuracy: 0.96875, Computation time: 1.2307186126708984\n",
      "Step: 4410, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 1.1168479919433594\n",
      "Step: 4411, Loss: 0.9376208186149597, Accuracy: 0.96875, Computation time: 1.2472562789916992\n",
      "Step: 4412, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.0034570693969727\n",
      "Step: 4413, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.0056533813476562\n",
      "Step: 4414, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.398747444152832\n",
      "Step: 4415, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 0.9631922245025635\n",
      "Step: 4416, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.0046234130859375\n",
      "Step: 4417, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.3482356071472168\n",
      "Step: 4418, Loss: 0.9158387780189514, Accuracy: 1.0, Computation time: 0.9782352447509766\n",
      "Step: 4419, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.039994716644287\n",
      "Step: 4420, Loss: 0.937864363193512, Accuracy: 0.96875, Computation time: 0.9879558086395264\n",
      "Step: 4421, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 1.1606602668762207\n",
      "Step: 4422, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.2182908058166504\n",
      "Step: 4423, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 1.0607388019561768\n",
      "Step: 4424, Loss: 0.9361258149147034, Accuracy: 0.96875, Computation time: 0.9955859184265137\n",
      "Step: 4425, Loss: 0.9365084171295166, Accuracy: 0.96875, Computation time: 1.25533127784729\n",
      "Step: 4426, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 0.9812939167022705\n",
      "Step: 4427, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.0760245323181152\n",
      "Step: 4428, Loss: 0.9158521294593811, Accuracy: 1.0, Computation time: 1.0647244453430176\n",
      "Step: 4429, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.1243362426757812\n",
      "Step: 4430, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.391775131225586\n",
      "Step: 4431, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.1045639514923096\n",
      "Step: 4432, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.7297532558441162\n",
      "Step: 4433, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.0972976684570312\n",
      "Step: 4434, Loss: 0.9376891851425171, Accuracy: 0.96875, Computation time: 1.3652608394622803\n",
      "Step: 4435, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.7336416244506836\n",
      "Step: 4436, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.0593504905700684\n",
      "Step: 4437, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.2317135334014893\n",
      "Step: 4438, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.183053731918335\n",
      "Step: 4439, Loss: 0.9375299215316772, Accuracy: 0.96875, Computation time: 1.1785755157470703\n",
      "Step: 4440, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 1.1934077739715576\n",
      "Step: 4441, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.4135611057281494\n",
      "Step: 4442, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.1984827518463135\n",
      "Step: 4443, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.0601615905761719\n",
      "Step: 4444, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 0.9233121871948242\n",
      "Step: 4445, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.0317509174346924\n",
      "Step: 4446, Loss: 0.9181128144264221, Accuracy: 1.0, Computation time: 1.8488264083862305\n",
      "########################\n",
      "Test loss: 1.1225199699401855, Test Accuracy_epoch32: 0.6995391845703125\n",
      "########################\n",
      "Step: 4447, Loss: 0.9260412454605103, Accuracy: 0.96875, Computation time: 1.2316224575042725\n",
      "Step: 4448, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 1.0501835346221924\n",
      "Step: 4449, Loss: 0.9159258008003235, Accuracy: 1.0, Computation time: 1.2701361179351807\n",
      "Step: 4450, Loss: 0.9373993873596191, Accuracy: 0.96875, Computation time: 2.1162843704223633\n",
      "Step: 4451, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 0.953758955001831\n",
      "Step: 4452, Loss: 0.9159401655197144, Accuracy: 1.0, Computation time: 1.0517141819000244\n",
      "Step: 4453, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 0.9385745525360107\n",
      "Step: 4454, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.1103434562683105\n",
      "Step: 4455, Loss: 0.9163309931755066, Accuracy: 1.0, Computation time: 1.0177476406097412\n",
      "Step: 4456, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 1.3588788509368896\n",
      "Step: 4457, Loss: 0.9162361025810242, Accuracy: 1.0, Computation time: 0.8883061408996582\n",
      "Step: 4458, Loss: 0.9182029962539673, Accuracy: 1.0, Computation time: 1.3635852336883545\n",
      "Step: 4459, Loss: 0.9160430431365967, Accuracy: 1.0, Computation time: 1.3003571033477783\n",
      "Step: 4460, Loss: 0.9376853704452515, Accuracy: 0.96875, Computation time: 1.19657301902771\n",
      "Step: 4461, Loss: 0.915952205657959, Accuracy: 1.0, Computation time: 1.1727006435394287\n",
      "Step: 4462, Loss: 0.9159330725669861, Accuracy: 1.0, Computation time: 1.3872344493865967\n",
      "Step: 4463, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.1311984062194824\n",
      "Step: 4464, Loss: 0.9375666379928589, Accuracy: 0.96875, Computation time: 1.1133742332458496\n",
      "Step: 4465, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.1278784275054932\n",
      "Step: 4466, Loss: 0.9159725904464722, Accuracy: 1.0, Computation time: 1.0807206630706787\n",
      "Step: 4467, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.0340046882629395\n",
      "Step: 4468, Loss: 0.9159837365150452, Accuracy: 1.0, Computation time: 1.0633518695831299\n",
      "Step: 4469, Loss: 0.9376481771469116, Accuracy: 0.96875, Computation time: 1.0179927349090576\n",
      "Step: 4470, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 0.9692821502685547\n",
      "Step: 4471, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 0.9526307582855225\n",
      "Step: 4472, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.2164311408996582\n",
      "Step: 4473, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.5046284198760986\n",
      "Step: 4474, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.1404130458831787\n",
      "Step: 4475, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 0.9674298763275146\n",
      "Step: 4476, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.9731817245483398\n",
      "Step: 4477, Loss: 0.9160419702529907, Accuracy: 1.0, Computation time: 0.9797766208648682\n",
      "Step: 4478, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 0.9909324645996094\n",
      "Step: 4479, Loss: 0.9379254579544067, Accuracy: 0.96875, Computation time: 1.1244440078735352\n",
      "Step: 4480, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.1187164783477783\n",
      "Step: 4481, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 1.004227638244629\n",
      "Step: 4482, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 0.9936940670013428\n",
      "Step: 4483, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 0.9408144950866699\n",
      "Step: 4484, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.4016923904418945\n",
      "Step: 4485, Loss: 0.9355927109718323, Accuracy: 0.96875, Computation time: 1.255955457687378\n",
      "Step: 4486, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.2478771209716797\n",
      "Step: 4487, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.1469006538391113\n",
      "Step: 4488, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.098163366317749\n",
      "Step: 4489, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.1308748722076416\n",
      "Step: 4490, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.298302412033081\n",
      "Step: 4491, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.2173974514007568\n",
      "Step: 4492, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 0.9714891910552979\n",
      "Step: 4493, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.2020971775054932\n",
      "Step: 4494, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 0.9779665470123291\n",
      "Step: 4495, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.633826494216919\n",
      "Step: 4496, Loss: 0.9362701177597046, Accuracy: 0.96875, Computation time: 1.140089988708496\n",
      "Step: 4497, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.2844524383544922\n",
      "Step: 4498, Loss: 0.9160629510879517, Accuracy: 1.0, Computation time: 1.0152723789215088\n",
      "Step: 4499, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.118340253829956\n",
      "Step: 4500, Loss: 0.9255937337875366, Accuracy: 0.96875, Computation time: 1.3946423530578613\n",
      "Step: 4501, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 0.8806462287902832\n",
      "Step: 4502, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 1.043006181716919\n",
      "Step: 4503, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.103844404220581\n",
      "Step: 4504, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.1395151615142822\n",
      "Step: 4505, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.059450626373291\n",
      "Step: 4506, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.088996171951294\n",
      "Step: 4507, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 0.9787969589233398\n",
      "Step: 4508, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.174422025680542\n",
      "Step: 4509, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.6153576374053955\n",
      "Step: 4510, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.0634543895721436\n",
      "Step: 4511, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 1.0421185493469238\n",
      "Step: 4512, Loss: 0.9167611002922058, Accuracy: 1.0, Computation time: 1.1480448246002197\n",
      "Step: 4513, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.2235419750213623\n",
      "Step: 4514, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.0478341579437256\n",
      "Step: 4515, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.3667058944702148\n",
      "Step: 4516, Loss: 0.937330961227417, Accuracy: 0.96875, Computation time: 1.3022096157073975\n",
      "Step: 4517, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.1080467700958252\n",
      "Step: 4518, Loss: 0.915968120098114, Accuracy: 1.0, Computation time: 1.0698795318603516\n",
      "Step: 4519, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 0.9740405082702637\n",
      "Step: 4520, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.2072489261627197\n",
      "Step: 4521, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.6319503784179688\n",
      "Step: 4522, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.0309884548187256\n",
      "Step: 4523, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.036407709121704\n",
      "Step: 4524, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.2499988079071045\n",
      "Step: 4525, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 1.598876953125\n",
      "Step: 4526, Loss: 0.915966272354126, Accuracy: 1.0, Computation time: 1.1025230884552002\n",
      "Step: 4527, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 1.3029875755310059\n",
      "Step: 4528, Loss: 0.9158900380134583, Accuracy: 1.0, Computation time: 1.4652934074401855\n",
      "Step: 4529, Loss: 0.9374186992645264, Accuracy: 0.96875, Computation time: 1.2316431999206543\n",
      "Step: 4530, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.2702786922454834\n",
      "Step: 4531, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.3225178718566895\n",
      "Step: 4532, Loss: 0.9376068115234375, Accuracy: 0.96875, Computation time: 1.1825799942016602\n",
      "Step: 4533, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.0260601043701172\n",
      "Step: 4534, Loss: 0.9161474108695984, Accuracy: 1.0, Computation time: 1.2252497673034668\n",
      "Step: 4535, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.4014179706573486\n",
      "Step: 4536, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.5358822345733643\n",
      "Step: 4537, Loss: 0.9171038269996643, Accuracy: 1.0, Computation time: 1.2411222457885742\n",
      "Step: 4538, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 1.1851379871368408\n",
      "Step: 4539, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.0373647212982178\n",
      "Step: 4540, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.0473332405090332\n",
      "Step: 4541, Loss: 0.9374501705169678, Accuracy: 0.96875, Computation time: 2.2924065589904785\n",
      "Step: 4542, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 1.1044485569000244\n",
      "Step: 4543, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.071929931640625\n",
      "Step: 4544, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.3347187042236328\n",
      "Step: 4545, Loss: 0.9163442254066467, Accuracy: 1.0, Computation time: 1.2110095024108887\n",
      "Step: 4546, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.1554248332977295\n",
      "Step: 4547, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 0.8964493274688721\n",
      "Step: 4548, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 0.8663206100463867\n",
      "Step: 4549, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 0.9964466094970703\n",
      "Step: 4550, Loss: 0.9159751534461975, Accuracy: 1.0, Computation time: 1.3638904094696045\n",
      "Step: 4551, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.1344373226165771\n",
      "Step: 4552, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.3661620616912842\n",
      "Step: 4553, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.4976887702941895\n",
      "Step: 4554, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.158876895904541\n",
      "Step: 4555, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.3600823879241943\n",
      "Step: 4556, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.2641117572784424\n",
      "Step: 4557, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.206700325012207\n",
      "Step: 4558, Loss: 0.916022002696991, Accuracy: 1.0, Computation time: 1.1453070640563965\n",
      "Step: 4559, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.3712034225463867\n",
      "Step: 4560, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 1.2596931457519531\n",
      "Step: 4561, Loss: 0.9274775981903076, Accuracy: 0.96875, Computation time: 1.2673909664154053\n",
      "Step: 4562, Loss: 0.9161052703857422, Accuracy: 1.0, Computation time: 1.1087276935577393\n",
      "Step: 4563, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.2868726253509521\n",
      "Step: 4564, Loss: 0.9161004424095154, Accuracy: 1.0, Computation time: 1.2255971431732178\n",
      "Step: 4565, Loss: 0.9159165024757385, Accuracy: 1.0, Computation time: 1.2666566371917725\n",
      "Step: 4566, Loss: 0.917296290397644, Accuracy: 1.0, Computation time: 3.0448970794677734\n",
      "Step: 4567, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 1.360184907913208\n",
      "Step: 4568, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.3924977779388428\n",
      "Step: 4569, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.4922287464141846\n",
      "Step: 4570, Loss: 0.9159451127052307, Accuracy: 1.0, Computation time: 1.3253061771392822\n",
      "Step: 4571, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.1716527938842773\n",
      "Step: 4572, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.3464832305908203\n",
      "Step: 4573, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.6835088729858398\n",
      "Step: 4574, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.5215651988983154\n",
      "Step: 4575, Loss: 0.9288763403892517, Accuracy: 0.96875, Computation time: 1.2729060649871826\n",
      "Step: 4576, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.5859384536743164\n",
      "Step: 4577, Loss: 0.936084508895874, Accuracy: 0.96875, Computation time: 1.521698236465454\n",
      "Step: 4578, Loss: 0.937552273273468, Accuracy: 0.96875, Computation time: 1.2751970291137695\n",
      "Step: 4579, Loss: 0.916907012462616, Accuracy: 1.0, Computation time: 1.0983476638793945\n",
      "Step: 4580, Loss: 0.9165056347846985, Accuracy: 1.0, Computation time: 1.2162930965423584\n",
      "Step: 4581, Loss: 0.9379534721374512, Accuracy: 0.96875, Computation time: 1.5520250797271729\n",
      "Step: 4582, Loss: 0.9379051923751831, Accuracy: 0.96875, Computation time: 1.5745582580566406\n",
      "Step: 4583, Loss: 0.9159971475601196, Accuracy: 1.0, Computation time: 1.3517773151397705\n",
      "Step: 4584, Loss: 0.916954755783081, Accuracy: 1.0, Computation time: 1.3685312271118164\n",
      "Step: 4585, Loss: 0.9165506362915039, Accuracy: 1.0, Computation time: 1.3911609649658203\n",
      "########################\n",
      "Test loss: 1.130229115486145, Test Accuracy_epoch33: 0.6866359710693359\n",
      "########################\n",
      "Step: 4586, Loss: 0.9164191484451294, Accuracy: 1.0, Computation time: 1.225900650024414\n",
      "Step: 4587, Loss: 0.9161924719810486, Accuracy: 1.0, Computation time: 1.2767529487609863\n",
      "Step: 4588, Loss: 0.9174505472183228, Accuracy: 1.0, Computation time: 1.4523813724517822\n",
      "Step: 4589, Loss: 0.9377703666687012, Accuracy: 0.96875, Computation time: 1.495290756225586\n",
      "Step: 4590, Loss: 0.9159704446792603, Accuracy: 1.0, Computation time: 1.52940034866333\n",
      "Step: 4591, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 1.4439661502838135\n",
      "Step: 4592, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.2744905948638916\n",
      "Step: 4593, Loss: 0.9159083962440491, Accuracy: 1.0, Computation time: 1.3241631984710693\n",
      "Step: 4594, Loss: 0.9376710057258606, Accuracy: 0.96875, Computation time: 1.2450926303863525\n",
      "Step: 4595, Loss: 0.9159801006317139, Accuracy: 1.0, Computation time: 1.4676094055175781\n",
      "Step: 4596, Loss: 0.9161945581436157, Accuracy: 1.0, Computation time: 1.5777275562286377\n",
      "Step: 4597, Loss: 0.9158914685249329, Accuracy: 1.0, Computation time: 1.3976092338562012\n",
      "Step: 4598, Loss: 0.9216723442077637, Accuracy: 1.0, Computation time: 1.7811899185180664\n",
      "Step: 4599, Loss: 0.9160746932029724, Accuracy: 1.0, Computation time: 1.6390762329101562\n",
      "Step: 4600, Loss: 0.9164975881576538, Accuracy: 1.0, Computation time: 1.32063889503479\n",
      "Step: 4601, Loss: 0.9162725210189819, Accuracy: 1.0, Computation time: 1.2003469467163086\n",
      "Step: 4602, Loss: 0.9161691665649414, Accuracy: 1.0, Computation time: 1.5801825523376465\n",
      "Step: 4603, Loss: 0.9161532521247864, Accuracy: 1.0, Computation time: 1.7275636196136475\n",
      "Step: 4604, Loss: 0.9159931540489197, Accuracy: 1.0, Computation time: 1.5449540615081787\n",
      "Step: 4605, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 1.5156910419464111\n",
      "Step: 4606, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.5262823104858398\n",
      "Step: 4607, Loss: 0.9159658551216125, Accuracy: 1.0, Computation time: 1.4046568870544434\n",
      "Step: 4608, Loss: 0.9161675572395325, Accuracy: 1.0, Computation time: 1.6007590293884277\n",
      "Step: 4609, Loss: 0.9161239862442017, Accuracy: 1.0, Computation time: 1.4017324447631836\n",
      "Step: 4610, Loss: 0.9162119626998901, Accuracy: 1.0, Computation time: 1.490748405456543\n",
      "Step: 4611, Loss: 0.9160366654396057, Accuracy: 1.0, Computation time: 1.46755051612854\n",
      "Step: 4612, Loss: 0.9159623980522156, Accuracy: 1.0, Computation time: 1.5786824226379395\n",
      "Step: 4613, Loss: 0.9159614443778992, Accuracy: 1.0, Computation time: 1.3009250164031982\n",
      "Step: 4614, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.4970436096191406\n",
      "Step: 4615, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.3429763317108154\n",
      "Step: 4616, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.7748727798461914\n",
      "Step: 4617, Loss: 0.9159265756607056, Accuracy: 1.0, Computation time: 1.3811280727386475\n",
      "Step: 4618, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.595245599746704\n",
      "Step: 4619, Loss: 0.9159152507781982, Accuracy: 1.0, Computation time: 1.6442523002624512\n",
      "Step: 4620, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.4389019012451172\n",
      "Step: 4621, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 1.606395959854126\n",
      "Step: 4622, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.9586455821990967\n",
      "Step: 4623, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 1.7533397674560547\n",
      "Step: 4624, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 1.7481403350830078\n",
      "Step: 4625, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.680286169052124\n",
      "Step: 4626, Loss: 0.9375905394554138, Accuracy: 0.96875, Computation time: 1.5243947505950928\n",
      "Step: 4627, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 1.5107338428497314\n",
      "Step: 4628, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.633227825164795\n",
      "Step: 4629, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.5099632740020752\n",
      "Step: 4630, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.655989170074463\n",
      "Step: 4631, Loss: 0.9159069657325745, Accuracy: 1.0, Computation time: 1.4144628047943115\n",
      "Step: 4632, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.8215792179107666\n",
      "Step: 4633, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.7479448318481445\n",
      "Step: 4634, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.3127646446228027\n",
      "Step: 4635, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.7044107913970947\n",
      "Step: 4636, Loss: 0.9258294701576233, Accuracy: 0.96875, Computation time: 1.3055408000946045\n",
      "Step: 4637, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.5285155773162842\n",
      "Step: 4638, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 1.4658000469207764\n",
      "Step: 4639, Loss: 0.9159480333328247, Accuracy: 1.0, Computation time: 1.5322229862213135\n",
      "Step: 4640, Loss: 0.9160957932472229, Accuracy: 1.0, Computation time: 1.4164304733276367\n",
      "Step: 4641, Loss: 0.9341132640838623, Accuracy: 0.96875, Computation time: 1.8288569450378418\n",
      "Step: 4642, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.608506441116333\n",
      "Step: 4643, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 1.7017598152160645\n",
      "Step: 4644, Loss: 0.9167070388793945, Accuracy: 1.0, Computation time: 1.573777675628662\n",
      "Step: 4645, Loss: 0.9159087538719177, Accuracy: 1.0, Computation time: 1.4545392990112305\n",
      "Step: 4646, Loss: 0.9162465333938599, Accuracy: 1.0, Computation time: 1.3908445835113525\n",
      "Step: 4647, Loss: 0.9159483313560486, Accuracy: 1.0, Computation time: 1.5368030071258545\n",
      "Step: 4648, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 1.4673969745635986\n",
      "Step: 4649, Loss: 0.9159773588180542, Accuracy: 1.0, Computation time: 1.3221116065979004\n",
      "Step: 4650, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.4703271389007568\n",
      "Step: 4651, Loss: 0.917427122592926, Accuracy: 1.0, Computation time: 1.5864529609680176\n",
      "Step: 4652, Loss: 0.9375492930412292, Accuracy: 0.96875, Computation time: 1.6205663681030273\n",
      "Step: 4653, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.4333009719848633\n",
      "Step: 4654, Loss: 0.9159618020057678, Accuracy: 1.0, Computation time: 1.4706628322601318\n",
      "Step: 4655, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.4408161640167236\n",
      "Step: 4656, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.492194414138794\n",
      "Step: 4657, Loss: 0.9581294655799866, Accuracy: 0.9375, Computation time: 1.833118200302124\n",
      "Step: 4658, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 1.4391722679138184\n",
      "Step: 4659, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.5049717426300049\n",
      "Step: 4660, Loss: 0.9375536441802979, Accuracy: 0.96875, Computation time: 1.5264039039611816\n",
      "Step: 4661, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.6021888256072998\n",
      "Step: 4662, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 1.378568172454834\n",
      "Step: 4663, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.2139034271240234\n",
      "Step: 4664, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.3508386611938477\n",
      "Step: 4665, Loss: 0.9161282777786255, Accuracy: 1.0, Computation time: 1.362755298614502\n",
      "Step: 4666, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.229454517364502\n",
      "Step: 4667, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.2613515853881836\n",
      "Step: 4668, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.5091440677642822\n",
      "Step: 4669, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.401228904724121\n",
      "Step: 4670, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.481947898864746\n",
      "Step: 4671, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.4116082191467285\n",
      "Step: 4672, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.2059087753295898\n",
      "Step: 4673, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.2721953392028809\n",
      "Step: 4674, Loss: 0.9167484641075134, Accuracy: 1.0, Computation time: 1.2696146965026855\n",
      "Step: 4675, Loss: 0.9191566705703735, Accuracy: 1.0, Computation time: 1.5873658657073975\n",
      "Step: 4676, Loss: 0.9163963198661804, Accuracy: 1.0, Computation time: 1.3455471992492676\n",
      "Step: 4677, Loss: 0.9159637093544006, Accuracy: 1.0, Computation time: 1.2984840869903564\n",
      "Step: 4678, Loss: 0.9159723520278931, Accuracy: 1.0, Computation time: 1.869678258895874\n",
      "Step: 4679, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.3927783966064453\n",
      "Step: 4680, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.438103199005127\n",
      "Step: 4681, Loss: 0.9375782608985901, Accuracy: 0.96875, Computation time: 1.4359474182128906\n",
      "Step: 4682, Loss: 0.9159224629402161, Accuracy: 1.0, Computation time: 1.381242275238037\n",
      "Step: 4683, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.2063677310943604\n",
      "Step: 4684, Loss: 0.937389075756073, Accuracy: 0.96875, Computation time: 1.532557487487793\n",
      "Step: 4685, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.2183380126953125\n",
      "Step: 4686, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 1.17789888381958\n",
      "Step: 4687, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.3105955123901367\n",
      "Step: 4688, Loss: 0.915995180606842, Accuracy: 1.0, Computation time: 1.5515131950378418\n",
      "Step: 4689, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.49446439743042\n",
      "Step: 4690, Loss: 0.9376577734947205, Accuracy: 0.96875, Computation time: 1.1966559886932373\n",
      "Step: 4691, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.2576425075531006\n",
      "Step: 4692, Loss: 0.9159755706787109, Accuracy: 1.0, Computation time: 1.301403522491455\n",
      "Step: 4693, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.2842931747436523\n",
      "Step: 4694, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.047314167022705\n",
      "Step: 4695, Loss: 0.9159902930259705, Accuracy: 1.0, Computation time: 1.6062712669372559\n",
      "Step: 4696, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.063997745513916\n",
      "Step: 4697, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 1.04295015335083\n",
      "Step: 4698, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.1457092761993408\n",
      "Step: 4699, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 1.209512710571289\n",
      "Step: 4700, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.3535211086273193\n",
      "Step: 4701, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.2107560634613037\n",
      "Step: 4702, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.5440289974212646\n",
      "Step: 4703, Loss: 0.9159846901893616, Accuracy: 1.0, Computation time: 1.2593157291412354\n",
      "Step: 4704, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.1102960109710693\n",
      "Step: 4705, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.1384634971618652\n",
      "Step: 4706, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.0646007061004639\n",
      "Step: 4707, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.1607754230499268\n",
      "Step: 4708, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.1085166931152344\n",
      "Step: 4709, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.3331139087677002\n",
      "Step: 4710, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 1.0912010669708252\n",
      "Step: 4711, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.0767817497253418\n",
      "Step: 4712, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.2814486026763916\n",
      "Step: 4713, Loss: 0.9175631999969482, Accuracy: 1.0, Computation time: 1.4532904624938965\n",
      "Step: 4714, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 1.0629215240478516\n",
      "Step: 4715, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 1.1967740058898926\n",
      "Step: 4716, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.2944355010986328\n",
      "Step: 4717, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.1496589183807373\n",
      "Step: 4718, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.2160749435424805\n",
      "Step: 4719, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.0893795490264893\n",
      "Step: 4720, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.1827967166900635\n",
      "Step: 4721, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.7299411296844482\n",
      "Step: 4722, Loss: 0.9159315228462219, Accuracy: 1.0, Computation time: 1.4153437614440918\n",
      "Step: 4723, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 1.0185825824737549\n",
      "Step: 4724, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.1185963153839111\n",
      "########################\n",
      "Test loss: 1.1150717735290527, Test Accuracy_epoch34: 0.7096773982048035\n",
      "########################\n",
      "Step: 4725, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.0767035484313965\n",
      "Step: 4726, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.2138581275939941\n",
      "Step: 4727, Loss: 0.9160241484642029, Accuracy: 1.0, Computation time: 1.104067087173462\n",
      "Step: 4728, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.027970552444458\n",
      "Step: 4729, Loss: 0.9164403676986694, Accuracy: 1.0, Computation time: 1.4250977039337158\n",
      "Step: 4730, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.2901744842529297\n",
      "Step: 4731, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 1.2849092483520508\n",
      "Step: 4732, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 1.0503277778625488\n",
      "Step: 4733, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 1.090714931488037\n",
      "Step: 4734, Loss: 0.9159353971481323, Accuracy: 1.0, Computation time: 1.3093249797821045\n",
      "Step: 4735, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.227236270904541\n",
      "Step: 4736, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 1.2956254482269287\n",
      "Step: 4737, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 1.0417792797088623\n",
      "Step: 4738, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 1.5680382251739502\n",
      "Step: 4739, Loss: 0.9161443710327148, Accuracy: 1.0, Computation time: 1.3684961795806885\n",
      "Step: 4740, Loss: 0.9368757009506226, Accuracy: 0.96875, Computation time: 1.211869478225708\n",
      "Step: 4741, Loss: 0.9345225095748901, Accuracy: 0.96875, Computation time: 1.5706102848052979\n",
      "Step: 4742, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.2749054431915283\n",
      "Step: 4743, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 1.1699914932250977\n",
      "Step: 4744, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 1.0132660865783691\n",
      "Step: 4745, Loss: 0.9375674724578857, Accuracy: 0.96875, Computation time: 1.1470386981964111\n",
      "Step: 4746, Loss: 0.9159700870513916, Accuracy: 1.0, Computation time: 1.7381937503814697\n",
      "Step: 4747, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 1.1082580089569092\n",
      "Step: 4748, Loss: 0.9363606572151184, Accuracy: 0.96875, Computation time: 1.4905571937561035\n",
      "Step: 4749, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 1.0641560554504395\n",
      "Step: 4750, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.2223193645477295\n",
      "Step: 4751, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.0453863143920898\n",
      "Step: 4752, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 0.9536988735198975\n",
      "Step: 4753, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.344421148300171\n",
      "Step: 4754, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.538895606994629\n",
      "Step: 4755, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.094022274017334\n",
      "Step: 4756, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.1163334846496582\n",
      "Step: 4757, Loss: 0.9375894665718079, Accuracy: 0.96875, Computation time: 1.0699107646942139\n",
      "Step: 4758, Loss: 0.9372177124023438, Accuracy: 0.96875, Computation time: 1.4652338027954102\n",
      "Step: 4759, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.072972059249878\n",
      "Step: 4760, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.4212007522583008\n",
      "Step: 4761, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.0715723037719727\n",
      "Step: 4762, Loss: 0.9159275889396667, Accuracy: 1.0, Computation time: 0.9129059314727783\n",
      "Step: 4763, Loss: 0.9162351489067078, Accuracy: 1.0, Computation time: 1.9697661399841309\n",
      "Step: 4764, Loss: 0.9375792145729065, Accuracy: 0.96875, Computation time: 1.6772797107696533\n",
      "Step: 4765, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 1.0469233989715576\n",
      "Step: 4766, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.0894196033477783\n",
      "Step: 4767, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 1.0716400146484375\n",
      "Step: 4768, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.032118558883667\n",
      "Step: 4769, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.080535650253296\n",
      "Step: 4770, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 1.2167267799377441\n",
      "Step: 4771, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.217574119567871\n",
      "Step: 4772, Loss: 0.9254599809646606, Accuracy: 1.0, Computation time: 1.3699438571929932\n",
      "Step: 4773, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.2264161109924316\n",
      "Step: 4774, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.2247519493103027\n",
      "Step: 4775, Loss: 0.9159428477287292, Accuracy: 1.0, Computation time: 1.2499072551727295\n",
      "Step: 4776, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 1.1530559062957764\n",
      "Step: 4777, Loss: 0.9376057386398315, Accuracy: 0.96875, Computation time: 1.171595811843872\n",
      "Step: 4778, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.1282892227172852\n",
      "Step: 4779, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 1.2627825736999512\n",
      "Step: 4780, Loss: 0.9187508821487427, Accuracy: 1.0, Computation time: 1.6247708797454834\n",
      "Step: 4781, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 0.9829857349395752\n",
      "Step: 4782, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.0455868244171143\n",
      "Step: 4783, Loss: 0.9593032002449036, Accuracy: 0.9375, Computation time: 1.063084363937378\n",
      "Step: 4784, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 1.025644302368164\n",
      "Step: 4785, Loss: 0.9165477156639099, Accuracy: 1.0, Computation time: 1.516756534576416\n",
      "Step: 4786, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.176351547241211\n",
      "Step: 4787, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.2312345504760742\n",
      "Step: 4788, Loss: 0.9372040629386902, Accuracy: 0.96875, Computation time: 1.36997389793396\n",
      "Step: 4789, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 1.0749077796936035\n",
      "Step: 4790, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 1.480238437652588\n",
      "Step: 4791, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 1.6310234069824219\n",
      "Step: 4792, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.5868275165557861\n",
      "Step: 4793, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.5251941680908203\n",
      "Step: 4794, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.3599536418914795\n",
      "Step: 4795, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.7813310623168945\n",
      "Step: 4796, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.8019325733184814\n",
      "Step: 4797, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.4788551330566406\n",
      "Step: 4798, Loss: 0.916826069355011, Accuracy: 1.0, Computation time: 1.4630179405212402\n",
      "Step: 4799, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.4052670001983643\n",
      "Step: 4800, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.5908615589141846\n",
      "Step: 4801, Loss: 0.9159853458404541, Accuracy: 1.0, Computation time: 1.3612139225006104\n",
      "Step: 4802, Loss: 0.928662121295929, Accuracy: 0.96875, Computation time: 1.4224135875701904\n",
      "Step: 4803, Loss: 0.9378393888473511, Accuracy: 0.96875, Computation time: 1.816516637802124\n",
      "Step: 4804, Loss: 0.91600501537323, Accuracy: 1.0, Computation time: 2.314669132232666\n",
      "Step: 4805, Loss: 0.916056752204895, Accuracy: 1.0, Computation time: 1.4548323154449463\n",
      "Step: 4806, Loss: 0.9160153269767761, Accuracy: 1.0, Computation time: 1.4439852237701416\n",
      "Step: 4807, Loss: 0.9163371324539185, Accuracy: 1.0, Computation time: 1.5332810878753662\n",
      "Step: 4808, Loss: 0.9159585237503052, Accuracy: 1.0, Computation time: 1.7345895767211914\n",
      "Step: 4809, Loss: 0.9159936308860779, Accuracy: 1.0, Computation time: 1.5123164653778076\n",
      "Step: 4810, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.186702013015747\n",
      "Step: 4811, Loss: 0.9160927534103394, Accuracy: 1.0, Computation time: 1.7556214332580566\n",
      "Step: 4812, Loss: 0.9161531329154968, Accuracy: 1.0, Computation time: 1.3698551654815674\n",
      "Step: 4813, Loss: 0.916134774684906, Accuracy: 1.0, Computation time: 1.4251775741577148\n",
      "Step: 4814, Loss: 0.9377301931381226, Accuracy: 0.96875, Computation time: 1.399775743484497\n",
      "Step: 4815, Loss: 0.9159343838691711, Accuracy: 1.0, Computation time: 1.2747244834899902\n",
      "Step: 4816, Loss: 0.9371175765991211, Accuracy: 0.96875, Computation time: 1.7288792133331299\n",
      "Step: 4817, Loss: 0.9382269978523254, Accuracy: 0.96875, Computation time: 1.2585570812225342\n",
      "Step: 4818, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 1.897522211074829\n",
      "Step: 4819, Loss: 0.9164601564407349, Accuracy: 1.0, Computation time: 1.3877415657043457\n",
      "Step: 4820, Loss: 0.9203639626502991, Accuracy: 1.0, Computation time: 1.4804754257202148\n",
      "Step: 4821, Loss: 0.9170867800712585, Accuracy: 1.0, Computation time: 1.101142168045044\n",
      "Step: 4822, Loss: 0.9164125323295593, Accuracy: 1.0, Computation time: 1.121229648590088\n",
      "Step: 4823, Loss: 0.9169938564300537, Accuracy: 1.0, Computation time: 1.6479771137237549\n",
      "Step: 4824, Loss: 0.9165134429931641, Accuracy: 1.0, Computation time: 1.050689458847046\n",
      "Step: 4825, Loss: 0.9166860580444336, Accuracy: 1.0, Computation time: 1.1174836158752441\n",
      "Step: 4826, Loss: 0.9160582423210144, Accuracy: 1.0, Computation time: 1.209435224533081\n",
      "Step: 4827, Loss: 0.9223197102546692, Accuracy: 1.0, Computation time: 1.701192855834961\n",
      "Step: 4828, Loss: 0.9162454009056091, Accuracy: 1.0, Computation time: 1.4884376525878906\n",
      "Step: 4829, Loss: 0.9238905310630798, Accuracy: 1.0, Computation time: 1.4438726902008057\n",
      "Step: 4830, Loss: 0.9162409901618958, Accuracy: 1.0, Computation time: 1.2161109447479248\n",
      "Step: 4831, Loss: 0.9161678552627563, Accuracy: 1.0, Computation time: 1.4068124294281006\n",
      "Step: 4832, Loss: 0.9161455631256104, Accuracy: 1.0, Computation time: 1.4789636135101318\n",
      "Step: 4833, Loss: 0.9160645604133606, Accuracy: 1.0, Computation time: 0.9342224597930908\n",
      "Step: 4834, Loss: 0.9160665273666382, Accuracy: 1.0, Computation time: 0.9370033740997314\n",
      "Step: 4835, Loss: 0.9160544872283936, Accuracy: 1.0, Computation time: 1.3732521533966064\n",
      "Step: 4836, Loss: 0.9384872913360596, Accuracy: 0.96875, Computation time: 1.1502106189727783\n",
      "Step: 4837, Loss: 0.9186294078826904, Accuracy: 1.0, Computation time: 0.9682776927947998\n",
      "Step: 4838, Loss: 0.9369955658912659, Accuracy: 0.96875, Computation time: 1.1041369438171387\n",
      "Step: 4839, Loss: 0.9161831140518188, Accuracy: 1.0, Computation time: 1.0631041526794434\n",
      "Step: 4840, Loss: 0.9161786437034607, Accuracy: 1.0, Computation time: 1.049616813659668\n",
      "Step: 4841, Loss: 0.916165292263031, Accuracy: 1.0, Computation time: 0.9772067070007324\n",
      "Step: 4842, Loss: 0.916079044342041, Accuracy: 1.0, Computation time: 0.9591372013092041\n",
      "Step: 4843, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 0.9630374908447266\n",
      "Step: 4844, Loss: 0.9336186647415161, Accuracy: 0.96875, Computation time: 1.083390474319458\n",
      "Step: 4845, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 0.9391167163848877\n",
      "Step: 4846, Loss: 0.9336512088775635, Accuracy: 0.96875, Computation time: 0.9853973388671875\n",
      "Step: 4847, Loss: 0.9161853194236755, Accuracy: 1.0, Computation time: 1.0106120109558105\n",
      "Step: 4848, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 1.1161792278289795\n",
      "Step: 4849, Loss: 0.9159835577011108, Accuracy: 1.0, Computation time: 1.0824439525604248\n",
      "Step: 4850, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 0.99452805519104\n",
      "Step: 4851, Loss: 0.9159396886825562, Accuracy: 1.0, Computation time: 0.8686947822570801\n",
      "Step: 4852, Loss: 0.9326854348182678, Accuracy: 0.96875, Computation time: 1.2181241512298584\n",
      "Step: 4853, Loss: 0.937674343585968, Accuracy: 0.96875, Computation time: 1.4212391376495361\n",
      "Step: 4854, Loss: 0.9159780144691467, Accuracy: 1.0, Computation time: 0.9818952083587646\n",
      "Step: 4855, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 0.8876872062683105\n",
      "Step: 4856, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 0.9939529895782471\n",
      "Step: 4857, Loss: 0.9159358143806458, Accuracy: 1.0, Computation time: 0.8138926029205322\n",
      "Step: 4858, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 0.828418493270874\n",
      "Step: 4859, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 0.8316330909729004\n",
      "Step: 4860, Loss: 0.9159740805625916, Accuracy: 1.0, Computation time: 0.8730111122131348\n",
      "Step: 4861, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 0.8812985420227051\n",
      "Step: 4862, Loss: 0.9160903692245483, Accuracy: 1.0, Computation time: 0.7772235870361328\n",
      "Step: 4863, Loss: 0.9181403517723083, Accuracy: 1.0, Computation time: 1.419914960861206\n",
      "########################\n",
      "Test loss: 1.1154817342758179, Test Accuracy_epoch35: 0.7124423980712891\n",
      "########################\n",
      "Step: 4864, Loss: 0.9173137545585632, Accuracy: 1.0, Computation time: 0.9368982315063477\n",
      "Step: 4865, Loss: 0.915917694568634, Accuracy: 1.0, Computation time: 0.8154149055480957\n",
      "Step: 4866, Loss: 0.9159748554229736, Accuracy: 1.0, Computation time: 0.7394704818725586\n",
      "Step: 4867, Loss: 0.9160454869270325, Accuracy: 1.0, Computation time: 0.8637967109680176\n",
      "Step: 4868, Loss: 0.9219177961349487, Accuracy: 1.0, Computation time: 1.6223981380462646\n",
      "Step: 4869, Loss: 0.9162013530731201, Accuracy: 1.0, Computation time: 0.8949813842773438\n",
      "Step: 4870, Loss: 0.9160395860671997, Accuracy: 1.0, Computation time: 1.2282233238220215\n",
      "Step: 4871, Loss: 0.9162766337394714, Accuracy: 1.0, Computation time: 0.764625072479248\n",
      "Step: 4872, Loss: 0.9160058498382568, Accuracy: 1.0, Computation time: 0.8305196762084961\n",
      "Step: 4873, Loss: 0.916098952293396, Accuracy: 1.0, Computation time: 0.7770764827728271\n",
      "Step: 4874, Loss: 0.9172253012657166, Accuracy: 1.0, Computation time: 1.1402184963226318\n",
      "Step: 4875, Loss: 0.9402900338172913, Accuracy: 0.96875, Computation time: 0.8138785362243652\n",
      "Step: 4876, Loss: 0.9159653782844543, Accuracy: 1.0, Computation time: 0.7978427410125732\n",
      "Step: 4877, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 0.9152498245239258\n",
      "Step: 4878, Loss: 0.9160486459732056, Accuracy: 1.0, Computation time: 0.72245192527771\n",
      "Step: 4879, Loss: 0.9425745010375977, Accuracy: 0.96875, Computation time: 1.0214219093322754\n",
      "Step: 4880, Loss: 0.9164353013038635, Accuracy: 1.0, Computation time: 1.0421760082244873\n",
      "Step: 4881, Loss: 0.916084349155426, Accuracy: 1.0, Computation time: 0.8254632949829102\n",
      "Step: 4882, Loss: 0.9162545204162598, Accuracy: 1.0, Computation time: 1.1899566650390625\n",
      "Step: 4883, Loss: 0.9160544872283936, Accuracy: 1.0, Computation time: 0.9703588485717773\n",
      "Step: 4884, Loss: 0.9160773754119873, Accuracy: 1.0, Computation time: 0.7883467674255371\n",
      "Step: 4885, Loss: 0.9167009592056274, Accuracy: 1.0, Computation time: 0.9765427112579346\n",
      "Step: 4886, Loss: 0.9592592716217041, Accuracy: 0.9375, Computation time: 1.0926923751831055\n",
      "Step: 4887, Loss: 0.9248010516166687, Accuracy: 1.0, Computation time: 1.2777512073516846\n",
      "Step: 4888, Loss: 0.9173529148101807, Accuracy: 1.0, Computation time: 0.8961107730865479\n",
      "Step: 4889, Loss: 0.9175130724906921, Accuracy: 1.0, Computation time: 1.2020328044891357\n",
      "Step: 4890, Loss: 0.9167795181274414, Accuracy: 1.0, Computation time: 1.0426859855651855\n",
      "Step: 4891, Loss: 0.9167751669883728, Accuracy: 1.0, Computation time: 1.0899147987365723\n",
      "Step: 4892, Loss: 0.9163862466812134, Accuracy: 1.0, Computation time: 1.068840503692627\n",
      "Step: 4893, Loss: 0.916130006313324, Accuracy: 1.0, Computation time: 1.056330919265747\n",
      "Step: 4894, Loss: 0.9180243015289307, Accuracy: 1.0, Computation time: 1.1864516735076904\n",
      "Step: 4895, Loss: 0.9160866737365723, Accuracy: 1.0, Computation time: 1.258188009262085\n",
      "Step: 4896, Loss: 0.9164353609085083, Accuracy: 1.0, Computation time: 1.4959752559661865\n",
      "Step: 4897, Loss: 0.9160997271537781, Accuracy: 1.0, Computation time: 1.2340443134307861\n",
      "Step: 4898, Loss: 0.9162898063659668, Accuracy: 1.0, Computation time: 1.0546228885650635\n",
      "Step: 4899, Loss: 0.9165013432502747, Accuracy: 1.0, Computation time: 1.1589069366455078\n",
      "Step: 4900, Loss: 0.9163036942481995, Accuracy: 1.0, Computation time: 1.3619143962860107\n",
      "Step: 4901, Loss: 0.916168212890625, Accuracy: 1.0, Computation time: 1.2298057079315186\n",
      "Step: 4902, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 1.297175407409668\n",
      "Step: 4903, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 1.2577581405639648\n",
      "Step: 4904, Loss: 0.9160028100013733, Accuracy: 1.0, Computation time: 1.3726153373718262\n",
      "Step: 4905, Loss: 0.9161174893379211, Accuracy: 1.0, Computation time: 1.13726806640625\n",
      "Step: 4906, Loss: 0.9160729646682739, Accuracy: 1.0, Computation time: 1.436410665512085\n",
      "Step: 4907, Loss: 0.9163579344749451, Accuracy: 1.0, Computation time: 1.2843120098114014\n",
      "Step: 4908, Loss: 0.9358993768692017, Accuracy: 0.96875, Computation time: 1.4262504577636719\n",
      "Step: 4909, Loss: 0.9374297261238098, Accuracy: 0.96875, Computation time: 1.2186000347137451\n",
      "Step: 4910, Loss: 0.916060745716095, Accuracy: 1.0, Computation time: 1.4701826572418213\n",
      "Step: 4911, Loss: 0.9159939885139465, Accuracy: 1.0, Computation time: 1.4577548503875732\n",
      "Step: 4912, Loss: 0.9159976243972778, Accuracy: 1.0, Computation time: 1.686542272567749\n",
      "Step: 4913, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.3491153717041016\n",
      "Step: 4914, Loss: 0.9159199595451355, Accuracy: 1.0, Computation time: 1.2845537662506104\n",
      "Step: 4915, Loss: 0.9376577734947205, Accuracy: 0.96875, Computation time: 1.3980731964111328\n",
      "Step: 4916, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 1.3650150299072266\n",
      "Step: 4917, Loss: 0.9160516858100891, Accuracy: 1.0, Computation time: 1.6611943244934082\n",
      "Step: 4918, Loss: 0.9161053895950317, Accuracy: 1.0, Computation time: 1.378610610961914\n",
      "Step: 4919, Loss: 0.9160497784614563, Accuracy: 1.0, Computation time: 1.4794104099273682\n",
      "Step: 4920, Loss: 0.9159733057022095, Accuracy: 1.0, Computation time: 1.2146036624908447\n",
      "Step: 4921, Loss: 0.9160191416740417, Accuracy: 1.0, Computation time: 2.147271156311035\n",
      "Step: 4922, Loss: 0.91605144739151, Accuracy: 1.0, Computation time: 1.325481653213501\n",
      "Step: 4923, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.1492412090301514\n",
      "Step: 4924, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 1.1553924083709717\n",
      "Step: 4925, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.3971426486968994\n",
      "Step: 4926, Loss: 0.9161827564239502, Accuracy: 1.0, Computation time: 1.4170403480529785\n",
      "Step: 4927, Loss: 0.9216372966766357, Accuracy: 1.0, Computation time: 1.768341302871704\n",
      "Step: 4928, Loss: 0.9174991846084595, Accuracy: 1.0, Computation time: 1.3727383613586426\n",
      "Step: 4929, Loss: 0.9380059838294983, Accuracy: 0.96875, Computation time: 1.467207908630371\n",
      "Step: 4930, Loss: 0.9160455465316772, Accuracy: 1.0, Computation time: 1.151219367980957\n",
      "Step: 4931, Loss: 0.9160290360450745, Accuracy: 1.0, Computation time: 1.1344692707061768\n",
      "Step: 4932, Loss: 0.9159941673278809, Accuracy: 1.0, Computation time: 1.1941840648651123\n",
      "Step: 4933, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 1.3548684120178223\n",
      "Step: 4934, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.0689294338226318\n",
      "Step: 4935, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.136939287185669\n",
      "Step: 4936, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.102447509765625\n",
      "Step: 4937, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.020132303237915\n",
      "Step: 4938, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.2265419960021973\n",
      "Step: 4939, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 1.052424669265747\n",
      "Step: 4940, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.3452045917510986\n",
      "Step: 4941, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.321711540222168\n",
      "Step: 4942, Loss: 0.9311183094978333, Accuracy: 0.96875, Computation time: 1.0802159309387207\n",
      "Step: 4943, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 1.1814510822296143\n",
      "Step: 4944, Loss: 0.9162030816078186, Accuracy: 1.0, Computation time: 1.2847325801849365\n",
      "Step: 4945, Loss: 0.9333011507987976, Accuracy: 0.96875, Computation time: 1.2951908111572266\n",
      "Step: 4946, Loss: 0.9350959062576294, Accuracy: 0.96875, Computation time: 1.1065001487731934\n",
      "Step: 4947, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 1.072824239730835\n",
      "Step: 4948, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.5977623462677002\n",
      "Step: 4949, Loss: 0.9167047739028931, Accuracy: 1.0, Computation time: 1.3398346900939941\n",
      "Step: 4950, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 1.117851972579956\n",
      "Step: 4951, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.1172752380371094\n",
      "Step: 4952, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.0528109073638916\n",
      "Step: 4953, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 1.1376712322235107\n",
      "Step: 4954, Loss: 0.9376397132873535, Accuracy: 0.96875, Computation time: 1.0320017337799072\n",
      "Step: 4955, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 1.1591815948486328\n",
      "Step: 4956, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.2205636501312256\n",
      "Step: 4957, Loss: 0.9159008264541626, Accuracy: 1.0, Computation time: 1.2195711135864258\n",
      "Step: 4958, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 1.2279884815216064\n",
      "Step: 4959, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.1222059726715088\n",
      "Step: 4960, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.080575942993164\n",
      "Step: 4961, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.112494707107544\n",
      "Step: 4962, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.3516530990600586\n",
      "Step: 4963, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 1.4475305080413818\n",
      "Step: 4964, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 1.0643274784088135\n",
      "Step: 4965, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.203650951385498\n",
      "Step: 4966, Loss: 0.9367699027061462, Accuracy: 0.96875, Computation time: 1.6210496425628662\n",
      "Step: 4967, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 1.1007888317108154\n",
      "Step: 4968, Loss: 0.9183553457260132, Accuracy: 1.0, Computation time: 1.3304991722106934\n",
      "Step: 4969, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 1.0372028350830078\n",
      "Step: 4970, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.198312759399414\n",
      "Step: 4971, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 1.1506636142730713\n",
      "Step: 4972, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.1533234119415283\n",
      "Step: 4973, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.1460137367248535\n",
      "Step: 4974, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.1122374534606934\n",
      "Step: 4975, Loss: 0.9376106262207031, Accuracy: 0.96875, Computation time: 1.1647117137908936\n",
      "Step: 4976, Loss: 0.9159497618675232, Accuracy: 1.0, Computation time: 1.214012622833252\n",
      "Step: 4977, Loss: 0.9368626475334167, Accuracy: 0.96875, Computation time: 1.2469840049743652\n",
      "Step: 4978, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.1759846210479736\n",
      "Step: 4979, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 1.298013687133789\n",
      "Step: 4980, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.2264273166656494\n",
      "Step: 4981, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 1.1716046333312988\n",
      "Step: 4982, Loss: 0.9159647822380066, Accuracy: 1.0, Computation time: 1.1182920932769775\n",
      "Step: 4983, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.3069519996643066\n",
      "Step: 4984, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.1188488006591797\n",
      "Step: 4985, Loss: 0.9375583529472351, Accuracy: 0.96875, Computation time: 1.2044131755828857\n",
      "Step: 4986, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.360351324081421\n",
      "Step: 4987, Loss: 0.9159433841705322, Accuracy: 1.0, Computation time: 1.0793859958648682\n",
      "Step: 4988, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.090787649154663\n",
      "Step: 4989, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.2636487483978271\n",
      "Step: 4990, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.3140242099761963\n",
      "Step: 4991, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 0.9828054904937744\n",
      "Step: 4992, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.3067805767059326\n",
      "Step: 4993, Loss: 0.9376701712608337, Accuracy: 0.96875, Computation time: 1.1549036502838135\n",
      "Step: 4994, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.1630349159240723\n",
      "Step: 4995, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.2332439422607422\n",
      "Step: 4996, Loss: 0.9376013875007629, Accuracy: 0.96875, Computation time: 1.222642183303833\n",
      "Step: 4997, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.1554839611053467\n",
      "Step: 4998, Loss: 0.9160826206207275, Accuracy: 1.0, Computation time: 1.1603097915649414\n",
      "Step: 4999, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.3594861030578613\n",
      "Step: 5000, Loss: 0.9159337878227234, Accuracy: 1.0, Computation time: 1.2534749507904053\n",
      "Step: 5001, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.0956640243530273\n",
      "Step: 5002, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.096299171447754\n",
      "########################\n",
      "Test loss: 1.1238689422607422, Test Accuracy_epoch36: 0.6967741847038269\n",
      "########################\n",
      "Step: 5003, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.2879514694213867\n",
      "Step: 5004, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.1914300918579102\n",
      "Step: 5005, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 1.3210020065307617\n",
      "Step: 5006, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.1249663829803467\n",
      "Step: 5007, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.209547996520996\n",
      "Step: 5008, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.146507978439331\n",
      "Step: 5009, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.3922317028045654\n",
      "Step: 5010, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.1630017757415771\n",
      "Step: 5011, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.2829630374908447\n",
      "Step: 5012, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 1.3854138851165771\n",
      "Step: 5013, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 1.1326203346252441\n",
      "Step: 5014, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.2854323387145996\n",
      "Step: 5015, Loss: 0.9161363840103149, Accuracy: 1.0, Computation time: 1.2145226001739502\n",
      "Step: 5016, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.2139463424682617\n",
      "Step: 5017, Loss: 0.9159659147262573, Accuracy: 1.0, Computation time: 1.0890932083129883\n",
      "Step: 5018, Loss: 0.9164513945579529, Accuracy: 1.0, Computation time: 1.1326966285705566\n",
      "Step: 5019, Loss: 0.9375544190406799, Accuracy: 0.96875, Computation time: 1.3708736896514893\n",
      "Step: 5020, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.1610798835754395\n",
      "Step: 5021, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.019273042678833\n",
      "Step: 5022, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.4040648937225342\n",
      "Step: 5023, Loss: 0.926856517791748, Accuracy: 0.96875, Computation time: 1.2629778385162354\n",
      "Step: 5024, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.3426835536956787\n",
      "Step: 5025, Loss: 0.9376069903373718, Accuracy: 0.96875, Computation time: 1.4935028553009033\n",
      "Step: 5026, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 1.328467845916748\n",
      "Step: 5027, Loss: 0.9375600218772888, Accuracy: 0.96875, Computation time: 1.1754441261291504\n",
      "Step: 5028, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.1963903903961182\n",
      "Step: 5029, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.1630549430847168\n",
      "Step: 5030, Loss: 0.916962206363678, Accuracy: 1.0, Computation time: 1.1628539562225342\n",
      "Step: 5031, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.1290555000305176\n",
      "Step: 5032, Loss: 0.9365386962890625, Accuracy: 0.96875, Computation time: 1.3392438888549805\n",
      "Step: 5033, Loss: 0.9159342050552368, Accuracy: 1.0, Computation time: 1.5379383563995361\n",
      "Step: 5034, Loss: 0.9209780693054199, Accuracy: 1.0, Computation time: 1.3417134284973145\n",
      "Step: 5035, Loss: 0.9369227886199951, Accuracy: 0.96875, Computation time: 1.2260243892669678\n",
      "Step: 5036, Loss: 0.9161441922187805, Accuracy: 1.0, Computation time: 1.8425676822662354\n",
      "Step: 5037, Loss: 0.9166452884674072, Accuracy: 1.0, Computation time: 1.191828727722168\n",
      "Step: 5038, Loss: 0.93426114320755, Accuracy: 0.96875, Computation time: 1.4095416069030762\n",
      "Step: 5039, Loss: 0.9162054061889648, Accuracy: 1.0, Computation time: 1.777646541595459\n",
      "Step: 5040, Loss: 0.9375969171524048, Accuracy: 0.96875, Computation time: 1.896127700805664\n",
      "Step: 5041, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 1.234222650527954\n",
      "Step: 5042, Loss: 0.9159427285194397, Accuracy: 1.0, Computation time: 1.313673734664917\n",
      "Step: 5043, Loss: 0.9162656664848328, Accuracy: 1.0, Computation time: 1.2833430767059326\n",
      "Step: 5044, Loss: 0.9160100221633911, Accuracy: 1.0, Computation time: 1.0798218250274658\n",
      "Step: 5045, Loss: 0.9162647724151611, Accuracy: 1.0, Computation time: 1.1887001991271973\n",
      "Step: 5046, Loss: 0.9167117476463318, Accuracy: 1.0, Computation time: 1.3850033283233643\n",
      "Step: 5047, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 1.56203031539917\n",
      "Step: 5048, Loss: 0.9161243438720703, Accuracy: 1.0, Computation time: 1.3638725280761719\n",
      "Step: 5049, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.3262526988983154\n",
      "Step: 5050, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 1.0874555110931396\n",
      "Step: 5051, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.19193434715271\n",
      "Step: 5052, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 1.4471447467803955\n",
      "Step: 5053, Loss: 0.9376084804534912, Accuracy: 0.96875, Computation time: 1.413517713546753\n",
      "Step: 5054, Loss: 0.917202353477478, Accuracy: 1.0, Computation time: 1.2917015552520752\n",
      "Step: 5055, Loss: 0.9160598516464233, Accuracy: 1.0, Computation time: 1.3277876377105713\n",
      "Step: 5056, Loss: 0.9160931706428528, Accuracy: 1.0, Computation time: 1.1112079620361328\n",
      "Step: 5057, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.3219678401947021\n",
      "Step: 5058, Loss: 0.934948205947876, Accuracy: 0.96875, Computation time: 1.290543556213379\n",
      "Step: 5059, Loss: 0.9158833026885986, Accuracy: 1.0, Computation time: 1.3618507385253906\n",
      "Step: 5060, Loss: 0.9160119891166687, Accuracy: 1.0, Computation time: 1.5255496501922607\n",
      "Step: 5061, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 1.3096582889556885\n",
      "Step: 5062, Loss: 0.923115611076355, Accuracy: 1.0, Computation time: 1.2705473899841309\n",
      "Step: 5063, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.2570738792419434\n",
      "Step: 5064, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.494694709777832\n",
      "Step: 5065, Loss: 0.9343008399009705, Accuracy: 0.96875, Computation time: 1.1806056499481201\n",
      "Step: 5066, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.1696672439575195\n",
      "Step: 5067, Loss: 0.9175970554351807, Accuracy: 1.0, Computation time: 1.7439777851104736\n",
      "Step: 5068, Loss: 0.916013240814209, Accuracy: 1.0, Computation time: 1.5377275943756104\n",
      "Step: 5069, Loss: 0.9159758687019348, Accuracy: 1.0, Computation time: 1.3689393997192383\n",
      "Step: 5070, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.3522734642028809\n",
      "Step: 5071, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.4234373569488525\n",
      "Step: 5072, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.1870341300964355\n",
      "Step: 5073, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.3919486999511719\n",
      "Step: 5074, Loss: 0.9168598651885986, Accuracy: 1.0, Computation time: 1.8224983215332031\n",
      "Step: 5075, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.1281428337097168\n",
      "Step: 5076, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.118523120880127\n",
      "Step: 5077, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 1.0722987651824951\n",
      "Step: 5078, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.4708995819091797\n",
      "Step: 5079, Loss: 0.9370223879814148, Accuracy: 0.96875, Computation time: 1.2685654163360596\n",
      "Step: 5080, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.511883020401001\n",
      "Step: 5081, Loss: 0.9162312746047974, Accuracy: 1.0, Computation time: 1.4115204811096191\n",
      "Step: 5082, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.322497844696045\n",
      "Step: 5083, Loss: 0.9160003662109375, Accuracy: 1.0, Computation time: 1.2966318130493164\n",
      "Step: 5084, Loss: 0.9159653186798096, Accuracy: 1.0, Computation time: 1.7159678936004639\n",
      "Step: 5085, Loss: 0.9161635637283325, Accuracy: 1.0, Computation time: 1.5811188220977783\n",
      "Step: 5086, Loss: 0.9159723520278931, Accuracy: 1.0, Computation time: 1.4165730476379395\n",
      "Step: 5087, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 1.3475770950317383\n",
      "Step: 5088, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.2444186210632324\n",
      "Step: 5089, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.0498578548431396\n",
      "Step: 5090, Loss: 0.9366199970245361, Accuracy: 0.96875, Computation time: 1.4736509323120117\n",
      "Step: 5091, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 1.171152114868164\n",
      "Step: 5092, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.6341443061828613\n",
      "Step: 5093, Loss: 0.9159507155418396, Accuracy: 1.0, Computation time: 1.4344260692596436\n",
      "Step: 5094, Loss: 0.9159250855445862, Accuracy: 1.0, Computation time: 1.122849941253662\n",
      "Step: 5095, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.122281789779663\n",
      "Step: 5096, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.329728364944458\n",
      "Step: 5097, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.3700759410858154\n",
      "Step: 5098, Loss: 0.9160476326942444, Accuracy: 1.0, Computation time: 1.1589901447296143\n",
      "Step: 5099, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.4302213191986084\n",
      "Step: 5100, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 1.6488332748413086\n",
      "Step: 5101, Loss: 0.915941596031189, Accuracy: 1.0, Computation time: 1.2817456722259521\n",
      "Step: 5102, Loss: 0.9167293906211853, Accuracy: 1.0, Computation time: 1.2293727397918701\n",
      "Step: 5103, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 1.5460541248321533\n",
      "Step: 5104, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.6620397567749023\n",
      "Step: 5105, Loss: 0.9161756634712219, Accuracy: 1.0, Computation time: 1.7598528861999512\n",
      "Step: 5106, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.1097691059112549\n",
      "Step: 5107, Loss: 0.9372831583023071, Accuracy: 0.96875, Computation time: 1.3454465866088867\n",
      "Step: 5108, Loss: 0.9160628914833069, Accuracy: 1.0, Computation time: 1.3826580047607422\n",
      "Step: 5109, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.5526275634765625\n",
      "Step: 5110, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.361769676208496\n",
      "Step: 5111, Loss: 0.9334734082221985, Accuracy: 0.96875, Computation time: 1.4789726734161377\n",
      "Step: 5112, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.523540735244751\n",
      "Step: 5113, Loss: 0.9162114262580872, Accuracy: 1.0, Computation time: 1.3291230201721191\n",
      "Step: 5114, Loss: 0.93498295545578, Accuracy: 0.96875, Computation time: 1.363039255142212\n",
      "Step: 5115, Loss: 0.9159061312675476, Accuracy: 1.0, Computation time: 1.2026138305664062\n",
      "Step: 5116, Loss: 0.9159310460090637, Accuracy: 1.0, Computation time: 1.768373966217041\n",
      "Step: 5117, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.100388765335083\n",
      "Step: 5118, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.682941198348999\n",
      "Step: 5119, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.4797236919403076\n",
      "Step: 5120, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.347510576248169\n",
      "Step: 5121, Loss: 0.9182944893836975, Accuracy: 1.0, Computation time: 1.4989879131317139\n",
      "Step: 5122, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.411576271057129\n",
      "Step: 5123, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 1.6865286827087402\n",
      "Step: 5124, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 1.37754487991333\n",
      "Step: 5125, Loss: 0.9158453941345215, Accuracy: 1.0, Computation time: 1.1847343444824219\n",
      "Step: 5126, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 1.212386131286621\n",
      "Step: 5127, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.7683379650115967\n",
      "Step: 5128, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.5116534233093262\n",
      "Step: 5129, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 1.1079623699188232\n",
      "Step: 5130, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.4475531578063965\n",
      "Step: 5131, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.3360610008239746\n",
      "Step: 5132, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.2728497982025146\n",
      "Step: 5133, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 1.6705031394958496\n",
      "Step: 5134, Loss: 0.9160575866699219, Accuracy: 1.0, Computation time: 1.4096462726593018\n",
      "Step: 5135, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 1.334974765777588\n",
      "Step: 5136, Loss: 0.9161086678504944, Accuracy: 1.0, Computation time: 1.4697694778442383\n",
      "Step: 5137, Loss: 0.9375364184379578, Accuracy: 0.96875, Computation time: 1.520491123199463\n",
      "Step: 5138, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.1328141689300537\n",
      "Step: 5139, Loss: 0.9159579277038574, Accuracy: 1.0, Computation time: 1.3829872608184814\n",
      "Step: 5140, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.2749886512756348\n",
      "Step: 5141, Loss: 0.9162576198577881, Accuracy: 1.0, Computation time: 1.330521821975708\n",
      "########################\n",
      "Test loss: 1.1227694749832153, Test Accuracy_epoch37: 0.6976958513259888\n",
      "########################\n",
      "Step: 5142, Loss: 0.91644287109375, Accuracy: 1.0, Computation time: 1.2855677604675293\n",
      "Step: 5143, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.390573501586914\n",
      "Step: 5144, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.2254047393798828\n",
      "Step: 5145, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.249204158782959\n",
      "Step: 5146, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.3010025024414062\n",
      "Step: 5147, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.331634521484375\n",
      "Step: 5148, Loss: 0.9375561475753784, Accuracy: 0.96875, Computation time: 1.2992711067199707\n",
      "Step: 5149, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.2246811389923096\n",
      "Step: 5150, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 1.4226622581481934\n",
      "Step: 5151, Loss: 0.9375445246696472, Accuracy: 0.96875, Computation time: 1.4521338939666748\n",
      "Step: 5152, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.1635878086090088\n",
      "Step: 5153, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.0504539012908936\n",
      "Step: 5154, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.4500319957733154\n",
      "Step: 5155, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.151716947555542\n",
      "Step: 5156, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.102506399154663\n",
      "Step: 5157, Loss: 0.9158458113670349, Accuracy: 1.0, Computation time: 1.1049787998199463\n",
      "Step: 5158, Loss: 0.9158384799957275, Accuracy: 1.0, Computation time: 1.1401398181915283\n",
      "Step: 5159, Loss: 0.9158425331115723, Accuracy: 1.0, Computation time: 1.2153327465057373\n",
      "Step: 5160, Loss: 0.9375198483467102, Accuracy: 0.96875, Computation time: 1.438183307647705\n",
      "Step: 5161, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.4447214603424072\n",
      "Step: 5162, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.31231689453125\n",
      "Step: 5163, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.1534631252288818\n",
      "Step: 5164, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.6066715717315674\n",
      "Step: 5165, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.6669995784759521\n",
      "Step: 5166, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 1.3453483581542969\n",
      "Step: 5167, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.4640471935272217\n",
      "Step: 5168, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.4532949924468994\n",
      "Step: 5169, Loss: 0.9158430695533752, Accuracy: 1.0, Computation time: 0.9953358173370361\n",
      "Step: 5170, Loss: 0.9371368885040283, Accuracy: 0.96875, Computation time: 1.3351531028747559\n",
      "Step: 5171, Loss: 0.9158347845077515, Accuracy: 1.0, Computation time: 1.3241479396820068\n",
      "Step: 5172, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.4171948432922363\n",
      "Step: 5173, Loss: 0.9342758655548096, Accuracy: 0.96875, Computation time: 1.3673083782196045\n",
      "Step: 5174, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.3297560214996338\n",
      "Step: 5175, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 1.5064895153045654\n",
      "Step: 5176, Loss: 0.9159289002418518, Accuracy: 1.0, Computation time: 1.2426211833953857\n",
      "Step: 5177, Loss: 0.9159354567527771, Accuracy: 1.0, Computation time: 1.2923765182495117\n",
      "Step: 5178, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.2673428058624268\n",
      "Step: 5179, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.3580427169799805\n",
      "Step: 5180, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.2484972476959229\n",
      "Step: 5181, Loss: 0.9375413060188293, Accuracy: 0.96875, Computation time: 1.2742819786071777\n",
      "Step: 5182, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.0897243022918701\n",
      "Step: 5183, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 1.3712804317474365\n",
      "Step: 5184, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 1.3849399089813232\n",
      "Step: 5185, Loss: 0.9164974689483643, Accuracy: 1.0, Computation time: 1.4125909805297852\n",
      "Step: 5186, Loss: 0.9199000597000122, Accuracy: 1.0, Computation time: 1.2429533004760742\n",
      "Step: 5187, Loss: 0.9375436902046204, Accuracy: 0.96875, Computation time: 1.2429544925689697\n",
      "Step: 5188, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.7603263854980469\n",
      "Step: 5189, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.3641343116760254\n",
      "Step: 5190, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.1657319068908691\n",
      "Step: 5191, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.3264260292053223\n",
      "Step: 5192, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.5312590599060059\n",
      "Step: 5193, Loss: 0.924055278301239, Accuracy: 1.0, Computation time: 1.3717694282531738\n",
      "Step: 5194, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.2492578029632568\n",
      "Step: 5195, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.2155675888061523\n",
      "Step: 5196, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.6864628791809082\n",
      "Step: 5197, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.3402724266052246\n",
      "Step: 5198, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.465670108795166\n",
      "Step: 5199, Loss: 0.9161257743835449, Accuracy: 1.0, Computation time: 1.300248146057129\n",
      "Step: 5200, Loss: 0.9159663319587708, Accuracy: 1.0, Computation time: 1.230837345123291\n",
      "Step: 5201, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.1221599578857422\n",
      "Step: 5202, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.10258150100708\n",
      "Step: 5203, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 1.1523456573486328\n",
      "Step: 5204, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.2712793350219727\n",
      "Step: 5205, Loss: 0.9352145195007324, Accuracy: 0.96875, Computation time: 1.400200605392456\n",
      "Step: 5206, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.3959381580352783\n",
      "Step: 5207, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.2386536598205566\n",
      "Step: 5208, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.1495330333709717\n",
      "Step: 5209, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 0.9895486831665039\n",
      "Step: 5210, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.1540164947509766\n",
      "Step: 5211, Loss: 0.9372080564498901, Accuracy: 0.96875, Computation time: 1.2438416481018066\n",
      "Step: 5212, Loss: 0.9164024591445923, Accuracy: 1.0, Computation time: 1.2616233825683594\n",
      "Step: 5213, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.3894600868225098\n",
      "Step: 5214, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.2702445983886719\n",
      "Step: 5215, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 0.9961719512939453\n",
      "Step: 5216, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.2069387435913086\n",
      "Step: 5217, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 1.2043890953063965\n",
      "Step: 5218, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.4233810901641846\n",
      "Step: 5219, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.256917953491211\n",
      "Step: 5220, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.6404411792755127\n",
      "Step: 5221, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.225832462310791\n",
      "Step: 5222, Loss: 0.9258133769035339, Accuracy: 0.96875, Computation time: 3.878061056137085\n",
      "Step: 5223, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 1.1665339469909668\n",
      "Step: 5224, Loss: 0.9160736799240112, Accuracy: 1.0, Computation time: 1.1415846347808838\n",
      "Step: 5225, Loss: 0.9165856242179871, Accuracy: 1.0, Computation time: 1.1157200336456299\n",
      "Step: 5226, Loss: 0.9163902997970581, Accuracy: 1.0, Computation time: 1.2552080154418945\n",
      "Step: 5227, Loss: 0.9162409901618958, Accuracy: 1.0, Computation time: 1.1662814617156982\n",
      "Step: 5228, Loss: 0.9160671830177307, Accuracy: 1.0, Computation time: 1.337449550628662\n",
      "Step: 5229, Loss: 0.9193521738052368, Accuracy: 1.0, Computation time: 1.0353426933288574\n",
      "Step: 5230, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 1.008558750152588\n",
      "Step: 5231, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.1542375087738037\n",
      "Step: 5232, Loss: 0.9160124063491821, Accuracy: 1.0, Computation time: 1.2633562088012695\n",
      "Step: 5233, Loss: 0.9162029027938843, Accuracy: 1.0, Computation time: 1.0796113014221191\n",
      "Step: 5234, Loss: 0.9163017868995667, Accuracy: 1.0, Computation time: 1.0587291717529297\n",
      "Step: 5235, Loss: 0.9162224531173706, Accuracy: 1.0, Computation time: 1.279128074645996\n",
      "Step: 5236, Loss: 0.9164679050445557, Accuracy: 1.0, Computation time: 1.0521953105926514\n",
      "Step: 5237, Loss: 0.9162715077400208, Accuracy: 1.0, Computation time: 1.0932912826538086\n",
      "Step: 5238, Loss: 0.9159463047981262, Accuracy: 1.0, Computation time: 1.1579279899597168\n",
      "Step: 5239, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.0394153594970703\n",
      "Step: 5240, Loss: 0.9168971180915833, Accuracy: 1.0, Computation time: 1.0413308143615723\n",
      "Step: 5241, Loss: 0.9160666465759277, Accuracy: 1.0, Computation time: 0.9543490409851074\n",
      "Step: 5242, Loss: 0.9385966062545776, Accuracy: 0.96875, Computation time: 1.0802929401397705\n",
      "Step: 5243, Loss: 0.9377375841140747, Accuracy: 0.96875, Computation time: 1.0500953197479248\n",
      "Step: 5244, Loss: 0.9220179915428162, Accuracy: 1.0, Computation time: 1.1411263942718506\n",
      "Step: 5245, Loss: 0.9160733819007874, Accuracy: 1.0, Computation time: 0.9894659519195557\n",
      "Step: 5246, Loss: 0.9162523746490479, Accuracy: 1.0, Computation time: 1.272089958190918\n",
      "Step: 5247, Loss: 0.9162488579750061, Accuracy: 1.0, Computation time: 1.029461145401001\n",
      "Step: 5248, Loss: 0.91611248254776, Accuracy: 1.0, Computation time: 1.2295958995819092\n",
      "Step: 5249, Loss: 0.9162025451660156, Accuracy: 1.0, Computation time: 1.4929826259613037\n",
      "Step: 5250, Loss: 0.9184235334396362, Accuracy: 1.0, Computation time: 1.3935260772705078\n",
      "Step: 5251, Loss: 0.9378190040588379, Accuracy: 0.96875, Computation time: 1.1458137035369873\n",
      "Step: 5252, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 0.9732038974761963\n",
      "Step: 5253, Loss: 0.917307436466217, Accuracy: 1.0, Computation time: 1.3170497417449951\n",
      "Step: 5254, Loss: 0.9161452054977417, Accuracy: 1.0, Computation time: 0.9406895637512207\n",
      "Step: 5255, Loss: 0.9161226749420166, Accuracy: 1.0, Computation time: 1.0640466213226318\n",
      "Step: 5256, Loss: 0.9372552037239075, Accuracy: 0.96875, Computation time: 1.235506534576416\n",
      "Step: 5257, Loss: 0.9160974621772766, Accuracy: 1.0, Computation time: 1.3479602336883545\n",
      "Step: 5258, Loss: 0.9160898923873901, Accuracy: 1.0, Computation time: 1.020719289779663\n",
      "Step: 5259, Loss: 0.9160217642784119, Accuracy: 1.0, Computation time: 1.0503647327423096\n",
      "Step: 5260, Loss: 0.9376144409179688, Accuracy: 0.96875, Computation time: 0.927635669708252\n",
      "Step: 5261, Loss: 0.9163603782653809, Accuracy: 1.0, Computation time: 1.149221658706665\n",
      "Step: 5262, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 1.0117909908294678\n",
      "Step: 5263, Loss: 0.9159179329872131, Accuracy: 1.0, Computation time: 1.126049280166626\n",
      "Step: 5264, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 1.175476312637329\n",
      "Step: 5265, Loss: 0.9377284646034241, Accuracy: 0.96875, Computation time: 1.0648932456970215\n",
      "Step: 5266, Loss: 0.9174016118049622, Accuracy: 1.0, Computation time: 1.1107900142669678\n",
      "Step: 5267, Loss: 0.9160382747650146, Accuracy: 1.0, Computation time: 0.931617259979248\n",
      "Step: 5268, Loss: 0.916050910949707, Accuracy: 1.0, Computation time: 0.9241182804107666\n",
      "Step: 5269, Loss: 0.916008472442627, Accuracy: 1.0, Computation time: 0.9683735370635986\n",
      "Step: 5270, Loss: 0.9229827523231506, Accuracy: 1.0, Computation time: 1.0228900909423828\n",
      "Step: 5271, Loss: 0.9323703646659851, Accuracy: 0.96875, Computation time: 0.9232068061828613\n",
      "Step: 5272, Loss: 0.9161760210990906, Accuracy: 1.0, Computation time: 1.009953260421753\n",
      "Step: 5273, Loss: 0.9379637837409973, Accuracy: 0.96875, Computation time: 1.1498594284057617\n",
      "Step: 5274, Loss: 0.9163584113121033, Accuracy: 1.0, Computation time: 0.9303560256958008\n",
      "Step: 5275, Loss: 0.9173650741577148, Accuracy: 1.0, Computation time: 1.147784948348999\n",
      "Step: 5276, Loss: 0.9167490005493164, Accuracy: 1.0, Computation time: 1.006389856338501\n",
      "Step: 5277, Loss: 0.9161089658737183, Accuracy: 1.0, Computation time: 0.9747812747955322\n",
      "Step: 5278, Loss: 0.9161575436592102, Accuracy: 1.0, Computation time: 1.199831247329712\n",
      "Step: 5279, Loss: 0.9167193174362183, Accuracy: 1.0, Computation time: 0.9643480777740479\n",
      "Step: 5280, Loss: 0.9166221618652344, Accuracy: 1.0, Computation time: 0.9372813701629639\n",
      "########################\n",
      "Test loss: 1.1204283237457275, Test Accuracy_epoch38: 0.7050691246986389\n",
      "########################\n",
      "Step: 5281, Loss: 0.9167702198028564, Accuracy: 1.0, Computation time: 1.029327392578125\n",
      "Step: 5282, Loss: 0.9199002385139465, Accuracy: 1.0, Computation time: 1.5830235481262207\n",
      "Step: 5283, Loss: 0.9166576862335205, Accuracy: 1.0, Computation time: 1.1302132606506348\n",
      "Step: 5284, Loss: 0.9161382913589478, Accuracy: 1.0, Computation time: 1.1419246196746826\n",
      "Step: 5285, Loss: 0.9163576364517212, Accuracy: 1.0, Computation time: 1.1202621459960938\n",
      "Step: 5286, Loss: 0.9198702573776245, Accuracy: 1.0, Computation time: 1.1466844081878662\n",
      "Step: 5287, Loss: 0.9162363409996033, Accuracy: 1.0, Computation time: 1.1802787780761719\n",
      "Step: 5288, Loss: 0.917507529258728, Accuracy: 1.0, Computation time: 1.1537694931030273\n",
      "Step: 5289, Loss: 0.9170589447021484, Accuracy: 1.0, Computation time: 1.4283208847045898\n",
      "Step: 5290, Loss: 0.916311502456665, Accuracy: 1.0, Computation time: 1.0775928497314453\n",
      "Step: 5291, Loss: 0.9164448976516724, Accuracy: 1.0, Computation time: 1.0873939990997314\n",
      "Step: 5292, Loss: 0.9160689115524292, Accuracy: 1.0, Computation time: 1.2890598773956299\n",
      "Step: 5293, Loss: 0.9163757562637329, Accuracy: 1.0, Computation time: 1.5358953475952148\n",
      "Step: 5294, Loss: 0.9163177013397217, Accuracy: 1.0, Computation time: 0.9327411651611328\n",
      "Step: 5295, Loss: 0.9381399154663086, Accuracy: 0.96875, Computation time: 1.1538307666778564\n",
      "Step: 5296, Loss: 0.9164838790893555, Accuracy: 1.0, Computation time: 0.9146277904510498\n",
      "Step: 5297, Loss: 0.9162197709083557, Accuracy: 1.0, Computation time: 1.0112366676330566\n",
      "Step: 5298, Loss: 0.9161131978034973, Accuracy: 1.0, Computation time: 1.1298868656158447\n",
      "Step: 5299, Loss: 0.916117250919342, Accuracy: 1.0, Computation time: 1.0212044715881348\n",
      "Step: 5300, Loss: 0.9375054836273193, Accuracy: 0.96875, Computation time: 1.85923433303833\n",
      "Step: 5301, Loss: 0.9161426424980164, Accuracy: 1.0, Computation time: 1.1739509105682373\n",
      "Step: 5302, Loss: 0.9163942933082581, Accuracy: 1.0, Computation time: 1.1405372619628906\n",
      "Step: 5303, Loss: 0.9371512532234192, Accuracy: 0.96875, Computation time: 1.1708223819732666\n",
      "Step: 5304, Loss: 0.9163005352020264, Accuracy: 1.0, Computation time: 1.0786466598510742\n",
      "Step: 5305, Loss: 0.9161052107810974, Accuracy: 1.0, Computation time: 1.1097726821899414\n",
      "Step: 5306, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 1.2665221691131592\n",
      "Step: 5307, Loss: 0.9163024425506592, Accuracy: 1.0, Computation time: 1.1139259338378906\n",
      "Step: 5308, Loss: 0.9162073731422424, Accuracy: 1.0, Computation time: 0.8828837871551514\n",
      "Step: 5309, Loss: 0.9302096962928772, Accuracy: 0.96875, Computation time: 1.0243980884552002\n",
      "Step: 5310, Loss: 0.9161709547042847, Accuracy: 1.0, Computation time: 1.0019540786743164\n",
      "Step: 5311, Loss: 0.916212797164917, Accuracy: 1.0, Computation time: 1.3394477367401123\n",
      "Step: 5312, Loss: 0.9185553193092346, Accuracy: 1.0, Computation time: 1.2000916004180908\n",
      "Step: 5313, Loss: 0.9161568880081177, Accuracy: 1.0, Computation time: 1.4889659881591797\n",
      "Step: 5314, Loss: 0.9161319732666016, Accuracy: 1.0, Computation time: 1.2147550582885742\n",
      "Step: 5315, Loss: 0.9160378575325012, Accuracy: 1.0, Computation time: 1.3527934551239014\n",
      "Step: 5316, Loss: 0.9161261916160583, Accuracy: 1.0, Computation time: 1.2723536491394043\n",
      "Step: 5317, Loss: 0.9162478446960449, Accuracy: 1.0, Computation time: 1.236093521118164\n",
      "Step: 5318, Loss: 0.938068687915802, Accuracy: 0.96875, Computation time: 1.1961979866027832\n",
      "Step: 5319, Loss: 0.91620934009552, Accuracy: 1.0, Computation time: 1.010420560836792\n",
      "Step: 5320, Loss: 0.9160536527633667, Accuracy: 1.0, Computation time: 0.9596700668334961\n",
      "Step: 5321, Loss: 0.9343132972717285, Accuracy: 0.96875, Computation time: 1.03782057762146\n",
      "Step: 5322, Loss: 0.9159228801727295, Accuracy: 1.0, Computation time: 0.8428041934967041\n",
      "Step: 5323, Loss: 0.9343109130859375, Accuracy: 0.96875, Computation time: 1.4177336692810059\n",
      "Step: 5324, Loss: 0.9160148501396179, Accuracy: 1.0, Computation time: 1.274810552597046\n",
      "Step: 5325, Loss: 0.9162497520446777, Accuracy: 1.0, Computation time: 0.9250166416168213\n",
      "Step: 5326, Loss: 0.9166396856307983, Accuracy: 1.0, Computation time: 1.0241692066192627\n",
      "Step: 5327, Loss: 0.9161394238471985, Accuracy: 1.0, Computation time: 1.06931471824646\n",
      "Step: 5328, Loss: 0.916096031665802, Accuracy: 1.0, Computation time: 1.1577496528625488\n",
      "Step: 5329, Loss: 0.9159622192382812, Accuracy: 1.0, Computation time: 1.1322557926177979\n",
      "Step: 5330, Loss: 0.9173479080200195, Accuracy: 1.0, Computation time: 1.0841095447540283\n",
      "Step: 5331, Loss: 0.915969729423523, Accuracy: 1.0, Computation time: 1.4084627628326416\n",
      "Step: 5332, Loss: 0.9202314019203186, Accuracy: 1.0, Computation time: 1.1377975940704346\n",
      "Step: 5333, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.0799205303192139\n",
      "Step: 5334, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 1.1991937160491943\n",
      "Step: 5335, Loss: 0.9159491062164307, Accuracy: 1.0, Computation time: 0.8378746509552002\n",
      "Step: 5336, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.2900140285491943\n",
      "Step: 5337, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 1.497347354888916\n",
      "Step: 5338, Loss: 0.9163513779640198, Accuracy: 1.0, Computation time: 0.9435009956359863\n",
      "Step: 5339, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.1036772727966309\n",
      "Step: 5340, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.3546302318572998\n",
      "Step: 5341, Loss: 0.9165166020393372, Accuracy: 1.0, Computation time: 1.1894350051879883\n",
      "Step: 5342, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.1496219635009766\n",
      "Step: 5343, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 0.8434638977050781\n",
      "Step: 5344, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.1405353546142578\n",
      "Step: 5345, Loss: 0.9376564025878906, Accuracy: 0.96875, Computation time: 1.2012739181518555\n",
      "Step: 5346, Loss: 0.9278728365898132, Accuracy: 0.96875, Computation time: 1.4179751873016357\n",
      "Step: 5347, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.1038858890533447\n",
      "Step: 5348, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.279926061630249\n",
      "Step: 5349, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.1184296607971191\n",
      "Step: 5350, Loss: 0.9170401096343994, Accuracy: 1.0, Computation time: 1.0406229496002197\n",
      "Step: 5351, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.1086649894714355\n",
      "Step: 5352, Loss: 0.9159570932388306, Accuracy: 1.0, Computation time: 1.0407049655914307\n",
      "Step: 5353, Loss: 0.9163762331008911, Accuracy: 1.0, Computation time: 1.4201927185058594\n",
      "Step: 5354, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 0.9802029132843018\n",
      "Step: 5355, Loss: 0.9376006722450256, Accuracy: 0.96875, Computation time: 1.26674485206604\n",
      "Step: 5356, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.0811448097229004\n",
      "Step: 5357, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.2719781398773193\n",
      "Step: 5358, Loss: 0.9159191250801086, Accuracy: 1.0, Computation time: 1.1231746673583984\n",
      "Step: 5359, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 0.9107537269592285\n",
      "Step: 5360, Loss: 0.9162483215332031, Accuracy: 1.0, Computation time: 1.1872799396514893\n",
      "Step: 5361, Loss: 0.9166997671127319, Accuracy: 1.0, Computation time: 1.0658190250396729\n",
      "Step: 5362, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.2409753799438477\n",
      "Step: 5363, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 1.367690086364746\n",
      "Step: 5364, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.0904526710510254\n",
      "Step: 5365, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 1.3462607860565186\n",
      "Step: 5366, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.6174640655517578\n",
      "Step: 5367, Loss: 0.9167648553848267, Accuracy: 1.0, Computation time: 1.1060259342193604\n",
      "Step: 5368, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.077789068222046\n",
      "Step: 5369, Loss: 0.9356246590614319, Accuracy: 0.96875, Computation time: 1.6186060905456543\n",
      "Step: 5370, Loss: 0.9161131381988525, Accuracy: 1.0, Computation time: 1.213965654373169\n",
      "Step: 5371, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.1373624801635742\n",
      "Step: 5372, Loss: 0.9161180853843689, Accuracy: 1.0, Computation time: 1.1342995166778564\n",
      "Step: 5373, Loss: 0.915931224822998, Accuracy: 1.0, Computation time: 1.1504287719726562\n",
      "Step: 5374, Loss: 0.9374509453773499, Accuracy: 0.96875, Computation time: 1.2105731964111328\n",
      "Step: 5375, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.0358755588531494\n",
      "Step: 5376, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 0.9829604625701904\n",
      "Step: 5377, Loss: 0.9162282347679138, Accuracy: 1.0, Computation time: 1.4804117679595947\n",
      "Step: 5378, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.1489105224609375\n",
      "Step: 5379, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.15339994430542\n",
      "Step: 5380, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.2840454578399658\n",
      "Step: 5381, Loss: 0.9382295608520508, Accuracy: 0.96875, Computation time: 1.1201932430267334\n",
      "Step: 5382, Loss: 0.9159592390060425, Accuracy: 1.0, Computation time: 1.111738681793213\n",
      "Step: 5383, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.1564576625823975\n",
      "Step: 5384, Loss: 0.937545657157898, Accuracy: 0.96875, Computation time: 1.245915412902832\n",
      "Step: 5385, Loss: 0.9163258075714111, Accuracy: 1.0, Computation time: 1.4234528541564941\n",
      "Step: 5386, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.135664463043213\n",
      "Step: 5387, Loss: 0.9168682098388672, Accuracy: 1.0, Computation time: 1.498992681503296\n",
      "Step: 5388, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 1.2190539836883545\n",
      "Step: 5389, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.1162147521972656\n",
      "Step: 5390, Loss: 0.9161915183067322, Accuracy: 1.0, Computation time: 1.1892869472503662\n",
      "Step: 5391, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.021211862564087\n",
      "Step: 5392, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.2531650066375732\n",
      "Step: 5393, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.3597126007080078\n",
      "Step: 5394, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.0852065086364746\n",
      "Step: 5395, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.3286266326904297\n",
      "Step: 5396, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.3063106536865234\n",
      "Step: 5397, Loss: 0.9377480745315552, Accuracy: 0.96875, Computation time: 1.45524263381958\n",
      "Step: 5398, Loss: 0.9158888459205627, Accuracy: 1.0, Computation time: 1.2445721626281738\n",
      "Step: 5399, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 1.0522935390472412\n",
      "Step: 5400, Loss: 0.9159325957298279, Accuracy: 1.0, Computation time: 1.0007498264312744\n",
      "Step: 5401, Loss: 0.9375195503234863, Accuracy: 0.96875, Computation time: 1.0112929344177246\n",
      "Step: 5402, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.3719682693481445\n",
      "Step: 5403, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.1169042587280273\n",
      "Step: 5404, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.1107673645019531\n",
      "Step: 5405, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.405604362487793\n",
      "Step: 5406, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.4963443279266357\n",
      "Step: 5407, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.330233097076416\n",
      "Step: 5408, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.5554280281066895\n",
      "Step: 5409, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.4563827514648438\n",
      "Step: 5410, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.458221673965454\n",
      "Step: 5411, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.3513877391815186\n",
      "Step: 5412, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.2557780742645264\n",
      "Step: 5413, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 1.265805959701538\n",
      "Step: 5414, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.1688225269317627\n",
      "Step: 5415, Loss: 0.9162694215774536, Accuracy: 1.0, Computation time: 1.1507093906402588\n",
      "Step: 5416, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.4242818355560303\n",
      "Step: 5417, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.5094707012176514\n",
      "Step: 5418, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.0184431076049805\n",
      "Step: 5419, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.4330565929412842\n",
      "########################\n",
      "Test loss: 1.118462085723877, Test Accuracy_epoch39: 0.7059907913208008\n",
      "########################\n",
      "Step: 5420, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 0.9904346466064453\n",
      "Step: 5421, Loss: 0.9535769820213318, Accuracy: 0.9375, Computation time: 1.2099096775054932\n",
      "Step: 5422, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.0761597156524658\n",
      "Step: 5423, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.2084760665893555\n",
      "Step: 5424, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.6744627952575684\n",
      "Step: 5425, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.3494741916656494\n",
      "Step: 5426, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.196608066558838\n",
      "Step: 5427, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.260509967803955\n",
      "Step: 5428, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 1.0797100067138672\n",
      "Step: 5429, Loss: 0.931272566318512, Accuracy: 0.96875, Computation time: 1.5004138946533203\n",
      "Step: 5430, Loss: 0.9189262986183167, Accuracy: 1.0, Computation time: 1.2071475982666016\n",
      "Step: 5431, Loss: 0.9160157442092896, Accuracy: 1.0, Computation time: 1.201467514038086\n",
      "Step: 5432, Loss: 0.9160056710243225, Accuracy: 1.0, Computation time: 1.099623680114746\n",
      "Step: 5433, Loss: 0.9162648916244507, Accuracy: 1.0, Computation time: 1.4819331169128418\n",
      "Step: 5434, Loss: 0.9163315296173096, Accuracy: 1.0, Computation time: 1.3112530708312988\n",
      "Step: 5435, Loss: 0.9164638519287109, Accuracy: 1.0, Computation time: 1.1666498184204102\n",
      "Step: 5436, Loss: 0.9369344115257263, Accuracy: 0.96875, Computation time: 1.2229301929473877\n",
      "Step: 5437, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.3699281215667725\n",
      "Step: 5438, Loss: 0.9378641247749329, Accuracy: 0.96875, Computation time: 1.3868203163146973\n",
      "Step: 5439, Loss: 0.9170993566513062, Accuracy: 1.0, Computation time: 1.35429048538208\n",
      "Step: 5440, Loss: 0.919418215751648, Accuracy: 1.0, Computation time: 2.055833101272583\n",
      "Step: 5441, Loss: 0.9163782000541687, Accuracy: 1.0, Computation time: 1.3300871849060059\n",
      "Step: 5442, Loss: 0.916439414024353, Accuracy: 1.0, Computation time: 1.1600401401519775\n",
      "Step: 5443, Loss: 0.9160869121551514, Accuracy: 1.0, Computation time: 1.1315762996673584\n",
      "Step: 5444, Loss: 0.9161660671234131, Accuracy: 1.0, Computation time: 1.093414545059204\n",
      "Step: 5445, Loss: 0.915987491607666, Accuracy: 1.0, Computation time: 1.0175678730010986\n",
      "Step: 5446, Loss: 0.9159619212150574, Accuracy: 1.0, Computation time: 1.2267184257507324\n",
      "Step: 5447, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 1.0116078853607178\n",
      "Step: 5448, Loss: 0.9160939455032349, Accuracy: 1.0, Computation time: 1.0680344104766846\n",
      "Step: 5449, Loss: 0.9161432981491089, Accuracy: 1.0, Computation time: 1.3837313652038574\n",
      "Step: 5450, Loss: 0.9160594940185547, Accuracy: 1.0, Computation time: 1.1405353546142578\n",
      "Step: 5451, Loss: 0.9160521626472473, Accuracy: 1.0, Computation time: 1.1582462787628174\n",
      "Step: 5452, Loss: 0.9159136414527893, Accuracy: 1.0, Computation time: 1.220423936843872\n",
      "Step: 5453, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 1.1950817108154297\n",
      "Step: 5454, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 1.200821876525879\n",
      "Step: 5455, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.2425642013549805\n",
      "Step: 5456, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.0880234241485596\n",
      "Step: 5457, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 1.0907516479492188\n",
      "Step: 5458, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.5335934162139893\n",
      "Step: 5459, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.359302043914795\n",
      "Step: 5460, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.4777913093566895\n",
      "Step: 5461, Loss: 0.9375993013381958, Accuracy: 0.96875, Computation time: 1.1366195678710938\n",
      "Step: 5462, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.3451855182647705\n",
      "Step: 5463, Loss: 0.9159534573554993, Accuracy: 1.0, Computation time: 1.2750678062438965\n",
      "Step: 5464, Loss: 0.9541971683502197, Accuracy: 0.9375, Computation time: 2.3127400875091553\n",
      "Step: 5465, Loss: 0.9317033886909485, Accuracy: 0.96875, Computation time: 1.2181766033172607\n",
      "Step: 5466, Loss: 0.9162959456443787, Accuracy: 1.0, Computation time: 1.0617806911468506\n",
      "Step: 5467, Loss: 0.9170094728469849, Accuracy: 1.0, Computation time: 1.7055461406707764\n",
      "Step: 5468, Loss: 0.9371403455734253, Accuracy: 0.96875, Computation time: 1.282862663269043\n",
      "Step: 5469, Loss: 0.9376755356788635, Accuracy: 0.96875, Computation time: 1.1850318908691406\n",
      "Step: 5470, Loss: 0.9438169598579407, Accuracy: 0.96875, Computation time: 1.5232958793640137\n",
      "Step: 5471, Loss: 0.9168399572372437, Accuracy: 1.0, Computation time: 1.3472576141357422\n",
      "Step: 5472, Loss: 0.9185400605201721, Accuracy: 1.0, Computation time: 1.2778644561767578\n",
      "Step: 5473, Loss: 0.9164003133773804, Accuracy: 1.0, Computation time: 1.147721529006958\n",
      "Step: 5474, Loss: 0.9169924855232239, Accuracy: 1.0, Computation time: 1.57065749168396\n",
      "Step: 5475, Loss: 0.916566789150238, Accuracy: 1.0, Computation time: 1.2318236827850342\n",
      "Step: 5476, Loss: 0.9164127707481384, Accuracy: 1.0, Computation time: 1.3436393737792969\n",
      "Step: 5477, Loss: 0.9164137840270996, Accuracy: 1.0, Computation time: 1.0263335704803467\n",
      "Step: 5478, Loss: 0.9190893173217773, Accuracy: 1.0, Computation time: 1.4396412372589111\n",
      "Step: 5479, Loss: 0.9162468314170837, Accuracy: 1.0, Computation time: 1.1289868354797363\n",
      "Step: 5480, Loss: 0.9164199829101562, Accuracy: 1.0, Computation time: 1.2971796989440918\n",
      "Step: 5481, Loss: 0.9173499941825867, Accuracy: 1.0, Computation time: 1.1857166290283203\n",
      "Step: 5482, Loss: 0.915999174118042, Accuracy: 1.0, Computation time: 1.151456356048584\n",
      "Step: 5483, Loss: 0.9163734316825867, Accuracy: 1.0, Computation time: 1.066478967666626\n",
      "Step: 5484, Loss: 0.9388430118560791, Accuracy: 0.96875, Computation time: 1.2115163803100586\n",
      "Step: 5485, Loss: 0.9171046614646912, Accuracy: 1.0, Computation time: 1.242586374282837\n",
      "Step: 5486, Loss: 0.9161669015884399, Accuracy: 1.0, Computation time: 1.3721020221710205\n",
      "Step: 5487, Loss: 0.9162123799324036, Accuracy: 1.0, Computation time: 1.533254623413086\n",
      "Step: 5488, Loss: 0.9162543416023254, Accuracy: 1.0, Computation time: 0.9834167957305908\n",
      "Step: 5489, Loss: 0.9257991909980774, Accuracy: 0.96875, Computation time: 1.285743236541748\n",
      "Step: 5490, Loss: 0.9165907502174377, Accuracy: 1.0, Computation time: 0.9168581962585449\n",
      "Step: 5491, Loss: 0.9166577458381653, Accuracy: 1.0, Computation time: 1.1078898906707764\n",
      "Step: 5492, Loss: 0.9161857962608337, Accuracy: 1.0, Computation time: 1.0200269222259521\n",
      "Step: 5493, Loss: 0.9166618585586548, Accuracy: 1.0, Computation time: 1.10870361328125\n",
      "Step: 5494, Loss: 0.9167785048484802, Accuracy: 1.0, Computation time: 1.347862958908081\n",
      "Step: 5495, Loss: 0.9162396788597107, Accuracy: 1.0, Computation time: 1.1485073566436768\n",
      "Step: 5496, Loss: 0.9160763025283813, Accuracy: 1.0, Computation time: 1.2280421257019043\n",
      "Step: 5497, Loss: 0.9159680604934692, Accuracy: 1.0, Computation time: 1.3541221618652344\n",
      "Step: 5498, Loss: 0.916535496711731, Accuracy: 1.0, Computation time: 1.0663402080535889\n",
      "Step: 5499, Loss: 0.9161463379859924, Accuracy: 1.0, Computation time: 1.3971705436706543\n",
      "Step: 5500, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 1.2451534271240234\n",
      "Step: 5501, Loss: 0.916074275970459, Accuracy: 1.0, Computation time: 1.1813898086547852\n",
      "Step: 5502, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 1.3517200946807861\n",
      "Step: 5503, Loss: 0.9159875512123108, Accuracy: 1.0, Computation time: 1.3295650482177734\n",
      "Step: 5504, Loss: 0.9159212112426758, Accuracy: 1.0, Computation time: 1.10231614112854\n",
      "Step: 5505, Loss: 0.9161917567253113, Accuracy: 1.0, Computation time: 1.021686315536499\n",
      "Step: 5506, Loss: 0.9172698855400085, Accuracy: 1.0, Computation time: 1.4534778594970703\n",
      "Step: 5507, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 1.2844123840332031\n",
      "Step: 5508, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.32722806930542\n",
      "Step: 5509, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.4469797611236572\n",
      "Step: 5510, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.3772175312042236\n",
      "Step: 5511, Loss: 0.9161311388015747, Accuracy: 1.0, Computation time: 1.3244144916534424\n",
      "Step: 5512, Loss: 0.9165946245193481, Accuracy: 1.0, Computation time: 1.3732097148895264\n",
      "Step: 5513, Loss: 0.9160600304603577, Accuracy: 1.0, Computation time: 1.1469168663024902\n",
      "Step: 5514, Loss: 0.9160996079444885, Accuracy: 1.0, Computation time: 1.2662532329559326\n",
      "Step: 5515, Loss: 0.9159893989562988, Accuracy: 1.0, Computation time: 1.5691468715667725\n",
      "Step: 5516, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.0937473773956299\n",
      "Step: 5517, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.328486442565918\n",
      "Step: 5518, Loss: 0.9173470735549927, Accuracy: 1.0, Computation time: 1.9131443500518799\n",
      "Step: 5519, Loss: 0.9161506295204163, Accuracy: 1.0, Computation time: 1.5318362712860107\n",
      "Step: 5520, Loss: 0.9166626334190369, Accuracy: 1.0, Computation time: 1.1157727241516113\n",
      "Step: 5521, Loss: 0.9160232543945312, Accuracy: 1.0, Computation time: 1.1262218952178955\n",
      "Step: 5522, Loss: 0.9160284996032715, Accuracy: 1.0, Computation time: 1.110039472579956\n",
      "Step: 5523, Loss: 0.9160999059677124, Accuracy: 1.0, Computation time: 1.2207438945770264\n",
      "Step: 5524, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 1.376115322113037\n",
      "Step: 5525, Loss: 0.9161345958709717, Accuracy: 1.0, Computation time: 0.9801349639892578\n",
      "Step: 5526, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 1.1031136512756348\n",
      "Step: 5527, Loss: 0.944728434085846, Accuracy: 0.96875, Computation time: 1.3522465229034424\n",
      "Step: 5528, Loss: 0.9160458445549011, Accuracy: 1.0, Computation time: 1.4067659378051758\n",
      "Step: 5529, Loss: 0.9295016527175903, Accuracy: 0.96875, Computation time: 1.182387351989746\n",
      "Step: 5530, Loss: 0.915951132774353, Accuracy: 1.0, Computation time: 0.9681203365325928\n",
      "Step: 5531, Loss: 0.9160557389259338, Accuracy: 1.0, Computation time: 1.0469739437103271\n",
      "Step: 5532, Loss: 0.9159906506538391, Accuracy: 1.0, Computation time: 1.2346363067626953\n",
      "Step: 5533, Loss: 0.9168081879615784, Accuracy: 1.0, Computation time: 0.9984416961669922\n",
      "Step: 5534, Loss: 0.9161268472671509, Accuracy: 1.0, Computation time: 1.1854844093322754\n",
      "Step: 5535, Loss: 0.9160052537918091, Accuracy: 1.0, Computation time: 0.9752695560455322\n",
      "Step: 5536, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.1247892379760742\n",
      "Step: 5537, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.129373550415039\n",
      "Step: 5538, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.0225229263305664\n",
      "Step: 5539, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 1.0308091640472412\n",
      "Step: 5540, Loss: 0.9399206638336182, Accuracy: 0.9375, Computation time: 0.958242654800415\n",
      "Step: 5541, Loss: 0.9160645008087158, Accuracy: 1.0, Computation time: 0.859978437423706\n",
      "Step: 5542, Loss: 0.9159471988677979, Accuracy: 1.0, Computation time: 1.0645427703857422\n",
      "Step: 5543, Loss: 0.915972888469696, Accuracy: 1.0, Computation time: 1.0248057842254639\n",
      "Step: 5544, Loss: 0.9161316156387329, Accuracy: 1.0, Computation time: 1.4228758811950684\n",
      "Step: 5545, Loss: 0.9195103645324707, Accuracy: 1.0, Computation time: 1.1169660091400146\n",
      "Step: 5546, Loss: 0.93778395652771, Accuracy: 0.96875, Computation time: 1.4406750202178955\n",
      "Step: 5547, Loss: 0.915941596031189, Accuracy: 1.0, Computation time: 0.962937593460083\n",
      "Step: 5548, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.7506349086761475\n",
      "Step: 5549, Loss: 0.9237809777259827, Accuracy: 1.0, Computation time: 1.3187565803527832\n",
      "Step: 5550, Loss: 0.9377273917198181, Accuracy: 0.96875, Computation time: 1.131509780883789\n",
      "Step: 5551, Loss: 0.9159225225448608, Accuracy: 1.0, Computation time: 1.020639181137085\n",
      "Step: 5552, Loss: 0.91591477394104, Accuracy: 1.0, Computation time: 1.211799144744873\n",
      "Step: 5553, Loss: 0.9421598315238953, Accuracy: 0.96875, Computation time: 1.2131366729736328\n",
      "Step: 5554, Loss: 0.917780876159668, Accuracy: 1.0, Computation time: 1.1272320747375488\n",
      "Step: 5555, Loss: 0.9160272479057312, Accuracy: 1.0, Computation time: 1.0473780632019043\n",
      "Step: 5556, Loss: 0.9207029342651367, Accuracy: 1.0, Computation time: 0.9468293190002441\n",
      "Step: 5557, Loss: 0.9160138964653015, Accuracy: 1.0, Computation time: 0.8408772945404053\n",
      "Step: 5558, Loss: 0.9368748068809509, Accuracy: 0.96875, Computation time: 0.8927021026611328\n",
      "########################\n",
      "Test loss: 1.124446988105774, Test Accuracy_epoch40: 0.6921659111976624\n",
      "########################\n",
      "Step: 5559, Loss: 0.9163300395011902, Accuracy: 1.0, Computation time: 1.1704771518707275\n",
      "Step: 5560, Loss: 0.9164440631866455, Accuracy: 1.0, Computation time: 0.9631333351135254\n",
      "Step: 5561, Loss: 0.916192889213562, Accuracy: 1.0, Computation time: 1.0181727409362793\n",
      "Step: 5562, Loss: 0.919649600982666, Accuracy: 1.0, Computation time: 0.9682140350341797\n",
      "Step: 5563, Loss: 0.9159905314445496, Accuracy: 1.0, Computation time: 0.85109543800354\n",
      "Step: 5564, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 0.914417028427124\n",
      "Step: 5565, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 1.0195963382720947\n",
      "Step: 5566, Loss: 0.9161074757575989, Accuracy: 1.0, Computation time: 0.8991658687591553\n",
      "Step: 5567, Loss: 0.9161106944084167, Accuracy: 1.0, Computation time: 0.8606431484222412\n",
      "Step: 5568, Loss: 0.9160203337669373, Accuracy: 1.0, Computation time: 1.0343632698059082\n",
      "Step: 5569, Loss: 0.9160524010658264, Accuracy: 1.0, Computation time: 1.0252716541290283\n",
      "Step: 5570, Loss: 0.9160202145576477, Accuracy: 1.0, Computation time: 0.907874584197998\n",
      "Step: 5571, Loss: 0.9159935116767883, Accuracy: 1.0, Computation time: 1.0431883335113525\n",
      "Step: 5572, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 0.9143099784851074\n",
      "Step: 5573, Loss: 0.9160183072090149, Accuracy: 1.0, Computation time: 1.4615819454193115\n",
      "Step: 5574, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 0.9608056545257568\n",
      "Step: 5575, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 0.850804328918457\n",
      "Step: 5576, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 0.9069697856903076\n",
      "Step: 5577, Loss: 0.9160175323486328, Accuracy: 1.0, Computation time: 0.8320503234863281\n",
      "Step: 5578, Loss: 0.9159461855888367, Accuracy: 1.0, Computation time: 1.6372172832489014\n",
      "Step: 5579, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 0.9683246612548828\n",
      "Step: 5580, Loss: 0.9376709461212158, Accuracy: 0.96875, Computation time: 0.865123987197876\n",
      "Step: 5581, Loss: 0.9166605472564697, Accuracy: 1.0, Computation time: 1.3338394165039062\n",
      "Step: 5582, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 1.2227332592010498\n",
      "Step: 5583, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 0.8442456722259521\n",
      "Step: 5584, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.3061480522155762\n",
      "Step: 5585, Loss: 0.9160251021385193, Accuracy: 1.0, Computation time: 1.1154165267944336\n",
      "Step: 5586, Loss: 0.9376147985458374, Accuracy: 0.96875, Computation time: 0.9860620498657227\n",
      "Step: 5587, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 0.9469966888427734\n",
      "Step: 5588, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 0.9825646877288818\n",
      "Step: 5589, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 0.9244508743286133\n",
      "Step: 5590, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.0510015487670898\n",
      "Step: 5591, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 0.9255549907684326\n",
      "Step: 5592, Loss: 0.91670161485672, Accuracy: 1.0, Computation time: 1.007847547531128\n",
      "Step: 5593, Loss: 0.9197689890861511, Accuracy: 1.0, Computation time: 1.695845365524292\n",
      "Step: 5594, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 0.932797908782959\n",
      "Step: 5595, Loss: 0.9161757230758667, Accuracy: 1.0, Computation time: 0.9262080192565918\n",
      "Step: 5596, Loss: 0.9159510731697083, Accuracy: 1.0, Computation time: 1.0580265522003174\n",
      "Step: 5597, Loss: 0.9242140650749207, Accuracy: 1.0, Computation time: 0.9749152660369873\n",
      "Step: 5598, Loss: 0.9371599555015564, Accuracy: 0.96875, Computation time: 1.0845136642456055\n",
      "Step: 5599, Loss: 0.9159411787986755, Accuracy: 1.0, Computation time: 1.0580401420593262\n",
      "Step: 5600, Loss: 0.9160963892936707, Accuracy: 1.0, Computation time: 1.0416979789733887\n",
      "Step: 5601, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 1.000370740890503\n",
      "Step: 5602, Loss: 0.9232990741729736, Accuracy: 1.0, Computation time: 1.5006463527679443\n",
      "Step: 5603, Loss: 0.9159466624259949, Accuracy: 1.0, Computation time: 0.8763477802276611\n",
      "Step: 5604, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.3317844867706299\n",
      "Step: 5605, Loss: 0.915963351726532, Accuracy: 1.0, Computation time: 1.0121879577636719\n",
      "Step: 5606, Loss: 0.9159629940986633, Accuracy: 1.0, Computation time: 0.8512823581695557\n",
      "Step: 5607, Loss: 0.9160012006759644, Accuracy: 1.0, Computation time: 0.9690384864807129\n",
      "Step: 5608, Loss: 0.9160081148147583, Accuracy: 1.0, Computation time: 0.9650990962982178\n",
      "Step: 5609, Loss: 0.9376701712608337, Accuracy: 0.96875, Computation time: 0.9110853672027588\n",
      "Step: 5610, Loss: 0.9388227462768555, Accuracy: 0.96875, Computation time: 0.8631088733673096\n",
      "Step: 5611, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 1.0831573009490967\n",
      "Step: 5612, Loss: 0.9163353443145752, Accuracy: 1.0, Computation time: 0.9706838130950928\n",
      "Step: 5613, Loss: 0.916804313659668, Accuracy: 1.0, Computation time: 1.1044807434082031\n",
      "Step: 5614, Loss: 0.9391107559204102, Accuracy: 0.96875, Computation time: 1.0200121402740479\n",
      "Step: 5615, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 0.936161994934082\n",
      "Step: 5616, Loss: 0.9160040616989136, Accuracy: 1.0, Computation time: 1.0847268104553223\n",
      "Step: 5617, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 0.9641141891479492\n",
      "Step: 5618, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 0.8733835220336914\n",
      "Step: 5619, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.1057522296905518\n",
      "Step: 5620, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 0.9326646327972412\n",
      "Step: 5621, Loss: 0.9159549474716187, Accuracy: 1.0, Computation time: 0.8819248676300049\n",
      "Step: 5622, Loss: 0.9159202575683594, Accuracy: 1.0, Computation time: 0.9978330135345459\n",
      "Step: 5623, Loss: 0.9161561131477356, Accuracy: 1.0, Computation time: 1.246091604232788\n",
      "Step: 5624, Loss: 0.9165397882461548, Accuracy: 1.0, Computation time: 0.94329833984375\n",
      "Step: 5625, Loss: 0.9184507727622986, Accuracy: 1.0, Computation time: 1.0272631645202637\n",
      "Step: 5626, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.0314722061157227\n",
      "Step: 5627, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 0.9799778461456299\n",
      "Step: 5628, Loss: 0.9162449240684509, Accuracy: 1.0, Computation time: 0.8680384159088135\n",
      "Step: 5629, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.203620195388794\n",
      "Step: 5630, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 0.8627443313598633\n",
      "Step: 5631, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 0.9863872528076172\n",
      "Step: 5632, Loss: 0.9162250757217407, Accuracy: 1.0, Computation time: 0.8904261589050293\n",
      "Step: 5633, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 0.8171854019165039\n",
      "Step: 5634, Loss: 0.9161394834518433, Accuracy: 1.0, Computation time: 0.9218261241912842\n",
      "Step: 5635, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 0.9544756412506104\n",
      "Step: 5636, Loss: 0.9175945520401001, Accuracy: 1.0, Computation time: 0.9329562187194824\n",
      "Step: 5637, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 0.9464480876922607\n",
      "Step: 5638, Loss: 0.9159934520721436, Accuracy: 1.0, Computation time: 1.0036165714263916\n",
      "Step: 5639, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.023723840713501\n",
      "Step: 5640, Loss: 0.9298432469367981, Accuracy: 0.96875, Computation time: 1.1052155494689941\n",
      "Step: 5641, Loss: 0.9376058578491211, Accuracy: 0.96875, Computation time: 0.8775651454925537\n",
      "Step: 5642, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 0.9223134517669678\n",
      "Step: 5643, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 0.8654155731201172\n",
      "Step: 5644, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 0.878922700881958\n",
      "Step: 5645, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 0.7898218631744385\n",
      "Step: 5646, Loss: 0.9159560799598694, Accuracy: 1.0, Computation time: 0.9648215770721436\n",
      "Step: 5647, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.8283097743988037\n",
      "Step: 5648, Loss: 0.9161703586578369, Accuracy: 1.0, Computation time: 1.32688570022583\n",
      "Step: 5649, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 0.9142947196960449\n",
      "Step: 5650, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 0.8858504295349121\n",
      "Step: 5651, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 0.8827862739562988\n",
      "Step: 5652, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 0.8739485740661621\n",
      "Step: 5653, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 0.8993320465087891\n",
      "Step: 5654, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.1107752323150635\n",
      "Step: 5655, Loss: 0.9159614443778992, Accuracy: 1.0, Computation time: 1.15946364402771\n",
      "Step: 5656, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.0596449375152588\n",
      "Step: 5657, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 0.9362115859985352\n",
      "Step: 5658, Loss: 0.9246560335159302, Accuracy: 1.0, Computation time: 1.2650794982910156\n",
      "Step: 5659, Loss: 0.9159514904022217, Accuracy: 1.0, Computation time: 0.8220930099487305\n",
      "Step: 5660, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.0178003311157227\n",
      "Step: 5661, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.0402190685272217\n",
      "Step: 5662, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 1.064166784286499\n",
      "Step: 5663, Loss: 0.9159708023071289, Accuracy: 1.0, Computation time: 0.9834926128387451\n",
      "Step: 5664, Loss: 0.915963351726532, Accuracy: 1.0, Computation time: 1.0445749759674072\n",
      "Step: 5665, Loss: 0.9159438014030457, Accuracy: 1.0, Computation time: 1.0123915672302246\n",
      "Step: 5666, Loss: 0.9358697533607483, Accuracy: 0.96875, Computation time: 1.1109368801116943\n",
      "Step: 5667, Loss: 0.9161123037338257, Accuracy: 1.0, Computation time: 1.132758378982544\n",
      "Step: 5668, Loss: 0.9160955548286438, Accuracy: 1.0, Computation time: 2.1008520126342773\n",
      "Step: 5669, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 1.0940518379211426\n",
      "Step: 5670, Loss: 0.9264857172966003, Accuracy: 0.96875, Computation time: 1.3599579334259033\n",
      "Step: 5671, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 1.1712462902069092\n",
      "Step: 5672, Loss: 0.916412353515625, Accuracy: 1.0, Computation time: 1.5550591945648193\n",
      "Step: 5673, Loss: 0.9245063066482544, Accuracy: 1.0, Computation time: 1.0342552661895752\n",
      "Step: 5674, Loss: 0.915962278842926, Accuracy: 1.0, Computation time: 0.9716835021972656\n",
      "Step: 5675, Loss: 0.9159383773803711, Accuracy: 1.0, Computation time: 1.003042459487915\n",
      "Step: 5676, Loss: 0.9159538149833679, Accuracy: 1.0, Computation time: 1.0234694480895996\n",
      "Step: 5677, Loss: 0.9380791187286377, Accuracy: 0.96875, Computation time: 0.9922394752502441\n",
      "Step: 5678, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 0.8399112224578857\n",
      "Step: 5679, Loss: 0.9160354733467102, Accuracy: 1.0, Computation time: 1.3832876682281494\n",
      "Step: 5680, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 1.0761632919311523\n",
      "Step: 5681, Loss: 0.9163017868995667, Accuracy: 1.0, Computation time: 1.5654735565185547\n",
      "Step: 5682, Loss: 0.9160499572753906, Accuracy: 1.0, Computation time: 1.0511910915374756\n",
      "Step: 5683, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 1.2744312286376953\n",
      "Step: 5684, Loss: 0.9159902930259705, Accuracy: 1.0, Computation time: 1.010671615600586\n",
      "Step: 5685, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.112767219543457\n",
      "Step: 5686, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 0.945504903793335\n",
      "Step: 5687, Loss: 0.9164151549339294, Accuracy: 1.0, Computation time: 1.3549439907073975\n",
      "Step: 5688, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 0.8773226737976074\n",
      "Step: 5689, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.0688393115997314\n",
      "Step: 5690, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 0.8765501976013184\n",
      "Step: 5691, Loss: 0.9159350991249084, Accuracy: 1.0, Computation time: 0.9439725875854492\n",
      "Step: 5692, Loss: 0.9159862399101257, Accuracy: 1.0, Computation time: 0.9441967010498047\n",
      "Step: 5693, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 0.9368793964385986\n",
      "Step: 5694, Loss: 0.9164844751358032, Accuracy: 1.0, Computation time: 1.1539225578308105\n",
      "Step: 5695, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 0.9271664619445801\n",
      "Step: 5696, Loss: 0.915938138961792, Accuracy: 1.0, Computation time: 0.8495626449584961\n",
      "Step: 5697, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 0.8998730182647705\n",
      "########################\n",
      "Test loss: 1.1175063848495483, Test Accuracy_epoch41: 0.7059907913208008\n",
      "########################\n",
      "Step: 5698, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 0.9350273609161377\n",
      "Step: 5699, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 0.9100096225738525\n",
      "Step: 5700, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.02897310256958\n",
      "Step: 5701, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 0.8757338523864746\n",
      "Step: 5702, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 0.9505493640899658\n",
      "Step: 5703, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 0.9839956760406494\n",
      "Step: 5704, Loss: 0.9159073233604431, Accuracy: 1.0, Computation time: 0.9619007110595703\n",
      "Step: 5705, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 0.8870313167572021\n",
      "Step: 5706, Loss: 0.9522958397865295, Accuracy: 0.9375, Computation time: 0.9410519599914551\n",
      "Step: 5707, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.102815866470337\n",
      "Step: 5708, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 1.069727897644043\n",
      "Step: 5709, Loss: 0.9199811220169067, Accuracy: 1.0, Computation time: 1.046142578125\n",
      "Step: 5710, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 0.9591617584228516\n",
      "Step: 5711, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 1.0521111488342285\n",
      "Step: 5712, Loss: 0.9160909652709961, Accuracy: 1.0, Computation time: 0.8804731369018555\n",
      "Step: 5713, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.0304286479949951\n",
      "Step: 5714, Loss: 0.9183076024055481, Accuracy: 1.0, Computation time: 1.03155517578125\n",
      "Step: 5715, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 0.963287353515625\n",
      "Step: 5716, Loss: 0.915973424911499, Accuracy: 1.0, Computation time: 1.190112590789795\n",
      "Step: 5717, Loss: 0.9375663995742798, Accuracy: 0.96875, Computation time: 1.0888495445251465\n",
      "Step: 5718, Loss: 0.9160153269767761, Accuracy: 1.0, Computation time: 1.546494722366333\n",
      "Step: 5719, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.1342036724090576\n",
      "Step: 5720, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 0.8376147747039795\n",
      "Step: 5721, Loss: 0.9159520864486694, Accuracy: 1.0, Computation time: 0.8497016429901123\n",
      "Step: 5722, Loss: 0.915985643863678, Accuracy: 1.0, Computation time: 1.025942325592041\n",
      "Step: 5723, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 0.8200290203094482\n",
      "Step: 5724, Loss: 0.9162245392799377, Accuracy: 1.0, Computation time: 1.1023478507995605\n",
      "Step: 5725, Loss: 0.9415766000747681, Accuracy: 0.96875, Computation time: 1.6075356006622314\n",
      "Step: 5726, Loss: 0.9159169793128967, Accuracy: 1.0, Computation time: 0.8540537357330322\n",
      "Step: 5727, Loss: 0.9160621762275696, Accuracy: 1.0, Computation time: 0.8883905410766602\n",
      "Step: 5728, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 0.923851728439331\n",
      "Step: 5729, Loss: 0.9160650372505188, Accuracy: 1.0, Computation time: 0.8055222034454346\n",
      "Step: 5730, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 0.8898060321807861\n",
      "Step: 5731, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 0.8600895404815674\n",
      "Step: 5732, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 0.8581893444061279\n",
      "Step: 5733, Loss: 0.9222167730331421, Accuracy: 1.0, Computation time: 0.8454477787017822\n",
      "Step: 5734, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.0469422340393066\n",
      "Step: 5735, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 0.9875714778900146\n",
      "Step: 5736, Loss: 0.9159621000289917, Accuracy: 1.0, Computation time: 0.8801536560058594\n",
      "Step: 5737, Loss: 0.9159795045852661, Accuracy: 1.0, Computation time: 0.8986780643463135\n",
      "Step: 5738, Loss: 0.9160016775131226, Accuracy: 1.0, Computation time: 0.9737226963043213\n",
      "Step: 5739, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 0.8934676647186279\n",
      "Step: 5740, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 0.9932951927185059\n",
      "Step: 5741, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.094660997390747\n",
      "Step: 5742, Loss: 0.9159764051437378, Accuracy: 1.0, Computation time: 0.8767740726470947\n",
      "Step: 5743, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.1552350521087646\n",
      "Step: 5744, Loss: 0.931885302066803, Accuracy: 0.96875, Computation time: 2.462751865386963\n",
      "Step: 5745, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 0.8534274101257324\n",
      "Step: 5746, Loss: 0.9160080552101135, Accuracy: 1.0, Computation time: 0.9669294357299805\n",
      "Step: 5747, Loss: 0.9160465002059937, Accuracy: 1.0, Computation time: 0.9354250431060791\n",
      "Step: 5748, Loss: 0.9160544276237488, Accuracy: 1.0, Computation time: 1.0544121265411377\n",
      "Step: 5749, Loss: 0.9160972237586975, Accuracy: 1.0, Computation time: 1.1079127788543701\n",
      "Step: 5750, Loss: 0.9160413146018982, Accuracy: 1.0, Computation time: 1.322037935256958\n",
      "Step: 5751, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 1.2776374816894531\n",
      "Step: 5752, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.4553496837615967\n",
      "Step: 5753, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.2973740100860596\n",
      "Step: 5754, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 1.6098501682281494\n",
      "Step: 5755, Loss: 0.9159606695175171, Accuracy: 1.0, Computation time: 1.8851125240325928\n",
      "Step: 5756, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 1.702176809310913\n",
      "Step: 5757, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.7597293853759766\n",
      "Step: 5758, Loss: 0.9160490036010742, Accuracy: 1.0, Computation time: 1.9483649730682373\n",
      "Step: 5759, Loss: 0.9184583425521851, Accuracy: 1.0, Computation time: 1.6795737743377686\n",
      "Step: 5760, Loss: 0.9376237988471985, Accuracy: 0.96875, Computation time: 1.355607509613037\n",
      "Step: 5761, Loss: 0.9414463639259338, Accuracy: 0.96875, Computation time: 2.070568561553955\n",
      "Step: 5762, Loss: 0.927810549736023, Accuracy: 0.96875, Computation time: 1.7327027320861816\n",
      "Step: 5763, Loss: 0.9173684120178223, Accuracy: 1.0, Computation time: 1.8804740905761719\n",
      "Step: 5764, Loss: 0.9167179465293884, Accuracy: 1.0, Computation time: 1.4665780067443848\n",
      "Step: 5765, Loss: 0.9173282384872437, Accuracy: 1.0, Computation time: 1.2583897113800049\n",
      "Step: 5766, Loss: 0.9165932536125183, Accuracy: 1.0, Computation time: 1.1684491634368896\n",
      "Step: 5767, Loss: 0.9165531992912292, Accuracy: 1.0, Computation time: 1.226193904876709\n",
      "Step: 5768, Loss: 0.9165067076683044, Accuracy: 1.0, Computation time: 1.3564605712890625\n",
      "Step: 5769, Loss: 0.9161422252655029, Accuracy: 1.0, Computation time: 1.1238491535186768\n",
      "Step: 5770, Loss: 0.9159199595451355, Accuracy: 1.0, Computation time: 1.433671474456787\n",
      "Step: 5771, Loss: 0.9167062044143677, Accuracy: 1.0, Computation time: 1.3754878044128418\n",
      "Step: 5772, Loss: 0.959445595741272, Accuracy: 0.9375, Computation time: 1.2860193252563477\n",
      "Step: 5773, Loss: 0.9390161633491516, Accuracy: 0.96875, Computation time: 1.1122288703918457\n",
      "Step: 5774, Loss: 0.9162572026252747, Accuracy: 1.0, Computation time: 1.1300992965698242\n",
      "Step: 5775, Loss: 0.9161712527275085, Accuracy: 1.0, Computation time: 1.237384557723999\n",
      "Step: 5776, Loss: 0.9163137078285217, Accuracy: 1.0, Computation time: 1.1289374828338623\n",
      "Step: 5777, Loss: 0.9168867468833923, Accuracy: 1.0, Computation time: 0.9633996486663818\n",
      "Step: 5778, Loss: 0.9163268804550171, Accuracy: 1.0, Computation time: 1.0854744911193848\n",
      "Step: 5779, Loss: 0.9160939455032349, Accuracy: 1.0, Computation time: 1.4040699005126953\n",
      "Step: 5780, Loss: 0.9161280989646912, Accuracy: 1.0, Computation time: 1.0981299877166748\n",
      "Step: 5781, Loss: 0.9160552620887756, Accuracy: 1.0, Computation time: 1.0567967891693115\n",
      "Step: 5782, Loss: 0.9159594774246216, Accuracy: 1.0, Computation time: 0.9728250503540039\n",
      "Step: 5783, Loss: 0.9256201386451721, Accuracy: 0.96875, Computation time: 1.0654716491699219\n",
      "Step: 5784, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 0.9411699771881104\n",
      "Step: 5785, Loss: 0.9159775972366333, Accuracy: 1.0, Computation time: 1.05155348777771\n",
      "Step: 5786, Loss: 0.9160041809082031, Accuracy: 1.0, Computation time: 0.9128971099853516\n",
      "Step: 5787, Loss: 0.9161803126335144, Accuracy: 1.0, Computation time: 0.943190336227417\n",
      "Step: 5788, Loss: 0.9162188768386841, Accuracy: 1.0, Computation time: 1.0111806392669678\n",
      "Step: 5789, Loss: 0.916417121887207, Accuracy: 1.0, Computation time: 0.9588932991027832\n",
      "Step: 5790, Loss: 0.9161171913146973, Accuracy: 1.0, Computation time: 1.1805284023284912\n",
      "Step: 5791, Loss: 0.9161439538002014, Accuracy: 1.0, Computation time: 1.17722487449646\n",
      "Step: 5792, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 0.8318057060241699\n",
      "Step: 5793, Loss: 0.9373490810394287, Accuracy: 0.96875, Computation time: 1.4838004112243652\n",
      "Step: 5794, Loss: 0.9383122324943542, Accuracy: 0.96875, Computation time: 1.3535900115966797\n",
      "Step: 5795, Loss: 0.9268038272857666, Accuracy: 0.96875, Computation time: 1.1441504955291748\n",
      "Step: 5796, Loss: 0.9160679578781128, Accuracy: 1.0, Computation time: 1.1241557598114014\n",
      "Step: 5797, Loss: 0.9160429239273071, Accuracy: 1.0, Computation time: 1.0351717472076416\n",
      "Step: 5798, Loss: 0.9160344004631042, Accuracy: 1.0, Computation time: 1.3172931671142578\n",
      "Step: 5799, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 1.2872414588928223\n",
      "Step: 5800, Loss: 0.9160385727882385, Accuracy: 1.0, Computation time: 1.4869084358215332\n",
      "Step: 5801, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.1777851581573486\n",
      "Step: 5802, Loss: 0.9159545302391052, Accuracy: 1.0, Computation time: 1.134817361831665\n",
      "Step: 5803, Loss: 0.9238770008087158, Accuracy: 1.0, Computation time: 1.591627836227417\n",
      "Step: 5804, Loss: 0.9375475645065308, Accuracy: 0.96875, Computation time: 1.0817937850952148\n",
      "Step: 5805, Loss: 0.9366376399993896, Accuracy: 0.96875, Computation time: 1.2883429527282715\n",
      "Step: 5806, Loss: 0.9159592390060425, Accuracy: 1.0, Computation time: 1.3358545303344727\n",
      "Step: 5807, Loss: 0.9159908890724182, Accuracy: 1.0, Computation time: 1.2245216369628906\n",
      "Step: 5808, Loss: 0.9216386675834656, Accuracy: 1.0, Computation time: 1.4027771949768066\n",
      "Step: 5809, Loss: 0.9160868525505066, Accuracy: 1.0, Computation time: 1.1917517185211182\n",
      "Step: 5810, Loss: 0.937673032283783, Accuracy: 0.96875, Computation time: 1.451967716217041\n",
      "Step: 5811, Loss: 0.9164831638336182, Accuracy: 1.0, Computation time: 1.2291104793548584\n",
      "Step: 5812, Loss: 0.916356086730957, Accuracy: 1.0, Computation time: 1.1949734687805176\n",
      "Step: 5813, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 1.0809130668640137\n",
      "Step: 5814, Loss: 0.9160796999931335, Accuracy: 1.0, Computation time: 1.157771348953247\n",
      "Step: 5815, Loss: 0.9159670472145081, Accuracy: 1.0, Computation time: 1.2616031169891357\n",
      "Step: 5816, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.543900489807129\n",
      "Step: 5817, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 1.2647628784179688\n",
      "Step: 5818, Loss: 0.9160354137420654, Accuracy: 1.0, Computation time: 1.617189645767212\n",
      "Step: 5819, Loss: 0.9160492420196533, Accuracy: 1.0, Computation time: 1.7058169841766357\n",
      "Step: 5820, Loss: 0.9221612215042114, Accuracy: 1.0, Computation time: 1.8930327892303467\n",
      "Step: 5821, Loss: 0.9377822279930115, Accuracy: 0.96875, Computation time: 1.1636595726013184\n",
      "Step: 5822, Loss: 0.9173034429550171, Accuracy: 1.0, Computation time: 1.3459830284118652\n",
      "Step: 5823, Loss: 0.9195592403411865, Accuracy: 1.0, Computation time: 1.5018470287322998\n",
      "Step: 5824, Loss: 0.9159614443778992, Accuracy: 1.0, Computation time: 1.420727014541626\n",
      "Step: 5825, Loss: 0.9346109628677368, Accuracy: 0.96875, Computation time: 1.1536672115325928\n",
      "Step: 5826, Loss: 0.9161148071289062, Accuracy: 1.0, Computation time: 1.291609764099121\n",
      "Step: 5827, Loss: 0.916172981262207, Accuracy: 1.0, Computation time: 1.039914846420288\n",
      "Step: 5828, Loss: 0.9176786541938782, Accuracy: 1.0, Computation time: 1.1692721843719482\n",
      "Step: 5829, Loss: 0.9161429405212402, Accuracy: 1.0, Computation time: 1.0675413608551025\n",
      "Step: 5830, Loss: 0.9160538911819458, Accuracy: 1.0, Computation time: 1.2701215744018555\n",
      "Step: 5831, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 1.4710404872894287\n",
      "Step: 5832, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.036541223526001\n",
      "Step: 5833, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 1.166445255279541\n",
      "Step: 5834, Loss: 0.9162247180938721, Accuracy: 1.0, Computation time: 1.0362906455993652\n",
      "Step: 5835, Loss: 0.9159517884254456, Accuracy: 1.0, Computation time: 0.865734338760376\n",
      "Step: 5836, Loss: 0.915967583656311, Accuracy: 1.0, Computation time: 1.081455945968628\n",
      "########################\n",
      "Test loss: 1.1141718626022339, Test Accuracy_epoch42: 0.7152073979377747\n",
      "########################\n",
      "Step: 5837, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 1.005767822265625\n",
      "Step: 5838, Loss: 0.9159451723098755, Accuracy: 1.0, Computation time: 0.9785516262054443\n",
      "Step: 5839, Loss: 0.9375212788581848, Accuracy: 0.96875, Computation time: 1.058960199356079\n",
      "Step: 5840, Loss: 0.937660813331604, Accuracy: 0.96875, Computation time: 1.0448083877563477\n",
      "Step: 5841, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.0196702480316162\n",
      "Step: 5842, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 0.925309419631958\n",
      "Step: 5843, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.0563263893127441\n",
      "Step: 5844, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 0.9776244163513184\n",
      "Step: 5845, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 0.8401913642883301\n",
      "Step: 5846, Loss: 0.916079044342041, Accuracy: 1.0, Computation time: 0.9625134468078613\n",
      "Step: 5847, Loss: 0.937436044216156, Accuracy: 0.96875, Computation time: 1.567054033279419\n",
      "Step: 5848, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 0.9047794342041016\n",
      "Step: 5849, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 0.8662121295928955\n",
      "Step: 5850, Loss: 0.9333067536354065, Accuracy: 0.96875, Computation time: 1.1144981384277344\n",
      "Step: 5851, Loss: 0.9373667240142822, Accuracy: 0.96875, Computation time: 0.9188754558563232\n",
      "Step: 5852, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 0.8840115070343018\n",
      "Step: 5853, Loss: 0.9160071611404419, Accuracy: 1.0, Computation time: 1.0438966751098633\n",
      "Step: 5854, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 0.9838447570800781\n",
      "Step: 5855, Loss: 0.9207291007041931, Accuracy: 1.0, Computation time: 1.1804132461547852\n",
      "Step: 5856, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 0.9958891868591309\n",
      "Step: 5857, Loss: 0.9176524877548218, Accuracy: 1.0, Computation time: 1.1104917526245117\n",
      "Step: 5858, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 1.0738120079040527\n",
      "Step: 5859, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.02054762840271\n",
      "Step: 5860, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 1.2737922668457031\n",
      "Step: 5861, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 1.1208159923553467\n",
      "Step: 5862, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 1.022798776626587\n",
      "Step: 5863, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 0.9813365936279297\n",
      "Step: 5864, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.0069315433502197\n",
      "Step: 5865, Loss: 0.9160418510437012, Accuracy: 1.0, Computation time: 1.0959675312042236\n",
      "Step: 5866, Loss: 0.9376133680343628, Accuracy: 0.96875, Computation time: 1.1946570873260498\n",
      "Step: 5867, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.0035719871520996\n",
      "Step: 5868, Loss: 0.916042149066925, Accuracy: 1.0, Computation time: 1.1203689575195312\n",
      "Step: 5869, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.314574956893921\n",
      "Step: 5870, Loss: 0.9274871349334717, Accuracy: 0.96875, Computation time: 0.979358434677124\n",
      "Step: 5871, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.3080909252166748\n",
      "Step: 5872, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.165806770324707\n",
      "Step: 5873, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.0262060165405273\n",
      "Step: 5874, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.1687166690826416\n",
      "Step: 5875, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 1.2897815704345703\n",
      "Step: 5876, Loss: 0.91590815782547, Accuracy: 1.0, Computation time: 1.160759449005127\n",
      "Step: 5877, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.1916885375976562\n",
      "Step: 5878, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 1.1463828086853027\n",
      "Step: 5879, Loss: 0.9375984072685242, Accuracy: 0.96875, Computation time: 1.0567150115966797\n",
      "Step: 5880, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.1084039211273193\n",
      "Step: 5881, Loss: 0.9160555601119995, Accuracy: 1.0, Computation time: 1.151543378829956\n",
      "Step: 5882, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.4560284614562988\n",
      "Step: 5883, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.2492783069610596\n",
      "Step: 5884, Loss: 0.9159870147705078, Accuracy: 1.0, Computation time: 1.1622376441955566\n",
      "Step: 5885, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.5457470417022705\n",
      "Step: 5886, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.5562822818756104\n",
      "Step: 5887, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.4468622207641602\n",
      "Step: 5888, Loss: 0.9159162640571594, Accuracy: 1.0, Computation time: 1.8507864475250244\n",
      "Step: 5889, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 1.2144966125488281\n",
      "Step: 5890, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.1324877738952637\n",
      "Step: 5891, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.131711483001709\n",
      "Step: 5892, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.0573654174804688\n",
      "Step: 5893, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 0.9773054122924805\n",
      "Step: 5894, Loss: 0.9167931079864502, Accuracy: 1.0, Computation time: 1.2087986469268799\n",
      "Step: 5895, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.207947015762329\n",
      "Step: 5896, Loss: 0.9160808324813843, Accuracy: 1.0, Computation time: 1.2435226440429688\n",
      "Step: 5897, Loss: 0.9162976145744324, Accuracy: 1.0, Computation time: 1.35233473777771\n",
      "Step: 5898, Loss: 0.9161510467529297, Accuracy: 1.0, Computation time: 1.157362937927246\n",
      "Step: 5899, Loss: 0.9200857877731323, Accuracy: 1.0, Computation time: 1.1975288391113281\n",
      "Step: 5900, Loss: 0.9361909627914429, Accuracy: 0.96875, Computation time: 2.018284797668457\n",
      "Step: 5901, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.3747353553771973\n",
      "Step: 5902, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.4463446140289307\n",
      "Step: 5903, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.4252119064331055\n",
      "Step: 5904, Loss: 0.916388750076294, Accuracy: 1.0, Computation time: 1.2144465446472168\n",
      "Step: 5905, Loss: 0.9160533547401428, Accuracy: 1.0, Computation time: 1.2398364543914795\n",
      "Step: 5906, Loss: 0.9160325527191162, Accuracy: 1.0, Computation time: 1.2025744915008545\n",
      "Step: 5907, Loss: 0.9159098863601685, Accuracy: 1.0, Computation time: 1.2823419570922852\n",
      "Step: 5908, Loss: 0.9159033894538879, Accuracy: 1.0, Computation time: 1.069472312927246\n",
      "Step: 5909, Loss: 0.9374881386756897, Accuracy: 0.96875, Computation time: 1.1837198734283447\n",
      "Step: 5910, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 1.6702728271484375\n",
      "Step: 5911, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.4043684005737305\n",
      "Step: 5912, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.3710589408874512\n",
      "Step: 5913, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.1299002170562744\n",
      "Step: 5914, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.0892665386199951\n",
      "Step: 5915, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.345289945602417\n",
      "Step: 5916, Loss: 0.9368901252746582, Accuracy: 0.96875, Computation time: 1.3051016330718994\n",
      "Step: 5917, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.0784416198730469\n",
      "Step: 5918, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.1537909507751465\n",
      "Step: 5919, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 0.9047949314117432\n",
      "Step: 5920, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.3348782062530518\n",
      "Step: 5921, Loss: 0.9255816340446472, Accuracy: 0.96875, Computation time: 1.201406717300415\n",
      "Step: 5922, Loss: 0.9375274181365967, Accuracy: 0.96875, Computation time: 1.2009062767028809\n",
      "Step: 5923, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 0.9640369415283203\n",
      "Step: 5924, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 0.9414594173431396\n",
      "Step: 5925, Loss: 0.9164160490036011, Accuracy: 1.0, Computation time: 1.380340576171875\n",
      "Step: 5926, Loss: 0.9391787052154541, Accuracy: 0.96875, Computation time: 1.1853175163269043\n",
      "Step: 5927, Loss: 0.9376577734947205, Accuracy: 0.96875, Computation time: 1.083592414855957\n",
      "Step: 5928, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 0.887195348739624\n",
      "Step: 5929, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.9967639446258545\n",
      "Step: 5930, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 0.9118289947509766\n",
      "Step: 5931, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.0775995254516602\n",
      "Step: 5932, Loss: 0.937211811542511, Accuracy: 0.96875, Computation time: 1.1512680053710938\n",
      "Step: 5933, Loss: 0.9375423192977905, Accuracy: 0.96875, Computation time: 0.9472208023071289\n",
      "Step: 5934, Loss: 0.9169459342956543, Accuracy: 1.0, Computation time: 1.0967950820922852\n",
      "Step: 5935, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.4678709506988525\n",
      "Step: 5936, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 0.970557451248169\n",
      "Step: 5937, Loss: 0.9159648418426514, Accuracy: 1.0, Computation time: 1.0067853927612305\n",
      "Step: 5938, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 0.9568414688110352\n",
      "Step: 5939, Loss: 0.93765789270401, Accuracy: 0.96875, Computation time: 1.077063798904419\n",
      "Step: 5940, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 0.9342589378356934\n",
      "Step: 5941, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.00535249710083\n",
      "Step: 5942, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 0.8891003131866455\n",
      "Step: 5943, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 0.92653489112854\n",
      "Step: 5944, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 0.9250993728637695\n",
      "Step: 5945, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.2456188201904297\n",
      "Step: 5946, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.0695598125457764\n",
      "Step: 5947, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.1734223365783691\n",
      "Step: 5948, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 0.927509069442749\n",
      "Step: 5949, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 0.9645235538482666\n",
      "Step: 5950, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 0.834306001663208\n",
      "Step: 5951, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 0.8810365200042725\n",
      "Step: 5952, Loss: 0.9170941710472107, Accuracy: 1.0, Computation time: 1.0721168518066406\n",
      "Step: 5953, Loss: 0.9202442169189453, Accuracy: 1.0, Computation time: 1.0502326488494873\n",
      "Step: 5954, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 0.9618422985076904\n",
      "Step: 5955, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 0.8052127361297607\n",
      "Step: 5956, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 0.9690697193145752\n",
      "Step: 5957, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 0.9324016571044922\n",
      "Step: 5958, Loss: 0.9160683155059814, Accuracy: 1.0, Computation time: 0.95810866355896\n",
      "Step: 5959, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.9348495006561279\n",
      "Step: 5960, Loss: 0.9163861870765686, Accuracy: 1.0, Computation time: 0.935382604598999\n",
      "Step: 5961, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.2781016826629639\n",
      "Step: 5962, Loss: 0.9257624745368958, Accuracy: 0.96875, Computation time: 1.214632272720337\n",
      "Step: 5963, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 0.8279552459716797\n",
      "Step: 5964, Loss: 0.9352022409439087, Accuracy: 0.96875, Computation time: 1.0737335681915283\n",
      "Step: 5965, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.0651049613952637\n",
      "Step: 5966, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 1.1324541568756104\n",
      "Step: 5967, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.4532103538513184\n",
      "Step: 5968, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.9266195297241211\n",
      "Step: 5969, Loss: 0.916059672832489, Accuracy: 1.0, Computation time: 0.9924724102020264\n",
      "Step: 5970, Loss: 0.9160948395729065, Accuracy: 1.0, Computation time: 1.2744834423065186\n",
      "Step: 5971, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 0.9368212223052979\n",
      "Step: 5972, Loss: 0.9160259962081909, Accuracy: 1.0, Computation time: 0.9538238048553467\n",
      "Step: 5973, Loss: 0.9177796244621277, Accuracy: 1.0, Computation time: 1.0643346309661865\n",
      "Step: 5974, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 0.9861304759979248\n",
      "Step: 5975, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.1392693519592285\n",
      "########################\n",
      "Test loss: 1.1248445510864258, Test Accuracy_epoch43: 0.6986175179481506\n",
      "########################\n",
      "Step: 5976, Loss: 0.9166834354400635, Accuracy: 1.0, Computation time: 1.1411983966827393\n",
      "Step: 5977, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.2517101764678955\n",
      "Step: 5978, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.2132363319396973\n",
      "Step: 5979, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.123931884765625\n",
      "Step: 5980, Loss: 0.9160391688346863, Accuracy: 1.0, Computation time: 1.0640554428100586\n",
      "Step: 5981, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.0671286582946777\n",
      "Step: 5982, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.1531422138214111\n",
      "Step: 5983, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.1858632564544678\n",
      "Step: 5984, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.1411893367767334\n",
      "Step: 5985, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.011681079864502\n",
      "Step: 5986, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 0.9148168563842773\n",
      "Step: 5987, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.0584502220153809\n",
      "Step: 5988, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 1.1969513893127441\n",
      "Step: 5989, Loss: 0.9158408045768738, Accuracy: 1.0, Computation time: 1.233583927154541\n",
      "Step: 5990, Loss: 0.9173304438591003, Accuracy: 1.0, Computation time: 1.312324047088623\n",
      "Step: 5991, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.3861122131347656\n",
      "Step: 5992, Loss: 0.9163996577262878, Accuracy: 1.0, Computation time: 1.2070717811584473\n",
      "Step: 5993, Loss: 0.9364113807678223, Accuracy: 0.96875, Computation time: 1.0647549629211426\n",
      "Step: 5994, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.270430088043213\n",
      "Step: 5995, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.114434003829956\n",
      "Step: 5996, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.3818421363830566\n",
      "Step: 5997, Loss: 0.9592198133468628, Accuracy: 0.9375, Computation time: 1.2306609153747559\n",
      "Step: 5998, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.466980218887329\n",
      "Step: 5999, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 1.0282974243164062\n",
      "Step: 6000, Loss: 0.9376118183135986, Accuracy: 0.96875, Computation time: 1.0572845935821533\n",
      "Step: 6001, Loss: 0.9369317293167114, Accuracy: 0.96875, Computation time: 1.5427806377410889\n",
      "Step: 6002, Loss: 0.9158401489257812, Accuracy: 1.0, Computation time: 1.2734646797180176\n",
      "Step: 6003, Loss: 0.915979266166687, Accuracy: 1.0, Computation time: 1.0222854614257812\n",
      "Step: 6004, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.2494397163391113\n",
      "Step: 6005, Loss: 0.9375619888305664, Accuracy: 0.96875, Computation time: 1.1324214935302734\n",
      "Step: 6006, Loss: 0.9164400696754456, Accuracy: 1.0, Computation time: 1.1896870136260986\n",
      "Step: 6007, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.0361614227294922\n",
      "Step: 6008, Loss: 0.9374348521232605, Accuracy: 0.96875, Computation time: 1.0635347366333008\n",
      "Step: 6009, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 0.9903256893157959\n",
      "Step: 6010, Loss: 0.9178006649017334, Accuracy: 1.0, Computation time: 1.2345030307769775\n",
      "Step: 6011, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.1948468685150146\n",
      "Step: 6012, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.0598368644714355\n",
      "Step: 6013, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 1.3635194301605225\n",
      "Step: 6014, Loss: 0.9160299301147461, Accuracy: 1.0, Computation time: 1.209531307220459\n",
      "Step: 6015, Loss: 0.946914792060852, Accuracy: 0.9375, Computation time: 1.4495997428894043\n",
      "Step: 6016, Loss: 0.9376881122589111, Accuracy: 0.96875, Computation time: 1.4344446659088135\n",
      "Step: 6017, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.1527349948883057\n",
      "Step: 6018, Loss: 0.9158980846405029, Accuracy: 1.0, Computation time: 1.1850242614746094\n",
      "Step: 6019, Loss: 0.9159695506095886, Accuracy: 1.0, Computation time: 1.3439912796020508\n",
      "Step: 6020, Loss: 0.9159168601036072, Accuracy: 1.0, Computation time: 1.4135420322418213\n",
      "Step: 6021, Loss: 0.9159223437309265, Accuracy: 1.0, Computation time: 1.1260416507720947\n",
      "Step: 6022, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.5313520431518555\n",
      "Step: 6023, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.1247148513793945\n",
      "Step: 6024, Loss: 0.9376331567764282, Accuracy: 0.96875, Computation time: 1.2122955322265625\n",
      "Step: 6025, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.4384472370147705\n",
      "Step: 6026, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.0781128406524658\n",
      "Step: 6027, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.1991515159606934\n",
      "Step: 6028, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.7171032428741455\n",
      "Step: 6029, Loss: 0.9376514554023743, Accuracy: 0.96875, Computation time: 0.9648380279541016\n",
      "Step: 6030, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 1.1659841537475586\n",
      "Step: 6031, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.3600244522094727\n",
      "Step: 6032, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.1321568489074707\n",
      "Step: 6033, Loss: 0.9373490810394287, Accuracy: 0.96875, Computation time: 1.1958880424499512\n",
      "Step: 6034, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.1628179550170898\n",
      "Step: 6035, Loss: 0.9158388376235962, Accuracy: 1.0, Computation time: 1.7490019798278809\n",
      "Step: 6036, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.575718879699707\n",
      "Step: 6037, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 1.3111603260040283\n",
      "Step: 6038, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.530280590057373\n",
      "Step: 6039, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.143085241317749\n",
      "Step: 6040, Loss: 0.9159381985664368, Accuracy: 1.0, Computation time: 1.4079804420471191\n",
      "Step: 6041, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.030125379562378\n",
      "Step: 6042, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.2379796504974365\n",
      "Step: 6043, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.2084968090057373\n",
      "Step: 6044, Loss: 0.9167190194129944, Accuracy: 1.0, Computation time: 1.261591911315918\n",
      "Step: 6045, Loss: 0.9158352017402649, Accuracy: 1.0, Computation time: 1.123387098312378\n",
      "Step: 6046, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.123474359512329\n",
      "Step: 6047, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.3428282737731934\n",
      "Step: 6048, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.1306073665618896\n",
      "Step: 6049, Loss: 0.9159343838691711, Accuracy: 1.0, Computation time: 1.1973049640655518\n",
      "Step: 6050, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 1.3750855922698975\n",
      "Step: 6051, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.3147788047790527\n",
      "Step: 6052, Loss: 0.916390061378479, Accuracy: 1.0, Computation time: 1.2409658432006836\n",
      "Step: 6053, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.2447199821472168\n",
      "Step: 6054, Loss: 0.9161547422409058, Accuracy: 1.0, Computation time: 1.459669589996338\n",
      "Step: 6055, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.1328761577606201\n",
      "Step: 6056, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.2442891597747803\n",
      "Step: 6057, Loss: 0.9360993504524231, Accuracy: 0.96875, Computation time: 1.4158682823181152\n",
      "Step: 6058, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.2964465618133545\n",
      "Step: 6059, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.2763009071350098\n",
      "Step: 6060, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 1.2640047073364258\n",
      "Step: 6061, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.1327815055847168\n",
      "Step: 6062, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.2003610134124756\n",
      "Step: 6063, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.1782252788543701\n",
      "Step: 6064, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.175689458847046\n",
      "Step: 6065, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.332789659500122\n",
      "Step: 6066, Loss: 0.9158521294593811, Accuracy: 1.0, Computation time: 1.4437057971954346\n",
      "Step: 6067, Loss: 0.9182308912277222, Accuracy: 1.0, Computation time: 1.5458121299743652\n",
      "Step: 6068, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.3245041370391846\n",
      "Step: 6069, Loss: 0.9160272479057312, Accuracy: 1.0, Computation time: 1.2894580364227295\n",
      "Step: 6070, Loss: 0.915903627872467, Accuracy: 1.0, Computation time: 1.225531816482544\n",
      "Step: 6071, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 1.3282811641693115\n",
      "Step: 6072, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.147740364074707\n",
      "Step: 6073, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 1.2688162326812744\n",
      "Step: 6074, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.1401691436767578\n",
      "Step: 6075, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.0062665939331055\n",
      "Step: 6076, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 1.2601361274719238\n",
      "Step: 6077, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.057856798171997\n",
      "Step: 6078, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.4672887325286865\n",
      "Step: 6079, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 1.284541130065918\n",
      "Step: 6080, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.4885766506195068\n",
      "Step: 6081, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.2958550453186035\n",
      "Step: 6082, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.636397123336792\n",
      "Step: 6083, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.6191039085388184\n",
      "Step: 6084, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.3213615417480469\n",
      "Step: 6085, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.1871943473815918\n",
      "Step: 6086, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.5778734683990479\n",
      "Step: 6087, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.461778163909912\n",
      "Step: 6088, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 1.087203025817871\n",
      "Step: 6089, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 1.569239854812622\n",
      "Step: 6090, Loss: 0.9543696641921997, Accuracy: 0.9375, Computation time: 1.0947184562683105\n",
      "Step: 6091, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.261070966720581\n",
      "Step: 6092, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.2846465110778809\n",
      "Step: 6093, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 1.1150462627410889\n",
      "Step: 6094, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.2651853561401367\n",
      "Step: 6095, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.3460907936096191\n",
      "Step: 6096, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.152409553527832\n",
      "Step: 6097, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 1.2151663303375244\n",
      "Step: 6098, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.12330961227417\n",
      "Step: 6099, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 1.4952619075775146\n",
      "Step: 6100, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.467677116394043\n",
      "Step: 6101, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 1.7336478233337402\n",
      "Step: 6102, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.451200246810913\n",
      "Step: 6103, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.5382254123687744\n",
      "Step: 6104, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.313356876373291\n",
      "Step: 6105, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.2216973304748535\n",
      "Step: 6106, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 1.447202205657959\n",
      "Step: 6107, Loss: 0.9375705122947693, Accuracy: 0.96875, Computation time: 1.5220212936401367\n",
      "Step: 6108, Loss: 0.9158357381820679, Accuracy: 1.0, Computation time: 1.724668264389038\n",
      "Step: 6109, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 1.256476879119873\n",
      "Step: 6110, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.3559074401855469\n",
      "Step: 6111, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 1.2164323329925537\n",
      "Step: 6112, Loss: 0.9337708353996277, Accuracy: 0.96875, Computation time: 1.0757298469543457\n",
      "Step: 6113, Loss: 0.9159523248672485, Accuracy: 1.0, Computation time: 1.1911776065826416\n",
      "Step: 6114, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 1.0111620426177979\n",
      "########################\n",
      "Test loss: 1.1219451427459717, Test Accuracy_epoch44: 0.6967741847038269\n",
      "########################\n",
      "Step: 6115, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.1358320713043213\n",
      "Step: 6116, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.540320873260498\n",
      "Step: 6117, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 0.9536013603210449\n",
      "Step: 6118, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 0.9306247234344482\n",
      "Step: 6119, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 0.9661328792572021\n",
      "Step: 6120, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.160163164138794\n",
      "Step: 6121, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.4747307300567627\n",
      "Step: 6122, Loss: 0.9278698563575745, Accuracy: 0.96875, Computation time: 1.4596166610717773\n",
      "Step: 6123, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.6487250328063965\n",
      "Step: 6124, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.3017058372497559\n",
      "Step: 6125, Loss: 0.9191116094589233, Accuracy: 1.0, Computation time: 1.481856346130371\n",
      "Step: 6126, Loss: 0.9159697890281677, Accuracy: 1.0, Computation time: 1.2244672775268555\n",
      "Step: 6127, Loss: 0.9160082340240479, Accuracy: 1.0, Computation time: 1.5349600315093994\n",
      "Step: 6128, Loss: 0.915931761264801, Accuracy: 1.0, Computation time: 1.2673468589782715\n",
      "Step: 6129, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.264693021774292\n",
      "Step: 6130, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.1417334079742432\n",
      "Step: 6131, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.3671815395355225\n",
      "Step: 6132, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.127812385559082\n",
      "Step: 6133, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 1.4518043994903564\n",
      "Step: 6134, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.4131524562835693\n",
      "Step: 6135, Loss: 0.9165393114089966, Accuracy: 1.0, Computation time: 1.401550531387329\n",
      "Step: 6136, Loss: 0.9161554574966431, Accuracy: 1.0, Computation time: 1.427966594696045\n",
      "Step: 6137, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.3941915035247803\n",
      "Step: 6138, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.2635369300842285\n",
      "Step: 6139, Loss: 0.9357142448425293, Accuracy: 0.96875, Computation time: 1.368797779083252\n",
      "Step: 6140, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 1.245042085647583\n",
      "Step: 6141, Loss: 0.9190506339073181, Accuracy: 1.0, Computation time: 1.3907477855682373\n",
      "Step: 6142, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.1308934688568115\n",
      "Step: 6143, Loss: 0.9159508347511292, Accuracy: 1.0, Computation time: 1.1958532333374023\n",
      "Step: 6144, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 0.9956111907958984\n",
      "Step: 6145, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 1.2517931461334229\n",
      "Step: 6146, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 1.112720251083374\n",
      "Step: 6147, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.2568151950836182\n",
      "Step: 6148, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 1.502535343170166\n",
      "Step: 6149, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 1.6044037342071533\n",
      "Step: 6150, Loss: 0.9159805178642273, Accuracy: 1.0, Computation time: 1.1708886623382568\n",
      "Step: 6151, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.7129933834075928\n",
      "Step: 6152, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.3218863010406494\n",
      "Step: 6153, Loss: 0.916070282459259, Accuracy: 1.0, Computation time: 1.7108368873596191\n",
      "Step: 6154, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.6856052875518799\n",
      "Step: 6155, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.6212518215179443\n",
      "Step: 6156, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.212946891784668\n",
      "Step: 6157, Loss: 0.9376281499862671, Accuracy: 0.96875, Computation time: 1.651158332824707\n",
      "Step: 6158, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.358870506286621\n",
      "Step: 6159, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.5942962169647217\n",
      "Step: 6160, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.4859063625335693\n",
      "Step: 6161, Loss: 0.9375154972076416, Accuracy: 0.96875, Computation time: 1.0872797966003418\n",
      "Step: 6162, Loss: 0.9160073399543762, Accuracy: 1.0, Computation time: 1.5519301891326904\n",
      "Step: 6163, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.6231701374053955\n",
      "Step: 6164, Loss: 0.9376878142356873, Accuracy: 0.96875, Computation time: 1.2162327766418457\n",
      "Step: 6165, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.3040618896484375\n",
      "Step: 6166, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.1771650314331055\n",
      "Step: 6167, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.3319015502929688\n",
      "Step: 6168, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 1.349898338317871\n",
      "Step: 6169, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.7494988441467285\n",
      "Step: 6170, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.3181109428405762\n",
      "Step: 6171, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.1910474300384521\n",
      "Step: 6172, Loss: 0.9159244298934937, Accuracy: 1.0, Computation time: 1.224926233291626\n",
      "Step: 6173, Loss: 0.9158458113670349, Accuracy: 1.0, Computation time: 1.2125968933105469\n",
      "Step: 6174, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 1.2581746578216553\n",
      "Step: 6175, Loss: 0.9313789010047913, Accuracy: 0.96875, Computation time: 1.3042151927947998\n",
      "Step: 6176, Loss: 0.9592841267585754, Accuracy: 0.9375, Computation time: 1.171708345413208\n",
      "Step: 6177, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.6582977771759033\n",
      "Step: 6178, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.744292974472046\n",
      "Step: 6179, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.1571505069732666\n",
      "Step: 6180, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.163599967956543\n",
      "Step: 6181, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.1646461486816406\n",
      "Step: 6182, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 1.1897990703582764\n",
      "Step: 6183, Loss: 0.9184684157371521, Accuracy: 1.0, Computation time: 1.8868098258972168\n",
      "Step: 6184, Loss: 0.9158400297164917, Accuracy: 1.0, Computation time: 1.4534389972686768\n",
      "Step: 6185, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.1685385704040527\n",
      "Step: 6186, Loss: 0.9342166781425476, Accuracy: 0.96875, Computation time: 1.1450865268707275\n",
      "Step: 6187, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.0600128173828125\n",
      "Step: 6188, Loss: 0.9216230511665344, Accuracy: 1.0, Computation time: 1.6542394161224365\n",
      "Step: 6189, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 1.5525908470153809\n",
      "Step: 6190, Loss: 0.9159509539604187, Accuracy: 1.0, Computation time: 1.0297694206237793\n",
      "Step: 6191, Loss: 0.9159659743309021, Accuracy: 1.0, Computation time: 1.480396032333374\n",
      "Step: 6192, Loss: 0.9159405827522278, Accuracy: 1.0, Computation time: 1.2447376251220703\n",
      "Step: 6193, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.6841773986816406\n",
      "Step: 6194, Loss: 0.9159843921661377, Accuracy: 1.0, Computation time: 1.2359459400177002\n",
      "Step: 6195, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.1224205493927002\n",
      "Step: 6196, Loss: 0.9160388708114624, Accuracy: 1.0, Computation time: 1.2985267639160156\n",
      "Step: 6197, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.4001667499542236\n",
      "Step: 6198, Loss: 0.9158998727798462, Accuracy: 1.0, Computation time: 0.9803829193115234\n",
      "Step: 6199, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.3372316360473633\n",
      "Step: 6200, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 1.0118789672851562\n",
      "Step: 6201, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 1.0541479587554932\n",
      "Step: 6202, Loss: 0.9159731864929199, Accuracy: 1.0, Computation time: 1.2728700637817383\n",
      "Step: 6203, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.5651206970214844\n",
      "Step: 6204, Loss: 0.9160691499710083, Accuracy: 1.0, Computation time: 1.233299732208252\n",
      "Step: 6205, Loss: 0.9158347249031067, Accuracy: 1.0, Computation time: 1.0604877471923828\n",
      "Step: 6206, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.408386468887329\n",
      "Step: 6207, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.3430910110473633\n",
      "Step: 6208, Loss: 0.9805630445480347, Accuracy: 0.90625, Computation time: 1.5703339576721191\n",
      "Step: 6209, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.521026849746704\n",
      "Step: 6210, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.5873587131500244\n",
      "Step: 6211, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 1.6238341331481934\n",
      "Step: 6212, Loss: 0.9160212278366089, Accuracy: 1.0, Computation time: 1.430706262588501\n",
      "Step: 6213, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.9342124462127686\n",
      "Step: 6214, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.0680229663848877\n",
      "Step: 6215, Loss: 0.9375585317611694, Accuracy: 0.96875, Computation time: 1.1072256565093994\n",
      "Step: 6216, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.0791034698486328\n",
      "Step: 6217, Loss: 0.915838360786438, Accuracy: 1.0, Computation time: 1.2568917274475098\n",
      "Step: 6218, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.288360357284546\n",
      "Step: 6219, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 1.2475357055664062\n",
      "Step: 6220, Loss: 0.9234253168106079, Accuracy: 1.0, Computation time: 1.3560912609100342\n",
      "Step: 6221, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.1067793369293213\n",
      "Step: 6222, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.1651129722595215\n",
      "Step: 6223, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 0.9623563289642334\n",
      "Step: 6224, Loss: 0.915951669216156, Accuracy: 1.0, Computation time: 1.1789417266845703\n",
      "Step: 6225, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.4550886154174805\n",
      "Step: 6226, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.0498716831207275\n",
      "Step: 6227, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.3817484378814697\n",
      "Step: 6228, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 1.3774628639221191\n",
      "Step: 6229, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.2931952476501465\n",
      "Step: 6230, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.4365880489349365\n",
      "Step: 6231, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.4819214344024658\n",
      "Step: 6232, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.6142292022705078\n",
      "Step: 6233, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.150639295578003\n",
      "Step: 6234, Loss: 0.9376496076583862, Accuracy: 0.96875, Computation time: 1.1970012187957764\n",
      "Step: 6235, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.4916884899139404\n",
      "Step: 6236, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.352226972579956\n",
      "Step: 6237, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.1238291263580322\n",
      "Step: 6238, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.603257656097412\n",
      "Step: 6239, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.3233363628387451\n",
      "Step: 6240, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 1.4042577743530273\n",
      "Step: 6241, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.3348803520202637\n",
      "Step: 6242, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.3264591693878174\n",
      "Step: 6243, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.9654784202575684\n",
      "Step: 6244, Loss: 0.9375888109207153, Accuracy: 0.96875, Computation time: 1.2299396991729736\n",
      "Step: 6245, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.421370267868042\n",
      "Step: 6246, Loss: 0.9376777410507202, Accuracy: 0.96875, Computation time: 1.169417381286621\n",
      "Step: 6247, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 1.462965488433838\n",
      "Step: 6248, Loss: 0.9187512397766113, Accuracy: 1.0, Computation time: 1.5934486389160156\n",
      "Step: 6249, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.1377527713775635\n",
      "Step: 6250, Loss: 0.916862428188324, Accuracy: 1.0, Computation time: 1.3036575317382812\n",
      "Step: 6251, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.424901008605957\n",
      "Step: 6252, Loss: 0.9160788059234619, Accuracy: 1.0, Computation time: 1.2959868907928467\n",
      "Step: 6253, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.7952163219451904\n",
      "########################\n",
      "Test loss: 1.1218435764312744, Test Accuracy_epoch45: 0.7013825178146362\n",
      "########################\n",
      "Step: 6254, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 1.343714714050293\n",
      "Step: 6255, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 1.226869821548462\n",
      "Step: 6256, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.69425368309021\n",
      "Step: 6257, Loss: 0.9177838563919067, Accuracy: 1.0, Computation time: 2.0836548805236816\n",
      "Step: 6258, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.02471923828125\n",
      "Step: 6259, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.299713373184204\n",
      "Step: 6260, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.2489049434661865\n",
      "Step: 6261, Loss: 0.928091824054718, Accuracy: 0.96875, Computation time: 1.3983564376831055\n",
      "Step: 6262, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.4063935279846191\n",
      "Step: 6263, Loss: 0.9337626695632935, Accuracy: 0.96875, Computation time: 1.1610002517700195\n",
      "Step: 6264, Loss: 0.9159833192825317, Accuracy: 1.0, Computation time: 1.2357232570648193\n",
      "Step: 6265, Loss: 0.9162676930427551, Accuracy: 1.0, Computation time: 1.554938793182373\n",
      "Step: 6266, Loss: 0.9159179329872131, Accuracy: 1.0, Computation time: 1.2509682178497314\n",
      "Step: 6267, Loss: 0.915961742401123, Accuracy: 1.0, Computation time: 1.877648115158081\n",
      "Step: 6268, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.5434691905975342\n",
      "Step: 6269, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.6805341243743896\n",
      "Step: 6270, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.4716057777404785\n",
      "Step: 6271, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.1818437576293945\n",
      "Step: 6272, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.2809288501739502\n",
      "Step: 6273, Loss: 0.9160265326499939, Accuracy: 1.0, Computation time: 1.1181631088256836\n",
      "Step: 6274, Loss: 0.9368175268173218, Accuracy: 0.96875, Computation time: 1.9335966110229492\n",
      "Step: 6275, Loss: 0.9181807637214661, Accuracy: 1.0, Computation time: 1.3652186393737793\n",
      "Step: 6276, Loss: 0.9159600138664246, Accuracy: 1.0, Computation time: 1.3677396774291992\n",
      "Step: 6277, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.3818936347961426\n",
      "Step: 6278, Loss: 0.9375897645950317, Accuracy: 0.96875, Computation time: 1.498852252960205\n",
      "Step: 6279, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.4724969863891602\n",
      "Step: 6280, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.5238661766052246\n",
      "Step: 6281, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.2159361839294434\n",
      "Step: 6282, Loss: 0.9159888625144958, Accuracy: 1.0, Computation time: 1.2845795154571533\n",
      "Step: 6283, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.2759718894958496\n",
      "Step: 6284, Loss: 0.9159852862358093, Accuracy: 1.0, Computation time: 1.381115198135376\n",
      "Step: 6285, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.314344882965088\n",
      "Step: 6286, Loss: 0.9159929156303406, Accuracy: 1.0, Computation time: 1.4470341205596924\n",
      "Step: 6287, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.1420581340789795\n",
      "Step: 6288, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.3978004455566406\n",
      "Step: 6289, Loss: 0.9345192909240723, Accuracy: 0.96875, Computation time: 1.2612123489379883\n",
      "Step: 6290, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.059016227722168\n",
      "Step: 6291, Loss: 0.9159359335899353, Accuracy: 1.0, Computation time: 1.4176695346832275\n",
      "Step: 6292, Loss: 0.9159698486328125, Accuracy: 1.0, Computation time: 1.3980283737182617\n",
      "Step: 6293, Loss: 0.9159805774688721, Accuracy: 1.0, Computation time: 1.0438976287841797\n",
      "Step: 6294, Loss: 0.9159545302391052, Accuracy: 1.0, Computation time: 1.4257819652557373\n",
      "Step: 6295, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 1.098968267440796\n",
      "Step: 6296, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.3322036266326904\n",
      "Step: 6297, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.1863110065460205\n",
      "Step: 6298, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.567307472229004\n",
      "Step: 6299, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.2117314338684082\n",
      "Step: 6300, Loss: 0.9380196332931519, Accuracy: 0.96875, Computation time: 1.3792243003845215\n",
      "Step: 6301, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.180201768875122\n",
      "Step: 6302, Loss: 0.9327676892280579, Accuracy: 0.96875, Computation time: 1.1284618377685547\n",
      "Step: 6303, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 1.1835298538208008\n",
      "Step: 6304, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.3101518154144287\n",
      "Step: 6305, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.361987829208374\n",
      "Step: 6306, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.274505853652954\n",
      "Step: 6307, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.3185029029846191\n",
      "Step: 6308, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.1804723739624023\n",
      "Step: 6309, Loss: 0.9176363348960876, Accuracy: 1.0, Computation time: 1.4571387767791748\n",
      "Step: 6310, Loss: 0.937512993812561, Accuracy: 0.96875, Computation time: 1.5313398838043213\n",
      "Step: 6311, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.2249279022216797\n",
      "Step: 6312, Loss: 0.9159896969795227, Accuracy: 1.0, Computation time: 1.2922823429107666\n",
      "Step: 6313, Loss: 0.9159409999847412, Accuracy: 1.0, Computation time: 1.2787580490112305\n",
      "Step: 6314, Loss: 0.915950357913971, Accuracy: 1.0, Computation time: 1.2890615463256836\n",
      "Step: 6315, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.1583380699157715\n",
      "Step: 6316, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.0813534259796143\n",
      "Step: 6317, Loss: 0.937351405620575, Accuracy: 0.96875, Computation time: 1.8073103427886963\n",
      "Step: 6318, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.4037413597106934\n",
      "Step: 6319, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.3091356754302979\n",
      "Step: 6320, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 1.2084159851074219\n",
      "Step: 6321, Loss: 0.9259060621261597, Accuracy: 0.96875, Computation time: 1.4641380310058594\n",
      "Step: 6322, Loss: 0.9159641265869141, Accuracy: 1.0, Computation time: 1.7200539112091064\n",
      "Step: 6323, Loss: 0.9161041975021362, Accuracy: 1.0, Computation time: 1.2186009883880615\n",
      "Step: 6324, Loss: 0.9160352349281311, Accuracy: 1.0, Computation time: 1.5588643550872803\n",
      "Step: 6325, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.2072429656982422\n",
      "Step: 6326, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.0362458229064941\n",
      "Step: 6327, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 1.4418084621429443\n",
      "Step: 6328, Loss: 0.9170328974723816, Accuracy: 1.0, Computation time: 1.1140458583831787\n",
      "Step: 6329, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 1.1223618984222412\n",
      "Step: 6330, Loss: 0.9160146117210388, Accuracy: 1.0, Computation time: 1.0675902366638184\n",
      "Step: 6331, Loss: 0.9366888403892517, Accuracy: 0.96875, Computation time: 1.1834449768066406\n",
      "Step: 6332, Loss: 0.9312322735786438, Accuracy: 0.96875, Computation time: 1.6201808452606201\n",
      "Step: 6333, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 1.291412591934204\n",
      "Step: 6334, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.1008105278015137\n",
      "Step: 6335, Loss: 0.9160597920417786, Accuracy: 1.0, Computation time: 1.3612582683563232\n",
      "Step: 6336, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.304168462753296\n",
      "Step: 6337, Loss: 0.915986955165863, Accuracy: 1.0, Computation time: 1.4935815334320068\n",
      "Step: 6338, Loss: 0.915998637676239, Accuracy: 1.0, Computation time: 1.3572571277618408\n",
      "Step: 6339, Loss: 0.916067898273468, Accuracy: 1.0, Computation time: 1.376739740371704\n",
      "Step: 6340, Loss: 0.9160088300704956, Accuracy: 1.0, Computation time: 1.2896671295166016\n",
      "Step: 6341, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.4035377502441406\n",
      "Step: 6342, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.2032184600830078\n",
      "Step: 6343, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 1.3216643333435059\n",
      "Step: 6344, Loss: 0.9160827994346619, Accuracy: 1.0, Computation time: 1.3773651123046875\n",
      "Step: 6345, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.1987206935882568\n",
      "Step: 6346, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.199812650680542\n",
      "Step: 6347, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 1.4856719970703125\n",
      "Step: 6348, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.5903174877166748\n",
      "Step: 6349, Loss: 0.9159482717514038, Accuracy: 1.0, Computation time: 1.406522274017334\n",
      "Step: 6350, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.9046542644500732\n",
      "Step: 6351, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.8728878498077393\n",
      "Step: 6352, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.4258921146392822\n",
      "Step: 6353, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.388422966003418\n",
      "Step: 6354, Loss: 0.9158541560173035, Accuracy: 1.0, Computation time: 1.3464503288269043\n",
      "Step: 6355, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.329780101776123\n",
      "Step: 6356, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.1774547100067139\n",
      "Step: 6357, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.367539405822754\n",
      "Step: 6358, Loss: 0.9158712029457092, Accuracy: 1.0, Computation time: 1.2958590984344482\n",
      "Step: 6359, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 1.378882646560669\n",
      "Step: 6360, Loss: 0.9158441424369812, Accuracy: 1.0, Computation time: 1.4925589561462402\n",
      "Step: 6361, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.2816722393035889\n",
      "Step: 6362, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.368532657623291\n",
      "Step: 6363, Loss: 0.916097104549408, Accuracy: 1.0, Computation time: 1.5300862789154053\n",
      "Step: 6364, Loss: 0.9376450777053833, Accuracy: 0.96875, Computation time: 1.134589672088623\n",
      "Step: 6365, Loss: 0.9158381819725037, Accuracy: 1.0, Computation time: 1.3332808017730713\n",
      "Step: 6366, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.6494226455688477\n",
      "Step: 6367, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.168396234512329\n",
      "Step: 6368, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.221421480178833\n",
      "Step: 6369, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 1.4834885597229004\n",
      "Step: 6370, Loss: 0.919631838798523, Accuracy: 1.0, Computation time: 1.6774938106536865\n",
      "Step: 6371, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.6718018054962158\n",
      "Step: 6372, Loss: 0.9159667491912842, Accuracy: 1.0, Computation time: 1.2955169677734375\n",
      "Step: 6373, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.3919975757598877\n",
      "Step: 6374, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.3708207607269287\n",
      "Step: 6375, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 1.8419206142425537\n",
      "Step: 6376, Loss: 0.9159750938415527, Accuracy: 1.0, Computation time: 1.4633433818817139\n",
      "Step: 6377, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.4004976749420166\n",
      "Step: 6378, Loss: 0.915963351726532, Accuracy: 1.0, Computation time: 1.4236462116241455\n",
      "Step: 6379, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.3539581298828125\n",
      "Step: 6380, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.0556492805480957\n",
      "Step: 6381, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.7047476768493652\n",
      "Step: 6382, Loss: 0.9297616481781006, Accuracy: 0.96875, Computation time: 1.2118098735809326\n",
      "Step: 6383, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.473494291305542\n",
      "Step: 6384, Loss: 0.916278064250946, Accuracy: 1.0, Computation time: 1.3464324474334717\n",
      "Step: 6385, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 1.062288761138916\n",
      "Step: 6386, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 1.0541930198669434\n",
      "Step: 6387, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.0655796527862549\n",
      "Step: 6388, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.1290416717529297\n",
      "Step: 6389, Loss: 0.9176381230354309, Accuracy: 1.0, Computation time: 1.8268887996673584\n",
      "Step: 6390, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.129833698272705\n",
      "Step: 6391, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.0453083515167236\n",
      "Step: 6392, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 0.9772965908050537\n",
      "########################\n",
      "Test loss: 1.1271193027496338, Test Accuracy_epoch46: 0.6921659111976624\n",
      "########################\n",
      "Step: 6393, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.1244518756866455\n",
      "Step: 6394, Loss: 0.9160309433937073, Accuracy: 1.0, Computation time: 1.3139104843139648\n",
      "Step: 6395, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 1.479457139968872\n",
      "Step: 6396, Loss: 0.925021231174469, Accuracy: 1.0, Computation time: 1.6549413204193115\n",
      "Step: 6397, Loss: 0.9246000051498413, Accuracy: 1.0, Computation time: 1.326322317123413\n",
      "Step: 6398, Loss: 0.916041374206543, Accuracy: 1.0, Computation time: 1.3135054111480713\n",
      "Step: 6399, Loss: 0.9161279201507568, Accuracy: 1.0, Computation time: 1.1711311340332031\n",
      "Step: 6400, Loss: 0.9161262512207031, Accuracy: 1.0, Computation time: 1.1498351097106934\n",
      "Step: 6401, Loss: 0.9160720705986023, Accuracy: 1.0, Computation time: 1.149017333984375\n",
      "Step: 6402, Loss: 0.9165407419204712, Accuracy: 1.0, Computation time: 1.4265942573547363\n",
      "Step: 6403, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 0.958625078201294\n",
      "Step: 6404, Loss: 0.915998637676239, Accuracy: 1.0, Computation time: 1.2383134365081787\n",
      "Step: 6405, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.0224525928497314\n",
      "Step: 6406, Loss: 0.9159247279167175, Accuracy: 1.0, Computation time: 1.1093289852142334\n",
      "Step: 6407, Loss: 0.9159799814224243, Accuracy: 1.0, Computation time: 1.1144213676452637\n",
      "Step: 6408, Loss: 0.9598153233528137, Accuracy: 0.9375, Computation time: 1.8624589443206787\n",
      "Step: 6409, Loss: 0.9160489439964294, Accuracy: 1.0, Computation time: 1.1345758438110352\n",
      "Step: 6410, Loss: 0.9375419020652771, Accuracy: 0.96875, Computation time: 1.0485498905181885\n",
      "Step: 6411, Loss: 0.9159855246543884, Accuracy: 1.0, Computation time: 0.9379696846008301\n",
      "Step: 6412, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 0.9563286304473877\n",
      "Step: 6413, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 1.2716014385223389\n",
      "Step: 6414, Loss: 0.9177320003509521, Accuracy: 1.0, Computation time: 0.9945425987243652\n",
      "Step: 6415, Loss: 0.9165711402893066, Accuracy: 1.0, Computation time: 1.363940954208374\n",
      "Step: 6416, Loss: 0.93260258436203, Accuracy: 0.96875, Computation time: 1.8047258853912354\n",
      "Step: 6417, Loss: 0.9161937832832336, Accuracy: 1.0, Computation time: 1.386652946472168\n",
      "Step: 6418, Loss: 0.9589827060699463, Accuracy: 0.9375, Computation time: 1.5960755348205566\n",
      "Step: 6419, Loss: 0.9160600900650024, Accuracy: 1.0, Computation time: 1.186439037322998\n",
      "Step: 6420, Loss: 0.9160319566726685, Accuracy: 1.0, Computation time: 1.032057762145996\n",
      "Step: 6421, Loss: 0.91600102186203, Accuracy: 1.0, Computation time: 0.96781325340271\n",
      "Step: 6422, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 1.5804712772369385\n",
      "Step: 6423, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 0.9806427955627441\n",
      "Step: 6424, Loss: 0.9403141736984253, Accuracy: 0.96875, Computation time: 1.344377040863037\n",
      "Step: 6425, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.1215035915374756\n",
      "Step: 6426, Loss: 0.9161091446876526, Accuracy: 1.0, Computation time: 1.1686396598815918\n",
      "Step: 6427, Loss: 0.9362539649009705, Accuracy: 0.96875, Computation time: 1.106318712234497\n",
      "Step: 6428, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.374068260192871\n",
      "Step: 6429, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.0178484916687012\n",
      "Step: 6430, Loss: 0.9375163912773132, Accuracy: 0.96875, Computation time: 1.1510255336761475\n",
      "Step: 6431, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 1.0740771293640137\n",
      "Step: 6432, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 1.7318246364593506\n",
      "Step: 6433, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.068464994430542\n",
      "Step: 6434, Loss: 0.9159186482429504, Accuracy: 1.0, Computation time: 0.9993340969085693\n",
      "Step: 6435, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.4195382595062256\n",
      "Step: 6436, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.270704984664917\n",
      "Step: 6437, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.173987627029419\n",
      "Step: 6438, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.4076800346374512\n",
      "Step: 6439, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.209754228591919\n",
      "Step: 6440, Loss: 0.9241971969604492, Accuracy: 1.0, Computation time: 1.1397395133972168\n",
      "Step: 6441, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.400374412536621\n",
      "Step: 6442, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 1.209813117980957\n",
      "Step: 6443, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 0.9934399127960205\n",
      "Step: 6444, Loss: 0.9160112142562866, Accuracy: 1.0, Computation time: 0.8540771007537842\n",
      "Step: 6445, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 0.9955265522003174\n",
      "Step: 6446, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 0.935164213180542\n",
      "Step: 6447, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.1905333995819092\n",
      "Step: 6448, Loss: 0.9376312494277954, Accuracy: 0.96875, Computation time: 1.1114692687988281\n",
      "Step: 6449, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.274062156677246\n",
      "Step: 6450, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.1488564014434814\n",
      "Step: 6451, Loss: 0.9370070695877075, Accuracy: 0.96875, Computation time: 1.4206619262695312\n",
      "Step: 6452, Loss: 0.9159355759620667, Accuracy: 1.0, Computation time: 1.141922950744629\n",
      "Step: 6453, Loss: 0.916109561920166, Accuracy: 1.0, Computation time: 1.1461420059204102\n",
      "Step: 6454, Loss: 0.9159709215164185, Accuracy: 1.0, Computation time: 1.1011083126068115\n",
      "Step: 6455, Loss: 0.9159994721412659, Accuracy: 1.0, Computation time: 1.070350170135498\n",
      "Step: 6456, Loss: 0.9159777760505676, Accuracy: 1.0, Computation time: 1.2201035022735596\n",
      "Step: 6457, Loss: 0.9375848174095154, Accuracy: 0.96875, Computation time: 1.7423453330993652\n",
      "Step: 6458, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.0105197429656982\n",
      "Step: 6459, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.2915544509887695\n",
      "Step: 6460, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 1.134641408920288\n",
      "Step: 6461, Loss: 0.9160578846931458, Accuracy: 1.0, Computation time: 1.005164623260498\n",
      "Step: 6462, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 0.9975523948669434\n",
      "Step: 6463, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 0.9238982200622559\n",
      "Step: 6464, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 0.9552478790283203\n",
      "Step: 6465, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 0.8832454681396484\n",
      "Step: 6466, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.2898211479187012\n",
      "Step: 6467, Loss: 0.9590369462966919, Accuracy: 0.9375, Computation time: 1.133878231048584\n",
      "Step: 6468, Loss: 0.916120171546936, Accuracy: 1.0, Computation time: 1.0152735710144043\n",
      "Step: 6469, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.1182010173797607\n",
      "Step: 6470, Loss: 0.9159976243972778, Accuracy: 1.0, Computation time: 1.3244733810424805\n",
      "Step: 6471, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.1445808410644531\n",
      "Step: 6472, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 0.8979647159576416\n",
      "Step: 6473, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.0434751510620117\n",
      "Step: 6474, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.3069453239440918\n",
      "Step: 6475, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.1670405864715576\n",
      "Step: 6476, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 0.9840221405029297\n",
      "Step: 6477, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 0.9216194152832031\n",
      "Step: 6478, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.2100918292999268\n",
      "Step: 6479, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 0.8745884895324707\n",
      "Step: 6480, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 0.9345824718475342\n",
      "Step: 6481, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.3850796222686768\n",
      "Step: 6482, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.0641169548034668\n",
      "Step: 6483, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 0.9315567016601562\n",
      "Step: 6484, Loss: 0.9376021027565002, Accuracy: 0.96875, Computation time: 1.0876753330230713\n",
      "Step: 6485, Loss: 0.9375377297401428, Accuracy: 0.96875, Computation time: 0.8538956642150879\n",
      "Step: 6486, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.1644761562347412\n",
      "Step: 6487, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.1303319931030273\n",
      "Step: 6488, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.6841156482696533\n",
      "Step: 6489, Loss: 0.9313485622406006, Accuracy: 0.96875, Computation time: 0.9912066459655762\n",
      "Step: 6490, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.0285911560058594\n",
      "Step: 6491, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.9568836688995361\n",
      "Step: 6492, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 0.8823106288909912\n",
      "Step: 6493, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 0.9965074062347412\n",
      "Step: 6494, Loss: 0.9159567952156067, Accuracy: 1.0, Computation time: 1.0187492370605469\n",
      "Step: 6495, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.0425662994384766\n",
      "Step: 6496, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.1408898830413818\n",
      "Step: 6497, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 1.0179393291473389\n",
      "Step: 6498, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 0.9872744083404541\n",
      "Step: 6499, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.0309362411499023\n",
      "Step: 6500, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.0808076858520508\n",
      "Step: 6501, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 0.9893255233764648\n",
      "Step: 6502, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.0650529861450195\n",
      "Step: 6503, Loss: 0.9376131892204285, Accuracy: 0.96875, Computation time: 0.9160552024841309\n",
      "Step: 6504, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 1.1367063522338867\n",
      "Step: 6505, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 0.9950735569000244\n",
      "Step: 6506, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 0.968869686126709\n",
      "Step: 6507, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.238030195236206\n",
      "Step: 6508, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 0.9293453693389893\n",
      "Step: 6509, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.0681989192962646\n",
      "Step: 6510, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 0.9560017585754395\n",
      "Step: 6511, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 0.9643170833587646\n",
      "Step: 6512, Loss: 0.9378002882003784, Accuracy: 0.96875, Computation time: 1.3952982425689697\n",
      "Step: 6513, Loss: 0.93739914894104, Accuracy: 0.96875, Computation time: 0.9194560050964355\n",
      "Step: 6514, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 0.9394419193267822\n",
      "Step: 6515, Loss: 0.9173024892807007, Accuracy: 1.0, Computation time: 0.9657669067382812\n",
      "Step: 6516, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.0499317646026611\n",
      "Step: 6517, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.7571580410003662\n",
      "Step: 6518, Loss: 0.9369539618492126, Accuracy: 0.96875, Computation time: 1.132795810699463\n",
      "Step: 6519, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 0.9086003303527832\n",
      "Step: 6520, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.199652910232544\n",
      "Step: 6521, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.1132783889770508\n",
      "Step: 6522, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.361896276473999\n",
      "Step: 6523, Loss: 0.9376165270805359, Accuracy: 0.96875, Computation time: 1.1208817958831787\n",
      "Step: 6524, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 0.9728147983551025\n",
      "Step: 6525, Loss: 0.915946364402771, Accuracy: 1.0, Computation time: 0.988642692565918\n",
      "Step: 6526, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 0.9623026847839355\n",
      "Step: 6527, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.596355676651001\n",
      "Step: 6528, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.1563138961791992\n",
      "Step: 6529, Loss: 0.9375625252723694, Accuracy: 0.96875, Computation time: 1.0794246196746826\n",
      "Step: 6530, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.6882615089416504\n",
      "Step: 6531, Loss: 0.9163146018981934, Accuracy: 1.0, Computation time: 0.9192831516265869\n",
      "########################\n",
      "Test loss: 1.1207267045974731, Test Accuracy_epoch47: 0.7050691246986389\n",
      "########################\n",
      "Step: 6532, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.060321569442749\n",
      "Step: 6533, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 0.924903392791748\n",
      "Step: 6534, Loss: 0.9159389734268188, Accuracy: 1.0, Computation time: 1.0147991180419922\n",
      "Step: 6535, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.1545557975769043\n",
      "Step: 6536, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.1427099704742432\n",
      "Step: 6537, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.1773650646209717\n",
      "Step: 6538, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 0.8777449131011963\n",
      "Step: 6539, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.0068366527557373\n",
      "Step: 6540, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.0105514526367188\n",
      "Step: 6541, Loss: 0.9161370992660522, Accuracy: 1.0, Computation time: 1.1236763000488281\n",
      "Step: 6542, Loss: 0.9160107374191284, Accuracy: 1.0, Computation time: 0.9923841953277588\n",
      "Step: 6543, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 0.902625322341919\n",
      "Step: 6544, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 0.9187312126159668\n",
      "Step: 6545, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.2972254753112793\n",
      "Step: 6546, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.2515597343444824\n",
      "Step: 6547, Loss: 0.9158394932746887, Accuracy: 1.0, Computation time: 1.3563406467437744\n",
      "Step: 6548, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.0710463523864746\n",
      "Step: 6549, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.0099329948425293\n",
      "Step: 6550, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.0265908241271973\n",
      "Step: 6551, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 1.0912046432495117\n",
      "Step: 6552, Loss: 0.9158395528793335, Accuracy: 1.0, Computation time: 0.9746036529541016\n",
      "Step: 6553, Loss: 0.9376319050788879, Accuracy: 0.96875, Computation time: 1.0442941188812256\n",
      "Step: 6554, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.0753138065338135\n",
      "Step: 6555, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.0537800788879395\n",
      "Step: 6556, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.0149738788604736\n",
      "Step: 6557, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.3985395431518555\n",
      "Step: 6558, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.0468089580535889\n",
      "Step: 6559, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 0.9018046855926514\n",
      "Step: 6560, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 0.9293606281280518\n",
      "Step: 6561, Loss: 0.9375287890434265, Accuracy: 0.96875, Computation time: 0.9948318004608154\n",
      "Step: 6562, Loss: 0.9167003631591797, Accuracy: 1.0, Computation time: 1.3940761089324951\n",
      "Step: 6563, Loss: 0.9158394932746887, Accuracy: 1.0, Computation time: 0.8436477184295654\n",
      "Step: 6564, Loss: 0.9158434271812439, Accuracy: 1.0, Computation time: 0.9771595001220703\n",
      "Step: 6565, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 1.1149370670318604\n",
      "Step: 6566, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.146843433380127\n",
      "Step: 6567, Loss: 0.9177276492118835, Accuracy: 1.0, Computation time: 1.0026307106018066\n",
      "Step: 6568, Loss: 0.9158350825309753, Accuracy: 1.0, Computation time: 0.957449197769165\n",
      "Step: 6569, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.2771260738372803\n",
      "Step: 6570, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.1003305912017822\n",
      "Step: 6571, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.0277857780456543\n",
      "Step: 6572, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.1464512348175049\n",
      "Step: 6573, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.1829273700714111\n",
      "Step: 6574, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.1194274425506592\n",
      "Step: 6575, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 0.9674310684204102\n",
      "Step: 6576, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.3097739219665527\n",
      "Step: 6577, Loss: 0.9159374237060547, Accuracy: 1.0, Computation time: 1.229971170425415\n",
      "Step: 6578, Loss: 0.9375902414321899, Accuracy: 0.96875, Computation time: 1.098017930984497\n",
      "Step: 6579, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.1326358318328857\n",
      "Step: 6580, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 0.9805855751037598\n",
      "Step: 6581, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 1.5900905132293701\n",
      "Step: 6582, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 1.7090439796447754\n",
      "Step: 6583, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 0.8712368011474609\n",
      "Step: 6584, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 0.9243135452270508\n",
      "Step: 6585, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 0.9731814861297607\n",
      "Step: 6586, Loss: 0.915838360786438, Accuracy: 1.0, Computation time: 1.1351702213287354\n",
      "Step: 6587, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.407667636871338\n",
      "Step: 6588, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.150010347366333\n",
      "Step: 6589, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 1.3737266063690186\n",
      "Step: 6590, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.1577410697937012\n",
      "Step: 6591, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 1.0543954372406006\n",
      "Step: 6592, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.1880888938903809\n",
      "Step: 6593, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 0.9585399627685547\n",
      "Step: 6594, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.0457725524902344\n",
      "Step: 6595, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 1.0709502696990967\n",
      "Step: 6596, Loss: 0.9158355593681335, Accuracy: 1.0, Computation time: 1.1471905708312988\n",
      "Step: 6597, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 1.0219075679779053\n",
      "Step: 6598, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 1.0192735195159912\n",
      "Step: 6599, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 1.1688904762268066\n",
      "Step: 6600, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 1.144930362701416\n",
      "Step: 6601, Loss: 0.9158360958099365, Accuracy: 1.0, Computation time: 1.0814409255981445\n",
      "Step: 6602, Loss: 0.9158338904380798, Accuracy: 1.0, Computation time: 1.0269243717193604\n",
      "Step: 6603, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.1098387241363525\n",
      "Step: 6604, Loss: 0.915836751461029, Accuracy: 1.0, Computation time: 1.0544357299804688\n",
      "Step: 6605, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.4508202075958252\n",
      "Step: 6606, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.0054645538330078\n",
      "Step: 6607, Loss: 0.9158357977867126, Accuracy: 1.0, Computation time: 0.991722583770752\n",
      "Step: 6608, Loss: 0.9158357977867126, Accuracy: 1.0, Computation time: 0.9669907093048096\n",
      "Step: 6609, Loss: 0.9158355593681335, Accuracy: 1.0, Computation time: 1.1855649948120117\n",
      "Step: 6610, Loss: 0.9158362746238708, Accuracy: 1.0, Computation time: 1.1792373657226562\n",
      "Step: 6611, Loss: 0.9377598166465759, Accuracy: 0.96875, Computation time: 1.493645429611206\n",
      "Step: 6612, Loss: 0.9160206317901611, Accuracy: 1.0, Computation time: 1.2618122100830078\n",
      "Step: 6613, Loss: 0.9163413643836975, Accuracy: 1.0, Computation time: 1.4659521579742432\n",
      "Step: 6614, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.1781787872314453\n",
      "Step: 6615, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.179948329925537\n",
      "Step: 6616, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.288764476776123\n",
      "Step: 6617, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 0.9956176280975342\n",
      "Step: 6618, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 0.9780936241149902\n",
      "Step: 6619, Loss: 0.9161027073860168, Accuracy: 1.0, Computation time: 1.344930648803711\n",
      "Step: 6620, Loss: 0.9360827207565308, Accuracy: 0.9375, Computation time: 1.2169840335845947\n",
      "Step: 6621, Loss: 0.937505841255188, Accuracy: 0.96875, Computation time: 0.9873068332672119\n",
      "Step: 6622, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.5359289646148682\n",
      "Step: 6623, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.9862616062164307\n",
      "Step: 6624, Loss: 0.9164090752601624, Accuracy: 1.0, Computation time: 1.4126091003417969\n",
      "Step: 6625, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.5373365879058838\n",
      "Step: 6626, Loss: 0.9215749502182007, Accuracy: 1.0, Computation time: 1.2888193130493164\n",
      "Step: 6627, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 1.1704738140106201\n",
      "Step: 6628, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.059626817703247\n",
      "Step: 6629, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.040931224822998\n",
      "Step: 6630, Loss: 0.9162372946739197, Accuracy: 1.0, Computation time: 1.545717477798462\n",
      "Step: 6631, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.0933969020843506\n",
      "Step: 6632, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.2606160640716553\n",
      "Step: 6633, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.234668254852295\n",
      "Step: 6634, Loss: 0.9158430695533752, Accuracy: 1.0, Computation time: 1.1298964023590088\n",
      "Step: 6635, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 0.9797425270080566\n",
      "Step: 6636, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.1623258590698242\n",
      "Step: 6637, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 1.217085361480713\n",
      "Step: 6638, Loss: 0.9366121888160706, Accuracy: 0.96875, Computation time: 1.4497954845428467\n",
      "Step: 6639, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.256178617477417\n",
      "Step: 6640, Loss: 0.9243723154067993, Accuracy: 1.0, Computation time: 1.2003657817840576\n",
      "Step: 6641, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 1.6908648014068604\n",
      "Step: 6642, Loss: 0.9162824153900146, Accuracy: 1.0, Computation time: 1.1261320114135742\n",
      "Step: 6643, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.0815808773040771\n",
      "Step: 6644, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.255002498626709\n",
      "Step: 6645, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.130749225616455\n",
      "Step: 6646, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 0.918858528137207\n",
      "Step: 6647, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 0.9337606430053711\n",
      "Step: 6648, Loss: 0.9158372282981873, Accuracy: 1.0, Computation time: 1.3185372352600098\n",
      "Step: 6649, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 0.992438554763794\n",
      "Step: 6650, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.1626462936401367\n",
      "Step: 6651, Loss: 0.9161189198493958, Accuracy: 1.0, Computation time: 1.2666590213775635\n",
      "Step: 6652, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.2045364379882812\n",
      "Step: 6653, Loss: 0.9373169541358948, Accuracy: 0.96875, Computation time: 1.1046948432922363\n",
      "Step: 6654, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 1.0739588737487793\n",
      "Step: 6655, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.3330368995666504\n",
      "Step: 6656, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.1804630756378174\n",
      "Step: 6657, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.258105993270874\n",
      "Step: 6658, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 1.4487199783325195\n",
      "Step: 6659, Loss: 0.9375715851783752, Accuracy: 0.96875, Computation time: 1.1164774894714355\n",
      "Step: 6660, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.151123285293579\n",
      "Step: 6661, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 1.2202134132385254\n",
      "Step: 6662, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.4128303527832031\n",
      "Step: 6663, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 1.075850486755371\n",
      "Step: 6664, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 1.0817251205444336\n",
      "Step: 6665, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.1084949970245361\n",
      "Step: 6666, Loss: 0.9375523924827576, Accuracy: 0.96875, Computation time: 0.9523930549621582\n",
      "Step: 6667, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.2061452865600586\n",
      "Step: 6668, Loss: 0.9158415794372559, Accuracy: 1.0, Computation time: 1.1704485416412354\n",
      "Step: 6669, Loss: 0.9375417232513428, Accuracy: 0.96875, Computation time: 1.3114824295043945\n",
      "########################\n",
      "Test loss: 1.1199549436569214, Test Accuracy_epoch48: 0.7050691246986389\n",
      "########################\n",
      "Step: 6670, Loss: 0.9375349283218384, Accuracy: 0.96875, Computation time: 1.1470065116882324\n",
      "Step: 6671, Loss: 0.9158362150192261, Accuracy: 1.0, Computation time: 1.2976362705230713\n",
      "Step: 6672, Loss: 0.9374590516090393, Accuracy: 0.96875, Computation time: 1.2305622100830078\n",
      "Step: 6673, Loss: 0.9375357627868652, Accuracy: 0.96875, Computation time: 1.3131802082061768\n",
      "Step: 6674, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.2892720699310303\n",
      "Step: 6675, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.3716716766357422\n",
      "Step: 6676, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.019834041595459\n",
      "Step: 6677, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.4562907218933105\n",
      "Step: 6678, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 1.7053112983703613\n",
      "Step: 6679, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 1.0825884342193604\n",
      "Step: 6680, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 1.2059836387634277\n",
      "Step: 6681, Loss: 0.9158351421356201, Accuracy: 1.0, Computation time: 1.036733627319336\n",
      "Step: 6682, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.18001389503479\n",
      "Step: 6683, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 1.0615074634552002\n",
      "Step: 6684, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 1.3170180320739746\n",
      "Step: 6685, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.330573320388794\n",
      "Step: 6686, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.600071668624878\n",
      "Step: 6687, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 0.9818012714385986\n",
      "Step: 6688, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.1911096572875977\n",
      "Step: 6689, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.333512783050537\n",
      "Step: 6690, Loss: 0.9375254511833191, Accuracy: 0.96875, Computation time: 1.1976077556610107\n",
      "Step: 6691, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.3063900470733643\n",
      "Step: 6692, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.5194683074951172\n",
      "Step: 6693, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 1.038464069366455\n",
      "Step: 6694, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.0174622535705566\n",
      "Step: 6695, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.071594476699829\n",
      "Step: 6696, Loss: 0.9158379435539246, Accuracy: 1.0, Computation time: 1.192883014678955\n",
      "Step: 6697, Loss: 0.9158346056938171, Accuracy: 1.0, Computation time: 1.0133631229400635\n",
      "Step: 6698, Loss: 0.9158331155776978, Accuracy: 1.0, Computation time: 1.0285792350769043\n",
      "Step: 6699, Loss: 0.9158331155776978, Accuracy: 1.0, Computation time: 1.209787368774414\n",
      "Step: 6700, Loss: 0.9159913063049316, Accuracy: 1.0, Computation time: 1.1159827709197998\n",
      "Step: 6701, Loss: 0.9158343076705933, Accuracy: 1.0, Computation time: 1.1253678798675537\n",
      "Step: 6702, Loss: 0.9158327579498291, Accuracy: 1.0, Computation time: 0.9615042209625244\n",
      "Step: 6703, Loss: 0.9158347845077515, Accuracy: 1.0, Computation time: 1.3885090351104736\n",
      "Step: 6704, Loss: 0.918588399887085, Accuracy: 1.0, Computation time: 1.027339220046997\n",
      "Step: 6705, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.6898601055145264\n",
      "Step: 6706, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.3870599269866943\n",
      "Step: 6707, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 1.2258546352386475\n",
      "Step: 6708, Loss: 0.9177792072296143, Accuracy: 1.0, Computation time: 1.1254656314849854\n",
      "Step: 6709, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.0972535610198975\n",
      "Step: 6710, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 1.0233004093170166\n",
      "Step: 6711, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.1345231533050537\n",
      "Step: 6712, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 0.971116304397583\n",
      "Step: 6713, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 1.6088480949401855\n",
      "Step: 6714, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.1287238597869873\n",
      "Step: 6715, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.4018943309783936\n",
      "Step: 6716, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 0.9359312057495117\n",
      "Step: 6717, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.2190828323364258\n",
      "Step: 6718, Loss: 0.9167324304580688, Accuracy: 1.0, Computation time: 1.528918743133545\n",
      "Step: 6719, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.092104196548462\n",
      "Step: 6720, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.0684027671813965\n",
      "Step: 6721, Loss: 0.9375749230384827, Accuracy: 0.96875, Computation time: 1.2515838146209717\n",
      "Step: 6722, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.4179084300994873\n",
      "Step: 6723, Loss: 0.9159720540046692, Accuracy: 1.0, Computation time: 1.1141889095306396\n",
      "Step: 6724, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 1.1191213130950928\n",
      "Step: 6725, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.2941405773162842\n",
      "Step: 6726, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 0.9424018859863281\n",
      "Step: 6727, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 1.1148641109466553\n",
      "Step: 6728, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 1.1513679027557373\n",
      "Step: 6729, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 0.973675012588501\n",
      "Step: 6730, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 0.9029269218444824\n",
      "Step: 6731, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.1353976726531982\n",
      "Step: 6732, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.0911524295806885\n",
      "Step: 6733, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 1.2120256423950195\n",
      "Step: 6734, Loss: 0.9172548055648804, Accuracy: 1.0, Computation time: 1.039116382598877\n",
      "Step: 6735, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.981581449508667\n",
      "Step: 6736, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.2115809917449951\n",
      "Step: 6737, Loss: 0.9373630285263062, Accuracy: 0.96875, Computation time: 1.230825424194336\n",
      "Step: 6738, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.0000815391540527\n",
      "Step: 6739, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.4668889045715332\n",
      "Step: 6740, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.4670634269714355\n",
      "Step: 6741, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 1.0528056621551514\n",
      "Step: 6742, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.0173046588897705\n",
      "Step: 6743, Loss: 0.9158373475074768, Accuracy: 1.0, Computation time: 1.2013707160949707\n",
      "Step: 6744, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 1.1262550354003906\n",
      "Step: 6745, Loss: 0.9331604838371277, Accuracy: 0.96875, Computation time: 1.0216081142425537\n",
      "Step: 6746, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.2724382877349854\n",
      "Step: 6747, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.3105711936950684\n",
      "Step: 6748, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.090756893157959\n",
      "Step: 6749, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.3369615077972412\n",
      "Step: 6750, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.0594699382781982\n",
      "Step: 6751, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.1209712028503418\n",
      "Step: 6752, Loss: 0.9372189044952393, Accuracy: 0.96875, Computation time: 1.14894700050354\n",
      "Step: 6753, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 0.9932911396026611\n",
      "Step: 6754, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.0798561573028564\n",
      "Step: 6755, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 0.9910986423492432\n",
      "Step: 6756, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.0769100189208984\n",
      "Step: 6757, Loss: 0.921488881111145, Accuracy: 1.0, Computation time: 1.4419174194335938\n",
      "Step: 6758, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 0.9625811576843262\n",
      "Step: 6759, Loss: 0.9159734845161438, Accuracy: 1.0, Computation time: 1.6241464614868164\n",
      "Step: 6760, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 1.0753779411315918\n",
      "Step: 6761, Loss: 0.9159212112426758, Accuracy: 1.0, Computation time: 1.383385419845581\n",
      "Step: 6762, Loss: 0.9159541726112366, Accuracy: 1.0, Computation time: 0.9461321830749512\n",
      "Step: 6763, Loss: 0.9159407615661621, Accuracy: 1.0, Computation time: 1.0880286693572998\n",
      "Step: 6764, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 1.413611650466919\n",
      "Step: 6765, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 1.461789608001709\n",
      "Step: 6766, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 1.193915843963623\n",
      "Step: 6767, Loss: 0.9159513115882874, Accuracy: 1.0, Computation time: 1.2967615127563477\n",
      "Step: 6768, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.2321624755859375\n",
      "Step: 6769, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 1.007542371749878\n",
      "Step: 6770, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.2269344329833984\n",
      "Step: 6771, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.2624082565307617\n",
      "Step: 6772, Loss: 0.9159715175628662, Accuracy: 1.0, Computation time: 1.134674310684204\n",
      "Step: 6773, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.0415019989013672\n",
      "Step: 6774, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.344397783279419\n",
      "Step: 6775, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.2808172702789307\n",
      "Step: 6776, Loss: 0.9313856363296509, Accuracy: 0.96875, Computation time: 1.1778340339660645\n",
      "Step: 6777, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.0750396251678467\n",
      "Step: 6778, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 1.0939311981201172\n",
      "Step: 6779, Loss: 0.9160142540931702, Accuracy: 1.0, Computation time: 1.143275260925293\n",
      "Step: 6780, Loss: 0.9161890149116516, Accuracy: 1.0, Computation time: 1.1877226829528809\n",
      "Step: 6781, Loss: 0.9160102009773254, Accuracy: 1.0, Computation time: 1.1526520252227783\n",
      "Step: 6782, Loss: 0.9159934520721436, Accuracy: 1.0, Computation time: 1.0496065616607666\n",
      "Step: 6783, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.2575178146362305\n",
      "Step: 6784, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.1486282348632812\n",
      "Step: 6785, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.1193418502807617\n",
      "Step: 6786, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.0338592529296875\n",
      "Step: 6787, Loss: 0.915969967842102, Accuracy: 1.0, Computation time: 1.4197485446929932\n",
      "Step: 6788, Loss: 0.9162193536758423, Accuracy: 1.0, Computation time: 1.0117182731628418\n",
      "Step: 6789, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.179654836654663\n",
      "Step: 6790, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.068413257598877\n",
      "Step: 6791, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.136667251586914\n",
      "Step: 6792, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.0337884426116943\n",
      "Step: 6793, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.59165358543396\n",
      "Step: 6794, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.3956551551818848\n",
      "Step: 6795, Loss: 0.9375227093696594, Accuracy: 0.96875, Computation time: 1.140115737915039\n",
      "Step: 6796, Loss: 0.9234729409217834, Accuracy: 1.0, Computation time: 1.2874412536621094\n",
      "Step: 6797, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 1.2296779155731201\n",
      "Step: 6798, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.0814168453216553\n",
      "Step: 6799, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 1.0633518695831299\n",
      "Step: 6800, Loss: 0.91835618019104, Accuracy: 1.0, Computation time: 1.106489896774292\n",
      "Step: 6801, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 0.875629186630249\n",
      "Step: 6802, Loss: 0.9160754680633545, Accuracy: 1.0, Computation time: 1.1103301048278809\n",
      "Step: 6803, Loss: 0.9376280903816223, Accuracy: 0.96875, Computation time: 1.6287822723388672\n",
      "Step: 6804, Loss: 0.9159544110298157, Accuracy: 1.0, Computation time: 2.0282273292541504\n",
      "Step: 6805, Loss: 0.9159497618675232, Accuracy: 1.0, Computation time: 1.1892516613006592\n",
      "Step: 6806, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.1031949520111084\n",
      "Step: 6807, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.0115711688995361\n",
      "Step: 6808, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.0171875953674316\n",
      "########################\n",
      "Test loss: 1.1245098114013672, Test Accuracy_epoch49: 0.6967741847038269\n",
      "########################\n",
      "Step: 6809, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.1098012924194336\n",
      "Step: 6810, Loss: 0.9286975860595703, Accuracy: 0.96875, Computation time: 0.9905176162719727\n",
      "Step: 6811, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 0.9320924282073975\n",
      "Step: 6812, Loss: 0.9159854054450989, Accuracy: 1.0, Computation time: 1.1169672012329102\n",
      "Step: 6813, Loss: 0.9163373112678528, Accuracy: 1.0, Computation time: 0.9468176364898682\n",
      "Step: 6814, Loss: 0.9159854650497437, Accuracy: 1.0, Computation time: 0.9181206226348877\n",
      "Step: 6815, Loss: 0.9159314036369324, Accuracy: 1.0, Computation time: 1.1348114013671875\n",
      "Step: 6816, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 1.1298565864562988\n",
      "Step: 6817, Loss: 0.9260697364807129, Accuracy: 0.96875, Computation time: 1.5313177108764648\n",
      "Step: 6818, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 1.326056957244873\n",
      "Step: 6819, Loss: 0.9159556031227112, Accuracy: 1.0, Computation time: 1.1145401000976562\n",
      "Step: 6820, Loss: 0.9160253405570984, Accuracy: 1.0, Computation time: 1.3166041374206543\n",
      "Step: 6821, Loss: 0.9160449504852295, Accuracy: 1.0, Computation time: 1.267662525177002\n",
      "Step: 6822, Loss: 0.9160358905792236, Accuracy: 1.0, Computation time: 1.5373361110687256\n",
      "Step: 6823, Loss: 0.9160525798797607, Accuracy: 1.0, Computation time: 1.218142032623291\n",
      "Step: 6824, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 1.3788886070251465\n",
      "Step: 6825, Loss: 0.9168520569801331, Accuracy: 1.0, Computation time: 1.7877304553985596\n",
      "Step: 6826, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.3437550067901611\n",
      "Step: 6827, Loss: 0.9161149263381958, Accuracy: 1.0, Computation time: 1.2954981327056885\n",
      "Step: 6828, Loss: 0.936159610748291, Accuracy: 0.96875, Computation time: 1.999664068222046\n",
      "Step: 6829, Loss: 0.9376634955406189, Accuracy: 0.96875, Computation time: 1.0983548164367676\n",
      "Step: 6830, Loss: 0.9162163138389587, Accuracy: 1.0, Computation time: 1.2930335998535156\n",
      "Step: 6831, Loss: 0.9164113402366638, Accuracy: 1.0, Computation time: 1.2174808979034424\n",
      "Step: 6832, Loss: 0.9160877466201782, Accuracy: 1.0, Computation time: 1.6177618503570557\n",
      "Step: 6833, Loss: 0.9159762263298035, Accuracy: 1.0, Computation time: 1.4094822406768799\n",
      "Step: 6834, Loss: 0.9164124727249146, Accuracy: 1.0, Computation time: 1.9601306915283203\n",
      "Step: 6835, Loss: 0.9172510504722595, Accuracy: 1.0, Computation time: 1.5601067543029785\n",
      "Step: 6836, Loss: 0.9159860014915466, Accuracy: 1.0, Computation time: 1.5739243030548096\n",
      "Step: 6837, Loss: 0.9159016013145447, Accuracy: 1.0, Computation time: 1.8188905715942383\n",
      "Step: 6838, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.695709228515625\n",
      "Step: 6839, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.2527365684509277\n",
      "Step: 6840, Loss: 0.9159684181213379, Accuracy: 1.0, Computation time: 1.4394607543945312\n",
      "Step: 6841, Loss: 0.9160148501396179, Accuracy: 1.0, Computation time: 1.508619785308838\n",
      "Step: 6842, Loss: 0.9160324931144714, Accuracy: 1.0, Computation time: 1.4934453964233398\n",
      "Step: 6843, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.4340667724609375\n",
      "Step: 6844, Loss: 0.9191368222236633, Accuracy: 1.0, Computation time: 1.702833890914917\n",
      "Step: 6845, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.296783208847046\n",
      "Step: 6846, Loss: 0.9377087354660034, Accuracy: 0.96875, Computation time: 1.2276604175567627\n",
      "Step: 6847, Loss: 0.9162622690200806, Accuracy: 1.0, Computation time: 1.359457015991211\n",
      "Step: 6848, Loss: 0.9160752296447754, Accuracy: 1.0, Computation time: 1.4445345401763916\n",
      "Step: 6849, Loss: 0.9162908792495728, Accuracy: 1.0, Computation time: 1.4897935390472412\n",
      "Step: 6850, Loss: 0.9159677028656006, Accuracy: 1.0, Computation time: 1.3358795642852783\n",
      "Step: 6851, Loss: 0.9162352085113525, Accuracy: 1.0, Computation time: 1.0460331439971924\n",
      "Step: 6852, Loss: 0.9163381457328796, Accuracy: 1.0, Computation time: 1.2123751640319824\n",
      "Step: 6853, Loss: 0.9159649014472961, Accuracy: 1.0, Computation time: 1.2710106372833252\n",
      "Step: 6854, Loss: 0.9159420728683472, Accuracy: 1.0, Computation time: 1.2706139087677002\n",
      "Step: 6855, Loss: 0.9159759879112244, Accuracy: 1.0, Computation time: 1.0606892108917236\n",
      "Step: 6856, Loss: 0.9376016855239868, Accuracy: 0.96875, Computation time: 1.2069880962371826\n",
      "Step: 6857, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.5506174564361572\n",
      "Step: 6858, Loss: 0.9160048961639404, Accuracy: 1.0, Computation time: 1.1913092136383057\n",
      "Step: 6859, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 0.9850356578826904\n",
      "Step: 6860, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 0.946089506149292\n",
      "Step: 6861, Loss: 0.9167596101760864, Accuracy: 1.0, Computation time: 1.4230124950408936\n",
      "Step: 6862, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.2283062934875488\n",
      "Step: 6863, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.4373352527618408\n",
      "Step: 6864, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 0.9262242317199707\n",
      "Step: 6865, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.1949431896209717\n",
      "Step: 6866, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 1.1886136531829834\n",
      "Step: 6867, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.0820484161376953\n",
      "Step: 6868, Loss: 0.9359346628189087, Accuracy: 0.96875, Computation time: 0.9820163249969482\n",
      "Step: 6869, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.0678417682647705\n",
      "Step: 6870, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.0276005268096924\n",
      "Step: 6871, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 1.4348318576812744\n",
      "Step: 6872, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.1998343467712402\n",
      "Step: 6873, Loss: 0.9373451471328735, Accuracy: 0.96875, Computation time: 1.0439813137054443\n",
      "Step: 6874, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 0.8885619640350342\n",
      "Step: 6875, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.2640442848205566\n",
      "Step: 6876, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.2226614952087402\n",
      "Step: 6877, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.146794319152832\n",
      "Step: 6878, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.0872423648834229\n",
      "Step: 6879, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.1085736751556396\n",
      "Step: 6880, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.0024969577789307\n",
      "Step: 6881, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 0.9970228672027588\n",
      "Step: 6882, Loss: 0.9387187957763672, Accuracy: 0.96875, Computation time: 0.94631028175354\n",
      "Step: 6883, Loss: 0.9161118865013123, Accuracy: 1.0, Computation time: 1.1683905124664307\n",
      "Step: 6884, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.2398643493652344\n",
      "Step: 6885, Loss: 0.9159784317016602, Accuracy: 1.0, Computation time: 0.8869750499725342\n",
      "Step: 6886, Loss: 0.9160246253013611, Accuracy: 1.0, Computation time: 0.9976685047149658\n",
      "Step: 6887, Loss: 0.9371705651283264, Accuracy: 0.96875, Computation time: 1.2085058689117432\n",
      "Step: 6888, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 1.0218629837036133\n",
      "Step: 6889, Loss: 0.9379851818084717, Accuracy: 0.96875, Computation time: 1.3208796977996826\n",
      "Step: 6890, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 1.3784489631652832\n",
      "Step: 6891, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.0995135307312012\n",
      "Step: 6892, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.164259433746338\n",
      "Step: 6893, Loss: 0.9159404039382935, Accuracy: 1.0, Computation time: 1.415452003479004\n",
      "Step: 6894, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 1.1577486991882324\n",
      "Step: 6895, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.1957178115844727\n",
      "Step: 6896, Loss: 0.9392691850662231, Accuracy: 0.96875, Computation time: 1.0729548931121826\n",
      "Step: 6897, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.086759328842163\n",
      "Step: 6898, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.1322383880615234\n",
      "Step: 6899, Loss: 0.9371301531791687, Accuracy: 0.96875, Computation time: 1.2632405757904053\n",
      "Step: 6900, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.038111925125122\n",
      "Step: 6901, Loss: 0.9159247279167175, Accuracy: 1.0, Computation time: 1.1102375984191895\n",
      "Step: 6902, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.5082299709320068\n",
      "Step: 6903, Loss: 0.915920615196228, Accuracy: 1.0, Computation time: 1.3795287609100342\n",
      "Step: 6904, Loss: 0.9373443126678467, Accuracy: 0.96875, Computation time: 1.3440568447113037\n",
      "Step: 6905, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.4263615608215332\n",
      "Step: 6906, Loss: 0.9375295639038086, Accuracy: 0.96875, Computation time: 1.3110058307647705\n",
      "Step: 6907, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.4268081188201904\n",
      "Step: 6908, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.056082010269165\n",
      "Step: 6909, Loss: 0.9159857034683228, Accuracy: 1.0, Computation time: 1.184753179550171\n",
      "Step: 6910, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 0.9944775104522705\n",
      "Step: 6911, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.158949851989746\n",
      "Step: 6912, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 1.0427043437957764\n",
      "Step: 6913, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 1.1319007873535156\n",
      "Step: 6914, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.4230382442474365\n",
      "Step: 6915, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.3523731231689453\n",
      "Step: 6916, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.7768127918243408\n",
      "Step: 6917, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.0329854488372803\n",
      "Step: 6918, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.0546984672546387\n",
      "Step: 6919, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.3431730270385742\n",
      "Step: 6920, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.9557671546936035\n",
      "Step: 6921, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 1.357607364654541\n",
      "Step: 6922, Loss: 0.9161000847816467, Accuracy: 1.0, Computation time: 1.117652177810669\n",
      "Step: 6923, Loss: 0.927240788936615, Accuracy: 0.96875, Computation time: 1.3537578582763672\n",
      "Step: 6924, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.1902785301208496\n",
      "Step: 6925, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.2465729713439941\n",
      "Step: 6926, Loss: 0.9159422516822815, Accuracy: 1.0, Computation time: 1.5338900089263916\n",
      "Step: 6927, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.0613007545471191\n",
      "Step: 6928, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.3645377159118652\n",
      "Step: 6929, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.2487754821777344\n",
      "Step: 6930, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.1742019653320312\n",
      "Step: 6931, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.284536600112915\n",
      "Step: 6932, Loss: 0.9171650409698486, Accuracy: 1.0, Computation time: 1.462325096130371\n",
      "Step: 6933, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.136394739151001\n",
      "Step: 6934, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.328951120376587\n",
      "Step: 6935, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 0.9190776348114014\n",
      "Step: 6936, Loss: 0.9159488677978516, Accuracy: 1.0, Computation time: 1.1890003681182861\n",
      "Step: 6937, Loss: 0.9185457229614258, Accuracy: 1.0, Computation time: 1.7565815448760986\n",
      "Step: 6938, Loss: 0.9159601330757141, Accuracy: 1.0, Computation time: 1.0119006633758545\n",
      "Step: 6939, Loss: 0.9160814881324768, Accuracy: 1.0, Computation time: 1.153031826019287\n",
      "Step: 6940, Loss: 0.9164488911628723, Accuracy: 1.0, Computation time: 1.26664137840271\n",
      "Step: 6941, Loss: 0.9160517454147339, Accuracy: 1.0, Computation time: 1.3676862716674805\n",
      "Step: 6942, Loss: 0.9160681962966919, Accuracy: 1.0, Computation time: 1.7379555702209473\n",
      "Step: 6943, Loss: 0.9236693382263184, Accuracy: 1.0, Computation time: 1.5439660549163818\n",
      "Step: 6944, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 1.132828950881958\n",
      "Step: 6945, Loss: 0.9159548282623291, Accuracy: 1.0, Computation time: 0.9511740207672119\n",
      "Step: 6946, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.116779088973999\n",
      "Step: 6947, Loss: 0.9159231185913086, Accuracy: 1.0, Computation time: 1.9470434188842773\n",
      "########################\n",
      "Test loss: 1.1211990118026733, Test Accuracy_epoch50: 0.704147458076477\n",
      "########################\n",
      "Step: 6948, Loss: 0.9162710905075073, Accuracy: 1.0, Computation time: 1.1299307346343994\n",
      "Step: 6949, Loss: 0.9160010814666748, Accuracy: 1.0, Computation time: 0.9788694381713867\n",
      "Step: 6950, Loss: 0.9159784913063049, Accuracy: 1.0, Computation time: 1.0010695457458496\n",
      "Step: 6951, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.2585926055908203\n",
      "Step: 6952, Loss: 0.9159754514694214, Accuracy: 1.0, Computation time: 1.0687334537506104\n",
      "Step: 6953, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 1.1042239665985107\n",
      "Step: 6954, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.384394884109497\n",
      "Step: 6955, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.0336425304412842\n",
      "Step: 6956, Loss: 0.9266468286514282, Accuracy: 0.96875, Computation time: 1.1122796535491943\n",
      "Step: 6957, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 0.9222667217254639\n",
      "Step: 6958, Loss: 0.9159756302833557, Accuracy: 1.0, Computation time: 1.1633803844451904\n",
      "Step: 6959, Loss: 0.9376706480979919, Accuracy: 0.96875, Computation time: 0.8930060863494873\n",
      "Step: 6960, Loss: 0.9379125237464905, Accuracy: 0.96875, Computation time: 1.339627742767334\n",
      "Step: 6961, Loss: 0.9159718751907349, Accuracy: 1.0, Computation time: 0.9475734233856201\n",
      "Step: 6962, Loss: 0.9160521030426025, Accuracy: 1.0, Computation time: 1.2005155086517334\n",
      "Step: 6963, Loss: 0.9161141514778137, Accuracy: 1.0, Computation time: 0.8444778919219971\n",
      "Step: 6964, Loss: 0.9160228371620178, Accuracy: 1.0, Computation time: 1.0214571952819824\n",
      "Step: 6965, Loss: 0.9159249663352966, Accuracy: 1.0, Computation time: 1.2452476024627686\n",
      "Step: 6966, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 0.9564435482025146\n",
      "Step: 6967, Loss: 0.9164555072784424, Accuracy: 1.0, Computation time: 1.4682011604309082\n",
      "Step: 6968, Loss: 0.9159622192382812, Accuracy: 1.0, Computation time: 0.9739179611206055\n",
      "Step: 6969, Loss: 0.9373763203620911, Accuracy: 0.96875, Computation time: 1.4252402782440186\n",
      "Step: 6970, Loss: 0.9312809109687805, Accuracy: 0.96875, Computation time: 2.3351809978485107\n",
      "Step: 6971, Loss: 0.916102945804596, Accuracy: 1.0, Computation time: 1.601696491241455\n",
      "Step: 6972, Loss: 0.9162218570709229, Accuracy: 1.0, Computation time: 1.59373140335083\n",
      "Step: 6973, Loss: 0.9164559245109558, Accuracy: 1.0, Computation time: 1.1941614151000977\n",
      "Step: 6974, Loss: 0.916145920753479, Accuracy: 1.0, Computation time: 1.2081780433654785\n",
      "Step: 6975, Loss: 0.9183485507965088, Accuracy: 1.0, Computation time: 1.2964706420898438\n",
      "Step: 6976, Loss: 0.9159436225891113, Accuracy: 1.0, Computation time: 1.1736600399017334\n",
      "Step: 6977, Loss: 0.9160779714584351, Accuracy: 1.0, Computation time: 1.1416020393371582\n",
      "Step: 6978, Loss: 0.9160535931587219, Accuracy: 1.0, Computation time: 1.1521408557891846\n",
      "Step: 6979, Loss: 0.9160853028297424, Accuracy: 1.0, Computation time: 1.2340428829193115\n",
      "Step: 6980, Loss: 0.9159944653511047, Accuracy: 1.0, Computation time: 1.2673335075378418\n",
      "Step: 6981, Loss: 0.9159955382347107, Accuracy: 1.0, Computation time: 1.488645076751709\n",
      "Step: 6982, Loss: 0.9159736037254333, Accuracy: 1.0, Computation time: 1.1172304153442383\n",
      "Step: 6983, Loss: 0.91596519947052, Accuracy: 1.0, Computation time: 1.0537455081939697\n",
      "Step: 6984, Loss: 0.9180131554603577, Accuracy: 1.0, Computation time: 1.1787943840026855\n",
      "Step: 6985, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 0.9228417873382568\n",
      "Step: 6986, Loss: 0.9168081879615784, Accuracy: 1.0, Computation time: 1.169903039932251\n",
      "Step: 6987, Loss: 0.9160028100013733, Accuracy: 1.0, Computation time: 0.9917581081390381\n",
      "Step: 6988, Loss: 0.9218615293502808, Accuracy: 1.0, Computation time: 0.9945428371429443\n",
      "Step: 6989, Loss: 0.9163415431976318, Accuracy: 1.0, Computation time: 1.3095030784606934\n",
      "Step: 6990, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 1.0128371715545654\n",
      "Step: 6991, Loss: 0.9622265100479126, Accuracy: 0.9375, Computation time: 1.241504192352295\n",
      "Step: 6992, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 1.2073020935058594\n",
      "Step: 6993, Loss: 0.9160016775131226, Accuracy: 1.0, Computation time: 1.0881540775299072\n",
      "Step: 6994, Loss: 0.9255162477493286, Accuracy: 0.96875, Computation time: 1.1153783798217773\n",
      "Step: 6995, Loss: 0.9160088300704956, Accuracy: 1.0, Computation time: 1.011831521987915\n",
      "Step: 6996, Loss: 0.9160245656967163, Accuracy: 1.0, Computation time: 1.1883976459503174\n",
      "Step: 6997, Loss: 0.9160157442092896, Accuracy: 1.0, Computation time: 1.3438310623168945\n",
      "Step: 6998, Loss: 0.9159770607948303, Accuracy: 1.0, Computation time: 1.0066602230072021\n",
      "Step: 6999, Loss: 0.9159491062164307, Accuracy: 1.0, Computation time: 1.0574524402618408\n",
      "Step: 7000, Loss: 0.9160141348838806, Accuracy: 1.0, Computation time: 1.346888542175293\n",
      "Step: 7001, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 1.310687780380249\n",
      "Step: 7002, Loss: 0.9376072883605957, Accuracy: 0.96875, Computation time: 1.043053388595581\n",
      "Step: 7003, Loss: 0.9375522136688232, Accuracy: 0.96875, Computation time: 1.0097033977508545\n",
      "Step: 7004, Loss: 0.9159334897994995, Accuracy: 1.0, Computation time: 1.2122900485992432\n",
      "Step: 7005, Loss: 0.9159517884254456, Accuracy: 1.0, Computation time: 0.9748272895812988\n",
      "Step: 7006, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.1352331638336182\n",
      "Step: 7007, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.0618529319763184\n",
      "Step: 7008, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 0.9515433311462402\n",
      "Step: 7009, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 0.9953091144561768\n",
      "Step: 7010, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.0123209953308105\n",
      "Step: 7011, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 0.8974723815917969\n",
      "Step: 7012, Loss: 0.9372290968894958, Accuracy: 0.96875, Computation time: 1.5977869033813477\n",
      "Step: 7013, Loss: 0.915920078754425, Accuracy: 1.0, Computation time: 1.0818109512329102\n",
      "Step: 7014, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.018441915512085\n",
      "Step: 7015, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.0935909748077393\n",
      "Step: 7016, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.3801016807556152\n",
      "Step: 7017, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.083181619644165\n",
      "Step: 7018, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 0.9620382785797119\n",
      "Step: 7019, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.0323231220245361\n",
      "Step: 7020, Loss: 0.9184536337852478, Accuracy: 1.0, Computation time: 0.9998314380645752\n",
      "Step: 7021, Loss: 0.9239484667778015, Accuracy: 1.0, Computation time: 0.95442795753479\n",
      "Step: 7022, Loss: 0.9343097805976868, Accuracy: 0.96875, Computation time: 1.0865366458892822\n",
      "Step: 7023, Loss: 0.9164115190505981, Accuracy: 1.0, Computation time: 0.9246454238891602\n",
      "Step: 7024, Loss: 0.918106198310852, Accuracy: 1.0, Computation time: 1.3254051208496094\n",
      "Step: 7025, Loss: 0.9160100817680359, Accuracy: 1.0, Computation time: 0.9568119049072266\n",
      "Step: 7026, Loss: 0.9169263243675232, Accuracy: 1.0, Computation time: 1.0915639400482178\n",
      "Step: 7027, Loss: 0.9190312623977661, Accuracy: 1.0, Computation time: 1.1428472995758057\n",
      "Step: 7028, Loss: 0.932581901550293, Accuracy: 0.96875, Computation time: 0.9356846809387207\n",
      "Step: 7029, Loss: 0.9160676002502441, Accuracy: 1.0, Computation time: 1.0814423561096191\n",
      "Step: 7030, Loss: 0.916006863117218, Accuracy: 1.0, Computation time: 1.0326807498931885\n",
      "Step: 7031, Loss: 0.9178133010864258, Accuracy: 1.0, Computation time: 1.331557035446167\n",
      "Step: 7032, Loss: 0.91608726978302, Accuracy: 1.0, Computation time: 1.1427292823791504\n",
      "Step: 7033, Loss: 0.9593971967697144, Accuracy: 0.9375, Computation time: 1.1292774677276611\n",
      "Step: 7034, Loss: 0.9160190224647522, Accuracy: 1.0, Computation time: 1.137511968612671\n",
      "Step: 7035, Loss: 0.916131317615509, Accuracy: 1.0, Computation time: 1.1259233951568604\n",
      "Step: 7036, Loss: 0.9161052107810974, Accuracy: 1.0, Computation time: 1.2272312641143799\n",
      "Step: 7037, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 1.01096510887146\n",
      "Step: 7038, Loss: 0.9377298355102539, Accuracy: 0.96875, Computation time: 1.0720343589782715\n",
      "Step: 7039, Loss: 0.9159863591194153, Accuracy: 1.0, Computation time: 1.0356874465942383\n",
      "Step: 7040, Loss: 0.9160346984863281, Accuracy: 1.0, Computation time: 1.1049535274505615\n",
      "Step: 7041, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.4415695667266846\n",
      "Step: 7042, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 0.9978141784667969\n",
      "Step: 7043, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.1755330562591553\n",
      "Step: 7044, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.2686190605163574\n",
      "Step: 7045, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.004377841949463\n",
      "Step: 7046, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 1.0808932781219482\n",
      "Step: 7047, Loss: 0.9159414768218994, Accuracy: 1.0, Computation time: 1.1944553852081299\n",
      "Step: 7048, Loss: 0.9158961176872253, Accuracy: 1.0, Computation time: 1.4333105087280273\n",
      "Step: 7049, Loss: 0.9159640073776245, Accuracy: 1.0, Computation time: 1.1128129959106445\n",
      "Step: 7050, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 1.169203758239746\n",
      "Step: 7051, Loss: 0.9198862314224243, Accuracy: 1.0, Computation time: 1.7631418704986572\n",
      "Step: 7052, Loss: 0.9303990602493286, Accuracy: 0.96875, Computation time: 1.3590538501739502\n",
      "Step: 7053, Loss: 0.9343089461326599, Accuracy: 0.96875, Computation time: 1.4761443138122559\n",
      "Step: 7054, Loss: 0.9255263805389404, Accuracy: 0.96875, Computation time: 1.4561138153076172\n",
      "Step: 7055, Loss: 0.9160377979278564, Accuracy: 1.0, Computation time: 1.0875699520111084\n",
      "Step: 7056, Loss: 0.9163482785224915, Accuracy: 1.0, Computation time: 1.1883065700531006\n",
      "Step: 7057, Loss: 0.9161613583564758, Accuracy: 1.0, Computation time: 1.1677749156951904\n",
      "Step: 7058, Loss: 0.9161487221717834, Accuracy: 1.0, Computation time: 1.043473243713379\n",
      "Step: 7059, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 1.934912919998169\n",
      "Step: 7060, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.1438348293304443\n",
      "Step: 7061, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 0.9247961044311523\n",
      "Step: 7062, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.0791144371032715\n",
      "Step: 7063, Loss: 0.9159311056137085, Accuracy: 1.0, Computation time: 1.149066686630249\n",
      "Step: 7064, Loss: 0.9266180396080017, Accuracy: 0.96875, Computation time: 1.1407954692840576\n",
      "Step: 7065, Loss: 0.9160309433937073, Accuracy: 1.0, Computation time: 1.3635454177856445\n",
      "Step: 7066, Loss: 0.9160585999488831, Accuracy: 1.0, Computation time: 1.198096752166748\n",
      "Step: 7067, Loss: 0.9160047173500061, Accuracy: 1.0, Computation time: 1.0226051807403564\n",
      "Step: 7068, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.1399407386779785\n",
      "Step: 7069, Loss: 0.9162331223487854, Accuracy: 1.0, Computation time: 1.1056854724884033\n",
      "Step: 7070, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 1.1200904846191406\n",
      "Step: 7071, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.0180482864379883\n",
      "Step: 7072, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 0.956141471862793\n",
      "Step: 7073, Loss: 0.9159830808639526, Accuracy: 1.0, Computation time: 1.1159391403198242\n",
      "Step: 7074, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.1746256351470947\n",
      "Step: 7075, Loss: 0.9159256219863892, Accuracy: 1.0, Computation time: 0.8332092761993408\n",
      "Step: 7076, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.0977725982666016\n",
      "Step: 7077, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.236612319946289\n",
      "Step: 7078, Loss: 0.9158453941345215, Accuracy: 1.0, Computation time: 1.0296380519866943\n",
      "Step: 7079, Loss: 0.9333328604698181, Accuracy: 0.96875, Computation time: 0.9903132915496826\n",
      "Step: 7080, Loss: 0.9162958860397339, Accuracy: 1.0, Computation time: 1.1564199924468994\n",
      "Step: 7081, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 1.0488862991333008\n",
      "Step: 7082, Loss: 0.93754643201828, Accuracy: 0.96875, Computation time: 1.3227450847625732\n",
      "Step: 7083, Loss: 0.915976345539093, Accuracy: 1.0, Computation time: 0.9837596416473389\n",
      "Step: 7084, Loss: 0.9359716773033142, Accuracy: 0.96875, Computation time: 0.8830845355987549\n",
      "Step: 7085, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 0.9349000453948975\n",
      "Test loss: 1.1176589727401733, Test Accuracy: 0.7059907913208008\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1610b6e-266f-4065-87bf-97449bf33b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
