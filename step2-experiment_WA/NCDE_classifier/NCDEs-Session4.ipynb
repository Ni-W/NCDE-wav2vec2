{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93f4065-ef08-4072-9dae-93e983aef88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c98b4d6-ce13-4da8-94e9-a75783edae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea50846-895e-482f-85f7-e212f339f195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 256, 768) (4500,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last2, wav2vec_last3, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last2,label_last3,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fabb62-58df-434f-bad8-29c53aa43d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed026df-ccd6-4b23-b8ec-2182b0a720f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb3a168-9518-4583-ab15-5490c0d81be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        #先将数据流降维再放入模型中训练\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0)\n",
    "            pred_var=prediction.var(axis=0)  \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)     \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc31f98-2e06-434d-9e8a-acab1d2c6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8cdc02-e655-4e69-9e14-c8149c227bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last4], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last4\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b4f18ec-439b-4365-89b3-21dfe0651909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22fe4d5b-33e1-4510-bd2a-6fefb91e3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a818f8b3-05db-4d11-99cc-776d604327ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4500,\n",
    "    dataset_test_size=1031,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=2345,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        total_acc = 0\n",
    "        total_num = total_size\n",
    "        predicted_class = jnp.argmax(pred, axis=1)\n",
    "        total_acc += jnp.sum(predicted_class == label_i)\n",
    "        return total_acc / total_num\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe65fd2-d09d-4b35-8927-6d67f90b57c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.3746626377105713, Accuracy: 0.1875, Computation time: 12.331700086593628\n",
      "Step: 1, Loss: 1.3902212381362915, Accuracy: 0.3125, Computation time: 2.8286325931549072\n",
      "Step: 2, Loss: 1.3900493383407593, Accuracy: 0.28125, Computation time: 2.4534189701080322\n",
      "Step: 3, Loss: 1.371595025062561, Accuracy: 0.3125, Computation time: 2.559760808944702\n",
      "Step: 4, Loss: 1.4160360097885132, Accuracy: 0.1875, Computation time: 2.50437593460083\n",
      "Step: 5, Loss: 1.3863366842269897, Accuracy: 0.5, Computation time: 1.9986796379089355\n",
      "Step: 6, Loss: 1.3322426080703735, Accuracy: 0.28125, Computation time: 2.4010026454925537\n",
      "Step: 7, Loss: 1.3670810461044312, Accuracy: 0.4375, Computation time: 2.521162271499634\n",
      "Step: 8, Loss: 1.3371281623840332, Accuracy: 0.5625, Computation time: 2.84652042388916\n",
      "Step: 9, Loss: 1.329692006111145, Accuracy: 0.28125, Computation time: 2.7671494483947754\n",
      "Step: 10, Loss: 1.2336786985397339, Accuracy: 0.6875, Computation time: 2.8649051189422607\n",
      "Step: 11, Loss: 1.3020415306091309, Accuracy: 0.5625, Computation time: 2.7357494831085205\n",
      "Step: 12, Loss: 1.2148449420928955, Accuracy: 0.65625, Computation time: 2.626997232437134\n",
      "Step: 13, Loss: 1.2270175218582153, Accuracy: 0.59375, Computation time: 2.8263955116271973\n",
      "Step: 14, Loss: 1.1340734958648682, Accuracy: 0.6875, Computation time: 2.1585633754730225\n",
      "Step: 15, Loss: 1.1441385746002197, Accuracy: 0.65625, Computation time: 2.3588335514068604\n",
      "Step: 16, Loss: 1.2032166719436646, Accuracy: 0.53125, Computation time: 2.498112201690674\n",
      "Step: 17, Loss: 1.1494845151901245, Accuracy: 0.71875, Computation time: 2.501908540725708\n",
      "Step: 18, Loss: 1.090399146080017, Accuracy: 0.84375, Computation time: 2.165916681289673\n",
      "Step: 19, Loss: 1.1028027534484863, Accuracy: 0.78125, Computation time: 1.9960699081420898\n",
      "Step: 20, Loss: 1.0506809949874878, Accuracy: 0.90625, Computation time: 2.2852818965911865\n",
      "Step: 21, Loss: 1.0513321161270142, Accuracy: 0.9375, Computation time: 2.321525812149048\n",
      "Step: 22, Loss: 1.0234912633895874, Accuracy: 1.0, Computation time: 2.663893699645996\n",
      "Step: 23, Loss: 1.036519169807434, Accuracy: 0.96875, Computation time: 2.650481939315796\n",
      "Step: 24, Loss: 1.014310359954834, Accuracy: 0.96875, Computation time: 2.291088819503784\n",
      "Step: 25, Loss: 1.0461210012435913, Accuracy: 0.9375, Computation time: 2.3765923976898193\n",
      "Step: 26, Loss: 1.017565131187439, Accuracy: 0.96875, Computation time: 2.239912271499634\n",
      "Step: 27, Loss: 0.9847772717475891, Accuracy: 1.0, Computation time: 2.4110376834869385\n",
      "Step: 28, Loss: 1.019131064414978, Accuracy: 0.90625, Computation time: 2.369062662124634\n",
      "Step: 29, Loss: 0.9860996007919312, Accuracy: 0.9375, Computation time: 2.168494462966919\n",
      "Step: 30, Loss: 0.9606117606163025, Accuracy: 1.0, Computation time: 2.034144639968872\n",
      "Step: 31, Loss: 0.9515119791030884, Accuracy: 1.0, Computation time: 1.9893691539764404\n",
      "Step: 32, Loss: 0.9538083672523499, Accuracy: 1.0, Computation time: 2.013892650604248\n",
      "Step: 33, Loss: 0.9801884889602661, Accuracy: 0.90625, Computation time: 1.9515249729156494\n",
      "Step: 34, Loss: 0.9317690134048462, Accuracy: 1.0, Computation time: 1.8379461765289307\n",
      "Step: 35, Loss: 0.9401668906211853, Accuracy: 1.0, Computation time: 1.9035580158233643\n",
      "Step: 36, Loss: 0.9352496266365051, Accuracy: 1.0, Computation time: 2.113375186920166\n",
      "Step: 37, Loss: 0.9290322065353394, Accuracy: 1.0, Computation time: 1.849626064300537\n",
      "Step: 38, Loss: 0.9275251030921936, Accuracy: 1.0, Computation time: 2.180189609527588\n",
      "Step: 39, Loss: 0.9236054420471191, Accuracy: 1.0, Computation time: 1.7595179080963135\n",
      "Step: 40, Loss: 0.9224756360054016, Accuracy: 1.0, Computation time: 1.9250190258026123\n",
      "Step: 41, Loss: 0.9446138143539429, Accuracy: 0.96875, Computation time: 2.0494439601898193\n",
      "Step: 42, Loss: 0.9344041347503662, Accuracy: 1.0, Computation time: 1.8099703788757324\n",
      "Step: 43, Loss: 0.9267131686210632, Accuracy: 1.0, Computation time: 2.0669190883636475\n",
      "Step: 44, Loss: 0.943266749382019, Accuracy: 0.96875, Computation time: 1.8098340034484863\n",
      "Step: 45, Loss: 0.9222561120986938, Accuracy: 1.0, Computation time: 1.7626614570617676\n",
      "Step: 46, Loss: 0.9243419766426086, Accuracy: 1.0, Computation time: 2.0356624126434326\n",
      "Step: 47, Loss: 0.9338245391845703, Accuracy: 0.96875, Computation time: 1.8610999584197998\n",
      "Step: 48, Loss: 0.9197900891304016, Accuracy: 1.0, Computation time: 1.7043423652648926\n",
      "Step: 49, Loss: 0.9373375773429871, Accuracy: 0.96875, Computation time: 1.9651556015014648\n",
      "Step: 50, Loss: 0.9239713549613953, Accuracy: 1.0, Computation time: 2.0993049144744873\n",
      "Step: 51, Loss: 0.9197127819061279, Accuracy: 1.0, Computation time: 2.0295004844665527\n",
      "Step: 52, Loss: 0.9401534795761108, Accuracy: 0.96875, Computation time: 1.7011997699737549\n",
      "Step: 53, Loss: 0.9196552038192749, Accuracy: 1.0, Computation time: 2.2364394664764404\n",
      "Step: 54, Loss: 0.920298159122467, Accuracy: 1.0, Computation time: 1.8725600242614746\n",
      "Step: 55, Loss: 0.9198920726776123, Accuracy: 1.0, Computation time: 1.7153007984161377\n",
      "Step: 56, Loss: 0.9387093782424927, Accuracy: 0.96875, Computation time: 1.9476852416992188\n",
      "Step: 57, Loss: 0.921115517616272, Accuracy: 1.0, Computation time: 2.5778682231903076\n",
      "Step: 58, Loss: 0.91829913854599, Accuracy: 1.0, Computation time: 2.445749521255493\n",
      "Step: 59, Loss: 0.9182774424552917, Accuracy: 1.0, Computation time: 1.9461915493011475\n",
      "Step: 60, Loss: 0.9184910655021667, Accuracy: 1.0, Computation time: 1.8382229804992676\n",
      "Step: 61, Loss: 0.9391841292381287, Accuracy: 0.9375, Computation time: 2.0534133911132812\n",
      "Step: 62, Loss: 0.9169025421142578, Accuracy: 1.0, Computation time: 1.7756361961364746\n",
      "Step: 63, Loss: 0.9238226413726807, Accuracy: 1.0, Computation time: 1.7302415370941162\n",
      "Step: 64, Loss: 0.9638245105743408, Accuracy: 0.90625, Computation time: 2.158240795135498\n",
      "Step: 65, Loss: 0.9177443981170654, Accuracy: 1.0, Computation time: 2.2044358253479004\n",
      "Step: 66, Loss: 0.9173911809921265, Accuracy: 1.0, Computation time: 2.1619749069213867\n",
      "Step: 67, Loss: 0.918727695941925, Accuracy: 1.0, Computation time: 1.9577980041503906\n",
      "Step: 68, Loss: 0.9329192638397217, Accuracy: 1.0, Computation time: 2.0653514862060547\n",
      "Step: 69, Loss: 0.9573813676834106, Accuracy: 0.9375, Computation time: 1.8810725212097168\n",
      "Step: 70, Loss: 0.9385999441146851, Accuracy: 0.96875, Computation time: 2.1402459144592285\n",
      "Step: 71, Loss: 0.9194649457931519, Accuracy: 1.0, Computation time: 2.135084629058838\n",
      "Step: 72, Loss: 0.9200246930122375, Accuracy: 1.0, Computation time: 2.107419490814209\n",
      "Step: 73, Loss: 0.9262439012527466, Accuracy: 1.0, Computation time: 1.9060084819793701\n",
      "Step: 74, Loss: 0.922379732131958, Accuracy: 1.0, Computation time: 2.4687302112579346\n",
      "Step: 75, Loss: 0.9197713136672974, Accuracy: 1.0, Computation time: 2.2123734951019287\n",
      "Step: 76, Loss: 0.9178265929222107, Accuracy: 1.0, Computation time: 1.9015164375305176\n",
      "Step: 77, Loss: 0.9184601902961731, Accuracy: 1.0, Computation time: 2.095017433166504\n",
      "Step: 78, Loss: 0.9274955987930298, Accuracy: 1.0, Computation time: 2.1828196048736572\n",
      "Step: 79, Loss: 0.937030553817749, Accuracy: 0.96875, Computation time: 1.9236278533935547\n",
      "Step: 80, Loss: 0.9257037043571472, Accuracy: 1.0, Computation time: 2.1919848918914795\n",
      "Step: 81, Loss: 0.9166200757026672, Accuracy: 1.0, Computation time: 2.351728677749634\n",
      "Step: 82, Loss: 0.938237726688385, Accuracy: 0.96875, Computation time: 1.9346120357513428\n",
      "Step: 83, Loss: 0.9185013175010681, Accuracy: 1.0, Computation time: 1.965022325515747\n",
      "Step: 84, Loss: 0.9386014938354492, Accuracy: 0.96875, Computation time: 1.9547016620635986\n",
      "Step: 85, Loss: 0.9174039959907532, Accuracy: 1.0, Computation time: 1.9354407787322998\n",
      "Step: 86, Loss: 0.9166917204856873, Accuracy: 1.0, Computation time: 1.8775269985198975\n",
      "Step: 87, Loss: 0.9362342953681946, Accuracy: 0.96875, Computation time: 1.6960549354553223\n",
      "Step: 88, Loss: 0.9398000240325928, Accuracy: 0.96875, Computation time: 2.2273271083831787\n",
      "Step: 89, Loss: 0.9168973565101624, Accuracy: 1.0, Computation time: 1.8873240947723389\n",
      "Step: 90, Loss: 0.9170398712158203, Accuracy: 1.0, Computation time: 1.8431077003479004\n",
      "Step: 91, Loss: 0.9197697639465332, Accuracy: 1.0, Computation time: 2.2844204902648926\n",
      "Step: 92, Loss: 0.9175361394882202, Accuracy: 1.0, Computation time: 2.15470552444458\n",
      "Step: 93, Loss: 0.9200318455696106, Accuracy: 1.0, Computation time: 2.2696144580841064\n",
      "Step: 94, Loss: 0.9169322848320007, Accuracy: 1.0, Computation time: 2.0757570266723633\n",
      "Step: 95, Loss: 0.9164952039718628, Accuracy: 1.0, Computation time: 1.7690865993499756\n",
      "Step: 96, Loss: 0.9166889786720276, Accuracy: 1.0, Computation time: 1.9012629985809326\n",
      "Step: 97, Loss: 0.9272332191467285, Accuracy: 0.96875, Computation time: 2.2196872234344482\n",
      "Step: 98, Loss: 0.9166971445083618, Accuracy: 1.0, Computation time: 2.0493078231811523\n",
      "Step: 99, Loss: 0.9187929034233093, Accuracy: 1.0, Computation time: 1.7370305061340332\n",
      "Step: 100, Loss: 0.9174060225486755, Accuracy: 1.0, Computation time: 2.386018991470337\n",
      "Step: 101, Loss: 0.9166156053543091, Accuracy: 1.0, Computation time: 2.0415666103363037\n",
      "Step: 102, Loss: 0.9181007146835327, Accuracy: 1.0, Computation time: 2.3370745182037354\n",
      "Step: 103, Loss: 0.9168056845664978, Accuracy: 1.0, Computation time: 1.737950086593628\n",
      "Step: 104, Loss: 0.9169900417327881, Accuracy: 1.0, Computation time: 1.7660884857177734\n",
      "Step: 105, Loss: 0.9167583584785461, Accuracy: 1.0, Computation time: 2.7399742603302\n",
      "Step: 106, Loss: 0.917130172252655, Accuracy: 1.0, Computation time: 2.026172637939453\n",
      "Step: 107, Loss: 0.9176800847053528, Accuracy: 1.0, Computation time: 1.8496077060699463\n",
      "Step: 108, Loss: 0.9165825247764587, Accuracy: 1.0, Computation time: 2.0462841987609863\n",
      "Step: 109, Loss: 0.9166750311851501, Accuracy: 1.0, Computation time: 1.7680003643035889\n",
      "Step: 110, Loss: 0.9210701584815979, Accuracy: 1.0, Computation time: 1.974745273590088\n",
      "Step: 111, Loss: 0.9277129173278809, Accuracy: 1.0, Computation time: 2.2022316455841064\n",
      "Step: 112, Loss: 0.9203698635101318, Accuracy: 1.0, Computation time: 1.7511043548583984\n",
      "Step: 113, Loss: 0.9482277631759644, Accuracy: 0.9375, Computation time: 2.185011625289917\n",
      "Step: 114, Loss: 0.9460055828094482, Accuracy: 0.96875, Computation time: 1.9482369422912598\n",
      "Step: 115, Loss: 0.917168140411377, Accuracy: 1.0, Computation time: 2.2266361713409424\n",
      "Step: 116, Loss: 0.9167309999465942, Accuracy: 1.0, Computation time: 1.9612641334533691\n",
      "Step: 117, Loss: 0.9165881276130676, Accuracy: 1.0, Computation time: 1.9492714405059814\n",
      "Step: 118, Loss: 0.9224097728729248, Accuracy: 1.0, Computation time: 2.2171690464019775\n",
      "Step: 119, Loss: 0.9194790720939636, Accuracy: 1.0, Computation time: 2.093874931335449\n",
      "Step: 120, Loss: 0.9166160225868225, Accuracy: 1.0, Computation time: 2.1950883865356445\n",
      "Step: 121, Loss: 0.9343526363372803, Accuracy: 0.96875, Computation time: 2.3818869590759277\n",
      "Step: 122, Loss: 0.9176163673400879, Accuracy: 1.0, Computation time: 2.223965644836426\n",
      "Step: 123, Loss: 0.9190276861190796, Accuracy: 1.0, Computation time: 2.2175557613372803\n",
      "Step: 124, Loss: 0.9227705597877502, Accuracy: 1.0, Computation time: 2.08402681350708\n",
      "Step: 125, Loss: 0.9412497878074646, Accuracy: 0.96875, Computation time: 2.6133666038513184\n",
      "Step: 126, Loss: 0.9191049933433533, Accuracy: 1.0, Computation time: 2.233177423477173\n",
      "Step: 127, Loss: 0.916499674320221, Accuracy: 1.0, Computation time: 1.9944353103637695\n",
      "Step: 128, Loss: 0.9170163869857788, Accuracy: 1.0, Computation time: 2.1943531036376953\n",
      "Step: 129, Loss: 0.9164673089981079, Accuracy: 1.0, Computation time: 2.034311532974243\n",
      "Step: 130, Loss: 0.9160819053649902, Accuracy: 1.0, Computation time: 2.1121420860290527\n",
      "Step: 131, Loss: 0.9177883863449097, Accuracy: 1.0, Computation time: 2.367540121078491\n",
      "Step: 132, Loss: 0.9342036247253418, Accuracy: 0.96875, Computation time: 2.337832450866699\n",
      "Step: 133, Loss: 0.9165208339691162, Accuracy: 1.0, Computation time: 2.0054051876068115\n",
      "Step: 134, Loss: 0.9270763397216797, Accuracy: 1.0, Computation time: 2.024049997329712\n",
      "Step: 135, Loss: 0.9217242002487183, Accuracy: 1.0, Computation time: 2.109196662902832\n",
      "Step: 136, Loss: 0.917304277420044, Accuracy: 1.0, Computation time: 1.9755661487579346\n",
      "Step: 137, Loss: 0.9183851480484009, Accuracy: 1.0, Computation time: 1.775589942932129\n",
      "Step: 138, Loss: 0.9164244532585144, Accuracy: 1.0, Computation time: 1.9435992240905762\n",
      "Step: 139, Loss: 0.91667640209198, Accuracy: 1.0, Computation time: 2.190232276916504\n",
      "########################\n",
      "Test loss: 1.0795528888702393, Test Accuracy_epoch1: 0.7584869265556335\n",
      "########################\n",
      "Step: 140, Loss: 0.9161750078201294, Accuracy: 1.0, Computation time: 1.94948410987854\n",
      "Step: 141, Loss: 0.9168269038200378, Accuracy: 1.0, Computation time: 1.9767053127288818\n",
      "Step: 142, Loss: 0.9162977933883667, Accuracy: 1.0, Computation time: 2.848449945449829\n",
      "Step: 143, Loss: 0.9162090420722961, Accuracy: 1.0, Computation time: 1.905672311782837\n",
      "Step: 144, Loss: 0.9202843904495239, Accuracy: 1.0, Computation time: 1.934690237045288\n",
      "Step: 145, Loss: 0.9249323010444641, Accuracy: 1.0, Computation time: 1.8675165176391602\n",
      "Step: 146, Loss: 0.9258493781089783, Accuracy: 1.0, Computation time: 2.1635591983795166\n",
      "Step: 147, Loss: 0.916633665561676, Accuracy: 1.0, Computation time: 1.9844608306884766\n",
      "Step: 148, Loss: 0.9254173636436462, Accuracy: 1.0, Computation time: 1.8765101432800293\n",
      "Step: 149, Loss: 0.9413620829582214, Accuracy: 0.96875, Computation time: 2.310297727584839\n",
      "Step: 150, Loss: 0.9163058996200562, Accuracy: 1.0, Computation time: 1.9297332763671875\n",
      "Step: 151, Loss: 0.9174129366874695, Accuracy: 1.0, Computation time: 1.8870089054107666\n",
      "Step: 152, Loss: 0.9328821301460266, Accuracy: 0.96875, Computation time: 1.9356849193572998\n",
      "Step: 153, Loss: 0.9166609048843384, Accuracy: 1.0, Computation time: 1.7979660034179688\n",
      "Step: 154, Loss: 0.9175347089767456, Accuracy: 1.0, Computation time: 2.5406386852264404\n",
      "Step: 155, Loss: 0.9164420366287231, Accuracy: 1.0, Computation time: 1.7820096015930176\n",
      "Step: 156, Loss: 0.9163932800292969, Accuracy: 1.0, Computation time: 1.7124712467193604\n",
      "Step: 157, Loss: 0.9271131157875061, Accuracy: 0.96875, Computation time: 2.3534555435180664\n",
      "Step: 158, Loss: 0.9245451092720032, Accuracy: 1.0, Computation time: 2.4247210025787354\n",
      "Step: 159, Loss: 0.9180036187171936, Accuracy: 1.0, Computation time: 1.9293544292449951\n",
      "Step: 160, Loss: 0.9170008301734924, Accuracy: 1.0, Computation time: 1.953857183456421\n",
      "Step: 161, Loss: 0.9169788360595703, Accuracy: 1.0, Computation time: 1.8433799743652344\n",
      "Step: 162, Loss: 0.9177963733673096, Accuracy: 1.0, Computation time: 1.9308202266693115\n",
      "Step: 163, Loss: 0.9289194941520691, Accuracy: 1.0, Computation time: 2.3556203842163086\n",
      "Step: 164, Loss: 0.9165506958961487, Accuracy: 1.0, Computation time: 2.125732421875\n",
      "Step: 165, Loss: 0.9375911355018616, Accuracy: 0.96875, Computation time: 2.120044231414795\n",
      "Step: 166, Loss: 0.9179599285125732, Accuracy: 1.0, Computation time: 2.109570026397705\n",
      "Step: 167, Loss: 0.9161258339881897, Accuracy: 1.0, Computation time: 1.8908488750457764\n",
      "Step: 168, Loss: 0.9385572671890259, Accuracy: 0.96875, Computation time: 2.212684392929077\n",
      "Step: 169, Loss: 0.9167985916137695, Accuracy: 1.0, Computation time: 1.9649925231933594\n",
      "Step: 170, Loss: 0.9188820719718933, Accuracy: 1.0, Computation time: 2.131840705871582\n",
      "Step: 171, Loss: 0.91913241147995, Accuracy: 1.0, Computation time: 1.961491346359253\n",
      "Step: 172, Loss: 0.9454826712608337, Accuracy: 0.96875, Computation time: 1.7800049781799316\n",
      "Step: 173, Loss: 0.9162455201148987, Accuracy: 1.0, Computation time: 1.9197793006896973\n",
      "Step: 174, Loss: 0.9162514805793762, Accuracy: 1.0, Computation time: 2.0920674800872803\n",
      "Step: 175, Loss: 0.9190663695335388, Accuracy: 1.0, Computation time: 2.288731575012207\n",
      "Step: 176, Loss: 0.917346179485321, Accuracy: 1.0, Computation time: 2.4134864807128906\n",
      "Step: 177, Loss: 0.9425550699234009, Accuracy: 0.96875, Computation time: 1.915677547454834\n",
      "Step: 178, Loss: 0.916462242603302, Accuracy: 1.0, Computation time: 2.4119231700897217\n",
      "Step: 179, Loss: 0.9160608053207397, Accuracy: 1.0, Computation time: 1.7939128875732422\n",
      "Step: 180, Loss: 0.9183285236358643, Accuracy: 1.0, Computation time: 2.1384780406951904\n",
      "Step: 181, Loss: 0.916571319103241, Accuracy: 1.0, Computation time: 1.955655813217163\n",
      "Step: 182, Loss: 0.9164932370185852, Accuracy: 1.0, Computation time: 2.3270373344421387\n",
      "Step: 183, Loss: 0.9166213870048523, Accuracy: 1.0, Computation time: 2.11716365814209\n",
      "Step: 184, Loss: 0.9423565864562988, Accuracy: 0.96875, Computation time: 2.3050217628479004\n",
      "Step: 185, Loss: 0.9267836809158325, Accuracy: 1.0, Computation time: 2.1097140312194824\n",
      "Step: 186, Loss: 0.9163318872451782, Accuracy: 1.0, Computation time: 2.219142436981201\n",
      "Step: 187, Loss: 0.9165348410606384, Accuracy: 1.0, Computation time: 1.7419559955596924\n",
      "Step: 188, Loss: 0.9169691205024719, Accuracy: 1.0, Computation time: 1.964818000793457\n",
      "Step: 189, Loss: 0.9170970916748047, Accuracy: 1.0, Computation time: 1.7486920356750488\n",
      "Step: 190, Loss: 0.9195713400840759, Accuracy: 1.0, Computation time: 2.315810441970825\n",
      "Step: 191, Loss: 0.9202491044998169, Accuracy: 1.0, Computation time: 1.8077681064605713\n",
      "Step: 192, Loss: 0.9175976514816284, Accuracy: 1.0, Computation time: 1.666079044342041\n",
      "Step: 193, Loss: 0.917029082775116, Accuracy: 1.0, Computation time: 2.118178367614746\n",
      "Step: 194, Loss: 0.9169086813926697, Accuracy: 1.0, Computation time: 1.9471194744110107\n",
      "Step: 195, Loss: 0.9378615617752075, Accuracy: 0.96875, Computation time: 2.083704948425293\n",
      "Step: 196, Loss: 0.9171629548072815, Accuracy: 1.0, Computation time: 2.2712018489837646\n",
      "Step: 197, Loss: 0.9163738489151001, Accuracy: 1.0, Computation time: 1.7818565368652344\n",
      "Step: 198, Loss: 0.9167076945304871, Accuracy: 1.0, Computation time: 2.100705146789551\n",
      "Step: 199, Loss: 0.9166159629821777, Accuracy: 1.0, Computation time: 1.9773550033569336\n",
      "Step: 200, Loss: 0.9189881682395935, Accuracy: 1.0, Computation time: 2.234062433242798\n",
      "Step: 201, Loss: 0.9183175563812256, Accuracy: 1.0, Computation time: 1.8630108833312988\n",
      "Step: 202, Loss: 0.9360586404800415, Accuracy: 0.96875, Computation time: 2.018862724304199\n",
      "Step: 203, Loss: 0.9166873693466187, Accuracy: 1.0, Computation time: 1.92219877243042\n",
      "Step: 204, Loss: 0.9390303492546082, Accuracy: 0.96875, Computation time: 1.9863851070404053\n",
      "Step: 205, Loss: 0.9167239665985107, Accuracy: 1.0, Computation time: 1.8147695064544678\n",
      "Step: 206, Loss: 0.9191237092018127, Accuracy: 1.0, Computation time: 1.9808485507965088\n",
      "Step: 207, Loss: 0.9162266850471497, Accuracy: 1.0, Computation time: 1.920621633529663\n",
      "Step: 208, Loss: 0.9209505915641785, Accuracy: 1.0, Computation time: 2.078559160232544\n",
      "Step: 209, Loss: 0.9164066314697266, Accuracy: 1.0, Computation time: 1.922511100769043\n",
      "Step: 210, Loss: 0.9161233305931091, Accuracy: 1.0, Computation time: 2.005692958831787\n",
      "Step: 211, Loss: 0.9162038564682007, Accuracy: 1.0, Computation time: 1.980586051940918\n",
      "Step: 212, Loss: 0.9166481494903564, Accuracy: 1.0, Computation time: 1.983229637145996\n",
      "Step: 213, Loss: 0.9161401391029358, Accuracy: 1.0, Computation time: 1.9685606956481934\n",
      "Step: 214, Loss: 0.9164285063743591, Accuracy: 1.0, Computation time: 2.6119582653045654\n",
      "Step: 215, Loss: 0.9161394834518433, Accuracy: 1.0, Computation time: 2.0087270736694336\n",
      "Step: 216, Loss: 0.9165754318237305, Accuracy: 1.0, Computation time: 1.7403290271759033\n",
      "Step: 217, Loss: 0.9371602535247803, Accuracy: 0.96875, Computation time: 2.2025439739227295\n",
      "Step: 218, Loss: 0.9374502301216125, Accuracy: 0.96875, Computation time: 2.396747589111328\n",
      "Step: 219, Loss: 0.9168210029602051, Accuracy: 1.0, Computation time: 1.869032382965088\n",
      "Step: 220, Loss: 0.9339413642883301, Accuracy: 0.96875, Computation time: 1.9698877334594727\n",
      "Step: 221, Loss: 0.9367693662643433, Accuracy: 0.96875, Computation time: 2.22607421875\n",
      "Step: 222, Loss: 0.9160069823265076, Accuracy: 1.0, Computation time: 2.1249876022338867\n",
      "Step: 223, Loss: 0.9160760045051575, Accuracy: 1.0, Computation time: 1.9416131973266602\n",
      "Step: 224, Loss: 0.9173628091812134, Accuracy: 1.0, Computation time: 2.15698504447937\n",
      "Step: 225, Loss: 0.9162300825119019, Accuracy: 1.0, Computation time: 2.0571329593658447\n",
      "Step: 226, Loss: 0.9215342402458191, Accuracy: 1.0, Computation time: 2.187312126159668\n",
      "Step: 227, Loss: 0.9285469651222229, Accuracy: 0.96875, Computation time: 1.9729220867156982\n",
      "Step: 228, Loss: 0.923936128616333, Accuracy: 1.0, Computation time: 2.8154654502868652\n",
      "Step: 229, Loss: 0.9185187220573425, Accuracy: 1.0, Computation time: 2.2456250190734863\n",
      "Step: 230, Loss: 0.9169278740882874, Accuracy: 1.0, Computation time: 2.024324893951416\n",
      "Step: 231, Loss: 0.9164302349090576, Accuracy: 1.0, Computation time: 2.4429287910461426\n",
      "Step: 232, Loss: 0.9348682165145874, Accuracy: 0.96875, Computation time: 2.0009045600891113\n",
      "Step: 233, Loss: 0.918073832988739, Accuracy: 1.0, Computation time: 2.004422664642334\n",
      "Step: 234, Loss: 0.928339958190918, Accuracy: 0.96875, Computation time: 2.460768222808838\n",
      "Step: 235, Loss: 0.9166383743286133, Accuracy: 1.0, Computation time: 1.8251793384552002\n",
      "Step: 236, Loss: 0.9255133271217346, Accuracy: 1.0, Computation time: 2.34957218170166\n",
      "Step: 237, Loss: 0.9299885034561157, Accuracy: 0.96875, Computation time: 2.0788097381591797\n",
      "Step: 238, Loss: 0.9377260208129883, Accuracy: 0.96875, Computation time: 1.8977298736572266\n",
      "Step: 239, Loss: 0.9382354617118835, Accuracy: 0.96875, Computation time: 1.851057529449463\n",
      "Step: 240, Loss: 0.9163443446159363, Accuracy: 1.0, Computation time: 2.1936416625976562\n",
      "Step: 241, Loss: 0.917107880115509, Accuracy: 1.0, Computation time: 2.0717053413391113\n",
      "Step: 242, Loss: 0.9162306785583496, Accuracy: 1.0, Computation time: 1.964273452758789\n",
      "Step: 243, Loss: 0.9348456263542175, Accuracy: 0.96875, Computation time: 2.3376338481903076\n",
      "Step: 244, Loss: 0.9169673919677734, Accuracy: 1.0, Computation time: 2.24407696723938\n",
      "Step: 245, Loss: 0.9166221022605896, Accuracy: 1.0, Computation time: 1.9553720951080322\n",
      "Step: 246, Loss: 0.9268974661827087, Accuracy: 0.96875, Computation time: 1.8653831481933594\n",
      "Step: 247, Loss: 0.9162296056747437, Accuracy: 1.0, Computation time: 1.9436428546905518\n",
      "Step: 248, Loss: 0.9159852266311646, Accuracy: 1.0, Computation time: 1.9912407398223877\n",
      "Step: 249, Loss: 0.9161791205406189, Accuracy: 1.0, Computation time: 1.860325813293457\n",
      "Step: 250, Loss: 0.9234163761138916, Accuracy: 1.0, Computation time: 2.201552152633667\n",
      "Step: 251, Loss: 0.9223110675811768, Accuracy: 1.0, Computation time: 1.9025819301605225\n",
      "Step: 252, Loss: 0.9180288314819336, Accuracy: 1.0, Computation time: 1.9565067291259766\n",
      "Step: 253, Loss: 0.9364772439002991, Accuracy: 0.96875, Computation time: 1.8462975025177002\n",
      "Step: 254, Loss: 0.9279212355613708, Accuracy: 0.96875, Computation time: 2.200165033340454\n",
      "Step: 255, Loss: 0.9379770159721375, Accuracy: 0.96875, Computation time: 1.8851954936981201\n",
      "Step: 256, Loss: 0.9197168946266174, Accuracy: 1.0, Computation time: 2.3704051971435547\n",
      "Step: 257, Loss: 0.9161630272865295, Accuracy: 1.0, Computation time: 1.8966968059539795\n",
      "Step: 258, Loss: 0.9161713719367981, Accuracy: 1.0, Computation time: 1.9445297718048096\n",
      "Step: 259, Loss: 0.9163540601730347, Accuracy: 1.0, Computation time: 1.968531608581543\n",
      "Step: 260, Loss: 0.9244334697723389, Accuracy: 1.0, Computation time: 2.063581705093384\n",
      "Step: 261, Loss: 0.9161938428878784, Accuracy: 1.0, Computation time: 2.182802200317383\n",
      "Step: 262, Loss: 0.9162759184837341, Accuracy: 1.0, Computation time: 2.4943642616271973\n",
      "Step: 263, Loss: 0.9164679050445557, Accuracy: 1.0, Computation time: 1.9014472961425781\n",
      "Step: 264, Loss: 0.9180577397346497, Accuracy: 1.0, Computation time: 2.1032629013061523\n",
      "Step: 265, Loss: 0.9163979887962341, Accuracy: 1.0, Computation time: 1.9126672744750977\n",
      "Step: 266, Loss: 0.917601466178894, Accuracy: 1.0, Computation time: 1.9957685470581055\n",
      "Step: 267, Loss: 0.9378195405006409, Accuracy: 0.96875, Computation time: 2.014005661010742\n",
      "Step: 268, Loss: 0.9167048931121826, Accuracy: 1.0, Computation time: 2.0687785148620605\n",
      "Step: 269, Loss: 0.9581422209739685, Accuracy: 0.9375, Computation time: 2.146733283996582\n",
      "Step: 270, Loss: 0.925073504447937, Accuracy: 1.0, Computation time: 1.9404008388519287\n",
      "Step: 271, Loss: 0.9173527359962463, Accuracy: 1.0, Computation time: 1.9684696197509766\n",
      "Step: 272, Loss: 0.9161356687545776, Accuracy: 1.0, Computation time: 2.0295674800872803\n",
      "Step: 273, Loss: 0.9343346953392029, Accuracy: 0.96875, Computation time: 1.9903204441070557\n",
      "Step: 274, Loss: 0.9160888195037842, Accuracy: 1.0, Computation time: 2.0293166637420654\n",
      "Step: 275, Loss: 0.9163011312484741, Accuracy: 1.0, Computation time: 2.0029711723327637\n",
      "Step: 276, Loss: 0.9175735712051392, Accuracy: 1.0, Computation time: 1.9273681640625\n",
      "Step: 277, Loss: 0.9312434792518616, Accuracy: 0.96875, Computation time: 1.9044618606567383\n",
      "Step: 278, Loss: 0.9186969995498657, Accuracy: 1.0, Computation time: 2.010512351989746\n",
      "########################\n",
      "Test loss: 1.0742595195770264, Test Accuracy_epoch2: 0.7701261043548584\n",
      "########################\n",
      "Step: 279, Loss: 0.9216678142547607, Accuracy: 1.0, Computation time: 2.088855743408203\n",
      "Step: 280, Loss: 0.9234941005706787, Accuracy: 1.0, Computation time: 2.3608007431030273\n",
      "Step: 281, Loss: 0.9385971426963806, Accuracy: 0.96875, Computation time: 1.8387513160705566\n",
      "Step: 282, Loss: 0.9289531707763672, Accuracy: 1.0, Computation time: 2.3617124557495117\n",
      "Step: 283, Loss: 0.9161554574966431, Accuracy: 1.0, Computation time: 1.845367670059204\n",
      "Step: 284, Loss: 0.9170847535133362, Accuracy: 1.0, Computation time: 2.027500629425049\n",
      "Step: 285, Loss: 0.9177271723747253, Accuracy: 1.0, Computation time: 1.8810269832611084\n",
      "Step: 286, Loss: 0.9167220592498779, Accuracy: 1.0, Computation time: 1.9048020839691162\n",
      "Step: 287, Loss: 0.938032865524292, Accuracy: 0.96875, Computation time: 2.0390660762786865\n",
      "Step: 288, Loss: 0.9162777662277222, Accuracy: 1.0, Computation time: 2.152791738510132\n",
      "Step: 289, Loss: 0.917437732219696, Accuracy: 1.0, Computation time: 1.9143526554107666\n",
      "Step: 290, Loss: 0.9166545271873474, Accuracy: 1.0, Computation time: 1.89095139503479\n",
      "Step: 291, Loss: 0.9160929918289185, Accuracy: 1.0, Computation time: 1.8126051425933838\n",
      "Step: 292, Loss: 0.9161869287490845, Accuracy: 1.0, Computation time: 2.1039748191833496\n",
      "Step: 293, Loss: 0.9160499572753906, Accuracy: 1.0, Computation time: 2.0155389308929443\n",
      "Step: 294, Loss: 0.9177606105804443, Accuracy: 1.0, Computation time: 2.0665698051452637\n",
      "Step: 295, Loss: 0.9161697030067444, Accuracy: 1.0, Computation time: 1.9827303886413574\n",
      "Step: 296, Loss: 0.916105329990387, Accuracy: 1.0, Computation time: 1.9147930145263672\n",
      "Step: 297, Loss: 0.9160799980163574, Accuracy: 1.0, Computation time: 1.7224071025848389\n",
      "Step: 298, Loss: 0.9161628484725952, Accuracy: 1.0, Computation time: 2.3228940963745117\n",
      "Step: 299, Loss: 0.9159906506538391, Accuracy: 1.0, Computation time: 1.8984301090240479\n",
      "Step: 300, Loss: 0.9371770024299622, Accuracy: 0.96875, Computation time: 2.295391082763672\n",
      "Step: 301, Loss: 0.9162026047706604, Accuracy: 1.0, Computation time: 1.892380714416504\n",
      "Step: 302, Loss: 0.9162327647209167, Accuracy: 1.0, Computation time: 1.9926650524139404\n",
      "Step: 303, Loss: 0.9377812743186951, Accuracy: 0.96875, Computation time: 1.8011088371276855\n",
      "Step: 304, Loss: 0.9365076422691345, Accuracy: 0.96875, Computation time: 1.9124228954315186\n",
      "Step: 305, Loss: 0.9161202311515808, Accuracy: 1.0, Computation time: 1.7559254169464111\n",
      "Step: 306, Loss: 0.9161021113395691, Accuracy: 1.0, Computation time: 2.053783893585205\n",
      "Step: 307, Loss: 0.9297642111778259, Accuracy: 0.96875, Computation time: 1.9844987392425537\n",
      "Step: 308, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.8297710418701172\n",
      "Step: 309, Loss: 0.9390398859977722, Accuracy: 0.96875, Computation time: 2.123872756958008\n",
      "Step: 310, Loss: 0.9162665009498596, Accuracy: 1.0, Computation time: 2.1384966373443604\n",
      "Step: 311, Loss: 0.917934000492096, Accuracy: 1.0, Computation time: 2.4287517070770264\n",
      "Step: 312, Loss: 0.9166879057884216, Accuracy: 1.0, Computation time: 2.1833465099334717\n",
      "Step: 313, Loss: 0.9161779284477234, Accuracy: 1.0, Computation time: 2.0669074058532715\n",
      "Step: 314, Loss: 0.9161828756332397, Accuracy: 1.0, Computation time: 1.793470859527588\n",
      "Step: 315, Loss: 0.9353580474853516, Accuracy: 0.96875, Computation time: 2.1619069576263428\n",
      "Step: 316, Loss: 0.9162086844444275, Accuracy: 1.0, Computation time: 1.6908724308013916\n",
      "Step: 317, Loss: 0.9169915914535522, Accuracy: 1.0, Computation time: 2.0761404037475586\n",
      "Step: 318, Loss: 0.9161836504936218, Accuracy: 1.0, Computation time: 2.0768775939941406\n",
      "Step: 319, Loss: 0.9171675443649292, Accuracy: 1.0, Computation time: 1.831075668334961\n",
      "Step: 320, Loss: 0.931609034538269, Accuracy: 0.96875, Computation time: 2.5328476428985596\n",
      "Step: 321, Loss: 0.9160873889923096, Accuracy: 1.0, Computation time: 1.9197137355804443\n",
      "Step: 322, Loss: 0.9177085757255554, Accuracy: 1.0, Computation time: 1.577955722808838\n",
      "Step: 323, Loss: 0.9159587025642395, Accuracy: 1.0, Computation time: 2.423025131225586\n",
      "Step: 324, Loss: 0.9163379073143005, Accuracy: 1.0, Computation time: 2.4490644931793213\n",
      "Step: 325, Loss: 0.9245575666427612, Accuracy: 1.0, Computation time: 2.093806266784668\n",
      "Step: 326, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 2.22062087059021\n",
      "Step: 327, Loss: 0.9163140058517456, Accuracy: 1.0, Computation time: 1.9537303447723389\n",
      "Step: 328, Loss: 0.9164175987243652, Accuracy: 1.0, Computation time: 1.9211361408233643\n",
      "Step: 329, Loss: 0.938026487827301, Accuracy: 0.96875, Computation time: 2.204054355621338\n",
      "Step: 330, Loss: 0.9162399172782898, Accuracy: 1.0, Computation time: 2.1304218769073486\n",
      "Step: 331, Loss: 0.9164185523986816, Accuracy: 1.0, Computation time: 2.0155296325683594\n",
      "Step: 332, Loss: 0.9268966317176819, Accuracy: 0.96875, Computation time: 2.2928659915924072\n",
      "Step: 333, Loss: 0.9159929752349854, Accuracy: 1.0, Computation time: 2.3841710090637207\n",
      "Step: 334, Loss: 0.9160807132720947, Accuracy: 1.0, Computation time: 1.9632105827331543\n",
      "Step: 335, Loss: 0.9164512753486633, Accuracy: 1.0, Computation time: 2.454741954803467\n",
      "Step: 336, Loss: 0.9160434603691101, Accuracy: 1.0, Computation time: 2.1264495849609375\n",
      "Step: 337, Loss: 0.916239321231842, Accuracy: 1.0, Computation time: 2.3302977085113525\n",
      "Step: 338, Loss: 0.9160471558570862, Accuracy: 1.0, Computation time: 2.1651642322540283\n",
      "Step: 339, Loss: 0.9164308905601501, Accuracy: 1.0, Computation time: 2.1814680099487305\n",
      "Step: 340, Loss: 0.9160141944885254, Accuracy: 1.0, Computation time: 2.1936452388763428\n",
      "Step: 341, Loss: 0.9160065650939941, Accuracy: 1.0, Computation time: 1.8954839706420898\n",
      "Step: 342, Loss: 0.9161104559898376, Accuracy: 1.0, Computation time: 1.9328632354736328\n",
      "Step: 343, Loss: 0.9161680936813354, Accuracy: 1.0, Computation time: 2.466407060623169\n",
      "Step: 344, Loss: 0.9163114428520203, Accuracy: 1.0, Computation time: 2.0073516368865967\n",
      "Step: 345, Loss: 0.9381437301635742, Accuracy: 0.96875, Computation time: 2.2343051433563232\n",
      "Step: 346, Loss: 0.9160422682762146, Accuracy: 1.0, Computation time: 2.301947593688965\n",
      "Step: 347, Loss: 0.9330438375473022, Accuracy: 0.96875, Computation time: 2.373753070831299\n",
      "Step: 348, Loss: 0.9161797761917114, Accuracy: 1.0, Computation time: 2.2192540168762207\n",
      "Step: 349, Loss: 0.916302502155304, Accuracy: 1.0, Computation time: 2.1051015853881836\n",
      "Step: 350, Loss: 0.9171406626701355, Accuracy: 1.0, Computation time: 2.1593005657196045\n",
      "Step: 351, Loss: 0.9380540251731873, Accuracy: 0.96875, Computation time: 2.263737916946411\n",
      "Step: 352, Loss: 0.9166268110275269, Accuracy: 1.0, Computation time: 1.9335055351257324\n",
      "Step: 353, Loss: 0.9279047250747681, Accuracy: 0.96875, Computation time: 1.8562591075897217\n",
      "Step: 354, Loss: 0.9163994789123535, Accuracy: 1.0, Computation time: 2.055577516555786\n",
      "Step: 355, Loss: 0.9161508679389954, Accuracy: 1.0, Computation time: 2.2229933738708496\n",
      "Step: 356, Loss: 0.9162440896034241, Accuracy: 1.0, Computation time: 2.4074082374572754\n",
      "Step: 357, Loss: 0.9161573052406311, Accuracy: 1.0, Computation time: 2.170917510986328\n",
      "Step: 358, Loss: 0.9355942606925964, Accuracy: 0.96875, Computation time: 2.199626922607422\n",
      "Step: 359, Loss: 0.9430821537971497, Accuracy: 0.96875, Computation time: 2.1553549766540527\n",
      "Step: 360, Loss: 0.9203667044639587, Accuracy: 1.0, Computation time: 2.063469171524048\n",
      "Step: 361, Loss: 0.9160954356193542, Accuracy: 1.0, Computation time: 1.9034130573272705\n",
      "Step: 362, Loss: 0.9159596562385559, Accuracy: 1.0, Computation time: 2.128669500350952\n",
      "Step: 363, Loss: 0.9160165786743164, Accuracy: 1.0, Computation time: 1.7476651668548584\n",
      "Step: 364, Loss: 0.9163181185722351, Accuracy: 1.0, Computation time: 2.1132423877716064\n",
      "Step: 365, Loss: 0.9355204701423645, Accuracy: 0.96875, Computation time: 1.9370834827423096\n",
      "Step: 366, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 1.9606668949127197\n",
      "Step: 367, Loss: 0.9190083146095276, Accuracy: 1.0, Computation time: 2.339430809020996\n",
      "Step: 368, Loss: 0.9175810813903809, Accuracy: 1.0, Computation time: 2.9531173706054688\n",
      "Step: 369, Loss: 0.9165642261505127, Accuracy: 1.0, Computation time: 2.2344043254852295\n",
      "Step: 370, Loss: 0.9411121606826782, Accuracy: 0.96875, Computation time: 2.7527661323547363\n",
      "Step: 371, Loss: 0.9258198142051697, Accuracy: 0.96875, Computation time: 2.525928497314453\n",
      "Step: 372, Loss: 0.9372940063476562, Accuracy: 0.96875, Computation time: 2.2350306510925293\n",
      "Step: 373, Loss: 0.9171068668365479, Accuracy: 1.0, Computation time: 1.9786014556884766\n",
      "Step: 374, Loss: 0.9163268804550171, Accuracy: 1.0, Computation time: 1.643310308456421\n",
      "Step: 375, Loss: 0.9414454102516174, Accuracy: 0.96875, Computation time: 1.8949966430664062\n",
      "Step: 376, Loss: 0.916195273399353, Accuracy: 1.0, Computation time: 1.9688477516174316\n",
      "Step: 377, Loss: 0.9168574810028076, Accuracy: 1.0, Computation time: 1.8591268062591553\n",
      "Step: 378, Loss: 0.916106104850769, Accuracy: 1.0, Computation time: 2.2078542709350586\n",
      "Step: 379, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 1.780876874923706\n",
      "Step: 380, Loss: 0.9190266728401184, Accuracy: 1.0, Computation time: 2.1413090229034424\n",
      "Step: 381, Loss: 0.9165686964988708, Accuracy: 1.0, Computation time: 1.7609658241271973\n",
      "Step: 382, Loss: 0.9164465069770813, Accuracy: 1.0, Computation time: 2.200218677520752\n",
      "Step: 383, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.82784104347229\n",
      "Step: 384, Loss: 0.9160776734352112, Accuracy: 1.0, Computation time: 2.182281970977783\n",
      "Step: 385, Loss: 0.9160403609275818, Accuracy: 1.0, Computation time: 2.0239408016204834\n",
      "Step: 386, Loss: 0.9163317084312439, Accuracy: 1.0, Computation time: 2.389669418334961\n",
      "Step: 387, Loss: 0.9379332065582275, Accuracy: 0.96875, Computation time: 2.2617440223693848\n",
      "Step: 388, Loss: 0.9387951493263245, Accuracy: 0.96875, Computation time: 2.0048539638519287\n",
      "Step: 389, Loss: 0.9350862503051758, Accuracy: 0.96875, Computation time: 2.027345895767212\n",
      "Step: 390, Loss: 0.9338876605033875, Accuracy: 0.96875, Computation time: 3.7304365634918213\n",
      "Step: 391, Loss: 0.9375051856040955, Accuracy: 0.96875, Computation time: 1.8674671649932861\n",
      "Step: 392, Loss: 0.9159781336784363, Accuracy: 1.0, Computation time: 2.3341031074523926\n",
      "Step: 393, Loss: 0.9375249743461609, Accuracy: 0.96875, Computation time: 1.8089423179626465\n",
      "Step: 394, Loss: 0.9161009192466736, Accuracy: 1.0, Computation time: 2.112433910369873\n",
      "Step: 395, Loss: 0.9161982536315918, Accuracy: 1.0, Computation time: 1.9665157794952393\n",
      "Step: 396, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 2.050542116165161\n",
      "Step: 397, Loss: 0.937524676322937, Accuracy: 0.96875, Computation time: 2.131225347518921\n",
      "Step: 398, Loss: 0.91620934009552, Accuracy: 1.0, Computation time: 2.153947353363037\n",
      "Step: 399, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 2.2239060401916504\n",
      "Step: 400, Loss: 0.916012167930603, Accuracy: 1.0, Computation time: 1.9250953197479248\n",
      "Step: 401, Loss: 0.9160875082015991, Accuracy: 1.0, Computation time: 2.0320613384246826\n",
      "Step: 402, Loss: 0.9167615175247192, Accuracy: 1.0, Computation time: 2.437298059463501\n",
      "Step: 403, Loss: 0.9161324501037598, Accuracy: 1.0, Computation time: 2.6782615184783936\n",
      "Step: 404, Loss: 0.9306387901306152, Accuracy: 0.96875, Computation time: 2.073235511779785\n",
      "Step: 405, Loss: 0.9159379601478577, Accuracy: 1.0, Computation time: 2.2175040245056152\n",
      "Step: 406, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 2.0940184593200684\n",
      "Step: 407, Loss: 0.9237784743309021, Accuracy: 1.0, Computation time: 1.98148775100708\n",
      "Step: 408, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 2.502438545227051\n",
      "Step: 409, Loss: 0.937442421913147, Accuracy: 0.96875, Computation time: 2.188385486602783\n",
      "Step: 410, Loss: 0.9162192940711975, Accuracy: 1.0, Computation time: 2.133155345916748\n",
      "Step: 411, Loss: 0.9159931540489197, Accuracy: 1.0, Computation time: 2.4355616569519043\n",
      "Step: 412, Loss: 0.9228178262710571, Accuracy: 1.0, Computation time: 2.272794008255005\n",
      "Step: 413, Loss: 0.9161327481269836, Accuracy: 1.0, Computation time: 3.020129680633545\n",
      "Step: 414, Loss: 0.9211987257003784, Accuracy: 1.0, Computation time: 2.5666158199310303\n",
      "Step: 415, Loss: 0.9606536030769348, Accuracy: 0.9375, Computation time: 2.3052589893341064\n",
      "Step: 416, Loss: 0.9169566035270691, Accuracy: 1.0, Computation time: 2.2189528942108154\n",
      "Step: 417, Loss: 0.9163042902946472, Accuracy: 1.0, Computation time: 2.2795305252075195\n",
      "########################\n",
      "Test loss: 1.0706171989440918, Test Accuracy_epoch3: 0.7740058302879333\n",
      "########################\n",
      "Step: 418, Loss: 0.9164878129959106, Accuracy: 1.0, Computation time: 2.5336272716522217\n",
      "Step: 419, Loss: 0.9359392523765564, Accuracy: 0.96875, Computation time: 2.488839626312256\n",
      "Step: 420, Loss: 0.9190532565116882, Accuracy: 1.0, Computation time: 1.9688010215759277\n",
      "Step: 421, Loss: 0.9163205027580261, Accuracy: 1.0, Computation time: 2.374699354171753\n",
      "Step: 422, Loss: 0.9189250469207764, Accuracy: 1.0, Computation time: 2.147479295730591\n",
      "Step: 423, Loss: 0.916337788105011, Accuracy: 1.0, Computation time: 1.8553252220153809\n",
      "Step: 424, Loss: 0.9217846989631653, Accuracy: 1.0, Computation time: 2.2640511989593506\n",
      "Step: 425, Loss: 0.9162779450416565, Accuracy: 1.0, Computation time: 2.273865222930908\n",
      "Step: 426, Loss: 0.9169126152992249, Accuracy: 1.0, Computation time: 2.459838628768921\n",
      "Step: 427, Loss: 0.9159368276596069, Accuracy: 1.0, Computation time: 2.129267692565918\n",
      "Step: 428, Loss: 0.9192318320274353, Accuracy: 1.0, Computation time: 2.093079090118408\n",
      "Step: 429, Loss: 0.9373241066932678, Accuracy: 0.96875, Computation time: 1.950423002243042\n",
      "Step: 430, Loss: 0.9369488954544067, Accuracy: 0.96875, Computation time: 2.227837324142456\n",
      "Step: 431, Loss: 0.919164776802063, Accuracy: 1.0, Computation time: 2.188006639480591\n",
      "Step: 432, Loss: 0.9163309931755066, Accuracy: 1.0, Computation time: 2.0047078132629395\n",
      "Step: 433, Loss: 0.9163227677345276, Accuracy: 1.0, Computation time: 2.009852409362793\n",
      "Step: 434, Loss: 0.9365983009338379, Accuracy: 0.96875, Computation time: 2.1500725746154785\n",
      "Step: 435, Loss: 0.9160410761833191, Accuracy: 1.0, Computation time: 1.7931694984436035\n",
      "Step: 436, Loss: 0.9160436391830444, Accuracy: 1.0, Computation time: 1.9512457847595215\n",
      "Step: 437, Loss: 0.9161415100097656, Accuracy: 1.0, Computation time: 2.0196821689605713\n",
      "Step: 438, Loss: 0.9164170026779175, Accuracy: 1.0, Computation time: 2.5032382011413574\n",
      "Step: 439, Loss: 0.9168928861618042, Accuracy: 1.0, Computation time: 2.676814079284668\n",
      "Step: 440, Loss: 0.9183304309844971, Accuracy: 1.0, Computation time: 2.2640724182128906\n",
      "Step: 441, Loss: 0.916368842124939, Accuracy: 1.0, Computation time: 1.9012598991394043\n",
      "Step: 442, Loss: 0.9224039912223816, Accuracy: 1.0, Computation time: 2.1155288219451904\n",
      "Step: 443, Loss: 0.9161285758018494, Accuracy: 1.0, Computation time: 1.9822704792022705\n",
      "Step: 444, Loss: 0.9160488247871399, Accuracy: 1.0, Computation time: 2.2041029930114746\n",
      "Step: 445, Loss: 0.9163840413093567, Accuracy: 1.0, Computation time: 2.3362109661102295\n",
      "Step: 446, Loss: 0.9403449296951294, Accuracy: 0.96875, Computation time: 2.27656626701355\n",
      "Step: 447, Loss: 0.9159337282180786, Accuracy: 1.0, Computation time: 2.265418291091919\n",
      "Step: 448, Loss: 0.916115403175354, Accuracy: 1.0, Computation time: 2.6887598037719727\n",
      "Step: 449, Loss: 0.938180148601532, Accuracy: 0.96875, Computation time: 2.2110157012939453\n",
      "Step: 450, Loss: 0.9379585385322571, Accuracy: 0.96875, Computation time: 2.635852813720703\n",
      "Step: 451, Loss: 0.9380845427513123, Accuracy: 0.96875, Computation time: 1.874734878540039\n",
      "Step: 452, Loss: 0.9164026379585266, Accuracy: 1.0, Computation time: 1.7580575942993164\n",
      "Step: 453, Loss: 0.9175336360931396, Accuracy: 1.0, Computation time: 2.4608078002929688\n",
      "Step: 454, Loss: 0.9159640669822693, Accuracy: 1.0, Computation time: 1.759380578994751\n",
      "Step: 455, Loss: 0.9160770177841187, Accuracy: 1.0, Computation time: 2.511448860168457\n",
      "Step: 456, Loss: 0.9165504574775696, Accuracy: 1.0, Computation time: 2.366339921951294\n",
      "Step: 457, Loss: 0.9323170781135559, Accuracy: 0.96875, Computation time: 2.405097246170044\n",
      "Step: 458, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 2.0791895389556885\n",
      "Step: 459, Loss: 0.9160296320915222, Accuracy: 1.0, Computation time: 2.145796775817871\n",
      "Step: 460, Loss: 0.9160592555999756, Accuracy: 1.0, Computation time: 2.1651792526245117\n",
      "Step: 461, Loss: 0.916232168674469, Accuracy: 1.0, Computation time: 2.286423444747925\n",
      "Step: 462, Loss: 0.9161708354949951, Accuracy: 1.0, Computation time: 2.0260822772979736\n",
      "Step: 463, Loss: 0.9230844974517822, Accuracy: 1.0, Computation time: 2.3858983516693115\n",
      "Step: 464, Loss: 0.9159186482429504, Accuracy: 1.0, Computation time: 1.834782600402832\n",
      "Step: 465, Loss: 0.9160684943199158, Accuracy: 1.0, Computation time: 1.7837064266204834\n",
      "Step: 466, Loss: 0.9388652443885803, Accuracy: 0.96875, Computation time: 2.126460552215576\n",
      "Step: 467, Loss: 0.9160500168800354, Accuracy: 1.0, Computation time: 1.5720493793487549\n",
      "Step: 468, Loss: 0.9160542488098145, Accuracy: 1.0, Computation time: 1.8105809688568115\n",
      "Step: 469, Loss: 0.916059672832489, Accuracy: 1.0, Computation time: 1.8826239109039307\n",
      "Step: 470, Loss: 0.9162678122520447, Accuracy: 1.0, Computation time: 2.0557074546813965\n",
      "Step: 471, Loss: 0.9161535501480103, Accuracy: 1.0, Computation time: 1.748671531677246\n",
      "Step: 472, Loss: 0.9161443114280701, Accuracy: 1.0, Computation time: 1.9077837467193604\n",
      "Step: 473, Loss: 0.9378733038902283, Accuracy: 0.96875, Computation time: 1.9152722358703613\n",
      "Step: 474, Loss: 0.936071515083313, Accuracy: 0.96875, Computation time: 1.966379165649414\n",
      "Step: 475, Loss: 0.9159874320030212, Accuracy: 1.0, Computation time: 2.057096242904663\n",
      "Step: 476, Loss: 0.9159857034683228, Accuracy: 1.0, Computation time: 2.0256989002227783\n",
      "Step: 477, Loss: 0.9164126515388489, Accuracy: 1.0, Computation time: 2.297079086303711\n",
      "Step: 478, Loss: 0.936048686504364, Accuracy: 0.96875, Computation time: 2.2876858711242676\n",
      "Step: 479, Loss: 0.9180120229721069, Accuracy: 1.0, Computation time: 2.176128625869751\n",
      "Step: 480, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 2.0069823265075684\n",
      "Step: 481, Loss: 0.9160906672477722, Accuracy: 1.0, Computation time: 2.0941812992095947\n",
      "Step: 482, Loss: 0.9160127639770508, Accuracy: 1.0, Computation time: 1.7484264373779297\n",
      "Step: 483, Loss: 0.9423860311508179, Accuracy: 0.96875, Computation time: 2.4549407958984375\n",
      "Step: 484, Loss: 0.9297129511833191, Accuracy: 0.96875, Computation time: 2.3476274013519287\n",
      "Step: 485, Loss: 0.9160722494125366, Accuracy: 1.0, Computation time: 1.9028046131134033\n",
      "Step: 486, Loss: 0.9161259531974792, Accuracy: 1.0, Computation time: 1.9447698593139648\n",
      "Step: 487, Loss: 0.9377652406692505, Accuracy: 0.96875, Computation time: 2.222256660461426\n",
      "Step: 488, Loss: 0.916111171245575, Accuracy: 1.0, Computation time: 2.8946166038513184\n",
      "Step: 489, Loss: 0.916731059551239, Accuracy: 1.0, Computation time: 1.904400110244751\n",
      "Step: 490, Loss: 0.937852680683136, Accuracy: 0.96875, Computation time: 1.8683617115020752\n",
      "Step: 491, Loss: 0.9500830769538879, Accuracy: 0.9375, Computation time: 2.3130850791931152\n",
      "Step: 492, Loss: 0.9160050749778748, Accuracy: 1.0, Computation time: 2.26377010345459\n",
      "Step: 493, Loss: 0.9163642525672913, Accuracy: 1.0, Computation time: 2.0524139404296875\n",
      "Step: 494, Loss: 0.9166634678840637, Accuracy: 1.0, Computation time: 2.23010516166687\n",
      "Step: 495, Loss: 0.9175342321395874, Accuracy: 1.0, Computation time: 2.1115188598632812\n",
      "Step: 496, Loss: 0.916197657585144, Accuracy: 1.0, Computation time: 1.8056836128234863\n",
      "Step: 497, Loss: 0.922886312007904, Accuracy: 1.0, Computation time: 1.9713122844696045\n",
      "Step: 498, Loss: 0.9168106913566589, Accuracy: 1.0, Computation time: 2.7269299030303955\n",
      "Step: 499, Loss: 0.9164379835128784, Accuracy: 1.0, Computation time: 2.117582321166992\n",
      "Step: 500, Loss: 0.9170528054237366, Accuracy: 1.0, Computation time: 2.0731425285339355\n",
      "Step: 501, Loss: 0.9170142412185669, Accuracy: 1.0, Computation time: 1.8951740264892578\n",
      "Step: 502, Loss: 0.916911244392395, Accuracy: 1.0, Computation time: 2.2611114978790283\n",
      "Step: 503, Loss: 0.927635908126831, Accuracy: 0.96875, Computation time: 2.483633518218994\n",
      "Step: 504, Loss: 0.9567169547080994, Accuracy: 0.9375, Computation time: 2.042081356048584\n",
      "Step: 505, Loss: 0.9162595272064209, Accuracy: 1.0, Computation time: 1.9793057441711426\n",
      "Step: 506, Loss: 0.9160330295562744, Accuracy: 1.0, Computation time: 1.8793725967407227\n",
      "Step: 507, Loss: 0.915958821773529, Accuracy: 1.0, Computation time: 2.2812352180480957\n",
      "Step: 508, Loss: 0.9161324501037598, Accuracy: 1.0, Computation time: 2.006260633468628\n",
      "Step: 509, Loss: 0.9227895736694336, Accuracy: 1.0, Computation time: 2.017862319946289\n",
      "Step: 510, Loss: 0.9214364886283875, Accuracy: 1.0, Computation time: 2.1192259788513184\n",
      "Step: 511, Loss: 0.918824315071106, Accuracy: 1.0, Computation time: 2.1583304405212402\n",
      "Step: 512, Loss: 0.9311489462852478, Accuracy: 0.96875, Computation time: 2.4154300689697266\n",
      "Step: 513, Loss: 0.9371544718742371, Accuracy: 0.96875, Computation time: 2.3071210384368896\n",
      "Step: 514, Loss: 0.929007887840271, Accuracy: 0.96875, Computation time: 2.254791736602783\n",
      "Step: 515, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 2.201951265335083\n",
      "Step: 516, Loss: 0.9160458445549011, Accuracy: 1.0, Computation time: 2.1459853649139404\n",
      "Step: 517, Loss: 0.9159650802612305, Accuracy: 1.0, Computation time: 2.108570098876953\n",
      "Step: 518, Loss: 0.916064977645874, Accuracy: 1.0, Computation time: 2.2025039196014404\n",
      "Step: 519, Loss: 0.9159515500068665, Accuracy: 1.0, Computation time: 2.1116766929626465\n",
      "Step: 520, Loss: 0.9163073301315308, Accuracy: 1.0, Computation time: 2.098318338394165\n",
      "Step: 521, Loss: 0.9178602695465088, Accuracy: 1.0, Computation time: 1.9603374004364014\n",
      "Step: 522, Loss: 0.9350315928459167, Accuracy: 0.96875, Computation time: 1.9382867813110352\n",
      "Step: 523, Loss: 0.9161233901977539, Accuracy: 1.0, Computation time: 2.0560903549194336\n",
      "Step: 524, Loss: 0.9163201451301575, Accuracy: 1.0, Computation time: 2.3650290966033936\n",
      "Step: 525, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 1.930656909942627\n",
      "Step: 526, Loss: 0.92461097240448, Accuracy: 1.0, Computation time: 2.080583333969116\n",
      "Step: 527, Loss: 0.9161167740821838, Accuracy: 1.0, Computation time: 2.171299457550049\n",
      "Step: 528, Loss: 0.9166619777679443, Accuracy: 1.0, Computation time: 2.1072850227355957\n",
      "Step: 529, Loss: 0.9161412119865417, Accuracy: 1.0, Computation time: 1.9044151306152344\n",
      "Step: 530, Loss: 0.9160446524620056, Accuracy: 1.0, Computation time: 1.932401418685913\n",
      "Step: 531, Loss: 0.9159975051879883, Accuracy: 1.0, Computation time: 2.117224931716919\n",
      "Step: 532, Loss: 0.9270206093788147, Accuracy: 1.0, Computation time: 2.109978437423706\n",
      "Step: 533, Loss: 0.9160715937614441, Accuracy: 1.0, Computation time: 2.2636146545410156\n",
      "Step: 534, Loss: 0.9163082838058472, Accuracy: 1.0, Computation time: 2.398932933807373\n",
      "Step: 535, Loss: 0.9182981848716736, Accuracy: 1.0, Computation time: 2.159433126449585\n",
      "Step: 536, Loss: 0.9172534942626953, Accuracy: 1.0, Computation time: 2.835503101348877\n",
      "Step: 537, Loss: 0.9168165922164917, Accuracy: 1.0, Computation time: 2.423806667327881\n",
      "Step: 538, Loss: 0.9167577028274536, Accuracy: 1.0, Computation time: 2.0034639835357666\n",
      "Step: 539, Loss: 0.9172825217247009, Accuracy: 1.0, Computation time: 2.0814850330352783\n",
      "Step: 540, Loss: 0.9166802167892456, Accuracy: 1.0, Computation time: 2.12343168258667\n",
      "Step: 541, Loss: 0.9161956906318665, Accuracy: 1.0, Computation time: 2.0920164585113525\n",
      "Step: 542, Loss: 0.9162184000015259, Accuracy: 1.0, Computation time: 2.350189208984375\n",
      "Step: 543, Loss: 0.9162347912788391, Accuracy: 1.0, Computation time: 2.259404420852661\n",
      "Step: 544, Loss: 0.917688250541687, Accuracy: 1.0, Computation time: 2.2541699409484863\n",
      "Step: 545, Loss: 0.9164636731147766, Accuracy: 1.0, Computation time: 2.163409471511841\n",
      "Step: 546, Loss: 0.9163850545883179, Accuracy: 1.0, Computation time: 2.308594226837158\n",
      "Step: 547, Loss: 0.9394533038139343, Accuracy: 0.96875, Computation time: 2.333286762237549\n",
      "Step: 548, Loss: 0.9273287653923035, Accuracy: 0.96875, Computation time: 2.3962769508361816\n",
      "Step: 549, Loss: 0.9160102009773254, Accuracy: 1.0, Computation time: 2.4918248653411865\n",
      "Step: 550, Loss: 0.9405021071434021, Accuracy: 0.96875, Computation time: 2.089718818664551\n",
      "Step: 551, Loss: 0.9160820841789246, Accuracy: 1.0, Computation time: 1.9884417057037354\n",
      "Step: 552, Loss: 0.9161568284034729, Accuracy: 1.0, Computation time: 2.2211215496063232\n",
      "Step: 553, Loss: 0.9346218705177307, Accuracy: 0.96875, Computation time: 2.3302676677703857\n",
      "Step: 554, Loss: 0.9164709448814392, Accuracy: 1.0, Computation time: 2.2964932918548584\n",
      "Step: 555, Loss: 0.9167311787605286, Accuracy: 1.0, Computation time: 2.149625062942505\n",
      "Step: 556, Loss: 0.9160187244415283, Accuracy: 1.0, Computation time: 2.017251968383789\n",
      "########################\n",
      "Test loss: 1.0727580785751343, Test Accuracy_epoch4: 0.7701261043548584\n",
      "########################\n",
      "Step: 557, Loss: 0.9185397624969482, Accuracy: 1.0, Computation time: 1.8437185287475586\n",
      "Step: 558, Loss: 0.9369382858276367, Accuracy: 0.96875, Computation time: 1.8627886772155762\n",
      "Step: 559, Loss: 0.9373037219047546, Accuracy: 0.96875, Computation time: 1.9150042533874512\n",
      "Step: 560, Loss: 0.916043758392334, Accuracy: 1.0, Computation time: 2.3210864067077637\n",
      "Step: 561, Loss: 0.9159804582595825, Accuracy: 1.0, Computation time: 1.9819962978363037\n",
      "Step: 562, Loss: 0.916086733341217, Accuracy: 1.0, Computation time: 2.2025437355041504\n",
      "Step: 563, Loss: 0.9163254499435425, Accuracy: 1.0, Computation time: 1.9303219318389893\n",
      "Step: 564, Loss: 0.9166210889816284, Accuracy: 1.0, Computation time: 2.294790506362915\n",
      "Step: 565, Loss: 0.9240308403968811, Accuracy: 1.0, Computation time: 2.3572113513946533\n",
      "Step: 566, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 1.9629602432250977\n",
      "Step: 567, Loss: 0.9160285592079163, Accuracy: 1.0, Computation time: 1.986330270767212\n",
      "Step: 568, Loss: 0.9159702062606812, Accuracy: 1.0, Computation time: 1.9944863319396973\n",
      "Step: 569, Loss: 0.937286913394928, Accuracy: 0.96875, Computation time: 2.311335802078247\n",
      "Step: 570, Loss: 0.9160972237586975, Accuracy: 1.0, Computation time: 2.6217777729034424\n",
      "Step: 571, Loss: 0.9353821277618408, Accuracy: 0.96875, Computation time: 2.6388676166534424\n",
      "Step: 572, Loss: 0.9161532521247864, Accuracy: 1.0, Computation time: 2.0912210941314697\n",
      "Step: 573, Loss: 0.9384320974349976, Accuracy: 0.96875, Computation time: 1.9324309825897217\n",
      "Step: 574, Loss: 0.9189944267272949, Accuracy: 1.0, Computation time: 2.1119544506073\n",
      "Step: 575, Loss: 0.9161834120750427, Accuracy: 1.0, Computation time: 2.2812135219573975\n",
      "Step: 576, Loss: 0.9160855412483215, Accuracy: 1.0, Computation time: 2.2674031257629395\n",
      "Step: 577, Loss: 0.9161269664764404, Accuracy: 1.0, Computation time: 1.9261481761932373\n",
      "Step: 578, Loss: 0.9159839153289795, Accuracy: 1.0, Computation time: 1.6455090045928955\n",
      "Step: 579, Loss: 0.9377622008323669, Accuracy: 0.96875, Computation time: 1.8382749557495117\n",
      "Step: 580, Loss: 0.9164113998413086, Accuracy: 1.0, Computation time: 2.3352272510528564\n",
      "Step: 581, Loss: 0.9159961342811584, Accuracy: 1.0, Computation time: 1.6787290573120117\n",
      "Step: 582, Loss: 0.9464461803436279, Accuracy: 0.96875, Computation time: 2.2013068199157715\n",
      "Step: 583, Loss: 0.9159573316574097, Accuracy: 1.0, Computation time: 2.1392228603363037\n",
      "Step: 584, Loss: 0.9361249208450317, Accuracy: 0.96875, Computation time: 2.0505948066711426\n",
      "Step: 585, Loss: 0.9160823822021484, Accuracy: 1.0, Computation time: 2.187849760055542\n",
      "Step: 586, Loss: 0.9162149429321289, Accuracy: 1.0, Computation time: 1.8576762676239014\n",
      "Step: 587, Loss: 0.9175592064857483, Accuracy: 1.0, Computation time: 1.8405146598815918\n",
      "Step: 588, Loss: 0.9160670638084412, Accuracy: 1.0, Computation time: 1.9608023166656494\n",
      "Step: 589, Loss: 0.9161245226860046, Accuracy: 1.0, Computation time: 1.8209943771362305\n",
      "Step: 590, Loss: 0.9369128942489624, Accuracy: 0.96875, Computation time: 2.6153476238250732\n",
      "Step: 591, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 1.9779901504516602\n",
      "Step: 592, Loss: 0.9167242050170898, Accuracy: 1.0, Computation time: 2.2095987796783447\n",
      "Step: 593, Loss: 0.9307877421379089, Accuracy: 0.96875, Computation time: 2.2218432426452637\n",
      "Step: 594, Loss: 0.9165149331092834, Accuracy: 1.0, Computation time: 1.8888037204742432\n",
      "Step: 595, Loss: 0.9164239764213562, Accuracy: 1.0, Computation time: 2.0596766471862793\n",
      "Step: 596, Loss: 0.916132390499115, Accuracy: 1.0, Computation time: 2.307953119277954\n",
      "Step: 597, Loss: 0.9159771203994751, Accuracy: 1.0, Computation time: 1.868549108505249\n",
      "Step: 598, Loss: 0.9160234928131104, Accuracy: 1.0, Computation time: 2.026344060897827\n",
      "Step: 599, Loss: 0.9160497784614563, Accuracy: 1.0, Computation time: 1.8430850505828857\n",
      "Step: 600, Loss: 0.9160183072090149, Accuracy: 1.0, Computation time: 2.1805624961853027\n",
      "Step: 601, Loss: 0.9160470366477966, Accuracy: 1.0, Computation time: 1.639897108078003\n",
      "Step: 602, Loss: 0.9298587441444397, Accuracy: 0.96875, Computation time: 2.0587728023529053\n",
      "Step: 603, Loss: 0.9184973239898682, Accuracy: 1.0, Computation time: 1.9235470294952393\n",
      "Step: 604, Loss: 0.9160982370376587, Accuracy: 1.0, Computation time: 2.1818597316741943\n",
      "Step: 605, Loss: 0.9160327315330505, Accuracy: 1.0, Computation time: 2.124605417251587\n",
      "Step: 606, Loss: 0.9373635053634644, Accuracy: 0.96875, Computation time: 1.9307963848114014\n",
      "Step: 607, Loss: 0.916144073009491, Accuracy: 1.0, Computation time: 1.908693552017212\n",
      "Step: 608, Loss: 0.9363399744033813, Accuracy: 0.96875, Computation time: 2.121816396713257\n",
      "Step: 609, Loss: 0.9162560105323792, Accuracy: 1.0, Computation time: 1.9786086082458496\n",
      "Step: 610, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 2.1793627738952637\n",
      "Step: 611, Loss: 0.9166132807731628, Accuracy: 1.0, Computation time: 1.8986811637878418\n",
      "Step: 612, Loss: 0.9159333109855652, Accuracy: 1.0, Computation time: 1.9471194744110107\n",
      "Step: 613, Loss: 0.9162226915359497, Accuracy: 1.0, Computation time: 1.8927185535430908\n",
      "Step: 614, Loss: 0.9372645020484924, Accuracy: 0.96875, Computation time: 1.9601082801818848\n",
      "Step: 615, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.9128482341766357\n",
      "Step: 616, Loss: 0.9159769415855408, Accuracy: 1.0, Computation time: 2.1281909942626953\n",
      "Step: 617, Loss: 0.915961503982544, Accuracy: 1.0, Computation time: 2.012645959854126\n",
      "Step: 618, Loss: 0.9584630131721497, Accuracy: 0.9375, Computation time: 1.802417278289795\n",
      "Step: 619, Loss: 0.915978193283081, Accuracy: 1.0, Computation time: 2.0585696697235107\n",
      "Step: 620, Loss: 0.9161061644554138, Accuracy: 1.0, Computation time: 2.230602979660034\n",
      "Step: 621, Loss: 0.9160019755363464, Accuracy: 1.0, Computation time: 2.6760454177856445\n",
      "Step: 622, Loss: 0.9172845482826233, Accuracy: 1.0, Computation time: 1.835792064666748\n",
      "Step: 623, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.7238245010375977\n",
      "Step: 624, Loss: 0.9381663799285889, Accuracy: 0.96875, Computation time: 1.9100713729858398\n",
      "Step: 625, Loss: 0.9160116314888, Accuracy: 1.0, Computation time: 1.9932847023010254\n",
      "Step: 626, Loss: 0.9160804748535156, Accuracy: 1.0, Computation time: 1.8884191513061523\n",
      "Step: 627, Loss: 0.9164278507232666, Accuracy: 1.0, Computation time: 1.8305203914642334\n",
      "Step: 628, Loss: 0.9159334897994995, Accuracy: 1.0, Computation time: 1.9098231792449951\n",
      "Step: 629, Loss: 0.9159783124923706, Accuracy: 1.0, Computation time: 1.808457851409912\n",
      "Step: 630, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.8567428588867188\n",
      "Step: 631, Loss: 0.9364593029022217, Accuracy: 0.96875, Computation time: 1.897193431854248\n",
      "Step: 632, Loss: 0.9178628325462341, Accuracy: 1.0, Computation time: 1.792623519897461\n",
      "Step: 633, Loss: 0.9384694695472717, Accuracy: 0.96875, Computation time: 1.9349660873413086\n",
      "Step: 634, Loss: 0.9161875247955322, Accuracy: 1.0, Computation time: 2.195934295654297\n",
      "Step: 635, Loss: 0.9377397894859314, Accuracy: 0.96875, Computation time: 2.0157597064971924\n",
      "Step: 636, Loss: 0.9202415943145752, Accuracy: 1.0, Computation time: 1.9360134601593018\n",
      "Step: 637, Loss: 0.9160270094871521, Accuracy: 1.0, Computation time: 1.9672400951385498\n",
      "Step: 638, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 1.6261136531829834\n",
      "Step: 639, Loss: 0.9161809682846069, Accuracy: 1.0, Computation time: 1.898893117904663\n",
      "Step: 640, Loss: 0.9160464406013489, Accuracy: 1.0, Computation time: 1.660623550415039\n",
      "Step: 641, Loss: 0.9160016179084778, Accuracy: 1.0, Computation time: 1.7221159934997559\n",
      "Step: 642, Loss: 0.9160879850387573, Accuracy: 1.0, Computation time: 1.9448413848876953\n",
      "Step: 643, Loss: 0.9160232543945312, Accuracy: 1.0, Computation time: 1.5674874782562256\n",
      "Step: 644, Loss: 0.9169045090675354, Accuracy: 1.0, Computation time: 1.5357954502105713\n",
      "Step: 645, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.624211311340332\n",
      "Step: 646, Loss: 0.9366904497146606, Accuracy: 0.96875, Computation time: 1.5890486240386963\n",
      "Step: 647, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.7342455387115479\n",
      "Step: 648, Loss: 0.9158779978752136, Accuracy: 1.0, Computation time: 1.6747562885284424\n",
      "Step: 649, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 2.09126877784729\n",
      "Step: 650, Loss: 0.9248180389404297, Accuracy: 1.0, Computation time: 1.8072969913482666\n",
      "Step: 651, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 1.9057879447937012\n",
      "Step: 652, Loss: 0.915956974029541, Accuracy: 1.0, Computation time: 1.6842560768127441\n",
      "Step: 653, Loss: 0.9160095453262329, Accuracy: 1.0, Computation time: 1.7327220439910889\n",
      "Step: 654, Loss: 0.9443324208259583, Accuracy: 0.96875, Computation time: 1.681150197982788\n",
      "Step: 655, Loss: 0.9563788771629333, Accuracy: 0.9375, Computation time: 1.996274709701538\n",
      "Step: 656, Loss: 0.9377683401107788, Accuracy: 0.96875, Computation time: 1.9458980560302734\n",
      "Step: 657, Loss: 0.9166529774665833, Accuracy: 1.0, Computation time: 1.954805612564087\n",
      "Step: 658, Loss: 0.9163011312484741, Accuracy: 1.0, Computation time: 2.11841082572937\n",
      "Step: 659, Loss: 0.9163001179695129, Accuracy: 1.0, Computation time: 2.2660560607910156\n",
      "Step: 660, Loss: 0.9162395596504211, Accuracy: 1.0, Computation time: 2.039294719696045\n",
      "Step: 661, Loss: 0.9161034226417542, Accuracy: 1.0, Computation time: 1.9103965759277344\n",
      "Step: 662, Loss: 0.9160932898521423, Accuracy: 1.0, Computation time: 2.5672178268432617\n",
      "Step: 663, Loss: 0.9160053730010986, Accuracy: 1.0, Computation time: 2.2231242656707764\n",
      "Step: 664, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 2.1900815963745117\n",
      "Step: 665, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.7192347049713135\n",
      "Step: 666, Loss: 0.9159622192382812, Accuracy: 1.0, Computation time: 2.2543468475341797\n",
      "Step: 667, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 2.427335739135742\n",
      "Step: 668, Loss: 0.9188822507858276, Accuracy: 1.0, Computation time: 2.1451103687286377\n",
      "Step: 669, Loss: 0.9160643815994263, Accuracy: 1.0, Computation time: 1.8266124725341797\n",
      "Step: 670, Loss: 0.9364771246910095, Accuracy: 0.96875, Computation time: 2.031273365020752\n",
      "Step: 671, Loss: 0.9304681420326233, Accuracy: 0.96875, Computation time: 2.3445329666137695\n",
      "Step: 672, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 1.9789302349090576\n",
      "Step: 673, Loss: 0.9164754152297974, Accuracy: 1.0, Computation time: 2.02299427986145\n",
      "Step: 674, Loss: 0.9241536855697632, Accuracy: 1.0, Computation time: 1.8019440174102783\n",
      "Step: 675, Loss: 0.9161579608917236, Accuracy: 1.0, Computation time: 1.8946564197540283\n",
      "Step: 676, Loss: 0.9350605010986328, Accuracy: 0.96875, Computation time: 1.804748296737671\n",
      "Step: 677, Loss: 0.9368209838867188, Accuracy: 0.96875, Computation time: 1.840533971786499\n",
      "Step: 678, Loss: 0.9161023497581482, Accuracy: 1.0, Computation time: 1.8789410591125488\n",
      "Step: 679, Loss: 0.9161583781242371, Accuracy: 1.0, Computation time: 1.6381351947784424\n",
      "Step: 680, Loss: 0.9235668182373047, Accuracy: 1.0, Computation time: 1.906062364578247\n",
      "Step: 681, Loss: 0.9160580039024353, Accuracy: 1.0, Computation time: 1.7935445308685303\n",
      "Step: 682, Loss: 0.917161762714386, Accuracy: 1.0, Computation time: 2.231842279434204\n",
      "Step: 683, Loss: 0.9163212180137634, Accuracy: 1.0, Computation time: 1.8542535305023193\n",
      "Step: 684, Loss: 0.9160633683204651, Accuracy: 1.0, Computation time: 1.8932781219482422\n",
      "Step: 685, Loss: 0.9329008460044861, Accuracy: 0.96875, Computation time: 2.074392080307007\n",
      "Step: 686, Loss: 0.9177703857421875, Accuracy: 1.0, Computation time: 1.9666435718536377\n",
      "Step: 687, Loss: 0.9162413477897644, Accuracy: 1.0, Computation time: 2.1883797645568848\n",
      "Step: 688, Loss: 0.9166674017906189, Accuracy: 1.0, Computation time: 2.3742403984069824\n",
      "Step: 689, Loss: 0.9343175292015076, Accuracy: 0.96875, Computation time: 1.754533052444458\n",
      "Step: 690, Loss: 0.9161797761917114, Accuracy: 1.0, Computation time: 1.6474273204803467\n",
      "Step: 691, Loss: 0.9167206287384033, Accuracy: 1.0, Computation time: 1.5588934421539307\n",
      "Step: 692, Loss: 0.917085587978363, Accuracy: 1.0, Computation time: 1.825361728668213\n",
      "Step: 693, Loss: 0.9162577390670776, Accuracy: 1.0, Computation time: 2.1230175495147705\n",
      "Step: 694, Loss: 0.9160590171813965, Accuracy: 1.0, Computation time: 1.78761887550354\n",
      "Step: 695, Loss: 0.9163533449172974, Accuracy: 1.0, Computation time: 2.177943468093872\n",
      "########################\n",
      "Test loss: 1.0700111389160156, Test Accuracy_epoch5: 0.7720659971237183\n",
      "########################\n",
      "Step: 696, Loss: 0.9161813259124756, Accuracy: 1.0, Computation time: 1.6736693382263184\n",
      "Step: 697, Loss: 0.9392358064651489, Accuracy: 0.96875, Computation time: 2.5357961654663086\n",
      "Step: 698, Loss: 0.916660487651825, Accuracy: 1.0, Computation time: 2.0661280155181885\n",
      "Step: 699, Loss: 0.9201318025588989, Accuracy: 1.0, Computation time: 1.9581036567687988\n",
      "Step: 700, Loss: 0.9374127388000488, Accuracy: 0.96875, Computation time: 1.776440143585205\n",
      "Step: 701, Loss: 0.9379758238792419, Accuracy: 0.96875, Computation time: 1.790961742401123\n",
      "Step: 702, Loss: 0.9163146615028381, Accuracy: 1.0, Computation time: 1.9101121425628662\n",
      "Step: 703, Loss: 0.9165545105934143, Accuracy: 1.0, Computation time: 1.5372192859649658\n",
      "Step: 704, Loss: 0.916735053062439, Accuracy: 1.0, Computation time: 1.5336029529571533\n",
      "Step: 705, Loss: 0.9502480030059814, Accuracy: 0.9375, Computation time: 2.052293539047241\n",
      "Step: 706, Loss: 0.9163230061531067, Accuracy: 1.0, Computation time: 1.9253649711608887\n",
      "Step: 707, Loss: 0.9162095785140991, Accuracy: 1.0, Computation time: 1.509368896484375\n",
      "Step: 708, Loss: 0.9164767265319824, Accuracy: 1.0, Computation time: 1.538811445236206\n",
      "Step: 709, Loss: 0.9172327518463135, Accuracy: 1.0, Computation time: 1.6897387504577637\n",
      "Step: 710, Loss: 0.9264115691184998, Accuracy: 0.96875, Computation time: 1.780289888381958\n",
      "Step: 711, Loss: 0.9162368178367615, Accuracy: 1.0, Computation time: 1.8062622547149658\n",
      "Step: 712, Loss: 0.9369741678237915, Accuracy: 0.96875, Computation time: 2.0917201042175293\n",
      "Step: 713, Loss: 0.9163390398025513, Accuracy: 1.0, Computation time: 1.842536449432373\n",
      "Step: 714, Loss: 0.9161019325256348, Accuracy: 1.0, Computation time: 1.5487442016601562\n",
      "Step: 715, Loss: 0.9160513281822205, Accuracy: 1.0, Computation time: 1.7260503768920898\n",
      "Step: 716, Loss: 0.9160143733024597, Accuracy: 1.0, Computation time: 1.5770325660705566\n",
      "Step: 717, Loss: 0.9174145460128784, Accuracy: 1.0, Computation time: 2.145728349685669\n",
      "Step: 718, Loss: 0.9160674810409546, Accuracy: 1.0, Computation time: 2.0133848190307617\n",
      "Step: 719, Loss: 0.916144847869873, Accuracy: 1.0, Computation time: 1.8442623615264893\n",
      "Step: 720, Loss: 0.9182284474372864, Accuracy: 1.0, Computation time: 2.01703143119812\n",
      "Step: 721, Loss: 0.9162312150001526, Accuracy: 1.0, Computation time: 1.8817298412322998\n",
      "Step: 722, Loss: 0.9162073731422424, Accuracy: 1.0, Computation time: 1.9265077114105225\n",
      "Step: 723, Loss: 0.9162319898605347, Accuracy: 1.0, Computation time: 1.6639647483825684\n",
      "Step: 724, Loss: 0.9165934324264526, Accuracy: 1.0, Computation time: 1.7134718894958496\n",
      "Step: 725, Loss: 0.9161390066146851, Accuracy: 1.0, Computation time: 1.7584116458892822\n",
      "Step: 726, Loss: 0.9159776568412781, Accuracy: 1.0, Computation time: 1.8147070407867432\n",
      "Step: 727, Loss: 0.9161495566368103, Accuracy: 1.0, Computation time: 1.794771432876587\n",
      "Step: 728, Loss: 0.9167512059211731, Accuracy: 1.0, Computation time: 1.629016399383545\n",
      "Step: 729, Loss: 0.9163681864738464, Accuracy: 1.0, Computation time: 1.4829444885253906\n",
      "Step: 730, Loss: 0.916080892086029, Accuracy: 1.0, Computation time: 1.6181089878082275\n",
      "Step: 731, Loss: 0.9187843203544617, Accuracy: 1.0, Computation time: 1.758033275604248\n",
      "Step: 732, Loss: 0.9195967316627502, Accuracy: 1.0, Computation time: 1.9086272716522217\n",
      "Step: 733, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 1.8140900135040283\n",
      "Step: 734, Loss: 0.9161372780799866, Accuracy: 1.0, Computation time: 1.8964183330535889\n",
      "Step: 735, Loss: 0.936491847038269, Accuracy: 0.96875, Computation time: 1.7314293384552002\n",
      "Step: 736, Loss: 0.9187208414077759, Accuracy: 1.0, Computation time: 2.0634844303131104\n",
      "Step: 737, Loss: 0.9167607426643372, Accuracy: 1.0, Computation time: 1.7327485084533691\n",
      "Step: 738, Loss: 0.9161709547042847, Accuracy: 1.0, Computation time: 1.6646711826324463\n",
      "Step: 739, Loss: 0.9159802794456482, Accuracy: 1.0, Computation time: 1.602487325668335\n",
      "Step: 740, Loss: 0.9159583449363708, Accuracy: 1.0, Computation time: 1.722848653793335\n",
      "Step: 741, Loss: 0.9160792231559753, Accuracy: 1.0, Computation time: 1.6436421871185303\n",
      "Step: 742, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.7359073162078857\n",
      "Step: 743, Loss: 0.9180790185928345, Accuracy: 1.0, Computation time: 1.5277280807495117\n",
      "Step: 744, Loss: 0.9372010827064514, Accuracy: 0.96875, Computation time: 1.8952045440673828\n",
      "Step: 745, Loss: 0.9379842281341553, Accuracy: 0.96875, Computation time: 1.9030184745788574\n",
      "Step: 746, Loss: 0.9184695482254028, Accuracy: 1.0, Computation time: 2.4461355209350586\n",
      "Step: 747, Loss: 0.9160013794898987, Accuracy: 1.0, Computation time: 1.5308425426483154\n",
      "Step: 748, Loss: 0.9402636289596558, Accuracy: 0.96875, Computation time: 1.7750098705291748\n",
      "Step: 749, Loss: 0.9190383553504944, Accuracy: 1.0, Computation time: 1.8820695877075195\n",
      "Step: 750, Loss: 0.9287962913513184, Accuracy: 0.96875, Computation time: 1.8950536251068115\n",
      "Step: 751, Loss: 0.9167869091033936, Accuracy: 1.0, Computation time: 1.8213841915130615\n",
      "Step: 752, Loss: 0.9162168502807617, Accuracy: 1.0, Computation time: 1.476891040802002\n",
      "Step: 753, Loss: 0.9160411953926086, Accuracy: 1.0, Computation time: 1.5991809368133545\n",
      "Step: 754, Loss: 0.9161007404327393, Accuracy: 1.0, Computation time: 1.8000388145446777\n",
      "Step: 755, Loss: 0.9245386123657227, Accuracy: 1.0, Computation time: 1.8430187702178955\n",
      "Step: 756, Loss: 0.9166449308395386, Accuracy: 1.0, Computation time: 1.7333216667175293\n",
      "Step: 757, Loss: 0.9357448816299438, Accuracy: 0.96875, Computation time: 1.6956791877746582\n",
      "Step: 758, Loss: 0.9166347980499268, Accuracy: 1.0, Computation time: 1.486628532409668\n",
      "Step: 759, Loss: 0.9162383079528809, Accuracy: 1.0, Computation time: 1.9373571872711182\n",
      "Step: 760, Loss: 0.9163438081741333, Accuracy: 1.0, Computation time: 1.5984818935394287\n",
      "Step: 761, Loss: 0.9166141748428345, Accuracy: 1.0, Computation time: 1.9942963123321533\n",
      "Step: 762, Loss: 0.9162893295288086, Accuracy: 1.0, Computation time: 1.465695858001709\n",
      "Step: 763, Loss: 0.916248619556427, Accuracy: 1.0, Computation time: 1.8284399509429932\n",
      "Step: 764, Loss: 0.9336273074150085, Accuracy: 0.96875, Computation time: 2.1527185440063477\n",
      "Step: 765, Loss: 0.9160793423652649, Accuracy: 1.0, Computation time: 1.699681043624878\n",
      "Step: 766, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 1.6263387203216553\n",
      "Step: 767, Loss: 0.9160114526748657, Accuracy: 1.0, Computation time: 1.6801307201385498\n",
      "Step: 768, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 1.501462459564209\n",
      "Step: 769, Loss: 0.9375856518745422, Accuracy: 0.96875, Computation time: 1.612938404083252\n",
      "Step: 770, Loss: 0.9320324659347534, Accuracy: 0.96875, Computation time: 2.0730466842651367\n",
      "Step: 771, Loss: 0.9161774516105652, Accuracy: 1.0, Computation time: 1.5072381496429443\n",
      "Step: 772, Loss: 0.9376009702682495, Accuracy: 0.96875, Computation time: 1.9320003986358643\n",
      "Step: 773, Loss: 0.916193962097168, Accuracy: 1.0, Computation time: 1.916142225265503\n",
      "Step: 774, Loss: 0.9164880514144897, Accuracy: 1.0, Computation time: 1.8792283535003662\n",
      "Step: 775, Loss: 0.938001811504364, Accuracy: 0.96875, Computation time: 2.214353322982788\n",
      "Step: 776, Loss: 0.9162094593048096, Accuracy: 1.0, Computation time: 1.573301076889038\n",
      "Step: 777, Loss: 0.915985107421875, Accuracy: 1.0, Computation time: 1.5933246612548828\n",
      "Step: 778, Loss: 0.9217321276664734, Accuracy: 1.0, Computation time: 1.7192742824554443\n",
      "Step: 779, Loss: 0.9175683856010437, Accuracy: 1.0, Computation time: 1.8340082168579102\n",
      "Step: 780, Loss: 0.9367530941963196, Accuracy: 0.96875, Computation time: 1.9983305931091309\n",
      "Step: 781, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 1.854151964187622\n",
      "Step: 782, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.6648218631744385\n",
      "Step: 783, Loss: 0.9160798192024231, Accuracy: 1.0, Computation time: 1.5797474384307861\n",
      "Step: 784, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 1.8040437698364258\n",
      "Step: 785, Loss: 0.9159507751464844, Accuracy: 1.0, Computation time: 1.677445411682129\n",
      "Step: 786, Loss: 0.9160904288291931, Accuracy: 1.0, Computation time: 1.4979116916656494\n",
      "Step: 787, Loss: 0.9160130620002747, Accuracy: 1.0, Computation time: 1.8003795146942139\n",
      "Step: 788, Loss: 0.9159720540046692, Accuracy: 1.0, Computation time: 1.72239351272583\n",
      "Step: 789, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 1.6776907444000244\n",
      "Step: 790, Loss: 0.9159404635429382, Accuracy: 1.0, Computation time: 1.589775562286377\n",
      "Step: 791, Loss: 0.9377758502960205, Accuracy: 0.96875, Computation time: 1.684732437133789\n",
      "Step: 792, Loss: 0.9159098863601685, Accuracy: 1.0, Computation time: 1.5816640853881836\n",
      "Step: 793, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 1.9342272281646729\n",
      "Step: 794, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.7783551216125488\n",
      "Step: 795, Loss: 0.9374538660049438, Accuracy: 0.96875, Computation time: 1.5344047546386719\n",
      "Step: 796, Loss: 0.9161228537559509, Accuracy: 1.0, Computation time: 1.4831743240356445\n",
      "Step: 797, Loss: 0.9162592887878418, Accuracy: 1.0, Computation time: 1.7418549060821533\n",
      "Step: 798, Loss: 0.9375212788581848, Accuracy: 0.96875, Computation time: 1.4403531551361084\n",
      "Step: 799, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 1.4952783584594727\n",
      "Step: 800, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.8474023342132568\n",
      "Step: 801, Loss: 0.9159514904022217, Accuracy: 1.0, Computation time: 1.8794951438903809\n",
      "Step: 802, Loss: 0.9159018993377686, Accuracy: 1.0, Computation time: 1.5918948650360107\n",
      "Step: 803, Loss: 0.9159666895866394, Accuracy: 1.0, Computation time: 1.979597806930542\n",
      "Step: 804, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 1.9711685180664062\n",
      "Step: 805, Loss: 0.9339675307273865, Accuracy: 0.96875, Computation time: 1.8022515773773193\n",
      "Step: 806, Loss: 0.9166277050971985, Accuracy: 1.0, Computation time: 2.2755134105682373\n",
      "Step: 807, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.76059889793396\n",
      "Step: 808, Loss: 0.9160977005958557, Accuracy: 1.0, Computation time: 2.0841314792633057\n",
      "Step: 809, Loss: 0.9159628748893738, Accuracy: 1.0, Computation time: 1.8862230777740479\n",
      "Step: 810, Loss: 0.9429246783256531, Accuracy: 0.96875, Computation time: 2.0730443000793457\n",
      "Step: 811, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 2.0342154502868652\n",
      "Step: 812, Loss: 0.9351201057434082, Accuracy: 0.96875, Computation time: 1.7616195678710938\n",
      "Step: 813, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 1.806680679321289\n",
      "Step: 814, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.8390223979949951\n",
      "Step: 815, Loss: 0.9163697957992554, Accuracy: 1.0, Computation time: 1.5675451755523682\n",
      "Step: 816, Loss: 0.9357223510742188, Accuracy: 0.96875, Computation time: 1.9703853130340576\n",
      "Step: 817, Loss: 0.9159507751464844, Accuracy: 1.0, Computation time: 1.805051326751709\n",
      "Step: 818, Loss: 0.9172723889350891, Accuracy: 1.0, Computation time: 2.3246328830718994\n",
      "Step: 819, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.6357269287109375\n",
      "Step: 820, Loss: 0.9376461505889893, Accuracy: 0.96875, Computation time: 1.9057891368865967\n",
      "Step: 821, Loss: 0.9397599101066589, Accuracy: 0.96875, Computation time: 2.1230504512786865\n",
      "Step: 822, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 2.1347384452819824\n",
      "Step: 823, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.8209640979766846\n",
      "Step: 824, Loss: 0.916048526763916, Accuracy: 1.0, Computation time: 2.0150527954101562\n",
      "Step: 825, Loss: 0.9160758852958679, Accuracy: 1.0, Computation time: 2.2105894088745117\n",
      "Step: 826, Loss: 0.916057288646698, Accuracy: 1.0, Computation time: 1.741826057434082\n",
      "Step: 827, Loss: 0.9160972237586975, Accuracy: 1.0, Computation time: 1.699500560760498\n",
      "Step: 828, Loss: 0.9160041809082031, Accuracy: 1.0, Computation time: 1.520662784576416\n",
      "Step: 829, Loss: 0.9159820079803467, Accuracy: 1.0, Computation time: 1.5977473258972168\n",
      "Step: 830, Loss: 0.9373309016227722, Accuracy: 0.96875, Computation time: 1.7141940593719482\n",
      "Step: 831, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.948420763015747\n",
      "Step: 832, Loss: 0.9160777926445007, Accuracy: 1.0, Computation time: 1.958021879196167\n",
      "Step: 833, Loss: 0.93644779920578, Accuracy: 0.96875, Computation time: 1.6105499267578125\n",
      "Step: 834, Loss: 0.9161484837532043, Accuracy: 1.0, Computation time: 1.94364595413208\n",
      "########################\n",
      "Test loss: 1.0715892314910889, Test Accuracy_epoch6: 0.7691561579704285\n",
      "########################\n",
      "Step: 835, Loss: 0.9188370704650879, Accuracy: 1.0, Computation time: 2.221086025238037\n",
      "Step: 836, Loss: 0.92363041639328, Accuracy: 1.0, Computation time: 1.754204273223877\n",
      "Step: 837, Loss: 0.9221373200416565, Accuracy: 1.0, Computation time: 1.801435947418213\n",
      "Step: 838, Loss: 0.9160746932029724, Accuracy: 1.0, Computation time: 1.458439588546753\n",
      "Step: 839, Loss: 0.9160969257354736, Accuracy: 1.0, Computation time: 1.5992505550384521\n",
      "Step: 840, Loss: 0.9161695241928101, Accuracy: 1.0, Computation time: 1.6692230701446533\n",
      "Step: 841, Loss: 0.9163267016410828, Accuracy: 1.0, Computation time: 1.6630208492279053\n",
      "Step: 842, Loss: 0.916192889213562, Accuracy: 1.0, Computation time: 1.757002592086792\n",
      "Step: 843, Loss: 0.9160484075546265, Accuracy: 1.0, Computation time: 1.7013428211212158\n",
      "Step: 844, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 1.6422502994537354\n",
      "Step: 845, Loss: 0.9159157872200012, Accuracy: 1.0, Computation time: 1.662926197052002\n",
      "Step: 846, Loss: 0.9168713092803955, Accuracy: 1.0, Computation time: 1.9433670043945312\n",
      "Step: 847, Loss: 0.9159786105155945, Accuracy: 1.0, Computation time: 1.5687224864959717\n",
      "Step: 848, Loss: 0.9375185370445251, Accuracy: 0.96875, Computation time: 1.7533788681030273\n",
      "Step: 849, Loss: 0.9258387088775635, Accuracy: 0.96875, Computation time: 1.5757124423980713\n",
      "Step: 850, Loss: 0.9160897731781006, Accuracy: 1.0, Computation time: 1.9026710987091064\n",
      "Step: 851, Loss: 0.9172037243843079, Accuracy: 1.0, Computation time: 1.8101975917816162\n",
      "Step: 852, Loss: 0.9163539409637451, Accuracy: 1.0, Computation time: 1.6655430793762207\n",
      "Step: 853, Loss: 0.9163194894790649, Accuracy: 1.0, Computation time: 1.571268081665039\n",
      "Step: 854, Loss: 0.9161324501037598, Accuracy: 1.0, Computation time: 1.5728423595428467\n",
      "Step: 855, Loss: 0.9160849452018738, Accuracy: 1.0, Computation time: 1.4289896488189697\n",
      "Step: 856, Loss: 0.9282072186470032, Accuracy: 0.96875, Computation time: 2.198986053466797\n",
      "Step: 857, Loss: 0.9160394668579102, Accuracy: 1.0, Computation time: 1.6852104663848877\n",
      "Step: 858, Loss: 0.9496052861213684, Accuracy: 0.9375, Computation time: 1.9416654109954834\n",
      "Step: 859, Loss: 0.9363669753074646, Accuracy: 0.96875, Computation time: 1.753692388534546\n",
      "Step: 860, Loss: 0.9160850048065186, Accuracy: 1.0, Computation time: 1.6894702911376953\n",
      "Step: 861, Loss: 0.9423965811729431, Accuracy: 0.96875, Computation time: 1.956294298171997\n",
      "Step: 862, Loss: 0.9377413988113403, Accuracy: 0.96875, Computation time: 1.6041436195373535\n",
      "Step: 863, Loss: 0.9371649622917175, Accuracy: 0.96875, Computation time: 1.6162796020507812\n",
      "Step: 864, Loss: 0.9169297218322754, Accuracy: 1.0, Computation time: 1.553863763809204\n",
      "Step: 865, Loss: 0.9162804484367371, Accuracy: 1.0, Computation time: 1.5308303833007812\n",
      "Step: 866, Loss: 0.9215278625488281, Accuracy: 1.0, Computation time: 1.5363221168518066\n",
      "Step: 867, Loss: 0.9382684230804443, Accuracy: 0.96875, Computation time: 1.8050482273101807\n",
      "Step: 868, Loss: 0.9167665839195251, Accuracy: 1.0, Computation time: 1.5039498805999756\n",
      "Step: 869, Loss: 0.9368022680282593, Accuracy: 0.96875, Computation time: 1.5503783226013184\n",
      "Step: 870, Loss: 0.9164739847183228, Accuracy: 1.0, Computation time: 1.6086435317993164\n",
      "Step: 871, Loss: 0.9197897911071777, Accuracy: 1.0, Computation time: 2.276785135269165\n",
      "Step: 872, Loss: 0.9171156883239746, Accuracy: 1.0, Computation time: 1.8618597984313965\n",
      "Step: 873, Loss: 0.9160681962966919, Accuracy: 1.0, Computation time: 1.4828462600708008\n",
      "Step: 874, Loss: 0.9379237294197083, Accuracy: 0.96875, Computation time: 1.523874044418335\n",
      "Step: 875, Loss: 0.9160024523735046, Accuracy: 1.0, Computation time: 1.7559480667114258\n",
      "Step: 876, Loss: 0.9160557985305786, Accuracy: 1.0, Computation time: 1.82008695602417\n",
      "Step: 877, Loss: 0.9317570328712463, Accuracy: 0.96875, Computation time: 2.1022493839263916\n",
      "Step: 878, Loss: 0.9359383583068848, Accuracy: 0.96875, Computation time: 1.8399250507354736\n",
      "Step: 879, Loss: 0.937721312046051, Accuracy: 0.96875, Computation time: 1.957688570022583\n",
      "Step: 880, Loss: 0.9164673686027527, Accuracy: 1.0, Computation time: 1.8072669506072998\n",
      "Step: 881, Loss: 0.9161123037338257, Accuracy: 1.0, Computation time: 1.7249541282653809\n",
      "Step: 882, Loss: 0.9159812331199646, Accuracy: 1.0, Computation time: 1.9924747943878174\n",
      "Step: 883, Loss: 0.9367321729660034, Accuracy: 0.96875, Computation time: 1.9956316947937012\n",
      "Step: 884, Loss: 0.9160381555557251, Accuracy: 1.0, Computation time: 2.1515212059020996\n",
      "Step: 885, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 2.1551904678344727\n",
      "Step: 886, Loss: 0.9159876704216003, Accuracy: 1.0, Computation time: 1.6891453266143799\n",
      "Step: 887, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.7268545627593994\n",
      "Step: 888, Loss: 0.947648286819458, Accuracy: 0.9375, Computation time: 2.3218414783477783\n",
      "Step: 889, Loss: 0.9196227192878723, Accuracy: 1.0, Computation time: 2.618814706802368\n",
      "Step: 890, Loss: 0.9449665546417236, Accuracy: 0.96875, Computation time: 2.151550054550171\n",
      "Step: 891, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 1.5374517440795898\n",
      "Step: 892, Loss: 0.917424201965332, Accuracy: 1.0, Computation time: 2.624471426010132\n",
      "Step: 893, Loss: 0.917644739151001, Accuracy: 1.0, Computation time: 2.2097959518432617\n",
      "Step: 894, Loss: 0.9179564714431763, Accuracy: 1.0, Computation time: 2.127758264541626\n",
      "Step: 895, Loss: 0.9172859787940979, Accuracy: 1.0, Computation time: 2.028407573699951\n",
      "Step: 896, Loss: 0.9164960384368896, Accuracy: 1.0, Computation time: 1.663874864578247\n",
      "Step: 897, Loss: 0.9162520170211792, Accuracy: 1.0, Computation time: 1.9591996669769287\n",
      "Step: 898, Loss: 0.9164921641349792, Accuracy: 1.0, Computation time: 1.9656381607055664\n",
      "Step: 899, Loss: 0.9159867167472839, Accuracy: 1.0, Computation time: 1.7359209060668945\n",
      "Step: 900, Loss: 0.916282057762146, Accuracy: 1.0, Computation time: 1.7987706661224365\n",
      "Step: 901, Loss: 0.9378793239593506, Accuracy: 0.96875, Computation time: 1.5644803047180176\n",
      "Step: 902, Loss: 0.9163135886192322, Accuracy: 1.0, Computation time: 2.377777576446533\n",
      "Step: 903, Loss: 0.916199803352356, Accuracy: 1.0, Computation time: 1.6756300926208496\n",
      "Step: 904, Loss: 0.916808545589447, Accuracy: 1.0, Computation time: 1.8813879489898682\n",
      "Step: 905, Loss: 0.9248862266540527, Accuracy: 1.0, Computation time: 2.645689010620117\n",
      "Step: 906, Loss: 0.9161790609359741, Accuracy: 1.0, Computation time: 1.6287641525268555\n",
      "Step: 907, Loss: 0.9173381924629211, Accuracy: 1.0, Computation time: 2.108393669128418\n",
      "Step: 908, Loss: 0.9161110520362854, Accuracy: 1.0, Computation time: 1.6376852989196777\n",
      "Step: 909, Loss: 0.9161417484283447, Accuracy: 1.0, Computation time: 1.4611103534698486\n",
      "Step: 910, Loss: 0.9160557389259338, Accuracy: 1.0, Computation time: 1.6010808944702148\n",
      "Step: 911, Loss: 0.916102945804596, Accuracy: 1.0, Computation time: 2.148601770401001\n",
      "Step: 912, Loss: 0.9160542488098145, Accuracy: 1.0, Computation time: 1.8953421115875244\n",
      "Step: 913, Loss: 0.9159855842590332, Accuracy: 1.0, Computation time: 1.7139270305633545\n",
      "Step: 914, Loss: 0.9159864783287048, Accuracy: 1.0, Computation time: 1.4990501403808594\n",
      "Step: 915, Loss: 0.9159750938415527, Accuracy: 1.0, Computation time: 1.797586441040039\n",
      "Step: 916, Loss: 0.9159947037696838, Accuracy: 1.0, Computation time: 1.6637670993804932\n",
      "Step: 917, Loss: 0.9582895636558533, Accuracy: 0.9375, Computation time: 1.6477863788604736\n",
      "Step: 918, Loss: 0.9161311984062195, Accuracy: 1.0, Computation time: 2.0023934841156006\n",
      "Step: 919, Loss: 0.9160345792770386, Accuracy: 1.0, Computation time: 1.6824724674224854\n",
      "Step: 920, Loss: 0.9164531230926514, Accuracy: 1.0, Computation time: 1.7098908424377441\n",
      "Step: 921, Loss: 0.915963351726532, Accuracy: 1.0, Computation time: 2.025921106338501\n",
      "Step: 922, Loss: 0.9160298705101013, Accuracy: 1.0, Computation time: 1.5626559257507324\n",
      "Step: 923, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.934539556503296\n",
      "Step: 924, Loss: 0.9159990549087524, Accuracy: 1.0, Computation time: 2.185722589492798\n",
      "Step: 925, Loss: 0.915923535823822, Accuracy: 1.0, Computation time: 1.6089136600494385\n",
      "Step: 926, Loss: 0.9159258008003235, Accuracy: 1.0, Computation time: 1.6054856777191162\n",
      "Step: 927, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 1.8429241180419922\n",
      "Step: 928, Loss: 0.9504139423370361, Accuracy: 0.96875, Computation time: 2.0972774028778076\n",
      "Step: 929, Loss: 0.9165343046188354, Accuracy: 1.0, Computation time: 1.9361538887023926\n",
      "Step: 930, Loss: 0.920242190361023, Accuracy: 1.0, Computation time: 2.3115811347961426\n",
      "Step: 931, Loss: 0.916314423084259, Accuracy: 1.0, Computation time: 1.9426772594451904\n",
      "Step: 932, Loss: 0.916736900806427, Accuracy: 1.0, Computation time: 1.623589277267456\n",
      "Step: 933, Loss: 0.9205794334411621, Accuracy: 1.0, Computation time: 1.6237308979034424\n",
      "Step: 934, Loss: 0.9172600507736206, Accuracy: 1.0, Computation time: 1.4250202178955078\n",
      "Step: 935, Loss: 0.9181826710700989, Accuracy: 1.0, Computation time: 1.5807499885559082\n",
      "Step: 936, Loss: 0.9166541695594788, Accuracy: 1.0, Computation time: 2.1158206462860107\n",
      "Step: 937, Loss: 0.9164257049560547, Accuracy: 1.0, Computation time: 1.7805006504058838\n",
      "Step: 938, Loss: 0.91647869348526, Accuracy: 1.0, Computation time: 1.539116621017456\n",
      "Step: 939, Loss: 0.9166489243507385, Accuracy: 1.0, Computation time: 1.6615166664123535\n",
      "Step: 940, Loss: 0.9162212610244751, Accuracy: 1.0, Computation time: 1.977576494216919\n",
      "Step: 941, Loss: 0.916267454624176, Accuracy: 1.0, Computation time: 1.9878270626068115\n",
      "Step: 942, Loss: 0.9170354008674622, Accuracy: 1.0, Computation time: 1.6256074905395508\n",
      "Step: 943, Loss: 0.9383429884910583, Accuracy: 0.96875, Computation time: 1.8940420150756836\n",
      "Step: 944, Loss: 0.9166674017906189, Accuracy: 1.0, Computation time: 1.565079927444458\n",
      "Step: 945, Loss: 0.9379070997238159, Accuracy: 0.96875, Computation time: 2.095578908920288\n",
      "Step: 946, Loss: 0.9161906242370605, Accuracy: 1.0, Computation time: 1.907947301864624\n",
      "Step: 947, Loss: 0.9388836026191711, Accuracy: 0.96875, Computation time: 1.9266071319580078\n",
      "Step: 948, Loss: 0.9372522830963135, Accuracy: 0.96875, Computation time: 1.8674159049987793\n",
      "Step: 949, Loss: 0.9162137508392334, Accuracy: 1.0, Computation time: 2.130496025085449\n",
      "Step: 950, Loss: 0.9161748886108398, Accuracy: 1.0, Computation time: 1.4379401206970215\n",
      "Step: 951, Loss: 0.9162366986274719, Accuracy: 1.0, Computation time: 1.6631321907043457\n",
      "Step: 952, Loss: 0.9602482318878174, Accuracy: 0.9375, Computation time: 1.9438114166259766\n",
      "Step: 953, Loss: 0.9160854816436768, Accuracy: 1.0, Computation time: 1.9752979278564453\n",
      "Step: 954, Loss: 0.9162881374359131, Accuracy: 1.0, Computation time: 1.878469467163086\n",
      "Step: 955, Loss: 0.9159654378890991, Accuracy: 1.0, Computation time: 1.5834376811981201\n",
      "Step: 956, Loss: 0.9160818457603455, Accuracy: 1.0, Computation time: 1.4934635162353516\n",
      "Step: 957, Loss: 0.9160929322242737, Accuracy: 1.0, Computation time: 1.9566996097564697\n",
      "Step: 958, Loss: 0.9161748290061951, Accuracy: 1.0, Computation time: 1.4179859161376953\n",
      "Step: 959, Loss: 0.9381234645843506, Accuracy: 0.96875, Computation time: 2.1514947414398193\n",
      "Step: 960, Loss: 0.9161626100540161, Accuracy: 1.0, Computation time: 2.0263993740081787\n",
      "Step: 961, Loss: 0.9162786602973938, Accuracy: 1.0, Computation time: 1.8589906692504883\n",
      "Step: 962, Loss: 0.9162455201148987, Accuracy: 1.0, Computation time: 2.19165301322937\n",
      "Step: 963, Loss: 0.9160520434379578, Accuracy: 1.0, Computation time: 1.6961112022399902\n",
      "Step: 964, Loss: 0.9365734457969666, Accuracy: 0.96875, Computation time: 1.8634507656097412\n",
      "Step: 965, Loss: 0.9159960746765137, Accuracy: 1.0, Computation time: 1.639296054840088\n",
      "Step: 966, Loss: 0.91607666015625, Accuracy: 1.0, Computation time: 2.16548228263855\n",
      "Step: 967, Loss: 0.9160654544830322, Accuracy: 1.0, Computation time: 1.7571005821228027\n",
      "Step: 968, Loss: 0.9317263960838318, Accuracy: 0.96875, Computation time: 2.1823537349700928\n",
      "Step: 969, Loss: 0.938189685344696, Accuracy: 0.96875, Computation time: 1.9065759181976318\n",
      "Step: 970, Loss: 0.9494503736495972, Accuracy: 0.9375, Computation time: 2.089296340942383\n",
      "Step: 971, Loss: 0.9376556873321533, Accuracy: 0.96875, Computation time: 2.0514559745788574\n",
      "Step: 972, Loss: 0.9594012498855591, Accuracy: 0.9375, Computation time: 1.9156994819641113\n",
      "Step: 973, Loss: 0.937329113483429, Accuracy: 0.96875, Computation time: 2.1038427352905273\n",
      "########################\n",
      "Test loss: 1.0771386623382568, Test Accuracy_epoch7: 0.7662463784217834\n",
      "########################\n",
      "Step: 974, Loss: 0.9371494650840759, Accuracy: 0.96875, Computation time: 1.8396048545837402\n",
      "Step: 975, Loss: 0.9196523427963257, Accuracy: 1.0, Computation time: 2.1738157272338867\n",
      "Step: 976, Loss: 0.9160621166229248, Accuracy: 1.0, Computation time: 1.5507187843322754\n",
      "Step: 977, Loss: 0.9160480499267578, Accuracy: 1.0, Computation time: 1.9363253116607666\n",
      "Step: 978, Loss: 0.9304805397987366, Accuracy: 0.96875, Computation time: 1.9082667827606201\n",
      "Step: 979, Loss: 0.9202056527137756, Accuracy: 1.0, Computation time: 2.3877556324005127\n",
      "Step: 980, Loss: 0.916574239730835, Accuracy: 1.0, Computation time: 1.8464889526367188\n",
      "Step: 981, Loss: 0.9165667295455933, Accuracy: 1.0, Computation time: 1.6608436107635498\n",
      "Step: 982, Loss: 0.9368234276771545, Accuracy: 0.96875, Computation time: 1.9027552604675293\n",
      "Step: 983, Loss: 0.916678249835968, Accuracy: 1.0, Computation time: 1.5842230319976807\n",
      "Step: 984, Loss: 0.9163202047348022, Accuracy: 1.0, Computation time: 2.0285582542419434\n",
      "Step: 985, Loss: 0.9199313521385193, Accuracy: 1.0, Computation time: 1.6108736991882324\n",
      "Step: 986, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 1.7643864154815674\n",
      "Step: 987, Loss: 0.9165120124816895, Accuracy: 1.0, Computation time: 1.9814367294311523\n",
      "Step: 988, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.4793055057525635\n",
      "Step: 989, Loss: 0.9186094403266907, Accuracy: 1.0, Computation time: 2.008385419845581\n",
      "Step: 990, Loss: 0.916054904460907, Accuracy: 1.0, Computation time: 1.6439387798309326\n",
      "Step: 991, Loss: 0.916245698928833, Accuracy: 1.0, Computation time: 1.5685803890228271\n",
      "Step: 992, Loss: 0.9220243692398071, Accuracy: 1.0, Computation time: 1.7592568397521973\n",
      "Step: 993, Loss: 0.9161583185195923, Accuracy: 1.0, Computation time: 1.464759349822998\n",
      "Step: 994, Loss: 0.9380314350128174, Accuracy: 0.96875, Computation time: 1.5548925399780273\n",
      "Step: 995, Loss: 0.9172907471656799, Accuracy: 1.0, Computation time: 1.978515625\n",
      "Step: 996, Loss: 0.9186010956764221, Accuracy: 1.0, Computation time: 1.6330819129943848\n",
      "Step: 997, Loss: 0.9164595007896423, Accuracy: 1.0, Computation time: 1.7045307159423828\n",
      "Step: 998, Loss: 0.9380543828010559, Accuracy: 0.96875, Computation time: 3.173168420791626\n",
      "Step: 999, Loss: 0.9230936765670776, Accuracy: 1.0, Computation time: 1.9602546691894531\n",
      "Step: 1000, Loss: 0.916073739528656, Accuracy: 1.0, Computation time: 2.058899164199829\n",
      "Step: 1001, Loss: 0.9161735773086548, Accuracy: 1.0, Computation time: 1.7892742156982422\n",
      "Step: 1002, Loss: 0.9372096061706543, Accuracy: 0.96875, Computation time: 1.611680507659912\n",
      "Step: 1003, Loss: 0.9161309599876404, Accuracy: 1.0, Computation time: 1.4389715194702148\n",
      "Step: 1004, Loss: 0.9166339635848999, Accuracy: 1.0, Computation time: 2.9293696880340576\n",
      "Step: 1005, Loss: 0.9163093566894531, Accuracy: 1.0, Computation time: 1.5283594131469727\n",
      "Step: 1006, Loss: 0.916255533695221, Accuracy: 1.0, Computation time: 1.4845755100250244\n",
      "Step: 1007, Loss: 0.9162454009056091, Accuracy: 1.0, Computation time: 1.6037485599517822\n",
      "Step: 1008, Loss: 0.9591798782348633, Accuracy: 0.9375, Computation time: 1.8318226337432861\n",
      "Step: 1009, Loss: 0.9159618020057678, Accuracy: 1.0, Computation time: 1.6715586185455322\n",
      "Step: 1010, Loss: 0.9159426689147949, Accuracy: 1.0, Computation time: 1.7274913787841797\n",
      "Step: 1011, Loss: 0.9159627556800842, Accuracy: 1.0, Computation time: 1.6571059226989746\n",
      "Step: 1012, Loss: 0.9183511137962341, Accuracy: 1.0, Computation time: 2.0490732192993164\n",
      "Step: 1013, Loss: 0.9215371608734131, Accuracy: 1.0, Computation time: 1.8719112873077393\n",
      "Step: 1014, Loss: 0.9165831804275513, Accuracy: 1.0, Computation time: 1.718876838684082\n",
      "Step: 1015, Loss: 0.9360578656196594, Accuracy: 0.96875, Computation time: 1.6769342422485352\n",
      "Step: 1016, Loss: 0.9160550832748413, Accuracy: 1.0, Computation time: 1.5411415100097656\n",
      "Step: 1017, Loss: 0.9378275871276855, Accuracy: 0.96875, Computation time: 1.6584036350250244\n",
      "Step: 1018, Loss: 0.9377972483634949, Accuracy: 0.96875, Computation time: 1.773869276046753\n",
      "Step: 1019, Loss: 0.9162376523017883, Accuracy: 1.0, Computation time: 1.673421859741211\n",
      "Step: 1020, Loss: 0.9389018416404724, Accuracy: 0.96875, Computation time: 1.7185626029968262\n",
      "Step: 1021, Loss: 0.9198988676071167, Accuracy: 1.0, Computation time: 1.5455398559570312\n",
      "Step: 1022, Loss: 0.9162268042564392, Accuracy: 1.0, Computation time: 1.7923266887664795\n",
      "Step: 1023, Loss: 0.9160026907920837, Accuracy: 1.0, Computation time: 1.6668553352355957\n",
      "Step: 1024, Loss: 0.9167584180831909, Accuracy: 1.0, Computation time: 2.1235668659210205\n",
      "Step: 1025, Loss: 0.9166525602340698, Accuracy: 1.0, Computation time: 1.5348072052001953\n",
      "Step: 1026, Loss: 0.9160580039024353, Accuracy: 1.0, Computation time: 1.9161405563354492\n",
      "Step: 1027, Loss: 0.9163844585418701, Accuracy: 1.0, Computation time: 1.687041997909546\n",
      "Step: 1028, Loss: 0.9201120734214783, Accuracy: 1.0, Computation time: 2.0671377182006836\n",
      "Step: 1029, Loss: 0.9161456823348999, Accuracy: 1.0, Computation time: 1.5326716899871826\n",
      "Step: 1030, Loss: 0.9162136912345886, Accuracy: 1.0, Computation time: 2.018008232116699\n",
      "Step: 1031, Loss: 0.9163177013397217, Accuracy: 1.0, Computation time: 1.9992787837982178\n",
      "Step: 1032, Loss: 0.9351338744163513, Accuracy: 0.96875, Computation time: 3.075119972229004\n",
      "Step: 1033, Loss: 0.9160372614860535, Accuracy: 1.0, Computation time: 1.6281025409698486\n",
      "Step: 1034, Loss: 0.9344160556793213, Accuracy: 0.96875, Computation time: 1.6773931980133057\n",
      "Step: 1035, Loss: 0.9160887598991394, Accuracy: 1.0, Computation time: 2.209679365158081\n",
      "Step: 1036, Loss: 0.917026162147522, Accuracy: 1.0, Computation time: 1.7350215911865234\n",
      "Step: 1037, Loss: 0.9372522234916687, Accuracy: 0.96875, Computation time: 1.6660032272338867\n",
      "Step: 1038, Loss: 0.9161520600318909, Accuracy: 1.0, Computation time: 1.5365538597106934\n",
      "Step: 1039, Loss: 0.9160523414611816, Accuracy: 1.0, Computation time: 1.735398769378662\n",
      "Step: 1040, Loss: 0.9159584641456604, Accuracy: 1.0, Computation time: 1.5719077587127686\n",
      "Step: 1041, Loss: 0.9159464836120605, Accuracy: 1.0, Computation time: 1.472884178161621\n",
      "Step: 1042, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.8145508766174316\n",
      "Step: 1043, Loss: 0.9159799814224243, Accuracy: 1.0, Computation time: 1.6379444599151611\n",
      "Step: 1044, Loss: 0.9220210313796997, Accuracy: 1.0, Computation time: 1.6659603118896484\n",
      "Step: 1045, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.6789746284484863\n",
      "Step: 1046, Loss: 0.9161444306373596, Accuracy: 1.0, Computation time: 1.4497714042663574\n",
      "Step: 1047, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.7057945728302002\n",
      "Step: 1048, Loss: 0.9163811802864075, Accuracy: 1.0, Computation time: 1.4791538715362549\n",
      "Step: 1049, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.3521106243133545\n",
      "Step: 1050, Loss: 0.9160979390144348, Accuracy: 1.0, Computation time: 1.6866986751556396\n",
      "Step: 1051, Loss: 0.9159358143806458, Accuracy: 1.0, Computation time: 1.6230332851409912\n",
      "Step: 1052, Loss: 0.9178979992866516, Accuracy: 1.0, Computation time: 1.8912711143493652\n",
      "Step: 1053, Loss: 0.9170984625816345, Accuracy: 1.0, Computation time: 1.473250389099121\n",
      "Step: 1054, Loss: 0.9159104228019714, Accuracy: 1.0, Computation time: 1.291926383972168\n",
      "Step: 1055, Loss: 0.915922999382019, Accuracy: 1.0, Computation time: 1.3744657039642334\n",
      "Step: 1056, Loss: 0.9169448614120483, Accuracy: 1.0, Computation time: 1.6057209968566895\n",
      "Step: 1057, Loss: 0.9158875346183777, Accuracy: 1.0, Computation time: 1.7525982856750488\n",
      "Step: 1058, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.2783136367797852\n",
      "Step: 1059, Loss: 0.9160223603248596, Accuracy: 1.0, Computation time: 1.7210822105407715\n",
      "Step: 1060, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.8205890655517578\n",
      "Step: 1061, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 1.341615915298462\n",
      "Step: 1062, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.379204511642456\n",
      "Step: 1063, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.8019795417785645\n",
      "Step: 1064, Loss: 0.9368152022361755, Accuracy: 0.96875, Computation time: 1.9236347675323486\n",
      "Step: 1065, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.3094394207000732\n",
      "Step: 1066, Loss: 0.915931224822998, Accuracy: 1.0, Computation time: 1.5978810787200928\n",
      "Step: 1067, Loss: 0.9165478944778442, Accuracy: 1.0, Computation time: 1.6489691734313965\n",
      "Step: 1068, Loss: 0.9166297316551208, Accuracy: 1.0, Computation time: 2.3297455310821533\n",
      "Step: 1069, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.5851008892059326\n",
      "Step: 1070, Loss: 0.9164267778396606, Accuracy: 1.0, Computation time: 1.8530471324920654\n",
      "Step: 1071, Loss: 0.9161214232444763, Accuracy: 1.0, Computation time: 1.7859935760498047\n",
      "Step: 1072, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.4486477375030518\n",
      "Step: 1073, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.89764404296875\n",
      "Step: 1074, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 1.6918206214904785\n",
      "Step: 1075, Loss: 0.9374769330024719, Accuracy: 0.96875, Computation time: 2.24200439453125\n",
      "Step: 1076, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.3221404552459717\n",
      "Step: 1077, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 1.7866480350494385\n",
      "Step: 1078, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.544524908065796\n",
      "Step: 1079, Loss: 0.9159145951271057, Accuracy: 1.0, Computation time: 1.5148680210113525\n",
      "Step: 1080, Loss: 0.917748212814331, Accuracy: 1.0, Computation time: 3.54988694190979\n",
      "Step: 1081, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 1.535571575164795\n",
      "Step: 1082, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.5636100769042969\n",
      "Step: 1083, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.8066625595092773\n",
      "Step: 1084, Loss: 0.9171207547187805, Accuracy: 1.0, Computation time: 1.8468818664550781\n",
      "Step: 1085, Loss: 0.9159241318702698, Accuracy: 1.0, Computation time: 1.5680866241455078\n",
      "Step: 1086, Loss: 0.9160882830619812, Accuracy: 1.0, Computation time: 1.9743914604187012\n",
      "Step: 1087, Loss: 0.9377238154411316, Accuracy: 0.96875, Computation time: 1.8797760009765625\n",
      "Step: 1088, Loss: 0.9318885803222656, Accuracy: 0.96875, Computation time: 1.9281554222106934\n",
      "Step: 1089, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.4563584327697754\n",
      "Step: 1090, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.441347360610962\n",
      "Step: 1091, Loss: 0.9159334301948547, Accuracy: 1.0, Computation time: 1.7681751251220703\n",
      "Step: 1092, Loss: 0.9159374237060547, Accuracy: 1.0, Computation time: 1.6203322410583496\n",
      "Step: 1093, Loss: 0.9370212554931641, Accuracy: 0.96875, Computation time: 1.836848258972168\n",
      "Step: 1094, Loss: 0.916009783744812, Accuracy: 1.0, Computation time: 2.00148868560791\n",
      "Step: 1095, Loss: 0.9160413146018982, Accuracy: 1.0, Computation time: 1.7467896938323975\n",
      "Step: 1096, Loss: 0.9576399326324463, Accuracy: 0.9375, Computation time: 1.4735190868377686\n",
      "Step: 1097, Loss: 0.9193849563598633, Accuracy: 1.0, Computation time: 1.7737452983856201\n",
      "Step: 1098, Loss: 0.9160615801811218, Accuracy: 1.0, Computation time: 1.8544726371765137\n",
      "Step: 1099, Loss: 0.9346529245376587, Accuracy: 0.96875, Computation time: 1.6034915447235107\n",
      "Step: 1100, Loss: 0.9159290194511414, Accuracy: 1.0, Computation time: 1.753807544708252\n",
      "Step: 1101, Loss: 0.9253255724906921, Accuracy: 1.0, Computation time: 2.6254544258117676\n",
      "Step: 1102, Loss: 0.952540934085846, Accuracy: 0.9375, Computation time: 2.623014211654663\n",
      "Step: 1103, Loss: 0.9170796275138855, Accuracy: 1.0, Computation time: 1.5635697841644287\n",
      "Step: 1104, Loss: 0.9165471792221069, Accuracy: 1.0, Computation time: 1.854353427886963\n",
      "Step: 1105, Loss: 0.9164646863937378, Accuracy: 1.0, Computation time: 1.9029834270477295\n",
      "Step: 1106, Loss: 0.9289849996566772, Accuracy: 0.96875, Computation time: 1.9203293323516846\n",
      "Step: 1107, Loss: 0.9163550138473511, Accuracy: 1.0, Computation time: 2.088797092437744\n",
      "Step: 1108, Loss: 0.9350565075874329, Accuracy: 0.96875, Computation time: 1.7732336521148682\n",
      "Step: 1109, Loss: 0.9160343408584595, Accuracy: 1.0, Computation time: 1.6385891437530518\n",
      "Step: 1110, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.4709351062774658\n",
      "Step: 1111, Loss: 0.9160794019699097, Accuracy: 1.0, Computation time: 1.4760572910308838\n",
      "Step: 1112, Loss: 0.9340037703514099, Accuracy: 0.96875, Computation time: 2.319843292236328\n",
      "########################\n",
      "Test loss: 1.0695267915725708, Test Accuracy_epoch8: 0.7740058302879333\n",
      "########################\n",
      "Step: 1113, Loss: 0.9160528779029846, Accuracy: 1.0, Computation time: 1.5543434619903564\n",
      "Step: 1114, Loss: 0.9163792729377747, Accuracy: 1.0, Computation time: 1.9802484512329102\n",
      "Step: 1115, Loss: 0.9162921905517578, Accuracy: 1.0, Computation time: 1.556811809539795\n",
      "Step: 1116, Loss: 0.916266918182373, Accuracy: 1.0, Computation time: 1.9283709526062012\n",
      "Step: 1117, Loss: 0.9162160158157349, Accuracy: 1.0, Computation time: 1.7211463451385498\n",
      "Step: 1118, Loss: 0.9392819404602051, Accuracy: 0.96875, Computation time: 1.4231996536254883\n",
      "Step: 1119, Loss: 0.9163466095924377, Accuracy: 1.0, Computation time: 2.0910909175872803\n",
      "Step: 1120, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.4888916015625\n",
      "Step: 1121, Loss: 0.916015088558197, Accuracy: 1.0, Computation time: 1.5387449264526367\n",
      "Step: 1122, Loss: 0.9161232113838196, Accuracy: 1.0, Computation time: 1.8824045658111572\n",
      "Step: 1123, Loss: 0.9159879088401794, Accuracy: 1.0, Computation time: 1.6944987773895264\n",
      "Step: 1124, Loss: 0.9379580616950989, Accuracy: 0.96875, Computation time: 1.7260935306549072\n",
      "Step: 1125, Loss: 0.9320931434631348, Accuracy: 0.96875, Computation time: 2.210397243499756\n",
      "Step: 1126, Loss: 0.9159917831420898, Accuracy: 1.0, Computation time: 1.838592767715454\n",
      "Step: 1127, Loss: 0.9178251028060913, Accuracy: 1.0, Computation time: 1.8909156322479248\n",
      "Step: 1128, Loss: 0.9160182476043701, Accuracy: 1.0, Computation time: 1.8013160228729248\n",
      "Step: 1129, Loss: 0.9162163734436035, Accuracy: 1.0, Computation time: 1.6283032894134521\n",
      "Step: 1130, Loss: 0.9260858297348022, Accuracy: 0.96875, Computation time: 1.9167389869689941\n",
      "Step: 1131, Loss: 0.9251468777656555, Accuracy: 1.0, Computation time: 1.5751996040344238\n",
      "Step: 1132, Loss: 0.915989875793457, Accuracy: 1.0, Computation time: 1.702183485031128\n",
      "Step: 1133, Loss: 0.9160463213920593, Accuracy: 1.0, Computation time: 1.5121655464172363\n",
      "Step: 1134, Loss: 0.9169954657554626, Accuracy: 1.0, Computation time: 1.7970733642578125\n",
      "Step: 1135, Loss: 0.9299522042274475, Accuracy: 0.96875, Computation time: 1.6929361820220947\n",
      "Step: 1136, Loss: 0.9160796403884888, Accuracy: 1.0, Computation time: 2.0985071659088135\n",
      "Step: 1137, Loss: 0.9160187244415283, Accuracy: 1.0, Computation time: 1.8866169452667236\n",
      "Step: 1138, Loss: 0.9161574840545654, Accuracy: 1.0, Computation time: 1.835947036743164\n",
      "Step: 1139, Loss: 0.9189301133155823, Accuracy: 1.0, Computation time: 2.0255470275878906\n",
      "Step: 1140, Loss: 0.9159609079360962, Accuracy: 1.0, Computation time: 1.9543826580047607\n",
      "Step: 1141, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.830967664718628\n",
      "Step: 1142, Loss: 0.925664484500885, Accuracy: 0.96875, Computation time: 1.9521276950836182\n",
      "Step: 1143, Loss: 0.9222065806388855, Accuracy: 1.0, Computation time: 2.219090700149536\n",
      "Step: 1144, Loss: 0.9171393513679504, Accuracy: 1.0, Computation time: 2.098933219909668\n",
      "Step: 1145, Loss: 0.9578483700752258, Accuracy: 0.9375, Computation time: 1.7679836750030518\n",
      "Step: 1146, Loss: 0.9164064526557922, Accuracy: 1.0, Computation time: 1.856964111328125\n",
      "Step: 1147, Loss: 0.9162407517433167, Accuracy: 1.0, Computation time: 2.1204633712768555\n",
      "Step: 1148, Loss: 0.9160979986190796, Accuracy: 1.0, Computation time: 1.7672269344329834\n",
      "Step: 1149, Loss: 0.9338477253913879, Accuracy: 0.96875, Computation time: 2.5266497135162354\n",
      "Step: 1150, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.9941210746765137\n",
      "Step: 1151, Loss: 0.9159125089645386, Accuracy: 1.0, Computation time: 1.9127562046051025\n",
      "Step: 1152, Loss: 0.923153817653656, Accuracy: 1.0, Computation time: 1.8548407554626465\n",
      "Step: 1153, Loss: 0.9209086894989014, Accuracy: 1.0, Computation time: 1.948310136795044\n",
      "Step: 1154, Loss: 0.9164745807647705, Accuracy: 1.0, Computation time: 1.57303786277771\n",
      "Step: 1155, Loss: 0.9163113832473755, Accuracy: 1.0, Computation time: 1.9274468421936035\n",
      "Step: 1156, Loss: 0.9360265731811523, Accuracy: 0.96875, Computation time: 1.6500420570373535\n",
      "Step: 1157, Loss: 0.9193099737167358, Accuracy: 1.0, Computation time: 2.1037344932556152\n",
      "Step: 1158, Loss: 0.9159877300262451, Accuracy: 1.0, Computation time: 1.971864938735962\n",
      "Step: 1159, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.8280606269836426\n",
      "Step: 1160, Loss: 0.9365625977516174, Accuracy: 0.96875, Computation time: 1.780637264251709\n",
      "Step: 1161, Loss: 0.9159653186798096, Accuracy: 1.0, Computation time: 1.8357632160186768\n",
      "Step: 1162, Loss: 0.9175441265106201, Accuracy: 1.0, Computation time: 2.2956764698028564\n",
      "Step: 1163, Loss: 0.9332900047302246, Accuracy: 0.96875, Computation time: 1.8030540943145752\n",
      "Step: 1164, Loss: 0.9160215854644775, Accuracy: 1.0, Computation time: 1.6226258277893066\n",
      "Step: 1165, Loss: 0.9161003828048706, Accuracy: 1.0, Computation time: 1.71732759475708\n",
      "Step: 1166, Loss: 0.9160165786743164, Accuracy: 1.0, Computation time: 1.868232011795044\n",
      "Step: 1167, Loss: 0.9194756746292114, Accuracy: 1.0, Computation time: 1.6926558017730713\n",
      "Step: 1168, Loss: 0.9166075587272644, Accuracy: 1.0, Computation time: 1.6045451164245605\n",
      "Step: 1169, Loss: 0.9374144673347473, Accuracy: 0.96875, Computation time: 2.2006051540374756\n",
      "Step: 1170, Loss: 0.9164727926254272, Accuracy: 1.0, Computation time: 1.7979676723480225\n",
      "Step: 1171, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.4013619422912598\n",
      "Step: 1172, Loss: 0.9375855326652527, Accuracy: 0.96875, Computation time: 1.6166179180145264\n",
      "Step: 1173, Loss: 0.9160267114639282, Accuracy: 1.0, Computation time: 1.562746524810791\n",
      "Step: 1174, Loss: 0.916034460067749, Accuracy: 1.0, Computation time: 1.7210001945495605\n",
      "Step: 1175, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.589888572692871\n",
      "Step: 1176, Loss: 0.9174039959907532, Accuracy: 1.0, Computation time: 1.8956680297851562\n",
      "Step: 1177, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.8763606548309326\n",
      "Step: 1178, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 1.741936206817627\n",
      "Step: 1179, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.9107606410980225\n",
      "Step: 1180, Loss: 0.9164194464683533, Accuracy: 1.0, Computation time: 1.9888241291046143\n",
      "Step: 1181, Loss: 0.9376084208488464, Accuracy: 0.96875, Computation time: 1.5477960109710693\n",
      "Step: 1182, Loss: 0.9159989953041077, Accuracy: 1.0, Computation time: 1.6499137878417969\n",
      "Step: 1183, Loss: 0.9159939289093018, Accuracy: 1.0, Computation time: 1.890899896621704\n",
      "Step: 1184, Loss: 0.9158942699432373, Accuracy: 1.0, Computation time: 1.7723801136016846\n",
      "Step: 1185, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.5943036079406738\n",
      "Step: 1186, Loss: 0.9204503893852234, Accuracy: 1.0, Computation time: 2.1721279621124268\n",
      "Step: 1187, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 2.133246660232544\n",
      "Step: 1188, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.658857822418213\n",
      "Step: 1189, Loss: 0.9159673452377319, Accuracy: 1.0, Computation time: 1.6957833766937256\n",
      "Step: 1190, Loss: 0.9196864366531372, Accuracy: 1.0, Computation time: 1.8419153690338135\n",
      "Step: 1191, Loss: 0.9159732460975647, Accuracy: 1.0, Computation time: 1.8486249446868896\n",
      "Step: 1192, Loss: 0.9159777760505676, Accuracy: 1.0, Computation time: 1.6389193534851074\n",
      "Step: 1193, Loss: 0.9160738587379456, Accuracy: 1.0, Computation time: 1.9438319206237793\n",
      "Step: 1194, Loss: 0.9160157442092896, Accuracy: 1.0, Computation time: 1.6319184303283691\n",
      "Step: 1195, Loss: 0.9375368356704712, Accuracy: 0.96875, Computation time: 1.8579866886138916\n",
      "Step: 1196, Loss: 0.9159082770347595, Accuracy: 1.0, Computation time: 1.7919530868530273\n",
      "Step: 1197, Loss: 0.9163543581962585, Accuracy: 1.0, Computation time: 2.0243849754333496\n",
      "Step: 1198, Loss: 0.9367839097976685, Accuracy: 0.96875, Computation time: 1.8334312438964844\n",
      "Step: 1199, Loss: 0.9370824694633484, Accuracy: 0.96875, Computation time: 1.7911319732666016\n",
      "Step: 1200, Loss: 0.9160330891609192, Accuracy: 1.0, Computation time: 1.9597198963165283\n",
      "Step: 1201, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 2.1591620445251465\n",
      "Step: 1202, Loss: 0.9372811317443848, Accuracy: 0.96875, Computation time: 1.669668436050415\n",
      "Step: 1203, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.9896330833435059\n",
      "Step: 1204, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.831824541091919\n",
      "Step: 1205, Loss: 0.9164053797721863, Accuracy: 1.0, Computation time: 1.9261572360992432\n",
      "Step: 1206, Loss: 0.9402318596839905, Accuracy: 0.96875, Computation time: 1.8101027011871338\n",
      "Step: 1207, Loss: 0.9163496494293213, Accuracy: 1.0, Computation time: 1.8559763431549072\n",
      "Step: 1208, Loss: 0.9202216267585754, Accuracy: 1.0, Computation time: 1.7807071208953857\n",
      "Step: 1209, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.808173656463623\n",
      "Step: 1210, Loss: 0.9159898161888123, Accuracy: 1.0, Computation time: 1.9865655899047852\n",
      "Step: 1211, Loss: 0.9165306687355042, Accuracy: 1.0, Computation time: 1.8802015781402588\n",
      "Step: 1212, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.9322834014892578\n",
      "Step: 1213, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.5358555316925049\n",
      "Step: 1214, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.7302114963531494\n",
      "Step: 1215, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 2.14738130569458\n",
      "Step: 1216, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 1.6034386157989502\n",
      "Step: 1217, Loss: 0.9159408807754517, Accuracy: 1.0, Computation time: 1.4265222549438477\n",
      "Step: 1218, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 1.6922276020050049\n",
      "Step: 1219, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 1.7650368213653564\n",
      "Step: 1220, Loss: 0.9159680604934692, Accuracy: 1.0, Computation time: 1.8888218402862549\n",
      "Step: 1221, Loss: 0.9319770336151123, Accuracy: 0.96875, Computation time: 2.020214796066284\n",
      "Step: 1222, Loss: 0.9159737825393677, Accuracy: 1.0, Computation time: 1.9667987823486328\n",
      "Step: 1223, Loss: 0.9183173179626465, Accuracy: 1.0, Computation time: 2.0778422355651855\n",
      "Step: 1224, Loss: 0.9161417484283447, Accuracy: 1.0, Computation time: 1.7499146461486816\n",
      "Step: 1225, Loss: 0.9159500598907471, Accuracy: 1.0, Computation time: 1.419788122177124\n",
      "Step: 1226, Loss: 0.9380519390106201, Accuracy: 0.96875, Computation time: 1.5104496479034424\n",
      "Step: 1227, Loss: 0.9159795045852661, Accuracy: 1.0, Computation time: 1.4601161479949951\n",
      "Step: 1228, Loss: 0.9175964593887329, Accuracy: 1.0, Computation time: 1.7430520057678223\n",
      "Step: 1229, Loss: 0.916388988494873, Accuracy: 1.0, Computation time: 2.0828781127929688\n",
      "Step: 1230, Loss: 0.9415034055709839, Accuracy: 0.96875, Computation time: 2.473747968673706\n",
      "Step: 1231, Loss: 0.9159620404243469, Accuracy: 1.0, Computation time: 1.5294873714447021\n",
      "Step: 1232, Loss: 0.915984034538269, Accuracy: 1.0, Computation time: 1.3769617080688477\n",
      "Step: 1233, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.438905954360962\n",
      "Step: 1234, Loss: 0.9159754514694214, Accuracy: 1.0, Computation time: 1.5623106956481934\n",
      "Step: 1235, Loss: 0.9160027503967285, Accuracy: 1.0, Computation time: 1.672685146331787\n",
      "Step: 1236, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 1.735964298248291\n",
      "Step: 1237, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.484327793121338\n",
      "Step: 1238, Loss: 0.9375045895576477, Accuracy: 0.96875, Computation time: 1.849806308746338\n",
      "Step: 1239, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.3768253326416016\n",
      "Step: 1240, Loss: 0.932985246181488, Accuracy: 0.96875, Computation time: 1.515824317932129\n",
      "Step: 1241, Loss: 0.9161167144775391, Accuracy: 1.0, Computation time: 1.5177967548370361\n",
      "Step: 1242, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.4914062023162842\n",
      "Step: 1243, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 1.5788488388061523\n",
      "Step: 1244, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.4022634029388428\n",
      "Step: 1245, Loss: 0.9350804090499878, Accuracy: 0.96875, Computation time: 1.6687700748443604\n",
      "Step: 1246, Loss: 0.9161470532417297, Accuracy: 1.0, Computation time: 1.6963920593261719\n",
      "Step: 1247, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.4857392311096191\n",
      "Step: 1248, Loss: 0.9376746416091919, Accuracy: 0.96875, Computation time: 1.3124215602874756\n",
      "Step: 1249, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.165024757385254\n",
      "Step: 1250, Loss: 0.9159325957298279, Accuracy: 1.0, Computation time: 1.4181547164916992\n",
      "Step: 1251, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.4791984558105469\n",
      "########################\n",
      "Test loss: 1.070365071296692, Test Accuracy_epoch9: 0.7730358839035034\n",
      "########################\n",
      "Step: 1252, Loss: 0.9159797430038452, Accuracy: 1.0, Computation time: 1.9770853519439697\n",
      "Step: 1253, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.4916050434112549\n",
      "Step: 1254, Loss: 0.9319437146186829, Accuracy: 0.96875, Computation time: 1.7768352031707764\n",
      "Step: 1255, Loss: 0.9369006156921387, Accuracy: 0.96875, Computation time: 1.4192779064178467\n",
      "Step: 1256, Loss: 0.9171990156173706, Accuracy: 1.0, Computation time: 1.5415456295013428\n",
      "Step: 1257, Loss: 0.9160166382789612, Accuracy: 1.0, Computation time: 1.6278784275054932\n",
      "Step: 1258, Loss: 0.9159882664680481, Accuracy: 1.0, Computation time: 1.6273272037506104\n",
      "Step: 1259, Loss: 0.9541504383087158, Accuracy: 0.9375, Computation time: 1.4155042171478271\n",
      "Step: 1260, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.6221117973327637\n",
      "Step: 1261, Loss: 0.9160609841346741, Accuracy: 1.0, Computation time: 1.4311106204986572\n",
      "Step: 1262, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 1.1695504188537598\n",
      "Step: 1263, Loss: 0.9182285070419312, Accuracy: 1.0, Computation time: 1.642627239227295\n",
      "Step: 1264, Loss: 0.9217731952667236, Accuracy: 1.0, Computation time: 2.1308441162109375\n",
      "Step: 1265, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 1.3012850284576416\n",
      "Step: 1266, Loss: 0.9159790277481079, Accuracy: 1.0, Computation time: 1.4203565120697021\n",
      "Step: 1267, Loss: 0.9171016216278076, Accuracy: 1.0, Computation time: 2.0528900623321533\n",
      "Step: 1268, Loss: 0.9570468068122864, Accuracy: 0.9375, Computation time: 1.7364966869354248\n",
      "Step: 1269, Loss: 0.9376997947692871, Accuracy: 0.96875, Computation time: 1.350367784500122\n",
      "Step: 1270, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.3164103031158447\n",
      "Step: 1271, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.1400070190429688\n",
      "Step: 1272, Loss: 0.9567977786064148, Accuracy: 0.9375, Computation time: 1.4528000354766846\n",
      "Step: 1273, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.527845859527588\n",
      "Step: 1274, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.4723789691925049\n",
      "Step: 1275, Loss: 0.9170396327972412, Accuracy: 1.0, Computation time: 1.3695950508117676\n",
      "Step: 1276, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 1.3491196632385254\n",
      "Step: 1277, Loss: 0.9160446524620056, Accuracy: 1.0, Computation time: 1.4901182651519775\n",
      "Step: 1278, Loss: 0.9189195036888123, Accuracy: 1.0, Computation time: 1.6480133533477783\n",
      "Step: 1279, Loss: 0.9161525964736938, Accuracy: 1.0, Computation time: 1.372025728225708\n",
      "Step: 1280, Loss: 0.9160309433937073, Accuracy: 1.0, Computation time: 1.3393728733062744\n",
      "Step: 1281, Loss: 0.9161588549613953, Accuracy: 1.0, Computation time: 1.5647218227386475\n",
      "Step: 1282, Loss: 0.9160581231117249, Accuracy: 1.0, Computation time: 1.5766537189483643\n",
      "Step: 1283, Loss: 0.9365911483764648, Accuracy: 0.96875, Computation time: 1.178575038909912\n",
      "Step: 1284, Loss: 0.9180334806442261, Accuracy: 1.0, Computation time: 1.4593000411987305\n",
      "Step: 1285, Loss: 0.9364040493965149, Accuracy: 0.96875, Computation time: 1.8723692893981934\n",
      "Step: 1286, Loss: 0.9159807562828064, Accuracy: 1.0, Computation time: 1.6651506423950195\n",
      "Step: 1287, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.5371572971343994\n",
      "Step: 1288, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.4085516929626465\n",
      "Step: 1289, Loss: 0.9376382231712341, Accuracy: 0.96875, Computation time: 1.7764151096343994\n",
      "Step: 1290, Loss: 0.9159556031227112, Accuracy: 1.0, Computation time: 1.753615379333496\n",
      "Step: 1291, Loss: 0.937828004360199, Accuracy: 0.96875, Computation time: 2.384331464767456\n",
      "Step: 1292, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 1.5402028560638428\n",
      "Step: 1293, Loss: 0.9159136414527893, Accuracy: 1.0, Computation time: 1.4068219661712646\n",
      "Step: 1294, Loss: 0.9160845279693604, Accuracy: 1.0, Computation time: 1.7082445621490479\n",
      "Step: 1295, Loss: 0.9365457892417908, Accuracy: 0.96875, Computation time: 1.5327177047729492\n",
      "Step: 1296, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 1.5839154720306396\n",
      "Step: 1297, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.6762750148773193\n",
      "Step: 1298, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 1.3241684436798096\n",
      "Step: 1299, Loss: 0.9210866093635559, Accuracy: 1.0, Computation time: 1.8031072616577148\n",
      "Step: 1300, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.53318190574646\n",
      "Step: 1301, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.2290987968444824\n",
      "Step: 1302, Loss: 0.9159961938858032, Accuracy: 1.0, Computation time: 1.6167511940002441\n",
      "Step: 1303, Loss: 0.9161553978919983, Accuracy: 1.0, Computation time: 1.6703505516052246\n",
      "Step: 1304, Loss: 0.9160385727882385, Accuracy: 1.0, Computation time: 1.2738471031188965\n",
      "Step: 1305, Loss: 0.9159938097000122, Accuracy: 1.0, Computation time: 1.9417650699615479\n",
      "Step: 1306, Loss: 0.9375998377799988, Accuracy: 0.96875, Computation time: 1.6381707191467285\n",
      "Step: 1307, Loss: 0.9159228801727295, Accuracy: 1.0, Computation time: 1.7489900588989258\n",
      "Step: 1308, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.3820130825042725\n",
      "Step: 1309, Loss: 0.9159291386604309, Accuracy: 1.0, Computation time: 1.6035902500152588\n",
      "Step: 1310, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.7954227924346924\n",
      "Step: 1311, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.5782217979431152\n",
      "Step: 1312, Loss: 0.916243314743042, Accuracy: 1.0, Computation time: 1.5715281963348389\n",
      "Step: 1313, Loss: 0.9163885116577148, Accuracy: 1.0, Computation time: 1.6083734035491943\n",
      "Step: 1314, Loss: 0.9159970283508301, Accuracy: 1.0, Computation time: 1.5995960235595703\n",
      "Step: 1315, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.70361328125\n",
      "Step: 1316, Loss: 0.925912082195282, Accuracy: 0.96875, Computation time: 1.5244777202606201\n",
      "Step: 1317, Loss: 0.9161036014556885, Accuracy: 1.0, Computation time: 1.6192402839660645\n",
      "Step: 1318, Loss: 0.9365974068641663, Accuracy: 0.96875, Computation time: 1.4807908535003662\n",
      "Step: 1319, Loss: 0.9161382913589478, Accuracy: 1.0, Computation time: 1.684565782546997\n",
      "Step: 1320, Loss: 0.91595458984375, Accuracy: 1.0, Computation time: 1.8393166065216064\n",
      "Step: 1321, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 1.577446460723877\n",
      "Step: 1322, Loss: 0.9213459491729736, Accuracy: 1.0, Computation time: 1.5995020866394043\n",
      "Step: 1323, Loss: 0.9159237742424011, Accuracy: 1.0, Computation time: 2.0447797775268555\n",
      "Step: 1324, Loss: 0.9338131546974182, Accuracy: 0.96875, Computation time: 1.5903599262237549\n",
      "Step: 1325, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 1.510300636291504\n",
      "Step: 1326, Loss: 0.9166434407234192, Accuracy: 1.0, Computation time: 1.909600019454956\n",
      "Step: 1327, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.7428765296936035\n",
      "Step: 1328, Loss: 0.9159953594207764, Accuracy: 1.0, Computation time: 1.6034791469573975\n",
      "Step: 1329, Loss: 0.9184420704841614, Accuracy: 1.0, Computation time: 1.6159982681274414\n",
      "Step: 1330, Loss: 0.9160751104354858, Accuracy: 1.0, Computation time: 1.7697279453277588\n",
      "Step: 1331, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 1.7499985694885254\n",
      "Step: 1332, Loss: 0.9366689920425415, Accuracy: 0.96875, Computation time: 2.143587589263916\n",
      "Step: 1333, Loss: 0.9164446592330933, Accuracy: 1.0, Computation time: 2.0863802433013916\n",
      "Step: 1334, Loss: 0.9159056544303894, Accuracy: 1.0, Computation time: 1.751753568649292\n",
      "Step: 1335, Loss: 0.915981113910675, Accuracy: 1.0, Computation time: 1.5564494132995605\n",
      "Step: 1336, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 1.6995270252227783\n",
      "Step: 1337, Loss: 0.9161487221717834, Accuracy: 1.0, Computation time: 2.0919106006622314\n",
      "Step: 1338, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 1.9325942993164062\n",
      "Step: 1339, Loss: 0.9163216352462769, Accuracy: 1.0, Computation time: 1.7856249809265137\n",
      "Step: 1340, Loss: 0.9160353541374207, Accuracy: 1.0, Computation time: 1.6820487976074219\n",
      "Step: 1341, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.655984878540039\n",
      "Step: 1342, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.426039695739746\n",
      "Step: 1343, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.8664567470550537\n",
      "Step: 1344, Loss: 0.9231413006782532, Accuracy: 1.0, Computation time: 2.0137875080108643\n",
      "Step: 1345, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 1.5370616912841797\n",
      "Step: 1346, Loss: 0.9168179035186768, Accuracy: 1.0, Computation time: 2.0296554565429688\n",
      "Step: 1347, Loss: 0.9204546213150024, Accuracy: 1.0, Computation time: 1.9684550762176514\n",
      "Step: 1348, Loss: 0.9328143000602722, Accuracy: 0.96875, Computation time: 1.9877612590789795\n",
      "Step: 1349, Loss: 0.9159603714942932, Accuracy: 1.0, Computation time: 1.4426283836364746\n",
      "Step: 1350, Loss: 0.9375799894332886, Accuracy: 0.96875, Computation time: 1.5919265747070312\n",
      "Step: 1351, Loss: 0.9159741401672363, Accuracy: 1.0, Computation time: 1.2557554244995117\n",
      "Step: 1352, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.2692737579345703\n",
      "Step: 1353, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.2575533390045166\n",
      "Step: 1354, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.733680009841919\n",
      "Step: 1355, Loss: 0.9374026656150818, Accuracy: 0.96875, Computation time: 1.1754143238067627\n",
      "Step: 1356, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.3987233638763428\n",
      "Step: 1357, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.3408842086791992\n",
      "Step: 1358, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.463745355606079\n",
      "Step: 1359, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.371429443359375\n",
      "Step: 1360, Loss: 0.9164456725120544, Accuracy: 1.0, Computation time: 2.2492518424987793\n",
      "Step: 1361, Loss: 0.915981113910675, Accuracy: 1.0, Computation time: 1.4810504913330078\n",
      "Step: 1362, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.312795877456665\n",
      "Step: 1363, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.4602203369140625\n",
      "Step: 1364, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.7134950160980225\n",
      "Step: 1365, Loss: 0.9159818291664124, Accuracy: 1.0, Computation time: 1.257500410079956\n",
      "Step: 1366, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 1.8863818645477295\n",
      "Step: 1367, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.240607738494873\n",
      "Step: 1368, Loss: 0.9324550628662109, Accuracy: 0.96875, Computation time: 1.6041600704193115\n",
      "Step: 1369, Loss: 0.9284626245498657, Accuracy: 0.96875, Computation time: 1.4921259880065918\n",
      "Step: 1370, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.4549343585968018\n",
      "Step: 1371, Loss: 0.9160371422767639, Accuracy: 1.0, Computation time: 1.6534247398376465\n",
      "Step: 1372, Loss: 0.9159175157546997, Accuracy: 1.0, Computation time: 1.3758213520050049\n",
      "Step: 1373, Loss: 0.9375651478767395, Accuracy: 0.96875, Computation time: 1.3879756927490234\n",
      "Step: 1374, Loss: 0.9161190986633301, Accuracy: 1.0, Computation time: 1.23795485496521\n",
      "Step: 1375, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.5201928615570068\n",
      "Step: 1376, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 1.825671911239624\n",
      "Step: 1377, Loss: 0.9159424304962158, Accuracy: 1.0, Computation time: 1.7125115394592285\n",
      "Step: 1378, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.7673578262329102\n",
      "Step: 1379, Loss: 0.937873125076294, Accuracy: 0.96875, Computation time: 1.4392359256744385\n",
      "Step: 1380, Loss: 0.9370108842849731, Accuracy: 0.96875, Computation time: 1.290874719619751\n",
      "Step: 1381, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.2762210369110107\n",
      "Step: 1382, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.2250747680664062\n",
      "Step: 1383, Loss: 0.915934145450592, Accuracy: 1.0, Computation time: 1.3659980297088623\n",
      "Step: 1384, Loss: 0.9158727526664734, Accuracy: 1.0, Computation time: 1.3248724937438965\n",
      "Step: 1385, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.310133934020996\n",
      "Step: 1386, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.3948571681976318\n",
      "Step: 1387, Loss: 0.915949821472168, Accuracy: 1.0, Computation time: 1.529844045639038\n",
      "Step: 1388, Loss: 0.9322890043258667, Accuracy: 0.96875, Computation time: 1.403970718383789\n",
      "Step: 1389, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.1222922801971436\n",
      "Step: 1390, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 1.6727674007415771\n",
      "########################\n",
      "Test loss: 1.0697635412216187, Test Accuracy_epoch10: 0.7778855562210083\n",
      "########################\n",
      "Step: 1391, Loss: 0.9374690055847168, Accuracy: 0.96875, Computation time: 1.6220738887786865\n",
      "Step: 1392, Loss: 0.9162055850028992, Accuracy: 1.0, Computation time: 1.6068081855773926\n",
      "Step: 1393, Loss: 0.9160465002059937, Accuracy: 1.0, Computation time: 1.3220641613006592\n",
      "Step: 1394, Loss: 0.91592937707901, Accuracy: 1.0, Computation time: 1.4696154594421387\n",
      "Step: 1395, Loss: 0.9260623455047607, Accuracy: 0.96875, Computation time: 1.6136131286621094\n",
      "Step: 1396, Loss: 0.9397274851799011, Accuracy: 0.96875, Computation time: 1.4888880252838135\n",
      "Step: 1397, Loss: 0.9376399517059326, Accuracy: 0.96875, Computation time: 1.4644672870635986\n",
      "Step: 1398, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.3404860496520996\n",
      "Step: 1399, Loss: 0.9163896441459656, Accuracy: 1.0, Computation time: 1.4299378395080566\n",
      "Step: 1400, Loss: 0.9160662293434143, Accuracy: 1.0, Computation time: 1.7509803771972656\n",
      "Step: 1401, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 1.468224287033081\n",
      "Step: 1402, Loss: 0.9159371852874756, Accuracy: 1.0, Computation time: 1.7903904914855957\n",
      "Step: 1403, Loss: 0.9159708619117737, Accuracy: 1.0, Computation time: 1.6907768249511719\n",
      "Step: 1404, Loss: 0.9159023761749268, Accuracy: 1.0, Computation time: 1.2689518928527832\n",
      "Step: 1405, Loss: 0.9415102601051331, Accuracy: 0.96875, Computation time: 1.5594604015350342\n",
      "Step: 1406, Loss: 0.9160370230674744, Accuracy: 1.0, Computation time: 1.8441264629364014\n",
      "Step: 1407, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.5047664642333984\n",
      "Step: 1408, Loss: 0.915977418422699, Accuracy: 1.0, Computation time: 1.8878631591796875\n",
      "Step: 1409, Loss: 0.9159477353096008, Accuracy: 1.0, Computation time: 1.496737003326416\n",
      "Step: 1410, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.7641489505767822\n",
      "Step: 1411, Loss: 0.9370695352554321, Accuracy: 0.96875, Computation time: 1.6549923419952393\n",
      "Step: 1412, Loss: 0.9290702939033508, Accuracy: 0.96875, Computation time: 2.1119813919067383\n",
      "Step: 1413, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.5757358074188232\n",
      "Step: 1414, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 1.5157074928283691\n",
      "Step: 1415, Loss: 0.9350686073303223, Accuracy: 0.96875, Computation time: 1.9136652946472168\n",
      "Step: 1416, Loss: 0.9319959878921509, Accuracy: 0.96875, Computation time: 1.850377082824707\n",
      "Step: 1417, Loss: 0.9161185026168823, Accuracy: 1.0, Computation time: 1.5811753273010254\n",
      "Step: 1418, Loss: 0.932732880115509, Accuracy: 0.96875, Computation time: 2.1416759490966797\n",
      "Step: 1419, Loss: 0.9744841456413269, Accuracy: 0.90625, Computation time: 2.093986749649048\n",
      "Step: 1420, Loss: 0.9161264300346375, Accuracy: 1.0, Computation time: 1.5379774570465088\n",
      "Step: 1421, Loss: 0.9388893246650696, Accuracy: 0.96875, Computation time: 1.7058000564575195\n",
      "Step: 1422, Loss: 0.9195831418037415, Accuracy: 1.0, Computation time: 1.6238369941711426\n",
      "Step: 1423, Loss: 0.9162270426750183, Accuracy: 1.0, Computation time: 1.9050023555755615\n",
      "Step: 1424, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 1.918086290359497\n",
      "Step: 1425, Loss: 0.9163715243339539, Accuracy: 1.0, Computation time: 2.094068765640259\n",
      "Step: 1426, Loss: 0.9301674962043762, Accuracy: 1.0, Computation time: 2.077789306640625\n",
      "Step: 1427, Loss: 0.936431884765625, Accuracy: 0.96875, Computation time: 1.7325830459594727\n",
      "Step: 1428, Loss: 0.9163606762886047, Accuracy: 1.0, Computation time: 1.381831169128418\n",
      "Step: 1429, Loss: 0.916634738445282, Accuracy: 1.0, Computation time: 1.6940290927886963\n",
      "Step: 1430, Loss: 0.9171326756477356, Accuracy: 1.0, Computation time: 2.1178293228149414\n",
      "Step: 1431, Loss: 0.9195232391357422, Accuracy: 1.0, Computation time: 1.8435986042022705\n",
      "Step: 1432, Loss: 0.9166122078895569, Accuracy: 1.0, Computation time: 1.563894271850586\n",
      "Step: 1433, Loss: 0.916201114654541, Accuracy: 1.0, Computation time: 1.8216819763183594\n",
      "Step: 1434, Loss: 0.9161173701286316, Accuracy: 1.0, Computation time: 1.676290512084961\n",
      "Step: 1435, Loss: 0.916031539440155, Accuracy: 1.0, Computation time: 2.099703073501587\n",
      "Step: 1436, Loss: 0.9159332513809204, Accuracy: 1.0, Computation time: 1.3643410205841064\n",
      "Step: 1437, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 1.5351295471191406\n",
      "Step: 1438, Loss: 0.91621994972229, Accuracy: 1.0, Computation time: 1.529439926147461\n",
      "Step: 1439, Loss: 0.9255895018577576, Accuracy: 0.96875, Computation time: 1.5448477268218994\n",
      "Step: 1440, Loss: 0.9162802696228027, Accuracy: 1.0, Computation time: 1.622105360031128\n",
      "Step: 1441, Loss: 0.9165242314338684, Accuracy: 1.0, Computation time: 1.7010395526885986\n",
      "Step: 1442, Loss: 0.9161729216575623, Accuracy: 1.0, Computation time: 1.371443271636963\n",
      "Step: 1443, Loss: 0.9162937998771667, Accuracy: 1.0, Computation time: 1.5292696952819824\n",
      "Step: 1444, Loss: 0.9161431193351746, Accuracy: 1.0, Computation time: 1.5018947124481201\n",
      "Step: 1445, Loss: 0.9167585372924805, Accuracy: 1.0, Computation time: 1.9746227264404297\n",
      "Step: 1446, Loss: 0.9538671374320984, Accuracy: 0.9375, Computation time: 1.7987349033355713\n",
      "Step: 1447, Loss: 0.9159391522407532, Accuracy: 1.0, Computation time: 1.3910934925079346\n",
      "Step: 1448, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 1.7936065196990967\n",
      "Step: 1449, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 1.847205638885498\n",
      "Step: 1450, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 2.0021543502807617\n",
      "Step: 1451, Loss: 0.9221363067626953, Accuracy: 1.0, Computation time: 1.929654598236084\n",
      "Step: 1452, Loss: 0.9162843823432922, Accuracy: 1.0, Computation time: 2.0016300678253174\n",
      "Step: 1453, Loss: 0.9162258505821228, Accuracy: 1.0, Computation time: 2.131498336791992\n",
      "Step: 1454, Loss: 0.9160857200622559, Accuracy: 1.0, Computation time: 1.7247464656829834\n",
      "Step: 1455, Loss: 0.9217201471328735, Accuracy: 1.0, Computation time: 2.0336825847625732\n",
      "Step: 1456, Loss: 0.9160290360450745, Accuracy: 1.0, Computation time: 1.5863065719604492\n",
      "Step: 1457, Loss: 0.915969729423523, Accuracy: 1.0, Computation time: 1.557924747467041\n",
      "Step: 1458, Loss: 0.9375160932540894, Accuracy: 0.96875, Computation time: 1.7637062072753906\n",
      "Step: 1459, Loss: 0.9375593662261963, Accuracy: 0.96875, Computation time: 1.952690839767456\n",
      "Step: 1460, Loss: 0.9161064028739929, Accuracy: 1.0, Computation time: 1.5349974632263184\n",
      "Step: 1461, Loss: 0.9159989953041077, Accuracy: 1.0, Computation time: 1.4996099472045898\n",
      "Step: 1462, Loss: 0.916084885597229, Accuracy: 1.0, Computation time: 1.3610410690307617\n",
      "Step: 1463, Loss: 0.9160116314888, Accuracy: 1.0, Computation time: 1.7394447326660156\n",
      "Step: 1464, Loss: 0.9160038828849792, Accuracy: 1.0, Computation time: 1.8934826850891113\n",
      "Step: 1465, Loss: 0.9165014028549194, Accuracy: 1.0, Computation time: 1.4719786643981934\n",
      "Step: 1466, Loss: 0.9159463047981262, Accuracy: 1.0, Computation time: 1.5783720016479492\n",
      "Step: 1467, Loss: 0.9159635305404663, Accuracy: 1.0, Computation time: 1.9850163459777832\n",
      "Step: 1468, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.6619699001312256\n",
      "Step: 1469, Loss: 0.916864275932312, Accuracy: 1.0, Computation time: 1.9116790294647217\n",
      "Step: 1470, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 1.5176525115966797\n",
      "Step: 1471, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 2.202784299850464\n",
      "Step: 1472, Loss: 0.91592937707901, Accuracy: 1.0, Computation time: 1.5704185962677002\n",
      "Step: 1473, Loss: 0.9162158966064453, Accuracy: 1.0, Computation time: 1.7527306079864502\n",
      "Step: 1474, Loss: 0.916175365447998, Accuracy: 1.0, Computation time: 1.7233779430389404\n",
      "Step: 1475, Loss: 0.9165360331535339, Accuracy: 1.0, Computation time: 1.8341617584228516\n",
      "Step: 1476, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.4909276962280273\n",
      "Step: 1477, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 1.6122586727142334\n",
      "Step: 1478, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.8557891845703125\n",
      "Step: 1479, Loss: 0.9162015318870544, Accuracy: 1.0, Computation time: 1.6875627040863037\n",
      "Step: 1480, Loss: 0.9159635305404663, Accuracy: 1.0, Computation time: 1.695356845855713\n",
      "Step: 1481, Loss: 0.9159935712814331, Accuracy: 1.0, Computation time: 1.6684274673461914\n",
      "Step: 1482, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.3817360401153564\n",
      "Step: 1483, Loss: 0.9362170696258545, Accuracy: 0.96875, Computation time: 1.6239707469940186\n",
      "Step: 1484, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.505345344543457\n",
      "Step: 1485, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.5708508491516113\n",
      "Step: 1486, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 1.3007774353027344\n",
      "Step: 1487, Loss: 0.9533519148826599, Accuracy: 0.9375, Computation time: 1.7528843879699707\n",
      "Step: 1488, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.5865147113800049\n",
      "Step: 1489, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.6421468257904053\n",
      "Step: 1490, Loss: 0.9162392020225525, Accuracy: 1.0, Computation time: 1.679215431213379\n",
      "Step: 1491, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.5545032024383545\n",
      "Step: 1492, Loss: 0.9160140156745911, Accuracy: 1.0, Computation time: 1.8806953430175781\n",
      "Step: 1493, Loss: 0.9159349203109741, Accuracy: 1.0, Computation time: 1.6518323421478271\n",
      "Step: 1494, Loss: 0.9377586841583252, Accuracy: 0.96875, Computation time: 1.712190866470337\n",
      "Step: 1495, Loss: 0.9169600605964661, Accuracy: 1.0, Computation time: 2.0772578716278076\n",
      "Step: 1496, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.5724444389343262\n",
      "Step: 1497, Loss: 0.9168642163276672, Accuracy: 1.0, Computation time: 1.6557185649871826\n",
      "Step: 1498, Loss: 0.9172747731208801, Accuracy: 1.0, Computation time: 2.030872106552124\n",
      "Step: 1499, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 1.4045026302337646\n",
      "Step: 1500, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 1.73726487159729\n",
      "Step: 1501, Loss: 0.9171115159988403, Accuracy: 1.0, Computation time: 1.6958441734313965\n",
      "Step: 1502, Loss: 0.9159350991249084, Accuracy: 1.0, Computation time: 1.4139513969421387\n",
      "Step: 1503, Loss: 0.9369877576828003, Accuracy: 0.96875, Computation time: 1.9971520900726318\n",
      "Step: 1504, Loss: 0.9168740510940552, Accuracy: 1.0, Computation time: 1.8652069568634033\n",
      "Step: 1505, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 1.697627067565918\n",
      "Step: 1506, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.2202627658843994\n",
      "Step: 1507, Loss: 0.9204839468002319, Accuracy: 1.0, Computation time: 1.717817783355713\n",
      "Step: 1508, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.853407859802246\n",
      "Step: 1509, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 1.8015921115875244\n",
      "Step: 1510, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.6434962749481201\n",
      "Step: 1511, Loss: 0.9375591278076172, Accuracy: 0.96875, Computation time: 1.6112182140350342\n",
      "Step: 1512, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 1.547192096710205\n",
      "Step: 1513, Loss: 0.9234155416488647, Accuracy: 1.0, Computation time: 1.7961061000823975\n",
      "Step: 1514, Loss: 0.9168327450752258, Accuracy: 1.0, Computation time: 1.7109730243682861\n",
      "Step: 1515, Loss: 0.917125940322876, Accuracy: 1.0, Computation time: 1.7471942901611328\n",
      "Step: 1516, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 1.4890878200531006\n",
      "Step: 1517, Loss: 0.9387624263763428, Accuracy: 0.96875, Computation time: 1.901789903640747\n",
      "Step: 1518, Loss: 0.9159566760063171, Accuracy: 1.0, Computation time: 1.5563793182373047\n",
      "Step: 1519, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 1.4884490966796875\n",
      "Step: 1520, Loss: 0.9161674976348877, Accuracy: 1.0, Computation time: 1.9752414226531982\n",
      "Step: 1521, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.6081349849700928\n",
      "Step: 1522, Loss: 0.9159157872200012, Accuracy: 1.0, Computation time: 1.2456870079040527\n",
      "Step: 1523, Loss: 0.9376181960105896, Accuracy: 0.96875, Computation time: 1.407541275024414\n",
      "Step: 1524, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.5882062911987305\n",
      "Step: 1525, Loss: 0.9364243149757385, Accuracy: 0.96875, Computation time: 1.6427769660949707\n",
      "Step: 1526, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.175400733947754\n",
      "Step: 1527, Loss: 0.936195433139801, Accuracy: 0.96875, Computation time: 1.3365767002105713\n",
      "Step: 1528, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.3615601062774658\n",
      "Step: 1529, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.2666480541229248\n",
      "########################\n",
      "Test loss: 1.0819623470306396, Test Accuracy_epoch11: 0.7555771470069885\n",
      "########################\n",
      "Step: 1530, Loss: 0.9361786842346191, Accuracy: 0.96875, Computation time: 1.5999095439910889\n",
      "Step: 1531, Loss: 0.9375672936439514, Accuracy: 0.96875, Computation time: 1.6597230434417725\n",
      "Step: 1532, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.5802443027496338\n",
      "Step: 1533, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.5002150535583496\n",
      "Step: 1534, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.1683635711669922\n",
      "Step: 1535, Loss: 0.9196392893791199, Accuracy: 1.0, Computation time: 1.5124077796936035\n",
      "Step: 1536, Loss: 0.9159734845161438, Accuracy: 1.0, Computation time: 1.8446323871612549\n",
      "Step: 1537, Loss: 0.9159905910491943, Accuracy: 1.0, Computation time: 1.5155830383300781\n",
      "Step: 1538, Loss: 0.9160708785057068, Accuracy: 1.0, Computation time: 1.4787468910217285\n",
      "Step: 1539, Loss: 0.9160493016242981, Accuracy: 1.0, Computation time: 1.5343286991119385\n",
      "Step: 1540, Loss: 0.9162842631340027, Accuracy: 1.0, Computation time: 1.388570785522461\n",
      "Step: 1541, Loss: 0.9159908890724182, Accuracy: 1.0, Computation time: 1.3920421600341797\n",
      "Step: 1542, Loss: 0.9303794503211975, Accuracy: 0.96875, Computation time: 2.0455338954925537\n",
      "Step: 1543, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.416243076324463\n",
      "Step: 1544, Loss: 0.9159862399101257, Accuracy: 1.0, Computation time: 1.318084955215454\n",
      "Step: 1545, Loss: 0.9160898923873901, Accuracy: 1.0, Computation time: 1.5432679653167725\n",
      "Step: 1546, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 1.6644203662872314\n",
      "Step: 1547, Loss: 0.9160041213035583, Accuracy: 1.0, Computation time: 1.322052001953125\n",
      "Step: 1548, Loss: 0.9363052845001221, Accuracy: 0.96875, Computation time: 1.6011641025543213\n",
      "Step: 1549, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 1.5384089946746826\n",
      "Step: 1550, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 2.1963090896606445\n",
      "Step: 1551, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 1.3693037033081055\n",
      "Step: 1552, Loss: 0.9371557831764221, Accuracy: 0.96875, Computation time: 1.4984972476959229\n",
      "Step: 1553, Loss: 0.9163921475410461, Accuracy: 1.0, Computation time: 1.307929277420044\n",
      "Step: 1554, Loss: 0.9376497268676758, Accuracy: 0.96875, Computation time: 1.5959479808807373\n",
      "Step: 1555, Loss: 0.9371528029441833, Accuracy: 0.96875, Computation time: 1.2884140014648438\n",
      "Step: 1556, Loss: 0.9318104386329651, Accuracy: 0.96875, Computation time: 1.761054277420044\n",
      "Step: 1557, Loss: 0.9374570846557617, Accuracy: 0.96875, Computation time: 1.7519207000732422\n",
      "Step: 1558, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 1.7245993614196777\n",
      "Step: 1559, Loss: 0.916023313999176, Accuracy: 1.0, Computation time: 1.2996792793273926\n",
      "Step: 1560, Loss: 0.9228691458702087, Accuracy: 1.0, Computation time: 1.8731939792633057\n",
      "Step: 1561, Loss: 0.9344518780708313, Accuracy: 0.96875, Computation time: 2.423323631286621\n",
      "Step: 1562, Loss: 0.9165478348731995, Accuracy: 1.0, Computation time: 1.7793471813201904\n",
      "Step: 1563, Loss: 0.9161373376846313, Accuracy: 1.0, Computation time: 1.5029630661010742\n",
      "Step: 1564, Loss: 0.9160783886909485, Accuracy: 1.0, Computation time: 1.534296989440918\n",
      "Step: 1565, Loss: 0.9160099625587463, Accuracy: 1.0, Computation time: 1.6450133323669434\n",
      "Step: 1566, Loss: 0.9160128831863403, Accuracy: 1.0, Computation time: 1.632155179977417\n",
      "Step: 1567, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.743053674697876\n",
      "Step: 1568, Loss: 0.9166941046714783, Accuracy: 1.0, Computation time: 2.148078203201294\n",
      "Step: 1569, Loss: 0.9372904300689697, Accuracy: 0.96875, Computation time: 1.9119987487792969\n",
      "Step: 1570, Loss: 0.9159634709358215, Accuracy: 1.0, Computation time: 1.725888729095459\n",
      "Step: 1571, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 1.791780948638916\n",
      "Step: 1572, Loss: 0.9367365837097168, Accuracy: 0.96875, Computation time: 2.1325385570526123\n",
      "Step: 1573, Loss: 0.9159439206123352, Accuracy: 1.0, Computation time: 1.469198226928711\n",
      "Step: 1574, Loss: 0.9377138614654541, Accuracy: 0.96875, Computation time: 1.7532787322998047\n",
      "Step: 1575, Loss: 0.916745662689209, Accuracy: 1.0, Computation time: 2.166738510131836\n",
      "Step: 1576, Loss: 0.9359975457191467, Accuracy: 0.96875, Computation time: 1.7893905639648438\n",
      "Step: 1577, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.8120336532592773\n",
      "Step: 1578, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 1.5377228260040283\n",
      "Step: 1579, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.5431311130523682\n",
      "Step: 1580, Loss: 0.9160236120223999, Accuracy: 1.0, Computation time: 1.6005473136901855\n",
      "Step: 1581, Loss: 0.916103720664978, Accuracy: 1.0, Computation time: 1.7076447010040283\n",
      "Step: 1582, Loss: 0.9160027503967285, Accuracy: 1.0, Computation time: 1.7915866374969482\n",
      "Step: 1583, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.5627329349517822\n",
      "Step: 1584, Loss: 0.9160745143890381, Accuracy: 1.0, Computation time: 1.7841520309448242\n",
      "Step: 1585, Loss: 0.9159643054008484, Accuracy: 1.0, Computation time: 1.5737385749816895\n",
      "Step: 1586, Loss: 0.9489550590515137, Accuracy: 0.9375, Computation time: 1.9447522163391113\n",
      "Step: 1587, Loss: 0.9370095729827881, Accuracy: 0.96875, Computation time: 1.751755714416504\n",
      "Step: 1588, Loss: 0.9162328839302063, Accuracy: 1.0, Computation time: 2.122288942337036\n",
      "Step: 1589, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.7385525703430176\n",
      "Step: 1590, Loss: 0.919394314289093, Accuracy: 1.0, Computation time: 1.7981784343719482\n",
      "Step: 1591, Loss: 0.9193829894065857, Accuracy: 1.0, Computation time: 1.9916222095489502\n",
      "Step: 1592, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 2.763688802719116\n",
      "Step: 1593, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 2.1358087062835693\n",
      "Step: 1594, Loss: 0.9165281653404236, Accuracy: 1.0, Computation time: 1.9460458755493164\n",
      "Step: 1595, Loss: 0.9243168830871582, Accuracy: 1.0, Computation time: 1.7971489429473877\n",
      "Step: 1596, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 1.51149320602417\n",
      "Step: 1597, Loss: 0.9159385561943054, Accuracy: 1.0, Computation time: 1.9213237762451172\n",
      "Step: 1598, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.5523335933685303\n",
      "Step: 1599, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 1.9141480922698975\n",
      "Step: 1600, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.549607276916504\n",
      "Step: 1601, Loss: 0.9160060882568359, Accuracy: 1.0, Computation time: 2.943215847015381\n",
      "Step: 1602, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.8748517036437988\n",
      "Step: 1603, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.7297651767730713\n",
      "Step: 1604, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 2.0070955753326416\n",
      "Step: 1605, Loss: 0.9160044193267822, Accuracy: 1.0, Computation time: 1.561967134475708\n",
      "Step: 1606, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.8350930213928223\n",
      "Step: 1607, Loss: 0.9369321465492249, Accuracy: 0.96875, Computation time: 2.5100910663604736\n",
      "Step: 1608, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 1.7631964683532715\n",
      "Step: 1609, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.488882064819336\n",
      "Step: 1610, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.7497680187225342\n",
      "Step: 1611, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.9768803119659424\n",
      "Step: 1612, Loss: 0.9359978437423706, Accuracy: 0.96875, Computation time: 1.9202122688293457\n",
      "Step: 1613, Loss: 0.9160891771316528, Accuracy: 1.0, Computation time: 1.8401482105255127\n",
      "Step: 1614, Loss: 0.9160347580909729, Accuracy: 1.0, Computation time: 1.6241466999053955\n",
      "Step: 1615, Loss: 0.9375119805335999, Accuracy: 0.96875, Computation time: 1.5473756790161133\n",
      "Step: 1616, Loss: 0.9357283711433411, Accuracy: 0.96875, Computation time: 1.6338374614715576\n",
      "Step: 1617, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.7709734439849854\n",
      "Step: 1618, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.6334936618804932\n",
      "Step: 1619, Loss: 0.9215688109397888, Accuracy: 1.0, Computation time: 1.8548483848571777\n",
      "Step: 1620, Loss: 0.9169058203697205, Accuracy: 1.0, Computation time: 2.09804105758667\n",
      "Step: 1621, Loss: 0.9164385199546814, Accuracy: 1.0, Computation time: 2.1301944255828857\n",
      "Step: 1622, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.7003850936889648\n",
      "Step: 1623, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.4017584323883057\n",
      "Step: 1624, Loss: 0.9376221299171448, Accuracy: 0.96875, Computation time: 1.9670124053955078\n",
      "Step: 1625, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.690587043762207\n",
      "Step: 1626, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.841676950454712\n",
      "Step: 1627, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 1.7815585136413574\n",
      "Step: 1628, Loss: 0.9175875782966614, Accuracy: 1.0, Computation time: 1.7857182025909424\n",
      "Step: 1629, Loss: 0.9375457167625427, Accuracy: 0.96875, Computation time: 1.6663181781768799\n",
      "Step: 1630, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 1.8401691913604736\n",
      "Step: 1631, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.7058570384979248\n",
      "Step: 1632, Loss: 0.9160674810409546, Accuracy: 1.0, Computation time: 1.5638201236724854\n",
      "Step: 1633, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 1.8137922286987305\n",
      "Step: 1634, Loss: 0.9340538382530212, Accuracy: 0.96875, Computation time: 1.945690393447876\n",
      "Step: 1635, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 1.7831969261169434\n",
      "Step: 1636, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 1.594282865524292\n",
      "Step: 1637, Loss: 0.9284006953239441, Accuracy: 0.96875, Computation time: 1.905055284500122\n",
      "Step: 1638, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.8452839851379395\n",
      "Step: 1639, Loss: 0.9197303652763367, Accuracy: 1.0, Computation time: 1.9049558639526367\n",
      "Step: 1640, Loss: 0.937541127204895, Accuracy: 0.96875, Computation time: 1.6889848709106445\n",
      "Step: 1641, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 1.6995623111724854\n",
      "Step: 1642, Loss: 0.9159695506095886, Accuracy: 1.0, Computation time: 1.6324241161346436\n",
      "Step: 1643, Loss: 0.9160088300704956, Accuracy: 1.0, Computation time: 1.7426300048828125\n",
      "Step: 1644, Loss: 0.9159119129180908, Accuracy: 1.0, Computation time: 1.9332046508789062\n",
      "Step: 1645, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.6196861267089844\n",
      "Step: 1646, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.42572021484375\n",
      "Step: 1647, Loss: 0.9375391602516174, Accuracy: 0.96875, Computation time: 1.9754836559295654\n",
      "Step: 1648, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.5764520168304443\n",
      "Step: 1649, Loss: 0.9180325865745544, Accuracy: 1.0, Computation time: 1.728745937347412\n",
      "Step: 1650, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.9003410339355469\n",
      "Step: 1651, Loss: 0.93746417760849, Accuracy: 0.96875, Computation time: 1.4864356517791748\n",
      "Step: 1652, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.4468119144439697\n",
      "Step: 1653, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.8079140186309814\n",
      "Step: 1654, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.6840929985046387\n",
      "Step: 1655, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.171865701675415\n",
      "Step: 1656, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.5659217834472656\n",
      "Step: 1657, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.5883443355560303\n",
      "Step: 1658, Loss: 0.9308481812477112, Accuracy: 0.96875, Computation time: 2.226666212081909\n",
      "Step: 1659, Loss: 0.9339675903320312, Accuracy: 0.96875, Computation time: 1.2515571117401123\n",
      "Step: 1660, Loss: 0.9171335101127625, Accuracy: 1.0, Computation time: 1.781334638595581\n",
      "Step: 1661, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.715876817703247\n",
      "Step: 1662, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 1.5509953498840332\n",
      "Step: 1663, Loss: 0.9162202477455139, Accuracy: 1.0, Computation time: 1.8505749702453613\n",
      "Step: 1664, Loss: 0.9159693121910095, Accuracy: 1.0, Computation time: 1.6163239479064941\n",
      "Step: 1665, Loss: 0.9208114147186279, Accuracy: 1.0, Computation time: 1.6786267757415771\n",
      "Step: 1666, Loss: 0.9170355200767517, Accuracy: 1.0, Computation time: 1.9214794635772705\n",
      "Step: 1667, Loss: 0.9161200523376465, Accuracy: 1.0, Computation time: 1.7829372882843018\n",
      "Step: 1668, Loss: 0.915955662727356, Accuracy: 1.0, Computation time: 1.635115623474121\n",
      "########################\n",
      "Test loss: 1.0709072351455688, Test Accuracy_epoch12: 0.7740058302879333\n",
      "########################\n",
      "Step: 1669, Loss: 0.9159699082374573, Accuracy: 1.0, Computation time: 1.5840399265289307\n",
      "Step: 1670, Loss: 0.9159274101257324, Accuracy: 1.0, Computation time: 2.025207281112671\n",
      "Step: 1671, Loss: 0.9160242080688477, Accuracy: 1.0, Computation time: 1.839775800704956\n",
      "Step: 1672, Loss: 0.923730194568634, Accuracy: 1.0, Computation time: 2.1854052543640137\n",
      "Step: 1673, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 2.059617757797241\n",
      "Step: 1674, Loss: 0.9160270094871521, Accuracy: 1.0, Computation time: 2.552910804748535\n",
      "Step: 1675, Loss: 0.9161400198936462, Accuracy: 1.0, Computation time: 1.8585443496704102\n",
      "Step: 1676, Loss: 0.9161816239356995, Accuracy: 1.0, Computation time: 2.0255866050720215\n",
      "Step: 1677, Loss: 0.9161533117294312, Accuracy: 1.0, Computation time: 1.411775827407837\n",
      "Step: 1678, Loss: 0.9160568118095398, Accuracy: 1.0, Computation time: 1.6832401752471924\n",
      "Step: 1679, Loss: 0.9165292978286743, Accuracy: 1.0, Computation time: 1.6668663024902344\n",
      "Step: 1680, Loss: 0.9164283871650696, Accuracy: 1.0, Computation time: 1.7953639030456543\n",
      "Step: 1681, Loss: 0.9159336686134338, Accuracy: 1.0, Computation time: 2.300981283187866\n",
      "Step: 1682, Loss: 0.9160109162330627, Accuracy: 1.0, Computation time: 1.575547218322754\n",
      "Step: 1683, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 1.893003225326538\n",
      "Step: 1684, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.6880428791046143\n",
      "Step: 1685, Loss: 0.9202059507369995, Accuracy: 1.0, Computation time: 2.415017604827881\n",
      "Step: 1686, Loss: 0.916012167930603, Accuracy: 1.0, Computation time: 1.7633416652679443\n",
      "Step: 1687, Loss: 0.9379316568374634, Accuracy: 0.96875, Computation time: 1.7753007411956787\n",
      "Step: 1688, Loss: 0.9238594770431519, Accuracy: 1.0, Computation time: 1.9198083877563477\n",
      "Step: 1689, Loss: 0.9170965552330017, Accuracy: 1.0, Computation time: 1.598059892654419\n",
      "Step: 1690, Loss: 0.9163309931755066, Accuracy: 1.0, Computation time: 1.6855874061584473\n",
      "Step: 1691, Loss: 0.916551947593689, Accuracy: 1.0, Computation time: 1.7629587650299072\n",
      "Step: 1692, Loss: 0.9163867235183716, Accuracy: 1.0, Computation time: 1.5496070384979248\n",
      "Step: 1693, Loss: 0.9163150787353516, Accuracy: 1.0, Computation time: 1.3427352905273438\n",
      "Step: 1694, Loss: 0.9160993099212646, Accuracy: 1.0, Computation time: 1.604656457901001\n",
      "Step: 1695, Loss: 0.9375280737876892, Accuracy: 0.96875, Computation time: 1.3677740097045898\n",
      "Step: 1696, Loss: 0.9159708023071289, Accuracy: 1.0, Computation time: 1.3458986282348633\n",
      "Step: 1697, Loss: 0.9374216198921204, Accuracy: 0.96875, Computation time: 1.4289865493774414\n",
      "Step: 1698, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.5969676971435547\n",
      "Step: 1699, Loss: 0.9343991279602051, Accuracy: 0.96875, Computation time: 1.790081262588501\n",
      "Step: 1700, Loss: 0.9159781336784363, Accuracy: 1.0, Computation time: 1.4707157611846924\n",
      "Step: 1701, Loss: 0.9291911721229553, Accuracy: 0.96875, Computation time: 1.6832177639007568\n",
      "Step: 1702, Loss: 0.9160389304161072, Accuracy: 1.0, Computation time: 1.4590332508087158\n",
      "Step: 1703, Loss: 0.9160752892494202, Accuracy: 1.0, Computation time: 1.3596525192260742\n",
      "Step: 1704, Loss: 0.9161103963851929, Accuracy: 1.0, Computation time: 1.2311122417449951\n",
      "Step: 1705, Loss: 0.9160529375076294, Accuracy: 1.0, Computation time: 1.4053876399993896\n",
      "Step: 1706, Loss: 0.9160807132720947, Accuracy: 1.0, Computation time: 1.2490861415863037\n",
      "Step: 1707, Loss: 0.9162901043891907, Accuracy: 1.0, Computation time: 1.579406976699829\n",
      "Step: 1708, Loss: 0.9186607003211975, Accuracy: 1.0, Computation time: 1.510193109512329\n",
      "Step: 1709, Loss: 0.9163558483123779, Accuracy: 1.0, Computation time: 1.3756482601165771\n",
      "Step: 1710, Loss: 0.9160638451576233, Accuracy: 1.0, Computation time: 1.8045856952667236\n",
      "Step: 1711, Loss: 0.9377572536468506, Accuracy: 0.96875, Computation time: 1.4879658222198486\n",
      "Step: 1712, Loss: 0.932452917098999, Accuracy: 0.96875, Computation time: 1.6451537609100342\n",
      "Step: 1713, Loss: 0.9359395503997803, Accuracy: 0.96875, Computation time: 1.2332634925842285\n",
      "Step: 1714, Loss: 0.925217866897583, Accuracy: 1.0, Computation time: 1.5675873756408691\n",
      "Step: 1715, Loss: 0.9159982204437256, Accuracy: 1.0, Computation time: 1.7280383110046387\n",
      "Step: 1716, Loss: 0.9162572622299194, Accuracy: 1.0, Computation time: 1.4002971649169922\n",
      "Step: 1717, Loss: 0.9160783290863037, Accuracy: 1.0, Computation time: 1.4619112014770508\n",
      "Step: 1718, Loss: 0.9235039949417114, Accuracy: 1.0, Computation time: 2.176414966583252\n",
      "Step: 1719, Loss: 0.9160676598548889, Accuracy: 1.0, Computation time: 1.3077266216278076\n",
      "Step: 1720, Loss: 0.915986955165863, Accuracy: 1.0, Computation time: 1.3406224250793457\n",
      "Step: 1721, Loss: 0.9159727692604065, Accuracy: 1.0, Computation time: 1.402082920074463\n",
      "Step: 1722, Loss: 0.9593889713287354, Accuracy: 0.9375, Computation time: 1.790804386138916\n",
      "Step: 1723, Loss: 0.9377681016921997, Accuracy: 0.96875, Computation time: 1.528578519821167\n",
      "Step: 1724, Loss: 0.9160740375518799, Accuracy: 1.0, Computation time: 1.3570024967193604\n",
      "Step: 1725, Loss: 0.91608065366745, Accuracy: 1.0, Computation time: 1.5108520984649658\n",
      "Step: 1726, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 1.5985281467437744\n",
      "Step: 1727, Loss: 0.917023777961731, Accuracy: 1.0, Computation time: 1.744145154953003\n",
      "Step: 1728, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 1.6832027435302734\n",
      "Step: 1729, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 1.2895519733428955\n",
      "Step: 1730, Loss: 0.9159737229347229, Accuracy: 1.0, Computation time: 1.3492424488067627\n",
      "Step: 1731, Loss: 0.9373832941055298, Accuracy: 0.96875, Computation time: 2.0958454608917236\n",
      "Step: 1732, Loss: 0.916196346282959, Accuracy: 1.0, Computation time: 1.182204008102417\n",
      "Step: 1733, Loss: 0.9160261750221252, Accuracy: 1.0, Computation time: 1.6883916854858398\n",
      "Step: 1734, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.7803823947906494\n",
      "Step: 1735, Loss: 0.9160637259483337, Accuracy: 1.0, Computation time: 1.6725428104400635\n",
      "Step: 1736, Loss: 0.9160457253456116, Accuracy: 1.0, Computation time: 1.3533823490142822\n",
      "Step: 1737, Loss: 0.91591477394104, Accuracy: 1.0, Computation time: 1.6299757957458496\n",
      "Step: 1738, Loss: 0.9160078763961792, Accuracy: 1.0, Computation time: 1.3368430137634277\n",
      "Step: 1739, Loss: 0.9322084784507751, Accuracy: 0.96875, Computation time: 1.46431303024292\n",
      "Step: 1740, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.5626201629638672\n",
      "Step: 1741, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.4547755718231201\n",
      "Step: 1742, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.4589331150054932\n",
      "Step: 1743, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 1.5275602340698242\n",
      "Step: 1744, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.292593240737915\n",
      "Step: 1745, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.3747773170471191\n",
      "Step: 1746, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.1566400527954102\n",
      "Step: 1747, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.2227280139923096\n",
      "Step: 1748, Loss: 0.9375545382499695, Accuracy: 0.96875, Computation time: 1.526106595993042\n",
      "Step: 1749, Loss: 0.9159864187240601, Accuracy: 1.0, Computation time: 1.608794927597046\n",
      "Step: 1750, Loss: 0.9590099453926086, Accuracy: 0.9375, Computation time: 1.4950988292694092\n",
      "Step: 1751, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.1748747825622559\n",
      "Step: 1752, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.1853742599487305\n",
      "Step: 1753, Loss: 0.9164940714836121, Accuracy: 1.0, Computation time: 2.078871011734009\n",
      "Step: 1754, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 1.4989948272705078\n",
      "Step: 1755, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.3215091228485107\n",
      "Step: 1756, Loss: 0.9164140820503235, Accuracy: 1.0, Computation time: 1.6617045402526855\n",
      "Step: 1757, Loss: 0.9200149774551392, Accuracy: 1.0, Computation time: 1.9325928688049316\n",
      "Step: 1758, Loss: 0.9159186482429504, Accuracy: 1.0, Computation time: 1.494964599609375\n",
      "Step: 1759, Loss: 0.9242030382156372, Accuracy: 1.0, Computation time: 2.1596522331237793\n",
      "Step: 1760, Loss: 0.9161790013313293, Accuracy: 1.0, Computation time: 1.4579696655273438\n",
      "Step: 1761, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.379518985748291\n",
      "Step: 1762, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.278092384338379\n",
      "Step: 1763, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.6812565326690674\n",
      "Step: 1764, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 2.0817267894744873\n",
      "Step: 1765, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.2677648067474365\n",
      "Step: 1766, Loss: 0.9379761219024658, Accuracy: 0.96875, Computation time: 1.8173999786376953\n",
      "Step: 1767, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.1928529739379883\n",
      "Step: 1768, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 1.326003074645996\n",
      "Step: 1769, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.660975456237793\n",
      "Step: 1770, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.800234317779541\n",
      "Step: 1771, Loss: 0.9375954270362854, Accuracy: 0.96875, Computation time: 1.949946641921997\n",
      "Step: 1772, Loss: 0.9365310668945312, Accuracy: 0.96875, Computation time: 1.6784698963165283\n",
      "Step: 1773, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.245147466659546\n",
      "Step: 1774, Loss: 0.9344931244850159, Accuracy: 0.96875, Computation time: 1.95328688621521\n",
      "Step: 1775, Loss: 0.916043221950531, Accuracy: 1.0, Computation time: 1.9111957550048828\n",
      "Step: 1776, Loss: 0.9159448742866516, Accuracy: 1.0, Computation time: 1.8018038272857666\n",
      "Step: 1777, Loss: 0.9160020351409912, Accuracy: 1.0, Computation time: 1.5657155513763428\n",
      "Step: 1778, Loss: 0.9172608256340027, Accuracy: 1.0, Computation time: 1.8729677200317383\n",
      "Step: 1779, Loss: 0.9362037777900696, Accuracy: 0.96875, Computation time: 1.684398889541626\n",
      "Step: 1780, Loss: 0.9159486889839172, Accuracy: 1.0, Computation time: 1.602065086364746\n",
      "Step: 1781, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.7402985095977783\n",
      "Step: 1782, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.358313798904419\n",
      "Step: 1783, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 1.637632131576538\n",
      "Step: 1784, Loss: 0.915922999382019, Accuracy: 1.0, Computation time: 1.244016408920288\n",
      "Step: 1785, Loss: 0.9163130521774292, Accuracy: 1.0, Computation time: 1.8930318355560303\n",
      "Step: 1786, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.8100478649139404\n",
      "Step: 1787, Loss: 0.9176368117332458, Accuracy: 1.0, Computation time: 1.6377599239349365\n",
      "Step: 1788, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.4992308616638184\n",
      "Step: 1789, Loss: 0.9170247316360474, Accuracy: 1.0, Computation time: 1.6571049690246582\n",
      "Step: 1790, Loss: 0.9168186783790588, Accuracy: 1.0, Computation time: 1.7828216552734375\n",
      "Step: 1791, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.7004897594451904\n",
      "Step: 1792, Loss: 0.9162027835845947, Accuracy: 1.0, Computation time: 1.7495269775390625\n",
      "Step: 1793, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.404505729675293\n",
      "Step: 1794, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 1.8001937866210938\n",
      "Step: 1795, Loss: 0.9159368276596069, Accuracy: 1.0, Computation time: 1.4740591049194336\n",
      "Step: 1796, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.9855327606201172\n",
      "Step: 1797, Loss: 0.9164415597915649, Accuracy: 1.0, Computation time: 1.874418020248413\n",
      "Step: 1798, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.4424026012420654\n",
      "Step: 1799, Loss: 0.916106104850769, Accuracy: 1.0, Computation time: 2.3830928802490234\n",
      "Step: 1800, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.5791728496551514\n",
      "Step: 1801, Loss: 0.9163112640380859, Accuracy: 1.0, Computation time: 1.5948541164398193\n",
      "Step: 1802, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.2772037982940674\n",
      "Step: 1803, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 1.5462663173675537\n",
      "Step: 1804, Loss: 0.9159488677978516, Accuracy: 1.0, Computation time: 1.6091265678405762\n",
      "Step: 1805, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.5521140098571777\n",
      "Step: 1806, Loss: 0.9329450130462646, Accuracy: 0.96875, Computation time: 2.2883639335632324\n",
      "Step: 1807, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 1.2155146598815918\n",
      "########################\n",
      "Test loss: 1.07197904586792, Test Accuracy_epoch13: 0.7730358839035034\n",
      "########################\n",
      "Step: 1808, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 2.0947866439819336\n",
      "Step: 1809, Loss: 0.9374998807907104, Accuracy: 0.96875, Computation time: 1.5998094081878662\n",
      "Step: 1810, Loss: 0.9159442782402039, Accuracy: 1.0, Computation time: 1.517270803451538\n",
      "Step: 1811, Loss: 0.9195207357406616, Accuracy: 1.0, Computation time: 1.5559313297271729\n",
      "Step: 1812, Loss: 0.9278446435928345, Accuracy: 0.96875, Computation time: 1.6683709621429443\n",
      "Step: 1813, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 1.8439979553222656\n",
      "Step: 1814, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.5242292881011963\n",
      "Step: 1815, Loss: 0.9164482951164246, Accuracy: 1.0, Computation time: 2.338315010070801\n",
      "Step: 1816, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 1.976440668106079\n",
      "Step: 1817, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.384434461593628\n",
      "Step: 1818, Loss: 0.9739858508110046, Accuracy: 0.90625, Computation time: 2.513984441757202\n",
      "Step: 1819, Loss: 0.9159802198410034, Accuracy: 1.0, Computation time: 1.6945886611938477\n",
      "Step: 1820, Loss: 0.916083037853241, Accuracy: 1.0, Computation time: 1.9342069625854492\n",
      "Step: 1821, Loss: 0.9322583079338074, Accuracy: 0.96875, Computation time: 1.8348116874694824\n",
      "Step: 1822, Loss: 0.9161303043365479, Accuracy: 1.0, Computation time: 1.5867338180541992\n",
      "Step: 1823, Loss: 0.9162622690200806, Accuracy: 1.0, Computation time: 1.960165023803711\n",
      "Step: 1824, Loss: 0.9379552602767944, Accuracy: 0.96875, Computation time: 1.5806331634521484\n",
      "Step: 1825, Loss: 0.9161402583122253, Accuracy: 1.0, Computation time: 2.2036023139953613\n",
      "Step: 1826, Loss: 0.9159584641456604, Accuracy: 1.0, Computation time: 1.5197618007659912\n",
      "Step: 1827, Loss: 0.9365134835243225, Accuracy: 0.96875, Computation time: 1.8876590728759766\n",
      "Step: 1828, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.4985439777374268\n",
      "Step: 1829, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 1.7139909267425537\n",
      "Step: 1830, Loss: 0.9160224199295044, Accuracy: 1.0, Computation time: 2.0544376373291016\n",
      "Step: 1831, Loss: 0.9166584014892578, Accuracy: 1.0, Computation time: 1.8235185146331787\n",
      "Step: 1832, Loss: 0.9159504175186157, Accuracy: 1.0, Computation time: 1.683105230331421\n",
      "Step: 1833, Loss: 0.915946364402771, Accuracy: 1.0, Computation time: 1.9053900241851807\n",
      "Step: 1834, Loss: 0.9377847909927368, Accuracy: 0.96875, Computation time: 2.0301027297973633\n",
      "Step: 1835, Loss: 0.9159336686134338, Accuracy: 1.0, Computation time: 2.0836856365203857\n",
      "Step: 1836, Loss: 0.9159983992576599, Accuracy: 1.0, Computation time: 1.7595751285552979\n",
      "Step: 1837, Loss: 0.9159649610519409, Accuracy: 1.0, Computation time: 1.5499701499938965\n",
      "Step: 1838, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 1.3664698600769043\n",
      "Step: 1839, Loss: 0.9160459637641907, Accuracy: 1.0, Computation time: 1.7016167640686035\n",
      "Step: 1840, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.349442720413208\n",
      "Step: 1841, Loss: 0.9160325527191162, Accuracy: 1.0, Computation time: 1.6073620319366455\n",
      "Step: 1842, Loss: 0.9344627261161804, Accuracy: 0.96875, Computation time: 1.6636037826538086\n",
      "Step: 1843, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.6314561367034912\n",
      "Step: 1844, Loss: 0.932174563407898, Accuracy: 0.96875, Computation time: 2.1573266983032227\n",
      "Step: 1845, Loss: 0.9160403609275818, Accuracy: 1.0, Computation time: 1.5746681690216064\n",
      "Step: 1846, Loss: 0.9358049035072327, Accuracy: 0.96875, Computation time: 1.855194330215454\n",
      "Step: 1847, Loss: 0.9372474551200867, Accuracy: 0.96875, Computation time: 1.7024383544921875\n",
      "Step: 1848, Loss: 0.9376798868179321, Accuracy: 0.96875, Computation time: 1.8050951957702637\n",
      "Step: 1849, Loss: 0.916231632232666, Accuracy: 1.0, Computation time: 2.1864583492279053\n",
      "Step: 1850, Loss: 0.9206467270851135, Accuracy: 1.0, Computation time: 1.9796195030212402\n",
      "Step: 1851, Loss: 0.9160221219062805, Accuracy: 1.0, Computation time: 1.625056505203247\n",
      "Step: 1852, Loss: 0.9159544706344604, Accuracy: 1.0, Computation time: 1.5853817462921143\n",
      "Step: 1853, Loss: 0.9269871711730957, Accuracy: 0.96875, Computation time: 2.078033924102783\n",
      "Step: 1854, Loss: 0.9159607291221619, Accuracy: 1.0, Computation time: 1.5944101810455322\n",
      "Step: 1855, Loss: 0.9160747528076172, Accuracy: 1.0, Computation time: 1.805854082107544\n",
      "Step: 1856, Loss: 0.9373984932899475, Accuracy: 0.96875, Computation time: 1.6328659057617188\n",
      "Step: 1857, Loss: 0.9375860095024109, Accuracy: 0.96875, Computation time: 1.7871499061584473\n",
      "Step: 1858, Loss: 0.9169487357139587, Accuracy: 1.0, Computation time: 2.229393482208252\n",
      "Step: 1859, Loss: 0.9159920811653137, Accuracy: 1.0, Computation time: 1.8376169204711914\n",
      "Step: 1860, Loss: 0.9169883131980896, Accuracy: 1.0, Computation time: 2.2609143257141113\n",
      "Step: 1861, Loss: 0.9372138977050781, Accuracy: 0.96875, Computation time: 2.0516483783721924\n",
      "Step: 1862, Loss: 0.937630295753479, Accuracy: 0.96875, Computation time: 1.3402345180511475\n",
      "Step: 1863, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 1.6189515590667725\n",
      "Step: 1864, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.5155837535858154\n",
      "Step: 1865, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.4386472702026367\n",
      "Step: 1866, Loss: 0.9484316110610962, Accuracy: 0.9375, Computation time: 1.8365254402160645\n",
      "Step: 1867, Loss: 0.915978729724884, Accuracy: 1.0, Computation time: 1.6782450675964355\n",
      "Step: 1868, Loss: 0.9159780144691467, Accuracy: 1.0, Computation time: 1.6233818531036377\n",
      "Step: 1869, Loss: 0.9161442518234253, Accuracy: 1.0, Computation time: 1.483710527420044\n",
      "Step: 1870, Loss: 0.9159980416297913, Accuracy: 1.0, Computation time: 1.746661901473999\n",
      "Step: 1871, Loss: 0.9160014390945435, Accuracy: 1.0, Computation time: 1.532836675643921\n",
      "Step: 1872, Loss: 0.9160819053649902, Accuracy: 1.0, Computation time: 1.6023247241973877\n",
      "Step: 1873, Loss: 0.9159608483314514, Accuracy: 1.0, Computation time: 1.4509122371673584\n",
      "Step: 1874, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.4527561664581299\n",
      "Step: 1875, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.4846279621124268\n",
      "Step: 1876, Loss: 0.9375479221343994, Accuracy: 0.96875, Computation time: 1.3118443489074707\n",
      "Step: 1877, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.4969143867492676\n",
      "Step: 1878, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.6853053569793701\n",
      "Step: 1879, Loss: 0.916016697883606, Accuracy: 1.0, Computation time: 1.5030300617218018\n",
      "Step: 1880, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.554903268814087\n",
      "Step: 1881, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 1.6022768020629883\n",
      "Step: 1882, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.4888603687286377\n",
      "Step: 1883, Loss: 0.9296234846115112, Accuracy: 0.96875, Computation time: 1.4794726371765137\n",
      "Step: 1884, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.480262279510498\n",
      "Step: 1885, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.2806973457336426\n",
      "Step: 1886, Loss: 0.9339732527732849, Accuracy: 0.96875, Computation time: 1.7584130764007568\n",
      "Step: 1887, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.468440055847168\n",
      "Step: 1888, Loss: 0.9160323143005371, Accuracy: 1.0, Computation time: 1.637183666229248\n",
      "Step: 1889, Loss: 0.9375262260437012, Accuracy: 0.96875, Computation time: 1.700758695602417\n",
      "Step: 1890, Loss: 0.9159241914749146, Accuracy: 1.0, Computation time: 1.3492858409881592\n",
      "Step: 1891, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 1.8058748245239258\n",
      "Step: 1892, Loss: 0.9192156791687012, Accuracy: 1.0, Computation time: 1.8251380920410156\n",
      "Step: 1893, Loss: 0.9159146547317505, Accuracy: 1.0, Computation time: 1.572500467300415\n",
      "Step: 1894, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.4034616947174072\n",
      "Step: 1895, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.2831554412841797\n",
      "Step: 1896, Loss: 0.9159979224205017, Accuracy: 1.0, Computation time: 1.2480709552764893\n",
      "Step: 1897, Loss: 0.9160341620445251, Accuracy: 1.0, Computation time: 1.5459351539611816\n",
      "Step: 1898, Loss: 0.915991485118866, Accuracy: 1.0, Computation time: 1.3292582035064697\n",
      "Step: 1899, Loss: 0.927504301071167, Accuracy: 0.96875, Computation time: 1.6903634071350098\n",
      "Step: 1900, Loss: 0.9159364104270935, Accuracy: 1.0, Computation time: 1.2891521453857422\n",
      "Step: 1901, Loss: 0.9159307479858398, Accuracy: 1.0, Computation time: 1.309504508972168\n",
      "Step: 1902, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.4172184467315674\n",
      "Step: 1903, Loss: 0.9162468314170837, Accuracy: 1.0, Computation time: 1.5744130611419678\n",
      "Step: 1904, Loss: 0.9370512962341309, Accuracy: 0.96875, Computation time: 1.7874751091003418\n",
      "Step: 1905, Loss: 0.9249017238616943, Accuracy: 1.0, Computation time: 1.5332269668579102\n",
      "Step: 1906, Loss: 0.9160197973251343, Accuracy: 1.0, Computation time: 1.676621675491333\n",
      "Step: 1907, Loss: 0.9163976311683655, Accuracy: 1.0, Computation time: 1.5919644832611084\n",
      "Step: 1908, Loss: 0.9160105586051941, Accuracy: 1.0, Computation time: 1.6378655433654785\n",
      "Step: 1909, Loss: 0.9160161018371582, Accuracy: 1.0, Computation time: 2.0284981727600098\n",
      "Step: 1910, Loss: 0.9160253405570984, Accuracy: 1.0, Computation time: 1.7608885765075684\n",
      "Step: 1911, Loss: 0.9161076545715332, Accuracy: 1.0, Computation time: 2.2491109371185303\n",
      "Step: 1912, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 1.9814784526824951\n",
      "Step: 1913, Loss: 0.9159743189811707, Accuracy: 1.0, Computation time: 1.6244566440582275\n",
      "Step: 1914, Loss: 0.915966808795929, Accuracy: 1.0, Computation time: 1.957226276397705\n",
      "Step: 1915, Loss: 0.9159555435180664, Accuracy: 1.0, Computation time: 1.6313612461090088\n",
      "Step: 1916, Loss: 0.916621208190918, Accuracy: 1.0, Computation time: 1.552872657775879\n",
      "Step: 1917, Loss: 0.9409911036491394, Accuracy: 0.96875, Computation time: 1.822211742401123\n",
      "Step: 1918, Loss: 0.9178013801574707, Accuracy: 1.0, Computation time: 1.789440631866455\n",
      "Step: 1919, Loss: 0.9160730242729187, Accuracy: 1.0, Computation time: 1.4873406887054443\n",
      "Step: 1920, Loss: 0.916304349899292, Accuracy: 1.0, Computation time: 1.8751275539398193\n",
      "Step: 1921, Loss: 0.9160090684890747, Accuracy: 1.0, Computation time: 1.6876494884490967\n",
      "Step: 1922, Loss: 0.9376157522201538, Accuracy: 0.96875, Computation time: 1.4761149883270264\n",
      "Step: 1923, Loss: 0.9170719385147095, Accuracy: 1.0, Computation time: 1.7625339031219482\n",
      "Step: 1924, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 1.5269660949707031\n",
      "Step: 1925, Loss: 0.9175522327423096, Accuracy: 1.0, Computation time: 2.1919193267822266\n",
      "Step: 1926, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.720376968383789\n",
      "Step: 1927, Loss: 0.9378843903541565, Accuracy: 0.96875, Computation time: 1.6883540153503418\n",
      "Step: 1928, Loss: 0.9163714647293091, Accuracy: 1.0, Computation time: 1.609403371810913\n",
      "Step: 1929, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.6351969242095947\n",
      "Step: 1930, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.4524219036102295\n",
      "Step: 1931, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.2952227592468262\n",
      "Step: 1932, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.4939236640930176\n",
      "Step: 1933, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 1.2749359607696533\n",
      "Step: 1934, Loss: 0.9160706996917725, Accuracy: 1.0, Computation time: 1.461735725402832\n",
      "Step: 1935, Loss: 0.9368990063667297, Accuracy: 0.96875, Computation time: 1.4703423976898193\n",
      "Step: 1936, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 1.449758768081665\n",
      "Step: 1937, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 1.705486536026001\n",
      "Step: 1938, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.4876708984375\n",
      "Step: 1939, Loss: 0.9160780906677246, Accuracy: 1.0, Computation time: 1.8828318119049072\n",
      "Step: 1940, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 1.2673437595367432\n",
      "Step: 1941, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.468827724456787\n",
      "Step: 1942, Loss: 0.9185672998428345, Accuracy: 1.0, Computation time: 1.4925432205200195\n",
      "Step: 1943, Loss: 0.9159025549888611, Accuracy: 1.0, Computation time: 1.3355441093444824\n",
      "Step: 1944, Loss: 0.9160274267196655, Accuracy: 1.0, Computation time: 1.6738700866699219\n",
      "Step: 1945, Loss: 0.9159952402114868, Accuracy: 1.0, Computation time: 1.5720527172088623\n",
      "Step: 1946, Loss: 0.9200736880302429, Accuracy: 1.0, Computation time: 1.752901554107666\n",
      "########################\n",
      "Test loss: 1.0693122148513794, Test Accuracy_epoch14: 0.7778855562210083\n",
      "########################\n",
      "Step: 1947, Loss: 0.9159780740737915, Accuracy: 1.0, Computation time: 1.4605836868286133\n",
      "Step: 1948, Loss: 0.9159575700759888, Accuracy: 1.0, Computation time: 1.4040851593017578\n",
      "Step: 1949, Loss: 0.9160169363021851, Accuracy: 1.0, Computation time: 1.5339558124542236\n",
      "Step: 1950, Loss: 0.9159274101257324, Accuracy: 1.0, Computation time: 1.649256944656372\n",
      "Step: 1951, Loss: 0.9202266335487366, Accuracy: 1.0, Computation time: 1.6878607273101807\n",
      "Step: 1952, Loss: 0.9159331321716309, Accuracy: 1.0, Computation time: 1.3262183666229248\n",
      "Step: 1953, Loss: 0.9159504771232605, Accuracy: 1.0, Computation time: 1.9436659812927246\n",
      "Step: 1954, Loss: 0.915946364402771, Accuracy: 1.0, Computation time: 1.6599318981170654\n",
      "Step: 1955, Loss: 0.9233076572418213, Accuracy: 1.0, Computation time: 2.5140676498413086\n",
      "Step: 1956, Loss: 0.9395368695259094, Accuracy: 0.96875, Computation time: 1.7421503067016602\n",
      "Step: 1957, Loss: 0.9160255789756775, Accuracy: 1.0, Computation time: 1.8027846813201904\n",
      "Step: 1958, Loss: 0.9165665507316589, Accuracy: 1.0, Computation time: 1.523876667022705\n",
      "Step: 1959, Loss: 0.9159807562828064, Accuracy: 1.0, Computation time: 1.9549834728240967\n",
      "Step: 1960, Loss: 0.9159476161003113, Accuracy: 1.0, Computation time: 2.1248137950897217\n",
      "Step: 1961, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 2.1913719177246094\n",
      "Step: 1962, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 1.8209939002990723\n",
      "Step: 1963, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.8524024486541748\n",
      "Step: 1964, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.5731630325317383\n",
      "Step: 1965, Loss: 0.9368702173233032, Accuracy: 0.96875, Computation time: 1.7906532287597656\n",
      "Step: 1966, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 1.6769402027130127\n",
      "Step: 1967, Loss: 0.9172864556312561, Accuracy: 1.0, Computation time: 2.1358399391174316\n",
      "Step: 1968, Loss: 0.9161425232887268, Accuracy: 1.0, Computation time: 1.6659190654754639\n",
      "Step: 1969, Loss: 0.9166321158409119, Accuracy: 1.0, Computation time: 2.5083770751953125\n",
      "Step: 1970, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.9293477535247803\n",
      "Step: 1971, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.7612037658691406\n",
      "Step: 1972, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 1.9648523330688477\n",
      "Step: 1973, Loss: 0.9159197807312012, Accuracy: 1.0, Computation time: 1.7465918064117432\n",
      "Step: 1974, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 1.4531607627868652\n",
      "Step: 1975, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 1.649637222290039\n",
      "Step: 1976, Loss: 0.9369628429412842, Accuracy: 0.96875, Computation time: 1.9981622695922852\n",
      "Step: 1977, Loss: 0.9360513687133789, Accuracy: 0.96875, Computation time: 1.6674268245697021\n",
      "Step: 1978, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 1.6306700706481934\n",
      "Step: 1979, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.7344937324523926\n",
      "Step: 1980, Loss: 0.9163182377815247, Accuracy: 1.0, Computation time: 1.614926815032959\n",
      "Step: 1981, Loss: 0.9195954203605652, Accuracy: 1.0, Computation time: 1.4828848838806152\n",
      "Step: 1982, Loss: 0.9160292148590088, Accuracy: 1.0, Computation time: 1.6304774284362793\n",
      "Step: 1983, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.8460564613342285\n",
      "Step: 1984, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.424835205078125\n",
      "Step: 1985, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 1.426255702972412\n",
      "Step: 1986, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.5435142517089844\n",
      "Step: 1987, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.5175466537475586\n",
      "Step: 1988, Loss: 0.9159613251686096, Accuracy: 1.0, Computation time: 1.5034205913543701\n",
      "Step: 1989, Loss: 0.916071355342865, Accuracy: 1.0, Computation time: 1.8141944408416748\n",
      "Step: 1990, Loss: 0.9417194128036499, Accuracy: 0.96875, Computation time: 1.4334406852722168\n",
      "Step: 1991, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.4853391647338867\n",
      "Step: 1992, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.6547961235046387\n",
      "Step: 1993, Loss: 0.9162203073501587, Accuracy: 1.0, Computation time: 1.9748029708862305\n",
      "Step: 1994, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 1.5309436321258545\n",
      "Step: 1995, Loss: 0.915930449962616, Accuracy: 1.0, Computation time: 1.3712990283966064\n",
      "Step: 1996, Loss: 0.9160993099212646, Accuracy: 1.0, Computation time: 1.5318200588226318\n",
      "Step: 1997, Loss: 0.9159641861915588, Accuracy: 1.0, Computation time: 1.5750646591186523\n",
      "Step: 1998, Loss: 0.9159415364265442, Accuracy: 1.0, Computation time: 1.523681402206421\n",
      "Step: 1999, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.4083354473114014\n",
      "Step: 2000, Loss: 0.9192829132080078, Accuracy: 1.0, Computation time: 1.6488213539123535\n",
      "Step: 2001, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 1.5406701564788818\n",
      "Step: 2002, Loss: 0.9374169111251831, Accuracy: 0.96875, Computation time: 1.4277727603912354\n",
      "Step: 2003, Loss: 0.9375607967376709, Accuracy: 0.96875, Computation time: 1.334378719329834\n",
      "Step: 2004, Loss: 0.9159483909606934, Accuracy: 1.0, Computation time: 1.5060224533081055\n",
      "Step: 2005, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 1.3943796157836914\n",
      "Step: 2006, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.5522284507751465\n",
      "Step: 2007, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.6004745960235596\n",
      "Step: 2008, Loss: 0.9159571528434753, Accuracy: 1.0, Computation time: 1.540837049484253\n",
      "Step: 2009, Loss: 0.9159650206565857, Accuracy: 1.0, Computation time: 1.8577229976654053\n",
      "Step: 2010, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.5913779735565186\n",
      "Step: 2011, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.6130590438842773\n",
      "Step: 2012, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.6793856620788574\n",
      "Step: 2013, Loss: 0.9160506129264832, Accuracy: 1.0, Computation time: 1.5825045108795166\n",
      "Step: 2014, Loss: 0.9172433018684387, Accuracy: 1.0, Computation time: 1.6805880069732666\n",
      "Step: 2015, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.756701946258545\n",
      "Step: 2016, Loss: 0.9462354779243469, Accuracy: 0.96875, Computation time: 1.937462329864502\n",
      "Step: 2017, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.7138490676879883\n",
      "Step: 2018, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 1.2986302375793457\n",
      "Step: 2019, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.5070006847381592\n",
      "Step: 2020, Loss: 0.9159885048866272, Accuracy: 1.0, Computation time: 1.3197991847991943\n",
      "Step: 2021, Loss: 0.936320960521698, Accuracy: 0.96875, Computation time: 1.5823092460632324\n",
      "Step: 2022, Loss: 0.9159944653511047, Accuracy: 1.0, Computation time: 1.338994026184082\n",
      "Step: 2023, Loss: 0.9301236867904663, Accuracy: 0.96875, Computation time: 1.758302927017212\n",
      "Step: 2024, Loss: 0.9377216696739197, Accuracy: 0.96875, Computation time: 1.4708807468414307\n",
      "Step: 2025, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.2293930053710938\n",
      "Step: 2026, Loss: 0.9162551760673523, Accuracy: 1.0, Computation time: 1.3410279750823975\n",
      "Step: 2027, Loss: 0.9160826206207275, Accuracy: 1.0, Computation time: 1.6426584720611572\n",
      "Step: 2028, Loss: 0.9372813701629639, Accuracy: 0.96875, Computation time: 1.9834632873535156\n",
      "Step: 2029, Loss: 0.9196524620056152, Accuracy: 1.0, Computation time: 1.8478317260742188\n",
      "Step: 2030, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 1.456305980682373\n",
      "Step: 2031, Loss: 0.9160046577453613, Accuracy: 1.0, Computation time: 1.9548566341400146\n",
      "Step: 2032, Loss: 0.9197345972061157, Accuracy: 1.0, Computation time: 1.5343825817108154\n",
      "Step: 2033, Loss: 0.9160730242729187, Accuracy: 1.0, Computation time: 2.148348331451416\n",
      "Step: 2034, Loss: 0.9376522302627563, Accuracy: 0.96875, Computation time: 1.919339895248413\n",
      "Step: 2035, Loss: 0.9160304665565491, Accuracy: 1.0, Computation time: 1.779407024383545\n",
      "Step: 2036, Loss: 0.9160898327827454, Accuracy: 1.0, Computation time: 1.6708471775054932\n",
      "Step: 2037, Loss: 0.9375748634338379, Accuracy: 0.96875, Computation time: 2.0468740463256836\n",
      "Step: 2038, Loss: 0.9159958958625793, Accuracy: 1.0, Computation time: 1.8832242488861084\n",
      "Step: 2039, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.838942289352417\n",
      "Step: 2040, Loss: 0.9159725904464722, Accuracy: 1.0, Computation time: 1.7866795063018799\n",
      "Step: 2041, Loss: 0.9171428680419922, Accuracy: 1.0, Computation time: 2.328362226486206\n",
      "Step: 2042, Loss: 0.9162635803222656, Accuracy: 1.0, Computation time: 1.7848076820373535\n",
      "Step: 2043, Loss: 0.9274425506591797, Accuracy: 0.96875, Computation time: 2.0246188640594482\n",
      "Step: 2044, Loss: 0.9161002039909363, Accuracy: 1.0, Computation time: 1.7631964683532715\n",
      "Step: 2045, Loss: 0.9161428213119507, Accuracy: 1.0, Computation time: 2.080232620239258\n",
      "Step: 2046, Loss: 0.9159773588180542, Accuracy: 1.0, Computation time: 2.0376882553100586\n",
      "Step: 2047, Loss: 0.9160013794898987, Accuracy: 1.0, Computation time: 1.9160423278808594\n",
      "Step: 2048, Loss: 0.9159602522850037, Accuracy: 1.0, Computation time: 1.698359489440918\n",
      "Step: 2049, Loss: 0.9159460663795471, Accuracy: 1.0, Computation time: 1.926607370376587\n",
      "Step: 2050, Loss: 0.9159671068191528, Accuracy: 1.0, Computation time: 1.979907512664795\n",
      "Step: 2051, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.9912397861480713\n",
      "Step: 2052, Loss: 0.9377104043960571, Accuracy: 0.96875, Computation time: 2.2158138751983643\n",
      "Step: 2053, Loss: 0.9166759848594666, Accuracy: 1.0, Computation time: 1.7194983959197998\n",
      "Step: 2054, Loss: 0.9238876104354858, Accuracy: 1.0, Computation time: 2.3244316577911377\n",
      "Step: 2055, Loss: 0.9159482717514038, Accuracy: 1.0, Computation time: 1.8292124271392822\n",
      "Step: 2056, Loss: 0.9161292910575867, Accuracy: 1.0, Computation time: 1.8930158615112305\n",
      "Step: 2057, Loss: 0.9160307049751282, Accuracy: 1.0, Computation time: 1.7294607162475586\n",
      "Step: 2058, Loss: 0.9162650108337402, Accuracy: 1.0, Computation time: 1.9764535427093506\n",
      "Step: 2059, Loss: 0.9164463877677917, Accuracy: 1.0, Computation time: 1.8288953304290771\n",
      "Step: 2060, Loss: 0.9375535249710083, Accuracy: 0.96875, Computation time: 2.0060489177703857\n",
      "Step: 2061, Loss: 0.9160454273223877, Accuracy: 1.0, Computation time: 1.7594232559204102\n",
      "Step: 2062, Loss: 0.9159663319587708, Accuracy: 1.0, Computation time: 2.0151162147521973\n",
      "Step: 2063, Loss: 0.933249831199646, Accuracy: 0.96875, Computation time: 1.9139924049377441\n",
      "Step: 2064, Loss: 0.9582917094230652, Accuracy: 0.9375, Computation time: 2.1572422981262207\n",
      "Step: 2065, Loss: 0.9166260957717896, Accuracy: 1.0, Computation time: 1.7837231159210205\n",
      "Step: 2066, Loss: 0.9168960452079773, Accuracy: 1.0, Computation time: 1.942819595336914\n",
      "Step: 2067, Loss: 0.9181593656539917, Accuracy: 1.0, Computation time: 2.018585443496704\n",
      "Step: 2068, Loss: 0.9187957048416138, Accuracy: 1.0, Computation time: 2.2607805728912354\n",
      "Step: 2069, Loss: 0.9173627495765686, Accuracy: 1.0, Computation time: 1.6983697414398193\n",
      "Step: 2070, Loss: 0.9347084164619446, Accuracy: 0.96875, Computation time: 2.592740774154663\n",
      "Step: 2071, Loss: 0.9176774024963379, Accuracy: 1.0, Computation time: 2.377551555633545\n",
      "Step: 2072, Loss: 0.9237812757492065, Accuracy: 1.0, Computation time: 2.20434832572937\n",
      "Step: 2073, Loss: 0.960028350353241, Accuracy: 0.9375, Computation time: 2.2912421226501465\n",
      "Step: 2074, Loss: 0.9163594245910645, Accuracy: 1.0, Computation time: 1.9462637901306152\n",
      "Step: 2075, Loss: 0.9381645321846008, Accuracy: 0.96875, Computation time: 1.6695642471313477\n",
      "Step: 2076, Loss: 0.9164665341377258, Accuracy: 1.0, Computation time: 1.7802114486694336\n",
      "Step: 2077, Loss: 0.9176739454269409, Accuracy: 1.0, Computation time: 1.9433512687683105\n",
      "Step: 2078, Loss: 0.9163753390312195, Accuracy: 1.0, Computation time: 1.6980600357055664\n",
      "Step: 2079, Loss: 0.9163355827331543, Accuracy: 1.0, Computation time: 1.5082182884216309\n",
      "Step: 2080, Loss: 0.9208610653877258, Accuracy: 1.0, Computation time: 1.6148574352264404\n",
      "Step: 2081, Loss: 0.9163041114807129, Accuracy: 1.0, Computation time: 1.4091598987579346\n",
      "Step: 2082, Loss: 0.9162017703056335, Accuracy: 1.0, Computation time: 1.6423060894012451\n",
      "Step: 2083, Loss: 0.9166467189788818, Accuracy: 1.0, Computation time: 1.8359220027923584\n",
      "Step: 2084, Loss: 0.9164524078369141, Accuracy: 1.0, Computation time: 1.5191094875335693\n",
      "Step: 2085, Loss: 0.9166619777679443, Accuracy: 1.0, Computation time: 1.4396016597747803\n",
      "########################\n",
      "Test loss: 1.0727877616882324, Test Accuracy_epoch15: 0.7691561579704285\n",
      "########################\n",
      "Step: 2086, Loss: 0.9162044525146484, Accuracy: 1.0, Computation time: 1.1759717464447021\n",
      "Step: 2087, Loss: 0.9161683320999146, Accuracy: 1.0, Computation time: 1.2888894081115723\n",
      "Step: 2088, Loss: 0.9159998893737793, Accuracy: 1.0, Computation time: 1.1991260051727295\n",
      "Step: 2089, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 1.0827765464782715\n",
      "Step: 2090, Loss: 0.9160568118095398, Accuracy: 1.0, Computation time: 1.3840034008026123\n",
      "Step: 2091, Loss: 0.9162233471870422, Accuracy: 1.0, Computation time: 1.1921803951263428\n",
      "Step: 2092, Loss: 0.9378795027732849, Accuracy: 0.96875, Computation time: 1.1416995525360107\n",
      "Step: 2093, Loss: 0.916095495223999, Accuracy: 1.0, Computation time: 1.3346035480499268\n",
      "Step: 2094, Loss: 0.9160476326942444, Accuracy: 1.0, Computation time: 1.3247628211975098\n",
      "Step: 2095, Loss: 0.9161351919174194, Accuracy: 1.0, Computation time: 1.4808449745178223\n",
      "Step: 2096, Loss: 0.9164409041404724, Accuracy: 1.0, Computation time: 1.3994569778442383\n",
      "Step: 2097, Loss: 0.9160918593406677, Accuracy: 1.0, Computation time: 1.3859288692474365\n",
      "Step: 2098, Loss: 0.9372869729995728, Accuracy: 0.96875, Computation time: 1.431058645248413\n",
      "Step: 2099, Loss: 0.9159951210021973, Accuracy: 1.0, Computation time: 1.64815354347229\n",
      "Step: 2100, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.0957093238830566\n",
      "Step: 2101, Loss: 0.9337273836135864, Accuracy: 0.96875, Computation time: 1.6916677951812744\n",
      "Step: 2102, Loss: 0.9357579350471497, Accuracy: 0.96875, Computation time: 1.454862117767334\n",
      "Step: 2103, Loss: 0.9161264896392822, Accuracy: 1.0, Computation time: 1.589057207107544\n",
      "Step: 2104, Loss: 0.915973424911499, Accuracy: 1.0, Computation time: 1.4100227355957031\n",
      "Step: 2105, Loss: 0.9159943461418152, Accuracy: 1.0, Computation time: 1.2803566455841064\n",
      "Step: 2106, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 1.0284545421600342\n",
      "Step: 2107, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 1.0095200538635254\n",
      "Step: 2108, Loss: 0.9161229133605957, Accuracy: 1.0, Computation time: 1.4556567668914795\n",
      "Step: 2109, Loss: 0.9160388112068176, Accuracy: 1.0, Computation time: 1.0762228965759277\n",
      "Step: 2110, Loss: 0.9159106612205505, Accuracy: 1.0, Computation time: 1.3552894592285156\n",
      "Step: 2111, Loss: 0.9220460057258606, Accuracy: 1.0, Computation time: 1.368314504623413\n",
      "Step: 2112, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.039299488067627\n",
      "Step: 2113, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 0.9341154098510742\n",
      "Step: 2114, Loss: 0.9162594079971313, Accuracy: 1.0, Computation time: 1.5351300239562988\n",
      "Step: 2115, Loss: 0.9160923957824707, Accuracy: 1.0, Computation time: 1.878330945968628\n",
      "Step: 2116, Loss: 0.9159343838691711, Accuracy: 1.0, Computation time: 1.2353997230529785\n",
      "Step: 2117, Loss: 0.9270710945129395, Accuracy: 0.96875, Computation time: 1.155486822128296\n",
      "Step: 2118, Loss: 0.9376718997955322, Accuracy: 0.96875, Computation time: 0.9637563228607178\n",
      "Step: 2119, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 0.9688341617584229\n",
      "Step: 2120, Loss: 0.915957510471344, Accuracy: 1.0, Computation time: 1.1131856441497803\n",
      "Step: 2121, Loss: 0.9159569144248962, Accuracy: 1.0, Computation time: 0.9817533493041992\n",
      "Step: 2122, Loss: 0.9161710739135742, Accuracy: 1.0, Computation time: 1.224870204925537\n",
      "Step: 2123, Loss: 0.9162081480026245, Accuracy: 1.0, Computation time: 0.8963823318481445\n",
      "Step: 2124, Loss: 0.9369261264801025, Accuracy: 0.96875, Computation time: 1.2232627868652344\n",
      "Step: 2125, Loss: 0.9161221981048584, Accuracy: 1.0, Computation time: 1.199657678604126\n",
      "Step: 2126, Loss: 0.9160183668136597, Accuracy: 1.0, Computation time: 1.2065150737762451\n",
      "Step: 2127, Loss: 0.9387764930725098, Accuracy: 0.96875, Computation time: 1.0551645755767822\n",
      "Step: 2128, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 0.9687519073486328\n",
      "Step: 2129, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.3336679935455322\n",
      "Step: 2130, Loss: 0.9162929654121399, Accuracy: 1.0, Computation time: 1.395833969116211\n",
      "Step: 2131, Loss: 0.9340856671333313, Accuracy: 0.96875, Computation time: 1.2295317649841309\n",
      "Step: 2132, Loss: 0.9160346388816833, Accuracy: 1.0, Computation time: 1.1523971557617188\n",
      "Step: 2133, Loss: 0.9167630672454834, Accuracy: 1.0, Computation time: 1.0784153938293457\n",
      "Step: 2134, Loss: 0.9261133670806885, Accuracy: 0.96875, Computation time: 1.5220129489898682\n",
      "Step: 2135, Loss: 0.9160804748535156, Accuracy: 1.0, Computation time: 1.1199979782104492\n",
      "Step: 2136, Loss: 0.9160423874855042, Accuracy: 1.0, Computation time: 1.3656387329101562\n",
      "Step: 2137, Loss: 0.9160586595535278, Accuracy: 1.0, Computation time: 1.077012538909912\n",
      "Step: 2138, Loss: 0.9375353455543518, Accuracy: 0.96875, Computation time: 1.1829848289489746\n",
      "Step: 2139, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 1.2250261306762695\n",
      "Step: 2140, Loss: 0.9316890239715576, Accuracy: 0.96875, Computation time: 1.4566073417663574\n",
      "Step: 2141, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.2010436058044434\n",
      "Step: 2142, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.165959358215332\n",
      "Step: 2143, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 1.1492211818695068\n",
      "Step: 2144, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 1.25117826461792\n",
      "Step: 2145, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.2938733100891113\n",
      "Step: 2146, Loss: 0.9159373044967651, Accuracy: 1.0, Computation time: 1.3791687488555908\n",
      "Step: 2147, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 1.3302996158599854\n",
      "Step: 2148, Loss: 0.9160375595092773, Accuracy: 1.0, Computation time: 1.3861205577850342\n",
      "Step: 2149, Loss: 0.9375360608100891, Accuracy: 0.96875, Computation time: 1.5439352989196777\n",
      "Step: 2150, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 1.7302989959716797\n",
      "Step: 2151, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.7400085926055908\n",
      "Step: 2152, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.3283307552337646\n",
      "Step: 2153, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.6220111846923828\n",
      "Step: 2154, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.5695021152496338\n",
      "Step: 2155, Loss: 0.9375544786453247, Accuracy: 0.96875, Computation time: 1.4880118370056152\n",
      "Step: 2156, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 1.5510008335113525\n",
      "Step: 2157, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.3695454597473145\n",
      "Step: 2158, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.34934401512146\n",
      "Step: 2159, Loss: 0.91960608959198, Accuracy: 1.0, Computation time: 1.8893003463745117\n",
      "Step: 2160, Loss: 0.937547504901886, Accuracy: 0.96875, Computation time: 1.8740663528442383\n",
      "Step: 2161, Loss: 0.9160082936286926, Accuracy: 1.0, Computation time: 1.6455729007720947\n",
      "Step: 2162, Loss: 0.915967583656311, Accuracy: 1.0, Computation time: 1.684183120727539\n",
      "Step: 2163, Loss: 0.9160066246986389, Accuracy: 1.0, Computation time: 2.001805305480957\n",
      "Step: 2164, Loss: 0.9160979986190796, Accuracy: 1.0, Computation time: 1.4482908248901367\n",
      "Step: 2165, Loss: 0.9159853458404541, Accuracy: 1.0, Computation time: 1.3555171489715576\n",
      "Step: 2166, Loss: 0.9159378409385681, Accuracy: 1.0, Computation time: 1.5329291820526123\n",
      "Step: 2167, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.4293086528778076\n",
      "Step: 2168, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 1.8917450904846191\n",
      "Step: 2169, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.5008893013000488\n",
      "Step: 2170, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.925424337387085\n",
      "Step: 2171, Loss: 0.9179477691650391, Accuracy: 1.0, Computation time: 1.98388671875\n",
      "Step: 2172, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.4453208446502686\n",
      "Step: 2173, Loss: 0.9162130951881409, Accuracy: 1.0, Computation time: 1.5556199550628662\n",
      "Step: 2174, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.5172779560089111\n",
      "Step: 2175, Loss: 0.9159653782844543, Accuracy: 1.0, Computation time: 1.207118272781372\n",
      "Step: 2176, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.325850009918213\n",
      "Step: 2177, Loss: 0.9162165522575378, Accuracy: 1.0, Computation time: 1.3357203006744385\n",
      "Step: 2178, Loss: 0.9376204013824463, Accuracy: 0.96875, Computation time: 1.3644323348999023\n",
      "Step: 2179, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 1.266700029373169\n",
      "Step: 2180, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.5527734756469727\n",
      "Step: 2181, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 1.387782096862793\n",
      "Step: 2182, Loss: 0.9365261793136597, Accuracy: 0.96875, Computation time: 1.5489501953125\n",
      "Step: 2183, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 1.5099430084228516\n",
      "Step: 2184, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.1168851852416992\n",
      "Step: 2185, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.2478282451629639\n",
      "Step: 2186, Loss: 0.9197400212287903, Accuracy: 1.0, Computation time: 1.4452474117279053\n",
      "Step: 2187, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.2131586074829102\n",
      "Step: 2188, Loss: 0.937009334564209, Accuracy: 0.96875, Computation time: 1.4485082626342773\n",
      "Step: 2189, Loss: 0.9375979900360107, Accuracy: 0.96875, Computation time: 1.1794471740722656\n",
      "Step: 2190, Loss: 0.9371103048324585, Accuracy: 0.96875, Computation time: 1.2590696811676025\n",
      "Step: 2191, Loss: 0.9376429319381714, Accuracy: 0.96875, Computation time: 1.2886433601379395\n",
      "Step: 2192, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.0811550617218018\n",
      "Step: 2193, Loss: 0.9376283288002014, Accuracy: 0.96875, Computation time: 1.2804627418518066\n",
      "Step: 2194, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.3876245021820068\n",
      "Step: 2195, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 1.5317471027374268\n",
      "Step: 2196, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.3072819709777832\n",
      "Step: 2197, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.1300292015075684\n",
      "Step: 2198, Loss: 0.919154703617096, Accuracy: 1.0, Computation time: 1.5637116432189941\n",
      "Step: 2199, Loss: 0.9370502829551697, Accuracy: 0.96875, Computation time: 1.3993287086486816\n",
      "Step: 2200, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.5093176364898682\n",
      "Step: 2201, Loss: 0.9159058332443237, Accuracy: 1.0, Computation time: 1.594860315322876\n",
      "Step: 2202, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.278123140335083\n",
      "Step: 2203, Loss: 0.9159334301948547, Accuracy: 1.0, Computation time: 1.4321112632751465\n",
      "Step: 2204, Loss: 0.9160908460617065, Accuracy: 1.0, Computation time: 1.404914140701294\n",
      "Step: 2205, Loss: 0.9304831027984619, Accuracy: 0.96875, Computation time: 1.8255469799041748\n",
      "Step: 2206, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.3874568939208984\n",
      "Step: 2207, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 1.2095341682434082\n",
      "Step: 2208, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.4826428890228271\n",
      "Step: 2209, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 1.4034051895141602\n",
      "Step: 2210, Loss: 0.9340617656707764, Accuracy: 0.96875, Computation time: 1.8218650817871094\n",
      "Step: 2211, Loss: 0.9370402693748474, Accuracy: 0.96875, Computation time: 1.227302074432373\n",
      "Step: 2212, Loss: 0.9281401634216309, Accuracy: 0.96875, Computation time: 1.2535288333892822\n",
      "Step: 2213, Loss: 0.9159977436065674, Accuracy: 1.0, Computation time: 1.6009116172790527\n",
      "Step: 2214, Loss: 0.9159668684005737, Accuracy: 1.0, Computation time: 1.1875886917114258\n",
      "Step: 2215, Loss: 0.9372744560241699, Accuracy: 0.96875, Computation time: 1.4424853324890137\n",
      "Step: 2216, Loss: 0.9160537123680115, Accuracy: 1.0, Computation time: 1.4445528984069824\n",
      "Step: 2217, Loss: 0.9161996245384216, Accuracy: 1.0, Computation time: 1.466071367263794\n",
      "Step: 2218, Loss: 0.9162828326225281, Accuracy: 1.0, Computation time: 1.760849952697754\n",
      "Step: 2219, Loss: 0.9160130023956299, Accuracy: 1.0, Computation time: 1.8604285717010498\n",
      "Step: 2220, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.2404184341430664\n",
      "Step: 2221, Loss: 0.915967583656311, Accuracy: 1.0, Computation time: 1.8118672370910645\n",
      "Step: 2222, Loss: 0.9166238903999329, Accuracy: 1.0, Computation time: 1.9647643566131592\n",
      "Step: 2223, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.4889130592346191\n",
      "########################\n",
      "Test loss: 1.0744075775146484, Test Accuracy_epoch16: 0.7672163248062134\n",
      "########################\n",
      "Step: 2224, Loss: 0.9163810014724731, Accuracy: 1.0, Computation time: 1.8673007488250732\n",
      "Step: 2225, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.6072170734405518\n",
      "Step: 2226, Loss: 0.9372880458831787, Accuracy: 0.96875, Computation time: 2.041083812713623\n",
      "Step: 2227, Loss: 0.9159042239189148, Accuracy: 1.0, Computation time: 1.712148904800415\n",
      "Step: 2228, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.9204237461090088\n",
      "Step: 2229, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.6631455421447754\n",
      "Step: 2230, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.913027286529541\n",
      "Step: 2231, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 2.401477336883545\n",
      "Step: 2232, Loss: 0.9160735011100769, Accuracy: 1.0, Computation time: 2.3641741275787354\n",
      "Step: 2233, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 1.8785018920898438\n",
      "Step: 2234, Loss: 0.9285911321640015, Accuracy: 0.96875, Computation time: 2.043243169784546\n",
      "Step: 2235, Loss: 0.9189841747283936, Accuracy: 1.0, Computation time: 2.376840829849243\n",
      "Step: 2236, Loss: 0.927322506904602, Accuracy: 0.96875, Computation time: 2.1328084468841553\n",
      "Step: 2237, Loss: 0.9190044403076172, Accuracy: 1.0, Computation time: 2.0971202850341797\n",
      "Step: 2238, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 1.7918813228607178\n",
      "Step: 2239, Loss: 0.9159567952156067, Accuracy: 1.0, Computation time: 1.334954023361206\n",
      "Step: 2240, Loss: 0.9440307021141052, Accuracy: 0.96875, Computation time: 1.741002082824707\n",
      "Step: 2241, Loss: 0.915990948677063, Accuracy: 1.0, Computation time: 1.5568499565124512\n",
      "Step: 2242, Loss: 0.915939211845398, Accuracy: 1.0, Computation time: 1.765838861465454\n",
      "Step: 2243, Loss: 0.9374720454216003, Accuracy: 0.96875, Computation time: 1.6215155124664307\n",
      "Step: 2244, Loss: 0.9162193536758423, Accuracy: 1.0, Computation time: 1.5173566341400146\n",
      "Step: 2245, Loss: 0.9374358057975769, Accuracy: 0.96875, Computation time: 1.2167468070983887\n",
      "Step: 2246, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.6316351890563965\n",
      "Step: 2247, Loss: 0.9160250425338745, Accuracy: 1.0, Computation time: 1.3298101425170898\n",
      "Step: 2248, Loss: 0.9160195589065552, Accuracy: 1.0, Computation time: 2.144284725189209\n",
      "Step: 2249, Loss: 0.916097104549408, Accuracy: 1.0, Computation time: 1.6964879035949707\n",
      "Step: 2250, Loss: 0.9163405299186707, Accuracy: 1.0, Computation time: 1.5047757625579834\n",
      "Step: 2251, Loss: 0.9159891605377197, Accuracy: 1.0, Computation time: 1.5033369064331055\n",
      "Step: 2252, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 1.375941514968872\n",
      "Step: 2253, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 1.8891727924346924\n",
      "Step: 2254, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.4604918956756592\n",
      "Step: 2255, Loss: 0.9161133170127869, Accuracy: 1.0, Computation time: 1.606727123260498\n",
      "Step: 2256, Loss: 0.9781202077865601, Accuracy: 0.90625, Computation time: 1.4763824939727783\n",
      "Step: 2257, Loss: 0.916028618812561, Accuracy: 1.0, Computation time: 1.6009271144866943\n",
      "Step: 2258, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 1.6092846393585205\n",
      "Step: 2259, Loss: 0.916050910949707, Accuracy: 1.0, Computation time: 1.3524808883666992\n",
      "Step: 2260, Loss: 0.924251914024353, Accuracy: 1.0, Computation time: 1.6603617668151855\n",
      "Step: 2261, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.637068510055542\n",
      "Step: 2262, Loss: 0.9254748821258545, Accuracy: 0.96875, Computation time: 1.5519142150878906\n",
      "Step: 2263, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 1.5427162647247314\n",
      "Step: 2264, Loss: 0.9161626696586609, Accuracy: 1.0, Computation time: 1.330235481262207\n",
      "Step: 2265, Loss: 0.9160159230232239, Accuracy: 1.0, Computation time: 1.5920016765594482\n",
      "Step: 2266, Loss: 0.9275070428848267, Accuracy: 0.96875, Computation time: 1.6309154033660889\n",
      "Step: 2267, Loss: 0.9160095453262329, Accuracy: 1.0, Computation time: 1.6340687274932861\n",
      "Step: 2268, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.3274664878845215\n",
      "Step: 2269, Loss: 0.9160096645355225, Accuracy: 1.0, Computation time: 1.4153761863708496\n",
      "Step: 2270, Loss: 0.9159936904907227, Accuracy: 1.0, Computation time: 1.604346752166748\n",
      "Step: 2271, Loss: 0.9159552454948425, Accuracy: 1.0, Computation time: 1.247187614440918\n",
      "Step: 2272, Loss: 0.9170490503311157, Accuracy: 1.0, Computation time: 1.8325655460357666\n",
      "Step: 2273, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 1.6522860527038574\n",
      "Step: 2274, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 1.3452093601226807\n",
      "Step: 2275, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.4785575866699219\n",
      "Step: 2276, Loss: 0.9161015748977661, Accuracy: 1.0, Computation time: 1.7023732662200928\n",
      "Step: 2277, Loss: 0.9340895414352417, Accuracy: 0.96875, Computation time: 1.5454356670379639\n",
      "Step: 2278, Loss: 0.9159355759620667, Accuracy: 1.0, Computation time: 1.3543579578399658\n",
      "Step: 2279, Loss: 0.915947675704956, Accuracy: 1.0, Computation time: 1.3546805381774902\n",
      "Step: 2280, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 1.4290647506713867\n",
      "Step: 2281, Loss: 0.9160647988319397, Accuracy: 1.0, Computation time: 1.229569673538208\n",
      "Step: 2282, Loss: 0.9159827828407288, Accuracy: 1.0, Computation time: 1.4000120162963867\n",
      "Step: 2283, Loss: 0.9364661574363708, Accuracy: 0.96875, Computation time: 1.5681970119476318\n",
      "Step: 2284, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.5846583843231201\n",
      "Step: 2285, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 1.54290771484375\n",
      "Step: 2286, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.1482367515563965\n",
      "Step: 2287, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.5795485973358154\n",
      "Step: 2288, Loss: 0.9158727526664734, Accuracy: 1.0, Computation time: 1.3856794834136963\n",
      "Step: 2289, Loss: 0.9160587191581726, Accuracy: 1.0, Computation time: 1.3883960247039795\n",
      "Step: 2290, Loss: 0.9158900380134583, Accuracy: 1.0, Computation time: 1.2633869647979736\n",
      "Step: 2291, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.2914013862609863\n",
      "Step: 2292, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.3391201496124268\n",
      "Step: 2293, Loss: 0.9218327403068542, Accuracy: 1.0, Computation time: 1.3633627891540527\n",
      "Step: 2294, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 1.4085619449615479\n",
      "Step: 2295, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.2977056503295898\n",
      "Step: 2296, Loss: 0.9159649014472961, Accuracy: 1.0, Computation time: 1.0400378704071045\n",
      "Step: 2297, Loss: 0.9374980330467224, Accuracy: 0.96875, Computation time: 1.215404987335205\n",
      "Step: 2298, Loss: 0.9159714579582214, Accuracy: 1.0, Computation time: 1.1907646656036377\n",
      "Step: 2299, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.3614494800567627\n",
      "Step: 2300, Loss: 0.9159485101699829, Accuracy: 1.0, Computation time: 1.5949411392211914\n",
      "Step: 2301, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.2113871574401855\n",
      "Step: 2302, Loss: 0.917985737323761, Accuracy: 1.0, Computation time: 1.5407521724700928\n",
      "Step: 2303, Loss: 0.9162304997444153, Accuracy: 1.0, Computation time: 1.2410414218902588\n",
      "Step: 2304, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.3280715942382812\n",
      "Step: 2305, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.4805867671966553\n",
      "Step: 2306, Loss: 0.9159570932388306, Accuracy: 1.0, Computation time: 1.3501195907592773\n",
      "Step: 2307, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.4439990520477295\n",
      "Step: 2308, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.740323543548584\n",
      "Step: 2309, Loss: 0.91590815782547, Accuracy: 1.0, Computation time: 1.2062864303588867\n",
      "Step: 2310, Loss: 0.919066309928894, Accuracy: 1.0, Computation time: 2.3200418949127197\n",
      "Step: 2311, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.3857238292694092\n",
      "Step: 2312, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.2548885345458984\n",
      "Step: 2313, Loss: 0.9161582589149475, Accuracy: 1.0, Computation time: 1.3123860359191895\n",
      "Step: 2314, Loss: 0.9273836612701416, Accuracy: 0.96875, Computation time: 1.4367241859436035\n",
      "Step: 2315, Loss: 0.9557784795761108, Accuracy: 0.9375, Computation time: 1.2627687454223633\n",
      "Step: 2316, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 1.2143926620483398\n",
      "Step: 2317, Loss: 0.9597839713096619, Accuracy: 0.9375, Computation time: 1.4146275520324707\n",
      "Step: 2318, Loss: 0.9161761999130249, Accuracy: 1.0, Computation time: 1.4307441711425781\n",
      "Step: 2319, Loss: 0.9241732358932495, Accuracy: 1.0, Computation time: 1.503554105758667\n",
      "Step: 2320, Loss: 0.9161570072174072, Accuracy: 1.0, Computation time: 1.2860493659973145\n",
      "Step: 2321, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 1.237969160079956\n",
      "Step: 2322, Loss: 0.9168142080307007, Accuracy: 1.0, Computation time: 1.1809141635894775\n",
      "Step: 2323, Loss: 0.9160434007644653, Accuracy: 1.0, Computation time: 1.1875391006469727\n",
      "Step: 2324, Loss: 0.916127622127533, Accuracy: 1.0, Computation time: 1.2378783226013184\n",
      "Step: 2325, Loss: 0.9164095520973206, Accuracy: 1.0, Computation time: 1.023686408996582\n",
      "Step: 2326, Loss: 0.9161683917045593, Accuracy: 1.0, Computation time: 1.6242296695709229\n",
      "Step: 2327, Loss: 0.916333794593811, Accuracy: 1.0, Computation time: 1.484238862991333\n",
      "Step: 2328, Loss: 0.937926709651947, Accuracy: 0.96875, Computation time: 1.3160796165466309\n",
      "Step: 2329, Loss: 0.916002094745636, Accuracy: 1.0, Computation time: 1.4446899890899658\n",
      "Step: 2330, Loss: 0.9159231781959534, Accuracy: 1.0, Computation time: 1.1430003643035889\n",
      "Step: 2331, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.1389539241790771\n",
      "Step: 2332, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.1991865634918213\n",
      "Step: 2333, Loss: 0.9159737825393677, Accuracy: 1.0, Computation time: 1.2353003025054932\n",
      "Step: 2334, Loss: 0.9159852862358093, Accuracy: 1.0, Computation time: 1.2830901145935059\n",
      "Step: 2335, Loss: 0.9161339402198792, Accuracy: 1.0, Computation time: 1.2667152881622314\n",
      "Step: 2336, Loss: 0.9160649180412292, Accuracy: 1.0, Computation time: 1.5715711116790771\n",
      "Step: 2337, Loss: 0.9159879088401794, Accuracy: 1.0, Computation time: 1.2035551071166992\n",
      "Step: 2338, Loss: 0.9202213883399963, Accuracy: 1.0, Computation time: 1.3208131790161133\n",
      "Step: 2339, Loss: 0.9375651478767395, Accuracy: 0.96875, Computation time: 1.3750574588775635\n",
      "Step: 2340, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 1.6588749885559082\n",
      "Step: 2341, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.1746137142181396\n",
      "Step: 2342, Loss: 0.9159795045852661, Accuracy: 1.0, Computation time: 1.6365315914154053\n",
      "Step: 2343, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.8071990013122559\n",
      "Step: 2344, Loss: 0.9377424716949463, Accuracy: 0.96875, Computation time: 1.1492016315460205\n",
      "Step: 2345, Loss: 0.9159227013587952, Accuracy: 1.0, Computation time: 1.5022523403167725\n",
      "Step: 2346, Loss: 0.9160158634185791, Accuracy: 1.0, Computation time: 1.299720048904419\n",
      "Step: 2347, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 1.7328972816467285\n",
      "Step: 2348, Loss: 0.9185826778411865, Accuracy: 1.0, Computation time: 1.3925507068634033\n",
      "Step: 2349, Loss: 0.9159383773803711, Accuracy: 1.0, Computation time: 1.4300203323364258\n",
      "Step: 2350, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.4721639156341553\n",
      "Step: 2351, Loss: 0.9183982014656067, Accuracy: 1.0, Computation time: 1.647038459777832\n",
      "Step: 2352, Loss: 0.9254359006881714, Accuracy: 0.96875, Computation time: 1.5109241008758545\n",
      "Step: 2353, Loss: 0.9159879684448242, Accuracy: 1.0, Computation time: 1.2986037731170654\n",
      "Step: 2354, Loss: 0.9159513711929321, Accuracy: 1.0, Computation time: 1.403515100479126\n",
      "Step: 2355, Loss: 0.9160456657409668, Accuracy: 1.0, Computation time: 1.3941354751586914\n",
      "Step: 2356, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.7125921249389648\n",
      "Step: 2357, Loss: 0.9356408715248108, Accuracy: 0.96875, Computation time: 2.236830472946167\n",
      "Step: 2358, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.2395873069763184\n",
      "Step: 2359, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.4522297382354736\n",
      "Step: 2360, Loss: 0.9362689256668091, Accuracy: 0.96875, Computation time: 1.8188607692718506\n",
      "Step: 2361, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 1.6177098751068115\n",
      "Step: 2362, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.9882495403289795\n",
      "########################\n",
      "Test loss: 1.072839379310608, Test Accuracy_epoch17: 0.7710960507392883\n",
      "########################\n",
      "Step: 2363, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 1.793245792388916\n",
      "Step: 2364, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.7782416343688965\n",
      "Step: 2365, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.5731158256530762\n",
      "Step: 2366, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.4720604419708252\n",
      "Step: 2367, Loss: 0.9376967549324036, Accuracy: 0.96875, Computation time: 1.9049232006072998\n",
      "Step: 2368, Loss: 0.9163734316825867, Accuracy: 1.0, Computation time: 1.7423274517059326\n",
      "Step: 2369, Loss: 0.9378390908241272, Accuracy: 0.96875, Computation time: 2.2303926944732666\n",
      "Step: 2370, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 1.7827930450439453\n",
      "Step: 2371, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.8728632926940918\n",
      "Step: 2372, Loss: 0.934736430644989, Accuracy: 0.96875, Computation time: 2.1696293354034424\n",
      "Step: 2373, Loss: 0.9159696102142334, Accuracy: 1.0, Computation time: 1.8175806999206543\n",
      "Step: 2374, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 1.7209389209747314\n",
      "Step: 2375, Loss: 0.915989339351654, Accuracy: 1.0, Computation time: 1.7908337116241455\n",
      "Step: 2376, Loss: 0.9183648228645325, Accuracy: 1.0, Computation time: 1.65655517578125\n",
      "Step: 2377, Loss: 0.9167624115943909, Accuracy: 1.0, Computation time: 2.4064481258392334\n",
      "Step: 2378, Loss: 0.9303927421569824, Accuracy: 0.96875, Computation time: 1.9825174808502197\n",
      "Step: 2379, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.587754249572754\n",
      "Step: 2380, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.976433515548706\n",
      "Step: 2381, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.9367585182189941\n",
      "Step: 2382, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.9095287322998047\n",
      "Step: 2383, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 1.8294463157653809\n",
      "Step: 2384, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 1.7214999198913574\n",
      "Step: 2385, Loss: 0.9160140156745911, Accuracy: 1.0, Computation time: 1.8602993488311768\n",
      "Step: 2386, Loss: 0.915995180606842, Accuracy: 1.0, Computation time: 1.8116209506988525\n",
      "Step: 2387, Loss: 0.915909469127655, Accuracy: 1.0, Computation time: 1.9529683589935303\n",
      "Step: 2388, Loss: 0.9159879088401794, Accuracy: 1.0, Computation time: 1.763641357421875\n",
      "Step: 2389, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 2.20772385597229\n",
      "Step: 2390, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.881537675857544\n",
      "Step: 2391, Loss: 0.9159256219863892, Accuracy: 1.0, Computation time: 2.5169484615325928\n",
      "Step: 2392, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.7062129974365234\n",
      "Step: 2393, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.726940631866455\n",
      "Step: 2394, Loss: 0.9376572370529175, Accuracy: 0.96875, Computation time: 1.9014253616333008\n",
      "Step: 2395, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 2.071073532104492\n",
      "Step: 2396, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.9608304500579834\n",
      "Step: 2397, Loss: 0.9284030199050903, Accuracy: 0.96875, Computation time: 2.2167646884918213\n",
      "Step: 2398, Loss: 0.9579700231552124, Accuracy: 0.9375, Computation time: 2.053766965866089\n",
      "Step: 2399, Loss: 0.9161167144775391, Accuracy: 1.0, Computation time: 1.9871180057525635\n",
      "Step: 2400, Loss: 0.9299296140670776, Accuracy: 0.96875, Computation time: 2.125361680984497\n",
      "Step: 2401, Loss: 0.9180431962013245, Accuracy: 1.0, Computation time: 1.8475470542907715\n",
      "Step: 2402, Loss: 0.9161295890808105, Accuracy: 1.0, Computation time: 1.7157261371612549\n",
      "Step: 2403, Loss: 0.9263836741447449, Accuracy: 0.96875, Computation time: 2.545938491821289\n",
      "Step: 2404, Loss: 0.9165292978286743, Accuracy: 1.0, Computation time: 1.4903149604797363\n",
      "Step: 2405, Loss: 0.9162896275520325, Accuracy: 1.0, Computation time: 1.1223173141479492\n",
      "Step: 2406, Loss: 0.9166336059570312, Accuracy: 1.0, Computation time: 1.3660621643066406\n",
      "Step: 2407, Loss: 0.9166845679283142, Accuracy: 1.0, Computation time: 1.4816272258758545\n",
      "Step: 2408, Loss: 0.9164652824401855, Accuracy: 1.0, Computation time: 1.2452301979064941\n",
      "Step: 2409, Loss: 0.9162887334823608, Accuracy: 1.0, Computation time: 1.3841650485992432\n",
      "Step: 2410, Loss: 0.9163769483566284, Accuracy: 1.0, Computation time: 1.0937600135803223\n",
      "Step: 2411, Loss: 0.9161993265151978, Accuracy: 1.0, Computation time: 1.1358613967895508\n",
      "Step: 2412, Loss: 0.9160087704658508, Accuracy: 1.0, Computation time: 1.0874683856964111\n",
      "Step: 2413, Loss: 0.9198057651519775, Accuracy: 1.0, Computation time: 1.2546021938323975\n",
      "Step: 2414, Loss: 0.9279142022132874, Accuracy: 0.96875, Computation time: 1.460890769958496\n",
      "Step: 2415, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.1552479267120361\n",
      "Step: 2416, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 0.9512515068054199\n",
      "Step: 2417, Loss: 0.9243838787078857, Accuracy: 1.0, Computation time: 1.8409476280212402\n",
      "Step: 2418, Loss: 0.916329026222229, Accuracy: 1.0, Computation time: 0.9751920700073242\n",
      "Step: 2419, Loss: 0.9163069128990173, Accuracy: 1.0, Computation time: 1.139228105545044\n",
      "Step: 2420, Loss: 0.9162939190864563, Accuracy: 1.0, Computation time: 1.0336596965789795\n",
      "Step: 2421, Loss: 0.9401106238365173, Accuracy: 0.96875, Computation time: 1.423844337463379\n",
      "Step: 2422, Loss: 0.9164905548095703, Accuracy: 1.0, Computation time: 0.9438915252685547\n",
      "Step: 2423, Loss: 0.9161872267723083, Accuracy: 1.0, Computation time: 1.309685468673706\n",
      "Step: 2424, Loss: 0.9167082905769348, Accuracy: 1.0, Computation time: 1.2361230850219727\n",
      "Step: 2425, Loss: 0.9159339666366577, Accuracy: 1.0, Computation time: 1.1332733631134033\n",
      "Step: 2426, Loss: 0.9159576296806335, Accuracy: 1.0, Computation time: 1.0953409671783447\n",
      "Step: 2427, Loss: 0.916002094745636, Accuracy: 1.0, Computation time: 0.9503076076507568\n",
      "Step: 2428, Loss: 0.9159952402114868, Accuracy: 1.0, Computation time: 1.0045974254608154\n",
      "Step: 2429, Loss: 0.9161940813064575, Accuracy: 1.0, Computation time: 1.074190378189087\n",
      "Step: 2430, Loss: 0.9159408807754517, Accuracy: 1.0, Computation time: 1.090233325958252\n",
      "Step: 2431, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 0.988800048828125\n",
      "Step: 2432, Loss: 0.9180111885070801, Accuracy: 1.0, Computation time: 1.2735109329223633\n",
      "Step: 2433, Loss: 0.9420767426490784, Accuracy: 0.96875, Computation time: 1.5466084480285645\n",
      "Step: 2434, Loss: 0.9159349203109741, Accuracy: 1.0, Computation time: 0.9624929428100586\n",
      "Step: 2435, Loss: 0.91606605052948, Accuracy: 1.0, Computation time: 1.018040418624878\n",
      "Step: 2436, Loss: 0.9378072023391724, Accuracy: 0.96875, Computation time: 1.0087783336639404\n",
      "Step: 2437, Loss: 0.9160262942314148, Accuracy: 1.0, Computation time: 0.9640038013458252\n",
      "Step: 2438, Loss: 0.9160533547401428, Accuracy: 1.0, Computation time: 1.033585548400879\n",
      "Step: 2439, Loss: 0.9160516858100891, Accuracy: 1.0, Computation time: 0.9261977672576904\n",
      "Step: 2440, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 0.9808659553527832\n",
      "Step: 2441, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 1.1243817806243896\n",
      "Step: 2442, Loss: 0.9163168668746948, Accuracy: 1.0, Computation time: 1.3106725215911865\n",
      "Step: 2443, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 0.9241254329681396\n",
      "Step: 2444, Loss: 0.9160799384117126, Accuracy: 1.0, Computation time: 1.1215994358062744\n",
      "Step: 2445, Loss: 0.9158954620361328, Accuracy: 1.0, Computation time: 1.3321900367736816\n",
      "Step: 2446, Loss: 0.9607213139533997, Accuracy: 0.9375, Computation time: 1.6070244312286377\n",
      "Step: 2447, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 1.257638931274414\n",
      "Step: 2448, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 1.0505163669586182\n",
      "Step: 2449, Loss: 0.9235818982124329, Accuracy: 1.0, Computation time: 1.2225069999694824\n",
      "Step: 2450, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.1370184421539307\n",
      "Step: 2451, Loss: 0.9159802794456482, Accuracy: 1.0, Computation time: 1.1400377750396729\n",
      "Step: 2452, Loss: 0.9161328077316284, Accuracy: 1.0, Computation time: 1.0839722156524658\n",
      "Step: 2453, Loss: 0.9161351323127747, Accuracy: 1.0, Computation time: 1.1203265190124512\n",
      "Step: 2454, Loss: 0.9159926176071167, Accuracy: 1.0, Computation time: 1.4254941940307617\n",
      "Step: 2455, Loss: 0.9159688353538513, Accuracy: 1.0, Computation time: 0.962613582611084\n",
      "Step: 2456, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 1.0448801517486572\n",
      "Step: 2457, Loss: 0.9160112738609314, Accuracy: 1.0, Computation time: 1.125488519668579\n",
      "Step: 2458, Loss: 0.9367573857307434, Accuracy: 0.96875, Computation time: 1.1570217609405518\n",
      "Step: 2459, Loss: 0.9366301894187927, Accuracy: 0.96875, Computation time: 1.0789382457733154\n",
      "Step: 2460, Loss: 0.9293836355209351, Accuracy: 0.96875, Computation time: 1.3288278579711914\n",
      "Step: 2461, Loss: 0.9160611629486084, Accuracy: 1.0, Computation time: 0.9901835918426514\n",
      "Step: 2462, Loss: 0.9265493750572205, Accuracy: 0.96875, Computation time: 2.4427332878112793\n",
      "Step: 2463, Loss: 0.9279149770736694, Accuracy: 0.96875, Computation time: 1.5437507629394531\n",
      "Step: 2464, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 0.9932394027709961\n",
      "Step: 2465, Loss: 0.9159654378890991, Accuracy: 1.0, Computation time: 1.1763131618499756\n",
      "Step: 2466, Loss: 0.9159817695617676, Accuracy: 1.0, Computation time: 1.154827356338501\n",
      "Step: 2467, Loss: 0.9160336852073669, Accuracy: 1.0, Computation time: 1.492959976196289\n",
      "Step: 2468, Loss: 0.9159667491912842, Accuracy: 1.0, Computation time: 1.6438069343566895\n",
      "Step: 2469, Loss: 0.9160303473472595, Accuracy: 1.0, Computation time: 1.434274673461914\n",
      "Step: 2470, Loss: 0.9160034656524658, Accuracy: 1.0, Computation time: 1.5986542701721191\n",
      "Step: 2471, Loss: 0.9185900688171387, Accuracy: 1.0, Computation time: 1.7818129062652588\n",
      "Step: 2472, Loss: 0.9374630451202393, Accuracy: 0.96875, Computation time: 1.888047218322754\n",
      "Step: 2473, Loss: 0.9159197211265564, Accuracy: 1.0, Computation time: 1.5195329189300537\n",
      "Step: 2474, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 1.6982829570770264\n",
      "Step: 2475, Loss: 0.9159731268882751, Accuracy: 1.0, Computation time: 1.8516004085540771\n",
      "Step: 2476, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.6915128231048584\n",
      "Step: 2477, Loss: 0.9372485280036926, Accuracy: 0.96875, Computation time: 1.7503774166107178\n",
      "Step: 2478, Loss: 0.9382627606391907, Accuracy: 0.96875, Computation time: 1.7590253353118896\n",
      "Step: 2479, Loss: 0.9217948317527771, Accuracy: 1.0, Computation time: 1.890845537185669\n",
      "Step: 2480, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 2.222820520401001\n",
      "Step: 2481, Loss: 0.9295196533203125, Accuracy: 0.96875, Computation time: 1.8104252815246582\n",
      "Step: 2482, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.7865917682647705\n",
      "Step: 2483, Loss: 0.9159435033798218, Accuracy: 1.0, Computation time: 1.5559041500091553\n",
      "Step: 2484, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.4711546897888184\n",
      "Step: 2485, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.3071551322937012\n",
      "Step: 2486, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.4747364521026611\n",
      "Step: 2487, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.8714730739593506\n",
      "Step: 2488, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 1.6583466529846191\n",
      "Step: 2489, Loss: 0.9277335405349731, Accuracy: 0.96875, Computation time: 1.493546485900879\n",
      "Step: 2490, Loss: 0.9362704753875732, Accuracy: 0.96875, Computation time: 1.420673131942749\n",
      "Step: 2491, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.8370003700256348\n",
      "Step: 2492, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.4518957138061523\n",
      "Step: 2493, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.5778930187225342\n",
      "Step: 2494, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.5202407836914062\n",
      "Step: 2495, Loss: 0.9161204695701599, Accuracy: 1.0, Computation time: 2.4629158973693848\n",
      "Step: 2496, Loss: 0.9159556031227112, Accuracy: 1.0, Computation time: 1.3516736030578613\n",
      "Step: 2497, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 1.9709603786468506\n",
      "Step: 2498, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.7300028800964355\n",
      "Step: 2499, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.4673740863800049\n",
      "Step: 2500, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.9927589893341064\n",
      "Step: 2501, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.6160328388214111\n",
      "########################\n",
      "Test loss: 1.0704818964004517, Test Accuracy_epoch18: 0.7749757766723633\n",
      "########################\n",
      "Step: 2502, Loss: 0.9159349799156189, Accuracy: 1.0, Computation time: 1.7662086486816406\n",
      "Step: 2503, Loss: 0.9374277591705322, Accuracy: 0.96875, Computation time: 1.5952954292297363\n",
      "Step: 2504, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.6904771327972412\n",
      "Step: 2505, Loss: 0.9161506295204163, Accuracy: 1.0, Computation time: 1.365950345993042\n",
      "Step: 2506, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.5505294799804688\n",
      "Step: 2507, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 1.5458614826202393\n",
      "Step: 2508, Loss: 0.9338598847389221, Accuracy: 0.96875, Computation time: 1.890928030014038\n",
      "Step: 2509, Loss: 0.9397058486938477, Accuracy: 0.96875, Computation time: 2.1950433254241943\n",
      "Step: 2510, Loss: 0.9162143468856812, Accuracy: 1.0, Computation time: 1.9327287673950195\n",
      "Step: 2511, Loss: 0.9159566760063171, Accuracy: 1.0, Computation time: 1.822540044784546\n",
      "Step: 2512, Loss: 0.9543015360832214, Accuracy: 0.9375, Computation time: 1.9685328006744385\n",
      "Step: 2513, Loss: 0.9167025089263916, Accuracy: 1.0, Computation time: 1.8054203987121582\n",
      "Step: 2514, Loss: 0.9161196351051331, Accuracy: 1.0, Computation time: 1.5785861015319824\n",
      "Step: 2515, Loss: 0.9381126761436462, Accuracy: 0.96875, Computation time: 1.5901100635528564\n",
      "Step: 2516, Loss: 0.9160237908363342, Accuracy: 1.0, Computation time: 1.8222424983978271\n",
      "Step: 2517, Loss: 0.915982186794281, Accuracy: 1.0, Computation time: 1.5306954383850098\n",
      "Step: 2518, Loss: 0.9160753488540649, Accuracy: 1.0, Computation time: 2.0817906856536865\n",
      "Step: 2519, Loss: 0.915946900844574, Accuracy: 1.0, Computation time: 1.7737202644348145\n",
      "Step: 2520, Loss: 0.9160083532333374, Accuracy: 1.0, Computation time: 1.7229983806610107\n",
      "Step: 2521, Loss: 0.9288474321365356, Accuracy: 0.96875, Computation time: 1.997591257095337\n",
      "Step: 2522, Loss: 0.915947675704956, Accuracy: 1.0, Computation time: 1.9567341804504395\n",
      "Step: 2523, Loss: 0.916276216506958, Accuracy: 1.0, Computation time: 1.9915642738342285\n",
      "Step: 2524, Loss: 0.9375720024108887, Accuracy: 0.96875, Computation time: 1.9022984504699707\n",
      "Step: 2525, Loss: 0.9365163445472717, Accuracy: 0.96875, Computation time: 1.4957301616668701\n",
      "Step: 2526, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 1.6787474155426025\n",
      "Step: 2527, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 1.618598461151123\n",
      "Step: 2528, Loss: 0.9159765839576721, Accuracy: 1.0, Computation time: 1.5581412315368652\n",
      "Step: 2529, Loss: 0.9528902173042297, Accuracy: 0.9375, Computation time: 1.6137189865112305\n",
      "Step: 2530, Loss: 0.9159486889839172, Accuracy: 1.0, Computation time: 1.5691559314727783\n",
      "Step: 2531, Loss: 0.935515820980072, Accuracy: 0.96875, Computation time: 1.944817066192627\n",
      "Step: 2532, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.730313777923584\n",
      "Step: 2533, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.5648572444915771\n",
      "Step: 2534, Loss: 0.9161462783813477, Accuracy: 1.0, Computation time: 1.7566745281219482\n",
      "Step: 2535, Loss: 0.9159606099128723, Accuracy: 1.0, Computation time: 1.4037857055664062\n",
      "Step: 2536, Loss: 0.9219315052032471, Accuracy: 1.0, Computation time: 1.9393274784088135\n",
      "Step: 2537, Loss: 0.9159975051879883, Accuracy: 1.0, Computation time: 1.4434549808502197\n",
      "Step: 2538, Loss: 0.9160952568054199, Accuracy: 1.0, Computation time: 2.004504919052124\n",
      "Step: 2539, Loss: 0.9163388013839722, Accuracy: 1.0, Computation time: 1.6700844764709473\n",
      "Step: 2540, Loss: 0.9165396690368652, Accuracy: 1.0, Computation time: 1.6148474216461182\n",
      "Step: 2541, Loss: 0.916117250919342, Accuracy: 1.0, Computation time: 1.3656411170959473\n",
      "Step: 2542, Loss: 0.9162654876708984, Accuracy: 1.0, Computation time: 1.534425973892212\n",
      "Step: 2543, Loss: 0.9159864783287048, Accuracy: 1.0, Computation time: 1.5723528861999512\n",
      "Step: 2544, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.3726963996887207\n",
      "Step: 2545, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.771005630493164\n",
      "Step: 2546, Loss: 0.9160214066505432, Accuracy: 1.0, Computation time: 1.8788073062896729\n",
      "Step: 2547, Loss: 0.9159865379333496, Accuracy: 1.0, Computation time: 1.7894039154052734\n",
      "Step: 2548, Loss: 0.9161329865455627, Accuracy: 1.0, Computation time: 1.745495080947876\n",
      "Step: 2549, Loss: 0.9346347451210022, Accuracy: 0.96875, Computation time: 1.5850918292999268\n",
      "Step: 2550, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.4793407917022705\n",
      "Step: 2551, Loss: 0.9278759956359863, Accuracy: 1.0, Computation time: 1.7311253547668457\n",
      "Step: 2552, Loss: 0.9159519672393799, Accuracy: 1.0, Computation time: 1.3243727684020996\n",
      "Step: 2553, Loss: 0.9161905646324158, Accuracy: 1.0, Computation time: 1.653412103652954\n",
      "Step: 2554, Loss: 0.9161708354949951, Accuracy: 1.0, Computation time: 2.0846595764160156\n",
      "Step: 2555, Loss: 0.916039764881134, Accuracy: 1.0, Computation time: 1.8388535976409912\n",
      "Step: 2556, Loss: 0.9364179372787476, Accuracy: 0.96875, Computation time: 2.0826735496520996\n",
      "Step: 2557, Loss: 0.915937066078186, Accuracy: 1.0, Computation time: 1.8186373710632324\n",
      "Step: 2558, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 1.838996171951294\n",
      "Step: 2559, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.9918982982635498\n",
      "Step: 2560, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.7047114372253418\n",
      "Step: 2561, Loss: 0.9159619808197021, Accuracy: 1.0, Computation time: 1.8883445262908936\n",
      "Step: 2562, Loss: 0.9160060286521912, Accuracy: 1.0, Computation time: 1.8114404678344727\n",
      "Step: 2563, Loss: 0.9432154893875122, Accuracy: 0.96875, Computation time: 1.7836823463439941\n",
      "Step: 2564, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 1.5122978687286377\n",
      "Step: 2565, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.6447420120239258\n",
      "Step: 2566, Loss: 0.9197832345962524, Accuracy: 1.0, Computation time: 2.2001054286956787\n",
      "Step: 2567, Loss: 0.9162459969520569, Accuracy: 1.0, Computation time: 1.578395128250122\n",
      "Step: 2568, Loss: 0.9160391092300415, Accuracy: 1.0, Computation time: 1.4146239757537842\n",
      "Step: 2569, Loss: 0.9379665851593018, Accuracy: 0.96875, Computation time: 1.427079677581787\n",
      "Step: 2570, Loss: 0.9159830808639526, Accuracy: 1.0, Computation time: 1.6973366737365723\n",
      "Step: 2571, Loss: 0.9477957487106323, Accuracy: 0.9375, Computation time: 1.947223424911499\n",
      "Step: 2572, Loss: 0.9160164594650269, Accuracy: 1.0, Computation time: 1.6786739826202393\n",
      "Step: 2573, Loss: 0.9164443016052246, Accuracy: 1.0, Computation time: 1.8081717491149902\n",
      "Step: 2574, Loss: 0.9161577224731445, Accuracy: 1.0, Computation time: 1.9639477729797363\n",
      "Step: 2575, Loss: 0.9362815618515015, Accuracy: 0.96875, Computation time: 2.612584352493286\n",
      "Step: 2576, Loss: 0.916046142578125, Accuracy: 1.0, Computation time: 1.79860258102417\n",
      "Step: 2577, Loss: 0.9375631809234619, Accuracy: 0.96875, Computation time: 2.1522204875946045\n",
      "Step: 2578, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 1.8847343921661377\n",
      "Step: 2579, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 1.6204729080200195\n",
      "Step: 2580, Loss: 0.9159569144248962, Accuracy: 1.0, Computation time: 1.717813491821289\n",
      "Step: 2581, Loss: 0.9178723692893982, Accuracy: 1.0, Computation time: 1.8171536922454834\n",
      "Step: 2582, Loss: 0.9187765717506409, Accuracy: 1.0, Computation time: 1.729494333267212\n",
      "Step: 2583, Loss: 0.9160459041595459, Accuracy: 1.0, Computation time: 1.7023870944976807\n",
      "Step: 2584, Loss: 0.9159870147705078, Accuracy: 1.0, Computation time: 1.8630261421203613\n",
      "Step: 2585, Loss: 0.9192566871643066, Accuracy: 1.0, Computation time: 2.6360721588134766\n",
      "Step: 2586, Loss: 0.9160160422325134, Accuracy: 1.0, Computation time: 1.5359816551208496\n",
      "Step: 2587, Loss: 0.916155219078064, Accuracy: 1.0, Computation time: 1.671860694885254\n",
      "Step: 2588, Loss: 0.9160751700401306, Accuracy: 1.0, Computation time: 1.3467836380004883\n",
      "Step: 2589, Loss: 0.9159944653511047, Accuracy: 1.0, Computation time: 1.6626901626586914\n",
      "Step: 2590, Loss: 0.9180838465690613, Accuracy: 1.0, Computation time: 1.6531484127044678\n",
      "Step: 2591, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.3978619575500488\n",
      "Step: 2592, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 1.5950889587402344\n",
      "Step: 2593, Loss: 0.9159871339797974, Accuracy: 1.0, Computation time: 1.8166189193725586\n",
      "Step: 2594, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.8640820980072021\n",
      "Step: 2595, Loss: 0.9161353707313538, Accuracy: 1.0, Computation time: 1.795125961303711\n",
      "Step: 2596, Loss: 0.9374884366989136, Accuracy: 0.96875, Computation time: 1.7002758979797363\n",
      "Step: 2597, Loss: 0.9161502718925476, Accuracy: 1.0, Computation time: 1.2479310035705566\n",
      "Step: 2598, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.8208363056182861\n",
      "Step: 2599, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.5219824314117432\n",
      "Step: 2600, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.4221525192260742\n",
      "Step: 2601, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 1.2030029296875\n",
      "Step: 2602, Loss: 0.915949285030365, Accuracy: 1.0, Computation time: 1.4245948791503906\n",
      "Step: 2603, Loss: 0.9591369032859802, Accuracy: 0.9375, Computation time: 1.4913558959960938\n",
      "Step: 2604, Loss: 0.916038990020752, Accuracy: 1.0, Computation time: 1.5590119361877441\n",
      "Step: 2605, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 1.6270275115966797\n",
      "Step: 2606, Loss: 0.9159414768218994, Accuracy: 1.0, Computation time: 1.595519781112671\n",
      "Step: 2607, Loss: 0.9353741407394409, Accuracy: 0.96875, Computation time: 1.656196117401123\n",
      "Step: 2608, Loss: 0.916260838508606, Accuracy: 1.0, Computation time: 2.190092086791992\n",
      "Step: 2609, Loss: 0.9159202575683594, Accuracy: 1.0, Computation time: 1.4590704441070557\n",
      "Step: 2610, Loss: 0.9161882996559143, Accuracy: 1.0, Computation time: 1.5933620929718018\n",
      "Step: 2611, Loss: 0.9229968786239624, Accuracy: 1.0, Computation time: 1.4934229850769043\n",
      "Step: 2612, Loss: 0.9376325011253357, Accuracy: 0.96875, Computation time: 1.4082517623901367\n",
      "Step: 2613, Loss: 0.9163159728050232, Accuracy: 1.0, Computation time: 1.4241163730621338\n",
      "Step: 2614, Loss: 0.91605144739151, Accuracy: 1.0, Computation time: 1.184523582458496\n",
      "Step: 2615, Loss: 0.9219043254852295, Accuracy: 1.0, Computation time: 1.207024335861206\n",
      "Step: 2616, Loss: 0.9160985350608826, Accuracy: 1.0, Computation time: 1.341069221496582\n",
      "Step: 2617, Loss: 0.9160411357879639, Accuracy: 1.0, Computation time: 1.275799036026001\n",
      "Step: 2618, Loss: 0.9160948395729065, Accuracy: 1.0, Computation time: 1.094167709350586\n",
      "Step: 2619, Loss: 0.9166609048843384, Accuracy: 1.0, Computation time: 1.6793203353881836\n",
      "Step: 2620, Loss: 0.9159314036369324, Accuracy: 1.0, Computation time: 1.1710927486419678\n",
      "Step: 2621, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.3441412448883057\n",
      "Step: 2622, Loss: 0.9377333521842957, Accuracy: 0.96875, Computation time: 1.1210451126098633\n",
      "Step: 2623, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.0988283157348633\n",
      "Step: 2624, Loss: 0.9159492254257202, Accuracy: 1.0, Computation time: 1.5007588863372803\n",
      "Step: 2625, Loss: 0.9159990549087524, Accuracy: 1.0, Computation time: 1.1649796962738037\n",
      "Step: 2626, Loss: 0.9159518480300903, Accuracy: 1.0, Computation time: 1.1027021408081055\n",
      "Step: 2627, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 1.0351598262786865\n",
      "Step: 2628, Loss: 0.9175518155097961, Accuracy: 1.0, Computation time: 1.0635900497436523\n",
      "Step: 2629, Loss: 0.9159981608390808, Accuracy: 1.0, Computation time: 1.3740565776824951\n",
      "Step: 2630, Loss: 0.9372729659080505, Accuracy: 0.96875, Computation time: 1.2627732753753662\n",
      "Step: 2631, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 1.2734248638153076\n",
      "Step: 2632, Loss: 0.9536316394805908, Accuracy: 0.9375, Computation time: 1.1656553745269775\n",
      "Step: 2633, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.2556393146514893\n",
      "Step: 2634, Loss: 0.9160205125808716, Accuracy: 1.0, Computation time: 1.1895592212677002\n",
      "Step: 2635, Loss: 0.9163689613342285, Accuracy: 1.0, Computation time: 1.0263190269470215\n",
      "Step: 2636, Loss: 0.9203250408172607, Accuracy: 1.0, Computation time: 1.1817827224731445\n",
      "Step: 2637, Loss: 0.9159564971923828, Accuracy: 1.0, Computation time: 1.3720448017120361\n",
      "Step: 2638, Loss: 0.9329445958137512, Accuracy: 0.96875, Computation time: 1.2599689960479736\n",
      "Step: 2639, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 1.1941392421722412\n",
      "Step: 2640, Loss: 0.9159322381019592, Accuracy: 1.0, Computation time: 0.9710876941680908\n",
      "########################\n",
      "Test loss: 1.0659677982330322, Test Accuracy_epoch19: 0.7817652821540833\n",
      "########################\n",
      "Step: 2641, Loss: 0.9161461591720581, Accuracy: 1.0, Computation time: 1.0325849056243896\n",
      "Step: 2642, Loss: 0.9393160939216614, Accuracy: 0.96875, Computation time: 1.894775390625\n",
      "Step: 2643, Loss: 0.9161174893379211, Accuracy: 1.0, Computation time: 1.1057636737823486\n",
      "Step: 2644, Loss: 0.9161674976348877, Accuracy: 1.0, Computation time: 1.8577237129211426\n",
      "Step: 2645, Loss: 0.916290819644928, Accuracy: 1.0, Computation time: 1.3690762519836426\n",
      "Step: 2646, Loss: 0.9161710143089294, Accuracy: 1.0, Computation time: 1.5989046096801758\n",
      "Step: 2647, Loss: 0.9160534143447876, Accuracy: 1.0, Computation time: 1.3961045742034912\n",
      "Step: 2648, Loss: 0.9159392714500427, Accuracy: 1.0, Computation time: 1.5564789772033691\n",
      "Step: 2649, Loss: 0.9173987507820129, Accuracy: 1.0, Computation time: 1.231076717376709\n",
      "Step: 2650, Loss: 0.9159742593765259, Accuracy: 1.0, Computation time: 1.4988079071044922\n",
      "Step: 2651, Loss: 0.9377508163452148, Accuracy: 0.96875, Computation time: 1.5790517330169678\n",
      "Step: 2652, Loss: 0.9161099195480347, Accuracy: 1.0, Computation time: 1.5999729633331299\n",
      "Step: 2653, Loss: 0.916056752204895, Accuracy: 1.0, Computation time: 1.681485652923584\n",
      "Step: 2654, Loss: 0.915988564491272, Accuracy: 1.0, Computation time: 1.6640064716339111\n",
      "Step: 2655, Loss: 0.9160011410713196, Accuracy: 1.0, Computation time: 1.5014455318450928\n",
      "Step: 2656, Loss: 0.9347814321517944, Accuracy: 0.96875, Computation time: 1.7204320430755615\n",
      "Step: 2657, Loss: 0.9159346222877502, Accuracy: 1.0, Computation time: 1.6251745223999023\n",
      "Step: 2658, Loss: 0.9159608483314514, Accuracy: 1.0, Computation time: 1.6943206787109375\n",
      "Step: 2659, Loss: 0.9160161018371582, Accuracy: 1.0, Computation time: 1.2954308986663818\n",
      "Step: 2660, Loss: 0.9159511923789978, Accuracy: 1.0, Computation time: 1.270752191543579\n",
      "Step: 2661, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.4985754489898682\n",
      "Step: 2662, Loss: 0.9159787893295288, Accuracy: 1.0, Computation time: 1.8879296779632568\n",
      "Step: 2663, Loss: 0.9380834102630615, Accuracy: 0.96875, Computation time: 1.634281873703003\n",
      "Step: 2664, Loss: 0.9162072539329529, Accuracy: 1.0, Computation time: 1.5161118507385254\n",
      "Step: 2665, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.4009065628051758\n",
      "Step: 2666, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 1.663102626800537\n",
      "Step: 2667, Loss: 0.9373843669891357, Accuracy: 0.96875, Computation time: 1.4177241325378418\n",
      "Step: 2668, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.3312599658966064\n",
      "Step: 2669, Loss: 0.9161651134490967, Accuracy: 1.0, Computation time: 1.73130202293396\n",
      "Step: 2670, Loss: 0.9376125931739807, Accuracy: 0.96875, Computation time: 1.6332817077636719\n",
      "Step: 2671, Loss: 0.9175759553909302, Accuracy: 1.0, Computation time: 1.5711276531219482\n",
      "Step: 2672, Loss: 0.9162320494651794, Accuracy: 1.0, Computation time: 2.1376538276672363\n",
      "Step: 2673, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.389883279800415\n",
      "Step: 2674, Loss: 0.9159300327301025, Accuracy: 1.0, Computation time: 1.7330586910247803\n",
      "Step: 2675, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 1.6474788188934326\n",
      "Step: 2676, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.297659158706665\n",
      "Step: 2677, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.5766913890838623\n",
      "Step: 2678, Loss: 0.9375989437103271, Accuracy: 0.96875, Computation time: 1.706620216369629\n",
      "Step: 2679, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.391894817352295\n",
      "Step: 2680, Loss: 0.9360454082489014, Accuracy: 0.96875, Computation time: 1.1938939094543457\n",
      "Step: 2681, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.5968806743621826\n",
      "Step: 2682, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 1.3809709548950195\n",
      "Step: 2683, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.364236831665039\n",
      "Step: 2684, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 1.8236374855041504\n",
      "Step: 2685, Loss: 0.9270389080047607, Accuracy: 0.96875, Computation time: 1.551802396774292\n",
      "Step: 2686, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.7783584594726562\n",
      "Step: 2687, Loss: 0.9215834140777588, Accuracy: 1.0, Computation time: 1.737778663635254\n",
      "Step: 2688, Loss: 0.9160307049751282, Accuracy: 1.0, Computation time: 1.4444365501403809\n",
      "Step: 2689, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 1.4673850536346436\n",
      "Step: 2690, Loss: 0.9160081744194031, Accuracy: 1.0, Computation time: 1.6809532642364502\n",
      "Step: 2691, Loss: 0.9160729646682739, Accuracy: 1.0, Computation time: 1.853234052658081\n",
      "Step: 2692, Loss: 0.9161185622215271, Accuracy: 1.0, Computation time: 1.4375526905059814\n",
      "Step: 2693, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.3056285381317139\n",
      "Step: 2694, Loss: 0.9159218668937683, Accuracy: 1.0, Computation time: 1.5487375259399414\n",
      "Step: 2695, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.532374382019043\n",
      "Step: 2696, Loss: 0.9162327647209167, Accuracy: 1.0, Computation time: 1.7964980602264404\n",
      "Step: 2697, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.6038661003112793\n",
      "Step: 2698, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 1.4382827281951904\n",
      "Step: 2699, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 1.870683193206787\n",
      "Step: 2700, Loss: 0.9159794449806213, Accuracy: 1.0, Computation time: 1.715029001235962\n",
      "Step: 2701, Loss: 0.9366381168365479, Accuracy: 0.96875, Computation time: 1.4289720058441162\n",
      "Step: 2702, Loss: 0.9160516262054443, Accuracy: 1.0, Computation time: 1.8440768718719482\n",
      "Step: 2703, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.762310266494751\n",
      "Step: 2704, Loss: 0.9159396290779114, Accuracy: 1.0, Computation time: 2.275437593460083\n",
      "Step: 2705, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.430579662322998\n",
      "Step: 2706, Loss: 0.9371817111968994, Accuracy: 0.96875, Computation time: 2.0469558238983154\n",
      "Step: 2707, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 2.0682835578918457\n",
      "Step: 2708, Loss: 0.9159714579582214, Accuracy: 1.0, Computation time: 1.7649214267730713\n",
      "Step: 2709, Loss: 0.916113555431366, Accuracy: 1.0, Computation time: 2.296602487564087\n",
      "Step: 2710, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 2.1394801139831543\n",
      "Step: 2711, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 2.0868852138519287\n",
      "Step: 2712, Loss: 0.9375777840614319, Accuracy: 0.96875, Computation time: 2.0054752826690674\n",
      "Step: 2713, Loss: 0.9371660947799683, Accuracy: 0.96875, Computation time: 1.4974591732025146\n",
      "Step: 2714, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.6004536151885986\n",
      "Step: 2715, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.2489378452301025\n",
      "Step: 2716, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.6861610412597656\n",
      "Step: 2717, Loss: 0.9211533665657043, Accuracy: 1.0, Computation time: 1.303863286972046\n",
      "Step: 2718, Loss: 0.9333173632621765, Accuracy: 0.96875, Computation time: 1.786686658859253\n",
      "Step: 2719, Loss: 0.9165850877761841, Accuracy: 1.0, Computation time: 2.047063112258911\n",
      "Step: 2720, Loss: 0.91606205701828, Accuracy: 1.0, Computation time: 1.4238879680633545\n",
      "Step: 2721, Loss: 0.91600102186203, Accuracy: 1.0, Computation time: 1.722904920578003\n",
      "Step: 2722, Loss: 0.9160392880439758, Accuracy: 1.0, Computation time: 1.428060531616211\n",
      "Step: 2723, Loss: 0.9159786701202393, Accuracy: 1.0, Computation time: 1.5575757026672363\n",
      "Step: 2724, Loss: 0.9160425066947937, Accuracy: 1.0, Computation time: 1.4647879600524902\n",
      "Step: 2725, Loss: 0.9169240593910217, Accuracy: 1.0, Computation time: 1.6217536926269531\n",
      "Step: 2726, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 1.6268112659454346\n",
      "Step: 2727, Loss: 0.9170144200325012, Accuracy: 1.0, Computation time: 1.6849637031555176\n",
      "Step: 2728, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.527517318725586\n",
      "Step: 2729, Loss: 0.916233241558075, Accuracy: 1.0, Computation time: 1.8744497299194336\n",
      "Step: 2730, Loss: 0.9161168336868286, Accuracy: 1.0, Computation time: 1.4458105564117432\n",
      "Step: 2731, Loss: 0.916006326675415, Accuracy: 1.0, Computation time: 1.6860861778259277\n",
      "Step: 2732, Loss: 0.9160507917404175, Accuracy: 1.0, Computation time: 1.9208745956420898\n",
      "Step: 2733, Loss: 0.915920078754425, Accuracy: 1.0, Computation time: 1.4303386211395264\n",
      "Step: 2734, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.8875353336334229\n",
      "Step: 2735, Loss: 0.9187800288200378, Accuracy: 1.0, Computation time: 1.883793592453003\n",
      "Step: 2736, Loss: 0.9159435033798218, Accuracy: 1.0, Computation time: 1.982466459274292\n",
      "Step: 2737, Loss: 0.9188880920410156, Accuracy: 1.0, Computation time: 2.618957996368408\n",
      "Step: 2738, Loss: 0.9378003478050232, Accuracy: 0.96875, Computation time: 1.6236484050750732\n",
      "Step: 2739, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.5922656059265137\n",
      "Step: 2740, Loss: 0.9162200093269348, Accuracy: 1.0, Computation time: 1.6182985305786133\n",
      "Step: 2741, Loss: 0.9191083312034607, Accuracy: 1.0, Computation time: 2.0564277172088623\n",
      "Step: 2742, Loss: 0.9159769415855408, Accuracy: 1.0, Computation time: 1.777297019958496\n",
      "Step: 2743, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 1.555678129196167\n",
      "Step: 2744, Loss: 0.915903627872467, Accuracy: 1.0, Computation time: 1.3887908458709717\n",
      "Step: 2745, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.3287124633789062\n",
      "Step: 2746, Loss: 0.9158914685249329, Accuracy: 1.0, Computation time: 1.3740730285644531\n",
      "Step: 2747, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.341005563735962\n",
      "Step: 2748, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.3316185474395752\n",
      "Step: 2749, Loss: 0.9160359501838684, Accuracy: 1.0, Computation time: 1.4890167713165283\n",
      "Step: 2750, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.3197507858276367\n",
      "Step: 2751, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.5534727573394775\n",
      "Step: 2752, Loss: 0.9374717473983765, Accuracy: 0.96875, Computation time: 1.3234288692474365\n",
      "Step: 2753, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 1.3640642166137695\n",
      "Step: 2754, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.3087098598480225\n",
      "Step: 2755, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.289964199066162\n",
      "Step: 2756, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.404510736465454\n",
      "Step: 2757, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 1.3429806232452393\n",
      "Step: 2758, Loss: 0.9572440981864929, Accuracy: 0.9375, Computation time: 1.7883267402648926\n",
      "Step: 2759, Loss: 0.9375885725021362, Accuracy: 0.96875, Computation time: 1.3379695415496826\n",
      "Step: 2760, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.4062986373901367\n",
      "Step: 2761, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.3497400283813477\n",
      "Step: 2762, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.2053050994873047\n",
      "Step: 2763, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.2414615154266357\n",
      "Step: 2764, Loss: 0.9160256385803223, Accuracy: 1.0, Computation time: 1.5342292785644531\n",
      "Step: 2765, Loss: 0.9161548018455505, Accuracy: 1.0, Computation time: 1.4869134426116943\n",
      "Step: 2766, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.4218552112579346\n",
      "Step: 2767, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.1905992031097412\n",
      "Step: 2768, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.243415117263794\n",
      "Step: 2769, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.1060094833374023\n",
      "Step: 2770, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.2072224617004395\n",
      "Step: 2771, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.288445234298706\n",
      "Step: 2772, Loss: 0.9375559091567993, Accuracy: 0.96875, Computation time: 1.7261016368865967\n",
      "Step: 2773, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.6407792568206787\n",
      "Step: 2774, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.2064244747161865\n",
      "Step: 2775, Loss: 0.9164379239082336, Accuracy: 1.0, Computation time: 2.109004497528076\n",
      "Step: 2776, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.2394013404846191\n",
      "Step: 2777, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.1408474445343018\n",
      "Step: 2778, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.391584873199463\n",
      "Step: 2779, Loss: 0.9162145256996155, Accuracy: 1.0, Computation time: 1.5508770942687988\n",
      "########################\n",
      "Test loss: 1.070510983467102, Test Accuracy_epoch20: 0.7769156098365784\n",
      "########################\n",
      "Step: 2780, Loss: 0.9182448387145996, Accuracy: 1.0, Computation time: 1.2404656410217285\n",
      "Step: 2781, Loss: 0.9375055432319641, Accuracy: 0.96875, Computation time: 1.295839786529541\n",
      "Step: 2782, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.3206276893615723\n",
      "Step: 2783, Loss: 0.9244506359100342, Accuracy: 1.0, Computation time: 1.5378212928771973\n",
      "Step: 2784, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 1.2826991081237793\n",
      "Step: 2785, Loss: 0.9375834465026855, Accuracy: 0.96875, Computation time: 1.2508175373077393\n",
      "Step: 2786, Loss: 0.9165898561477661, Accuracy: 1.0, Computation time: 1.325138807296753\n",
      "Step: 2787, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.4068360328674316\n",
      "Step: 2788, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.396202802658081\n",
      "Step: 2789, Loss: 0.9163146615028381, Accuracy: 1.0, Computation time: 1.6206858158111572\n",
      "Step: 2790, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.4524612426757812\n",
      "Step: 2791, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.2193751335144043\n",
      "Step: 2792, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.3312537670135498\n",
      "Step: 2793, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 1.6648774147033691\n",
      "Step: 2794, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.2218313217163086\n",
      "Step: 2795, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.2112009525299072\n",
      "Step: 2796, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 1.2365107536315918\n",
      "Step: 2797, Loss: 0.915984570980072, Accuracy: 1.0, Computation time: 1.2306432723999023\n",
      "Step: 2798, Loss: 0.9166472554206848, Accuracy: 1.0, Computation time: 1.6611418724060059\n",
      "Step: 2799, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.7248916625976562\n",
      "Step: 2800, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.5882723331451416\n",
      "Step: 2801, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.141035556793213\n",
      "Step: 2802, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.2996697425842285\n",
      "Step: 2803, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.061011552810669\n",
      "Step: 2804, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.2223515510559082\n",
      "Step: 2805, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.009847640991211\n",
      "Step: 2806, Loss: 0.9160104393959045, Accuracy: 1.0, Computation time: 1.598876953125\n",
      "Step: 2807, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.1118290424346924\n",
      "Step: 2808, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.1772208213806152\n",
      "Step: 2809, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.3108863830566406\n",
      "Step: 2810, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 1.4538443088531494\n",
      "Step: 2811, Loss: 0.9371078014373779, Accuracy: 0.96875, Computation time: 1.2496356964111328\n",
      "Step: 2812, Loss: 0.9201995730400085, Accuracy: 1.0, Computation time: 2.116990089416504\n",
      "Step: 2813, Loss: 0.9385930299758911, Accuracy: 0.96875, Computation time: 1.1593518257141113\n",
      "Step: 2814, Loss: 0.9159702062606812, Accuracy: 1.0, Computation time: 1.4452202320098877\n",
      "Step: 2815, Loss: 0.9161573648452759, Accuracy: 1.0, Computation time: 1.2666387557983398\n",
      "Step: 2816, Loss: 0.9377894401550293, Accuracy: 0.96875, Computation time: 1.3904149532318115\n",
      "Step: 2817, Loss: 0.916277289390564, Accuracy: 1.0, Computation time: 1.165980577468872\n",
      "Step: 2818, Loss: 0.9162313342094421, Accuracy: 1.0, Computation time: 1.051579236984253\n",
      "Step: 2819, Loss: 0.938970685005188, Accuracy: 0.96875, Computation time: 1.4686310291290283\n",
      "Step: 2820, Loss: 0.9161264896392822, Accuracy: 1.0, Computation time: 1.4695727825164795\n",
      "Step: 2821, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.107184886932373\n",
      "Step: 2822, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 1.361109733581543\n",
      "Step: 2823, Loss: 0.937563419342041, Accuracy: 0.96875, Computation time: 1.3168847560882568\n",
      "Step: 2824, Loss: 0.9170122146606445, Accuracy: 1.0, Computation time: 1.6998252868652344\n",
      "Step: 2825, Loss: 0.9160894155502319, Accuracy: 1.0, Computation time: 1.590963363647461\n",
      "Step: 2826, Loss: 0.9161595702171326, Accuracy: 1.0, Computation time: 1.1628940105438232\n",
      "Step: 2827, Loss: 0.9183751940727234, Accuracy: 1.0, Computation time: 1.3550987243652344\n",
      "Step: 2828, Loss: 0.9162930846214294, Accuracy: 1.0, Computation time: 1.148571491241455\n",
      "Step: 2829, Loss: 0.9162012338638306, Accuracy: 1.0, Computation time: 2.0167617797851562\n",
      "Step: 2830, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 1.0844447612762451\n",
      "Step: 2831, Loss: 0.9159416556358337, Accuracy: 1.0, Computation time: 1.0242140293121338\n",
      "Step: 2832, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.1735141277313232\n",
      "Step: 2833, Loss: 0.9160617589950562, Accuracy: 1.0, Computation time: 1.077059030532837\n",
      "Step: 2834, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.034616470336914\n",
      "Step: 2835, Loss: 0.9159726500511169, Accuracy: 1.0, Computation time: 1.4936556816101074\n",
      "Step: 2836, Loss: 0.9371119141578674, Accuracy: 0.96875, Computation time: 1.1398756504058838\n",
      "Step: 2837, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.3153126239776611\n",
      "Step: 2838, Loss: 0.9159474968910217, Accuracy: 1.0, Computation time: 1.2776191234588623\n",
      "Step: 2839, Loss: 0.9159398674964905, Accuracy: 1.0, Computation time: 1.1812758445739746\n",
      "Step: 2840, Loss: 0.9159700274467468, Accuracy: 1.0, Computation time: 1.253990888595581\n",
      "Step: 2841, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.1551513671875\n",
      "Step: 2842, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 1.319180965423584\n",
      "Step: 2843, Loss: 0.9165349006652832, Accuracy: 1.0, Computation time: 1.2973618507385254\n",
      "Step: 2844, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.351203441619873\n",
      "Step: 2845, Loss: 0.9159835577011108, Accuracy: 1.0, Computation time: 1.1383605003356934\n",
      "Step: 2846, Loss: 0.9159332513809204, Accuracy: 1.0, Computation time: 1.1534302234649658\n",
      "Step: 2847, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.0445690155029297\n",
      "Step: 2848, Loss: 0.932241678237915, Accuracy: 0.96875, Computation time: 1.285278081893921\n",
      "Step: 2849, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.1132879257202148\n",
      "Step: 2850, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.0658555030822754\n",
      "Step: 2851, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.2399623394012451\n",
      "Step: 2852, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.3533260822296143\n",
      "Step: 2853, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.022439956665039\n",
      "Step: 2854, Loss: 0.9159327745437622, Accuracy: 1.0, Computation time: 1.2912414073944092\n",
      "Step: 2855, Loss: 0.9159510135650635, Accuracy: 1.0, Computation time: 1.2600934505462646\n",
      "Step: 2856, Loss: 0.9159380197525024, Accuracy: 1.0, Computation time: 1.0695624351501465\n",
      "Step: 2857, Loss: 0.9325754046440125, Accuracy: 0.96875, Computation time: 1.2201426029205322\n",
      "Step: 2858, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.1860721111297607\n",
      "Step: 2859, Loss: 0.9185069799423218, Accuracy: 1.0, Computation time: 1.280602216720581\n",
      "Step: 2860, Loss: 0.9162048697471619, Accuracy: 1.0, Computation time: 1.4756829738616943\n",
      "Step: 2861, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 1.1001245975494385\n",
      "Step: 2862, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 1.2788140773773193\n",
      "Step: 2863, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.164478063583374\n",
      "Step: 2864, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.1428711414337158\n",
      "Step: 2865, Loss: 0.9380825161933899, Accuracy: 0.96875, Computation time: 1.0405571460723877\n",
      "Step: 2866, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 1.1403589248657227\n",
      "Step: 2867, Loss: 0.9374610185623169, Accuracy: 0.96875, Computation time: 2.1269068717956543\n",
      "Step: 2868, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.443469524383545\n",
      "Step: 2869, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.488175630569458\n",
      "Step: 2870, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.078289270401001\n",
      "Step: 2871, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.2549149990081787\n",
      "Step: 2872, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 1.1827125549316406\n",
      "Step: 2873, Loss: 0.9375745058059692, Accuracy: 0.96875, Computation time: 1.5783772468566895\n",
      "Step: 2874, Loss: 0.9371722340583801, Accuracy: 0.96875, Computation time: 1.169093132019043\n",
      "Step: 2875, Loss: 0.9372949004173279, Accuracy: 0.96875, Computation time: 1.0183374881744385\n",
      "Step: 2876, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.1869187355041504\n",
      "Step: 2877, Loss: 0.9162998795509338, Accuracy: 1.0, Computation time: 1.098259449005127\n",
      "Step: 2878, Loss: 0.9159787893295288, Accuracy: 1.0, Computation time: 1.2369489669799805\n",
      "Step: 2879, Loss: 0.9371462464332581, Accuracy: 0.96875, Computation time: 1.0386834144592285\n",
      "Step: 2880, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.2706727981567383\n",
      "Step: 2881, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.024223804473877\n",
      "Step: 2882, Loss: 0.916781485080719, Accuracy: 1.0, Computation time: 1.1379234790802002\n",
      "Step: 2883, Loss: 0.9221426844596863, Accuracy: 1.0, Computation time: 1.6195547580718994\n",
      "Step: 2884, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.2909703254699707\n",
      "Step: 2885, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.0626049041748047\n",
      "Step: 2886, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.3429620265960693\n",
      "Step: 2887, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.2408342361450195\n",
      "Step: 2888, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 1.4039134979248047\n",
      "Step: 2889, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 1.1313068866729736\n",
      "Step: 2890, Loss: 0.9373670220375061, Accuracy: 0.96875, Computation time: 1.1895062923431396\n",
      "Step: 2891, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.396855354309082\n",
      "Step: 2892, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.1518239974975586\n",
      "Step: 2893, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 1.1169624328613281\n",
      "Step: 2894, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.8900761604309082\n",
      "Step: 2895, Loss: 0.9159267544746399, Accuracy: 1.0, Computation time: 1.5105621814727783\n",
      "Step: 2896, Loss: 0.9418178796768188, Accuracy: 0.96875, Computation time: 1.497541904449463\n",
      "Step: 2897, Loss: 0.9159628748893738, Accuracy: 1.0, Computation time: 1.3528826236724854\n",
      "Step: 2898, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 1.558642864227295\n",
      "Step: 2899, Loss: 0.9377259612083435, Accuracy: 0.96875, Computation time: 1.5708537101745605\n",
      "Step: 2900, Loss: 0.9159411787986755, Accuracy: 1.0, Computation time: 1.6192853450775146\n",
      "Step: 2901, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 1.2305126190185547\n",
      "Step: 2902, Loss: 0.9258394837379456, Accuracy: 0.96875, Computation time: 1.714524507522583\n",
      "Step: 2903, Loss: 0.9160194993019104, Accuracy: 1.0, Computation time: 1.3134346008300781\n",
      "Step: 2904, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.4121088981628418\n",
      "Step: 2905, Loss: 0.9388623237609863, Accuracy: 0.96875, Computation time: 1.745054006576538\n",
      "Step: 2906, Loss: 0.9159530997276306, Accuracy: 1.0, Computation time: 1.7761948108673096\n",
      "Step: 2907, Loss: 0.9404886960983276, Accuracy: 0.96875, Computation time: 2.208174228668213\n",
      "Step: 2908, Loss: 0.9381217360496521, Accuracy: 0.96875, Computation time: 1.9972171783447266\n",
      "Step: 2909, Loss: 0.9160670042037964, Accuracy: 1.0, Computation time: 1.2984836101531982\n",
      "Step: 2910, Loss: 0.916777491569519, Accuracy: 1.0, Computation time: 1.5695466995239258\n",
      "Step: 2911, Loss: 0.9163165092468262, Accuracy: 1.0, Computation time: 1.2433669567108154\n",
      "Step: 2912, Loss: 0.9163869619369507, Accuracy: 1.0, Computation time: 1.6325736045837402\n",
      "Step: 2913, Loss: 0.9167550802230835, Accuracy: 1.0, Computation time: 1.7530717849731445\n",
      "Step: 2914, Loss: 0.9161703586578369, Accuracy: 1.0, Computation time: 1.4283034801483154\n",
      "Step: 2915, Loss: 0.9160898327827454, Accuracy: 1.0, Computation time: 1.24373459815979\n",
      "Step: 2916, Loss: 0.9159293174743652, Accuracy: 1.0, Computation time: 1.16733980178833\n",
      "Step: 2917, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.484727382659912\n",
      "Step: 2918, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.186880350112915\n",
      "########################\n",
      "Test loss: 1.0673872232437134, Test Accuracy_epoch21: 0.7798254489898682\n",
      "########################\n",
      "Step: 2919, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 1.562204360961914\n",
      "Step: 2920, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 2.1331787109375\n",
      "Step: 2921, Loss: 0.9373360276222229, Accuracy: 0.96875, Computation time: 1.5761451721191406\n",
      "Step: 2922, Loss: 0.9161359071731567, Accuracy: 1.0, Computation time: 1.312410831451416\n",
      "Step: 2923, Loss: 0.9159681797027588, Accuracy: 1.0, Computation time: 1.3896892070770264\n",
      "Step: 2924, Loss: 0.9160311222076416, Accuracy: 1.0, Computation time: 1.8792333602905273\n",
      "Step: 2925, Loss: 0.9159861207008362, Accuracy: 1.0, Computation time: 1.6904683113098145\n",
      "Step: 2926, Loss: 0.9160099625587463, Accuracy: 1.0, Computation time: 1.859090805053711\n",
      "Step: 2927, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 1.3332538604736328\n",
      "Step: 2928, Loss: 0.9594956636428833, Accuracy: 0.9375, Computation time: 1.8649873733520508\n",
      "Step: 2929, Loss: 0.9165323376655579, Accuracy: 1.0, Computation time: 1.3259384632110596\n",
      "Step: 2930, Loss: 0.9375008344650269, Accuracy: 0.96875, Computation time: 1.2665681838989258\n",
      "Step: 2931, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.2982664108276367\n",
      "Step: 2932, Loss: 0.9375904202461243, Accuracy: 0.96875, Computation time: 1.1754794120788574\n",
      "Step: 2933, Loss: 0.9159538149833679, Accuracy: 1.0, Computation time: 1.2773380279541016\n",
      "Step: 2934, Loss: 0.9208928942680359, Accuracy: 1.0, Computation time: 1.5140163898468018\n",
      "Step: 2935, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.297724962234497\n",
      "Step: 2936, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.3641140460968018\n",
      "Step: 2937, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.4213230609893799\n",
      "Step: 2938, Loss: 0.9337784051895142, Accuracy: 0.96875, Computation time: 2.0777571201324463\n",
      "Step: 2939, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.338425874710083\n",
      "Step: 2940, Loss: 0.9160457253456116, Accuracy: 1.0, Computation time: 1.4504542350769043\n",
      "Step: 2941, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 1.267848253250122\n",
      "Step: 2942, Loss: 0.9161758422851562, Accuracy: 1.0, Computation time: 1.989384651184082\n",
      "Step: 2943, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.4556596279144287\n",
      "Step: 2944, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.6202192306518555\n",
      "Step: 2945, Loss: 0.935916543006897, Accuracy: 0.96875, Computation time: 1.5374062061309814\n",
      "Step: 2946, Loss: 0.9159826040267944, Accuracy: 1.0, Computation time: 1.357046127319336\n",
      "Step: 2947, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.470935344696045\n",
      "Step: 2948, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 1.3179261684417725\n",
      "Step: 2949, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.3950865268707275\n",
      "Step: 2950, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.628169298171997\n",
      "Step: 2951, Loss: 0.9160890579223633, Accuracy: 1.0, Computation time: 1.6548378467559814\n",
      "Step: 2952, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 1.420677661895752\n",
      "Step: 2953, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.8330888748168945\n",
      "Step: 2954, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.913036823272705\n",
      "Step: 2955, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.8053245544433594\n",
      "Step: 2956, Loss: 0.9487747550010681, Accuracy: 0.9375, Computation time: 1.5828514099121094\n",
      "Step: 2957, Loss: 0.9161057472229004, Accuracy: 1.0, Computation time: 1.6385929584503174\n",
      "Step: 2958, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 1.397756576538086\n",
      "Step: 2959, Loss: 0.9173834919929504, Accuracy: 1.0, Computation time: 1.8286967277526855\n",
      "Step: 2960, Loss: 0.9191548228263855, Accuracy: 1.0, Computation time: 2.2572858333587646\n",
      "Step: 2961, Loss: 0.9173202514648438, Accuracy: 1.0, Computation time: 1.815950870513916\n",
      "Step: 2962, Loss: 0.9159961342811584, Accuracy: 1.0, Computation time: 1.8007583618164062\n",
      "Step: 2963, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.6940264701843262\n",
      "Step: 2964, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.6593492031097412\n",
      "Step: 2965, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.2491180896759033\n",
      "Step: 2966, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.706653356552124\n",
      "Step: 2967, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 2.2543349266052246\n",
      "Step: 2968, Loss: 0.9160515666007996, Accuracy: 1.0, Computation time: 1.7960708141326904\n",
      "Step: 2969, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.2744767665863037\n",
      "Step: 2970, Loss: 0.9363564252853394, Accuracy: 0.96875, Computation time: 1.7260503768920898\n",
      "Step: 2971, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.5855588912963867\n",
      "Step: 2972, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 1.7768418788909912\n",
      "Step: 2973, Loss: 0.9376124143600464, Accuracy: 0.96875, Computation time: 1.2748465538024902\n",
      "Step: 2974, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.600299596786499\n",
      "Step: 2975, Loss: 0.9158852696418762, Accuracy: 1.0, Computation time: 1.4098331928253174\n",
      "Step: 2976, Loss: 0.937371551990509, Accuracy: 0.96875, Computation time: 2.165943145751953\n",
      "Step: 2977, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.2240939140319824\n",
      "Step: 2978, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.2441844940185547\n",
      "Step: 2979, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.5848991870880127\n",
      "Step: 2980, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.4127137660980225\n",
      "Step: 2981, Loss: 0.9163838624954224, Accuracy: 1.0, Computation time: 1.7282178401947021\n",
      "Step: 2982, Loss: 0.937606155872345, Accuracy: 0.96875, Computation time: 1.3548572063446045\n",
      "Step: 2983, Loss: 0.9375699162483215, Accuracy: 0.96875, Computation time: 1.4256675243377686\n",
      "Step: 2984, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 1.2034378051757812\n",
      "Step: 2985, Loss: 0.9590870141983032, Accuracy: 0.9375, Computation time: 1.3984510898590088\n",
      "Step: 2986, Loss: 0.9174033999443054, Accuracy: 1.0, Computation time: 1.6712257862091064\n",
      "Step: 2987, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 1.4746053218841553\n",
      "Step: 2988, Loss: 0.9158987402915955, Accuracy: 1.0, Computation time: 1.5100655555725098\n",
      "Step: 2989, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 1.5135715007781982\n",
      "Step: 2990, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.3298985958099365\n",
      "Step: 2991, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 1.501934289932251\n",
      "Step: 2992, Loss: 0.9165489077568054, Accuracy: 1.0, Computation time: 1.248750925064087\n",
      "Step: 2993, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 1.6681783199310303\n",
      "Step: 2994, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 1.1733379364013672\n",
      "Step: 2995, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 2.020908832550049\n",
      "Step: 2996, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.3592336177825928\n",
      "Step: 2997, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.1604094505310059\n",
      "Step: 2998, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.699641466140747\n",
      "Step: 2999, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.8630824089050293\n",
      "Step: 3000, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.1985313892364502\n",
      "Step: 3001, Loss: 0.9158360362052917, Accuracy: 1.0, Computation time: 1.0082316398620605\n",
      "Step: 3002, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.6077451705932617\n",
      "Step: 3003, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.3877248764038086\n",
      "Step: 3004, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 1.1794261932373047\n",
      "Step: 3005, Loss: 0.915972888469696, Accuracy: 1.0, Computation time: 1.8233022689819336\n",
      "Step: 3006, Loss: 0.9165867567062378, Accuracy: 1.0, Computation time: 1.7448923587799072\n",
      "Step: 3007, Loss: 0.9166789650917053, Accuracy: 1.0, Computation time: 1.668858528137207\n",
      "Step: 3008, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.2216811180114746\n",
      "Step: 3009, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.2470319271087646\n",
      "Step: 3010, Loss: 0.9375459551811218, Accuracy: 0.96875, Computation time: 1.1994750499725342\n",
      "Step: 3011, Loss: 0.9189378619194031, Accuracy: 1.0, Computation time: 1.93058180809021\n",
      "Step: 3012, Loss: 0.9159342646598816, Accuracy: 1.0, Computation time: 1.0066990852355957\n",
      "Step: 3013, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 1.4042725563049316\n",
      "Step: 3014, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.8072481155395508\n",
      "Step: 3015, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.2925801277160645\n",
      "Step: 3016, Loss: 0.9177793264389038, Accuracy: 1.0, Computation time: 1.8746201992034912\n",
      "Step: 3017, Loss: 0.9521219730377197, Accuracy: 0.9375, Computation time: 1.6780166625976562\n",
      "Step: 3018, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.676527500152588\n",
      "Step: 3019, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 1.2951958179473877\n",
      "Step: 3020, Loss: 0.9159414768218994, Accuracy: 1.0, Computation time: 1.5264899730682373\n",
      "Step: 3021, Loss: 0.9159151911735535, Accuracy: 1.0, Computation time: 1.8600428104400635\n",
      "Step: 3022, Loss: 0.9159725904464722, Accuracy: 1.0, Computation time: 1.7746074199676514\n",
      "Step: 3023, Loss: 0.9376364350318909, Accuracy: 0.96875, Computation time: 1.6707603931427002\n",
      "Step: 3024, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.455380916595459\n",
      "Step: 3025, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 1.7347915172576904\n",
      "Step: 3026, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.537895679473877\n",
      "Step: 3027, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 1.249328851699829\n",
      "Step: 3028, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.6224908828735352\n",
      "Step: 3029, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 1.4381003379821777\n",
      "Step: 3030, Loss: 0.9159623980522156, Accuracy: 1.0, Computation time: 1.3773674964904785\n",
      "Step: 3031, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.4184257984161377\n",
      "Step: 3032, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 1.2336773872375488\n",
      "Step: 3033, Loss: 0.9160302877426147, Accuracy: 1.0, Computation time: 1.7352325916290283\n",
      "Step: 3034, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.5573785305023193\n",
      "Step: 3035, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.3132107257843018\n",
      "Step: 3036, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.3584051132202148\n",
      "Step: 3037, Loss: 0.9376345276832581, Accuracy: 0.96875, Computation time: 1.3946316242218018\n",
      "Step: 3038, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.2169933319091797\n",
      "Step: 3039, Loss: 0.916247546672821, Accuracy: 1.0, Computation time: 1.7688205242156982\n",
      "Step: 3040, Loss: 0.9375076293945312, Accuracy: 0.96875, Computation time: 1.559528112411499\n",
      "Step: 3041, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.37642240524292\n",
      "Step: 3042, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.5717658996582031\n",
      "Step: 3043, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.6039535999298096\n",
      "Step: 3044, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 1.6758599281311035\n",
      "Step: 3045, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.6450321674346924\n",
      "Step: 3046, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 1.4247689247131348\n",
      "Step: 3047, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.6610519886016846\n",
      "Step: 3048, Loss: 0.9369276762008667, Accuracy: 0.96875, Computation time: 1.2034740447998047\n",
      "Step: 3049, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.671579360961914\n",
      "Step: 3050, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.454188346862793\n",
      "Step: 3051, Loss: 0.9159160852432251, Accuracy: 1.0, Computation time: 1.7855117321014404\n",
      "Step: 3052, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.5143742561340332\n",
      "Step: 3053, Loss: 0.916027307510376, Accuracy: 1.0, Computation time: 1.972447156906128\n",
      "Step: 3054, Loss: 0.9200916290283203, Accuracy: 1.0, Computation time: 2.142467975616455\n",
      "Step: 3055, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.486121654510498\n",
      "Step: 3056, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.4516618251800537\n",
      "Step: 3057, Loss: 0.959118127822876, Accuracy: 0.9375, Computation time: 1.5361835956573486\n",
      "########################\n",
      "Test loss: 1.0710597038269043, Test Accuracy_epoch22: 0.7710960507392883\n",
      "########################\n",
      "Step: 3058, Loss: 0.9159302115440369, Accuracy: 1.0, Computation time: 1.4413139820098877\n",
      "Step: 3059, Loss: 0.9292659163475037, Accuracy: 0.96875, Computation time: 2.270360231399536\n",
      "Step: 3060, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.4052438735961914\n",
      "Step: 3061, Loss: 0.9159635901451111, Accuracy: 1.0, Computation time: 1.2712507247924805\n",
      "Step: 3062, Loss: 0.9161446690559387, Accuracy: 1.0, Computation time: 1.3168151378631592\n",
      "Step: 3063, Loss: 0.9161273837089539, Accuracy: 1.0, Computation time: 1.4673752784729004\n",
      "Step: 3064, Loss: 0.9161129593849182, Accuracy: 1.0, Computation time: 1.6527984142303467\n",
      "Step: 3065, Loss: 0.9160429835319519, Accuracy: 1.0, Computation time: 1.5601966381072998\n",
      "Step: 3066, Loss: 0.9160234928131104, Accuracy: 1.0, Computation time: 1.6078400611877441\n",
      "Step: 3067, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 1.5924856662750244\n",
      "Step: 3068, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.386411428451538\n",
      "Step: 3069, Loss: 0.9159688949584961, Accuracy: 1.0, Computation time: 1.6643853187561035\n",
      "Step: 3070, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 1.5647962093353271\n",
      "Step: 3071, Loss: 0.9761381149291992, Accuracy: 0.90625, Computation time: 2.3436062335968018\n",
      "Step: 3072, Loss: 0.9160161018371582, Accuracy: 1.0, Computation time: 2.2292113304138184\n",
      "Step: 3073, Loss: 0.91596519947052, Accuracy: 1.0, Computation time: 1.7205698490142822\n",
      "Step: 3074, Loss: 0.937907874584198, Accuracy: 0.96875, Computation time: 1.5182690620422363\n",
      "Step: 3075, Loss: 0.9217949509620667, Accuracy: 1.0, Computation time: 2.0330657958984375\n",
      "Step: 3076, Loss: 0.9162036776542664, Accuracy: 1.0, Computation time: 2.179703950881958\n",
      "Step: 3077, Loss: 0.9162328243255615, Accuracy: 1.0, Computation time: 1.795743703842163\n",
      "Step: 3078, Loss: 0.9163641929626465, Accuracy: 1.0, Computation time: 1.721829891204834\n",
      "Step: 3079, Loss: 0.9160526394844055, Accuracy: 1.0, Computation time: 2.1736555099487305\n",
      "Step: 3080, Loss: 0.9159584641456604, Accuracy: 1.0, Computation time: 1.5906062126159668\n",
      "Step: 3081, Loss: 0.9159683585166931, Accuracy: 1.0, Computation time: 1.7506608963012695\n",
      "Step: 3082, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.4742238521575928\n",
      "Step: 3083, Loss: 0.9403649568557739, Accuracy: 0.96875, Computation time: 1.907832145690918\n",
      "Step: 3084, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.3074328899383545\n",
      "Step: 3085, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.753093957901001\n",
      "Step: 3086, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.4719555377960205\n",
      "Step: 3087, Loss: 0.9374166131019592, Accuracy: 0.96875, Computation time: 1.5872786045074463\n",
      "Step: 3088, Loss: 0.91653972864151, Accuracy: 1.0, Computation time: 1.6478097438812256\n",
      "Step: 3089, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 1.8935413360595703\n",
      "Step: 3090, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.4695103168487549\n",
      "Step: 3091, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.5430693626403809\n",
      "Step: 3092, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.3503987789154053\n",
      "Step: 3093, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.2998735904693604\n",
      "Step: 3094, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.3765904903411865\n",
      "Step: 3095, Loss: 0.9375135898590088, Accuracy: 0.96875, Computation time: 1.5292327404022217\n",
      "Step: 3096, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.4199495315551758\n",
      "Step: 3097, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.3185453414916992\n",
      "Step: 3098, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.7157411575317383\n",
      "Step: 3099, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 1.8578822612762451\n",
      "Step: 3100, Loss: 0.9173139929771423, Accuracy: 1.0, Computation time: 1.5804224014282227\n",
      "Step: 3101, Loss: 0.9333228468894958, Accuracy: 0.96875, Computation time: 1.39945387840271\n",
      "Step: 3102, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.222916603088379\n",
      "Step: 3103, Loss: 0.9169491529464722, Accuracy: 1.0, Computation time: 1.3674876689910889\n",
      "Step: 3104, Loss: 0.9159537553787231, Accuracy: 1.0, Computation time: 1.6328024864196777\n",
      "Step: 3105, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.4852452278137207\n",
      "Step: 3106, Loss: 0.9161167144775391, Accuracy: 1.0, Computation time: 1.6174781322479248\n",
      "Step: 3107, Loss: 0.9162386655807495, Accuracy: 1.0, Computation time: 1.2069494724273682\n",
      "Step: 3108, Loss: 0.9374338388442993, Accuracy: 0.96875, Computation time: 1.684941053390503\n",
      "Step: 3109, Loss: 0.93626868724823, Accuracy: 0.96875, Computation time: 1.735703706741333\n",
      "Step: 3110, Loss: 0.9159042239189148, Accuracy: 1.0, Computation time: 1.3635952472686768\n",
      "Step: 3111, Loss: 0.9374597668647766, Accuracy: 0.96875, Computation time: 1.3537859916687012\n",
      "Step: 3112, Loss: 0.916242241859436, Accuracy: 1.0, Computation time: 1.2868165969848633\n",
      "Step: 3113, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.7175540924072266\n",
      "Step: 3114, Loss: 0.9159338474273682, Accuracy: 1.0, Computation time: 1.510695219039917\n",
      "Step: 3115, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.468111515045166\n",
      "Step: 3116, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.6613879203796387\n",
      "Step: 3117, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 1.6789929866790771\n",
      "Step: 3118, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.1586267948150635\n",
      "Step: 3119, Loss: 0.9159125089645386, Accuracy: 1.0, Computation time: 1.575340747833252\n",
      "Step: 3120, Loss: 0.9160081744194031, Accuracy: 1.0, Computation time: 1.7400476932525635\n",
      "Step: 3121, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 1.4438681602478027\n",
      "Step: 3122, Loss: 0.9203851222991943, Accuracy: 1.0, Computation time: 1.9389140605926514\n",
      "Step: 3123, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.3154361248016357\n",
      "Step: 3124, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.4860858917236328\n",
      "Step: 3125, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.5826828479766846\n",
      "Step: 3126, Loss: 0.9185735583305359, Accuracy: 1.0, Computation time: 1.9918148517608643\n",
      "Step: 3127, Loss: 0.9171022772789001, Accuracy: 1.0, Computation time: 1.668365240097046\n",
      "Step: 3128, Loss: 0.9159824252128601, Accuracy: 1.0, Computation time: 1.7266180515289307\n",
      "Step: 3129, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.2540099620819092\n",
      "Step: 3130, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 1.5019128322601318\n",
      "Step: 3131, Loss: 0.916328489780426, Accuracy: 1.0, Computation time: 1.6040987968444824\n",
      "Step: 3132, Loss: 0.91596519947052, Accuracy: 1.0, Computation time: 1.7719807624816895\n",
      "Step: 3133, Loss: 0.9159450531005859, Accuracy: 1.0, Computation time: 1.405444622039795\n",
      "Step: 3134, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 1.3810744285583496\n",
      "Step: 3135, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 1.4995439052581787\n",
      "Step: 3136, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.436554193496704\n",
      "Step: 3137, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.321002721786499\n",
      "Step: 3138, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.2872989177703857\n",
      "Step: 3139, Loss: 0.937638521194458, Accuracy: 0.96875, Computation time: 1.3102521896362305\n",
      "Step: 3140, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.2182273864746094\n",
      "Step: 3141, Loss: 0.9206902384757996, Accuracy: 1.0, Computation time: 2.117316246032715\n",
      "Step: 3142, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 1.4151058197021484\n",
      "Step: 3143, Loss: 0.9160524606704712, Accuracy: 1.0, Computation time: 1.7948365211486816\n",
      "Step: 3144, Loss: 0.9159693121910095, Accuracy: 1.0, Computation time: 1.546198844909668\n",
      "Step: 3145, Loss: 0.9159451127052307, Accuracy: 1.0, Computation time: 1.552767276763916\n",
      "Step: 3146, Loss: 0.9245129823684692, Accuracy: 1.0, Computation time: 1.5086784362792969\n",
      "Step: 3147, Loss: 0.9159603714942932, Accuracy: 1.0, Computation time: 1.3764631748199463\n",
      "Step: 3148, Loss: 0.9375675916671753, Accuracy: 0.96875, Computation time: 1.1921191215515137\n",
      "Step: 3149, Loss: 0.9160153865814209, Accuracy: 1.0, Computation time: 1.4914820194244385\n",
      "Step: 3150, Loss: 0.9159416556358337, Accuracy: 1.0, Computation time: 1.4429044723510742\n",
      "Step: 3151, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.6557295322418213\n",
      "Step: 3152, Loss: 0.9159175157546997, Accuracy: 1.0, Computation time: 1.7613420486450195\n",
      "Step: 3153, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.6447865962982178\n",
      "Step: 3154, Loss: 0.9159331321716309, Accuracy: 1.0, Computation time: 1.6430683135986328\n",
      "Step: 3155, Loss: 0.9160327911376953, Accuracy: 1.0, Computation time: 1.9743142127990723\n",
      "Step: 3156, Loss: 0.9375339150428772, Accuracy: 0.96875, Computation time: 2.0001978874206543\n",
      "Step: 3157, Loss: 0.9164819717407227, Accuracy: 1.0, Computation time: 1.9844512939453125\n",
      "Step: 3158, Loss: 0.9364018440246582, Accuracy: 0.96875, Computation time: 2.394415855407715\n",
      "Step: 3159, Loss: 0.9159253239631653, Accuracy: 1.0, Computation time: 1.7513628005981445\n",
      "Step: 3160, Loss: 0.9385239481925964, Accuracy: 0.96875, Computation time: 1.8009014129638672\n",
      "Step: 3161, Loss: 0.9159241318702698, Accuracy: 1.0, Computation time: 1.6853280067443848\n",
      "Step: 3162, Loss: 0.9162144064903259, Accuracy: 1.0, Computation time: 2.0086140632629395\n",
      "Step: 3163, Loss: 0.9357050657272339, Accuracy: 0.96875, Computation time: 2.052645444869995\n",
      "Step: 3164, Loss: 0.9160025715827942, Accuracy: 1.0, Computation time: 2.0580661296844482\n",
      "Step: 3165, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.9031023979187012\n",
      "Step: 3166, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.7279934883117676\n",
      "Step: 3167, Loss: 0.916041910648346, Accuracy: 1.0, Computation time: 1.6451268196105957\n",
      "Step: 3168, Loss: 0.9160816073417664, Accuracy: 1.0, Computation time: 1.518479824066162\n",
      "Step: 3169, Loss: 0.916044294834137, Accuracy: 1.0, Computation time: 1.65006422996521\n",
      "Step: 3170, Loss: 0.9164520502090454, Accuracy: 1.0, Computation time: 2.0635294914245605\n",
      "Step: 3171, Loss: 0.9159457087516785, Accuracy: 1.0, Computation time: 1.827287197113037\n",
      "Step: 3172, Loss: 0.9375549554824829, Accuracy: 0.96875, Computation time: 1.8029046058654785\n",
      "Step: 3173, Loss: 0.9167962074279785, Accuracy: 1.0, Computation time: 1.7062838077545166\n",
      "Step: 3174, Loss: 0.915953516960144, Accuracy: 1.0, Computation time: 2.2297720909118652\n",
      "Step: 3175, Loss: 0.9160121083259583, Accuracy: 1.0, Computation time: 2.284968852996826\n",
      "Step: 3176, Loss: 0.9174917936325073, Accuracy: 1.0, Computation time: 1.9689505100250244\n",
      "Step: 3177, Loss: 0.9168263673782349, Accuracy: 1.0, Computation time: 1.604102611541748\n",
      "Step: 3178, Loss: 0.937975287437439, Accuracy: 0.96875, Computation time: 1.6738781929016113\n",
      "Step: 3179, Loss: 0.9161322712898254, Accuracy: 1.0, Computation time: 1.5100440979003906\n",
      "Step: 3180, Loss: 0.9162283539772034, Accuracy: 1.0, Computation time: 1.7292020320892334\n",
      "Step: 3181, Loss: 0.9162108302116394, Accuracy: 1.0, Computation time: 1.6646411418914795\n",
      "Step: 3182, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 1.6238741874694824\n",
      "Step: 3183, Loss: 0.938531756401062, Accuracy: 0.96875, Computation time: 1.9383630752563477\n",
      "Step: 3184, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.8725571632385254\n",
      "Step: 3185, Loss: 0.915916383266449, Accuracy: 1.0, Computation time: 1.7620375156402588\n",
      "Step: 3186, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.5572841167449951\n",
      "Step: 3187, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.8535473346710205\n",
      "Step: 3188, Loss: 0.9159722328186035, Accuracy: 1.0, Computation time: 1.6576323509216309\n",
      "Step: 3189, Loss: 0.9160474538803101, Accuracy: 1.0, Computation time: 1.6404905319213867\n",
      "Step: 3190, Loss: 0.9177005290985107, Accuracy: 1.0, Computation time: 1.923861026763916\n",
      "Step: 3191, Loss: 0.9233761429786682, Accuracy: 1.0, Computation time: 1.7717115879058838\n",
      "Step: 3192, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.4155161380767822\n",
      "Step: 3193, Loss: 0.9469286203384399, Accuracy: 0.9375, Computation time: 2.2869575023651123\n",
      "Step: 3194, Loss: 0.9161016941070557, Accuracy: 1.0, Computation time: 1.6499226093292236\n",
      "Step: 3195, Loss: 0.9162923097610474, Accuracy: 1.0, Computation time: 1.6500511169433594\n",
      "Step: 3196, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.4591329097747803\n",
      "########################\n",
      "Test loss: 1.0736286640167236, Test Accuracy_epoch23: 0.7710960507392883\n",
      "########################\n",
      "Step: 3197, Loss: 0.9396278262138367, Accuracy: 0.96875, Computation time: 1.8731334209442139\n",
      "Step: 3198, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.2998666763305664\n",
      "Step: 3199, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.5246829986572266\n",
      "Step: 3200, Loss: 0.9374042749404907, Accuracy: 0.96875, Computation time: 1.4797918796539307\n",
      "Step: 3201, Loss: 0.9160130620002747, Accuracy: 1.0, Computation time: 1.3389897346496582\n",
      "Step: 3202, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 1.251025676727295\n",
      "Step: 3203, Loss: 0.9162792563438416, Accuracy: 1.0, Computation time: 1.5269968509674072\n",
      "Step: 3204, Loss: 0.9162048697471619, Accuracy: 1.0, Computation time: 1.2312586307525635\n",
      "Step: 3205, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.3422424793243408\n",
      "Step: 3206, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.265437364578247\n",
      "Step: 3207, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.8146071434020996\n",
      "Step: 3208, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.1345977783203125\n",
      "Step: 3209, Loss: 0.9159181714057922, Accuracy: 1.0, Computation time: 1.200834035873413\n",
      "Step: 3210, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.5271058082580566\n",
      "Step: 3211, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.7166764736175537\n",
      "Step: 3212, Loss: 0.9167345762252808, Accuracy: 1.0, Computation time: 2.153724431991577\n",
      "Step: 3213, Loss: 0.9159911870956421, Accuracy: 1.0, Computation time: 1.4738695621490479\n",
      "Step: 3214, Loss: 0.9159972071647644, Accuracy: 1.0, Computation time: 1.4472746849060059\n",
      "Step: 3215, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.3066534996032715\n",
      "Step: 3216, Loss: 0.937626302242279, Accuracy: 0.96875, Computation time: 1.3285024166107178\n",
      "Step: 3217, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.4976015090942383\n",
      "Step: 3218, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.730332851409912\n",
      "Step: 3219, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.5925071239471436\n",
      "Step: 3220, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.6233172416687012\n",
      "Step: 3221, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.687077522277832\n",
      "Step: 3222, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 1.1288530826568604\n",
      "Step: 3223, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.5308606624603271\n",
      "Step: 3224, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.545166015625\n",
      "Step: 3225, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.963970422744751\n",
      "Step: 3226, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.0250778198242188\n",
      "Step: 3227, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 1.2890591621398926\n",
      "Step: 3228, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.1178555488586426\n",
      "Step: 3229, Loss: 0.9558368921279907, Accuracy: 0.9375, Computation time: 2.1542558670043945\n",
      "Step: 3230, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.2823007106781006\n",
      "Step: 3231, Loss: 0.9161372780799866, Accuracy: 1.0, Computation time: 1.6615159511566162\n",
      "Step: 3232, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.146040678024292\n",
      "Step: 3233, Loss: 0.9377413392066956, Accuracy: 0.96875, Computation time: 1.2061026096343994\n",
      "Step: 3234, Loss: 0.9159749150276184, Accuracy: 1.0, Computation time: 1.079460859298706\n",
      "Step: 3235, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.3624482154846191\n",
      "Step: 3236, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.3765041828155518\n",
      "Step: 3237, Loss: 0.9162777662277222, Accuracy: 1.0, Computation time: 1.1034998893737793\n",
      "Step: 3238, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.4971160888671875\n",
      "Step: 3239, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.4145281314849854\n",
      "Step: 3240, Loss: 0.9164981245994568, Accuracy: 1.0, Computation time: 1.686917781829834\n",
      "Step: 3241, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.5917623043060303\n",
      "Step: 3242, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.133537769317627\n",
      "Step: 3243, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.1305737495422363\n",
      "Step: 3244, Loss: 0.9376468062400818, Accuracy: 0.96875, Computation time: 1.4376673698425293\n",
      "Step: 3245, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.1527009010314941\n",
      "Step: 3246, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 1.2584259510040283\n",
      "Step: 3247, Loss: 0.9161417484283447, Accuracy: 1.0, Computation time: 1.340409517288208\n",
      "Step: 3248, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 1.1934549808502197\n",
      "Step: 3249, Loss: 0.9160711765289307, Accuracy: 1.0, Computation time: 1.3745274543762207\n",
      "Step: 3250, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.26405668258667\n",
      "Step: 3251, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.1188223361968994\n",
      "Step: 3252, Loss: 0.9251967668533325, Accuracy: 1.0, Computation time: 2.034224033355713\n",
      "Step: 3253, Loss: 0.9159933924674988, Accuracy: 1.0, Computation time: 1.1690583229064941\n",
      "Step: 3254, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 1.1940805912017822\n",
      "Step: 3255, Loss: 0.9172343015670776, Accuracy: 1.0, Computation time: 1.226769208908081\n",
      "Step: 3256, Loss: 0.9159644842147827, Accuracy: 1.0, Computation time: 1.3451223373413086\n",
      "Step: 3257, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.1616179943084717\n",
      "Step: 3258, Loss: 0.9173408150672913, Accuracy: 1.0, Computation time: 1.6325299739837646\n",
      "Step: 3259, Loss: 0.9299024939537048, Accuracy: 0.96875, Computation time: 2.0072827339172363\n",
      "Step: 3260, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 1.133873701095581\n",
      "Step: 3261, Loss: 0.9159675240516663, Accuracy: 1.0, Computation time: 1.4463391304016113\n",
      "Step: 3262, Loss: 0.9405702352523804, Accuracy: 0.96875, Computation time: 1.8772764205932617\n",
      "Step: 3263, Loss: 0.93775475025177, Accuracy: 0.96875, Computation time: 1.2177128791809082\n",
      "Step: 3264, Loss: 0.9160811901092529, Accuracy: 1.0, Computation time: 1.2970025539398193\n",
      "Step: 3265, Loss: 0.9160336256027222, Accuracy: 1.0, Computation time: 1.356774091720581\n",
      "Step: 3266, Loss: 0.916118860244751, Accuracy: 1.0, Computation time: 1.5665981769561768\n",
      "Step: 3267, Loss: 0.9178431630134583, Accuracy: 1.0, Computation time: 2.866760492324829\n",
      "Step: 3268, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 0.9704921245574951\n",
      "Step: 3269, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 1.6059668064117432\n",
      "Step: 3270, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 1.3290612697601318\n",
      "Step: 3271, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.2490015029907227\n",
      "Step: 3272, Loss: 0.9159698486328125, Accuracy: 1.0, Computation time: 1.566868543624878\n",
      "Step: 3273, Loss: 0.9159717559814453, Accuracy: 1.0, Computation time: 1.774608850479126\n",
      "Step: 3274, Loss: 0.9161859154701233, Accuracy: 1.0, Computation time: 1.6161272525787354\n",
      "Step: 3275, Loss: 0.9350199699401855, Accuracy: 0.96875, Computation time: 1.8182263374328613\n",
      "Step: 3276, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.5241503715515137\n",
      "Step: 3277, Loss: 0.9369806051254272, Accuracy: 0.96875, Computation time: 1.9230074882507324\n",
      "Step: 3278, Loss: 0.9159902930259705, Accuracy: 1.0, Computation time: 1.5565636157989502\n",
      "Step: 3279, Loss: 0.9375627040863037, Accuracy: 0.96875, Computation time: 1.672623634338379\n",
      "Step: 3280, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.577885389328003\n",
      "Step: 3281, Loss: 0.9159534573554993, Accuracy: 1.0, Computation time: 1.1911797523498535\n",
      "Step: 3282, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 1.4257891178131104\n",
      "Step: 3283, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 1.5839154720306396\n",
      "Step: 3284, Loss: 0.9158987402915955, Accuracy: 1.0, Computation time: 1.4971983432769775\n",
      "Step: 3285, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.3042984008789062\n",
      "Step: 3286, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.7413694858551025\n",
      "Step: 3287, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.981719732284546\n",
      "Step: 3288, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.337782859802246\n",
      "Step: 3289, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.8949730396270752\n",
      "Step: 3290, Loss: 0.9158712029457092, Accuracy: 1.0, Computation time: 1.3772008419036865\n",
      "Step: 3291, Loss: 0.9162119030952454, Accuracy: 1.0, Computation time: 1.3261146545410156\n",
      "Step: 3292, Loss: 0.9162354469299316, Accuracy: 1.0, Computation time: 1.2434141635894775\n",
      "Step: 3293, Loss: 0.9158541560173035, Accuracy: 1.0, Computation time: 1.7213776111602783\n",
      "Step: 3294, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.589087963104248\n",
      "Step: 3295, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.3379576206207275\n",
      "Step: 3296, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.411515712738037\n",
      "Step: 3297, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 1.4785938262939453\n",
      "Step: 3298, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.4105808734893799\n",
      "Step: 3299, Loss: 0.9160606265068054, Accuracy: 1.0, Computation time: 1.711583137512207\n",
      "Step: 3300, Loss: 0.915930449962616, Accuracy: 1.0, Computation time: 1.8963782787322998\n",
      "Step: 3301, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.629077672958374\n",
      "Step: 3302, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 1.388456106185913\n",
      "Step: 3303, Loss: 0.9187799692153931, Accuracy: 1.0, Computation time: 2.212693452835083\n",
      "Step: 3304, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.3932998180389404\n",
      "Step: 3305, Loss: 0.936048150062561, Accuracy: 0.96875, Computation time: 1.6901004314422607\n",
      "Step: 3306, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.5994470119476318\n",
      "Step: 3307, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.7911262512207031\n",
      "Step: 3308, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 1.6628642082214355\n",
      "Step: 3309, Loss: 0.9173335433006287, Accuracy: 1.0, Computation time: 1.7613694667816162\n",
      "Step: 3310, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.746711015701294\n",
      "Step: 3311, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.4891555309295654\n",
      "Step: 3312, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 1.638063907623291\n",
      "Step: 3313, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.325068712234497\n",
      "Step: 3314, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 2.048506736755371\n",
      "Step: 3315, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.4936740398406982\n",
      "Step: 3316, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.4974863529205322\n",
      "Step: 3317, Loss: 0.9378230571746826, Accuracy: 0.96875, Computation time: 1.7855815887451172\n",
      "Step: 3318, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.3999791145324707\n",
      "Step: 3319, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 1.2272467613220215\n",
      "Step: 3320, Loss: 0.9465840458869934, Accuracy: 0.96875, Computation time: 1.9410991668701172\n",
      "Step: 3321, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.8526439666748047\n",
      "Step: 3322, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.7329788208007812\n",
      "Step: 3323, Loss: 0.9333141446113586, Accuracy: 0.96875, Computation time: 1.5160791873931885\n",
      "Step: 3324, Loss: 0.915957510471344, Accuracy: 1.0, Computation time: 1.3180532455444336\n",
      "Step: 3325, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.419860601425171\n",
      "Step: 3326, Loss: 0.9375452995300293, Accuracy: 0.96875, Computation time: 1.805565595626831\n",
      "Step: 3327, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 1.3707756996154785\n",
      "Step: 3328, Loss: 0.9160443544387817, Accuracy: 1.0, Computation time: 1.788135051727295\n",
      "Step: 3329, Loss: 0.9159520864486694, Accuracy: 1.0, Computation time: 1.466827392578125\n",
      "Step: 3330, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.5307691097259521\n",
      "Step: 3331, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.2434802055358887\n",
      "Step: 3332, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.6748216152191162\n",
      "Step: 3333, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.2354681491851807\n",
      "Step: 3334, Loss: 0.9604493975639343, Accuracy: 0.9375, Computation time: 1.704024314880371\n",
      "Step: 3335, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.200916051864624\n",
      "########################\n",
      "Test loss: 1.0719618797302246, Test Accuracy_epoch24: 0.7740058302879333\n",
      "########################\n",
      "Step: 3336, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 1.5302643775939941\n",
      "Step: 3337, Loss: 0.9197637438774109, Accuracy: 1.0, Computation time: 2.0423991680145264\n",
      "Step: 3338, Loss: 0.9160522818565369, Accuracy: 1.0, Computation time: 1.5464444160461426\n",
      "Step: 3339, Loss: 0.9160861372947693, Accuracy: 1.0, Computation time: 1.7568833827972412\n",
      "Step: 3340, Loss: 0.916206419467926, Accuracy: 1.0, Computation time: 2.0309362411499023\n",
      "Step: 3341, Loss: 0.9178750514984131, Accuracy: 1.0, Computation time: 1.3901023864746094\n",
      "Step: 3342, Loss: 0.9337233304977417, Accuracy: 0.96875, Computation time: 1.6547517776489258\n",
      "Step: 3343, Loss: 0.936711311340332, Accuracy: 0.96875, Computation time: 1.3067409992218018\n",
      "Step: 3344, Loss: 0.916074812412262, Accuracy: 1.0, Computation time: 1.3644590377807617\n",
      "Step: 3345, Loss: 0.9594444036483765, Accuracy: 0.9375, Computation time: 1.3992857933044434\n",
      "Step: 3346, Loss: 0.9159819483757019, Accuracy: 1.0, Computation time: 1.6411876678466797\n",
      "Step: 3347, Loss: 0.9159483909606934, Accuracy: 1.0, Computation time: 1.1618165969848633\n",
      "Step: 3348, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.2358720302581787\n",
      "Step: 3349, Loss: 0.9159278273582458, Accuracy: 1.0, Computation time: 1.1843841075897217\n",
      "Step: 3350, Loss: 0.9160091876983643, Accuracy: 1.0, Computation time: 1.245055913925171\n",
      "Step: 3351, Loss: 0.9159590005874634, Accuracy: 1.0, Computation time: 1.6564199924468994\n",
      "Step: 3352, Loss: 0.9159833192825317, Accuracy: 1.0, Computation time: 1.225562572479248\n",
      "Step: 3353, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 1.330902338027954\n",
      "Step: 3354, Loss: 0.9159390330314636, Accuracy: 1.0, Computation time: 1.3375260829925537\n",
      "Step: 3355, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 1.0425803661346436\n",
      "Step: 3356, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.2505595684051514\n",
      "Step: 3357, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 1.2617619037628174\n",
      "Step: 3358, Loss: 0.9159250855445862, Accuracy: 1.0, Computation time: 1.445446491241455\n",
      "Step: 3359, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.5135228633880615\n",
      "Step: 3360, Loss: 0.9375115036964417, Accuracy: 0.96875, Computation time: 1.7689385414123535\n",
      "Step: 3361, Loss: 0.9159570932388306, Accuracy: 1.0, Computation time: 1.2784955501556396\n",
      "Step: 3362, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.5098223686218262\n",
      "Step: 3363, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.3347814083099365\n",
      "Step: 3364, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.598454236984253\n",
      "Step: 3365, Loss: 0.9479483366012573, Accuracy: 0.9375, Computation time: 1.6331982612609863\n",
      "Step: 3366, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.891315221786499\n",
      "Step: 3367, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 1.3296680450439453\n",
      "Step: 3368, Loss: 0.9352797865867615, Accuracy: 0.96875, Computation time: 1.4142882823944092\n",
      "Step: 3369, Loss: 0.9159737229347229, Accuracy: 1.0, Computation time: 1.113250732421875\n",
      "Step: 3370, Loss: 0.9160680770874023, Accuracy: 1.0, Computation time: 1.4764740467071533\n",
      "Step: 3371, Loss: 0.9159619808197021, Accuracy: 1.0, Computation time: 1.5491702556610107\n",
      "Step: 3372, Loss: 0.9316573143005371, Accuracy: 0.96875, Computation time: 1.4823017120361328\n",
      "Step: 3373, Loss: 0.9162515997886658, Accuracy: 1.0, Computation time: 1.5465946197509766\n",
      "Step: 3374, Loss: 0.9159350395202637, Accuracy: 1.0, Computation time: 1.2627277374267578\n",
      "Step: 3375, Loss: 0.91595059633255, Accuracy: 1.0, Computation time: 1.327812910079956\n",
      "Step: 3376, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.073096513748169\n",
      "Step: 3377, Loss: 0.9171361923217773, Accuracy: 1.0, Computation time: 1.0298495292663574\n",
      "Step: 3378, Loss: 0.9159828424453735, Accuracy: 1.0, Computation time: 0.9616756439208984\n",
      "Step: 3379, Loss: 0.9211169481277466, Accuracy: 1.0, Computation time: 0.9903578758239746\n",
      "Step: 3380, Loss: 0.9158998727798462, Accuracy: 1.0, Computation time: 1.2534964084625244\n",
      "Step: 3381, Loss: 0.9376020431518555, Accuracy: 0.96875, Computation time: 1.0908536911010742\n",
      "Step: 3382, Loss: 0.9159911870956421, Accuracy: 1.0, Computation time: 1.097125768661499\n",
      "Step: 3383, Loss: 0.9160863757133484, Accuracy: 1.0, Computation time: 1.1628782749176025\n",
      "Step: 3384, Loss: 0.9376183748245239, Accuracy: 0.96875, Computation time: 1.3604614734649658\n",
      "Step: 3385, Loss: 0.9161437749862671, Accuracy: 1.0, Computation time: 1.2734596729278564\n",
      "Step: 3386, Loss: 0.9161908626556396, Accuracy: 1.0, Computation time: 1.2671549320220947\n",
      "Step: 3387, Loss: 0.9163500070571899, Accuracy: 1.0, Computation time: 1.346489667892456\n",
      "Step: 3388, Loss: 0.9375470876693726, Accuracy: 0.96875, Computation time: 1.1842525005340576\n",
      "Step: 3389, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 0.912334680557251\n",
      "Step: 3390, Loss: 0.9164264798164368, Accuracy: 1.0, Computation time: 1.5119893550872803\n",
      "Step: 3391, Loss: 0.9159802794456482, Accuracy: 1.0, Computation time: 1.0633728504180908\n",
      "Step: 3392, Loss: 0.9160074591636658, Accuracy: 1.0, Computation time: 1.5653138160705566\n",
      "Step: 3393, Loss: 0.9159513711929321, Accuracy: 1.0, Computation time: 1.305518627166748\n",
      "Step: 3394, Loss: 0.9159477949142456, Accuracy: 1.0, Computation time: 1.1416165828704834\n",
      "Step: 3395, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.2415692806243896\n",
      "Step: 3396, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.3623826503753662\n",
      "Step: 3397, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.2368216514587402\n",
      "Step: 3398, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.119370460510254\n",
      "Step: 3399, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 1.4832170009613037\n",
      "Step: 3400, Loss: 0.9160963296890259, Accuracy: 1.0, Computation time: 1.6261522769927979\n",
      "Step: 3401, Loss: 0.915966272354126, Accuracy: 1.0, Computation time: 1.2754929065704346\n",
      "Step: 3402, Loss: 0.9160458445549011, Accuracy: 1.0, Computation time: 1.4497697353363037\n",
      "Step: 3403, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.215712547302246\n",
      "Step: 3404, Loss: 0.9159517288208008, Accuracy: 1.0, Computation time: 1.4145588874816895\n",
      "Step: 3405, Loss: 0.9377622604370117, Accuracy: 0.96875, Computation time: 1.217106580734253\n",
      "Step: 3406, Loss: 0.9160851836204529, Accuracy: 1.0, Computation time: 1.8879783153533936\n",
      "Step: 3407, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.1767244338989258\n",
      "Step: 3408, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.2855117321014404\n",
      "Step: 3409, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.4667744636535645\n",
      "Step: 3410, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.3870432376861572\n",
      "Step: 3411, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.2720539569854736\n",
      "Step: 3412, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.0958173274993896\n",
      "Step: 3413, Loss: 0.9159548878669739, Accuracy: 1.0, Computation time: 1.2633864879608154\n",
      "Step: 3414, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 1.1134014129638672\n",
      "Step: 3415, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 1.3171892166137695\n",
      "Step: 3416, Loss: 0.9171808958053589, Accuracy: 1.0, Computation time: 1.4592046737670898\n",
      "Step: 3417, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.15110182762146\n",
      "Step: 3418, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.3016693592071533\n",
      "Step: 3419, Loss: 0.9579119682312012, Accuracy: 0.9375, Computation time: 1.5403950214385986\n",
      "Step: 3420, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.1671810150146484\n",
      "Step: 3421, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 1.388625144958496\n",
      "Step: 3422, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.2944631576538086\n",
      "Step: 3423, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.1075854301452637\n",
      "Step: 3424, Loss: 0.9374697208404541, Accuracy: 0.96875, Computation time: 1.6068658828735352\n",
      "Step: 3425, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.2535724639892578\n",
      "Step: 3426, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.281151533126831\n",
      "Step: 3427, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.2581443786621094\n",
      "Step: 3428, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.2380540370941162\n",
      "Step: 3429, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.4832844734191895\n",
      "Step: 3430, Loss: 0.9370975494384766, Accuracy: 0.96875, Computation time: 1.6049599647521973\n",
      "Step: 3431, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.1165025234222412\n",
      "Step: 3432, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.2781636714935303\n",
      "Step: 3433, Loss: 0.9159612059593201, Accuracy: 1.0, Computation time: 1.3019862174987793\n",
      "Step: 3434, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.384089469909668\n",
      "Step: 3435, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.2941267490386963\n",
      "Step: 3436, Loss: 0.9169642925262451, Accuracy: 1.0, Computation time: 1.3828785419464111\n",
      "Step: 3437, Loss: 0.9164348840713501, Accuracy: 1.0, Computation time: 1.5031273365020752\n",
      "Step: 3438, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.2048816680908203\n",
      "Step: 3439, Loss: 0.9159460663795471, Accuracy: 1.0, Computation time: 1.4129750728607178\n",
      "Step: 3440, Loss: 0.9161016941070557, Accuracy: 1.0, Computation time: 1.3307766914367676\n",
      "Step: 3441, Loss: 0.9376766681671143, Accuracy: 0.96875, Computation time: 1.3507843017578125\n",
      "Step: 3442, Loss: 0.9363534450531006, Accuracy: 0.96875, Computation time: 1.5253076553344727\n",
      "Step: 3443, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.151193618774414\n",
      "Step: 3444, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.3108534812927246\n",
      "Step: 3445, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.3798027038574219\n",
      "Step: 3446, Loss: 0.9162591695785522, Accuracy: 1.0, Computation time: 1.443960189819336\n",
      "Step: 3447, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 1.0617198944091797\n",
      "Step: 3448, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.1708760261535645\n",
      "Step: 3449, Loss: 0.9377568960189819, Accuracy: 0.96875, Computation time: 1.1957931518554688\n",
      "Step: 3450, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.2774899005889893\n",
      "Step: 3451, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.4265613555908203\n",
      "Step: 3452, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.3917315006256104\n",
      "Step: 3453, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.2781469821929932\n",
      "Step: 3454, Loss: 0.9161896109580994, Accuracy: 1.0, Computation time: 1.5148837566375732\n",
      "Step: 3455, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.2181625366210938\n",
      "Step: 3456, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.720702886581421\n",
      "Step: 3457, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.4309272766113281\n",
      "Step: 3458, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.5084168910980225\n",
      "Step: 3459, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.2618281841278076\n",
      "Step: 3460, Loss: 0.9170822501182556, Accuracy: 1.0, Computation time: 1.6088216304779053\n",
      "Step: 3461, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 1.3601210117340088\n",
      "Step: 3462, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.1938443183898926\n",
      "Step: 3463, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.1951630115509033\n",
      "Step: 3464, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.5029089450836182\n",
      "Step: 3465, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.0883586406707764\n",
      "Step: 3466, Loss: 0.9159716367721558, Accuracy: 1.0, Computation time: 1.6297659873962402\n",
      "Step: 3467, Loss: 0.9371557235717773, Accuracy: 0.96875, Computation time: 1.7905285358428955\n",
      "Step: 3468, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.1710515022277832\n",
      "Step: 3469, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.1603989601135254\n",
      "Step: 3470, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.0241169929504395\n",
      "Step: 3471, Loss: 0.9375683069229126, Accuracy: 0.96875, Computation time: 1.4941916465759277\n",
      "Step: 3472, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.198747158050537\n",
      "Step: 3473, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.1575627326965332\n",
      "Step: 3474, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 1.1499946117401123\n",
      "########################\n",
      "Test loss: 1.0734872817993164, Test Accuracy_epoch25: 0.7720659971237183\n",
      "########################\n",
      "Step: 3475, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 1.1740822792053223\n",
      "Step: 3476, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.0803134441375732\n",
      "Step: 3477, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.3138973712921143\n",
      "Step: 3478, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.162827968597412\n",
      "Step: 3479, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 1.2921028137207031\n",
      "Step: 3480, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.637136697769165\n",
      "Step: 3481, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.2744410037994385\n",
      "Step: 3482, Loss: 0.9376420974731445, Accuracy: 0.96875, Computation time: 1.2465951442718506\n",
      "Step: 3483, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.088582992553711\n",
      "Step: 3484, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.2839453220367432\n",
      "Step: 3485, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.1171693801879883\n",
      "Step: 3486, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.1406171321868896\n",
      "Step: 3487, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.2333521842956543\n",
      "Step: 3488, Loss: 0.9159278273582458, Accuracy: 1.0, Computation time: 1.3588342666625977\n",
      "Step: 3489, Loss: 0.9356889724731445, Accuracy: 0.96875, Computation time: 1.7449278831481934\n",
      "Step: 3490, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 1.1029078960418701\n",
      "Step: 3491, Loss: 0.937476634979248, Accuracy: 0.96875, Computation time: 1.1561133861541748\n",
      "Step: 3492, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.2168114185333252\n",
      "Step: 3493, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.1780574321746826\n",
      "Step: 3494, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.3909556865692139\n",
      "Step: 3495, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.151177167892456\n",
      "Step: 3496, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.0169684886932373\n",
      "Step: 3497, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.1011919975280762\n",
      "Step: 3498, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 1.3298091888427734\n",
      "Step: 3499, Loss: 0.9369619488716125, Accuracy: 0.96875, Computation time: 1.3448419570922852\n",
      "Step: 3500, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 1.1040210723876953\n",
      "Step: 3501, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.2134764194488525\n",
      "Step: 3502, Loss: 0.9160745739936829, Accuracy: 1.0, Computation time: 1.5946440696716309\n",
      "Step: 3503, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.327392578125\n",
      "Step: 3504, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.2666230201721191\n",
      "Step: 3505, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.2038698196411133\n",
      "Step: 3506, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 1.1708064079284668\n",
      "Step: 3507, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.6409530639648438\n",
      "Step: 3508, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.2847857475280762\n",
      "Step: 3509, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 1.2732574939727783\n",
      "Step: 3510, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.4087376594543457\n",
      "Step: 3511, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.0204195976257324\n",
      "Step: 3512, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.0927059650421143\n",
      "Step: 3513, Loss: 0.9367943406105042, Accuracy: 0.96875, Computation time: 1.3595962524414062\n",
      "Step: 3514, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.493171215057373\n",
      "Step: 3515, Loss: 0.9375703930854797, Accuracy: 0.96875, Computation time: 1.2424633502960205\n",
      "Step: 3516, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.1218514442443848\n",
      "Step: 3517, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.2266871929168701\n",
      "Step: 3518, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.4942810535430908\n",
      "Step: 3519, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 1.2315716743469238\n",
      "Step: 3520, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.2253398895263672\n",
      "Step: 3521, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.4017302989959717\n",
      "Step: 3522, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.1903655529022217\n",
      "Step: 3523, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.2734003067016602\n",
      "Step: 3524, Loss: 0.9176727533340454, Accuracy: 1.0, Computation time: 1.4114320278167725\n",
      "Step: 3525, Loss: 0.9200873970985413, Accuracy: 1.0, Computation time: 1.4759929180145264\n",
      "Step: 3526, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.6536517143249512\n",
      "Step: 3527, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 1.178511142730713\n",
      "Step: 3528, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.2740747928619385\n",
      "Step: 3529, Loss: 0.9159433841705322, Accuracy: 1.0, Computation time: 1.1940302848815918\n",
      "Step: 3530, Loss: 0.9361770749092102, Accuracy: 0.96875, Computation time: 1.0626955032348633\n",
      "Step: 3531, Loss: 0.9165051579475403, Accuracy: 1.0, Computation time: 1.4538075923919678\n",
      "Step: 3532, Loss: 0.937519907951355, Accuracy: 0.96875, Computation time: 0.952824592590332\n",
      "Step: 3533, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.304321527481079\n",
      "Step: 3534, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 1.3085005283355713\n",
      "Step: 3535, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.1272637844085693\n",
      "Step: 3536, Loss: 0.9160306453704834, Accuracy: 1.0, Computation time: 1.1770927906036377\n",
      "Step: 3537, Loss: 0.9345659613609314, Accuracy: 0.96875, Computation time: 1.0721030235290527\n",
      "Step: 3538, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.49674391746521\n",
      "Step: 3539, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.373103141784668\n",
      "Step: 3540, Loss: 0.9375908374786377, Accuracy: 0.96875, Computation time: 1.0872008800506592\n",
      "Step: 3541, Loss: 0.9390848278999329, Accuracy: 0.96875, Computation time: 1.2182352542877197\n",
      "Step: 3542, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.2813687324523926\n",
      "Step: 3543, Loss: 0.9159460663795471, Accuracy: 1.0, Computation time: 1.216623067855835\n",
      "Step: 3544, Loss: 0.9159179329872131, Accuracy: 1.0, Computation time: 1.2220358848571777\n",
      "Step: 3545, Loss: 0.9162307977676392, Accuracy: 1.0, Computation time: 1.4775075912475586\n",
      "Step: 3546, Loss: 0.915953516960144, Accuracy: 1.0, Computation time: 1.179645299911499\n",
      "Step: 3547, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 0.9999392032623291\n",
      "Step: 3548, Loss: 0.9376245141029358, Accuracy: 0.96875, Computation time: 1.5127954483032227\n",
      "Step: 3549, Loss: 0.9199772477149963, Accuracy: 1.0, Computation time: 1.0857231616973877\n",
      "Step: 3550, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.119206190109253\n",
      "Step: 3551, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.3029899597167969\n",
      "Step: 3552, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.2233507633209229\n",
      "Step: 3553, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 0.981647253036499\n",
      "Step: 3554, Loss: 0.917063295841217, Accuracy: 1.0, Computation time: 1.2516422271728516\n",
      "Step: 3555, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.3679289817810059\n",
      "Step: 3556, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.1841111183166504\n",
      "Step: 3557, Loss: 0.9365903735160828, Accuracy: 0.96875, Computation time: 1.2463200092315674\n",
      "Step: 3558, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 0.9828157424926758\n",
      "Step: 3559, Loss: 0.9164214134216309, Accuracy: 1.0, Computation time: 0.9735400676727295\n",
      "Step: 3560, Loss: 0.9181602001190186, Accuracy: 1.0, Computation time: 1.5652873516082764\n",
      "Step: 3561, Loss: 0.9375848174095154, Accuracy: 0.96875, Computation time: 1.3997008800506592\n",
      "Step: 3562, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 1.091343879699707\n",
      "Step: 3563, Loss: 0.9159740209579468, Accuracy: 1.0, Computation time: 1.213698387145996\n",
      "Step: 3564, Loss: 0.9159700870513916, Accuracy: 1.0, Computation time: 1.14296555519104\n",
      "Step: 3565, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 0.8900904655456543\n",
      "Step: 3566, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.048548936843872\n",
      "Step: 3567, Loss: 0.9196653962135315, Accuracy: 1.0, Computation time: 1.0968694686889648\n",
      "Step: 3568, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.2647836208343506\n",
      "Step: 3569, Loss: 0.915901780128479, Accuracy: 1.0, Computation time: 1.2246055603027344\n",
      "Step: 3570, Loss: 0.9159398674964905, Accuracy: 1.0, Computation time: 1.0828914642333984\n",
      "Step: 3571, Loss: 0.9159694314002991, Accuracy: 1.0, Computation time: 1.0416889190673828\n",
      "Step: 3572, Loss: 0.9377310276031494, Accuracy: 0.96875, Computation time: 0.9800035953521729\n",
      "Step: 3573, Loss: 0.9159354567527771, Accuracy: 1.0, Computation time: 0.98897385597229\n",
      "Step: 3574, Loss: 0.9161782264709473, Accuracy: 1.0, Computation time: 1.022001028060913\n",
      "Step: 3575, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.0239529609680176\n",
      "Step: 3576, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 1.0479984283447266\n",
      "Step: 3577, Loss: 0.9161310195922852, Accuracy: 1.0, Computation time: 1.1068577766418457\n",
      "Step: 3578, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 1.1964349746704102\n",
      "Step: 3579, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.3438928127288818\n",
      "Step: 3580, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 1.150444507598877\n",
      "Step: 3581, Loss: 0.9240567684173584, Accuracy: 1.0, Computation time: 1.2977678775787354\n",
      "Step: 3582, Loss: 0.9159569144248962, Accuracy: 1.0, Computation time: 0.9961245059967041\n",
      "Step: 3583, Loss: 0.9161034822463989, Accuracy: 1.0, Computation time: 1.0602197647094727\n",
      "Step: 3584, Loss: 0.9163185954093933, Accuracy: 1.0, Computation time: 1.4099078178405762\n",
      "Step: 3585, Loss: 0.9163351058959961, Accuracy: 1.0, Computation time: 1.6117572784423828\n",
      "Step: 3586, Loss: 0.9377751350402832, Accuracy: 0.96875, Computation time: 1.5281364917755127\n",
      "Step: 3587, Loss: 0.9159749150276184, Accuracy: 1.0, Computation time: 1.8946692943572998\n",
      "Step: 3588, Loss: 0.9164389967918396, Accuracy: 1.0, Computation time: 2.157480001449585\n",
      "Step: 3589, Loss: 0.9159635901451111, Accuracy: 1.0, Computation time: 1.2536211013793945\n",
      "Step: 3590, Loss: 0.9160837531089783, Accuracy: 1.0, Computation time: 1.4180889129638672\n",
      "Step: 3591, Loss: 0.9162511825561523, Accuracy: 1.0, Computation time: 1.4337711334228516\n",
      "Step: 3592, Loss: 0.9162180423736572, Accuracy: 1.0, Computation time: 1.4938220977783203\n",
      "Step: 3593, Loss: 0.9228251576423645, Accuracy: 1.0, Computation time: 1.7676160335540771\n",
      "Step: 3594, Loss: 0.9159908294677734, Accuracy: 1.0, Computation time: 1.5030198097229004\n",
      "Step: 3595, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 1.6850662231445312\n",
      "Step: 3596, Loss: 0.9165392518043518, Accuracy: 1.0, Computation time: 1.9670202732086182\n",
      "Step: 3597, Loss: 0.9160676598548889, Accuracy: 1.0, Computation time: 1.533644437789917\n",
      "Step: 3598, Loss: 0.9160760641098022, Accuracy: 1.0, Computation time: 1.7010776996612549\n",
      "Step: 3599, Loss: 0.9163297414779663, Accuracy: 1.0, Computation time: 1.6286511421203613\n",
      "Step: 3600, Loss: 0.9160450100898743, Accuracy: 1.0, Computation time: 1.418553113937378\n",
      "Step: 3601, Loss: 0.9160197973251343, Accuracy: 1.0, Computation time: 1.6253445148468018\n",
      "Step: 3602, Loss: 0.9159396886825562, Accuracy: 1.0, Computation time: 1.3757803440093994\n",
      "Step: 3603, Loss: 0.9161820411682129, Accuracy: 1.0, Computation time: 1.530895471572876\n",
      "Step: 3604, Loss: 0.9159671068191528, Accuracy: 1.0, Computation time: 1.550997257232666\n",
      "Step: 3605, Loss: 0.952574610710144, Accuracy: 0.9375, Computation time: 1.9720265865325928\n",
      "Step: 3606, Loss: 0.916340172290802, Accuracy: 1.0, Computation time: 1.2939720153808594\n",
      "Step: 3607, Loss: 0.9162032604217529, Accuracy: 1.0, Computation time: 1.441279649734497\n",
      "Step: 3608, Loss: 0.9163314700126648, Accuracy: 1.0, Computation time: 1.344386100769043\n",
      "Step: 3609, Loss: 0.9161335825920105, Accuracy: 1.0, Computation time: 1.4477410316467285\n",
      "Step: 3610, Loss: 0.9378498196601868, Accuracy: 0.96875, Computation time: 1.452247142791748\n",
      "Step: 3611, Loss: 0.9167027473449707, Accuracy: 1.0, Computation time: 1.90091872215271\n",
      "Step: 3612, Loss: 0.9375645518302917, Accuracy: 0.96875, Computation time: 1.5492303371429443\n",
      "Step: 3613, Loss: 0.9161900281906128, Accuracy: 1.0, Computation time: 1.8418724536895752\n",
      "########################\n",
      "Test loss: 1.0747387409210205, Test Accuracy_epoch26: 0.7623666524887085\n",
      "########################\n",
      "Step: 3614, Loss: 0.9161236882209778, Accuracy: 1.0, Computation time: 1.7183303833007812\n",
      "Step: 3615, Loss: 0.9162200689315796, Accuracy: 1.0, Computation time: 1.5490086078643799\n",
      "Step: 3616, Loss: 0.9160274267196655, Accuracy: 1.0, Computation time: 1.6589901447296143\n",
      "Step: 3617, Loss: 0.9374701976776123, Accuracy: 0.96875, Computation time: 1.5143742561340332\n",
      "Step: 3618, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 1.6674599647521973\n",
      "Step: 3619, Loss: 0.9159637093544006, Accuracy: 1.0, Computation time: 1.6763854026794434\n",
      "Step: 3620, Loss: 0.9164720773696899, Accuracy: 1.0, Computation time: 1.609032154083252\n",
      "Step: 3621, Loss: 0.9160913228988647, Accuracy: 1.0, Computation time: 1.5886626243591309\n",
      "Step: 3622, Loss: 0.9160776734352112, Accuracy: 1.0, Computation time: 1.5255117416381836\n",
      "Step: 3623, Loss: 0.9160308837890625, Accuracy: 1.0, Computation time: 1.708498477935791\n",
      "Step: 3624, Loss: 0.9401102662086487, Accuracy: 0.96875, Computation time: 1.7713596820831299\n",
      "Step: 3625, Loss: 0.9164783954620361, Accuracy: 1.0, Computation time: 1.9154870510101318\n",
      "Step: 3626, Loss: 0.9160078167915344, Accuracy: 1.0, Computation time: 1.75663423538208\n",
      "Step: 3627, Loss: 0.9163677096366882, Accuracy: 1.0, Computation time: 1.606659173965454\n",
      "Step: 3628, Loss: 0.9160152673721313, Accuracy: 1.0, Computation time: 1.5172603130340576\n",
      "Step: 3629, Loss: 0.9160703420639038, Accuracy: 1.0, Computation time: 2.0875260829925537\n",
      "Step: 3630, Loss: 0.937635600566864, Accuracy: 0.96875, Computation time: 1.408332347869873\n",
      "Step: 3631, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 1.5230209827423096\n",
      "Step: 3632, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.475475549697876\n",
      "Step: 3633, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.7007296085357666\n",
      "Step: 3634, Loss: 0.9159478545188904, Accuracy: 1.0, Computation time: 1.4336574077606201\n",
      "Step: 3635, Loss: 0.9159440398216248, Accuracy: 1.0, Computation time: 1.4253754615783691\n",
      "Step: 3636, Loss: 0.915961503982544, Accuracy: 1.0, Computation time: 1.9023659229278564\n",
      "Step: 3637, Loss: 0.9159225225448608, Accuracy: 1.0, Computation time: 1.590266227722168\n",
      "Step: 3638, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.802269697189331\n",
      "Step: 3639, Loss: 0.915936291217804, Accuracy: 1.0, Computation time: 1.4911646842956543\n",
      "Step: 3640, Loss: 0.9299099445343018, Accuracy: 0.96875, Computation time: 1.4954278469085693\n",
      "Step: 3641, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 1.4841117858886719\n",
      "Step: 3642, Loss: 0.9176420569419861, Accuracy: 1.0, Computation time: 1.9166553020477295\n",
      "Step: 3643, Loss: 0.9159671068191528, Accuracy: 1.0, Computation time: 1.3038830757141113\n",
      "Step: 3644, Loss: 0.9160284996032715, Accuracy: 1.0, Computation time: 1.3003661632537842\n",
      "Step: 3645, Loss: 0.9160016179084778, Accuracy: 1.0, Computation time: 1.5386102199554443\n",
      "Step: 3646, Loss: 0.9376875162124634, Accuracy: 0.96875, Computation time: 1.7067112922668457\n",
      "Step: 3647, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 1.3801414966583252\n",
      "Step: 3648, Loss: 0.9373538494110107, Accuracy: 0.96875, Computation time: 1.3650429248809814\n",
      "Step: 3649, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.288407802581787\n",
      "Step: 3650, Loss: 0.9160072207450867, Accuracy: 1.0, Computation time: 1.2650694847106934\n",
      "Step: 3651, Loss: 0.9161162972450256, Accuracy: 1.0, Computation time: 1.8242909908294678\n",
      "Step: 3652, Loss: 0.9159839153289795, Accuracy: 1.0, Computation time: 1.2387609481811523\n",
      "Step: 3653, Loss: 0.916488528251648, Accuracy: 1.0, Computation time: 1.6751253604888916\n",
      "Step: 3654, Loss: 0.915928065776825, Accuracy: 1.0, Computation time: 1.149857759475708\n",
      "Step: 3655, Loss: 0.9159437417984009, Accuracy: 1.0, Computation time: 1.3453257083892822\n",
      "Step: 3656, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.4692668914794922\n",
      "Step: 3657, Loss: 0.9159570336341858, Accuracy: 1.0, Computation time: 1.3664662837982178\n",
      "Step: 3658, Loss: 0.9160139560699463, Accuracy: 1.0, Computation time: 1.514937162399292\n",
      "Step: 3659, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.2952187061309814\n",
      "Step: 3660, Loss: 0.9364340901374817, Accuracy: 0.96875, Computation time: 1.49647855758667\n",
      "Step: 3661, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.2492609024047852\n",
      "Step: 3662, Loss: 0.9389960169792175, Accuracy: 0.96875, Computation time: 1.6003174781799316\n",
      "Step: 3663, Loss: 0.9159880876541138, Accuracy: 1.0, Computation time: 1.2348501682281494\n",
      "Step: 3664, Loss: 0.9160040616989136, Accuracy: 1.0, Computation time: 1.4705357551574707\n",
      "Step: 3665, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 1.444286823272705\n",
      "Step: 3666, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 1.493417501449585\n",
      "Step: 3667, Loss: 0.9159323573112488, Accuracy: 1.0, Computation time: 1.4913954734802246\n",
      "Step: 3668, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 1.3663535118103027\n",
      "Step: 3669, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.244739294052124\n",
      "Step: 3670, Loss: 0.9164339900016785, Accuracy: 1.0, Computation time: 1.5649659633636475\n",
      "Step: 3671, Loss: 0.9374327063560486, Accuracy: 0.96875, Computation time: 1.268183708190918\n",
      "Step: 3672, Loss: 0.9161881804466248, Accuracy: 1.0, Computation time: 1.7106783390045166\n",
      "Step: 3673, Loss: 0.9378835558891296, Accuracy: 0.96875, Computation time: 1.8272910118103027\n",
      "Step: 3674, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.6268107891082764\n",
      "Step: 3675, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.1240904331207275\n",
      "Step: 3676, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.2316226959228516\n",
      "Step: 3677, Loss: 0.937497615814209, Accuracy: 0.96875, Computation time: 1.3830440044403076\n",
      "Step: 3678, Loss: 0.9375903010368347, Accuracy: 0.96875, Computation time: 1.6236512660980225\n",
      "Step: 3679, Loss: 0.9231932163238525, Accuracy: 1.0, Computation time: 1.6001944541931152\n",
      "Step: 3680, Loss: 0.9273079633712769, Accuracy: 0.96875, Computation time: 1.3739001750946045\n",
      "Step: 3681, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.3337385654449463\n",
      "Step: 3682, Loss: 0.9374217391014099, Accuracy: 0.96875, Computation time: 1.873481035232544\n",
      "Step: 3683, Loss: 0.91608726978302, Accuracy: 1.0, Computation time: 1.3939836025238037\n",
      "Step: 3684, Loss: 0.9161068797111511, Accuracy: 1.0, Computation time: 1.7066681385040283\n",
      "Step: 3685, Loss: 0.9161625504493713, Accuracy: 1.0, Computation time: 1.5736451148986816\n",
      "Step: 3686, Loss: 0.916025698184967, Accuracy: 1.0, Computation time: 1.6786260604858398\n",
      "Step: 3687, Loss: 0.916388213634491, Accuracy: 1.0, Computation time: 1.57780122756958\n",
      "Step: 3688, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.6945321559906006\n",
      "Step: 3689, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.7814700603485107\n",
      "Step: 3690, Loss: 0.9160556197166443, Accuracy: 1.0, Computation time: 1.7638633251190186\n",
      "Step: 3691, Loss: 0.9166388511657715, Accuracy: 1.0, Computation time: 1.3503508567810059\n",
      "Step: 3692, Loss: 0.9160358905792236, Accuracy: 1.0, Computation time: 1.7407817840576172\n",
      "Step: 3693, Loss: 0.9161071181297302, Accuracy: 1.0, Computation time: 1.194610834121704\n",
      "Step: 3694, Loss: 0.9159630537033081, Accuracy: 1.0, Computation time: 1.2204227447509766\n",
      "Step: 3695, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 1.671400785446167\n",
      "Step: 3696, Loss: 0.9376450181007385, Accuracy: 0.96875, Computation time: 1.341895341873169\n",
      "Step: 3697, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 1.2134950160980225\n",
      "Step: 3698, Loss: 0.9367793202400208, Accuracy: 0.96875, Computation time: 1.8597488403320312\n",
      "Step: 3699, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.557715892791748\n",
      "Step: 3700, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.2395789623260498\n",
      "Step: 3701, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.5213367938995361\n",
      "Step: 3702, Loss: 0.915935218334198, Accuracy: 1.0, Computation time: 1.3747673034667969\n",
      "Step: 3703, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.1847522258758545\n",
      "Step: 3704, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.3356282711029053\n",
      "Step: 3705, Loss: 0.9375332593917847, Accuracy: 0.96875, Computation time: 1.5374741554260254\n",
      "Step: 3706, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.5797274112701416\n",
      "Step: 3707, Loss: 0.9158712029457092, Accuracy: 1.0, Computation time: 1.5478386878967285\n",
      "Step: 3708, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.3776593208312988\n",
      "Step: 3709, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 1.2206413745880127\n",
      "Step: 3710, Loss: 0.9160592555999756, Accuracy: 1.0, Computation time: 1.2277510166168213\n",
      "Step: 3711, Loss: 0.9163197875022888, Accuracy: 1.0, Computation time: 1.4993877410888672\n",
      "Step: 3712, Loss: 0.935477077960968, Accuracy: 0.96875, Computation time: 1.813650369644165\n",
      "Step: 3713, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.666031837463379\n",
      "Step: 3714, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.6753818988800049\n",
      "Step: 3715, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.433887004852295\n",
      "Step: 3716, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.5482425689697266\n",
      "Step: 3717, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.762364387512207\n",
      "Step: 3718, Loss: 0.9187138080596924, Accuracy: 1.0, Computation time: 1.4999544620513916\n",
      "Step: 3719, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.7514479160308838\n",
      "Step: 3720, Loss: 0.9366177320480347, Accuracy: 0.96875, Computation time: 1.4068920612335205\n",
      "Step: 3721, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 1.3938963413238525\n",
      "Step: 3722, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.3960506916046143\n",
      "Step: 3723, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 1.6155502796173096\n",
      "Step: 3724, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.5317082405090332\n",
      "Step: 3725, Loss: 0.9161059856414795, Accuracy: 1.0, Computation time: 1.6030807495117188\n",
      "Step: 3726, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.5336074829101562\n",
      "Step: 3727, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 1.3392233848571777\n",
      "Step: 3728, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.480315923690796\n",
      "Step: 3729, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.56937837600708\n",
      "Step: 3730, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.166459321975708\n",
      "Step: 3731, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.3154175281524658\n",
      "Step: 3732, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.2327146530151367\n",
      "Step: 3733, Loss: 0.9202055335044861, Accuracy: 1.0, Computation time: 1.3955326080322266\n",
      "Step: 3734, Loss: 0.9371660351753235, Accuracy: 0.96875, Computation time: 2.0036184787750244\n",
      "Step: 3735, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.2865557670593262\n",
      "Step: 3736, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 1.411761999130249\n",
      "Step: 3737, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.095132827758789\n",
      "Step: 3738, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.4513499736785889\n",
      "Step: 3739, Loss: 0.9164167642593384, Accuracy: 1.0, Computation time: 1.658212661743164\n",
      "Step: 3740, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.4463257789611816\n",
      "Step: 3741, Loss: 0.9367988109588623, Accuracy: 0.96875, Computation time: 1.5387909412384033\n",
      "Step: 3742, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.201202392578125\n",
      "Step: 3743, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.1406123638153076\n",
      "Step: 3744, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 1.2482664585113525\n",
      "Step: 3745, Loss: 0.9375274777412415, Accuracy: 0.96875, Computation time: 1.7659988403320312\n",
      "Step: 3746, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.645143747329712\n",
      "Step: 3747, Loss: 0.91989666223526, Accuracy: 1.0, Computation time: 1.3091175556182861\n",
      "Step: 3748, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 1.1771266460418701\n",
      "Step: 3749, Loss: 0.9370515942573547, Accuracy: 0.96875, Computation time: 1.2620975971221924\n",
      "Step: 3750, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.498406171798706\n",
      "Step: 3751, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.102614164352417\n",
      "Step: 3752, Loss: 0.9159162640571594, Accuracy: 1.0, Computation time: 1.244215965270996\n",
      "########################\n",
      "Test loss: 1.0692678689956665, Test Accuracy_epoch27: 0.7759457230567932\n",
      "########################\n",
      "Step: 3753, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.4260659217834473\n",
      "Step: 3754, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.1395530700683594\n",
      "Step: 3755, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.171675205230713\n",
      "Step: 3756, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.263702392578125\n",
      "Step: 3757, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.4866974353790283\n",
      "Step: 3758, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.299333095550537\n",
      "Step: 3759, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.3342163562774658\n",
      "Step: 3760, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.0523366928100586\n",
      "Step: 3761, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 1.4302427768707275\n",
      "Step: 3762, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.13393235206604\n",
      "Step: 3763, Loss: 0.9368418455123901, Accuracy: 0.96875, Computation time: 1.360647439956665\n",
      "Step: 3764, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.5291154384613037\n",
      "Step: 3765, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.5488381385803223\n",
      "Step: 3766, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.5568568706512451\n",
      "Step: 3767, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.2377498149871826\n",
      "Step: 3768, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.3437061309814453\n",
      "Step: 3769, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.489081859588623\n",
      "Step: 3770, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.5029361248016357\n",
      "Step: 3771, Loss: 0.9158415794372559, Accuracy: 1.0, Computation time: 1.4272019863128662\n",
      "Step: 3772, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.6980092525482178\n",
      "Step: 3773, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.239542007446289\n",
      "Step: 3774, Loss: 0.915840208530426, Accuracy: 1.0, Computation time: 1.3254146575927734\n",
      "Step: 3775, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.4396936893463135\n",
      "Step: 3776, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 1.2155325412750244\n",
      "Step: 3777, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.414504051208496\n",
      "Step: 3778, Loss: 0.9403385519981384, Accuracy: 0.96875, Computation time: 1.4443364143371582\n",
      "Step: 3779, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.152768850326538\n",
      "Step: 3780, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 1.349287748336792\n",
      "Step: 3781, Loss: 0.9168174862861633, Accuracy: 1.0, Computation time: 1.2787799835205078\n",
      "Step: 3782, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 1.4091572761535645\n",
      "Step: 3783, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 1.2759950160980225\n",
      "Step: 3784, Loss: 0.937556803226471, Accuracy: 0.96875, Computation time: 1.1756410598754883\n",
      "Step: 3785, Loss: 0.9159996509552002, Accuracy: 1.0, Computation time: 1.1944267749786377\n",
      "Step: 3786, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 1.238832712173462\n",
      "Step: 3787, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.279294729232788\n",
      "Step: 3788, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.3342821598052979\n",
      "Step: 3789, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 1.0683345794677734\n",
      "Step: 3790, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.141230583190918\n",
      "Step: 3791, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 0.9064407348632812\n",
      "Step: 3792, Loss: 0.9160056710243225, Accuracy: 1.0, Computation time: 1.275264024734497\n",
      "Step: 3793, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 0.9415555000305176\n",
      "Step: 3794, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.5837018489837646\n",
      "Step: 3795, Loss: 0.9163734316825867, Accuracy: 1.0, Computation time: 1.97133469581604\n",
      "Step: 3796, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.3615727424621582\n",
      "Step: 3797, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.0486814975738525\n",
      "Step: 3798, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.0795929431915283\n",
      "Step: 3799, Loss: 0.9375728368759155, Accuracy: 0.96875, Computation time: 1.2594873905181885\n",
      "Step: 3800, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.0885882377624512\n",
      "Step: 3801, Loss: 0.9375172853469849, Accuracy: 0.96875, Computation time: 1.204913854598999\n",
      "Step: 3802, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.0254628658294678\n",
      "Step: 3803, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.0352187156677246\n",
      "Step: 3804, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.2116105556488037\n",
      "Step: 3805, Loss: 0.936530590057373, Accuracy: 0.96875, Computation time: 1.0424220561981201\n",
      "Step: 3806, Loss: 0.9159647822380066, Accuracy: 1.0, Computation time: 0.9892756938934326\n",
      "Step: 3807, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 1.3022162914276123\n",
      "Step: 3808, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.010563850402832\n",
      "Step: 3809, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.3705289363861084\n",
      "Step: 3810, Loss: 0.916027843952179, Accuracy: 1.0, Computation time: 1.2471058368682861\n",
      "Step: 3811, Loss: 0.9373999238014221, Accuracy: 0.96875, Computation time: 1.4723641872406006\n",
      "Step: 3812, Loss: 0.9197325110435486, Accuracy: 1.0, Computation time: 1.5995070934295654\n",
      "Step: 3813, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.0483145713806152\n",
      "Step: 3814, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 1.0139260292053223\n",
      "Step: 3815, Loss: 0.9159601926803589, Accuracy: 1.0, Computation time: 1.1118135452270508\n",
      "Step: 3816, Loss: 0.9365060925483704, Accuracy: 0.96875, Computation time: 1.306333065032959\n",
      "Step: 3817, Loss: 0.9159492254257202, Accuracy: 1.0, Computation time: 1.1577329635620117\n",
      "Step: 3818, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.054936170578003\n",
      "Step: 3819, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 0.9514038562774658\n",
      "Step: 3820, Loss: 0.9374753832817078, Accuracy: 0.96875, Computation time: 1.2357842922210693\n",
      "Step: 3821, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.1587581634521484\n",
      "Step: 3822, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 1.226625919342041\n",
      "Step: 3823, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 1.4368641376495361\n",
      "Step: 3824, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.302690029144287\n",
      "Step: 3825, Loss: 0.9278302788734436, Accuracy: 0.96875, Computation time: 1.9163568019866943\n",
      "Step: 3826, Loss: 0.9158360362052917, Accuracy: 1.0, Computation time: 1.0749647617340088\n",
      "Step: 3827, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.047534465789795\n",
      "Step: 3828, Loss: 0.9161984920501709, Accuracy: 1.0, Computation time: 1.2753808498382568\n",
      "Step: 3829, Loss: 0.9159730672836304, Accuracy: 1.0, Computation time: 1.0772197246551514\n",
      "Step: 3830, Loss: 0.9160327315330505, Accuracy: 1.0, Computation time: 1.2116382122039795\n",
      "Step: 3831, Loss: 0.9160141348838806, Accuracy: 1.0, Computation time: 1.3941526412963867\n",
      "Step: 3832, Loss: 0.9160030484199524, Accuracy: 1.0, Computation time: 1.2545180320739746\n",
      "Step: 3833, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 1.333963394165039\n",
      "Step: 3834, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 1.1800289154052734\n",
      "Step: 3835, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.0854518413543701\n",
      "Step: 3836, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 1.3169004917144775\n",
      "Step: 3837, Loss: 0.91623455286026, Accuracy: 1.0, Computation time: 2.252105712890625\n",
      "Step: 3838, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.3823192119598389\n",
      "Step: 3839, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 1.4766697883605957\n",
      "Step: 3840, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.2589223384857178\n",
      "Step: 3841, Loss: 0.9159027934074402, Accuracy: 1.0, Computation time: 1.9906513690948486\n",
      "Step: 3842, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.4315052032470703\n",
      "Step: 3843, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.1971595287322998\n",
      "Step: 3844, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.0526134967803955\n",
      "Step: 3845, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 1.7816615104675293\n",
      "Step: 3846, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.3012244701385498\n",
      "Step: 3847, Loss: 0.9378989338874817, Accuracy: 0.96875, Computation time: 1.3357949256896973\n",
      "Step: 3848, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 0.9924838542938232\n",
      "Step: 3849, Loss: 0.915989339351654, Accuracy: 1.0, Computation time: 1.2385473251342773\n",
      "Step: 3850, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 1.3015034198760986\n",
      "Step: 3851, Loss: 0.9159849882125854, Accuracy: 1.0, Computation time: 1.4280431270599365\n",
      "Step: 3852, Loss: 0.9380574226379395, Accuracy: 0.96875, Computation time: 1.5079395771026611\n",
      "Step: 3853, Loss: 0.9569808840751648, Accuracy: 0.9375, Computation time: 1.7240757942199707\n",
      "Step: 3854, Loss: 0.9164474010467529, Accuracy: 1.0, Computation time: 1.7085092067718506\n",
      "Step: 3855, Loss: 0.9159698486328125, Accuracy: 1.0, Computation time: 1.2787079811096191\n",
      "Step: 3856, Loss: 0.9167883396148682, Accuracy: 1.0, Computation time: 2.3432185649871826\n",
      "Step: 3857, Loss: 0.9160659313201904, Accuracy: 1.0, Computation time: 1.6988182067871094\n",
      "Step: 3858, Loss: 0.9161710739135742, Accuracy: 1.0, Computation time: 1.962813377380371\n",
      "Step: 3859, Loss: 0.9160373210906982, Accuracy: 1.0, Computation time: 1.2881340980529785\n",
      "Step: 3860, Loss: 0.9160585999488831, Accuracy: 1.0, Computation time: 1.6476950645446777\n",
      "Step: 3861, Loss: 0.9160277843475342, Accuracy: 1.0, Computation time: 1.4160277843475342\n",
      "Step: 3862, Loss: 0.9376327991485596, Accuracy: 0.96875, Computation time: 1.394580364227295\n",
      "Step: 3863, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 1.4442923069000244\n",
      "Step: 3864, Loss: 0.9375787973403931, Accuracy: 0.96875, Computation time: 1.1046035289764404\n",
      "Step: 3865, Loss: 0.9158914685249329, Accuracy: 1.0, Computation time: 1.3068480491638184\n",
      "Step: 3866, Loss: 0.9169196486473083, Accuracy: 1.0, Computation time: 1.7100443840026855\n",
      "Step: 3867, Loss: 0.9160001873970032, Accuracy: 1.0, Computation time: 1.4219551086425781\n",
      "Step: 3868, Loss: 0.9161596298217773, Accuracy: 1.0, Computation time: 1.5665853023529053\n",
      "Step: 3869, Loss: 0.9170472621917725, Accuracy: 1.0, Computation time: 1.3945343494415283\n",
      "Step: 3870, Loss: 0.9246434569358826, Accuracy: 1.0, Computation time: 2.0921547412872314\n",
      "Step: 3871, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.24570894241333\n",
      "Step: 3872, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.3110110759735107\n",
      "Step: 3873, Loss: 0.9176392555236816, Accuracy: 1.0, Computation time: 1.5906760692596436\n",
      "Step: 3874, Loss: 0.916033923625946, Accuracy: 1.0, Computation time: 1.3249461650848389\n",
      "Step: 3875, Loss: 0.9161158204078674, Accuracy: 1.0, Computation time: 1.3480496406555176\n",
      "Step: 3876, Loss: 0.9380227327346802, Accuracy: 0.96875, Computation time: 1.600386619567871\n",
      "Step: 3877, Loss: 0.9362087249755859, Accuracy: 0.96875, Computation time: 1.6072099208831787\n",
      "Step: 3878, Loss: 0.9163823127746582, Accuracy: 1.0, Computation time: 1.8577330112457275\n",
      "Step: 3879, Loss: 0.9162778258323669, Accuracy: 1.0, Computation time: 1.4949147701263428\n",
      "Step: 3880, Loss: 0.9160454273223877, Accuracy: 1.0, Computation time: 1.9000763893127441\n",
      "Step: 3881, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 1.7150800228118896\n",
      "Step: 3882, Loss: 0.9159643054008484, Accuracy: 1.0, Computation time: 1.519059181213379\n",
      "Step: 3883, Loss: 0.9159507155418396, Accuracy: 1.0, Computation time: 1.471398115158081\n",
      "Step: 3884, Loss: 0.9160868525505066, Accuracy: 1.0, Computation time: 1.2902140617370605\n",
      "Step: 3885, Loss: 0.9159631133079529, Accuracy: 1.0, Computation time: 1.852602481842041\n",
      "Step: 3886, Loss: 0.9378565549850464, Accuracy: 0.96875, Computation time: 1.1668415069580078\n",
      "Step: 3887, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 1.310591459274292\n",
      "Step: 3888, Loss: 0.916070282459259, Accuracy: 1.0, Computation time: 1.33837890625\n",
      "Step: 3889, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 1.3320648670196533\n",
      "Step: 3890, Loss: 0.9375483393669128, Accuracy: 0.96875, Computation time: 1.7688267230987549\n",
      "Step: 3891, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 1.1582279205322266\n",
      "########################\n",
      "Test loss: 1.0738078355789185, Test Accuracy_epoch28: 0.7701261043548584\n",
      "########################\n",
      "Step: 3892, Loss: 0.9198957085609436, Accuracy: 1.0, Computation time: 1.6388640403747559\n",
      "Step: 3893, Loss: 0.9165441989898682, Accuracy: 1.0, Computation time: 1.5617437362670898\n",
      "Step: 3894, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.1296164989471436\n",
      "Step: 3895, Loss: 0.9375384449958801, Accuracy: 0.96875, Computation time: 1.3760254383087158\n",
      "Step: 3896, Loss: 0.9159855842590332, Accuracy: 1.0, Computation time: 1.898808240890503\n",
      "Step: 3897, Loss: 0.9160193204879761, Accuracy: 1.0, Computation time: 1.4576716423034668\n",
      "Step: 3898, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.034520149230957\n",
      "Step: 3899, Loss: 0.9160282611846924, Accuracy: 1.0, Computation time: 1.6896610260009766\n",
      "Step: 3900, Loss: 0.9230669736862183, Accuracy: 1.0, Computation time: 2.4006800651550293\n",
      "Step: 3901, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.2443294525146484\n",
      "Step: 3902, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.558786153793335\n",
      "Step: 3903, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.6293318271636963\n",
      "Step: 3904, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 1.2818117141723633\n",
      "Step: 3905, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.7776250839233398\n",
      "Step: 3906, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.5944252014160156\n",
      "Step: 3907, Loss: 0.9374902248382568, Accuracy: 0.96875, Computation time: 1.3266046047210693\n",
      "Step: 3908, Loss: 0.9161044955253601, Accuracy: 1.0, Computation time: 2.44441294670105\n",
      "Step: 3909, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.6369328498840332\n",
      "Step: 3910, Loss: 0.9159545302391052, Accuracy: 1.0, Computation time: 2.019838333129883\n",
      "Step: 3911, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.7464532852172852\n",
      "Step: 3912, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.3918571472167969\n",
      "Step: 3913, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 1.5768256187438965\n",
      "Step: 3914, Loss: 0.9382696747779846, Accuracy: 0.96875, Computation time: 2.141815185546875\n",
      "Step: 3915, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.3268487453460693\n",
      "Step: 3916, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.4246139526367188\n",
      "Step: 3917, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.9094431400299072\n",
      "Step: 3918, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.6573612689971924\n",
      "Step: 3919, Loss: 0.9159641861915588, Accuracy: 1.0, Computation time: 1.4800574779510498\n",
      "Step: 3920, Loss: 0.9159548282623291, Accuracy: 1.0, Computation time: 1.4643001556396484\n",
      "Step: 3921, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 2.0520830154418945\n",
      "Step: 3922, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.447378396987915\n",
      "Step: 3923, Loss: 0.9373919367790222, Accuracy: 0.96875, Computation time: 1.1392548084259033\n",
      "Step: 3924, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 1.5732038021087646\n",
      "Step: 3925, Loss: 0.9171817302703857, Accuracy: 1.0, Computation time: 1.6196699142456055\n",
      "Step: 3926, Loss: 0.9159437417984009, Accuracy: 1.0, Computation time: 1.2631542682647705\n",
      "Step: 3927, Loss: 0.9159042239189148, Accuracy: 1.0, Computation time: 1.1554584503173828\n",
      "Step: 3928, Loss: 0.9235846400260925, Accuracy: 1.0, Computation time: 2.2364368438720703\n",
      "Step: 3929, Loss: 0.9375209212303162, Accuracy: 0.96875, Computation time: 1.6511626243591309\n",
      "Step: 3930, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 1.1485369205474854\n",
      "Step: 3931, Loss: 0.9374964237213135, Accuracy: 0.96875, Computation time: 1.3106954097747803\n",
      "Step: 3932, Loss: 0.9162797927856445, Accuracy: 1.0, Computation time: 1.2535574436187744\n",
      "Step: 3933, Loss: 0.9160853624343872, Accuracy: 1.0, Computation time: 1.4587445259094238\n",
      "Step: 3934, Loss: 0.920620858669281, Accuracy: 1.0, Computation time: 2.040599822998047\n",
      "Step: 3935, Loss: 0.9161560535430908, Accuracy: 1.0, Computation time: 1.529186487197876\n",
      "Step: 3936, Loss: 0.9378460645675659, Accuracy: 0.96875, Computation time: 1.379387378692627\n",
      "Step: 3937, Loss: 0.9160671830177307, Accuracy: 1.0, Computation time: 1.481717586517334\n",
      "Step: 3938, Loss: 0.916193962097168, Accuracy: 1.0, Computation time: 1.087406873703003\n",
      "Step: 3939, Loss: 0.9165060520172119, Accuracy: 1.0, Computation time: 1.2075650691986084\n",
      "Step: 3940, Loss: 0.9379530549049377, Accuracy: 0.96875, Computation time: 1.7161674499511719\n",
      "Step: 3941, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 1.2053577899932861\n",
      "Step: 3942, Loss: 0.9159967303276062, Accuracy: 1.0, Computation time: 1.2391514778137207\n",
      "Step: 3943, Loss: 0.916069507598877, Accuracy: 1.0, Computation time: 1.395880937576294\n",
      "Step: 3944, Loss: 0.9162757992744446, Accuracy: 1.0, Computation time: 1.4705266952514648\n",
      "Step: 3945, Loss: 0.9160138964653015, Accuracy: 1.0, Computation time: 1.1684412956237793\n",
      "Step: 3946, Loss: 0.9354988932609558, Accuracy: 0.96875, Computation time: 1.5157625675201416\n",
      "Step: 3947, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 1.3228387832641602\n",
      "Step: 3948, Loss: 0.91591477394104, Accuracy: 1.0, Computation time: 1.1684305667877197\n",
      "Step: 3949, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 0.9194915294647217\n",
      "Step: 3950, Loss: 0.9340344667434692, Accuracy: 0.96875, Computation time: 0.9124045372009277\n",
      "Step: 3951, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 0.9228653907775879\n",
      "Step: 3952, Loss: 0.9159447550773621, Accuracy: 1.0, Computation time: 0.8775951862335205\n",
      "Step: 3953, Loss: 0.9159409999847412, Accuracy: 1.0, Computation time: 1.030071496963501\n",
      "Step: 3954, Loss: 0.9190448522567749, Accuracy: 1.0, Computation time: 1.2216687202453613\n",
      "Step: 3955, Loss: 0.9159966111183167, Accuracy: 1.0, Computation time: 0.864290714263916\n",
      "Step: 3956, Loss: 0.9161036610603333, Accuracy: 1.0, Computation time: 1.0561580657958984\n",
      "Step: 3957, Loss: 0.9262636303901672, Accuracy: 0.96875, Computation time: 1.2378251552581787\n",
      "Step: 3958, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 1.0644254684448242\n",
      "Step: 3959, Loss: 0.9159886240959167, Accuracy: 1.0, Computation time: 0.7284224033355713\n",
      "Step: 3960, Loss: 0.9173116087913513, Accuracy: 1.0, Computation time: 1.1935503482818604\n",
      "Step: 3961, Loss: 0.9160813093185425, Accuracy: 1.0, Computation time: 0.9266357421875\n",
      "Step: 3962, Loss: 0.9160532355308533, Accuracy: 1.0, Computation time: 0.8609249591827393\n",
      "Step: 3963, Loss: 0.9167107343673706, Accuracy: 1.0, Computation time: 1.0169243812561035\n",
      "Step: 3964, Loss: 0.9161274433135986, Accuracy: 1.0, Computation time: 0.8515877723693848\n",
      "Step: 3965, Loss: 0.9160900712013245, Accuracy: 1.0, Computation time: 0.8565478324890137\n",
      "Step: 3966, Loss: 0.9168945550918579, Accuracy: 1.0, Computation time: 0.8281829357147217\n",
      "Step: 3967, Loss: 0.9159353971481323, Accuracy: 1.0, Computation time: 0.757779598236084\n",
      "Step: 3968, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 0.7066159248352051\n",
      "Step: 3969, Loss: 0.9160628318786621, Accuracy: 1.0, Computation time: 0.8341066837310791\n",
      "Step: 3970, Loss: 0.9160863161087036, Accuracy: 1.0, Computation time: 1.3605811595916748\n",
      "Step: 3971, Loss: 0.9159873127937317, Accuracy: 1.0, Computation time: 0.7355248928070068\n",
      "Step: 3972, Loss: 0.937407374382019, Accuracy: 0.96875, Computation time: 1.0677015781402588\n",
      "Step: 3973, Loss: 0.9159369468688965, Accuracy: 1.0, Computation time: 0.7972009181976318\n",
      "Step: 3974, Loss: 0.9335212111473083, Accuracy: 0.96875, Computation time: 0.8031892776489258\n",
      "Step: 3975, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 0.7505114078521729\n",
      "Step: 3976, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 0.7444660663604736\n",
      "Step: 3977, Loss: 0.9160496592521667, Accuracy: 1.0, Computation time: 0.7990391254425049\n",
      "Step: 3978, Loss: 0.9160357713699341, Accuracy: 1.0, Computation time: 0.7534205913543701\n",
      "Step: 3979, Loss: 0.9159616827964783, Accuracy: 1.0, Computation time: 0.6932084560394287\n",
      "Step: 3980, Loss: 0.9159871935844421, Accuracy: 1.0, Computation time: 1.274803638458252\n",
      "Step: 3981, Loss: 0.9377169609069824, Accuracy: 0.96875, Computation time: 0.8964629173278809\n",
      "Step: 3982, Loss: 0.915990948677063, Accuracy: 1.0, Computation time: 0.9829776287078857\n",
      "Step: 3983, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 0.8484349250793457\n",
      "Step: 3984, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.1783413887023926\n",
      "Step: 3985, Loss: 0.9159293174743652, Accuracy: 1.0, Computation time: 0.8668901920318604\n",
      "Step: 3986, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 0.8048090934753418\n",
      "Step: 3987, Loss: 0.9373029470443726, Accuracy: 0.96875, Computation time: 0.7800412178039551\n",
      "Step: 3988, Loss: 0.916601836681366, Accuracy: 1.0, Computation time: 0.9797012805938721\n",
      "Step: 3989, Loss: 0.9159801602363586, Accuracy: 1.0, Computation time: 0.8117446899414062\n",
      "Step: 3990, Loss: 0.9161470532417297, Accuracy: 1.0, Computation time: 0.7906417846679688\n",
      "Step: 3991, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 0.7860860824584961\n",
      "Step: 3992, Loss: 0.9371868968009949, Accuracy: 0.96875, Computation time: 1.0651724338531494\n",
      "Step: 3993, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 0.7919340133666992\n",
      "Step: 3994, Loss: 0.9375431537628174, Accuracy: 0.96875, Computation time: 1.068985939025879\n",
      "Step: 3995, Loss: 0.9160836338996887, Accuracy: 1.0, Computation time: 0.8108358383178711\n",
      "Step: 3996, Loss: 0.9171047210693359, Accuracy: 1.0, Computation time: 0.7301998138427734\n",
      "Step: 3997, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 0.8119494915008545\n",
      "Step: 3998, Loss: 0.9160068035125732, Accuracy: 1.0, Computation time: 0.7243373394012451\n",
      "Step: 3999, Loss: 0.9160640835762024, Accuracy: 1.0, Computation time: 1.0626850128173828\n",
      "Step: 4000, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 0.8546912670135498\n",
      "Step: 4001, Loss: 0.916056752204895, Accuracy: 1.0, Computation time: 0.8323640823364258\n",
      "Step: 4002, Loss: 0.915982723236084, Accuracy: 1.0, Computation time: 0.7404873371124268\n",
      "Step: 4003, Loss: 0.9159457683563232, Accuracy: 1.0, Computation time: 0.8295278549194336\n",
      "Step: 4004, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 1.2017357349395752\n",
      "Step: 4005, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 0.8907124996185303\n",
      "Step: 4006, Loss: 0.9160926342010498, Accuracy: 1.0, Computation time: 0.789778470993042\n",
      "Step: 4007, Loss: 0.9159618616104126, Accuracy: 1.0, Computation time: 0.794654369354248\n",
      "Step: 4008, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 0.7235732078552246\n",
      "Step: 4009, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 0.780562162399292\n",
      "Step: 4010, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 0.7623517513275146\n",
      "Step: 4011, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 0.8831968307495117\n",
      "Step: 4012, Loss: 0.9372820854187012, Accuracy: 0.96875, Computation time: 0.8691513538360596\n",
      "Step: 4013, Loss: 0.9347102046012878, Accuracy: 0.96875, Computation time: 1.0577127933502197\n",
      "Step: 4014, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 0.8789889812469482\n",
      "Step: 4015, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 0.7520172595977783\n",
      "Step: 4016, Loss: 0.9159655570983887, Accuracy: 1.0, Computation time: 0.9201939105987549\n",
      "Step: 4017, Loss: 0.9177682399749756, Accuracy: 1.0, Computation time: 1.1300132274627686\n",
      "Step: 4018, Loss: 0.9159966111183167, Accuracy: 1.0, Computation time: 1.121049165725708\n",
      "Step: 4019, Loss: 0.9242265224456787, Accuracy: 1.0, Computation time: 1.1940691471099854\n",
      "Step: 4020, Loss: 0.9588108062744141, Accuracy: 0.9375, Computation time: 0.9089260101318359\n",
      "Step: 4021, Loss: 0.9376571774482727, Accuracy: 0.96875, Computation time: 0.7986011505126953\n",
      "Step: 4022, Loss: 0.9162713885307312, Accuracy: 1.0, Computation time: 0.9402005672454834\n",
      "Step: 4023, Loss: 0.9376962780952454, Accuracy: 0.96875, Computation time: 0.7628738880157471\n",
      "Step: 4024, Loss: 0.916050910949707, Accuracy: 1.0, Computation time: 0.8664305210113525\n",
      "Step: 4025, Loss: 0.9159507751464844, Accuracy: 1.0, Computation time: 0.8718957901000977\n",
      "Step: 4026, Loss: 0.937143862247467, Accuracy: 0.96875, Computation time: 0.8908684253692627\n",
      "Step: 4027, Loss: 0.9159445762634277, Accuracy: 1.0, Computation time: 1.0624992847442627\n",
      "Step: 4028, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 0.8309135437011719\n",
      "Step: 4029, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 0.8460164070129395\n",
      "Step: 4030, Loss: 0.9377017021179199, Accuracy: 0.96875, Computation time: 0.8916704654693604\n",
      "########################\n",
      "Test loss: 1.0701204538345337, Test Accuracy_epoch29: 0.7759457230567932\n",
      "########################\n",
      "Step: 4031, Loss: 0.9160940647125244, Accuracy: 1.0, Computation time: 0.9237158298492432\n",
      "Step: 4032, Loss: 0.9160240292549133, Accuracy: 1.0, Computation time: 0.8050844669342041\n",
      "Step: 4033, Loss: 0.9184852838516235, Accuracy: 1.0, Computation time: 1.1171798706054688\n",
      "Step: 4034, Loss: 0.9159486889839172, Accuracy: 1.0, Computation time: 0.8327927589416504\n",
      "Step: 4035, Loss: 0.9159265756607056, Accuracy: 1.0, Computation time: 1.0343060493469238\n",
      "Step: 4036, Loss: 0.9365916848182678, Accuracy: 0.96875, Computation time: 0.888648509979248\n",
      "Step: 4037, Loss: 0.9160431027412415, Accuracy: 1.0, Computation time: 1.0256266593933105\n",
      "Step: 4038, Loss: 0.916206419467926, Accuracy: 1.0, Computation time: 0.8484513759613037\n",
      "Step: 4039, Loss: 0.9159724712371826, Accuracy: 1.0, Computation time: 1.2749934196472168\n",
      "Step: 4040, Loss: 0.9159523248672485, Accuracy: 1.0, Computation time: 1.016890287399292\n",
      "Step: 4041, Loss: 0.9159748554229736, Accuracy: 1.0, Computation time: 0.952038049697876\n",
      "Step: 4042, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.059410572052002\n",
      "Step: 4043, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 0.8979434967041016\n",
      "Step: 4044, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 0.9342267513275146\n",
      "Step: 4045, Loss: 0.9180436134338379, Accuracy: 1.0, Computation time: 1.3222987651824951\n",
      "Step: 4046, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 1.1598262786865234\n",
      "Step: 4047, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 0.9568729400634766\n",
      "Step: 4048, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.0913488864898682\n",
      "Step: 4049, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 0.8557834625244141\n",
      "Step: 4050, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 1.0532922744750977\n",
      "Step: 4051, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.1174001693725586\n",
      "Step: 4052, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.0829951763153076\n",
      "Step: 4053, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.0957086086273193\n",
      "Step: 4054, Loss: 0.9374966621398926, Accuracy: 0.96875, Computation time: 1.285726547241211\n",
      "Step: 4055, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.2447030544281006\n",
      "Step: 4056, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 1.319042682647705\n",
      "Step: 4057, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.630277395248413\n",
      "Step: 4058, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.0886621475219727\n",
      "Step: 4059, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 0.9450705051422119\n",
      "Step: 4060, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.2647104263305664\n",
      "Step: 4061, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.2553167343139648\n",
      "Step: 4062, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 1.5167362689971924\n",
      "Step: 4063, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.547318458557129\n",
      "Step: 4064, Loss: 0.9160106778144836, Accuracy: 1.0, Computation time: 1.786759376525879\n",
      "Step: 4065, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.17352294921875\n",
      "Step: 4066, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.6287086009979248\n",
      "Step: 4067, Loss: 0.9374270439147949, Accuracy: 0.96875, Computation time: 1.5144739151000977\n",
      "Step: 4068, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.8598387241363525\n",
      "Step: 4069, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.573213815689087\n",
      "Step: 4070, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.499668836593628\n",
      "Step: 4071, Loss: 0.9158398509025574, Accuracy: 1.0, Computation time: 1.389286994934082\n",
      "Step: 4072, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 1.3576695919036865\n",
      "Step: 4073, Loss: 0.9356342554092407, Accuracy: 0.96875, Computation time: 1.829155445098877\n",
      "Step: 4074, Loss: 0.9161044359207153, Accuracy: 1.0, Computation time: 1.5034830570220947\n",
      "Step: 4075, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.1681652069091797\n",
      "Step: 4076, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.410032033920288\n",
      "Step: 4077, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.9242150783538818\n",
      "Step: 4078, Loss: 0.9160666465759277, Accuracy: 1.0, Computation time: 2.4231417179107666\n",
      "Step: 4079, Loss: 0.9585773944854736, Accuracy: 0.9375, Computation time: 1.5334665775299072\n",
      "Step: 4080, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.4473280906677246\n",
      "Step: 4081, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.4971592426300049\n",
      "Step: 4082, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.7307698726654053\n",
      "Step: 4083, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 1.3115415573120117\n",
      "Step: 4084, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.7565865516662598\n",
      "Step: 4085, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 1.6551685333251953\n",
      "Step: 4086, Loss: 0.9306831359863281, Accuracy: 0.96875, Computation time: 1.271303653717041\n",
      "Step: 4087, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 1.2720706462860107\n",
      "Step: 4088, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.3712153434753418\n",
      "Step: 4089, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.2581901550292969\n",
      "Step: 4090, Loss: 0.937581479549408, Accuracy: 0.96875, Computation time: 1.5654056072235107\n",
      "Step: 4091, Loss: 0.915949821472168, Accuracy: 1.0, Computation time: 1.2683212757110596\n",
      "Step: 4092, Loss: 0.9376590847969055, Accuracy: 0.96875, Computation time: 1.4797089099884033\n",
      "Step: 4093, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.4991183280944824\n",
      "Step: 4094, Loss: 0.9172316193580627, Accuracy: 1.0, Computation time: 2.1813337802886963\n",
      "Step: 4095, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.499929666519165\n",
      "Step: 4096, Loss: 0.9158948063850403, Accuracy: 1.0, Computation time: 1.3032150268554688\n",
      "Step: 4097, Loss: 0.9160261154174805, Accuracy: 1.0, Computation time: 1.4405970573425293\n",
      "Step: 4098, Loss: 0.9160231947898865, Accuracy: 1.0, Computation time: 1.591247797012329\n",
      "Step: 4099, Loss: 0.9160196781158447, Accuracy: 1.0, Computation time: 1.9723997116088867\n",
      "Step: 4100, Loss: 0.9159300327301025, Accuracy: 1.0, Computation time: 1.365053415298462\n",
      "Step: 4101, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 1.8179774284362793\n",
      "Step: 4102, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.530430555343628\n",
      "Step: 4103, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.3862123489379883\n",
      "Step: 4104, Loss: 0.937663197517395, Accuracy: 0.96875, Computation time: 1.305295467376709\n",
      "Step: 4105, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.2829246520996094\n",
      "Step: 4106, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.645277500152588\n",
      "Step: 4107, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.5401062965393066\n",
      "Step: 4108, Loss: 0.9376516938209534, Accuracy: 0.96875, Computation time: 1.3470985889434814\n",
      "Step: 4109, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.2143149375915527\n",
      "Step: 4110, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.6123642921447754\n",
      "Step: 4111, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.4064056873321533\n",
      "Step: 4112, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 1.8616886138916016\n",
      "Step: 4113, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.51149320602417\n",
      "Step: 4114, Loss: 0.9375673532485962, Accuracy: 0.96875, Computation time: 1.5374462604522705\n",
      "Step: 4115, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.375138759613037\n",
      "Step: 4116, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.2979755401611328\n",
      "Step: 4117, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.684739589691162\n",
      "Step: 4118, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 1.274209976196289\n",
      "Step: 4119, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.4490373134613037\n",
      "Step: 4120, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.3562884330749512\n",
      "Step: 4121, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.3252642154693604\n",
      "Step: 4122, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 1.3631927967071533\n",
      "Step: 4123, Loss: 0.9168985486030579, Accuracy: 1.0, Computation time: 1.261965274810791\n",
      "Step: 4124, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.3678650856018066\n",
      "Step: 4125, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.655409812927246\n",
      "Step: 4126, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 1.0958807468414307\n",
      "Step: 4127, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 1.7178409099578857\n",
      "Step: 4128, Loss: 0.9158942699432373, Accuracy: 1.0, Computation time: 1.8188199996948242\n",
      "Step: 4129, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.2978711128234863\n",
      "Step: 4130, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 1.1241354942321777\n",
      "Step: 4131, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 1.1123507022857666\n",
      "Step: 4132, Loss: 0.9162169098854065, Accuracy: 1.0, Computation time: 1.8084487915039062\n",
      "Step: 4133, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 1.2057509422302246\n",
      "Step: 4134, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.2382900714874268\n",
      "Step: 4135, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.099256992340088\n",
      "Step: 4136, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.36197829246521\n",
      "Step: 4137, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.3476428985595703\n",
      "Step: 4138, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.1584835052490234\n",
      "Step: 4139, Loss: 0.9243219494819641, Accuracy: 1.0, Computation time: 2.4101343154907227\n",
      "Step: 4140, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.0757372379302979\n",
      "Step: 4141, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 0.9855482578277588\n",
      "Step: 4142, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.0264711380004883\n",
      "Step: 4143, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.1083564758300781\n",
      "Step: 4144, Loss: 0.9375535249710083, Accuracy: 0.96875, Computation time: 1.0205707550048828\n",
      "Step: 4145, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.2599689960479736\n",
      "Step: 4146, Loss: 0.9159623980522156, Accuracy: 1.0, Computation time: 1.0049927234649658\n",
      "Step: 4147, Loss: 0.9158430695533752, Accuracy: 1.0, Computation time: 1.0555026531219482\n",
      "Step: 4148, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 0.9569952487945557\n",
      "Step: 4149, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 0.911780834197998\n",
      "Step: 4150, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 0.9380588531494141\n",
      "Step: 4151, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.014528512954712\n",
      "Step: 4152, Loss: 0.925504744052887, Accuracy: 0.96875, Computation time: 1.5303313732147217\n",
      "Step: 4153, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.0708537101745605\n",
      "Step: 4154, Loss: 0.934760332107544, Accuracy: 0.96875, Computation time: 1.0941119194030762\n",
      "Step: 4155, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.009857177734375\n",
      "Step: 4156, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.117725133895874\n",
      "Step: 4157, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 0.8837828636169434\n",
      "Step: 4158, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 0.8632602691650391\n",
      "Step: 4159, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 0.9769697189331055\n",
      "Step: 4160, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.0587255954742432\n",
      "Step: 4161, Loss: 0.93773353099823, Accuracy: 0.96875, Computation time: 1.074664831161499\n",
      "Step: 4162, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 0.8617870807647705\n",
      "Step: 4163, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 0.858090877532959\n",
      "Step: 4164, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.0205457210540771\n",
      "Step: 4165, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 0.8870820999145508\n",
      "Step: 4166, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 0.9728987216949463\n",
      "Step: 4167, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 0.9827175140380859\n",
      "Step: 4168, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 0.8834078311920166\n",
      "Step: 4169, Loss: 0.9326840043067932, Accuracy: 0.96875, Computation time: 1.2818684577941895\n",
      "########################\n",
      "Test loss: 1.0701102018356323, Test Accuracy_epoch30: 0.7769156098365784\n",
      "########################\n",
      "Step: 4170, Loss: 0.9591723084449768, Accuracy: 0.9375, Computation time: 0.9625425338745117\n",
      "Step: 4171, Loss: 0.9159124493598938, Accuracy: 1.0, Computation time: 0.8256573677062988\n",
      "Step: 4172, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 0.9138286113739014\n",
      "Step: 4173, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 0.8946621417999268\n",
      "Step: 4174, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 0.8205718994140625\n",
      "Step: 4175, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 0.8690340518951416\n",
      "Step: 4176, Loss: 0.9376205205917358, Accuracy: 0.96875, Computation time: 0.8648061752319336\n",
      "Step: 4177, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 0.9137122631072998\n",
      "Step: 4178, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 0.7838859558105469\n",
      "Step: 4179, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 0.8755629062652588\n",
      "Step: 4180, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 0.8319082260131836\n",
      "Step: 4181, Loss: 0.9354119300842285, Accuracy: 0.96875, Computation time: 0.9840240478515625\n",
      "Step: 4182, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.0514285564422607\n",
      "Step: 4183, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 0.9285664558410645\n",
      "Step: 4184, Loss: 0.9167171716690063, Accuracy: 1.0, Computation time: 1.2384774684906006\n",
      "Step: 4185, Loss: 0.9536739587783813, Accuracy: 0.9375, Computation time: 0.9134542942047119\n",
      "Step: 4186, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 0.8341789245605469\n",
      "Step: 4187, Loss: 0.9159830808639526, Accuracy: 1.0, Computation time: 0.839993953704834\n",
      "Step: 4188, Loss: 0.9159404039382935, Accuracy: 1.0, Computation time: 0.8431811332702637\n",
      "Step: 4189, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 0.8777511119842529\n",
      "Step: 4190, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 0.8711328506469727\n",
      "Step: 4191, Loss: 0.9159267544746399, Accuracy: 1.0, Computation time: 0.9292535781860352\n",
      "Step: 4192, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 0.9475216865539551\n",
      "Step: 4193, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 0.9900896549224854\n",
      "Step: 4194, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 0.8377871513366699\n",
      "Step: 4195, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 0.9728388786315918\n",
      "Step: 4196, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 1.110846757888794\n",
      "Step: 4197, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 0.8596587181091309\n",
      "Step: 4198, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 0.8205499649047852\n",
      "Step: 4199, Loss: 0.9190881848335266, Accuracy: 1.0, Computation time: 1.0359225273132324\n",
      "Step: 4200, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.9464378356933594\n",
      "Step: 4201, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 0.8624622821807861\n",
      "Step: 4202, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 0.7682521343231201\n",
      "Step: 4203, Loss: 0.9160175323486328, Accuracy: 1.0, Computation time: 0.9018046855926514\n",
      "Step: 4204, Loss: 0.9361483454704285, Accuracy: 0.96875, Computation time: 1.0029773712158203\n",
      "Step: 4205, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 0.8414392471313477\n",
      "Step: 4206, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 0.8763394355773926\n",
      "Step: 4207, Loss: 0.9585351347923279, Accuracy: 0.9375, Computation time: 0.9879534244537354\n",
      "Step: 4208, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 0.9522743225097656\n",
      "Step: 4209, Loss: 0.916344940662384, Accuracy: 1.0, Computation time: 1.203719139099121\n",
      "Step: 4210, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.0014808177947998\n",
      "Step: 4211, Loss: 0.9374793171882629, Accuracy: 0.96875, Computation time: 1.303882360458374\n",
      "Step: 4212, Loss: 0.9160560369491577, Accuracy: 1.0, Computation time: 0.9862079620361328\n",
      "Step: 4213, Loss: 0.9373984336853027, Accuracy: 0.96875, Computation time: 0.9046573638916016\n",
      "Step: 4214, Loss: 0.9376010298728943, Accuracy: 0.96875, Computation time: 1.0242066383361816\n",
      "Step: 4215, Loss: 0.9160279631614685, Accuracy: 1.0, Computation time: 1.137058973312378\n",
      "Step: 4216, Loss: 0.9181896448135376, Accuracy: 1.0, Computation time: 1.5911905765533447\n",
      "Step: 4217, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 0.9347212314605713\n",
      "Step: 4218, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.1319770812988281\n",
      "Step: 4219, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 0.9955167770385742\n",
      "Step: 4220, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 0.8784515857696533\n",
      "Step: 4221, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 1.174936056137085\n",
      "Step: 4222, Loss: 0.9159405827522278, Accuracy: 1.0, Computation time: 0.9421637058258057\n",
      "Step: 4223, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.070202350616455\n",
      "Step: 4224, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.1726362705230713\n",
      "Step: 4225, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 0.966832160949707\n",
      "Step: 4226, Loss: 0.9160239696502686, Accuracy: 1.0, Computation time: 0.9958169460296631\n",
      "Step: 4227, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 1.0978543758392334\n",
      "Step: 4228, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 0.8860945701599121\n",
      "Step: 4229, Loss: 0.9159256219863892, Accuracy: 1.0, Computation time: 1.080012559890747\n",
      "Step: 4230, Loss: 0.9160186648368835, Accuracy: 1.0, Computation time: 0.9730863571166992\n",
      "Step: 4231, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.0389735698699951\n",
      "Step: 4232, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 0.9854469299316406\n",
      "Step: 4233, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.264009952545166\n",
      "Step: 4234, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.061058759689331\n",
      "Step: 4235, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 0.9152531623840332\n",
      "Step: 4236, Loss: 0.9160698056221008, Accuracy: 1.0, Computation time: 1.0582952499389648\n",
      "Step: 4237, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 0.9184584617614746\n",
      "Step: 4238, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.262399435043335\n",
      "Step: 4239, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.8804094791412354\n",
      "Step: 4240, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 0.9678120613098145\n",
      "Step: 4241, Loss: 0.9370695948600769, Accuracy: 0.96875, Computation time: 0.8721599578857422\n",
      "Step: 4242, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 1.217761754989624\n",
      "Step: 4243, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 0.7891077995300293\n",
      "Step: 4244, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 0.9628043174743652\n",
      "Step: 4245, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 0.9889957904815674\n",
      "Step: 4246, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 0.9034278392791748\n",
      "Step: 4247, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.123671531677246\n",
      "Step: 4248, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 1.3423359394073486\n",
      "Step: 4249, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.053135871887207\n",
      "Step: 4250, Loss: 0.9374920129776001, Accuracy: 0.96875, Computation time: 0.9301571846008301\n",
      "Step: 4251, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 0.8914103507995605\n",
      "Step: 4252, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.0301649570465088\n",
      "Step: 4253, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 0.8856770992279053\n",
      "Step: 4254, Loss: 0.9181867837905884, Accuracy: 1.0, Computation time: 1.48402738571167\n",
      "Step: 4255, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 0.9083433151245117\n",
      "Step: 4256, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.951270580291748\n",
      "Step: 4257, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 0.8374588489532471\n",
      "Step: 4258, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.265557050704956\n",
      "Step: 4259, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 0.8806490898132324\n",
      "Step: 4260, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.1464307308197021\n",
      "Step: 4261, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.145169734954834\n",
      "Step: 4262, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.054558515548706\n",
      "Step: 4263, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.0415260791778564\n",
      "Step: 4264, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.044135332107544\n",
      "Step: 4265, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 0.8804090023040771\n",
      "Step: 4266, Loss: 0.9158390164375305, Accuracy: 1.0, Computation time: 1.0009942054748535\n",
      "Step: 4267, Loss: 0.9389697909355164, Accuracy: 0.96875, Computation time: 1.3710637092590332\n",
      "Step: 4268, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.1459391117095947\n",
      "Step: 4269, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.1599886417388916\n",
      "Step: 4270, Loss: 0.9375529289245605, Accuracy: 0.96875, Computation time: 1.1292293071746826\n",
      "Step: 4271, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.145658254623413\n",
      "Step: 4272, Loss: 0.9375072121620178, Accuracy: 0.96875, Computation time: 1.0231893062591553\n",
      "Step: 4273, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.0037527084350586\n",
      "Step: 4274, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.2118525505065918\n",
      "Step: 4275, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.2544300556182861\n",
      "Step: 4276, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.0450849533081055\n",
      "Step: 4277, Loss: 0.9158543944358826, Accuracy: 1.0, Computation time: 0.9444179534912109\n",
      "Step: 4278, Loss: 0.9358444213867188, Accuracy: 0.96875, Computation time: 1.2276551723480225\n",
      "Step: 4279, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.0759878158569336\n",
      "Step: 4280, Loss: 0.9338774085044861, Accuracy: 0.96875, Computation time: 1.1911797523498535\n",
      "Step: 4281, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.3074588775634766\n",
      "Step: 4282, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 0.9629006385803223\n",
      "Step: 4283, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 0.9123904705047607\n",
      "Step: 4284, Loss: 0.9160245656967163, Accuracy: 1.0, Computation time: 1.217822790145874\n",
      "Step: 4285, Loss: 0.9374929070472717, Accuracy: 0.96875, Computation time: 0.9989407062530518\n",
      "Step: 4286, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 0.9734539985656738\n",
      "Step: 4287, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.219235897064209\n",
      "Step: 4288, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.092996597290039\n",
      "Step: 4289, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.0533397197723389\n",
      "Step: 4290, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.4190406799316406\n",
      "Step: 4291, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.1519105434417725\n",
      "Step: 4292, Loss: 0.9160870313644409, Accuracy: 1.0, Computation time: 1.4565916061401367\n",
      "Step: 4293, Loss: 0.9375953078269958, Accuracy: 0.96875, Computation time: 1.3790268898010254\n",
      "Step: 4294, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.14701247215271\n",
      "Step: 4295, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.463371992111206\n",
      "Step: 4296, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 1.6706957817077637\n",
      "Step: 4297, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.123767375946045\n",
      "Step: 4298, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.314164400100708\n",
      "Step: 4299, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 1.7274367809295654\n",
      "Step: 4300, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.1524384021759033\n",
      "Step: 4301, Loss: 0.9374740123748779, Accuracy: 0.96875, Computation time: 1.2204325199127197\n",
      "Step: 4302, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.613123893737793\n",
      "Step: 4303, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.264695644378662\n",
      "Step: 4304, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.1039066314697266\n",
      "Step: 4305, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.348597764968872\n",
      "Step: 4306, Loss: 0.9363024234771729, Accuracy: 0.96875, Computation time: 1.4459879398345947\n",
      "Step: 4307, Loss: 0.9162266254425049, Accuracy: 1.0, Computation time: 1.5257790088653564\n",
      "Step: 4308, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.727616786956787\n",
      "########################\n",
      "Test loss: 1.0725607872009277, Test Accuracy_epoch31: 0.7720659971237183\n",
      "########################\n",
      "Step: 4309, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.21974778175354\n",
      "Step: 4310, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.3245506286621094\n",
      "Step: 4311, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.7067434787750244\n",
      "Step: 4312, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.440016508102417\n",
      "Step: 4313, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.3551862239837646\n",
      "Step: 4314, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.0030419826507568\n",
      "Step: 4315, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.070523977279663\n",
      "Step: 4316, Loss: 0.9158363342285156, Accuracy: 1.0, Computation time: 1.3209240436553955\n",
      "Step: 4317, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 1.8307774066925049\n",
      "Step: 4318, Loss: 0.9165902733802795, Accuracy: 1.0, Computation time: 1.4755222797393799\n",
      "Step: 4319, Loss: 0.9160017967224121, Accuracy: 1.0, Computation time: 1.2926971912384033\n",
      "Step: 4320, Loss: 0.9158470034599304, Accuracy: 1.0, Computation time: 1.150925874710083\n",
      "Step: 4321, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 1.3076589107513428\n",
      "Step: 4322, Loss: 0.9375519156455994, Accuracy: 0.96875, Computation time: 1.0798871517181396\n",
      "Step: 4323, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.2170732021331787\n",
      "Step: 4324, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.157348394393921\n",
      "Step: 4325, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.0748951435089111\n",
      "Step: 4326, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.7131493091583252\n",
      "Step: 4327, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.4765386581420898\n",
      "Step: 4328, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.3956546783447266\n",
      "Step: 4329, Loss: 0.9180856347084045, Accuracy: 1.0, Computation time: 1.145585536956787\n",
      "Step: 4330, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.9479436874389648\n",
      "Step: 4331, Loss: 0.9162676334381104, Accuracy: 1.0, Computation time: 1.4767343997955322\n",
      "Step: 4332, Loss: 0.9158603549003601, Accuracy: 1.0, Computation time: 1.1924402713775635\n",
      "Step: 4333, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.3463191986083984\n",
      "Step: 4334, Loss: 0.9158821105957031, Accuracy: 1.0, Computation time: 2.187178373336792\n",
      "Step: 4335, Loss: 0.9374306797981262, Accuracy: 0.96875, Computation time: 1.0861659049987793\n",
      "Step: 4336, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 1.4377505779266357\n",
      "Step: 4337, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 1.0892934799194336\n",
      "Step: 4338, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.220529556274414\n",
      "Step: 4339, Loss: 0.9374226927757263, Accuracy: 0.96875, Computation time: 1.7227015495300293\n",
      "Step: 4340, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.1564602851867676\n",
      "Step: 4341, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.348667860031128\n",
      "Step: 4342, Loss: 0.9171382188796997, Accuracy: 1.0, Computation time: 1.0964117050170898\n",
      "Step: 4343, Loss: 0.9159005284309387, Accuracy: 1.0, Computation time: 1.1871974468231201\n",
      "Step: 4344, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.5847554206848145\n",
      "Step: 4345, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.1230549812316895\n",
      "Step: 4346, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.3222403526306152\n",
      "Step: 4347, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.3101332187652588\n",
      "Step: 4348, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.2361118793487549\n",
      "Step: 4349, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.2630293369293213\n",
      "Step: 4350, Loss: 0.9364176392555237, Accuracy: 0.96875, Computation time: 1.4931907653808594\n",
      "Step: 4351, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.4735262393951416\n",
      "Step: 4352, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 0.9291515350341797\n",
      "Step: 4353, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.9309258460998535\n",
      "Step: 4354, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.0319597721099854\n",
      "Step: 4355, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.192631721496582\n",
      "Step: 4356, Loss: 0.9159222841262817, Accuracy: 1.0, Computation time: 1.4351892471313477\n",
      "Step: 4357, Loss: 0.937584400177002, Accuracy: 0.96875, Computation time: 1.7547781467437744\n",
      "Step: 4358, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.1149330139160156\n",
      "Step: 4359, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.0185658931732178\n",
      "Step: 4360, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.0203423500061035\n",
      "Step: 4361, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 1.2694146633148193\n",
      "Step: 4362, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.1995818614959717\n",
      "Step: 4363, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.5863685607910156\n",
      "Step: 4364, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.2733805179595947\n",
      "Step: 4365, Loss: 0.9159347414970398, Accuracy: 1.0, Computation time: 1.5582926273345947\n",
      "Step: 4366, Loss: 0.9159307479858398, Accuracy: 1.0, Computation time: 1.11765718460083\n",
      "Step: 4367, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.249671459197998\n",
      "Step: 4368, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 1.0709502696990967\n",
      "Step: 4369, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.1411008834838867\n",
      "Step: 4370, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.1250300407409668\n",
      "Step: 4371, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.4311091899871826\n",
      "Step: 4372, Loss: 0.9175502061843872, Accuracy: 1.0, Computation time: 1.8981685638427734\n",
      "Step: 4373, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 1.4394845962524414\n",
      "Step: 4374, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.0209720134735107\n",
      "Step: 4375, Loss: 0.9374354481697083, Accuracy: 0.96875, Computation time: 1.2198739051818848\n",
      "Step: 4376, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.3406398296356201\n",
      "Step: 4377, Loss: 0.9163023233413696, Accuracy: 1.0, Computation time: 1.504145860671997\n",
      "Step: 4378, Loss: 0.9375211596488953, Accuracy: 0.96875, Computation time: 1.2338988780975342\n",
      "Step: 4379, Loss: 0.9359868764877319, Accuracy: 0.96875, Computation time: 1.120568037033081\n",
      "Step: 4380, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.5472357273101807\n",
      "Step: 4381, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.296077013015747\n",
      "Step: 4382, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.1398530006408691\n",
      "Step: 4383, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.0073401927947998\n",
      "Step: 4384, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.2810673713684082\n",
      "Step: 4385, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 1.4010493755340576\n",
      "Step: 4386, Loss: 0.9328547716140747, Accuracy: 0.96875, Computation time: 2.4464924335479736\n",
      "Step: 4387, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.3784000873565674\n",
      "Step: 4388, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.1056454181671143\n",
      "Step: 4389, Loss: 0.9399084448814392, Accuracy: 0.96875, Computation time: 1.6031105518341064\n",
      "Step: 4390, Loss: 0.916037917137146, Accuracy: 1.0, Computation time: 1.155855655670166\n",
      "Step: 4391, Loss: 0.9169702529907227, Accuracy: 1.0, Computation time: 1.3262035846710205\n",
      "Step: 4392, Loss: 0.9165998101234436, Accuracy: 1.0, Computation time: 1.4219248294830322\n",
      "Step: 4393, Loss: 0.9159941077232361, Accuracy: 1.0, Computation time: 1.1505703926086426\n",
      "Step: 4394, Loss: 0.9160934090614319, Accuracy: 1.0, Computation time: 1.116532564163208\n",
      "Step: 4395, Loss: 0.9165956377983093, Accuracy: 1.0, Computation time: 1.547753095626831\n",
      "Step: 4396, Loss: 0.9379270076751709, Accuracy: 0.96875, Computation time: 1.4212870597839355\n",
      "Step: 4397, Loss: 0.9176381826400757, Accuracy: 1.0, Computation time: 1.5664591789245605\n",
      "Step: 4398, Loss: 0.9160764813423157, Accuracy: 1.0, Computation time: 1.0594589710235596\n",
      "Step: 4399, Loss: 0.9165615439414978, Accuracy: 1.0, Computation time: 1.0598475933074951\n",
      "Step: 4400, Loss: 0.9163561463356018, Accuracy: 1.0, Computation time: 1.1389727592468262\n",
      "Step: 4401, Loss: 0.9167992472648621, Accuracy: 1.0, Computation time: 1.4162976741790771\n",
      "Step: 4402, Loss: 0.9379182457923889, Accuracy: 0.96875, Computation time: 1.9252028465270996\n",
      "Step: 4403, Loss: 0.9163199663162231, Accuracy: 1.0, Computation time: 1.0802550315856934\n",
      "Step: 4404, Loss: 0.915945291519165, Accuracy: 1.0, Computation time: 1.0054469108581543\n",
      "Step: 4405, Loss: 0.9160636067390442, Accuracy: 1.0, Computation time: 1.500932216644287\n",
      "Step: 4406, Loss: 0.9159724712371826, Accuracy: 1.0, Computation time: 1.0913934707641602\n",
      "Step: 4407, Loss: 0.9159069657325745, Accuracy: 1.0, Computation time: 1.4737319946289062\n",
      "Step: 4408, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.3820910453796387\n",
      "Step: 4409, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.3477280139923096\n",
      "Step: 4410, Loss: 0.9160025715827942, Accuracy: 1.0, Computation time: 1.5783183574676514\n",
      "Step: 4411, Loss: 0.9159528017044067, Accuracy: 1.0, Computation time: 1.473304271697998\n",
      "Step: 4412, Loss: 0.9163793921470642, Accuracy: 1.0, Computation time: 1.7641277313232422\n",
      "Step: 4413, Loss: 0.9159374833106995, Accuracy: 1.0, Computation time: 0.8877773284912109\n",
      "Step: 4414, Loss: 0.9161777496337891, Accuracy: 1.0, Computation time: 1.1834566593170166\n",
      "Step: 4415, Loss: 0.9162726998329163, Accuracy: 1.0, Computation time: 1.820852518081665\n",
      "Step: 4416, Loss: 0.915962278842926, Accuracy: 1.0, Computation time: 1.8950657844543457\n",
      "Step: 4417, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.133924961090088\n",
      "Step: 4418, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.3620235919952393\n",
      "Step: 4419, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.2020142078399658\n",
      "Step: 4420, Loss: 0.9363316297531128, Accuracy: 0.96875, Computation time: 1.7367031574249268\n",
      "Step: 4421, Loss: 0.9159757494926453, Accuracy: 1.0, Computation time: 1.121037483215332\n",
      "Step: 4422, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 1.044058084487915\n",
      "Step: 4423, Loss: 0.9592050313949585, Accuracy: 0.9375, Computation time: 1.4856605529785156\n",
      "Step: 4424, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 0.972151517868042\n",
      "Step: 4425, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.1353988647460938\n",
      "Step: 4426, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 1.5642576217651367\n",
      "Step: 4427, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.207033395767212\n",
      "Step: 4428, Loss: 0.9159371256828308, Accuracy: 1.0, Computation time: 0.9617266654968262\n",
      "Step: 4429, Loss: 0.9158379435539246, Accuracy: 1.0, Computation time: 1.0563642978668213\n",
      "Step: 4430, Loss: 0.9375351071357727, Accuracy: 0.96875, Computation time: 1.4638853073120117\n",
      "Step: 4431, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.381786584854126\n",
      "Step: 4432, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.0277915000915527\n",
      "Step: 4433, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.0288951396942139\n",
      "Step: 4434, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.1729135513305664\n",
      "Step: 4435, Loss: 0.9160329699516296, Accuracy: 1.0, Computation time: 1.392826795578003\n",
      "Step: 4436, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.228090524673462\n",
      "Step: 4437, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.3507537841796875\n",
      "Step: 4438, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 1.1546642780303955\n",
      "Step: 4439, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.1383652687072754\n",
      "Step: 4440, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 0.9732260704040527\n",
      "Step: 4441, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.0482704639434814\n",
      "Step: 4442, Loss: 0.9210500717163086, Accuracy: 1.0, Computation time: 1.495182991027832\n",
      "Step: 4443, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.0406992435455322\n",
      "Step: 4444, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 0.9051644802093506\n",
      "Step: 4445, Loss: 0.9367758631706238, Accuracy: 0.96875, Computation time: 1.1919844150543213\n",
      "Step: 4446, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.030932903289795\n",
      "########################\n",
      "Test loss: 1.0725277662277222, Test Accuracy_epoch32: 0.7730358839035034\n",
      "########################\n",
      "Step: 4447, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.1428864002227783\n",
      "Step: 4448, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 0.9960756301879883\n",
      "Step: 4449, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 0.8615179061889648\n",
      "Step: 4450, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.3999667167663574\n",
      "Step: 4451, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.2455506324768066\n",
      "Step: 4452, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.1569948196411133\n",
      "Step: 4453, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 0.9737215042114258\n",
      "Step: 4454, Loss: 0.9375792145729065, Accuracy: 0.96875, Computation time: 1.238666296005249\n",
      "Step: 4455, Loss: 0.9159932136535645, Accuracy: 1.0, Computation time: 1.8645765781402588\n",
      "Step: 4456, Loss: 0.9170054197311401, Accuracy: 1.0, Computation time: 1.8949406147003174\n",
      "Step: 4457, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.1172997951507568\n",
      "Step: 4458, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.0866999626159668\n",
      "Step: 4459, Loss: 0.9180538654327393, Accuracy: 1.0, Computation time: 1.2310888767242432\n",
      "Step: 4460, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 1.6469573974609375\n",
      "Step: 4461, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 1.0483384132385254\n",
      "Step: 4462, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 0.9743607044219971\n",
      "Step: 4463, Loss: 0.9375844597816467, Accuracy: 0.96875, Computation time: 1.6670753955841064\n",
      "Step: 4464, Loss: 0.9167010188102722, Accuracy: 1.0, Computation time: 2.4075896739959717\n",
      "Step: 4465, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 1.535956621170044\n",
      "Step: 4466, Loss: 0.9374972581863403, Accuracy: 0.96875, Computation time: 1.2172434329986572\n",
      "Step: 4467, Loss: 0.9375526309013367, Accuracy: 0.96875, Computation time: 1.164982557296753\n",
      "Step: 4468, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.1194193363189697\n",
      "Step: 4469, Loss: 0.9375142455101013, Accuracy: 0.96875, Computation time: 1.227449655532837\n",
      "Step: 4470, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.067755937576294\n",
      "Step: 4471, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.1598880290985107\n",
      "Step: 4472, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.0184993743896484\n",
      "Step: 4473, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.6044597625732422\n",
      "Step: 4474, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.2740280628204346\n",
      "Step: 4475, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.0821022987365723\n",
      "Step: 4476, Loss: 0.915834367275238, Accuracy: 1.0, Computation time: 0.9983265399932861\n",
      "Step: 4477, Loss: 0.9158340692520142, Accuracy: 1.0, Computation time: 0.8686094284057617\n",
      "Step: 4478, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.140174388885498\n",
      "Step: 4479, Loss: 0.9375953674316406, Accuracy: 0.96875, Computation time: 1.114241361618042\n",
      "Step: 4480, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.320648431777954\n",
      "Step: 4481, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 0.9632384777069092\n",
      "Step: 4482, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.0817267894744873\n",
      "Step: 4483, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 1.125943660736084\n",
      "Step: 4484, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.3362374305725098\n",
      "Step: 4485, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.5392601490020752\n",
      "Step: 4486, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.0755424499511719\n",
      "Step: 4487, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.0767056941986084\n",
      "Step: 4488, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 0.9621613025665283\n",
      "Step: 4489, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.1602716445922852\n",
      "Step: 4490, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.0241339206695557\n",
      "Step: 4491, Loss: 0.9158368110656738, Accuracy: 1.0, Computation time: 0.9405899047851562\n",
      "Step: 4492, Loss: 0.9158368110656738, Accuracy: 1.0, Computation time: 1.2696306705474854\n",
      "Step: 4493, Loss: 0.9158347249031067, Accuracy: 1.0, Computation time: 0.9575903415679932\n",
      "Step: 4494, Loss: 0.9158351421356201, Accuracy: 1.0, Computation time: 1.0320608615875244\n",
      "Step: 4495, Loss: 0.9374411106109619, Accuracy: 0.96875, Computation time: 0.9249110221862793\n",
      "Step: 4496, Loss: 0.937523603439331, Accuracy: 0.96875, Computation time: 1.2177984714508057\n",
      "Step: 4497, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.115248203277588\n",
      "Step: 4498, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.9809606075286865\n",
      "Step: 4499, Loss: 0.9376186728477478, Accuracy: 0.96875, Computation time: 1.1241776943206787\n",
      "Step: 4500, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.1143815517425537\n",
      "Step: 4501, Loss: 0.915987491607666, Accuracy: 1.0, Computation time: 1.3053252696990967\n",
      "Step: 4502, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.0811941623687744\n",
      "Step: 4503, Loss: 0.9375274777412415, Accuracy: 0.96875, Computation time: 0.9657618999481201\n",
      "Step: 4504, Loss: 0.9160277843475342, Accuracy: 1.0, Computation time: 1.2446174621582031\n",
      "Step: 4505, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.3992033004760742\n",
      "Step: 4506, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.9944930076599121\n",
      "Step: 4507, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 0.8921749591827393\n",
      "Step: 4508, Loss: 0.9158354997634888, Accuracy: 1.0, Computation time: 0.9788177013397217\n",
      "Step: 4509, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.1798148155212402\n",
      "Step: 4510, Loss: 0.9161468744277954, Accuracy: 1.0, Computation time: 2.3006625175476074\n",
      "Step: 4511, Loss: 0.9375025033950806, Accuracy: 0.96875, Computation time: 1.277219533920288\n",
      "Step: 4512, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.2814738750457764\n",
      "Step: 4513, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.0005853176116943\n",
      "Step: 4514, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 0.9461078643798828\n",
      "Step: 4515, Loss: 0.9162490963935852, Accuracy: 1.0, Computation time: 1.1860904693603516\n",
      "Step: 4516, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 0.9620637893676758\n",
      "Step: 4517, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 0.9469397068023682\n",
      "Step: 4518, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 1.3092608451843262\n",
      "Step: 4519, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.5739121437072754\n",
      "Step: 4520, Loss: 0.9375324845314026, Accuracy: 0.96875, Computation time: 0.8847405910491943\n",
      "Step: 4521, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.5907058715820312\n",
      "Step: 4522, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.0870847702026367\n",
      "Step: 4523, Loss: 0.9363774657249451, Accuracy: 0.96875, Computation time: 0.9300334453582764\n",
      "Step: 4524, Loss: 0.9158335328102112, Accuracy: 1.0, Computation time: 1.0760934352874756\n",
      "Step: 4525, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 1.8845164775848389\n",
      "Step: 4526, Loss: 0.9159850478172302, Accuracy: 1.0, Computation time: 1.1734707355499268\n",
      "Step: 4527, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 1.1741702556610107\n",
      "Step: 4528, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.1589891910552979\n",
      "Step: 4529, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.4499132633209229\n",
      "Step: 4530, Loss: 0.9159023761749268, Accuracy: 1.0, Computation time: 1.2448549270629883\n",
      "Step: 4531, Loss: 0.9371981024742126, Accuracy: 0.96875, Computation time: 2.0994369983673096\n",
      "Step: 4532, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 2.0280137062072754\n",
      "Step: 4533, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.483433723449707\n",
      "Step: 4534, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.0863778591156006\n",
      "Step: 4535, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 0.876349925994873\n",
      "Step: 4536, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 1.0226023197174072\n",
      "Step: 4537, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.044203519821167\n",
      "Step: 4538, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 0.9995636940002441\n",
      "Step: 4539, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 1.0165424346923828\n",
      "Step: 4540, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.2447588443756104\n",
      "Step: 4541, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.027219295501709\n",
      "Step: 4542, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.2588999271392822\n",
      "Step: 4543, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.0444586277008057\n",
      "Step: 4544, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.115222692489624\n",
      "Step: 4545, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.114903450012207\n",
      "Step: 4546, Loss: 0.9158377647399902, Accuracy: 1.0, Computation time: 1.1302952766418457\n",
      "Step: 4547, Loss: 0.9158390760421753, Accuracy: 1.0, Computation time: 1.1585984230041504\n",
      "Step: 4548, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.114210844039917\n",
      "Step: 4549, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.206716775894165\n",
      "Step: 4550, Loss: 0.9158379435539246, Accuracy: 1.0, Computation time: 1.043043851852417\n",
      "Step: 4551, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 1.2383301258087158\n",
      "Step: 4552, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.3362555503845215\n",
      "Step: 4553, Loss: 0.9374858140945435, Accuracy: 0.96875, Computation time: 1.2429172992706299\n",
      "Step: 4554, Loss: 0.9259970784187317, Accuracy: 0.96875, Computation time: 1.910151720046997\n",
      "Step: 4555, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.5155587196350098\n",
      "Step: 4556, Loss: 0.919264018535614, Accuracy: 1.0, Computation time: 2.05782413482666\n",
      "Step: 4557, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.2564435005187988\n",
      "Step: 4558, Loss: 0.9160138964653015, Accuracy: 1.0, Computation time: 1.1464731693267822\n",
      "Step: 4559, Loss: 0.9160562753677368, Accuracy: 1.0, Computation time: 1.2769501209259033\n",
      "Step: 4560, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 1.222355604171753\n",
      "Step: 4561, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 1.4368155002593994\n",
      "Step: 4562, Loss: 0.9375614523887634, Accuracy: 0.96875, Computation time: 1.06764554977417\n",
      "Step: 4563, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.3254616260528564\n",
      "Step: 4564, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.0914256572723389\n",
      "Step: 4565, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.0781455039978027\n",
      "Step: 4566, Loss: 0.9366046190261841, Accuracy: 0.96875, Computation time: 1.2162277698516846\n",
      "Step: 4567, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.0925164222717285\n",
      "Step: 4568, Loss: 0.9159921407699585, Accuracy: 1.0, Computation time: 1.6877200603485107\n",
      "Step: 4569, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.375218391418457\n",
      "Step: 4570, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.3095660209655762\n",
      "Step: 4571, Loss: 0.9375671744346619, Accuracy: 0.96875, Computation time: 1.299875020980835\n",
      "Step: 4572, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.3963065147399902\n",
      "Step: 4573, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.21760892868042\n",
      "Step: 4574, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.4609746932983398\n",
      "Step: 4575, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.4459264278411865\n",
      "Step: 4576, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.2531423568725586\n",
      "Step: 4577, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.2348387241363525\n",
      "Step: 4578, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.0270917415618896\n",
      "Step: 4579, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.3579075336456299\n",
      "Step: 4580, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 1.555783987045288\n",
      "Step: 4581, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 1.7922484874725342\n",
      "Step: 4582, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.683523416519165\n",
      "Step: 4583, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.247896671295166\n",
      "Step: 4584, Loss: 0.9375512599945068, Accuracy: 0.96875, Computation time: 1.4643275737762451\n",
      "Step: 4585, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.5481421947479248\n",
      "########################\n",
      "Test loss: 1.0731909275054932, Test Accuracy_epoch33: 0.7701261043548584\n",
      "########################\n",
      "Step: 4586, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.3116118907928467\n",
      "Step: 4587, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.6902391910552979\n",
      "Step: 4588, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 1.3037109375\n",
      "Step: 4589, Loss: 0.9374315738677979, Accuracy: 0.96875, Computation time: 1.7629997730255127\n",
      "Step: 4590, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.2983486652374268\n",
      "Step: 4591, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.3702270984649658\n",
      "Step: 4592, Loss: 0.9158936738967896, Accuracy: 1.0, Computation time: 1.8153584003448486\n",
      "Step: 4593, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.5029568672180176\n",
      "Step: 4594, Loss: 0.9375369548797607, Accuracy: 0.96875, Computation time: 1.246445655822754\n",
      "Step: 4595, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 1.648904800415039\n",
      "Step: 4596, Loss: 0.9375812411308289, Accuracy: 0.96875, Computation time: 1.3965027332305908\n",
      "Step: 4597, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.5114119052886963\n",
      "Step: 4598, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 1.2162039279937744\n",
      "Step: 4599, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.4067659378051758\n",
      "Step: 4600, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.828916311264038\n",
      "Step: 4601, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.5374889373779297\n",
      "Step: 4602, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.3153932094573975\n",
      "Step: 4603, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.3038811683654785\n",
      "Step: 4604, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.4958264827728271\n",
      "Step: 4605, Loss: 0.9158420562744141, Accuracy: 1.0, Computation time: 1.3599069118499756\n",
      "Step: 4606, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.1845476627349854\n",
      "Step: 4607, Loss: 0.9162097573280334, Accuracy: 1.0, Computation time: 1.4415202140808105\n",
      "Step: 4608, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.6644632816314697\n",
      "Step: 4609, Loss: 0.9159292578697205, Accuracy: 1.0, Computation time: 1.4542245864868164\n",
      "Step: 4610, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.3926692008972168\n",
      "Step: 4611, Loss: 0.9354639053344727, Accuracy: 0.96875, Computation time: 1.4011204242706299\n",
      "Step: 4612, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.8628919124603271\n",
      "Step: 4613, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.227398157119751\n",
      "Step: 4614, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.3548626899719238\n",
      "Step: 4615, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.3922162055969238\n",
      "Step: 4616, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.2467267513275146\n",
      "Step: 4617, Loss: 0.9162120223045349, Accuracy: 1.0, Computation time: 1.6595234870910645\n",
      "Step: 4618, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.3419268131256104\n",
      "Step: 4619, Loss: 0.9158420562744141, Accuracy: 1.0, Computation time: 1.0461432933807373\n",
      "Step: 4620, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.0897672176361084\n",
      "Step: 4621, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 1.6053788661956787\n",
      "Step: 4622, Loss: 0.9375170469284058, Accuracy: 0.96875, Computation time: 1.3411009311676025\n",
      "Step: 4623, Loss: 0.9160647392272949, Accuracy: 1.0, Computation time: 1.68141770362854\n",
      "Step: 4624, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 1.4319546222686768\n",
      "Step: 4625, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.7278146743774414\n",
      "Step: 4626, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.6378858089447021\n",
      "Step: 4627, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 1.4960379600524902\n",
      "Step: 4628, Loss: 0.936427891254425, Accuracy: 0.96875, Computation time: 1.1238300800323486\n",
      "Step: 4629, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.1920356750488281\n",
      "Step: 4630, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.2142908573150635\n",
      "Step: 4631, Loss: 0.9158347249031067, Accuracy: 1.0, Computation time: 1.0316839218139648\n",
      "Step: 4632, Loss: 0.9163157939910889, Accuracy: 1.0, Computation time: 1.4668989181518555\n",
      "Step: 4633, Loss: 0.9158349633216858, Accuracy: 1.0, Computation time: 1.1994719505310059\n",
      "Step: 4634, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.370652675628662\n",
      "Step: 4635, Loss: 0.9167458415031433, Accuracy: 1.0, Computation time: 1.9902286529541016\n",
      "Step: 4636, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.29496431350708\n",
      "Step: 4637, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.2306039333343506\n",
      "Step: 4638, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.376021146774292\n",
      "Step: 4639, Loss: 0.9375552535057068, Accuracy: 0.96875, Computation time: 1.3854892253875732\n",
      "Step: 4640, Loss: 0.9375849962234497, Accuracy: 0.96875, Computation time: 1.1746110916137695\n",
      "Step: 4641, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.1774318218231201\n",
      "Step: 4642, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.3224315643310547\n",
      "Step: 4643, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.4862079620361328\n",
      "Step: 4644, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 1.260878086090088\n",
      "Step: 4645, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.1501846313476562\n",
      "Step: 4646, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 1.4506816864013672\n",
      "Step: 4647, Loss: 0.9160962104797363, Accuracy: 1.0, Computation time: 1.12333083152771\n",
      "Step: 4648, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.4357633590698242\n",
      "Step: 4649, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 1.3953657150268555\n",
      "Step: 4650, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.3441174030303955\n",
      "Step: 4651, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.0903253555297852\n",
      "Step: 4652, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.4786553382873535\n",
      "Step: 4653, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.2392942905426025\n",
      "Step: 4654, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.15327787399292\n",
      "Step: 4655, Loss: 0.9375572800636292, Accuracy: 0.96875, Computation time: 1.7491507530212402\n",
      "Step: 4656, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 2.0966105461120605\n",
      "Step: 4657, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 1.1630549430847168\n",
      "Step: 4658, Loss: 0.9158397912979126, Accuracy: 1.0, Computation time: 1.2972228527069092\n",
      "Step: 4659, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 2.090023994445801\n",
      "Step: 4660, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.0819392204284668\n",
      "Step: 4661, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.498436450958252\n",
      "Step: 4662, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.4920012950897217\n",
      "Step: 4663, Loss: 0.9159567356109619, Accuracy: 1.0, Computation time: 2.096604347229004\n",
      "Step: 4664, Loss: 0.9158370494842529, Accuracy: 1.0, Computation time: 1.1956982612609863\n",
      "Step: 4665, Loss: 0.9160257577896118, Accuracy: 1.0, Computation time: 1.226116418838501\n",
      "Step: 4666, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.5224831104278564\n",
      "Step: 4667, Loss: 0.9334485530853271, Accuracy: 0.96875, Computation time: 1.4357218742370605\n",
      "Step: 4668, Loss: 0.9158377647399902, Accuracy: 1.0, Computation time: 1.6137089729309082\n",
      "Step: 4669, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 1.1186020374298096\n",
      "Step: 4670, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.2310757637023926\n",
      "Step: 4671, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.4542059898376465\n",
      "Step: 4672, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.1841001510620117\n",
      "Step: 4673, Loss: 0.9177340865135193, Accuracy: 1.0, Computation time: 1.6492629051208496\n",
      "Step: 4674, Loss: 0.9361171126365662, Accuracy: 0.96875, Computation time: 1.2990577220916748\n",
      "Step: 4675, Loss: 0.9159181714057922, Accuracy: 1.0, Computation time: 1.022449016571045\n",
      "Step: 4676, Loss: 0.9158667922019958, Accuracy: 1.0, Computation time: 1.1629986763000488\n",
      "Step: 4677, Loss: 0.9159857034683228, Accuracy: 1.0, Computation time: 1.6218347549438477\n",
      "Step: 4678, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 2.7170000076293945\n",
      "Step: 4679, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.4092669486999512\n",
      "Step: 4680, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.402031660079956\n",
      "Step: 4681, Loss: 0.9165142178535461, Accuracy: 1.0, Computation time: 1.4332959651947021\n",
      "Step: 4682, Loss: 0.9164140224456787, Accuracy: 1.0, Computation time: 1.6905975341796875\n",
      "Step: 4683, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 1.062791109085083\n",
      "Step: 4684, Loss: 0.9159717559814453, Accuracy: 1.0, Computation time: 1.384807825088501\n",
      "Step: 4685, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.5162293910980225\n",
      "Step: 4686, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.4647619724273682\n",
      "Step: 4687, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.242002010345459\n",
      "Step: 4688, Loss: 0.9160358905792236, Accuracy: 1.0, Computation time: 1.5209259986877441\n",
      "Step: 4689, Loss: 0.937518835067749, Accuracy: 0.96875, Computation time: 1.4924848079681396\n",
      "Step: 4690, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.2938957214355469\n",
      "Step: 4691, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.3923497200012207\n",
      "Step: 4692, Loss: 0.9164082407951355, Accuracy: 1.0, Computation time: 2.0101349353790283\n",
      "Step: 4693, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.2646574974060059\n",
      "Step: 4694, Loss: 0.929987370967865, Accuracy: 0.96875, Computation time: 2.336474895477295\n",
      "Step: 4695, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.0141193866729736\n",
      "Step: 4696, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 1.1621644496917725\n",
      "Step: 4697, Loss: 0.9159712791442871, Accuracy: 1.0, Computation time: 1.2571663856506348\n",
      "Step: 4698, Loss: 0.9159919023513794, Accuracy: 1.0, Computation time: 1.0321784019470215\n",
      "Step: 4699, Loss: 0.915935218334198, Accuracy: 1.0, Computation time: 1.0834197998046875\n",
      "Step: 4700, Loss: 0.9159099459648132, Accuracy: 1.0, Computation time: 1.0873486995697021\n",
      "Step: 4701, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.1011104583740234\n",
      "Step: 4702, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.6157124042510986\n",
      "Step: 4703, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 0.8036246299743652\n",
      "Step: 4704, Loss: 0.9169673919677734, Accuracy: 1.0, Computation time: 1.7296504974365234\n",
      "Step: 4705, Loss: 0.9161010980606079, Accuracy: 1.0, Computation time: 1.1748754978179932\n",
      "Step: 4706, Loss: 0.9159778952598572, Accuracy: 1.0, Computation time: 1.0904088020324707\n",
      "Step: 4707, Loss: 0.9162108898162842, Accuracy: 1.0, Computation time: 1.2576911449432373\n",
      "Step: 4708, Loss: 0.915971040725708, Accuracy: 1.0, Computation time: 1.242187738418579\n",
      "Step: 4709, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.3249316215515137\n",
      "Step: 4710, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.0690011978149414\n",
      "Step: 4711, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 1.1541426181793213\n",
      "Step: 4712, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 0.9583678245544434\n",
      "Step: 4713, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.0255284309387207\n",
      "Step: 4714, Loss: 0.9172305464744568, Accuracy: 1.0, Computation time: 1.7268047332763672\n",
      "Step: 4715, Loss: 0.9159073233604431, Accuracy: 1.0, Computation time: 1.0164692401885986\n",
      "Step: 4716, Loss: 0.9817688465118408, Accuracy: 0.90625, Computation time: 1.1299519538879395\n",
      "Step: 4717, Loss: 0.9159364104270935, Accuracy: 1.0, Computation time: 1.037163496017456\n",
      "Step: 4718, Loss: 0.9159337282180786, Accuracy: 1.0, Computation time: 1.1892623901367188\n",
      "Step: 4719, Loss: 0.9159337282180786, Accuracy: 1.0, Computation time: 0.8673505783081055\n",
      "Step: 4720, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 0.946434736251831\n",
      "Step: 4721, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 0.957129716873169\n",
      "Step: 4722, Loss: 0.9373965263366699, Accuracy: 0.96875, Computation time: 2.260256767272949\n",
      "Step: 4723, Loss: 0.937526524066925, Accuracy: 0.96875, Computation time: 1.0144076347351074\n",
      "Step: 4724, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.0974698066711426\n",
      "########################\n",
      "Test loss: 1.0740034580230713, Test Accuracy_epoch34: 0.7701261043548584\n",
      "########################\n",
      "Step: 4725, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 0.9013106822967529\n",
      "Step: 4726, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.045043706893921\n",
      "Step: 4727, Loss: 0.916131854057312, Accuracy: 1.0, Computation time: 1.2493736743927002\n",
      "Step: 4728, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.0182669162750244\n",
      "Step: 4729, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 1.1551830768585205\n",
      "Step: 4730, Loss: 0.946231484413147, Accuracy: 0.96875, Computation time: 1.9675462245941162\n",
      "Step: 4731, Loss: 0.9160446524620056, Accuracy: 1.0, Computation time: 0.933434247970581\n",
      "Step: 4732, Loss: 0.9161847829818726, Accuracy: 1.0, Computation time: 1.666856288909912\n",
      "Step: 4733, Loss: 0.9165102243423462, Accuracy: 1.0, Computation time: 1.1265146732330322\n",
      "Step: 4734, Loss: 0.9161968231201172, Accuracy: 1.0, Computation time: 1.0124456882476807\n",
      "Step: 4735, Loss: 0.9160395860671997, Accuracy: 1.0, Computation time: 1.3810153007507324\n",
      "Step: 4736, Loss: 0.9159441590309143, Accuracy: 1.0, Computation time: 1.1794614791870117\n",
      "Step: 4737, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 0.9969770908355713\n",
      "Step: 4738, Loss: 0.9377903938293457, Accuracy: 0.96875, Computation time: 0.941636323928833\n",
      "Step: 4739, Loss: 0.9165559411048889, Accuracy: 1.0, Computation time: 0.980567455291748\n",
      "Step: 4740, Loss: 0.9215526580810547, Accuracy: 1.0, Computation time: 1.1640877723693848\n",
      "Step: 4741, Loss: 0.9161351919174194, Accuracy: 1.0, Computation time: 0.9788479804992676\n",
      "Step: 4742, Loss: 0.9162152409553528, Accuracy: 1.0, Computation time: 1.3469326496124268\n",
      "Step: 4743, Loss: 0.9162375926971436, Accuracy: 1.0, Computation time: 1.0298471450805664\n",
      "Step: 4744, Loss: 0.9161939024925232, Accuracy: 1.0, Computation time: 0.8761944770812988\n",
      "Step: 4745, Loss: 0.9160755276679993, Accuracy: 1.0, Computation time: 0.9539740085601807\n",
      "Step: 4746, Loss: 0.9163010716438293, Accuracy: 1.0, Computation time: 1.1840615272521973\n",
      "Step: 4747, Loss: 0.916003406047821, Accuracy: 1.0, Computation time: 1.179579257965088\n",
      "Step: 4748, Loss: 0.9168701171875, Accuracy: 1.0, Computation time: 2.1525332927703857\n",
      "Step: 4749, Loss: 0.9323583245277405, Accuracy: 0.96875, Computation time: 1.1618177890777588\n",
      "Step: 4750, Loss: 0.937336266040802, Accuracy: 0.96875, Computation time: 1.1421291828155518\n",
      "Step: 4751, Loss: 0.9161602854728699, Accuracy: 1.0, Computation time: 1.1108970642089844\n",
      "Step: 4752, Loss: 0.9377350807189941, Accuracy: 0.96875, Computation time: 1.2156479358673096\n",
      "Step: 4753, Loss: 0.9159811735153198, Accuracy: 1.0, Computation time: 0.968764066696167\n",
      "Step: 4754, Loss: 0.9363111853599548, Accuracy: 0.96875, Computation time: 1.1803147792816162\n",
      "Step: 4755, Loss: 0.9159213900566101, Accuracy: 1.0, Computation time: 0.9093635082244873\n",
      "Step: 4756, Loss: 0.9159572720527649, Accuracy: 1.0, Computation time: 0.8383166790008545\n",
      "Step: 4757, Loss: 0.9161272644996643, Accuracy: 1.0, Computation time: 1.0991089344024658\n",
      "Step: 4758, Loss: 0.9377661347389221, Accuracy: 0.96875, Computation time: 1.4022767543792725\n",
      "Step: 4759, Loss: 0.9381580948829651, Accuracy: 0.96875, Computation time: 1.1742441654205322\n",
      "Step: 4760, Loss: 0.9160380959510803, Accuracy: 1.0, Computation time: 1.0652647018432617\n",
      "Step: 4761, Loss: 0.9159669876098633, Accuracy: 1.0, Computation time: 1.015528678894043\n",
      "Step: 4762, Loss: 0.9159696698188782, Accuracy: 1.0, Computation time: 1.2937967777252197\n",
      "Step: 4763, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 0.9106490612030029\n",
      "Step: 4764, Loss: 0.9159508347511292, Accuracy: 1.0, Computation time: 1.1201438903808594\n",
      "Step: 4765, Loss: 0.9159645438194275, Accuracy: 1.0, Computation time: 0.8357415199279785\n",
      "Step: 4766, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 0.9463839530944824\n",
      "Step: 4767, Loss: 0.9159284830093384, Accuracy: 1.0, Computation time: 0.909182071685791\n",
      "Step: 4768, Loss: 0.9375287890434265, Accuracy: 0.96875, Computation time: 1.0173859596252441\n",
      "Step: 4769, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.061960220336914\n",
      "Step: 4770, Loss: 0.9158921837806702, Accuracy: 1.0, Computation time: 0.8958141803741455\n",
      "Step: 4771, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 0.8644716739654541\n",
      "Step: 4772, Loss: 0.9375299215316772, Accuracy: 0.96875, Computation time: 1.0917773246765137\n",
      "Step: 4773, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.166367530822754\n",
      "Step: 4774, Loss: 0.9160289764404297, Accuracy: 1.0, Computation time: 0.9364113807678223\n",
      "Step: 4775, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 0.932574987411499\n",
      "Step: 4776, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 1.4375474452972412\n",
      "Step: 4777, Loss: 0.9159505367279053, Accuracy: 1.0, Computation time: 1.0858449935913086\n",
      "Step: 4778, Loss: 0.9293662309646606, Accuracy: 0.96875, Computation time: 1.4372880458831787\n",
      "Step: 4779, Loss: 0.9159939885139465, Accuracy: 1.0, Computation time: 1.0103485584259033\n",
      "Step: 4780, Loss: 0.9159958958625793, Accuracy: 1.0, Computation time: 1.0551385879516602\n",
      "Step: 4781, Loss: 0.9162799715995789, Accuracy: 1.0, Computation time: 1.0303022861480713\n",
      "Step: 4782, Loss: 0.9174428582191467, Accuracy: 1.0, Computation time: 1.3514814376831055\n",
      "Step: 4783, Loss: 0.9161101579666138, Accuracy: 1.0, Computation time: 1.2654414176940918\n",
      "Step: 4784, Loss: 0.916004478931427, Accuracy: 1.0, Computation time: 1.1094541549682617\n",
      "Step: 4785, Loss: 0.9170452356338501, Accuracy: 1.0, Computation time: 1.3341093063354492\n",
      "Step: 4786, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 1.196061611175537\n",
      "Step: 4787, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.0185914039611816\n",
      "Step: 4788, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.1366667747497559\n",
      "Step: 4789, Loss: 0.9159800410270691, Accuracy: 1.0, Computation time: 1.240046501159668\n",
      "Step: 4790, Loss: 0.9159837365150452, Accuracy: 1.0, Computation time: 0.9681210517883301\n",
      "Step: 4791, Loss: 0.9170997738838196, Accuracy: 1.0, Computation time: 1.3289368152618408\n",
      "Step: 4792, Loss: 0.9159901142120361, Accuracy: 1.0, Computation time: 0.9686682224273682\n",
      "Step: 4793, Loss: 0.935674250125885, Accuracy: 0.96875, Computation time: 1.152139663696289\n",
      "Step: 4794, Loss: 0.9360902905464172, Accuracy: 0.96875, Computation time: 1.0000863075256348\n",
      "Step: 4795, Loss: 0.9159959554672241, Accuracy: 1.0, Computation time: 0.9840793609619141\n",
      "Step: 4796, Loss: 0.9159478545188904, Accuracy: 1.0, Computation time: 0.9683198928833008\n",
      "Step: 4797, Loss: 0.9376397132873535, Accuracy: 0.96875, Computation time: 1.0265085697174072\n",
      "Step: 4798, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 0.8709640502929688\n",
      "Step: 4799, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.0411128997802734\n",
      "Step: 4800, Loss: 0.959216833114624, Accuracy: 0.9375, Computation time: 1.1034340858459473\n",
      "Step: 4801, Loss: 0.937633752822876, Accuracy: 0.96875, Computation time: 1.064650058746338\n",
      "Step: 4802, Loss: 0.916015088558197, Accuracy: 1.0, Computation time: 1.2990801334381104\n",
      "Step: 4803, Loss: 0.9371999502182007, Accuracy: 0.96875, Computation time: 1.021458625793457\n",
      "Step: 4804, Loss: 0.9159602522850037, Accuracy: 1.0, Computation time: 1.125762939453125\n",
      "Step: 4805, Loss: 0.9160460829734802, Accuracy: 1.0, Computation time: 1.138395071029663\n",
      "Step: 4806, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 0.9360268115997314\n",
      "Step: 4807, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 0.9458298683166504\n",
      "Step: 4808, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.298194169998169\n",
      "Step: 4809, Loss: 0.9375998973846436, Accuracy: 0.96875, Computation time: 0.8870267868041992\n",
      "Step: 4810, Loss: 0.9375041127204895, Accuracy: 0.96875, Computation time: 1.1380467414855957\n",
      "Step: 4811, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 0.9150071144104004\n",
      "Step: 4812, Loss: 0.9212332367897034, Accuracy: 1.0, Computation time: 1.2207140922546387\n",
      "Step: 4813, Loss: 0.9161728024482727, Accuracy: 1.0, Computation time: 1.2384617328643799\n",
      "Step: 4814, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 0.9578192234039307\n",
      "Step: 4815, Loss: 0.9160153269767761, Accuracy: 1.0, Computation time: 1.1398346424102783\n",
      "Step: 4816, Loss: 0.9161124229431152, Accuracy: 1.0, Computation time: 1.179213523864746\n",
      "Step: 4817, Loss: 0.9160007238388062, Accuracy: 1.0, Computation time: 1.1680998802185059\n",
      "Step: 4818, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.0323882102966309\n",
      "Step: 4819, Loss: 0.9160705208778381, Accuracy: 1.0, Computation time: 1.536684513092041\n",
      "Step: 4820, Loss: 0.9161431193351746, Accuracy: 1.0, Computation time: 1.7494268417358398\n",
      "Step: 4821, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 0.9610013961791992\n",
      "Step: 4822, Loss: 0.9227436780929565, Accuracy: 1.0, Computation time: 1.3481605052947998\n",
      "Step: 4823, Loss: 0.9162200093269348, Accuracy: 1.0, Computation time: 1.1620025634765625\n",
      "Step: 4824, Loss: 0.9160823822021484, Accuracy: 1.0, Computation time: 1.1841802597045898\n",
      "Step: 4825, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.1448791027069092\n",
      "Step: 4826, Loss: 0.9160054922103882, Accuracy: 1.0, Computation time: 1.147383451461792\n",
      "Step: 4827, Loss: 0.9159784913063049, Accuracy: 1.0, Computation time: 1.021742820739746\n",
      "Step: 4828, Loss: 0.9158995151519775, Accuracy: 1.0, Computation time: 1.3585457801818848\n",
      "Step: 4829, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.0928363800048828\n",
      "Step: 4830, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 1.0333969593048096\n",
      "Step: 4831, Loss: 0.9159601330757141, Accuracy: 1.0, Computation time: 1.137268304824829\n",
      "Step: 4832, Loss: 0.9160217642784119, Accuracy: 1.0, Computation time: 1.1096348762512207\n",
      "Step: 4833, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.0204925537109375\n",
      "Step: 4834, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 0.9683876037597656\n",
      "Step: 4835, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 1.0085201263427734\n",
      "Step: 4836, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.053708553314209\n",
      "Step: 4837, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 0.8864941596984863\n",
      "Step: 4838, Loss: 0.9377743005752563, Accuracy: 0.96875, Computation time: 1.127760410308838\n",
      "Step: 4839, Loss: 0.9160948991775513, Accuracy: 1.0, Computation time: 0.9845764636993408\n",
      "Step: 4840, Loss: 0.9161617159843445, Accuracy: 1.0, Computation time: 0.9409892559051514\n",
      "Step: 4841, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 1.067681074142456\n",
      "Step: 4842, Loss: 0.9200668334960938, Accuracy: 1.0, Computation time: 1.147423505783081\n",
      "Step: 4843, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.0205950736999512\n",
      "Step: 4844, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 0.877565860748291\n",
      "Step: 4845, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 0.8978562355041504\n",
      "Step: 4846, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 0.8928489685058594\n",
      "Step: 4847, Loss: 0.9160474538803101, Accuracy: 1.0, Computation time: 1.1751017570495605\n",
      "Step: 4848, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 0.9462518692016602\n",
      "Step: 4849, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 0.9080212116241455\n",
      "Step: 4850, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.1103038787841797\n",
      "Step: 4851, Loss: 0.9158961176872253, Accuracy: 1.0, Computation time: 0.9408068656921387\n",
      "Step: 4852, Loss: 0.9378389716148376, Accuracy: 0.96875, Computation time: 1.7987968921661377\n",
      "Step: 4853, Loss: 0.9376400113105774, Accuracy: 0.96875, Computation time: 1.4460372924804688\n",
      "Step: 4854, Loss: 0.9180144667625427, Accuracy: 1.0, Computation time: 1.2415812015533447\n",
      "Step: 4855, Loss: 0.9593011140823364, Accuracy: 0.9375, Computation time: 0.8812453746795654\n",
      "Step: 4856, Loss: 0.9163524508476257, Accuracy: 1.0, Computation time: 1.0551753044128418\n",
      "Step: 4857, Loss: 0.9161658883094788, Accuracy: 1.0, Computation time: 1.1130971908569336\n",
      "Step: 4858, Loss: 0.9160372614860535, Accuracy: 1.0, Computation time: 1.2417306900024414\n",
      "Step: 4859, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 0.9870212078094482\n",
      "Step: 4860, Loss: 0.9377825856208801, Accuracy: 0.96875, Computation time: 0.9284484386444092\n",
      "Step: 4861, Loss: 0.9259085655212402, Accuracy: 0.96875, Computation time: 1.248849630355835\n",
      "Step: 4862, Loss: 0.9162479639053345, Accuracy: 1.0, Computation time: 1.2098846435546875\n",
      "Step: 4863, Loss: 0.9371628761291504, Accuracy: 0.96875, Computation time: 1.1221997737884521\n",
      "########################\n",
      "Test loss: 1.0725363492965698, Test Accuracy_epoch35: 0.7730358839035034\n",
      "########################\n",
      "Step: 4864, Loss: 0.9162106513977051, Accuracy: 1.0, Computation time: 1.1171669960021973\n",
      "Step: 4865, Loss: 0.9162693619728088, Accuracy: 1.0, Computation time: 1.047062635421753\n",
      "Step: 4866, Loss: 0.9161446690559387, Accuracy: 1.0, Computation time: 1.1928668022155762\n",
      "Step: 4867, Loss: 0.9159495234489441, Accuracy: 1.0, Computation time: 1.1206750869750977\n",
      "Step: 4868, Loss: 0.935516893863678, Accuracy: 0.96875, Computation time: 1.043454647064209\n",
      "Step: 4869, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 1.1854124069213867\n",
      "Step: 4870, Loss: 0.915954053401947, Accuracy: 1.0, Computation time: 0.9706363677978516\n",
      "Step: 4871, Loss: 0.9160169959068298, Accuracy: 1.0, Computation time: 1.1234054565429688\n",
      "Step: 4872, Loss: 0.9160724878311157, Accuracy: 1.0, Computation time: 0.9223275184631348\n",
      "Step: 4873, Loss: 0.9159517288208008, Accuracy: 1.0, Computation time: 1.7609930038452148\n",
      "Step: 4874, Loss: 0.916215181350708, Accuracy: 1.0, Computation time: 1.3299572467803955\n",
      "Step: 4875, Loss: 0.915999710559845, Accuracy: 1.0, Computation time: 1.1234612464904785\n",
      "Step: 4876, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.2943739891052246\n",
      "Step: 4877, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.238433599472046\n",
      "Step: 4878, Loss: 0.9376958608627319, Accuracy: 0.96875, Computation time: 1.2240073680877686\n",
      "Step: 4879, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 0.9631574153900146\n",
      "Step: 4880, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.0485186576843262\n",
      "Step: 4881, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.044513463973999\n",
      "Step: 4882, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.1657700538635254\n",
      "Step: 4883, Loss: 0.9159570932388306, Accuracy: 1.0, Computation time: 1.2816898822784424\n",
      "Step: 4884, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.2136290073394775\n",
      "Step: 4885, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.0563263893127441\n",
      "Step: 4886, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 1.140561819076538\n",
      "Step: 4887, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.0207548141479492\n",
      "Step: 4888, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.1377880573272705\n",
      "Step: 4889, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 0.9277985095977783\n",
      "Step: 4890, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.1430931091308594\n",
      "Step: 4891, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.2670986652374268\n",
      "Step: 4892, Loss: 0.9366875290870667, Accuracy: 0.96875, Computation time: 0.9807960987091064\n",
      "Step: 4893, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.0726830959320068\n",
      "Step: 4894, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 0.9067752361297607\n",
      "Step: 4895, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.1677038669586182\n",
      "Step: 4896, Loss: 0.9374510049819946, Accuracy: 0.96875, Computation time: 1.5441570281982422\n",
      "Step: 4897, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.1655917167663574\n",
      "Step: 4898, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 1.513596773147583\n",
      "Step: 4899, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 0.9446065425872803\n",
      "Step: 4900, Loss: 0.9158388376235962, Accuracy: 1.0, Computation time: 1.8569018840789795\n",
      "Step: 4901, Loss: 0.9158341884613037, Accuracy: 1.0, Computation time: 1.1776647567749023\n",
      "Step: 4902, Loss: 0.9162731766700745, Accuracy: 1.0, Computation time: 1.348799467086792\n",
      "Step: 4903, Loss: 0.917697548866272, Accuracy: 1.0, Computation time: 1.4933037757873535\n",
      "Step: 4904, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.0770559310913086\n",
      "Step: 4905, Loss: 0.9375182390213013, Accuracy: 0.96875, Computation time: 1.086721658706665\n",
      "Step: 4906, Loss: 0.9377375245094299, Accuracy: 0.96875, Computation time: 1.6900103092193604\n",
      "Step: 4907, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.3822317123413086\n",
      "Step: 4908, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.1481919288635254\n",
      "Step: 4909, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 0.9663231372833252\n",
      "Step: 4910, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.9681534767150879\n",
      "Step: 4911, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.2974281311035156\n",
      "Step: 4912, Loss: 0.915838897228241, Accuracy: 1.0, Computation time: 0.8633763790130615\n",
      "Step: 4913, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.0680861473083496\n",
      "Step: 4914, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 1.1092634201049805\n",
      "Step: 4915, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.0501923561096191\n",
      "Step: 4916, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 0.926194429397583\n",
      "Step: 4917, Loss: 0.9229660034179688, Accuracy: 1.0, Computation time: 1.3487355709075928\n",
      "Step: 4918, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.0147655010223389\n",
      "Step: 4919, Loss: 0.9161872863769531, Accuracy: 1.0, Computation time: 1.040893316268921\n",
      "Step: 4920, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 1.137221336364746\n",
      "Step: 4921, Loss: 0.9160687923431396, Accuracy: 1.0, Computation time: 1.2381212711334229\n",
      "Step: 4922, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 0.9842722415924072\n",
      "Step: 4923, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 0.967919111251831\n",
      "Step: 4924, Loss: 0.9374296069145203, Accuracy: 0.96875, Computation time: 1.4571473598480225\n",
      "Step: 4925, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 0.9981746673583984\n",
      "Step: 4926, Loss: 0.9159930944442749, Accuracy: 1.0, Computation time: 1.5320689678192139\n",
      "Step: 4927, Loss: 0.9159445762634277, Accuracy: 1.0, Computation time: 1.0646648406982422\n",
      "Step: 4928, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.1930663585662842\n",
      "Step: 4929, Loss: 0.9158954620361328, Accuracy: 1.0, Computation time: 1.1680002212524414\n",
      "Step: 4930, Loss: 0.9353089928627014, Accuracy: 0.96875, Computation time: 1.3333020210266113\n",
      "Step: 4931, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.2621750831604004\n",
      "Step: 4932, Loss: 0.9159790873527527, Accuracy: 1.0, Computation time: 1.1912717819213867\n",
      "Step: 4933, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 0.9446663856506348\n",
      "Step: 4934, Loss: 0.9159202575683594, Accuracy: 1.0, Computation time: 0.9250233173370361\n",
      "Step: 4935, Loss: 0.9386819005012512, Accuracy: 0.96875, Computation time: 2.2647414207458496\n",
      "Step: 4936, Loss: 0.9159353971481323, Accuracy: 1.0, Computation time: 1.2821252346038818\n",
      "Step: 4937, Loss: 0.937823474407196, Accuracy: 0.96875, Computation time: 1.2531445026397705\n",
      "Step: 4938, Loss: 0.9161115884780884, Accuracy: 1.0, Computation time: 1.2354850769042969\n",
      "Step: 4939, Loss: 0.9161638617515564, Accuracy: 1.0, Computation time: 1.487304449081421\n",
      "Step: 4940, Loss: 0.9161254167556763, Accuracy: 1.0, Computation time: 1.0074572563171387\n",
      "Step: 4941, Loss: 0.9160811305046082, Accuracy: 1.0, Computation time: 1.3234548568725586\n",
      "Step: 4942, Loss: 0.9159731268882751, Accuracy: 1.0, Computation time: 1.1767120361328125\n",
      "Step: 4943, Loss: 0.9161146283149719, Accuracy: 1.0, Computation time: 0.9976906776428223\n",
      "Step: 4944, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 1.1603903770446777\n",
      "Step: 4945, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.0789873600006104\n",
      "Step: 4946, Loss: 0.9159897565841675, Accuracy: 1.0, Computation time: 1.046081781387329\n",
      "Step: 4947, Loss: 0.9456875324249268, Accuracy: 0.96875, Computation time: 1.445516586303711\n",
      "Step: 4948, Loss: 0.9160043597221375, Accuracy: 1.0, Computation time: 1.8256101608276367\n",
      "Step: 4949, Loss: 0.9159846305847168, Accuracy: 1.0, Computation time: 1.4310815334320068\n",
      "Step: 4950, Loss: 0.9160071015357971, Accuracy: 1.0, Computation time: 1.0337889194488525\n",
      "Step: 4951, Loss: 0.9279579520225525, Accuracy: 0.96875, Computation time: 1.2403273582458496\n",
      "Step: 4952, Loss: 0.937984824180603, Accuracy: 0.96875, Computation time: 1.4077808856964111\n",
      "Step: 4953, Loss: 0.9161131978034973, Accuracy: 1.0, Computation time: 1.8200170993804932\n",
      "Step: 4954, Loss: 0.9163879156112671, Accuracy: 1.0, Computation time: 1.128202199935913\n",
      "Step: 4955, Loss: 0.9162483811378479, Accuracy: 1.0, Computation time: 1.1819679737091064\n",
      "Step: 4956, Loss: 0.9163362383842468, Accuracy: 1.0, Computation time: 1.6355867385864258\n",
      "Step: 4957, Loss: 0.9162824153900146, Accuracy: 1.0, Computation time: 1.934636116027832\n",
      "Step: 4958, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.1954388618469238\n",
      "Step: 4959, Loss: 0.9159480333328247, Accuracy: 1.0, Computation time: 1.012321949005127\n",
      "Step: 4960, Loss: 0.9161368012428284, Accuracy: 1.0, Computation time: 1.2256438732147217\n",
      "Step: 4961, Loss: 0.9194064736366272, Accuracy: 1.0, Computation time: 1.0513639450073242\n",
      "Step: 4962, Loss: 0.9314514398574829, Accuracy: 0.96875, Computation time: 1.9563825130462646\n",
      "Step: 4963, Loss: 0.9160174131393433, Accuracy: 1.0, Computation time: 1.2538511753082275\n",
      "Step: 4964, Loss: 0.9160186052322388, Accuracy: 1.0, Computation time: 1.3540196418762207\n",
      "Step: 4965, Loss: 0.9377081990242004, Accuracy: 0.96875, Computation time: 1.359309434890747\n",
      "Step: 4966, Loss: 0.9163028001785278, Accuracy: 1.0, Computation time: 1.4531035423278809\n",
      "Step: 4967, Loss: 0.9163520932197571, Accuracy: 1.0, Computation time: 1.2247200012207031\n",
      "Step: 4968, Loss: 0.9192343354225159, Accuracy: 1.0, Computation time: 2.2299578189849854\n",
      "Step: 4969, Loss: 0.9162286520004272, Accuracy: 1.0, Computation time: 1.3951811790466309\n",
      "Step: 4970, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 1.2678720951080322\n",
      "Step: 4971, Loss: 0.9445959329605103, Accuracy: 0.96875, Computation time: 1.706301212310791\n",
      "Step: 4972, Loss: 0.9163233041763306, Accuracy: 1.0, Computation time: 1.2688565254211426\n",
      "Step: 4973, Loss: 0.916774570941925, Accuracy: 1.0, Computation time: 1.1699481010437012\n",
      "Step: 4974, Loss: 0.9385531544685364, Accuracy: 0.96875, Computation time: 1.3687477111816406\n",
      "Step: 4975, Loss: 0.9167941212654114, Accuracy: 1.0, Computation time: 1.2777938842773438\n",
      "Step: 4976, Loss: 0.9167584180831909, Accuracy: 1.0, Computation time: 1.4074347019195557\n",
      "Step: 4977, Loss: 0.9166484475135803, Accuracy: 1.0, Computation time: 1.6685009002685547\n",
      "Step: 4978, Loss: 0.9231308698654175, Accuracy: 1.0, Computation time: 2.190784454345703\n",
      "Step: 4979, Loss: 0.9380742311477661, Accuracy: 0.96875, Computation time: 1.3260207176208496\n",
      "Step: 4980, Loss: 0.9160440564155579, Accuracy: 1.0, Computation time: 1.1645581722259521\n",
      "Step: 4981, Loss: 0.9159517884254456, Accuracy: 1.0, Computation time: 1.4873499870300293\n",
      "Step: 4982, Loss: 0.91591876745224, Accuracy: 1.0, Computation time: 1.329514980316162\n",
      "Step: 4983, Loss: 0.9160778522491455, Accuracy: 1.0, Computation time: 1.6999232769012451\n",
      "Step: 4984, Loss: 0.9173352122306824, Accuracy: 1.0, Computation time: 0.9321191310882568\n",
      "Step: 4985, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.1269900798797607\n",
      "Step: 4986, Loss: 0.916068971157074, Accuracy: 1.0, Computation time: 1.4525747299194336\n",
      "Step: 4987, Loss: 0.9169216156005859, Accuracy: 1.0, Computation time: 1.1028149127960205\n",
      "Step: 4988, Loss: 0.9373931884765625, Accuracy: 0.96875, Computation time: 0.8664815425872803\n",
      "Step: 4989, Loss: 0.9186190366744995, Accuracy: 1.0, Computation time: 1.1901602745056152\n",
      "Step: 4990, Loss: 0.9367944002151489, Accuracy: 0.96875, Computation time: 1.8530139923095703\n",
      "Step: 4991, Loss: 0.916063129901886, Accuracy: 1.0, Computation time: 1.369140386581421\n",
      "Step: 4992, Loss: 0.9177918434143066, Accuracy: 1.0, Computation time: 1.6856529712677002\n",
      "Step: 4993, Loss: 0.958182692527771, Accuracy: 0.9375, Computation time: 1.8191649913787842\n",
      "Step: 4994, Loss: 0.9180566072463989, Accuracy: 1.0, Computation time: 2.030102252960205\n",
      "Step: 4995, Loss: 0.916114330291748, Accuracy: 1.0, Computation time: 1.115574598312378\n",
      "Step: 4996, Loss: 0.9160332679748535, Accuracy: 1.0, Computation time: 1.105640172958374\n",
      "Step: 4997, Loss: 0.9162443280220032, Accuracy: 1.0, Computation time: 1.0894525051116943\n",
      "Step: 4998, Loss: 0.9161298871040344, Accuracy: 1.0, Computation time: 1.0901193618774414\n",
      "Step: 4999, Loss: 0.9159727096557617, Accuracy: 1.0, Computation time: 1.0092072486877441\n",
      "Step: 5000, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 0.9036972522735596\n",
      "Step: 5001, Loss: 0.9160165190696716, Accuracy: 1.0, Computation time: 1.3097765445709229\n",
      "Step: 5002, Loss: 0.9160649180412292, Accuracy: 1.0, Computation time: 1.5777828693389893\n",
      "########################\n",
      "Test loss: 1.0808178186416626, Test Accuracy_epoch36: 0.7613967061042786\n",
      "########################\n",
      "Step: 5003, Loss: 0.9337751269340515, Accuracy: 0.96875, Computation time: 1.6676843166351318\n",
      "Step: 5004, Loss: 0.9160391092300415, Accuracy: 1.0, Computation time: 1.2039384841918945\n",
      "Step: 5005, Loss: 0.9159975051879883, Accuracy: 1.0, Computation time: 1.1716139316558838\n",
      "Step: 5006, Loss: 0.9162465333938599, Accuracy: 1.0, Computation time: 1.2308852672576904\n",
      "Step: 5007, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 1.2685000896453857\n",
      "Step: 5008, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 1.7824726104736328\n",
      "Step: 5009, Loss: 0.9159919023513794, Accuracy: 1.0, Computation time: 1.991852045059204\n",
      "Step: 5010, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 1.2307865619659424\n",
      "Step: 5011, Loss: 0.9159859418869019, Accuracy: 1.0, Computation time: 1.2627825736999512\n",
      "Step: 5012, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 1.830003261566162\n",
      "Step: 5013, Loss: 0.9162989854812622, Accuracy: 1.0, Computation time: 1.4192018508911133\n",
      "Step: 5014, Loss: 0.9263188242912292, Accuracy: 0.96875, Computation time: 1.8601624965667725\n",
      "Step: 5015, Loss: 0.937721312046051, Accuracy: 0.96875, Computation time: 1.3803648948669434\n",
      "Step: 5016, Loss: 0.9159718155860901, Accuracy: 1.0, Computation time: 1.8325283527374268\n",
      "Step: 5017, Loss: 0.9218519926071167, Accuracy: 1.0, Computation time: 1.8102531433105469\n",
      "Step: 5018, Loss: 0.916456401348114, Accuracy: 1.0, Computation time: 1.319993495941162\n",
      "Step: 5019, Loss: 0.9160750508308411, Accuracy: 1.0, Computation time: 1.2217156887054443\n",
      "Step: 5020, Loss: 0.916329026222229, Accuracy: 1.0, Computation time: 1.5074701309204102\n",
      "Step: 5021, Loss: 0.9162610769271851, Accuracy: 1.0, Computation time: 1.4251632690429688\n",
      "Step: 5022, Loss: 0.9161727428436279, Accuracy: 1.0, Computation time: 1.2420928478240967\n",
      "Step: 5023, Loss: 0.9160817265510559, Accuracy: 1.0, Computation time: 1.2610266208648682\n",
      "Step: 5024, Loss: 0.9160712957382202, Accuracy: 1.0, Computation time: 1.0683107376098633\n",
      "Step: 5025, Loss: 0.9172614216804504, Accuracy: 1.0, Computation time: 1.2631161212921143\n",
      "Step: 5026, Loss: 0.9163101315498352, Accuracy: 1.0, Computation time: 1.6338517665863037\n",
      "Step: 5027, Loss: 0.9160520434379578, Accuracy: 1.0, Computation time: 1.1844000816345215\n",
      "Step: 5028, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.0651881694793701\n",
      "Step: 5029, Loss: 0.9159859418869019, Accuracy: 1.0, Computation time: 0.9843904972076416\n",
      "Step: 5030, Loss: 0.9161088466644287, Accuracy: 1.0, Computation time: 1.02781081199646\n",
      "Step: 5031, Loss: 0.9162580966949463, Accuracy: 1.0, Computation time: 1.3300046920776367\n",
      "Step: 5032, Loss: 0.9160985350608826, Accuracy: 1.0, Computation time: 1.0276117324829102\n",
      "Step: 5033, Loss: 0.9160641431808472, Accuracy: 1.0, Computation time: 1.0174403190612793\n",
      "Step: 5034, Loss: 0.9161449670791626, Accuracy: 1.0, Computation time: 1.0713307857513428\n",
      "Step: 5035, Loss: 0.9178048372268677, Accuracy: 1.0, Computation time: 1.4941513538360596\n",
      "Step: 5036, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 1.160501480102539\n",
      "Step: 5037, Loss: 0.9160546660423279, Accuracy: 1.0, Computation time: 1.1479294300079346\n",
      "Step: 5038, Loss: 0.9379062056541443, Accuracy: 0.96875, Computation time: 1.2776188850402832\n",
      "Step: 5039, Loss: 0.9223230481147766, Accuracy: 1.0, Computation time: 1.391101360321045\n",
      "Step: 5040, Loss: 0.9257469773292542, Accuracy: 0.96875, Computation time: 2.4756858348846436\n",
      "Step: 5041, Loss: 0.916063129901886, Accuracy: 1.0, Computation time: 1.7173850536346436\n",
      "Step: 5042, Loss: 0.9160894751548767, Accuracy: 1.0, Computation time: 0.9647769927978516\n",
      "Step: 5043, Loss: 0.915971577167511, Accuracy: 1.0, Computation time: 1.0964751243591309\n",
      "Step: 5044, Loss: 0.9167431592941284, Accuracy: 1.0, Computation time: 1.0825579166412354\n",
      "Step: 5045, Loss: 0.9161204099655151, Accuracy: 1.0, Computation time: 1.3378643989562988\n",
      "Step: 5046, Loss: 0.9375483393669128, Accuracy: 0.96875, Computation time: 2.0600485801696777\n",
      "Step: 5047, Loss: 0.9160959720611572, Accuracy: 1.0, Computation time: 0.9779086112976074\n",
      "Step: 5048, Loss: 0.9162209630012512, Accuracy: 1.0, Computation time: 1.001244306564331\n",
      "Step: 5049, Loss: 0.9160052537918091, Accuracy: 1.0, Computation time: 1.2033040523529053\n",
      "Step: 5050, Loss: 0.9160077571868896, Accuracy: 1.0, Computation time: 1.1290323734283447\n",
      "Step: 5051, Loss: 0.916375458240509, Accuracy: 1.0, Computation time: 1.2629024982452393\n",
      "Step: 5052, Loss: 0.9160863757133484, Accuracy: 1.0, Computation time: 1.0000319480895996\n",
      "Step: 5053, Loss: 0.9172263145446777, Accuracy: 1.0, Computation time: 1.589174509048462\n",
      "Step: 5054, Loss: 0.9165659546852112, Accuracy: 1.0, Computation time: 1.4250729084014893\n",
      "Step: 5055, Loss: 0.9162716269493103, Accuracy: 1.0, Computation time: 1.1734182834625244\n",
      "Step: 5056, Loss: 0.9381624460220337, Accuracy: 0.96875, Computation time: 1.491027593612671\n",
      "Step: 5057, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 0.9924111366271973\n",
      "Step: 5058, Loss: 0.9212173819541931, Accuracy: 1.0, Computation time: 1.1183974742889404\n",
      "Step: 5059, Loss: 0.916000247001648, Accuracy: 1.0, Computation time: 1.3059709072113037\n",
      "Step: 5060, Loss: 0.9162037372589111, Accuracy: 1.0, Computation time: 1.2495834827423096\n",
      "Step: 5061, Loss: 0.9165605902671814, Accuracy: 1.0, Computation time: 1.3056015968322754\n",
      "Step: 5062, Loss: 0.9316743016242981, Accuracy: 0.96875, Computation time: 1.2716281414031982\n",
      "Step: 5063, Loss: 0.916468620300293, Accuracy: 1.0, Computation time: 1.37628173828125\n",
      "Step: 5064, Loss: 0.9169729351997375, Accuracy: 1.0, Computation time: 1.3339309692382812\n",
      "Step: 5065, Loss: 0.9159762859344482, Accuracy: 1.0, Computation time: 1.481132984161377\n",
      "Step: 5066, Loss: 0.916018009185791, Accuracy: 1.0, Computation time: 1.653306007385254\n",
      "Step: 5067, Loss: 0.9287712574005127, Accuracy: 0.96875, Computation time: 1.170562744140625\n",
      "Step: 5068, Loss: 0.9183069467544556, Accuracy: 1.0, Computation time: 1.0365056991577148\n",
      "Step: 5069, Loss: 0.9162105917930603, Accuracy: 1.0, Computation time: 1.1296262741088867\n",
      "Step: 5070, Loss: 0.9163075089454651, Accuracy: 1.0, Computation time: 1.1233484745025635\n",
      "Step: 5071, Loss: 0.9162849187850952, Accuracy: 1.0, Computation time: 0.9763269424438477\n",
      "Step: 5072, Loss: 0.9352657198905945, Accuracy: 0.96875, Computation time: 1.4203486442565918\n",
      "Step: 5073, Loss: 0.91616290807724, Accuracy: 1.0, Computation time: 1.0878067016601562\n",
      "Step: 5074, Loss: 0.9160299897193909, Accuracy: 1.0, Computation time: 1.067840576171875\n",
      "Step: 5075, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 1.0763657093048096\n",
      "Step: 5076, Loss: 0.9382084012031555, Accuracy: 0.96875, Computation time: 2.6152164936065674\n",
      "Step: 5077, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 1.016244888305664\n",
      "Step: 5078, Loss: 0.937663197517395, Accuracy: 0.96875, Computation time: 0.9933078289031982\n",
      "Step: 5079, Loss: 0.9403442740440369, Accuracy: 0.96875, Computation time: 1.0278244018554688\n",
      "Step: 5080, Loss: 0.9160095453262329, Accuracy: 1.0, Computation time: 1.0492801666259766\n",
      "Step: 5081, Loss: 0.9161980748176575, Accuracy: 1.0, Computation time: 1.2706208229064941\n",
      "Step: 5082, Loss: 0.9160683155059814, Accuracy: 1.0, Computation time: 0.9767799377441406\n",
      "Step: 5083, Loss: 0.9161090850830078, Accuracy: 1.0, Computation time: 1.3615984916687012\n",
      "Step: 5084, Loss: 0.9160180687904358, Accuracy: 1.0, Computation time: 0.988823652267456\n",
      "Step: 5085, Loss: 0.9160780906677246, Accuracy: 1.0, Computation time: 1.0442097187042236\n",
      "Step: 5086, Loss: 0.9162574410438538, Accuracy: 1.0, Computation time: 1.0456047058105469\n",
      "Step: 5087, Loss: 0.937775194644928, Accuracy: 0.96875, Computation time: 0.9837040901184082\n",
      "Step: 5088, Loss: 0.9163157343864441, Accuracy: 1.0, Computation time: 1.0177874565124512\n",
      "Step: 5089, Loss: 0.9214015603065491, Accuracy: 1.0, Computation time: 1.1590027809143066\n",
      "Step: 5090, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.0038232803344727\n",
      "Step: 5091, Loss: 0.9376765489578247, Accuracy: 0.96875, Computation time: 1.2769839763641357\n",
      "Step: 5092, Loss: 0.9159624576568604, Accuracy: 1.0, Computation time: 0.9974124431610107\n",
      "Step: 5093, Loss: 0.9160780906677246, Accuracy: 1.0, Computation time: 1.1374003887176514\n",
      "Step: 5094, Loss: 0.9159882068634033, Accuracy: 1.0, Computation time: 1.0239653587341309\n",
      "Step: 5095, Loss: 0.9159353971481323, Accuracy: 1.0, Computation time: 1.2622878551483154\n",
      "Step: 5096, Loss: 0.9378840923309326, Accuracy: 0.96875, Computation time: 1.70912504196167\n",
      "Step: 5097, Loss: 0.9159376621246338, Accuracy: 1.0, Computation time: 1.195777416229248\n",
      "Step: 5098, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.1991605758666992\n",
      "Step: 5099, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.12021803855896\n",
      "Step: 5100, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.082397222518921\n",
      "Step: 5101, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.0958423614501953\n",
      "Step: 5102, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 1.0179660320281982\n",
      "Step: 5103, Loss: 0.9175992012023926, Accuracy: 1.0, Computation time: 1.1146631240844727\n",
      "Step: 5104, Loss: 0.9376177191734314, Accuracy: 0.96875, Computation time: 1.0100140571594238\n",
      "Step: 5105, Loss: 0.9383009076118469, Accuracy: 0.96875, Computation time: 1.4035685062408447\n",
      "Step: 5106, Loss: 0.9163405895233154, Accuracy: 1.0, Computation time: 0.9931230545043945\n",
      "Step: 5107, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.421137809753418\n",
      "Step: 5108, Loss: 0.9159204363822937, Accuracy: 1.0, Computation time: 1.0315639972686768\n",
      "Step: 5109, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 1.3920342922210693\n",
      "Step: 5110, Loss: 0.9182912111282349, Accuracy: 1.0, Computation time: 1.3546011447906494\n",
      "Step: 5111, Loss: 0.9158921837806702, Accuracy: 1.0, Computation time: 1.3408589363098145\n",
      "Step: 5112, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.248246669769287\n",
      "Step: 5113, Loss: 0.9159297347068787, Accuracy: 1.0, Computation time: 1.4670898914337158\n",
      "Step: 5114, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.2340004444122314\n",
      "Step: 5115, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 1.1093530654907227\n",
      "Step: 5116, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 1.5525660514831543\n",
      "Step: 5117, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.114164113998413\n",
      "Step: 5118, Loss: 0.9159491658210754, Accuracy: 1.0, Computation time: 1.2231736183166504\n",
      "Step: 5119, Loss: 0.9369373917579651, Accuracy: 0.96875, Computation time: 1.0873305797576904\n",
      "Step: 5120, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.4596478939056396\n",
      "Step: 5121, Loss: 0.9374761581420898, Accuracy: 0.96875, Computation time: 1.3451507091522217\n",
      "Step: 5122, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.032792329788208\n",
      "Step: 5123, Loss: 0.9159530401229858, Accuracy: 1.0, Computation time: 1.091299057006836\n",
      "Step: 5124, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.3473215103149414\n",
      "Step: 5125, Loss: 0.9375, Accuracy: 0.96875, Computation time: 1.0492441654205322\n",
      "Step: 5126, Loss: 0.9159863591194153, Accuracy: 1.0, Computation time: 1.2859570980072021\n",
      "Step: 5127, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.2372658252716064\n",
      "Step: 5128, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.3092098236083984\n",
      "Step: 5129, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.2851431369781494\n",
      "Step: 5130, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.5107800960540771\n",
      "Step: 5131, Loss: 0.9160332083702087, Accuracy: 1.0, Computation time: 1.2746033668518066\n",
      "Step: 5132, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.1485164165496826\n",
      "Step: 5133, Loss: 0.9347795844078064, Accuracy: 0.96875, Computation time: 1.4275360107421875\n",
      "Step: 5134, Loss: 0.915931761264801, Accuracy: 1.0, Computation time: 1.0497980117797852\n",
      "Step: 5135, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 0.9514167308807373\n",
      "Step: 5136, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.5318341255187988\n",
      "Step: 5137, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.2980506420135498\n",
      "Step: 5138, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.1618611812591553\n",
      "Step: 5139, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 1.0449202060699463\n",
      "Step: 5140, Loss: 0.9159573912620544, Accuracy: 1.0, Computation time: 1.406113862991333\n",
      "Step: 5141, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.0599863529205322\n",
      "########################\n",
      "Test loss: 1.0714892148971558, Test Accuracy_epoch37: 0.7730358839035034\n",
      "########################\n",
      "Step: 5142, Loss: 0.937543511390686, Accuracy: 0.96875, Computation time: 1.1162457466125488\n",
      "Step: 5143, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.3659873008728027\n",
      "Step: 5144, Loss: 0.9226876497268677, Accuracy: 1.0, Computation time: 1.585890769958496\n",
      "Step: 5145, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.4619550704956055\n",
      "Step: 5146, Loss: 0.9592609405517578, Accuracy: 0.9375, Computation time: 1.1331892013549805\n",
      "Step: 5147, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.4409103393554688\n",
      "Step: 5148, Loss: 0.9165644645690918, Accuracy: 1.0, Computation time: 1.3403024673461914\n",
      "Step: 5149, Loss: 0.9158954620361328, Accuracy: 1.0, Computation time: 1.2556829452514648\n",
      "Step: 5150, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 1.2476658821105957\n",
      "Step: 5151, Loss: 0.91591876745224, Accuracy: 1.0, Computation time: 1.189584732055664\n",
      "Step: 5152, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.245004415512085\n",
      "Step: 5153, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.388509750366211\n",
      "Step: 5154, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.1660826206207275\n",
      "Step: 5155, Loss: 0.9161390662193298, Accuracy: 1.0, Computation time: 1.681936502456665\n",
      "Step: 5156, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.0680499076843262\n",
      "Step: 5157, Loss: 0.9161013960838318, Accuracy: 1.0, Computation time: 1.664762258529663\n",
      "Step: 5158, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.2098286151885986\n",
      "Step: 5159, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 1.689619541168213\n",
      "Step: 5160, Loss: 0.9296445250511169, Accuracy: 0.96875, Computation time: 1.460881233215332\n",
      "Step: 5161, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 1.3556513786315918\n",
      "Step: 5162, Loss: 0.916005551815033, Accuracy: 1.0, Computation time: 1.2218990325927734\n",
      "Step: 5163, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.3487508296966553\n",
      "Step: 5164, Loss: 0.9162923097610474, Accuracy: 1.0, Computation time: 1.0216240882873535\n",
      "Step: 5165, Loss: 0.9160602688789368, Accuracy: 1.0, Computation time: 1.3906641006469727\n",
      "Step: 5166, Loss: 0.9159421324729919, Accuracy: 1.0, Computation time: 1.1243302822113037\n",
      "Step: 5167, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.6857469081878662\n",
      "Step: 5168, Loss: 0.9159793257713318, Accuracy: 1.0, Computation time: 1.570044994354248\n",
      "Step: 5169, Loss: 0.9161349534988403, Accuracy: 1.0, Computation time: 1.5712792873382568\n",
      "Step: 5170, Loss: 0.9162209033966064, Accuracy: 1.0, Computation time: 1.72182297706604\n",
      "Step: 5171, Loss: 0.915985643863678, Accuracy: 1.0, Computation time: 1.2072827816009521\n",
      "Step: 5172, Loss: 0.9356319308280945, Accuracy: 0.96875, Computation time: 1.256561517715454\n",
      "Step: 5173, Loss: 0.9373077750205994, Accuracy: 0.96875, Computation time: 1.3263776302337646\n",
      "Step: 5174, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 2.2448220252990723\n",
      "Step: 5175, Loss: 0.9159544706344604, Accuracy: 1.0, Computation time: 1.4772312641143799\n",
      "Step: 5176, Loss: 0.922571063041687, Accuracy: 1.0, Computation time: 1.4825189113616943\n",
      "Step: 5177, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.3047730922698975\n",
      "Step: 5178, Loss: 0.9159573316574097, Accuracy: 1.0, Computation time: 1.3461003303527832\n",
      "Step: 5179, Loss: 0.9205982089042664, Accuracy: 1.0, Computation time: 1.3103647232055664\n",
      "Step: 5180, Loss: 0.9378485083580017, Accuracy: 0.96875, Computation time: 1.382582187652588\n",
      "Step: 5181, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 1.2464640140533447\n",
      "Step: 5182, Loss: 0.9159606695175171, Accuracy: 1.0, Computation time: 1.1947133541107178\n",
      "Step: 5183, Loss: 0.9169582724571228, Accuracy: 1.0, Computation time: 1.6489546298980713\n",
      "Step: 5184, Loss: 0.9159635305404663, Accuracy: 1.0, Computation time: 1.300739049911499\n",
      "Step: 5185, Loss: 0.9164459109306335, Accuracy: 1.0, Computation time: 1.469811201095581\n",
      "Step: 5186, Loss: 0.9159994721412659, Accuracy: 1.0, Computation time: 1.6891460418701172\n",
      "Step: 5187, Loss: 0.9159359335899353, Accuracy: 1.0, Computation time: 1.349172592163086\n",
      "Step: 5188, Loss: 0.91604083776474, Accuracy: 1.0, Computation time: 1.2913658618927002\n",
      "Step: 5189, Loss: 0.9160414934158325, Accuracy: 1.0, Computation time: 1.2979702949523926\n",
      "Step: 5190, Loss: 0.9159443378448486, Accuracy: 1.0, Computation time: 1.4176814556121826\n",
      "Step: 5191, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.3473644256591797\n",
      "Step: 5192, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.635993242263794\n",
      "Step: 5193, Loss: 0.9159450531005859, Accuracy: 1.0, Computation time: 1.4035000801086426\n",
      "Step: 5194, Loss: 0.9369837641716003, Accuracy: 0.96875, Computation time: 1.4793193340301514\n",
      "Step: 5195, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.465095043182373\n",
      "Step: 5196, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.355072259902954\n",
      "Step: 5197, Loss: 0.9375593066215515, Accuracy: 0.96875, Computation time: 1.728883981704712\n",
      "Step: 5198, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.2349326610565186\n",
      "Step: 5199, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.377528190612793\n",
      "Step: 5200, Loss: 0.9179074764251709, Accuracy: 1.0, Computation time: 1.426499366760254\n",
      "Step: 5201, Loss: 0.9173588752746582, Accuracy: 1.0, Computation time: 1.6267762184143066\n",
      "Step: 5202, Loss: 0.9159616827964783, Accuracy: 1.0, Computation time: 1.2695603370666504\n",
      "Step: 5203, Loss: 0.9301570057868958, Accuracy: 0.96875, Computation time: 1.7609975337982178\n",
      "Step: 5204, Loss: 0.9159621596336365, Accuracy: 1.0, Computation time: 1.5319957733154297\n",
      "Step: 5205, Loss: 0.9160791635513306, Accuracy: 1.0, Computation time: 1.2541961669921875\n",
      "Step: 5206, Loss: 0.9378437399864197, Accuracy: 0.96875, Computation time: 1.2587857246398926\n",
      "Step: 5207, Loss: 0.9377946853637695, Accuracy: 0.96875, Computation time: 1.2964160442352295\n",
      "Step: 5208, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 1.273885726928711\n",
      "Step: 5209, Loss: 0.916907787322998, Accuracy: 1.0, Computation time: 1.3436150550842285\n",
      "Step: 5210, Loss: 0.9377567768096924, Accuracy: 0.96875, Computation time: 1.0851936340332031\n",
      "Step: 5211, Loss: 0.9159667491912842, Accuracy: 1.0, Computation time: 1.274961233139038\n",
      "Step: 5212, Loss: 0.916034996509552, Accuracy: 1.0, Computation time: 1.1958897113800049\n",
      "Step: 5213, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 1.1047630310058594\n",
      "Step: 5214, Loss: 0.9340847730636597, Accuracy: 0.96875, Computation time: 1.4158663749694824\n",
      "Step: 5215, Loss: 0.9159461259841919, Accuracy: 1.0, Computation time: 1.178917407989502\n",
      "Step: 5216, Loss: 0.915935218334198, Accuracy: 1.0, Computation time: 1.1819610595703125\n",
      "Step: 5217, Loss: 0.9377562999725342, Accuracy: 0.96875, Computation time: 1.2689671516418457\n",
      "Step: 5218, Loss: 0.9159508347511292, Accuracy: 1.0, Computation time: 1.0080444812774658\n",
      "Step: 5219, Loss: 0.9162150621414185, Accuracy: 1.0, Computation time: 1.4322867393493652\n",
      "Step: 5220, Loss: 0.9202313423156738, Accuracy: 1.0, Computation time: 1.4366827011108398\n",
      "Step: 5221, Loss: 0.9161624312400818, Accuracy: 1.0, Computation time: 1.1677935123443604\n",
      "Step: 5222, Loss: 0.9162065386772156, Accuracy: 1.0, Computation time: 1.1728367805480957\n",
      "Step: 5223, Loss: 0.9160284996032715, Accuracy: 1.0, Computation time: 1.4857089519500732\n",
      "Step: 5224, Loss: 0.9160555005073547, Accuracy: 1.0, Computation time: 0.934729814529419\n",
      "Step: 5225, Loss: 0.9163121581077576, Accuracy: 1.0, Computation time: 0.9746608734130859\n",
      "Step: 5226, Loss: 0.9159758687019348, Accuracy: 1.0, Computation time: 0.9641997814178467\n",
      "Step: 5227, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.162126064300537\n",
      "Step: 5228, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.0678253173828125\n",
      "Step: 5229, Loss: 0.9159597754478455, Accuracy: 1.0, Computation time: 1.0736849308013916\n",
      "Step: 5230, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 1.2245116233825684\n",
      "Step: 5231, Loss: 0.9159954786300659, Accuracy: 1.0, Computation time: 0.916013240814209\n",
      "Step: 5232, Loss: 0.9162446856498718, Accuracy: 1.0, Computation time: 1.6866512298583984\n",
      "Step: 5233, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 0.8737888336181641\n",
      "Step: 5234, Loss: 0.9159805774688721, Accuracy: 1.0, Computation time: 1.278151512145996\n",
      "Step: 5235, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.0729248523712158\n",
      "Step: 5236, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.2523674964904785\n",
      "Step: 5237, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 0.7795937061309814\n",
      "Step: 5238, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 0.866605281829834\n",
      "Step: 5239, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.3974800109863281\n",
      "Step: 5240, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 0.8700907230377197\n",
      "Step: 5241, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 0.9433019161224365\n",
      "Step: 5242, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 1.3846261501312256\n",
      "Step: 5243, Loss: 0.9158950448036194, Accuracy: 1.0, Computation time: 1.1015408039093018\n",
      "Step: 5244, Loss: 0.9583860039710999, Accuracy: 0.9375, Computation time: 1.4706318378448486\n",
      "Step: 5245, Loss: 0.9160953164100647, Accuracy: 1.0, Computation time: 1.3047499656677246\n",
      "Step: 5246, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.9259638786315918\n",
      "Step: 5247, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.2296485900878906\n",
      "Step: 5248, Loss: 0.916424036026001, Accuracy: 1.0, Computation time: 1.1945815086364746\n",
      "Step: 5249, Loss: 0.9588326215744019, Accuracy: 0.9375, Computation time: 1.4354708194732666\n",
      "Step: 5250, Loss: 0.9336841702461243, Accuracy: 0.96875, Computation time: 1.3027713298797607\n",
      "Step: 5251, Loss: 0.9158888459205627, Accuracy: 1.0, Computation time: 1.2287302017211914\n",
      "Step: 5252, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.3172249794006348\n",
      "Step: 5253, Loss: 0.9161002039909363, Accuracy: 1.0, Computation time: 1.138237714767456\n",
      "Step: 5254, Loss: 0.9375439286231995, Accuracy: 0.96875, Computation time: 0.8450233936309814\n",
      "Step: 5255, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.2089688777923584\n",
      "Step: 5256, Loss: 0.9163728952407837, Accuracy: 1.0, Computation time: 1.1830382347106934\n",
      "Step: 5257, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.0858662128448486\n",
      "Step: 5258, Loss: 0.9170567989349365, Accuracy: 1.0, Computation time: 0.9857978820800781\n",
      "Step: 5259, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.030487298965454\n",
      "Step: 5260, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.1116034984588623\n",
      "Step: 5261, Loss: 0.9172918796539307, Accuracy: 1.0, Computation time: 1.3226194381713867\n",
      "Step: 5262, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.2203409671783447\n",
      "Step: 5263, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.2994861602783203\n",
      "Step: 5264, Loss: 0.937542200088501, Accuracy: 0.96875, Computation time: 1.2757031917572021\n",
      "Step: 5265, Loss: 0.9376652836799622, Accuracy: 0.96875, Computation time: 1.164100170135498\n",
      "Step: 5266, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 1.1015701293945312\n",
      "Step: 5267, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 0.9680876731872559\n",
      "Step: 5268, Loss: 0.9164400696754456, Accuracy: 1.0, Computation time: 1.3290016651153564\n",
      "Step: 5269, Loss: 0.9161857962608337, Accuracy: 1.0, Computation time: 1.2573282718658447\n",
      "Step: 5270, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.2432174682617188\n",
      "Step: 5271, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.0024640560150146\n",
      "Step: 5272, Loss: 0.9161227941513062, Accuracy: 1.0, Computation time: 1.420684814453125\n",
      "Step: 5273, Loss: 0.915923535823822, Accuracy: 1.0, Computation time: 1.2337372303009033\n",
      "Step: 5274, Loss: 0.9375118613243103, Accuracy: 0.96875, Computation time: 1.0047192573547363\n",
      "Step: 5275, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 0.9997241497039795\n",
      "Step: 5276, Loss: 0.9159483313560486, Accuracy: 1.0, Computation time: 1.220902681350708\n",
      "Step: 5277, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.0185117721557617\n",
      "Step: 5278, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.5901007652282715\n",
      "Step: 5279, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.3607401847839355\n",
      "Step: 5280, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 1.0359280109405518\n",
      "########################\n",
      "Test loss: 1.07313072681427, Test Accuracy_epoch38: 0.7720659971237183\n",
      "########################\n",
      "Step: 5281, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.1236426830291748\n",
      "Step: 5282, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.1018304824829102\n",
      "Step: 5283, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.355104923248291\n",
      "Step: 5284, Loss: 0.9169344305992126, Accuracy: 1.0, Computation time: 1.4017672538757324\n",
      "Step: 5285, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.0926449298858643\n",
      "Step: 5286, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.091794729232788\n",
      "Step: 5287, Loss: 0.9160767793655396, Accuracy: 1.0, Computation time: 1.374070644378662\n",
      "Step: 5288, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.2851204872131348\n",
      "Step: 5289, Loss: 0.9161095023155212, Accuracy: 1.0, Computation time: 1.0215404033660889\n",
      "Step: 5290, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.081754207611084\n",
      "Step: 5291, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.390730619430542\n",
      "Step: 5292, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.7399508953094482\n",
      "Step: 5293, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.1728873252868652\n",
      "Step: 5294, Loss: 0.937562882900238, Accuracy: 0.96875, Computation time: 1.0211851596832275\n",
      "Step: 5295, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 1.0553994178771973\n",
      "Step: 5296, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.0549685955047607\n",
      "Step: 5297, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.124225378036499\n",
      "Step: 5298, Loss: 0.9220166802406311, Accuracy: 1.0, Computation time: 1.4022579193115234\n",
      "Step: 5299, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.0870437622070312\n",
      "Step: 5300, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 1.654271125793457\n",
      "Step: 5301, Loss: 0.9160647392272949, Accuracy: 1.0, Computation time: 1.0420198440551758\n",
      "Step: 5302, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.2263984680175781\n",
      "Step: 5303, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 0.9795246124267578\n",
      "Step: 5304, Loss: 0.9343580603599548, Accuracy: 0.96875, Computation time: 1.233783483505249\n",
      "Step: 5305, Loss: 0.9375234842300415, Accuracy: 0.96875, Computation time: 1.1644835472106934\n",
      "Step: 5306, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 1.603529930114746\n",
      "Step: 5307, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.1272711753845215\n",
      "Step: 5308, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 1.2179381847381592\n",
      "Step: 5309, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.0031025409698486\n",
      "Step: 5310, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.4679107666015625\n",
      "Step: 5311, Loss: 0.9158833026885986, Accuracy: 1.0, Computation time: 1.4618535041809082\n",
      "Step: 5312, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.3607418537139893\n",
      "Step: 5313, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.2773466110229492\n",
      "Step: 5314, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.1941914558410645\n",
      "Step: 5315, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 1.3551864624023438\n",
      "Step: 5316, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 0.9745447635650635\n",
      "Step: 5317, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.0630688667297363\n",
      "Step: 5318, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 0.9920082092285156\n",
      "Step: 5319, Loss: 0.9376794695854187, Accuracy: 0.96875, Computation time: 1.2678704261779785\n",
      "Step: 5320, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 1.0410034656524658\n",
      "Step: 5321, Loss: 0.9159716367721558, Accuracy: 1.0, Computation time: 1.324338436126709\n",
      "Step: 5322, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.1527159214019775\n",
      "Step: 5323, Loss: 0.937671959400177, Accuracy: 0.96875, Computation time: 1.363196611404419\n",
      "Step: 5324, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.223081350326538\n",
      "Step: 5325, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 0.9957895278930664\n",
      "Step: 5326, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.4548819065093994\n",
      "Step: 5327, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.2235171794891357\n",
      "Step: 5328, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 0.9181725978851318\n",
      "Step: 5329, Loss: 0.9375656247138977, Accuracy: 0.96875, Computation time: 1.0873501300811768\n",
      "Step: 5330, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.3876912593841553\n",
      "Step: 5331, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.312389612197876\n",
      "Step: 5332, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 1.5121872425079346\n",
      "Step: 5333, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.148862361907959\n",
      "Step: 5334, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.458395004272461\n",
      "Step: 5335, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.3696298599243164\n",
      "Step: 5336, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.1183795928955078\n",
      "Step: 5337, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 1.4719572067260742\n",
      "Step: 5338, Loss: 0.9374948740005493, Accuracy: 0.96875, Computation time: 1.0208680629730225\n",
      "Step: 5339, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.1009278297424316\n",
      "Step: 5340, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.173363447189331\n",
      "Step: 5341, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.0233657360076904\n",
      "Step: 5342, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.319425106048584\n",
      "Step: 5343, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 1.1138081550598145\n",
      "Step: 5344, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.091550588607788\n",
      "Step: 5345, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.1805744171142578\n",
      "Step: 5346, Loss: 0.9162510633468628, Accuracy: 1.0, Computation time: 1.5476107597351074\n",
      "Step: 5347, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 0.9504940509796143\n",
      "Step: 5348, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.1442267894744873\n",
      "Step: 5349, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.01285982131958\n",
      "Step: 5350, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.3505544662475586\n",
      "Step: 5351, Loss: 0.9375731945037842, Accuracy: 0.96875, Computation time: 1.3849296569824219\n",
      "Step: 5352, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.1111936569213867\n",
      "Step: 5353, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 0.9667026996612549\n",
      "Step: 5354, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.2948143482208252\n",
      "Step: 5355, Loss: 0.9375540614128113, Accuracy: 0.96875, Computation time: 1.1078457832336426\n",
      "Step: 5356, Loss: 0.9375201463699341, Accuracy: 0.96875, Computation time: 1.293445348739624\n",
      "Step: 5357, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.2935600280761719\n",
      "Step: 5358, Loss: 0.9262785315513611, Accuracy: 0.96875, Computation time: 1.0198719501495361\n",
      "Step: 5359, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.1730239391326904\n",
      "Step: 5360, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.0285253524780273\n",
      "Step: 5361, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 0.9862110614776611\n",
      "Step: 5362, Loss: 0.9160632491111755, Accuracy: 1.0, Computation time: 1.308246374130249\n",
      "Step: 5363, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 0.8890209197998047\n",
      "Step: 5364, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 0.9790410995483398\n",
      "Step: 5365, Loss: 0.9407085180282593, Accuracy: 0.96875, Computation time: 1.1405069828033447\n",
      "Step: 5366, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.0555050373077393\n",
      "Step: 5367, Loss: 0.9162783622741699, Accuracy: 1.0, Computation time: 1.6125941276550293\n",
      "Step: 5368, Loss: 0.9194513559341431, Accuracy: 1.0, Computation time: 2.072094202041626\n",
      "Step: 5369, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 1.0895488262176514\n",
      "Step: 5370, Loss: 0.9159310460090637, Accuracy: 1.0, Computation time: 1.1220262050628662\n",
      "Step: 5371, Loss: 0.915908694267273, Accuracy: 1.0, Computation time: 1.2596440315246582\n",
      "Step: 5372, Loss: 0.916088342666626, Accuracy: 1.0, Computation time: 1.345221996307373\n",
      "Step: 5373, Loss: 0.9160506129264832, Accuracy: 1.0, Computation time: 1.0860016345977783\n",
      "Step: 5374, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.130279541015625\n",
      "Step: 5375, Loss: 0.9166410565376282, Accuracy: 1.0, Computation time: 1.6469841003417969\n",
      "Step: 5376, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.1683933734893799\n",
      "Step: 5377, Loss: 0.9164736270904541, Accuracy: 1.0, Computation time: 1.7133803367614746\n",
      "Step: 5378, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.0497441291809082\n",
      "Step: 5379, Loss: 0.9375383257865906, Accuracy: 0.96875, Computation time: 1.1502091884613037\n",
      "Step: 5380, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 1.1808452606201172\n",
      "Step: 5381, Loss: 0.9375752210617065, Accuracy: 0.96875, Computation time: 1.138422966003418\n",
      "Step: 5382, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 1.4154818058013916\n",
      "Step: 5383, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.108020544052124\n",
      "Step: 5384, Loss: 0.9356207847595215, Accuracy: 0.96875, Computation time: 1.6137456893920898\n",
      "Step: 5385, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 1.0098557472229004\n",
      "Step: 5386, Loss: 0.9160196185112, Accuracy: 1.0, Computation time: 1.3881423473358154\n",
      "Step: 5387, Loss: 0.9369058609008789, Accuracy: 0.96875, Computation time: 2.542081594467163\n",
      "Step: 5388, Loss: 0.9166156649589539, Accuracy: 1.0, Computation time: 1.433495044708252\n",
      "Step: 5389, Loss: 0.9374401569366455, Accuracy: 0.96875, Computation time: 1.212282657623291\n",
      "Step: 5390, Loss: 0.9159978628158569, Accuracy: 1.0, Computation time: 1.3374733924865723\n",
      "Step: 5391, Loss: 0.9159222841262817, Accuracy: 1.0, Computation time: 1.2888941764831543\n",
      "Step: 5392, Loss: 0.915959894657135, Accuracy: 1.0, Computation time: 1.3732776641845703\n",
      "Step: 5393, Loss: 0.9159373044967651, Accuracy: 1.0, Computation time: 1.2952966690063477\n",
      "Step: 5394, Loss: 0.9159461259841919, Accuracy: 1.0, Computation time: 1.7022314071655273\n",
      "Step: 5395, Loss: 0.9159953594207764, Accuracy: 1.0, Computation time: 1.4907820224761963\n",
      "Step: 5396, Loss: 0.9158900380134583, Accuracy: 1.0, Computation time: 0.9697375297546387\n",
      "Step: 5397, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.1795213222503662\n",
      "Step: 5398, Loss: 0.9376722574234009, Accuracy: 0.96875, Computation time: 1.2839274406433105\n",
      "Step: 5399, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 1.0459206104278564\n",
      "Step: 5400, Loss: 0.9364961981773376, Accuracy: 0.96875, Computation time: 1.0835766792297363\n",
      "Step: 5401, Loss: 0.9159348011016846, Accuracy: 1.0, Computation time: 1.4489822387695312\n",
      "Step: 5402, Loss: 0.9159497022628784, Accuracy: 1.0, Computation time: 0.9982578754425049\n",
      "Step: 5403, Loss: 0.9159144163131714, Accuracy: 1.0, Computation time: 1.201479434967041\n",
      "Step: 5404, Loss: 0.9379307627677917, Accuracy: 0.96875, Computation time: 1.813692331314087\n",
      "Step: 5405, Loss: 0.9161954522132874, Accuracy: 1.0, Computation time: 1.7989826202392578\n",
      "Step: 5406, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.3693015575408936\n",
      "Step: 5407, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.0425572395324707\n",
      "Step: 5408, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.0388290882110596\n",
      "Step: 5409, Loss: 0.9195589423179626, Accuracy: 1.0, Computation time: 1.5606982707977295\n",
      "Step: 5410, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.2115848064422607\n",
      "Step: 5411, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.090388536453247\n",
      "Step: 5412, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.0049848556518555\n",
      "Step: 5413, Loss: 0.9376208186149597, Accuracy: 0.96875, Computation time: 0.9346590042114258\n",
      "Step: 5414, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 0.8711137771606445\n",
      "Step: 5415, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 0.8164365291595459\n",
      "Step: 5416, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.15952730178833\n",
      "Step: 5417, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 1.062157154083252\n",
      "Step: 5418, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.1745233535766602\n",
      "Step: 5419, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.0028142929077148\n",
      "########################\n",
      "Test loss: 1.071480393409729, Test Accuracy_epoch39: 0.7759457230567932\n",
      "########################\n",
      "Step: 5420, Loss: 0.9159459471702576, Accuracy: 1.0, Computation time: 1.1381886005401611\n",
      "Step: 5421, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 0.9729659557342529\n",
      "Step: 5422, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.1210172176361084\n",
      "Step: 5423, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.219411849975586\n",
      "Step: 5424, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.181121587753296\n",
      "Step: 5425, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.3623356819152832\n",
      "Step: 5426, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.2296366691589355\n",
      "Step: 5427, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 1.052586555480957\n",
      "Step: 5428, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.0910544395446777\n",
      "Step: 5429, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.2087657451629639\n",
      "Step: 5430, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.3375129699707031\n",
      "Step: 5431, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.0649409294128418\n",
      "Step: 5432, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 1.1304421424865723\n",
      "Step: 5433, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.0396537780761719\n",
      "Step: 5434, Loss: 0.9158390760421753, Accuracy: 1.0, Computation time: 1.131943702697754\n",
      "Step: 5435, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.1455047130584717\n",
      "Step: 5436, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.1227855682373047\n",
      "Step: 5437, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 0.9340212345123291\n",
      "Step: 5438, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.2447197437286377\n",
      "Step: 5439, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 1.3175575733184814\n",
      "Step: 5440, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.3374786376953125\n",
      "Step: 5441, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.1334209442138672\n",
      "Step: 5442, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.2618601322174072\n",
      "Step: 5443, Loss: 0.9158387780189514, Accuracy: 1.0, Computation time: 1.2759754657745361\n",
      "Step: 5444, Loss: 0.9162214398384094, Accuracy: 1.0, Computation time: 1.191476821899414\n",
      "Step: 5445, Loss: 0.9172644019126892, Accuracy: 1.0, Computation time: 1.2876145839691162\n",
      "Step: 5446, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 1.1483654975891113\n",
      "Step: 5447, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.296452522277832\n",
      "Step: 5448, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.0524675846099854\n",
      "Step: 5449, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 1.0607151985168457\n",
      "Step: 5450, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.0803217887878418\n",
      "Step: 5451, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 0.8815755844116211\n",
      "Step: 5452, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.3059849739074707\n",
      "Step: 5453, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.1739251613616943\n",
      "Step: 5454, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.0483098030090332\n",
      "Step: 5455, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.1451992988586426\n",
      "Step: 5456, Loss: 0.9167128205299377, Accuracy: 1.0, Computation time: 1.1386303901672363\n",
      "Step: 5457, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 0.9548096656799316\n",
      "Step: 5458, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.0811903476715088\n",
      "Step: 5459, Loss: 0.9355815649032593, Accuracy: 0.96875, Computation time: 1.0151410102844238\n",
      "Step: 5460, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.0272362232208252\n",
      "Step: 5461, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.2191131114959717\n",
      "Step: 5462, Loss: 0.9159073233604431, Accuracy: 1.0, Computation time: 1.1871294975280762\n",
      "Step: 5463, Loss: 0.9374269843101501, Accuracy: 0.96875, Computation time: 0.9160966873168945\n",
      "Step: 5464, Loss: 0.9159471988677979, Accuracy: 1.0, Computation time: 1.2679798603057861\n",
      "Step: 5465, Loss: 0.9159914255142212, Accuracy: 1.0, Computation time: 1.1845083236694336\n",
      "Step: 5466, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 1.204421043395996\n",
      "Step: 5467, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.170262336730957\n",
      "Step: 5468, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 0.9458858966827393\n",
      "Step: 5469, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.0002868175506592\n",
      "Step: 5470, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 1.0176677703857422\n",
      "Step: 5471, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.1104869842529297\n",
      "Step: 5472, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.1090092658996582\n",
      "Step: 5473, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.1639328002929688\n",
      "Step: 5474, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.2836828231811523\n",
      "Step: 5475, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.085080862045288\n",
      "Step: 5476, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.1063296794891357\n",
      "Step: 5477, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 0.8969161510467529\n",
      "Step: 5478, Loss: 0.9375342130661011, Accuracy: 0.96875, Computation time: 1.0112557411193848\n",
      "Step: 5479, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.1636931896209717\n",
      "Step: 5480, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.0472831726074219\n",
      "Step: 5481, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 1.16524076461792\n",
      "Step: 5482, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.4029302597045898\n",
      "Step: 5483, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.226135492324829\n",
      "Step: 5484, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 0.980161190032959\n",
      "Step: 5485, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.5639967918395996\n",
      "Step: 5486, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 1.2193503379821777\n",
      "Step: 5487, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.1815497875213623\n",
      "Step: 5488, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.0312561988830566\n",
      "Step: 5489, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 1.4477741718292236\n",
      "Step: 5490, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.6714825630187988\n",
      "Step: 5491, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 1.0180470943450928\n",
      "Step: 5492, Loss: 0.9375346899032593, Accuracy: 0.96875, Computation time: 1.923579216003418\n",
      "Step: 5493, Loss: 0.9337697625160217, Accuracy: 0.96875, Computation time: 1.1797070503234863\n",
      "Step: 5494, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.1702449321746826\n",
      "Step: 5495, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 1.032153844833374\n",
      "Step: 5496, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.4277746677398682\n",
      "Step: 5497, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.008286476135254\n",
      "Step: 5498, Loss: 0.9162695407867432, Accuracy: 1.0, Computation time: 1.0046396255493164\n",
      "Step: 5499, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.2832973003387451\n",
      "Step: 5500, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.2465720176696777\n",
      "Step: 5501, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.5405426025390625\n",
      "Step: 5502, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.2485322952270508\n",
      "Step: 5503, Loss: 0.9160502552986145, Accuracy: 1.0, Computation time: 0.9649937152862549\n",
      "Step: 5504, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.16562819480896\n",
      "Step: 5505, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.3282361030578613\n",
      "Step: 5506, Loss: 0.9375855922698975, Accuracy: 0.96875, Computation time: 1.4730310440063477\n",
      "Step: 5507, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 1.0025572776794434\n",
      "Step: 5508, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.1799030303955078\n",
      "Step: 5509, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 0.979252815246582\n",
      "Step: 5510, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.0535120964050293\n",
      "Step: 5511, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.557992696762085\n",
      "Step: 5512, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.2018928527832031\n",
      "Step: 5513, Loss: 0.9374887943267822, Accuracy: 0.96875, Computation time: 1.1664628982543945\n",
      "Step: 5514, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.1066052913665771\n",
      "Step: 5515, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.2772822380065918\n",
      "Step: 5516, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 0.9679760932922363\n",
      "Step: 5517, Loss: 0.9159653186798096, Accuracy: 1.0, Computation time: 1.7037949562072754\n",
      "Step: 5518, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 0.9270837306976318\n",
      "Step: 5519, Loss: 0.9375958442687988, Accuracy: 0.96875, Computation time: 1.0654804706573486\n",
      "Step: 5520, Loss: 0.9314098358154297, Accuracy: 0.96875, Computation time: 1.6251554489135742\n",
      "Step: 5521, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.4072096347808838\n",
      "Step: 5522, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 0.8974733352661133\n",
      "Step: 5523, Loss: 0.9160483479499817, Accuracy: 1.0, Computation time: 1.352221965789795\n",
      "Step: 5524, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 1.3366777896881104\n",
      "Step: 5525, Loss: 0.9161134958267212, Accuracy: 1.0, Computation time: 1.3637216091156006\n",
      "Step: 5526, Loss: 0.9160037040710449, Accuracy: 1.0, Computation time: 1.0706148147583008\n",
      "Step: 5527, Loss: 0.9159598350524902, Accuracy: 1.0, Computation time: 1.175372838973999\n",
      "Step: 5528, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.2962682247161865\n",
      "Step: 5529, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.0140395164489746\n",
      "Step: 5530, Loss: 0.9369890093803406, Accuracy: 0.96875, Computation time: 1.3835279941558838\n",
      "Step: 5531, Loss: 0.9203991889953613, Accuracy: 1.0, Computation time: 1.958568811416626\n",
      "Step: 5532, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 1.2565841674804688\n",
      "Step: 5533, Loss: 0.916077733039856, Accuracy: 1.0, Computation time: 1.5927541255950928\n",
      "Step: 5534, Loss: 0.916100800037384, Accuracy: 1.0, Computation time: 1.0962493419647217\n",
      "Step: 5535, Loss: 0.9162853360176086, Accuracy: 1.0, Computation time: 1.3858232498168945\n",
      "Step: 5536, Loss: 0.9163364768028259, Accuracy: 1.0, Computation time: 1.2520959377288818\n",
      "Step: 5537, Loss: 0.9277244806289673, Accuracy: 1.0, Computation time: 1.7500650882720947\n",
      "Step: 5538, Loss: 0.9161458611488342, Accuracy: 1.0, Computation time: 1.2074954509735107\n",
      "Step: 5539, Loss: 0.9382240176200867, Accuracy: 0.96875, Computation time: 1.3871614933013916\n",
      "Step: 5540, Loss: 0.9189512133598328, Accuracy: 1.0, Computation time: 1.421748161315918\n",
      "Step: 5541, Loss: 0.9163169860839844, Accuracy: 1.0, Computation time: 1.166377305984497\n",
      "Step: 5542, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.485914945602417\n",
      "Step: 5543, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.044259786605835\n",
      "Step: 5544, Loss: 0.9384641647338867, Accuracy: 0.96875, Computation time: 1.0801260471343994\n",
      "Step: 5545, Loss: 0.9159757494926453, Accuracy: 1.0, Computation time: 1.4559507369995117\n",
      "Step: 5546, Loss: 0.9161774516105652, Accuracy: 1.0, Computation time: 1.1060559749603271\n",
      "Step: 5547, Loss: 0.9166132807731628, Accuracy: 1.0, Computation time: 1.2751023769378662\n",
      "Step: 5548, Loss: 0.9379990100860596, Accuracy: 0.96875, Computation time: 1.0937421321868896\n",
      "Step: 5549, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 1.2405030727386475\n",
      "Step: 5550, Loss: 0.9160985946655273, Accuracy: 1.0, Computation time: 1.2279739379882812\n",
      "Step: 5551, Loss: 0.9229333400726318, Accuracy: 1.0, Computation time: 1.1360423564910889\n",
      "Step: 5552, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.2145228385925293\n",
      "Step: 5553, Loss: 0.9160959720611572, Accuracy: 1.0, Computation time: 1.0645179748535156\n",
      "Step: 5554, Loss: 0.9160512685775757, Accuracy: 1.0, Computation time: 1.0809733867645264\n",
      "Step: 5555, Loss: 0.915990948677063, Accuracy: 1.0, Computation time: 0.9509546756744385\n",
      "Step: 5556, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 0.9475522041320801\n",
      "Step: 5557, Loss: 0.9158998727798462, Accuracy: 1.0, Computation time: 1.2206532955169678\n",
      "Step: 5558, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 0.9944758415222168\n",
      "########################\n",
      "Test loss: 1.0729210376739502, Test Accuracy_epoch40: 0.7730358839035034\n",
      "########################\n",
      "Step: 5559, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.183077335357666\n",
      "Step: 5560, Loss: 0.9170279502868652, Accuracy: 1.0, Computation time: 1.2635431289672852\n",
      "Step: 5561, Loss: 0.9389293193817139, Accuracy: 0.96875, Computation time: 1.6578471660614014\n",
      "Step: 5562, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 1.0262327194213867\n",
      "Step: 5563, Loss: 0.9159326553344727, Accuracy: 1.0, Computation time: 1.0503110885620117\n",
      "Step: 5564, Loss: 0.9159517288208008, Accuracy: 1.0, Computation time: 1.1556267738342285\n",
      "Step: 5565, Loss: 0.915968120098114, Accuracy: 1.0, Computation time: 0.9507622718811035\n",
      "Step: 5566, Loss: 0.9160419702529907, Accuracy: 1.0, Computation time: 0.8836033344268799\n",
      "Step: 5567, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 1.0254521369934082\n",
      "Step: 5568, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 1.0487778186798096\n",
      "Step: 5569, Loss: 0.9375447630882263, Accuracy: 0.96875, Computation time: 1.171743631362915\n",
      "Step: 5570, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 0.9455082416534424\n",
      "Step: 5571, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.439889669418335\n",
      "Step: 5572, Loss: 0.9159679412841797, Accuracy: 1.0, Computation time: 0.8753244876861572\n",
      "Step: 5573, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 1.429948091506958\n",
      "Step: 5574, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 0.8912901878356934\n",
      "Step: 5575, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 0.8712038993835449\n",
      "Step: 5576, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 0.9637377262115479\n",
      "Step: 5577, Loss: 0.9374674558639526, Accuracy: 0.96875, Computation time: 1.0266950130462646\n",
      "Step: 5578, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.0316426753997803\n",
      "Step: 5579, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 0.937366247177124\n",
      "Step: 5580, Loss: 0.9375881552696228, Accuracy: 0.96875, Computation time: 0.9646470546722412\n",
      "Step: 5581, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.2218952178955078\n",
      "Step: 5582, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 0.9753055572509766\n",
      "Step: 5583, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 0.8952775001525879\n",
      "Step: 5584, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 0.9436571598052979\n",
      "Step: 5585, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 0.9637904167175293\n",
      "Step: 5586, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 1.3108813762664795\n",
      "Step: 5587, Loss: 0.915931224822998, Accuracy: 1.0, Computation time: 1.0516667366027832\n",
      "Step: 5588, Loss: 0.915992021560669, Accuracy: 1.0, Computation time: 0.9315900802612305\n",
      "Step: 5589, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.0325756072998047\n",
      "Step: 5590, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 0.8570630550384521\n",
      "Step: 5591, Loss: 0.9159570336341858, Accuracy: 1.0, Computation time: 1.34710693359375\n",
      "Step: 5592, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.0409741401672363\n",
      "Step: 5593, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 0.8865303993225098\n",
      "Step: 5594, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 1.2903339862823486\n",
      "Step: 5595, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.1794874668121338\n",
      "Step: 5596, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.0089960098266602\n",
      "Step: 5597, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.0769176483154297\n",
      "Step: 5598, Loss: 0.9356714487075806, Accuracy: 0.96875, Computation time: 0.9841115474700928\n",
      "Step: 5599, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 0.9537568092346191\n",
      "Step: 5600, Loss: 0.9375626444816589, Accuracy: 0.96875, Computation time: 0.945059061050415\n",
      "Step: 5601, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 0.9709913730621338\n",
      "Step: 5602, Loss: 0.9375156164169312, Accuracy: 0.96875, Computation time: 1.187253475189209\n",
      "Step: 5603, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.276158094406128\n",
      "Step: 5604, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 1.249934434890747\n",
      "Step: 5605, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 0.9558162689208984\n",
      "Step: 5606, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 0.8760230541229248\n",
      "Step: 5607, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.0775504112243652\n",
      "Step: 5608, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.2206807136535645\n",
      "Step: 5609, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.0956733226776123\n",
      "Step: 5610, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.071894645690918\n",
      "Step: 5611, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.0049848556518555\n",
      "Step: 5612, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.0309762954711914\n",
      "Step: 5613, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 0.8549211025238037\n",
      "Step: 5614, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.1857962608337402\n",
      "Step: 5615, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.5520789623260498\n",
      "Step: 5616, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.1107494831085205\n",
      "Step: 5617, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.0158421993255615\n",
      "Step: 5618, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.2426159381866455\n",
      "Step: 5619, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 0.9638729095458984\n",
      "Step: 5620, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.051711082458496\n",
      "Step: 5621, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 0.9692268371582031\n",
      "Step: 5622, Loss: 0.9158387780189514, Accuracy: 1.0, Computation time: 0.9007277488708496\n",
      "Step: 5623, Loss: 0.9336639642715454, Accuracy: 0.96875, Computation time: 0.8866422176361084\n",
      "Step: 5624, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 0.8250656127929688\n",
      "Step: 5625, Loss: 0.9375782012939453, Accuracy: 0.96875, Computation time: 1.0896246433258057\n",
      "Step: 5626, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.072697401046753\n",
      "Step: 5627, Loss: 0.9160419702529907, Accuracy: 1.0, Computation time: 1.713191270828247\n",
      "Step: 5628, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 0.8920981884002686\n",
      "Step: 5629, Loss: 0.9375787377357483, Accuracy: 0.96875, Computation time: 1.4716033935546875\n",
      "Step: 5630, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.0456311702728271\n",
      "Step: 5631, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 0.9237256050109863\n",
      "Step: 5632, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 0.9766354560852051\n",
      "Step: 5633, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 0.9256970882415771\n",
      "Step: 5634, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 0.8331451416015625\n",
      "Step: 5635, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 0.9526932239532471\n",
      "Step: 5636, Loss: 0.9158393740653992, Accuracy: 1.0, Computation time: 1.0761432647705078\n",
      "Step: 5637, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 0.9608173370361328\n",
      "Step: 5638, Loss: 0.9375650882720947, Accuracy: 0.96875, Computation time: 1.46287202835083\n",
      "Step: 5639, Loss: 0.9374587535858154, Accuracy: 0.96875, Computation time: 1.0591745376586914\n",
      "Step: 5640, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 1.3746337890625\n",
      "Step: 5641, Loss: 0.930871844291687, Accuracy: 0.96875, Computation time: 1.564945936203003\n",
      "Step: 5642, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.5126442909240723\n",
      "Step: 5643, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.2979018688201904\n",
      "Step: 5644, Loss: 0.9173428416252136, Accuracy: 1.0, Computation time: 1.0236341953277588\n",
      "Step: 5645, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.1074869632720947\n",
      "Step: 5646, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.3295886516571045\n",
      "Step: 5647, Loss: 0.9202772974967957, Accuracy: 1.0, Computation time: 0.8270742893218994\n",
      "Step: 5648, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.188934564590454\n",
      "Step: 5649, Loss: 0.9158453941345215, Accuracy: 1.0, Computation time: 0.8155851364135742\n",
      "Step: 5650, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.1604747772216797\n",
      "Step: 5651, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 1.140681266784668\n",
      "Step: 5652, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 1.043135404586792\n",
      "Step: 5653, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.158848762512207\n",
      "Step: 5654, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.54854416847229\n",
      "Step: 5655, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.0642304420471191\n",
      "Step: 5656, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 0.8227999210357666\n",
      "Step: 5657, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.169358730316162\n",
      "Step: 5658, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.2496063709259033\n",
      "Step: 5659, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.0392889976501465\n",
      "Step: 5660, Loss: 0.937508225440979, Accuracy: 0.96875, Computation time: 0.9652693271636963\n",
      "Step: 5661, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.4136357307434082\n",
      "Step: 5662, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.0362536907196045\n",
      "Step: 5663, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.8538041114807129\n",
      "Step: 5664, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.1177716255187988\n",
      "Step: 5665, Loss: 0.9375854134559631, Accuracy: 0.96875, Computation time: 1.1427030563354492\n",
      "Step: 5666, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 0.8749465942382812\n",
      "Step: 5667, Loss: 0.9163294434547424, Accuracy: 1.0, Computation time: 1.7550158500671387\n",
      "Step: 5668, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 0.9037396907806396\n",
      "Step: 5669, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.0368611812591553\n",
      "Step: 5670, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.3997013568878174\n",
      "Step: 5671, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.251035451889038\n",
      "Step: 5672, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.2034125328063965\n",
      "Step: 5673, Loss: 0.9159318208694458, Accuracy: 1.0, Computation time: 1.5521390438079834\n",
      "Step: 5674, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.20760178565979\n",
      "Step: 5675, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 0.9068925380706787\n",
      "Step: 5676, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.5798118114471436\n",
      "Step: 5677, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 0.9196245670318604\n",
      "Step: 5678, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.1239566802978516\n",
      "Step: 5679, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 1.2541821002960205\n",
      "Step: 5680, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.1437244415283203\n",
      "Step: 5681, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 0.9840004444122314\n",
      "Step: 5682, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 1.0271644592285156\n",
      "Step: 5683, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 1.1712911128997803\n",
      "Step: 5684, Loss: 0.9170712828636169, Accuracy: 1.0, Computation time: 1.6281993389129639\n",
      "Step: 5685, Loss: 0.9175718426704407, Accuracy: 1.0, Computation time: 1.7748382091522217\n",
      "Step: 5686, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 0.9686098098754883\n",
      "Step: 5687, Loss: 0.9196667075157166, Accuracy: 1.0, Computation time: 1.06606125831604\n",
      "Step: 5688, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 1.1811165809631348\n",
      "Step: 5689, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 0.9844646453857422\n",
      "Step: 5690, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.0236947536468506\n",
      "Step: 5691, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 0.8651299476623535\n",
      "Step: 5692, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.681095838546753\n",
      "Step: 5693, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.5817863941192627\n",
      "Step: 5694, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 0.9264533519744873\n",
      "Step: 5695, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.2420928478240967\n",
      "Step: 5696, Loss: 0.938539981842041, Accuracy: 0.96875, Computation time: 1.263878583908081\n",
      "Step: 5697, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 1.2009296417236328\n",
      "########################\n",
      "Test loss: 1.0728199481964111, Test Accuracy_epoch41: 0.7710960507392883\n",
      "########################\n",
      "Step: 5698, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.60361909866333\n",
      "Step: 5699, Loss: 0.9163464307785034, Accuracy: 1.0, Computation time: 1.658362627029419\n",
      "Step: 5700, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 1.0974607467651367\n",
      "Step: 5701, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.4216017723083496\n",
      "Step: 5702, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.19423246383667\n",
      "Step: 5703, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.091792345046997\n",
      "Step: 5704, Loss: 0.9159682989120483, Accuracy: 1.0, Computation time: 1.3274836540222168\n",
      "Step: 5705, Loss: 0.9159778952598572, Accuracy: 1.0, Computation time: 1.3939588069915771\n",
      "Step: 5706, Loss: 0.9374765157699585, Accuracy: 0.96875, Computation time: 1.208059549331665\n",
      "Step: 5707, Loss: 0.9160603284835815, Accuracy: 1.0, Computation time: 1.365964651107788\n",
      "Step: 5708, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.0160315036773682\n",
      "Step: 5709, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 0.8915915489196777\n",
      "Step: 5710, Loss: 0.9374998211860657, Accuracy: 0.96875, Computation time: 1.2485074996948242\n",
      "Step: 5711, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.5539727210998535\n",
      "Step: 5712, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.2823290824890137\n",
      "Step: 5713, Loss: 0.9241501092910767, Accuracy: 1.0, Computation time: 2.293760061264038\n",
      "Step: 5714, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 1.1006574630737305\n",
      "Step: 5715, Loss: 0.9159945249557495, Accuracy: 1.0, Computation time: 1.0761113166809082\n",
      "Step: 5716, Loss: 0.9379116296768188, Accuracy: 0.96875, Computation time: 1.3274626731872559\n",
      "Step: 5717, Loss: 0.9376768469810486, Accuracy: 0.96875, Computation time: 0.957362174987793\n",
      "Step: 5718, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.3900768756866455\n",
      "Step: 5719, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.4602680206298828\n",
      "Step: 5720, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.0694477558135986\n",
      "Step: 5721, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.4428982734680176\n",
      "Step: 5722, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.3059823513031006\n",
      "Step: 5723, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.3401155471801758\n",
      "Step: 5724, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 0.9796590805053711\n",
      "Step: 5725, Loss: 0.9159465432167053, Accuracy: 1.0, Computation time: 1.5530285835266113\n",
      "Step: 5726, Loss: 0.9163782596588135, Accuracy: 1.0, Computation time: 1.1511707305908203\n",
      "Step: 5727, Loss: 0.9159349799156189, Accuracy: 1.0, Computation time: 1.16768217086792\n",
      "Step: 5728, Loss: 0.9162991046905518, Accuracy: 1.0, Computation time: 1.083176612854004\n",
      "Step: 5729, Loss: 0.9160460233688354, Accuracy: 1.0, Computation time: 0.9831440448760986\n",
      "Step: 5730, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.0070083141326904\n",
      "Step: 5731, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.4786112308502197\n",
      "Step: 5732, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.2590487003326416\n",
      "Step: 5733, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 1.0523979663848877\n",
      "Step: 5734, Loss: 0.9291624426841736, Accuracy: 0.96875, Computation time: 1.1419520378112793\n",
      "Step: 5735, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.0156750679016113\n",
      "Step: 5736, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.0195910930633545\n",
      "Step: 5737, Loss: 0.9159205555915833, Accuracy: 1.0, Computation time: 0.9467642307281494\n",
      "Step: 5738, Loss: 0.9159995317459106, Accuracy: 1.0, Computation time: 1.278895616531372\n",
      "Step: 5739, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.118767499923706\n",
      "Step: 5740, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 0.955498456954956\n",
      "Step: 5741, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.3926374912261963\n",
      "Step: 5742, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.147104024887085\n",
      "Step: 5743, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 0.9681839942932129\n",
      "Step: 5744, Loss: 0.937503457069397, Accuracy: 0.96875, Computation time: 1.1657922267913818\n",
      "Step: 5745, Loss: 0.9337214231491089, Accuracy: 0.96875, Computation time: 1.772118091583252\n",
      "Step: 5746, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.1094884872436523\n",
      "Step: 5747, Loss: 0.9159075617790222, Accuracy: 1.0, Computation time: 1.2164480686187744\n",
      "Step: 5748, Loss: 0.9159680604934692, Accuracy: 1.0, Computation time: 0.9992899894714355\n",
      "Step: 5749, Loss: 0.9782949090003967, Accuracy: 0.90625, Computation time: 1.009220838546753\n",
      "Step: 5750, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 1.0362000465393066\n",
      "Step: 5751, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 0.9188964366912842\n",
      "Step: 5752, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.0568366050720215\n",
      "Step: 5753, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 0.9903414249420166\n",
      "Step: 5754, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.0810062885284424\n",
      "Step: 5755, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.9399030208587646\n",
      "Step: 5756, Loss: 0.9159218668937683, Accuracy: 1.0, Computation time: 0.9454858303070068\n",
      "Step: 5757, Loss: 0.9186541438102722, Accuracy: 1.0, Computation time: 1.2363483905792236\n",
      "Step: 5758, Loss: 0.9159268140792847, Accuracy: 1.0, Computation time: 1.6589462757110596\n",
      "Step: 5759, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.3211486339569092\n",
      "Step: 5760, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.0433349609375\n",
      "Step: 5761, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.2125511169433594\n",
      "Step: 5762, Loss: 0.9345294237136841, Accuracy: 0.96875, Computation time: 1.3904623985290527\n",
      "Step: 5763, Loss: 0.9187248349189758, Accuracy: 1.0, Computation time: 1.1257240772247314\n",
      "Step: 5764, Loss: 0.9159700274467468, Accuracy: 1.0, Computation time: 1.1011314392089844\n",
      "Step: 5765, Loss: 0.9158921837806702, Accuracy: 1.0, Computation time: 1.3432199954986572\n",
      "Step: 5766, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.1979594230651855\n",
      "Step: 5767, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 0.9712789058685303\n",
      "Step: 5768, Loss: 0.9159628748893738, Accuracy: 1.0, Computation time: 1.6200792789459229\n",
      "Step: 5769, Loss: 0.9159217476844788, Accuracy: 1.0, Computation time: 1.3760616779327393\n",
      "Step: 5770, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.3122034072875977\n",
      "Step: 5771, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.0490813255310059\n",
      "Step: 5772, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.132864236831665\n",
      "Step: 5773, Loss: 0.9221236109733582, Accuracy: 1.0, Computation time: 1.797985553741455\n",
      "Step: 5774, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.1267061233520508\n",
      "Step: 5775, Loss: 0.9159480333328247, Accuracy: 1.0, Computation time: 1.2255401611328125\n",
      "Step: 5776, Loss: 0.9159655570983887, Accuracy: 1.0, Computation time: 1.0348260402679443\n",
      "Step: 5777, Loss: 0.9160690307617188, Accuracy: 1.0, Computation time: 1.3393666744232178\n",
      "Step: 5778, Loss: 0.9161117672920227, Accuracy: 1.0, Computation time: 1.2840306758880615\n",
      "Step: 5779, Loss: 0.9160833358764648, Accuracy: 1.0, Computation time: 1.062283992767334\n",
      "Step: 5780, Loss: 0.9161416292190552, Accuracy: 1.0, Computation time: 1.135416030883789\n",
      "Step: 5781, Loss: 0.9159305691719055, Accuracy: 1.0, Computation time: 1.145301103591919\n",
      "Step: 5782, Loss: 0.9158843159675598, Accuracy: 1.0, Computation time: 1.4299170970916748\n",
      "Step: 5783, Loss: 0.9160357713699341, Accuracy: 1.0, Computation time: 1.5970189571380615\n",
      "Step: 5784, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 1.7413535118103027\n",
      "Step: 5785, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 1.086155652999878\n",
      "Step: 5786, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.663996696472168\n",
      "Step: 5787, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.294107437133789\n",
      "Step: 5788, Loss: 0.9374626278877258, Accuracy: 0.96875, Computation time: 1.3542673587799072\n",
      "Step: 5789, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.0579755306243896\n",
      "Step: 5790, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 1.2078931331634521\n",
      "Step: 5791, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.105863332748413\n",
      "Step: 5792, Loss: 0.9161252975463867, Accuracy: 1.0, Computation time: 1.7948486804962158\n",
      "Step: 5793, Loss: 0.9733227491378784, Accuracy: 0.90625, Computation time: 1.3856487274169922\n",
      "Step: 5794, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.1498234272003174\n",
      "Step: 5795, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.0549347400665283\n",
      "Step: 5796, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.407726764678955\n",
      "Step: 5797, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.1963269710540771\n",
      "Step: 5798, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 1.5156381130218506\n",
      "Step: 5799, Loss: 0.9159217476844788, Accuracy: 1.0, Computation time: 0.9773237705230713\n",
      "Step: 5800, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.2533931732177734\n",
      "Step: 5801, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.310293197631836\n",
      "Step: 5802, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.274193525314331\n",
      "Step: 5803, Loss: 0.9200885891914368, Accuracy: 1.0, Computation time: 1.5308430194854736\n",
      "Step: 5804, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.185084581375122\n",
      "Step: 5805, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.1868484020233154\n",
      "Step: 5806, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 1.1620042324066162\n",
      "Step: 5807, Loss: 0.9160099029541016, Accuracy: 1.0, Computation time: 1.066929578781128\n",
      "Step: 5808, Loss: 0.9160639047622681, Accuracy: 1.0, Computation time: 1.020446538925171\n",
      "Step: 5809, Loss: 0.9160428047180176, Accuracy: 1.0, Computation time: 1.0704033374786377\n",
      "Step: 5810, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 1.3737471103668213\n",
      "Step: 5811, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.1082789897918701\n",
      "Step: 5812, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.0812854766845703\n",
      "Step: 5813, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.250159502029419\n",
      "Step: 5814, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 1.2161445617675781\n",
      "Step: 5815, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.2011373043060303\n",
      "Step: 5816, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.1085808277130127\n",
      "Step: 5817, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.1481752395629883\n",
      "Step: 5818, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.6524014472961426\n",
      "Step: 5819, Loss: 0.9375598430633545, Accuracy: 0.96875, Computation time: 1.209498643875122\n",
      "Step: 5820, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.2121155261993408\n",
      "Step: 5821, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.0340430736541748\n",
      "Step: 5822, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.2024805545806885\n",
      "Step: 5823, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.0713727474212646\n",
      "Step: 5824, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.2634353637695312\n",
      "Step: 5825, Loss: 0.9158340096473694, Accuracy: 1.0, Computation time: 1.0058748722076416\n",
      "Step: 5826, Loss: 0.9158388376235962, Accuracy: 1.0, Computation time: 1.2597899436950684\n",
      "Step: 5827, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.025832176208496\n",
      "Step: 5828, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 0.9571216106414795\n",
      "Step: 5829, Loss: 0.9158380627632141, Accuracy: 1.0, Computation time: 1.159447193145752\n",
      "Step: 5830, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.3314869403839111\n",
      "Step: 5831, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.4773964881896973\n",
      "Step: 5832, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.066619634628296\n",
      "Step: 5833, Loss: 0.9201853275299072, Accuracy: 1.0, Computation time: 2.1777126789093018\n",
      "Step: 5834, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.1852056980133057\n",
      "Step: 5835, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.3487226963043213\n",
      "Step: 5836, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.1739625930786133\n",
      "########################\n",
      "Test loss: 1.0756083726882935, Test Accuracy_epoch42: 0.7681862711906433\n",
      "########################\n",
      "Step: 5837, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.5074095726013184\n",
      "Step: 5838, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.1780846118927002\n",
      "Step: 5839, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.0761315822601318\n",
      "Step: 5840, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 1.2129795551300049\n",
      "Step: 5841, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.5934622287750244\n",
      "Step: 5842, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.1462302207946777\n",
      "Step: 5843, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 1.460139513015747\n",
      "Step: 5844, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.030369520187378\n",
      "Step: 5845, Loss: 0.9158729910850525, Accuracy: 1.0, Computation time: 1.0829391479492188\n",
      "Step: 5846, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.0528607368469238\n",
      "Step: 5847, Loss: 0.9375086426734924, Accuracy: 0.96875, Computation time: 1.273564100265503\n",
      "Step: 5848, Loss: 0.9221838712692261, Accuracy: 1.0, Computation time: 1.13511323928833\n",
      "Step: 5849, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.185610055923462\n",
      "Step: 5850, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.064281940460205\n",
      "Step: 5851, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.0062823295593262\n",
      "Step: 5852, Loss: 0.9162044525146484, Accuracy: 1.0, Computation time: 1.2322089672088623\n",
      "Step: 5853, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.27044677734375\n",
      "Step: 5854, Loss: 0.937637209892273, Accuracy: 0.96875, Computation time: 0.9549086093902588\n",
      "Step: 5855, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.8840584754943848\n",
      "Step: 5856, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 0.8966388702392578\n",
      "Step: 5857, Loss: 0.9200918674468994, Accuracy: 1.0, Computation time: 1.3558638095855713\n",
      "Step: 5858, Loss: 0.9159753322601318, Accuracy: 1.0, Computation time: 1.30900239944458\n",
      "Step: 5859, Loss: 0.9160337448120117, Accuracy: 1.0, Computation time: 1.4853169918060303\n",
      "Step: 5860, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.3490486145019531\n",
      "Step: 5861, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 1.091888666152954\n",
      "Step: 5862, Loss: 0.9159634709358215, Accuracy: 1.0, Computation time: 1.14042067527771\n",
      "Step: 5863, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.179114580154419\n",
      "Step: 5864, Loss: 0.9375656247138977, Accuracy: 0.96875, Computation time: 1.25449538230896\n",
      "Step: 5865, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 1.4527466297149658\n",
      "Step: 5866, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 0.9972388744354248\n",
      "Step: 5867, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.0309545993804932\n",
      "Step: 5868, Loss: 0.9229559302330017, Accuracy: 1.0, Computation time: 2.2205140590667725\n",
      "Step: 5869, Loss: 0.9197446703910828, Accuracy: 1.0, Computation time: 1.926772117614746\n",
      "Step: 5870, Loss: 0.9376848340034485, Accuracy: 0.96875, Computation time: 1.4525785446166992\n",
      "Step: 5871, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.3111903667449951\n",
      "Step: 5872, Loss: 0.9389382004737854, Accuracy: 0.96875, Computation time: 1.2547240257263184\n",
      "Step: 5873, Loss: 0.9159746766090393, Accuracy: 1.0, Computation time: 1.2310044765472412\n",
      "Step: 5874, Loss: 0.9159751534461975, Accuracy: 1.0, Computation time: 1.1071577072143555\n",
      "Step: 5875, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 1.0528433322906494\n",
      "Step: 5876, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.0116231441497803\n",
      "Step: 5877, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 0.996483564376831\n",
      "Step: 5878, Loss: 0.9375215768814087, Accuracy: 0.96875, Computation time: 1.027449369430542\n",
      "Step: 5879, Loss: 0.9161543846130371, Accuracy: 1.0, Computation time: 1.2203330993652344\n",
      "Step: 5880, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 0.9164342880249023\n",
      "Step: 5881, Loss: 0.9159374237060547, Accuracy: 1.0, Computation time: 1.0190308094024658\n",
      "Step: 5882, Loss: 0.9375477433204651, Accuracy: 0.96875, Computation time: 1.2432265281677246\n",
      "Step: 5883, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 0.9019472599029541\n",
      "Step: 5884, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 0.8921678066253662\n",
      "Step: 5885, Loss: 0.9378219246864319, Accuracy: 0.96875, Computation time: 1.5186259746551514\n",
      "Step: 5886, Loss: 0.9377836585044861, Accuracy: 0.96875, Computation time: 0.986405611038208\n",
      "Step: 5887, Loss: 0.9164495468139648, Accuracy: 1.0, Computation time: 1.840266227722168\n",
      "Step: 5888, Loss: 0.9158942699432373, Accuracy: 1.0, Computation time: 0.8833169937133789\n",
      "Step: 5889, Loss: 0.9159876108169556, Accuracy: 1.0, Computation time: 0.9969432353973389\n",
      "Step: 5890, Loss: 0.9159387350082397, Accuracy: 1.0, Computation time: 0.8604910373687744\n",
      "Step: 5891, Loss: 0.9164714813232422, Accuracy: 1.0, Computation time: 0.8615915775299072\n",
      "Step: 5892, Loss: 0.9165710210800171, Accuracy: 1.0, Computation time: 1.0858590602874756\n",
      "Step: 5893, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 0.8991281986236572\n",
      "Step: 5894, Loss: 0.9379147887229919, Accuracy: 0.96875, Computation time: 1.1401374340057373\n",
      "Step: 5895, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 0.8706040382385254\n",
      "Step: 5896, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 0.9267103672027588\n",
      "Step: 5897, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.163135290145874\n",
      "Step: 5898, Loss: 0.9160519242286682, Accuracy: 1.0, Computation time: 1.072981595993042\n",
      "Step: 5899, Loss: 0.9159896373748779, Accuracy: 1.0, Computation time: 0.9874985218048096\n",
      "Step: 5900, Loss: 0.9159838557243347, Accuracy: 1.0, Computation time: 0.8437683582305908\n",
      "Step: 5901, Loss: 0.9376649856567383, Accuracy: 0.96875, Computation time: 1.2882583141326904\n",
      "Step: 5902, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 0.9253954887390137\n",
      "Step: 5903, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 1.1650011539459229\n",
      "Step: 5904, Loss: 0.9159713387489319, Accuracy: 1.0, Computation time: 1.0845279693603516\n",
      "Step: 5905, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.8722336292266846\n",
      "Step: 5906, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.0634636878967285\n",
      "Step: 5907, Loss: 0.9164114594459534, Accuracy: 1.0, Computation time: 1.31736159324646\n",
      "Step: 5908, Loss: 0.9158712029457092, Accuracy: 1.0, Computation time: 0.8235580921173096\n",
      "Step: 5909, Loss: 0.9159551858901978, Accuracy: 1.0, Computation time: 1.1960852146148682\n",
      "Step: 5910, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 1.1159231662750244\n",
      "Step: 5911, Loss: 0.9173305034637451, Accuracy: 1.0, Computation time: 0.8457911014556885\n",
      "Step: 5912, Loss: 0.9166450500488281, Accuracy: 1.0, Computation time: 0.8687231540679932\n",
      "Step: 5913, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 0.8700118064880371\n",
      "Step: 5914, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.920931339263916\n",
      "Step: 5915, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 0.8981771469116211\n",
      "Step: 5916, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.0148708820343018\n",
      "Step: 5917, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 0.8350419998168945\n",
      "Step: 5918, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 1.1939599514007568\n",
      "Step: 5919, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.2464370727539062\n",
      "Step: 5920, Loss: 0.9376044869422913, Accuracy: 0.96875, Computation time: 1.2850213050842285\n",
      "Step: 5921, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 1.648383378982544\n",
      "Step: 5922, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 0.8953168392181396\n",
      "Step: 5923, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.1554582118988037\n",
      "Step: 5924, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 0.9752383232116699\n",
      "Step: 5925, Loss: 0.9159799814224243, Accuracy: 1.0, Computation time: 1.0449609756469727\n",
      "Step: 5926, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 0.8727877140045166\n",
      "Step: 5927, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 3.536816358566284\n",
      "Step: 5928, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 0.8360466957092285\n",
      "Step: 5929, Loss: 0.9374263882637024, Accuracy: 0.96875, Computation time: 1.5051751136779785\n",
      "Step: 5930, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 0.9640457630157471\n",
      "Step: 5931, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 0.9053282737731934\n",
      "Step: 5932, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 0.849564790725708\n",
      "Step: 5933, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 1.5034279823303223\n",
      "Step: 5934, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 0.9443514347076416\n",
      "Step: 5935, Loss: 0.9160306453704834, Accuracy: 1.0, Computation time: 1.6569292545318604\n",
      "Step: 5936, Loss: 0.9163079261779785, Accuracy: 1.0, Computation time: 0.8668360710144043\n",
      "Step: 5937, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 0.8255524635314941\n",
      "Step: 5938, Loss: 0.9374957084655762, Accuracy: 0.96875, Computation time: 1.0889737606048584\n",
      "Step: 5939, Loss: 0.937604546546936, Accuracy: 0.96875, Computation time: 1.046555995941162\n",
      "Step: 5940, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 0.868229866027832\n",
      "Step: 5941, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.8418407440185547\n",
      "Step: 5942, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 1.186063289642334\n",
      "Step: 5943, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 0.9664726257324219\n",
      "Step: 5944, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 0.7648441791534424\n",
      "Step: 5945, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 0.9408822059631348\n",
      "Step: 5946, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 0.8141515254974365\n",
      "Step: 5947, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 0.8433949947357178\n",
      "Step: 5948, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 0.8834702968597412\n",
      "Step: 5949, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 0.8951010704040527\n",
      "Step: 5950, Loss: 0.9374685287475586, Accuracy: 0.96875, Computation time: 0.833519458770752\n",
      "Step: 5951, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.1163792610168457\n",
      "Step: 5952, Loss: 0.9375192523002625, Accuracy: 0.96875, Computation time: 0.8943359851837158\n",
      "Step: 5953, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.095916748046875\n",
      "Step: 5954, Loss: 0.9605409502983093, Accuracy: 0.9375, Computation time: 1.3399548530578613\n",
      "Step: 5955, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 0.9398832321166992\n",
      "Step: 5956, Loss: 0.9400728940963745, Accuracy: 0.96875, Computation time: 1.7920336723327637\n",
      "Step: 5957, Loss: 0.9376228451728821, Accuracy: 0.96875, Computation time: 1.1165616512298584\n",
      "Step: 5958, Loss: 0.9336398839950562, Accuracy: 0.96875, Computation time: 1.9215598106384277\n",
      "Step: 5959, Loss: 0.9163302779197693, Accuracy: 1.0, Computation time: 0.9133236408233643\n",
      "Step: 5960, Loss: 0.9163147211074829, Accuracy: 1.0, Computation time: 0.8926389217376709\n",
      "Step: 5961, Loss: 0.9169583320617676, Accuracy: 1.0, Computation time: 1.0011165142059326\n",
      "Step: 5962, Loss: 0.9212237000465393, Accuracy: 1.0, Computation time: 1.6516671180725098\n",
      "Step: 5963, Loss: 0.9166921377182007, Accuracy: 1.0, Computation time: 1.1075177192687988\n",
      "Step: 5964, Loss: 0.9166516661643982, Accuracy: 1.0, Computation time: 1.0198428630828857\n",
      "Step: 5965, Loss: 0.9161254167556763, Accuracy: 1.0, Computation time: 1.277144193649292\n",
      "Step: 5966, Loss: 0.9160411357879639, Accuracy: 1.0, Computation time: 1.3463330268859863\n",
      "Step: 5967, Loss: 0.9341296553611755, Accuracy: 0.96875, Computation time: 1.1441140174865723\n",
      "Step: 5968, Loss: 0.9162002205848694, Accuracy: 1.0, Computation time: 1.2766196727752686\n",
      "Step: 5969, Loss: 0.9189187288284302, Accuracy: 1.0, Computation time: 1.3115451335906982\n",
      "Step: 5970, Loss: 0.917908251285553, Accuracy: 1.0, Computation time: 0.9929659366607666\n",
      "Step: 5971, Loss: 0.9376621842384338, Accuracy: 0.96875, Computation time: 1.1033220291137695\n",
      "Step: 5972, Loss: 0.9164221286773682, Accuracy: 1.0, Computation time: 1.243077039718628\n",
      "Step: 5973, Loss: 0.934486448764801, Accuracy: 0.96875, Computation time: 1.3912432193756104\n",
      "Step: 5974, Loss: 0.9162643551826477, Accuracy: 1.0, Computation time: 1.0326151847839355\n",
      "Step: 5975, Loss: 0.9166092872619629, Accuracy: 1.0, Computation time: 1.318211317062378\n",
      "########################\n",
      "Test loss: 1.0721805095672607, Test Accuracy_epoch43: 0.7730358839035034\n",
      "########################\n",
      "Step: 5976, Loss: 0.9384181499481201, Accuracy: 0.96875, Computation time: 1.0738592147827148\n",
      "Step: 5977, Loss: 0.9168603420257568, Accuracy: 1.0, Computation time: 1.1866896152496338\n",
      "Step: 5978, Loss: 0.916510820388794, Accuracy: 1.0, Computation time: 1.412280559539795\n",
      "Step: 5979, Loss: 0.9199827909469604, Accuracy: 1.0, Computation time: 1.120638132095337\n",
      "Step: 5980, Loss: 0.9167743921279907, Accuracy: 1.0, Computation time: 1.1875221729278564\n",
      "Step: 5981, Loss: 0.9163885116577148, Accuracy: 1.0, Computation time: 1.1801087856292725\n",
      "Step: 5982, Loss: 0.9202658534049988, Accuracy: 1.0, Computation time: 1.1870551109313965\n",
      "Step: 5983, Loss: 0.9361096620559692, Accuracy: 0.96875, Computation time: 1.2785027027130127\n",
      "Step: 5984, Loss: 0.9159936308860779, Accuracy: 1.0, Computation time: 1.414125919342041\n",
      "Step: 5985, Loss: 0.916046142578125, Accuracy: 1.0, Computation time: 1.4342868328094482\n",
      "Step: 5986, Loss: 0.9591609239578247, Accuracy: 0.9375, Computation time: 1.2096037864685059\n",
      "Step: 5987, Loss: 0.9160358905792236, Accuracy: 1.0, Computation time: 1.1464197635650635\n",
      "Step: 5988, Loss: 0.916034996509552, Accuracy: 1.0, Computation time: 1.412785530090332\n",
      "Step: 5989, Loss: 0.9162790179252625, Accuracy: 1.0, Computation time: 1.0530800819396973\n",
      "Step: 5990, Loss: 0.9162442684173584, Accuracy: 1.0, Computation time: 1.1941852569580078\n",
      "Step: 5991, Loss: 0.9385237693786621, Accuracy: 0.96875, Computation time: 1.1370923519134521\n",
      "Step: 5992, Loss: 0.9159916639328003, Accuracy: 1.0, Computation time: 1.2004420757293701\n",
      "Step: 5993, Loss: 0.9271060824394226, Accuracy: 0.96875, Computation time: 1.5407471656799316\n",
      "Step: 5994, Loss: 0.9162140488624573, Accuracy: 1.0, Computation time: 1.477315902709961\n",
      "Step: 5995, Loss: 0.9165613055229187, Accuracy: 1.0, Computation time: 1.414721965789795\n",
      "Step: 5996, Loss: 0.920690655708313, Accuracy: 1.0, Computation time: 1.7119174003601074\n",
      "Step: 5997, Loss: 0.9323039650917053, Accuracy: 0.96875, Computation time: 1.1513862609863281\n",
      "Step: 5998, Loss: 0.91610187292099, Accuracy: 1.0, Computation time: 1.6323699951171875\n",
      "Step: 5999, Loss: 0.9167148470878601, Accuracy: 1.0, Computation time: 1.5724914073944092\n",
      "Step: 6000, Loss: 0.9159407615661621, Accuracy: 1.0, Computation time: 1.1708645820617676\n",
      "Step: 6001, Loss: 0.9160074591636658, Accuracy: 1.0, Computation time: 0.9484269618988037\n",
      "Step: 6002, Loss: 0.9167113900184631, Accuracy: 1.0, Computation time: 1.0571084022521973\n",
      "Step: 6003, Loss: 0.9164733290672302, Accuracy: 1.0, Computation time: 1.1882455348968506\n",
      "Step: 6004, Loss: 0.916651725769043, Accuracy: 1.0, Computation time: 1.0272424221038818\n",
      "Step: 6005, Loss: 0.916804850101471, Accuracy: 1.0, Computation time: 1.32267165184021\n",
      "Step: 6006, Loss: 0.9161660075187683, Accuracy: 1.0, Computation time: 1.4337432384490967\n",
      "Step: 6007, Loss: 0.9173080325126648, Accuracy: 1.0, Computation time: 1.6850252151489258\n",
      "Step: 6008, Loss: 0.9159950613975525, Accuracy: 1.0, Computation time: 0.9848387241363525\n",
      "Step: 6009, Loss: 0.9159565567970276, Accuracy: 1.0, Computation time: 1.015848159790039\n",
      "Step: 6010, Loss: 0.9161335229873657, Accuracy: 1.0, Computation time: 1.0316522121429443\n",
      "Step: 6011, Loss: 0.9161006212234497, Accuracy: 1.0, Computation time: 1.20847487449646\n",
      "Step: 6012, Loss: 0.9180780649185181, Accuracy: 1.0, Computation time: 1.3999385833740234\n",
      "Step: 6013, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 0.9761431217193604\n",
      "Step: 6014, Loss: 0.9376736283302307, Accuracy: 0.96875, Computation time: 1.1373558044433594\n",
      "Step: 6015, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 1.0611939430236816\n",
      "Step: 6016, Loss: 0.9159228801727295, Accuracy: 1.0, Computation time: 1.1161777973175049\n",
      "Step: 6017, Loss: 0.9164400100708008, Accuracy: 1.0, Computation time: 1.264254093170166\n",
      "Step: 6018, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.25384521484375\n",
      "Step: 6019, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 0.8416805267333984\n",
      "Step: 6020, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.274038314819336\n",
      "Step: 6021, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.3100614547729492\n",
      "Step: 6022, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 1.1700069904327393\n",
      "Step: 6023, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.2357463836669922\n",
      "Step: 6024, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.3995394706726074\n",
      "Step: 6025, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.0180072784423828\n",
      "Step: 6026, Loss: 0.9168298840522766, Accuracy: 1.0, Computation time: 1.0227484703063965\n",
      "Step: 6027, Loss: 0.9160913228988647, Accuracy: 1.0, Computation time: 2.165796995162964\n",
      "Step: 6028, Loss: 0.9160166382789612, Accuracy: 1.0, Computation time: 1.2271654605865479\n",
      "Step: 6029, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.43231201171875\n",
      "Step: 6030, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 1.0719246864318848\n",
      "Step: 6031, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 1.2777042388916016\n",
      "Step: 6032, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 1.229506492614746\n",
      "Step: 6033, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 0.988825798034668\n",
      "Step: 6034, Loss: 0.916913628578186, Accuracy: 1.0, Computation time: 1.4535322189331055\n",
      "Step: 6035, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.1646473407745361\n",
      "Step: 6036, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.1677391529083252\n",
      "Step: 6037, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.2030348777770996\n",
      "Step: 6038, Loss: 0.9160430431365967, Accuracy: 1.0, Computation time: 1.1580569744110107\n",
      "Step: 6039, Loss: 0.9159977436065674, Accuracy: 1.0, Computation time: 1.1131513118743896\n",
      "Step: 6040, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.2308306694030762\n",
      "Step: 6041, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 1.0633318424224854\n",
      "Step: 6042, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.0777812004089355\n",
      "Step: 6043, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 1.3235805034637451\n",
      "Step: 6044, Loss: 0.9181580543518066, Accuracy: 1.0, Computation time: 1.2024765014648438\n",
      "Step: 6045, Loss: 0.9158371090888977, Accuracy: 1.0, Computation time: 1.309044599533081\n",
      "Step: 6046, Loss: 0.9158428907394409, Accuracy: 1.0, Computation time: 1.2049567699432373\n",
      "Step: 6047, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.1368722915649414\n",
      "Step: 6048, Loss: 0.9376474618911743, Accuracy: 0.96875, Computation time: 0.9551889896392822\n",
      "Step: 6049, Loss: 0.915987491607666, Accuracy: 1.0, Computation time: 1.3076965808868408\n",
      "Step: 6050, Loss: 0.9162271618843079, Accuracy: 1.0, Computation time: 1.0772051811218262\n",
      "Step: 6051, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.013162612915039\n",
      "Step: 6052, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 0.9432210922241211\n",
      "Step: 6053, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 0.8928356170654297\n",
      "Step: 6054, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 0.9763190746307373\n",
      "Step: 6055, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.1094987392425537\n",
      "Step: 6056, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 0.9835402965545654\n",
      "Step: 6057, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 0.8802239894866943\n",
      "Step: 6058, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 1.0438721179962158\n",
      "Step: 6059, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.16524338722229\n",
      "Step: 6060, Loss: 0.9249421954154968, Accuracy: 1.0, Computation time: 1.0199999809265137\n",
      "Step: 6061, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.2347564697265625\n",
      "Step: 6062, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.2189085483551025\n",
      "Step: 6063, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.4145588874816895\n",
      "Step: 6064, Loss: 0.9159270524978638, Accuracy: 1.0, Computation time: 1.4996087551116943\n",
      "Step: 6065, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.7499110698699951\n",
      "Step: 6066, Loss: 0.9375126957893372, Accuracy: 0.96875, Computation time: 1.2434725761413574\n",
      "Step: 6067, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.4441180229187012\n",
      "Step: 6068, Loss: 0.9375657439231873, Accuracy: 0.96875, Computation time: 1.2628445625305176\n",
      "Step: 6069, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.2883951663970947\n",
      "Step: 6070, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 1.4723396301269531\n",
      "Step: 6071, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.0196082592010498\n",
      "Step: 6072, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 0.9710590839385986\n",
      "Step: 6073, Loss: 0.9380776286125183, Accuracy: 0.96875, Computation time: 1.3551006317138672\n",
      "Step: 6074, Loss: 0.9335017204284668, Accuracy: 0.96875, Computation time: 1.3355412483215332\n",
      "Step: 6075, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.4791734218597412\n",
      "Step: 6076, Loss: 0.9176698923110962, Accuracy: 1.0, Computation time: 0.9959948062896729\n",
      "Step: 6077, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.1323866844177246\n",
      "Step: 6078, Loss: 0.9159273505210876, Accuracy: 1.0, Computation time: 1.136803150177002\n",
      "Step: 6079, Loss: 0.9159507155418396, Accuracy: 1.0, Computation time: 1.1072063446044922\n",
      "Step: 6080, Loss: 0.9161459803581238, Accuracy: 1.0, Computation time: 1.0924327373504639\n",
      "Step: 6081, Loss: 0.915882408618927, Accuracy: 1.0, Computation time: 1.2658162117004395\n",
      "Step: 6082, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.186448335647583\n",
      "Step: 6083, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.228431224822998\n",
      "Step: 6084, Loss: 0.9161504507064819, Accuracy: 1.0, Computation time: 1.2185351848602295\n",
      "Step: 6085, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.2952048778533936\n",
      "Step: 6086, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.2896432876586914\n",
      "Step: 6087, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.2886369228363037\n",
      "Step: 6088, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.15968918800354\n",
      "Step: 6089, Loss: 0.915947437286377, Accuracy: 1.0, Computation time: 1.07295823097229\n",
      "Step: 6090, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.2386515140533447\n",
      "Step: 6091, Loss: 0.945279061794281, Accuracy: 0.9375, Computation time: 1.538102626800537\n",
      "Step: 6092, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.2449924945831299\n",
      "Step: 6093, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.4176907539367676\n",
      "Step: 6094, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.302058458328247\n",
      "Step: 6095, Loss: 0.9159436225891113, Accuracy: 1.0, Computation time: 1.4636688232421875\n",
      "Step: 6096, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.2138590812683105\n",
      "Step: 6097, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.1778502464294434\n",
      "Step: 6098, Loss: 0.9159973859786987, Accuracy: 1.0, Computation time: 1.011091709136963\n",
      "Step: 6099, Loss: 0.9160834550857544, Accuracy: 1.0, Computation time: 1.135974407196045\n",
      "Step: 6100, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.2672805786132812\n",
      "Step: 6101, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 1.3385546207427979\n",
      "Step: 6102, Loss: 0.9173938035964966, Accuracy: 1.0, Computation time: 1.6639137268066406\n",
      "Step: 6103, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.0418694019317627\n",
      "Step: 6104, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 1.2138373851776123\n",
      "Step: 6105, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.070188283920288\n",
      "Step: 6106, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.064239501953125\n",
      "Step: 6107, Loss: 0.9160168170928955, Accuracy: 1.0, Computation time: 2.0891263484954834\n",
      "Step: 6108, Loss: 0.9375874996185303, Accuracy: 0.96875, Computation time: 1.2702946662902832\n",
      "Step: 6109, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 1.3138580322265625\n",
      "Step: 6110, Loss: 0.9377439022064209, Accuracy: 0.96875, Computation time: 1.9687421321868896\n",
      "Step: 6111, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.358903408050537\n",
      "Step: 6112, Loss: 0.9160063862800598, Accuracy: 1.0, Computation time: 1.343013048171997\n",
      "Step: 6113, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.2493500709533691\n",
      "Step: 6114, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 0.9802844524383545\n",
      "########################\n",
      "Test loss: 1.0714999437332153, Test Accuracy_epoch44: 0.7730358839035034\n",
      "########################\n",
      "Step: 6115, Loss: 0.916047990322113, Accuracy: 1.0, Computation time: 1.1954898834228516\n",
      "Step: 6116, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.048386573791504\n",
      "Step: 6117, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.0655016899108887\n",
      "Step: 6118, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.0065979957580566\n",
      "Step: 6119, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.5230958461761475\n",
      "Step: 6120, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 1.2182071208953857\n",
      "Step: 6121, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.4823999404907227\n",
      "Step: 6122, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.3635151386260986\n",
      "Step: 6123, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.4579071998596191\n",
      "Step: 6124, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.0663764476776123\n",
      "Step: 6125, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 1.1658973693847656\n",
      "Step: 6126, Loss: 0.9158603549003601, Accuracy: 1.0, Computation time: 1.102440595626831\n",
      "Step: 6127, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.1142871379852295\n",
      "Step: 6128, Loss: 0.9161534905433655, Accuracy: 1.0, Computation time: 1.1477231979370117\n",
      "Step: 6129, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 1.2152984142303467\n",
      "Step: 6130, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 1.0804407596588135\n",
      "Step: 6131, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.2408041954040527\n",
      "Step: 6132, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.0501728057861328\n",
      "Step: 6133, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.1234352588653564\n",
      "Step: 6134, Loss: 0.9158448576927185, Accuracy: 1.0, Computation time: 1.3411355018615723\n",
      "Step: 6135, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.1170589923858643\n",
      "Step: 6136, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.1312227249145508\n",
      "Step: 6137, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.070702314376831\n",
      "Step: 6138, Loss: 0.9375391602516174, Accuracy: 0.96875, Computation time: 1.4105629920959473\n",
      "Step: 6139, Loss: 0.9387660026550293, Accuracy: 0.96875, Computation time: 1.2145698070526123\n",
      "Step: 6140, Loss: 0.9175196886062622, Accuracy: 1.0, Computation time: 1.448699951171875\n",
      "Step: 6141, Loss: 0.9375926852226257, Accuracy: 0.96875, Computation time: 1.1758160591125488\n",
      "Step: 6142, Loss: 0.941544234752655, Accuracy: 0.96875, Computation time: 1.846358060836792\n",
      "Step: 6143, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 0.9867527484893799\n",
      "Step: 6144, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 1.0279419422149658\n",
      "Step: 6145, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.1928701400756836\n",
      "Step: 6146, Loss: 0.9159640669822693, Accuracy: 1.0, Computation time: 1.025869607925415\n",
      "Step: 6147, Loss: 0.9159508943557739, Accuracy: 1.0, Computation time: 1.4041109085083008\n",
      "Step: 6148, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 1.0436315536499023\n",
      "Step: 6149, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 0.8842332363128662\n",
      "Step: 6150, Loss: 0.9360966682434082, Accuracy: 0.96875, Computation time: 2.0126166343688965\n",
      "Step: 6151, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.3365240097045898\n",
      "Step: 6152, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 0.8604884147644043\n",
      "Step: 6153, Loss: 0.9375719428062439, Accuracy: 0.96875, Computation time: 1.115128755569458\n",
      "Step: 6154, Loss: 0.9375682473182678, Accuracy: 0.96875, Computation time: 1.2782893180847168\n",
      "Step: 6155, Loss: 0.9159583449363708, Accuracy: 1.0, Computation time: 0.9384562969207764\n",
      "Step: 6156, Loss: 0.9159865975379944, Accuracy: 1.0, Computation time: 0.9400172233581543\n",
      "Step: 6157, Loss: 0.9172695279121399, Accuracy: 1.0, Computation time: 1.419003963470459\n",
      "Step: 6158, Loss: 0.9376295804977417, Accuracy: 0.96875, Computation time: 0.8893535137176514\n",
      "Step: 6159, Loss: 0.9159578084945679, Accuracy: 1.0, Computation time: 1.1162664890289307\n",
      "Step: 6160, Loss: 0.9375569820404053, Accuracy: 0.96875, Computation time: 1.1740944385528564\n",
      "Step: 6161, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 1.1916625499725342\n",
      "Step: 6162, Loss: 0.937619686126709, Accuracy: 0.96875, Computation time: 1.2194116115570068\n",
      "Step: 6163, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 0.9194815158843994\n",
      "Step: 6164, Loss: 0.9374979734420776, Accuracy: 0.96875, Computation time: 0.9814262390136719\n",
      "Step: 6165, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.1971776485443115\n",
      "Step: 6166, Loss: 0.9375365972518921, Accuracy: 0.96875, Computation time: 1.0159974098205566\n",
      "Step: 6167, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 1.01273512840271\n",
      "Step: 6168, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 1.0873987674713135\n",
      "Step: 6169, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.1689167022705078\n",
      "Step: 6170, Loss: 0.9264264106750488, Accuracy: 0.96875, Computation time: 1.021097183227539\n",
      "Step: 6171, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.0058505535125732\n",
      "Step: 6172, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.0181217193603516\n",
      "Step: 6173, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.3746342658996582\n",
      "Step: 6174, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.096769094467163\n",
      "Step: 6175, Loss: 0.937630832195282, Accuracy: 0.96875, Computation time: 1.1216087341308594\n",
      "Step: 6176, Loss: 0.9304067492485046, Accuracy: 0.96875, Computation time: 1.7439696788787842\n",
      "Step: 6177, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.4243757724761963\n",
      "Step: 6178, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.3071448802947998\n",
      "Step: 6179, Loss: 0.9163863062858582, Accuracy: 1.0, Computation time: 1.7259142398834229\n",
      "Step: 6180, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 0.9795486927032471\n",
      "Step: 6181, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 1.4458198547363281\n",
      "Step: 6182, Loss: 0.9159705638885498, Accuracy: 1.0, Computation time: 0.9690134525299072\n",
      "Step: 6183, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 0.9861123561859131\n",
      "Step: 6184, Loss: 0.9160711765289307, Accuracy: 1.0, Computation time: 1.4750697612762451\n",
      "Step: 6185, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 0.9848241806030273\n",
      "Step: 6186, Loss: 0.915922999382019, Accuracy: 1.0, Computation time: 1.4302117824554443\n",
      "Step: 6187, Loss: 0.9159741997718811, Accuracy: 1.0, Computation time: 1.60359525680542\n",
      "Step: 6188, Loss: 0.9173526763916016, Accuracy: 1.0, Computation time: 1.3079099655151367\n",
      "Step: 6189, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.0356640815734863\n",
      "Step: 6190, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.0537095069885254\n",
      "Step: 6191, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 1.1716580390930176\n",
      "Step: 6192, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 1.0841619968414307\n",
      "Step: 6193, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 1.1939489841461182\n",
      "Step: 6194, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.3417260646820068\n",
      "Step: 6195, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.243455410003662\n",
      "Step: 6196, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.7301082611083984\n",
      "Step: 6197, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.1349003314971924\n",
      "Step: 6198, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.1905691623687744\n",
      "Step: 6199, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.2471203804016113\n",
      "Step: 6200, Loss: 0.9356045722961426, Accuracy: 0.96875, Computation time: 1.7396275997161865\n",
      "Step: 6201, Loss: 0.9320958256721497, Accuracy: 0.96875, Computation time: 1.2933950424194336\n",
      "Step: 6202, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.1466748714447021\n",
      "Step: 6203, Loss: 0.9159649610519409, Accuracy: 1.0, Computation time: 1.138641595840454\n",
      "Step: 6204, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 1.2218093872070312\n",
      "Step: 6205, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 1.209385871887207\n",
      "Step: 6206, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.1934404373168945\n",
      "Step: 6207, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.0730435848236084\n",
      "Step: 6208, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.5265467166900635\n",
      "Step: 6209, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 1.3303332328796387\n",
      "Step: 6210, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.4003615379333496\n",
      "Step: 6211, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.2023999691009521\n",
      "Step: 6212, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.2422499656677246\n",
      "Step: 6213, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.2104902267456055\n",
      "Step: 6214, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.3503022193908691\n",
      "Step: 6215, Loss: 0.9176602959632874, Accuracy: 1.0, Computation time: 1.3603250980377197\n",
      "Step: 6216, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.2512543201446533\n",
      "Step: 6217, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.4318549633026123\n",
      "Step: 6218, Loss: 0.9375566840171814, Accuracy: 0.96875, Computation time: 1.2502825260162354\n",
      "Step: 6219, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.5730702877044678\n",
      "Step: 6220, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.1491808891296387\n",
      "Step: 6221, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 1.385770320892334\n",
      "Step: 6222, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 0.887582540512085\n",
      "Step: 6223, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 1.591282844543457\n",
      "Step: 6224, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.4871339797973633\n",
      "Step: 6225, Loss: 0.9158371686935425, Accuracy: 1.0, Computation time: 1.2581102848052979\n",
      "Step: 6226, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.4341635704040527\n",
      "Step: 6227, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.3329029083251953\n",
      "Step: 6228, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.4593546390533447\n",
      "Step: 6229, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 0.774871826171875\n",
      "Step: 6230, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.5680439472198486\n",
      "Step: 6231, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.0723371505737305\n",
      "Step: 6232, Loss: 0.915956974029541, Accuracy: 1.0, Computation time: 1.5547802448272705\n",
      "Step: 6233, Loss: 0.9375813603401184, Accuracy: 0.96875, Computation time: 1.2412464618682861\n",
      "Step: 6234, Loss: 0.9159278869628906, Accuracy: 1.0, Computation time: 1.0537867546081543\n",
      "Step: 6235, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.2964375019073486\n",
      "Step: 6236, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 1.1730842590332031\n",
      "Step: 6237, Loss: 0.9159896373748779, Accuracy: 1.0, Computation time: 1.4408559799194336\n",
      "Step: 6238, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.7163937091827393\n",
      "Step: 6239, Loss: 0.9158397912979126, Accuracy: 1.0, Computation time: 1.0980103015899658\n",
      "Step: 6240, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.2313289642333984\n",
      "Step: 6241, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.1368589401245117\n",
      "Step: 6242, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 0.9352016448974609\n",
      "Step: 6243, Loss: 0.9160915017127991, Accuracy: 1.0, Computation time: 1.951491117477417\n",
      "Step: 6244, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.275200605392456\n",
      "Step: 6245, Loss: 0.9159132242202759, Accuracy: 1.0, Computation time: 1.1747519969940186\n",
      "Step: 6246, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 0.9732575416564941\n",
      "Step: 6247, Loss: 0.9375286102294922, Accuracy: 0.96875, Computation time: 1.1333866119384766\n",
      "Step: 6248, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.3266830444335938\n",
      "Step: 6249, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.1334319114685059\n",
      "Step: 6250, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 1.219468116760254\n",
      "Step: 6251, Loss: 0.9158408045768738, Accuracy: 1.0, Computation time: 0.9951152801513672\n",
      "Step: 6252, Loss: 0.9165722727775574, Accuracy: 1.0, Computation time: 1.945425033569336\n",
      "Step: 6253, Loss: 0.915834367275238, Accuracy: 1.0, Computation time: 1.9797508716583252\n",
      "########################\n",
      "Test loss: 1.0719822645187378, Test Accuracy_epoch45: 0.7730358839035034\n",
      "########################\n",
      "Step: 6254, Loss: 0.9375227093696594, Accuracy: 0.96875, Computation time: 1.5507137775421143\n",
      "Step: 6255, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 1.2153618335723877\n",
      "Step: 6256, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 1.061514139175415\n",
      "Step: 6257, Loss: 0.9340450763702393, Accuracy: 0.96875, Computation time: 1.0548851490020752\n",
      "Step: 6258, Loss: 0.9374955296516418, Accuracy: 0.96875, Computation time: 1.117715835571289\n",
      "Step: 6259, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 0.9722449779510498\n",
      "Step: 6260, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 0.9886887073516846\n",
      "Step: 6261, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.7171952724456787\n",
      "Step: 6262, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.5009815692901611\n",
      "Step: 6263, Loss: 0.9365519881248474, Accuracy: 0.96875, Computation time: 1.6897494792938232\n",
      "Step: 6264, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.010378360748291\n",
      "Step: 6265, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.5039629936218262\n",
      "Step: 6266, Loss: 0.9258421063423157, Accuracy: 0.96875, Computation time: 1.9289507865905762\n",
      "Step: 6267, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.1365749835968018\n",
      "Step: 6268, Loss: 0.9196059703826904, Accuracy: 1.0, Computation time: 0.9034891128540039\n",
      "Step: 6269, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.0760507583618164\n",
      "Step: 6270, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.068073034286499\n",
      "Step: 6271, Loss: 0.9375870227813721, Accuracy: 0.96875, Computation time: 0.9548633098602295\n",
      "Step: 6272, Loss: 0.9375250339508057, Accuracy: 0.96875, Computation time: 1.2361319065093994\n",
      "Step: 6273, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.1948256492614746\n",
      "Step: 6274, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 0.9973504543304443\n",
      "Step: 6275, Loss: 0.9162092804908752, Accuracy: 1.0, Computation time: 1.4772694110870361\n",
      "Step: 6276, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 0.8849799633026123\n",
      "Step: 6277, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 1.1605863571166992\n",
      "Step: 6278, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 0.8663415908813477\n",
      "Step: 6279, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.3532578945159912\n",
      "Step: 6280, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 0.9472663402557373\n",
      "Step: 6281, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 1.078345537185669\n",
      "Step: 6282, Loss: 0.9376251697540283, Accuracy: 0.96875, Computation time: 1.2361950874328613\n",
      "Step: 6283, Loss: 0.915919303894043, Accuracy: 1.0, Computation time: 0.9853942394256592\n",
      "Step: 6284, Loss: 0.9170807003974915, Accuracy: 1.0, Computation time: 1.1892211437225342\n",
      "Step: 6285, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.237553358078003\n",
      "Step: 6286, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.120098352432251\n",
      "Step: 6287, Loss: 0.9352099895477295, Accuracy: 0.96875, Computation time: 1.1100413799285889\n",
      "Step: 6288, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.46297287940979\n",
      "Step: 6289, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.0497989654541016\n",
      "Step: 6290, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 0.9415099620819092\n",
      "Step: 6291, Loss: 0.9159244298934937, Accuracy: 1.0, Computation time: 1.195387363433838\n",
      "Step: 6292, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.2812607288360596\n",
      "Step: 6293, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.3486964702606201\n",
      "Step: 6294, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.0245442390441895\n",
      "Step: 6295, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 0.996631383895874\n",
      "Step: 6296, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 0.9683272838592529\n",
      "Step: 6297, Loss: 0.9208792448043823, Accuracy: 1.0, Computation time: 1.535231113433838\n",
      "Step: 6298, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 1.0103561878204346\n",
      "Step: 6299, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 1.1469345092773438\n",
      "Step: 6300, Loss: 0.9159905314445496, Accuracy: 1.0, Computation time: 1.0797219276428223\n",
      "Step: 6301, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 0.988243579864502\n",
      "Step: 6302, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.549034595489502\n",
      "Step: 6303, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.192131757736206\n",
      "Step: 6304, Loss: 0.9216215014457703, Accuracy: 1.0, Computation time: 1.274385929107666\n",
      "Step: 6305, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.1521751880645752\n",
      "Step: 6306, Loss: 0.934417188167572, Accuracy: 0.96875, Computation time: 1.3822128772735596\n",
      "Step: 6307, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 1.4765794277191162\n",
      "Step: 6308, Loss: 0.9382070899009705, Accuracy: 0.96875, Computation time: 1.108365774154663\n",
      "Step: 6309, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.1737864017486572\n",
      "Step: 6310, Loss: 0.9374874234199524, Accuracy: 0.96875, Computation time: 0.903059720993042\n",
      "Step: 6311, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 1.157353401184082\n",
      "Step: 6312, Loss: 0.91602623462677, Accuracy: 1.0, Computation time: 1.5586040019989014\n",
      "Step: 6313, Loss: 0.9159819483757019, Accuracy: 1.0, Computation time: 1.2975118160247803\n",
      "Step: 6314, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.1185667514801025\n",
      "Step: 6315, Loss: 0.9414917230606079, Accuracy: 0.96875, Computation time: 1.6103553771972656\n",
      "Step: 6316, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.052008867263794\n",
      "Step: 6317, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 1.0872414112091064\n",
      "Step: 6318, Loss: 0.9352383017539978, Accuracy: 0.96875, Computation time: 1.3170535564422607\n",
      "Step: 6319, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.2237746715545654\n",
      "Step: 6320, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 0.8993864059448242\n",
      "Step: 6321, Loss: 0.9189462661743164, Accuracy: 1.0, Computation time: 1.4030706882476807\n",
      "Step: 6322, Loss: 0.9161840677261353, Accuracy: 1.0, Computation time: 1.0992612838745117\n",
      "Step: 6323, Loss: 0.9161324501037598, Accuracy: 1.0, Computation time: 1.4777956008911133\n",
      "Step: 6324, Loss: 0.9379392266273499, Accuracy: 0.96875, Computation time: 1.4758713245391846\n",
      "Step: 6325, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 1.0646214485168457\n",
      "Step: 6326, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.2340760231018066\n",
      "Step: 6327, Loss: 0.915945291519165, Accuracy: 1.0, Computation time: 1.2502524852752686\n",
      "Step: 6328, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.8441100120544434\n",
      "Step: 6329, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.1885132789611816\n",
      "Step: 6330, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.4070496559143066\n",
      "Step: 6331, Loss: 0.9376336336135864, Accuracy: 0.96875, Computation time: 1.7543561458587646\n",
      "Step: 6332, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 0.9524846076965332\n",
      "Step: 6333, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 0.8431198596954346\n",
      "Step: 6334, Loss: 0.9377373456954956, Accuracy: 0.96875, Computation time: 1.3040258884429932\n",
      "Step: 6335, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 0.9207925796508789\n",
      "Step: 6336, Loss: 0.9163132309913635, Accuracy: 1.0, Computation time: 1.8475370407104492\n",
      "Step: 6337, Loss: 0.9159464836120605, Accuracy: 1.0, Computation time: 0.9851205348968506\n",
      "Step: 6338, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.0879807472229004\n",
      "Step: 6339, Loss: 0.9159760475158691, Accuracy: 1.0, Computation time: 1.0990703105926514\n",
      "Step: 6340, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.144432783126831\n",
      "Step: 6341, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.1526625156402588\n",
      "Step: 6342, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 0.9301242828369141\n",
      "Step: 6343, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 0.9359095096588135\n",
      "Step: 6344, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 1.0362792015075684\n",
      "Step: 6345, Loss: 0.9377186894416809, Accuracy: 0.96875, Computation time: 2.114718437194824\n",
      "Step: 6346, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.8058414459228516\n",
      "Step: 6347, Loss: 0.9159131646156311, Accuracy: 1.0, Computation time: 1.3502871990203857\n",
      "Step: 6348, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.0319874286651611\n",
      "Step: 6349, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 0.84891676902771\n",
      "Step: 6350, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 0.8684067726135254\n",
      "Step: 6351, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 0.8982341289520264\n",
      "Step: 6352, Loss: 0.9374424815177917, Accuracy: 0.96875, Computation time: 1.3681364059448242\n",
      "Step: 6353, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.1353468894958496\n",
      "Step: 6354, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 0.9059634208679199\n",
      "Step: 6355, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.0611681938171387\n",
      "Step: 6356, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 0.9104197025299072\n",
      "Step: 6357, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 0.9950942993164062\n",
      "Step: 6358, Loss: 0.9158470034599304, Accuracy: 1.0, Computation time: 0.8970005512237549\n",
      "Step: 6359, Loss: 0.9375879764556885, Accuracy: 0.96875, Computation time: 1.0118157863616943\n",
      "Step: 6360, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 0.9547045230865479\n",
      "Step: 6361, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.0798661708831787\n",
      "Step: 6362, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.4861295223236084\n",
      "Step: 6363, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.3158538341522217\n",
      "Step: 6364, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.197221279144287\n",
      "Step: 6365, Loss: 0.9229450821876526, Accuracy: 1.0, Computation time: 0.906137228012085\n",
      "Step: 6366, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.1974964141845703\n",
      "Step: 6367, Loss: 0.9158942699432373, Accuracy: 1.0, Computation time: 1.1819627285003662\n",
      "Step: 6368, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.158233880996704\n",
      "Step: 6369, Loss: 0.9159893989562988, Accuracy: 1.0, Computation time: 0.8888320922851562\n",
      "Step: 6370, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 1.0817079544067383\n",
      "Step: 6371, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 1.2687344551086426\n",
      "Step: 6372, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.3050179481506348\n",
      "Step: 6373, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 0.8843421936035156\n",
      "Step: 6374, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.2424180507659912\n",
      "Step: 6375, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.2365150451660156\n",
      "Step: 6376, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.0266213417053223\n",
      "Step: 6377, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.4556334018707275\n",
      "Step: 6378, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.184473991394043\n",
      "Step: 6379, Loss: 0.9235981702804565, Accuracy: 1.0, Computation time: 1.6106460094451904\n",
      "Step: 6380, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.0057015419006348\n",
      "Step: 6381, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.3317742347717285\n",
      "Step: 6382, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.3350634574890137\n",
      "Step: 6383, Loss: 0.9159666299819946, Accuracy: 1.0, Computation time: 1.2573413848876953\n",
      "Step: 6384, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.3143341541290283\n",
      "Step: 6385, Loss: 0.9160546064376831, Accuracy: 1.0, Computation time: 1.3408560752868652\n",
      "Step: 6386, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.3641769886016846\n",
      "Step: 6387, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.0594029426574707\n",
      "Step: 6388, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.1364734172821045\n",
      "Step: 6389, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.1266365051269531\n",
      "Step: 6390, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.5971989631652832\n",
      "Step: 6391, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 1.1574199199676514\n",
      "Step: 6392, Loss: 0.9172149896621704, Accuracy: 1.0, Computation time: 1.5057828426361084\n",
      "########################\n",
      "Test loss: 1.0701168775558472, Test Accuracy_epoch46: 0.7788555026054382\n",
      "########################\n",
      "Step: 6393, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.5073792934417725\n",
      "Step: 6394, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.3287794589996338\n",
      "Step: 6395, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.5500521659851074\n",
      "Step: 6396, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 0.9546265602111816\n",
      "Step: 6397, Loss: 0.9160661697387695, Accuracy: 1.0, Computation time: 1.0985798835754395\n",
      "Step: 6398, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 1.3975334167480469\n",
      "Step: 6399, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.2634658813476562\n",
      "Step: 6400, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.1479606628417969\n",
      "Step: 6401, Loss: 0.9191620945930481, Accuracy: 1.0, Computation time: 1.2360973358154297\n",
      "Step: 6402, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.121453046798706\n",
      "Step: 6403, Loss: 0.9376803040504456, Accuracy: 0.96875, Computation time: 1.648345947265625\n",
      "Step: 6404, Loss: 0.9159719944000244, Accuracy: 1.0, Computation time: 1.4817371368408203\n",
      "Step: 6405, Loss: 0.9356800317764282, Accuracy: 0.96875, Computation time: 1.6701598167419434\n",
      "Step: 6406, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.3062200546264648\n",
      "Step: 6407, Loss: 0.9159258008003235, Accuracy: 1.0, Computation time: 1.4367015361785889\n",
      "Step: 6408, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.3361823558807373\n",
      "Step: 6409, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.3957455158233643\n",
      "Step: 6410, Loss: 0.9159666895866394, Accuracy: 1.0, Computation time: 1.5015013217926025\n",
      "Step: 6411, Loss: 0.9375124573707581, Accuracy: 0.96875, Computation time: 1.6275808811187744\n",
      "Step: 6412, Loss: 0.9376608729362488, Accuracy: 0.96875, Computation time: 1.3730874061584473\n",
      "Step: 6413, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.657052993774414\n",
      "Step: 6414, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.1646339893341064\n",
      "Step: 6415, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.1995856761932373\n",
      "Step: 6416, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 1.3296475410461426\n",
      "Step: 6417, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.9681215286254883\n",
      "Step: 6418, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 1.3861770629882812\n",
      "Step: 6419, Loss: 0.9358547329902649, Accuracy: 0.96875, Computation time: 2.022677421569824\n",
      "Step: 6420, Loss: 0.9198920726776123, Accuracy: 1.0, Computation time: 1.770308256149292\n",
      "Step: 6421, Loss: 0.916343629360199, Accuracy: 1.0, Computation time: 1.2040352821350098\n",
      "Step: 6422, Loss: 0.9375730156898499, Accuracy: 0.96875, Computation time: 1.3674650192260742\n",
      "Step: 6423, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.3184473514556885\n",
      "Step: 6424, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.2817363739013672\n",
      "Step: 6425, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.0544381141662598\n",
      "Step: 6426, Loss: 0.9376785755157471, Accuracy: 0.96875, Computation time: 1.1841614246368408\n",
      "Step: 6427, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 1.2559781074523926\n",
      "Step: 6428, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 1.2021639347076416\n",
      "Step: 6429, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.3993277549743652\n",
      "Step: 6430, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.0310924053192139\n",
      "Step: 6431, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.2853357791900635\n",
      "Step: 6432, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.1799159049987793\n",
      "Step: 6433, Loss: 0.9160295724868774, Accuracy: 1.0, Computation time: 1.6306939125061035\n",
      "Step: 6434, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.2745916843414307\n",
      "Step: 6435, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.1839168071746826\n",
      "Step: 6436, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 1.2128362655639648\n",
      "Step: 6437, Loss: 0.9159451723098755, Accuracy: 1.0, Computation time: 1.3199727535247803\n",
      "Step: 6438, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 1.2867865562438965\n",
      "Step: 6439, Loss: 0.9161038398742676, Accuracy: 1.0, Computation time: 1.361753225326538\n",
      "Step: 6440, Loss: 0.9158393740653992, Accuracy: 1.0, Computation time: 1.0953094959259033\n",
      "Step: 6441, Loss: 0.9158401489257812, Accuracy: 1.0, Computation time: 0.9639945030212402\n",
      "Step: 6442, Loss: 0.9374340772628784, Accuracy: 0.96875, Computation time: 1.7397196292877197\n",
      "Step: 6443, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 1.3537464141845703\n",
      "Step: 6444, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.1413943767547607\n",
      "Step: 6445, Loss: 0.9376512169837952, Accuracy: 0.96875, Computation time: 1.0406489372253418\n",
      "Step: 6446, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.1440372467041016\n",
      "Step: 6447, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.0555973052978516\n",
      "Step: 6448, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.0564992427825928\n",
      "Step: 6449, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.1337416172027588\n",
      "Step: 6450, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 1.1102283000946045\n",
      "Step: 6451, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 0.846996545791626\n",
      "Step: 6452, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 0.9849722385406494\n",
      "Step: 6453, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.2651512622833252\n",
      "Step: 6454, Loss: 0.937504768371582, Accuracy: 0.96875, Computation time: 1.106572151184082\n",
      "Step: 6455, Loss: 0.9370588660240173, Accuracy: 0.96875, Computation time: 1.551220178604126\n",
      "Step: 6456, Loss: 0.9158428907394409, Accuracy: 1.0, Computation time: 0.9952108860015869\n",
      "Step: 6457, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.1901636123657227\n",
      "Step: 6458, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 1.0213980674743652\n",
      "Step: 6459, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 1.0680360794067383\n",
      "Step: 6460, Loss: 0.9158385396003723, Accuracy: 1.0, Computation time: 1.1245205402374268\n",
      "Step: 6461, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 0.9676141738891602\n",
      "Step: 6462, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.1053466796875\n",
      "Step: 6463, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.1870458126068115\n",
      "Step: 6464, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.1002635955810547\n",
      "Step: 6465, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 1.1155211925506592\n",
      "Step: 6466, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.090735673904419\n",
      "Step: 6467, Loss: 0.9375035166740417, Accuracy: 0.96875, Computation time: 1.1610133647918701\n",
      "Step: 6468, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 0.952704668045044\n",
      "Step: 6469, Loss: 0.9158397912979126, Accuracy: 1.0, Computation time: 1.1241157054901123\n",
      "Step: 6470, Loss: 0.9158406257629395, Accuracy: 1.0, Computation time: 0.966597318649292\n",
      "Step: 6471, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 1.2276837825775146\n",
      "Step: 6472, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.6283819675445557\n",
      "Step: 6473, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.009270191192627\n",
      "Step: 6474, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.284771203994751\n",
      "Step: 6475, Loss: 0.9158385396003723, Accuracy: 1.0, Computation time: 1.0613303184509277\n",
      "Step: 6476, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 0.9386482238769531\n",
      "Step: 6477, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 1.003662347793579\n",
      "Step: 6478, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 0.980675458908081\n",
      "Step: 6479, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.0434596538543701\n",
      "Step: 6480, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.4910309314727783\n",
      "Step: 6481, Loss: 0.9158337712287903, Accuracy: 1.0, Computation time: 1.2495763301849365\n",
      "Step: 6482, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 1.126185655593872\n",
      "Step: 6483, Loss: 0.9158329963684082, Accuracy: 1.0, Computation time: 1.5187489986419678\n",
      "Step: 6484, Loss: 0.9158334136009216, Accuracy: 1.0, Computation time: 1.0419466495513916\n",
      "Step: 6485, Loss: 0.9158340692520142, Accuracy: 1.0, Computation time: 1.0233092308044434\n",
      "Step: 6486, Loss: 0.9158355593681335, Accuracy: 1.0, Computation time: 1.3676891326904297\n",
      "Step: 6487, Loss: 0.9158355593681335, Accuracy: 1.0, Computation time: 1.0912537574768066\n",
      "Step: 6488, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.0792245864868164\n",
      "Step: 6489, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 1.039999008178711\n",
      "Step: 6490, Loss: 0.9160347580909729, Accuracy: 1.0, Computation time: 1.346642255783081\n",
      "Step: 6491, Loss: 0.915835440158844, Accuracy: 1.0, Computation time: 0.9877362251281738\n",
      "Step: 6492, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.0213263034820557\n",
      "Step: 6493, Loss: 0.9160937070846558, Accuracy: 1.0, Computation time: 1.944014310836792\n",
      "Step: 6494, Loss: 0.9158317446708679, Accuracy: 1.0, Computation time: 1.4799754619598389\n",
      "Step: 6495, Loss: 0.9158343076705933, Accuracy: 1.0, Computation time: 0.9880342483520508\n",
      "Step: 6496, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.0857808589935303\n",
      "Step: 6497, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.1575329303741455\n",
      "Step: 6498, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.0754690170288086\n",
      "Step: 6499, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.010305404663086\n",
      "Step: 6500, Loss: 0.9173866510391235, Accuracy: 1.0, Computation time: 1.0204846858978271\n",
      "Step: 6501, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 0.9615952968597412\n",
      "Step: 6502, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.1573424339294434\n",
      "Step: 6503, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 1.2423760890960693\n",
      "Step: 6504, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 0.9939661026000977\n",
      "Step: 6505, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 1.066187858581543\n",
      "Step: 6506, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.0475826263427734\n",
      "Step: 6507, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.2502880096435547\n",
      "Step: 6508, Loss: 0.9158430695533752, Accuracy: 1.0, Computation time: 1.2367312908172607\n",
      "Step: 6509, Loss: 0.9158335328102112, Accuracy: 1.0, Computation time: 0.877753734588623\n",
      "Step: 6510, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.0047574043273926\n",
      "Step: 6511, Loss: 0.9374680519104004, Accuracy: 0.96875, Computation time: 1.0321578979492188\n",
      "Step: 6512, Loss: 0.9158381819725037, Accuracy: 1.0, Computation time: 0.95039963722229\n",
      "Step: 6513, Loss: 0.9159594774246216, Accuracy: 1.0, Computation time: 1.1610865592956543\n",
      "Step: 6514, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.0660905838012695\n",
      "Step: 6515, Loss: 0.9158371090888977, Accuracy: 1.0, Computation time: 0.9281697273254395\n",
      "Step: 6516, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.3780977725982666\n",
      "Step: 6517, Loss: 0.9374791383743286, Accuracy: 0.96875, Computation time: 1.1891069412231445\n",
      "Step: 6518, Loss: 0.9308025240898132, Accuracy: 0.96875, Computation time: 1.23966383934021\n",
      "Step: 6519, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.1400866508483887\n",
      "Step: 6520, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 1.0314264297485352\n",
      "Step: 6521, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.3696014881134033\n",
      "Step: 6522, Loss: 0.9376234412193298, Accuracy: 0.96875, Computation time: 0.9348592758178711\n",
      "Step: 6523, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 1.173375129699707\n",
      "Step: 6524, Loss: 0.9159879088401794, Accuracy: 1.0, Computation time: 1.1300179958343506\n",
      "Step: 6525, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.1740126609802246\n",
      "Step: 6526, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.4545009136199951\n",
      "Step: 6527, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 0.9989340305328369\n",
      "Step: 6528, Loss: 0.9159491658210754, Accuracy: 1.0, Computation time: 0.9287347793579102\n",
      "Step: 6529, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.315337896347046\n",
      "Step: 6530, Loss: 0.937610387802124, Accuracy: 0.96875, Computation time: 1.0972497463226318\n",
      "Step: 6531, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 0.93717360496521\n",
      "########################\n",
      "Test loss: 1.0746374130249023, Test Accuracy_epoch47: 0.7662463784217834\n",
      "########################\n",
      "Step: 6532, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.0646934509277344\n",
      "Step: 6533, Loss: 0.9375758171081543, Accuracy: 0.96875, Computation time: 1.1850357055664062\n",
      "Step: 6534, Loss: 0.917510986328125, Accuracy: 1.0, Computation time: 1.1901953220367432\n",
      "Step: 6535, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 1.2417073249816895\n",
      "Step: 6536, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.7193584442138672\n",
      "Step: 6537, Loss: 0.9166181087493896, Accuracy: 1.0, Computation time: 1.566575527191162\n",
      "Step: 6538, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 0.9295570850372314\n",
      "Step: 6539, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 1.1004023551940918\n",
      "Step: 6540, Loss: 0.9162566065788269, Accuracy: 1.0, Computation time: 1.5406827926635742\n",
      "Step: 6541, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 0.9807119369506836\n",
      "Step: 6542, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 0.9867205619812012\n",
      "Step: 6543, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 0.9140605926513672\n",
      "Step: 6544, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.082834243774414\n",
      "Step: 6545, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 0.9644930362701416\n",
      "Step: 6546, Loss: 0.9173421859741211, Accuracy: 1.0, Computation time: 1.0323264598846436\n",
      "Step: 6547, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 1.3713808059692383\n",
      "Step: 6548, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 0.9685096740722656\n",
      "Step: 6549, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.2272937297821045\n",
      "Step: 6550, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.1358699798583984\n",
      "Step: 6551, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.0491893291473389\n",
      "Step: 6552, Loss: 0.9165547490119934, Accuracy: 1.0, Computation time: 1.176997423171997\n",
      "Step: 6553, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.216651439666748\n",
      "Step: 6554, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 0.9349303245544434\n",
      "Step: 6555, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 0.9901530742645264\n",
      "Step: 6556, Loss: 0.9158344268798828, Accuracy: 1.0, Computation time: 0.9831204414367676\n",
      "Step: 6557, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.0209498405456543\n",
      "Step: 6558, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.2004592418670654\n",
      "Step: 6559, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 1.1135022640228271\n",
      "Step: 6560, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.8898735046386719\n",
      "Step: 6561, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.0852971076965332\n",
      "Step: 6562, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 0.9177653789520264\n",
      "Step: 6563, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 0.8764894008636475\n",
      "Step: 6564, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.0428693294525146\n",
      "Step: 6565, Loss: 0.937446117401123, Accuracy: 0.96875, Computation time: 1.2864232063293457\n",
      "Step: 6566, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 1.0638668537139893\n",
      "Step: 6567, Loss: 0.9163876175880432, Accuracy: 1.0, Computation time: 1.339432954788208\n",
      "Step: 6568, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.135105848312378\n",
      "Step: 6569, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.2093253135681152\n",
      "Step: 6570, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.1693403720855713\n",
      "Step: 6571, Loss: 0.9377742409706116, Accuracy: 0.96875, Computation time: 1.073951005935669\n",
      "Step: 6572, Loss: 0.9375944137573242, Accuracy: 0.96875, Computation time: 1.030909538269043\n",
      "Step: 6573, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.0158655643463135\n",
      "Step: 6574, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.209662914276123\n",
      "Step: 6575, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.257955551147461\n",
      "Step: 6576, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.2663688659667969\n",
      "Step: 6577, Loss: 0.9374875426292419, Accuracy: 0.96875, Computation time: 1.308582067489624\n",
      "Step: 6578, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.7185869216918945\n",
      "Step: 6579, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.0432355403900146\n",
      "Step: 6580, Loss: 0.9174988865852356, Accuracy: 1.0, Computation time: 1.4279131889343262\n",
      "Step: 6581, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.7053098678588867\n",
      "Step: 6582, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.0589158535003662\n",
      "Step: 6583, Loss: 0.9376569986343384, Accuracy: 0.96875, Computation time: 0.9369411468505859\n",
      "Step: 6584, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 0.9638185501098633\n",
      "Step: 6585, Loss: 0.9159612059593201, Accuracy: 1.0, Computation time: 0.9982235431671143\n",
      "Step: 6586, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 0.8211979866027832\n",
      "Step: 6587, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.211927890777588\n",
      "Step: 6588, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 0.9830718040466309\n",
      "Step: 6589, Loss: 0.9375835061073303, Accuracy: 0.96875, Computation time: 1.1693880558013916\n",
      "Step: 6590, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.1081862449645996\n",
      "Step: 6591, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.5445818901062012\n",
      "Step: 6592, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.3546934127807617\n",
      "Step: 6593, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 1.3391120433807373\n",
      "Step: 6594, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 0.9470834732055664\n",
      "Step: 6595, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.3926026821136475\n",
      "Step: 6596, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.2446129322052002\n",
      "Step: 6597, Loss: 0.9572619795799255, Accuracy: 0.9375, Computation time: 1.084092617034912\n",
      "Step: 6598, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 1.015334129333496\n",
      "Step: 6599, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.0660290718078613\n",
      "Step: 6600, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 0.8231010437011719\n",
      "Step: 6601, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.16469144821167\n",
      "Step: 6602, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.0935451984405518\n",
      "Step: 6603, Loss: 0.9159082770347595, Accuracy: 1.0, Computation time: 1.0402984619140625\n",
      "Step: 6604, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.0625693798065186\n",
      "Step: 6605, Loss: 0.91652512550354, Accuracy: 1.0, Computation time: 1.415590763092041\n",
      "Step: 6606, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.3447391986846924\n",
      "Step: 6607, Loss: 0.9158458113670349, Accuracy: 1.0, Computation time: 1.3310346603393555\n",
      "Step: 6608, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.327965497970581\n",
      "Step: 6609, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 0.9199049472808838\n",
      "Step: 6610, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.0852479934692383\n",
      "Step: 6611, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.4114634990692139\n",
      "Step: 6612, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.230731725692749\n",
      "Step: 6613, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.0754921436309814\n",
      "Step: 6614, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 1.2038302421569824\n",
      "Step: 6615, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 1.3370044231414795\n",
      "Step: 6616, Loss: 0.9158308506011963, Accuracy: 1.0, Computation time: 0.7216300964355469\n",
      "Step: 6617, Loss: 0.9158317446708679, Accuracy: 1.0, Computation time: 1.3228275775909424\n",
      "Step: 6618, Loss: 0.9158369898796082, Accuracy: 1.0, Computation time: 1.382993221282959\n",
      "Step: 6619, Loss: 0.9158458113670349, Accuracy: 1.0, Computation time: 1.6198198795318604\n",
      "Step: 6620, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.0215768814086914\n",
      "Step: 6621, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.1302053928375244\n",
      "Step: 6622, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.0373601913452148\n",
      "Step: 6623, Loss: 0.9158359169960022, Accuracy: 1.0, Computation time: 0.9230554103851318\n",
      "Step: 6624, Loss: 0.9158354997634888, Accuracy: 1.0, Computation time: 1.0416619777679443\n",
      "Step: 6625, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.0562303066253662\n",
      "Step: 6626, Loss: 0.9158368110656738, Accuracy: 1.0, Computation time: 1.1605315208435059\n",
      "Step: 6627, Loss: 0.9368512034416199, Accuracy: 0.96875, Computation time: 1.4926352500915527\n",
      "Step: 6628, Loss: 0.9158319234848022, Accuracy: 1.0, Computation time: 1.0714433193206787\n",
      "Step: 6629, Loss: 0.9158319234848022, Accuracy: 1.0, Computation time: 0.9841206073760986\n",
      "Step: 6630, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.0887165069580078\n",
      "Step: 6631, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.3015739917755127\n",
      "Step: 6632, Loss: 0.9158377051353455, Accuracy: 1.0, Computation time: 1.0948269367218018\n",
      "Step: 6633, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 1.164386510848999\n",
      "Step: 6634, Loss: 0.9158428907394409, Accuracy: 1.0, Computation time: 1.2178959846496582\n",
      "Step: 6635, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.2279279232025146\n",
      "Step: 6636, Loss: 0.9160584211349487, Accuracy: 1.0, Computation time: 1.3480439186096191\n",
      "Step: 6637, Loss: 0.9374775886535645, Accuracy: 0.96875, Computation time: 0.925079345703125\n",
      "Step: 6638, Loss: 0.9158415794372559, Accuracy: 1.0, Computation time: 1.0863573551177979\n",
      "Step: 6639, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.4636712074279785\n",
      "Step: 6640, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.1807968616485596\n",
      "Step: 6641, Loss: 0.9158373475074768, Accuracy: 1.0, Computation time: 1.3360154628753662\n",
      "Step: 6642, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.0244297981262207\n",
      "Step: 6643, Loss: 0.9158331751823425, Accuracy: 1.0, Computation time: 1.096125841140747\n",
      "Step: 6644, Loss: 0.9158341884613037, Accuracy: 1.0, Computation time: 1.2389237880706787\n",
      "Step: 6645, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.1514267921447754\n",
      "Step: 6646, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 0.9185903072357178\n",
      "Step: 6647, Loss: 0.9160243272781372, Accuracy: 1.0, Computation time: 1.107255220413208\n",
      "Step: 6648, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 1.2303988933563232\n",
      "Step: 6649, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.1009366512298584\n",
      "Step: 6650, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.188422441482544\n",
      "Step: 6651, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 1.1270151138305664\n",
      "Step: 6652, Loss: 0.9158370494842529, Accuracy: 1.0, Computation time: 1.3840558528900146\n",
      "Step: 6653, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.3686296939849854\n",
      "Step: 6654, Loss: 0.9161686897277832, Accuracy: 1.0, Computation time: 1.4163596630096436\n",
      "Step: 6655, Loss: 0.9375247359275818, Accuracy: 0.96875, Computation time: 0.9406342506408691\n",
      "Step: 6656, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.7153761386871338\n",
      "Step: 6657, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 0.927330732345581\n",
      "Step: 6658, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 1.4156756401062012\n",
      "Step: 6659, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.0003314018249512\n",
      "Step: 6660, Loss: 0.9374717473983765, Accuracy: 0.96875, Computation time: 1.2566888332366943\n",
      "Step: 6661, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.4230077266693115\n",
      "Step: 6662, Loss: 0.9374678134918213, Accuracy: 0.96875, Computation time: 0.9662318229675293\n",
      "Step: 6663, Loss: 0.9375612735748291, Accuracy: 0.96875, Computation time: 1.0794377326965332\n",
      "Step: 6664, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.171787977218628\n",
      "Step: 6665, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.0206856727600098\n",
      "Step: 6666, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.1670286655426025\n",
      "Step: 6667, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.0183651447296143\n",
      "Step: 6668, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.1662001609802246\n",
      "Step: 6669, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 0.9399163722991943\n",
      "########################\n",
      "Test loss: 1.074061632156372, Test Accuracy_epoch48: 0.7720659971237183\n",
      "########################\n",
      "Step: 6670, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.307215929031372\n",
      "Step: 6671, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 0.9527842998504639\n",
      "Step: 6672, Loss: 0.9197590351104736, Accuracy: 1.0, Computation time: 1.2312960624694824\n",
      "Step: 6673, Loss: 0.9374955892562866, Accuracy: 0.96875, Computation time: 1.3278388977050781\n",
      "Step: 6674, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.2380921840667725\n",
      "Step: 6675, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.144381046295166\n",
      "Step: 6676, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.0764451026916504\n",
      "Step: 6677, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 0.887296199798584\n",
      "Step: 6678, Loss: 0.9376732110977173, Accuracy: 0.96875, Computation time: 0.961442232131958\n",
      "Step: 6679, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.0636610984802246\n",
      "Step: 6680, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.4313735961914062\n",
      "Step: 6681, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.0614516735076904\n",
      "Step: 6682, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 1.0595324039459229\n",
      "Step: 6683, Loss: 0.9375684857368469, Accuracy: 0.96875, Computation time: 1.2842249870300293\n",
      "Step: 6684, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 1.0019276142120361\n",
      "Step: 6685, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.2636094093322754\n",
      "Step: 6686, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.0631599426269531\n",
      "Step: 6687, Loss: 0.9158521294593811, Accuracy: 1.0, Computation time: 1.2528975009918213\n",
      "Step: 6688, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.0909159183502197\n",
      "Step: 6689, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.3435709476470947\n",
      "Step: 6690, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.1596546173095703\n",
      "Step: 6691, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 0.9446446895599365\n",
      "Step: 6692, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 1.3369767665863037\n",
      "Step: 6693, Loss: 0.916469395160675, Accuracy: 1.0, Computation time: 1.6753184795379639\n",
      "Step: 6694, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.3801965713500977\n",
      "Step: 6695, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 0.9776721000671387\n",
      "Step: 6696, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 0.9683563709259033\n",
      "Step: 6697, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 0.9813792705535889\n",
      "Step: 6698, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.0988283157348633\n",
      "Step: 6699, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.017728567123413\n",
      "Step: 6700, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.1580657958984375\n",
      "Step: 6701, Loss: 0.9158434271812439, Accuracy: 1.0, Computation time: 1.2908639907836914\n",
      "Step: 6702, Loss: 0.916964590549469, Accuracy: 1.0, Computation time: 0.945530891418457\n",
      "Step: 6703, Loss: 0.9158441424369812, Accuracy: 1.0, Computation time: 1.0739209651947021\n",
      "Step: 6704, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 0.9675109386444092\n",
      "Step: 6705, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.138871669769287\n",
      "Step: 6706, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 0.9283714294433594\n",
      "Step: 6707, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.6211462020874023\n",
      "Step: 6708, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.248331069946289\n",
      "Step: 6709, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.9748177528381348\n",
      "Step: 6710, Loss: 0.9159324765205383, Accuracy: 1.0, Computation time: 1.2710578441619873\n",
      "Step: 6711, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 1.564310073852539\n",
      "Step: 6712, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.1436407566070557\n",
      "Step: 6713, Loss: 0.9158372282981873, Accuracy: 1.0, Computation time: 1.197148084640503\n",
      "Step: 6714, Loss: 0.9374651908874512, Accuracy: 0.96875, Computation time: 1.5258116722106934\n",
      "Step: 6715, Loss: 0.9376511573791504, Accuracy: 0.96875, Computation time: 1.1278271675109863\n",
      "Step: 6716, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.6991119384765625\n",
      "Step: 6717, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 0.8830184936523438\n",
      "Step: 6718, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.4396929740905762\n",
      "Step: 6719, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 1.1355531215667725\n",
      "Step: 6720, Loss: 0.9158370494842529, Accuracy: 1.0, Computation time: 1.1860802173614502\n",
      "Step: 6721, Loss: 0.9158310294151306, Accuracy: 1.0, Computation time: 1.0430819988250732\n",
      "Step: 6722, Loss: 0.9158315062522888, Accuracy: 1.0, Computation time: 1.4304180145263672\n",
      "Step: 6723, Loss: 0.9355063438415527, Accuracy: 0.96875, Computation time: 1.776134967803955\n",
      "Step: 6724, Loss: 0.9374992251396179, Accuracy: 0.96875, Computation time: 1.5253686904907227\n",
      "Step: 6725, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.139937162399292\n",
      "Step: 6726, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.0727810859680176\n",
      "Step: 6727, Loss: 0.9160234928131104, Accuracy: 1.0, Computation time: 1.3135135173797607\n",
      "Step: 6728, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 0.8599534034729004\n",
      "Step: 6729, Loss: 0.9168866276741028, Accuracy: 1.0, Computation time: 1.0335743427276611\n",
      "Step: 6730, Loss: 0.9159368276596069, Accuracy: 1.0, Computation time: 1.162459373474121\n",
      "Step: 6731, Loss: 0.9159995317459106, Accuracy: 1.0, Computation time: 1.6716551780700684\n",
      "Step: 6732, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.3063817024230957\n",
      "Step: 6733, Loss: 0.9159921407699585, Accuracy: 1.0, Computation time: 1.3518319129943848\n",
      "Step: 6734, Loss: 0.9158894419670105, Accuracy: 1.0, Computation time: 1.2561957836151123\n",
      "Step: 6735, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.1371159553527832\n",
      "Step: 6736, Loss: 0.9158388376235962, Accuracy: 1.0, Computation time: 1.0217058658599854\n",
      "Step: 6737, Loss: 0.9373647570610046, Accuracy: 0.96875, Computation time: 1.2141773700714111\n",
      "Step: 6738, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.4575562477111816\n",
      "Step: 6739, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.3158626556396484\n",
      "Step: 6740, Loss: 0.9163571000099182, Accuracy: 1.0, Computation time: 1.5410001277923584\n",
      "Step: 6741, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.499279260635376\n",
      "Step: 6742, Loss: 0.9375793933868408, Accuracy: 0.96875, Computation time: 1.116018295288086\n",
      "Step: 6743, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.186272144317627\n",
      "Step: 6744, Loss: 0.9158393740653992, Accuracy: 1.0, Computation time: 1.2354602813720703\n",
      "Step: 6745, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.3692598342895508\n",
      "Step: 6746, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.3622817993164062\n",
      "Step: 6747, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.2560739517211914\n",
      "Step: 6748, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.4395215511322021\n",
      "Step: 6749, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 1.5072617530822754\n",
      "Step: 6750, Loss: 0.9158324003219604, Accuracy: 1.0, Computation time: 1.2382757663726807\n",
      "Step: 6751, Loss: 0.9158316254615784, Accuracy: 1.0, Computation time: 1.218796730041504\n",
      "Step: 6752, Loss: 0.9158306121826172, Accuracy: 1.0, Computation time: 1.4036540985107422\n",
      "Step: 6753, Loss: 0.9158379435539246, Accuracy: 1.0, Computation time: 1.499335765838623\n",
      "Step: 6754, Loss: 0.9158350229263306, Accuracy: 1.0, Computation time: 1.2156574726104736\n",
      "Step: 6755, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 1.0774013996124268\n",
      "Step: 6756, Loss: 0.9374388456344604, Accuracy: 0.96875, Computation time: 1.0254428386688232\n",
      "Step: 6757, Loss: 0.9158353805541992, Accuracy: 1.0, Computation time: 1.1370160579681396\n",
      "Step: 6758, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 1.4606373310089111\n",
      "Step: 6759, Loss: 0.9158350825309753, Accuracy: 1.0, Computation time: 1.3274650573730469\n",
      "Step: 6760, Loss: 0.915835976600647, Accuracy: 1.0, Computation time: 1.4069433212280273\n",
      "Step: 6761, Loss: 0.9158372282981873, Accuracy: 1.0, Computation time: 1.6304476261138916\n",
      "Step: 6762, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.2410805225372314\n",
      "Step: 6763, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.230806589126587\n",
      "Step: 6764, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.2003867626190186\n",
      "Step: 6765, Loss: 0.9429211020469666, Accuracy: 0.96875, Computation time: 1.6843338012695312\n",
      "Step: 6766, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.1737971305847168\n",
      "Step: 6767, Loss: 0.9375045895576477, Accuracy: 0.96875, Computation time: 1.0933032035827637\n",
      "Step: 6768, Loss: 0.9375728368759155, Accuracy: 0.96875, Computation time: 1.0080718994140625\n",
      "Step: 6769, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 0.8987007141113281\n",
      "Step: 6770, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.5134267807006836\n",
      "Step: 6771, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.154355764389038\n",
      "Step: 6772, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.0001850128173828\n",
      "Step: 6773, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.6076481342315674\n",
      "Step: 6774, Loss: 0.9158353209495544, Accuracy: 1.0, Computation time: 1.1347146034240723\n",
      "Step: 6775, Loss: 0.915833592414856, Accuracy: 1.0, Computation time: 1.1080572605133057\n",
      "Step: 6776, Loss: 0.9376081824302673, Accuracy: 0.96875, Computation time: 1.1419763565063477\n",
      "Step: 6777, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 0.9733121395111084\n",
      "Step: 6778, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 1.3530724048614502\n",
      "Step: 6779, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 0.982964277267456\n",
      "Step: 6780, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.4875781536102295\n",
      "Step: 6781, Loss: 0.9161289930343628, Accuracy: 1.0, Computation time: 1.7760608196258545\n",
      "Step: 6782, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.3048205375671387\n",
      "Step: 6783, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.2195591926574707\n",
      "Step: 6784, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.1630902290344238\n",
      "Step: 6785, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.2841174602508545\n",
      "Step: 6786, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 1.0246455669403076\n",
      "Step: 6787, Loss: 0.9158361554145813, Accuracy: 1.0, Computation time: 0.984818696975708\n",
      "Step: 6788, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 1.1751043796539307\n",
      "Step: 6789, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 1.254120111465454\n",
      "Step: 6790, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 0.998586893081665\n",
      "Step: 6791, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.187981367111206\n",
      "Step: 6792, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.5757145881652832\n",
      "Step: 6793, Loss: 0.916241466999054, Accuracy: 1.0, Computation time: 1.0177662372589111\n",
      "Step: 6794, Loss: 0.9158351421356201, Accuracy: 1.0, Computation time: 1.5543065071105957\n",
      "Step: 6795, Loss: 0.9158395528793335, Accuracy: 1.0, Computation time: 1.4405555725097656\n",
      "Step: 6796, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 0.9969604015350342\n",
      "Step: 6797, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.2632660865783691\n",
      "Step: 6798, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.2813637256622314\n",
      "Step: 6799, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 1.1455402374267578\n",
      "Step: 6800, Loss: 0.9158340692520142, Accuracy: 1.0, Computation time: 1.0205814838409424\n",
      "Step: 6801, Loss: 0.9158356785774231, Accuracy: 1.0, Computation time: 1.3286752700805664\n",
      "Step: 6802, Loss: 0.9158346652984619, Accuracy: 1.0, Computation time: 1.4356021881103516\n",
      "Step: 6803, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.6206903457641602\n",
      "Step: 6804, Loss: 0.9158377647399902, Accuracy: 1.0, Computation time: 1.0913314819335938\n",
      "Step: 6805, Loss: 0.9158372282981873, Accuracy: 1.0, Computation time: 1.43796706199646\n",
      "Step: 6806, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 0.9587545394897461\n",
      "Step: 6807, Loss: 0.9158344268798828, Accuracy: 1.0, Computation time: 0.9268360137939453\n",
      "Step: 6808, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.6688644886016846\n",
      "########################\n",
      "Test loss: 1.0741791725158691, Test Accuracy_epoch49: 0.7710960507392883\n",
      "########################\n",
      "Step: 6809, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 1.1565814018249512\n",
      "Step: 6810, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.011540174484253\n",
      "Step: 6811, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.0645596981048584\n",
      "Step: 6812, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 1.289255142211914\n",
      "Step: 6813, Loss: 0.9233726263046265, Accuracy: 1.0, Computation time: 1.0846679210662842\n",
      "Step: 6814, Loss: 0.9161367416381836, Accuracy: 1.0, Computation time: 1.4223198890686035\n",
      "Step: 6815, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 0.9951672554016113\n",
      "Step: 6816, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 0.881829023361206\n",
      "Step: 6817, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 0.8628511428833008\n",
      "Step: 6818, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 1.3015813827514648\n",
      "Step: 6819, Loss: 0.9374741911888123, Accuracy: 0.96875, Computation time: 0.9437408447265625\n",
      "Step: 6820, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.1589014530181885\n",
      "Step: 6821, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 1.0491664409637451\n",
      "Step: 6822, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 0.997936487197876\n",
      "Step: 6823, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.5204846858978271\n",
      "Step: 6824, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.1121363639831543\n",
      "Step: 6825, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 0.7908711433410645\n",
      "Step: 6826, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 0.8843655586242676\n",
      "Step: 6827, Loss: 0.915934145450592, Accuracy: 1.0, Computation time: 1.1671485900878906\n",
      "Step: 6828, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 1.0608620643615723\n",
      "Step: 6829, Loss: 0.9375521540641785, Accuracy: 0.96875, Computation time: 1.0125484466552734\n",
      "Step: 6830, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 1.0698680877685547\n",
      "Step: 6831, Loss: 0.9472236633300781, Accuracy: 0.9375, Computation time: 1.0966744422912598\n",
      "Step: 6832, Loss: 0.9159101247787476, Accuracy: 1.0, Computation time: 1.037956953048706\n",
      "Step: 6833, Loss: 0.9159327745437622, Accuracy: 1.0, Computation time: 1.0939552783966064\n",
      "Step: 6834, Loss: 0.915995180606842, Accuracy: 1.0, Computation time: 1.646923303604126\n",
      "Step: 6835, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.161186695098877\n",
      "Step: 6836, Loss: 0.9160273671150208, Accuracy: 1.0, Computation time: 1.059784173965454\n",
      "Step: 6837, Loss: 0.9375908970832825, Accuracy: 0.96875, Computation time: 1.3407764434814453\n",
      "Step: 6838, Loss: 0.9172212481498718, Accuracy: 1.0, Computation time: 1.5731325149536133\n",
      "Step: 6839, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 0.8600082397460938\n",
      "Step: 6840, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.0090923309326172\n",
      "Step: 6841, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.44346022605896\n",
      "Step: 6842, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 0.9849703311920166\n",
      "Step: 6843, Loss: 0.9160279631614685, Accuracy: 1.0, Computation time: 0.8434216976165771\n",
      "Step: 6844, Loss: 0.9159319400787354, Accuracy: 1.0, Computation time: 1.0516951084136963\n",
      "Step: 6845, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.0683884620666504\n",
      "Step: 6846, Loss: 0.9375416040420532, Accuracy: 0.96875, Computation time: 1.0506236553192139\n",
      "Step: 6847, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 0.9706599712371826\n",
      "Step: 6848, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.037015676498413\n",
      "Step: 6849, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 1.268707275390625\n",
      "Step: 6850, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.1224098205566406\n",
      "Step: 6851, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.21293306350708\n",
      "Step: 6852, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.2710785865783691\n",
      "Step: 6853, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.046665906906128\n",
      "Step: 6854, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.0061156749725342\n",
      "Step: 6855, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.2871580123901367\n",
      "Step: 6856, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 1.0435106754302979\n",
      "Step: 6857, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 1.162135124206543\n",
      "Step: 6858, Loss: 0.915834903717041, Accuracy: 1.0, Computation time: 0.8168125152587891\n",
      "Step: 6859, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.0414228439331055\n",
      "Step: 6860, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.0157222747802734\n",
      "Step: 6861, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.1986467838287354\n",
      "Step: 6862, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 0.9680893421173096\n",
      "Step: 6863, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.1105918884277344\n",
      "Step: 6864, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 0.9890985488891602\n",
      "Step: 6865, Loss: 0.9158317446708679, Accuracy: 1.0, Computation time: 1.005570650100708\n",
      "Step: 6866, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.1688776016235352\n",
      "Step: 6867, Loss: 0.9158342480659485, Accuracy: 1.0, Computation time: 1.121232271194458\n",
      "Step: 6868, Loss: 0.9158331751823425, Accuracy: 1.0, Computation time: 0.8549180030822754\n",
      "Step: 6869, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.2445108890533447\n",
      "Step: 6870, Loss: 0.9591173529624939, Accuracy: 0.9375, Computation time: 1.0918188095092773\n",
      "Step: 6871, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 1.0203516483306885\n",
      "Step: 6872, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 0.9558320045471191\n",
      "Step: 6873, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.090315818786621\n",
      "Step: 6874, Loss: 0.9374513030052185, Accuracy: 0.96875, Computation time: 1.0266718864440918\n",
      "Step: 6875, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.2060801982879639\n",
      "Step: 6876, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.167754888534546\n",
      "Step: 6877, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 0.875828742980957\n",
      "Step: 6878, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 1.1076233386993408\n",
      "Step: 6879, Loss: 0.9159612059593201, Accuracy: 1.0, Computation time: 1.4321391582489014\n",
      "Step: 6880, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 0.9848618507385254\n",
      "Step: 6881, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.4913649559020996\n",
      "Step: 6882, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.014085054397583\n",
      "Step: 6883, Loss: 0.9375945925712585, Accuracy: 0.96875, Computation time: 0.9515275955200195\n",
      "Step: 6884, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 1.1880145072937012\n",
      "Step: 6885, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.1960926055908203\n",
      "Step: 6886, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.2403697967529297\n",
      "Step: 6887, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.0581214427947998\n",
      "Step: 6888, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.0764074325561523\n",
      "Step: 6889, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 1.0035080909729004\n",
      "Step: 6890, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.234123706817627\n",
      "Step: 6891, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 0.9441325664520264\n",
      "Step: 6892, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.071178674697876\n",
      "Step: 6893, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 1.0715255737304688\n",
      "Step: 6894, Loss: 0.9375255107879639, Accuracy: 0.96875, Computation time: 1.2058370113372803\n",
      "Step: 6895, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.1850483417510986\n",
      "Step: 6896, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 0.9009318351745605\n",
      "Step: 6897, Loss: 0.9158387780189514, Accuracy: 1.0, Computation time: 1.1805801391601562\n",
      "Step: 6898, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.2366018295288086\n",
      "Step: 6899, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.2715075016021729\n",
      "Step: 6900, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.264643669128418\n",
      "Step: 6901, Loss: 0.9158339500427246, Accuracy: 1.0, Computation time: 1.0193216800689697\n",
      "Step: 6902, Loss: 0.9158322215080261, Accuracy: 1.0, Computation time: 1.3184435367584229\n",
      "Step: 6903, Loss: 0.9158334732055664, Accuracy: 1.0, Computation time: 1.0825951099395752\n",
      "Step: 6904, Loss: 0.9158353805541992, Accuracy: 1.0, Computation time: 1.1088616847991943\n",
      "Step: 6905, Loss: 0.9171918034553528, Accuracy: 1.0, Computation time: 1.4522287845611572\n",
      "Step: 6906, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.1387767791748047\n",
      "Step: 6907, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.1529200077056885\n",
      "Step: 6908, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.195291519165039\n",
      "Step: 6909, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.2312512397766113\n",
      "Step: 6910, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.5168366432189941\n",
      "Step: 6911, Loss: 0.915838360786438, Accuracy: 1.0, Computation time: 1.2945787906646729\n",
      "Step: 6912, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.0648090839385986\n",
      "Step: 6913, Loss: 0.9158334732055664, Accuracy: 1.0, Computation time: 1.079939603805542\n",
      "Step: 6914, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 1.2060844898223877\n",
      "Step: 6915, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 1.421743392944336\n",
      "Step: 6916, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.1437289714813232\n",
      "Step: 6917, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 0.9827337265014648\n",
      "Step: 6918, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.199378252029419\n",
      "Step: 6919, Loss: 0.937545120716095, Accuracy: 0.96875, Computation time: 0.9879858493804932\n",
      "Step: 6920, Loss: 0.9158340096473694, Accuracy: 1.0, Computation time: 0.9596216678619385\n",
      "Step: 6921, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 1.3093204498291016\n",
      "Step: 6922, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.0282788276672363\n",
      "Step: 6923, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.0229618549346924\n",
      "Step: 6924, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.042039394378662\n",
      "Step: 6925, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 1.0702037811279297\n",
      "Step: 6926, Loss: 0.9158323407173157, Accuracy: 1.0, Computation time: 1.039520502090454\n",
      "Step: 6927, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.0222828388214111\n",
      "Step: 6928, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 0.9190077781677246\n",
      "Step: 6929, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 1.1986727714538574\n",
      "Step: 6930, Loss: 0.9158381819725037, Accuracy: 1.0, Computation time: 0.9912712574005127\n",
      "Step: 6931, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.2756226062774658\n",
      "Step: 6932, Loss: 0.9158347249031067, Accuracy: 1.0, Computation time: 1.3789050579071045\n",
      "Step: 6933, Loss: 0.9375279545783997, Accuracy: 0.96875, Computation time: 1.0070319175720215\n",
      "Step: 6934, Loss: 0.9158324599266052, Accuracy: 1.0, Computation time: 1.0381813049316406\n",
      "Step: 6935, Loss: 0.9158340692520142, Accuracy: 1.0, Computation time: 0.966010570526123\n",
      "Step: 6936, Loss: 0.9158350825309753, Accuracy: 1.0, Computation time: 1.0393035411834717\n",
      "Step: 6937, Loss: 0.9324395060539246, Accuracy: 0.96875, Computation time: 1.1054823398590088\n",
      "Step: 6938, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.2552027702331543\n",
      "Step: 6939, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.0383386611938477\n",
      "Step: 6940, Loss: 0.9376206994056702, Accuracy: 0.96875, Computation time: 1.037529706954956\n",
      "Step: 6941, Loss: 0.9593917727470398, Accuracy: 0.9375, Computation time: 1.4385628700256348\n",
      "Step: 6942, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 1.0944480895996094\n",
      "Step: 6943, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 0.9216189384460449\n",
      "Step: 6944, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.1148269176483154\n",
      "Step: 6945, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 0.9759314060211182\n",
      "Step: 6946, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 1.049795389175415\n",
      "Step: 6947, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.234834909439087\n",
      "########################\n",
      "Test loss: 1.0742818117141724, Test Accuracy_epoch50: 0.7681862711906433\n",
      "########################\n",
      "Step: 6948, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.2619271278381348\n",
      "Step: 6949, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.2175326347351074\n",
      "Step: 6950, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.1433486938476562\n",
      "Step: 6951, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.0326521396636963\n",
      "Step: 6952, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.8162040710449219\n",
      "Step: 6953, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.7104856967926025\n",
      "Step: 6954, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 1.2866404056549072\n",
      "Step: 6955, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.0715456008911133\n",
      "Step: 6956, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 0.9905838966369629\n",
      "Step: 6957, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 1.0088844299316406\n",
      "Step: 6958, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.391474962234497\n",
      "Step: 6959, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.8289377689361572\n",
      "Step: 6960, Loss: 0.9158458113670349, Accuracy: 1.0, Computation time: 1.2019140720367432\n",
      "Step: 6961, Loss: 0.9158353209495544, Accuracy: 1.0, Computation time: 1.1570029258728027\n",
      "Step: 6962, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.317213773727417\n",
      "Step: 6963, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 1.114903211593628\n",
      "Step: 6964, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.3253355026245117\n",
      "Step: 6965, Loss: 0.9158395528793335, Accuracy: 1.0, Computation time: 0.9467377662658691\n",
      "Step: 6966, Loss: 0.9161844253540039, Accuracy: 1.0, Computation time: 1.1138677597045898\n",
      "Step: 6967, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.191349983215332\n",
      "Step: 6968, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 0.8601365089416504\n",
      "Step: 6969, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.218292236328125\n",
      "Step: 6970, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 1.341662883758545\n",
      "Step: 6971, Loss: 0.9160420298576355, Accuracy: 1.0, Computation time: 1.150963306427002\n",
      "Step: 6972, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 1.050501823425293\n",
      "Step: 6973, Loss: 0.9158363938331604, Accuracy: 1.0, Computation time: 0.9895937442779541\n",
      "Step: 6974, Loss: 0.9158321022987366, Accuracy: 1.0, Computation time: 0.9402284622192383\n",
      "Step: 6975, Loss: 0.9158324599266052, Accuracy: 1.0, Computation time: 1.0632593631744385\n",
      "Step: 6976, Loss: 0.9160120487213135, Accuracy: 1.0, Computation time: 0.9195961952209473\n",
      "Step: 6977, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 0.9889769554138184\n",
      "Step: 6978, Loss: 0.9158318042755127, Accuracy: 1.0, Computation time: 1.0008113384246826\n",
      "Step: 6979, Loss: 0.9158316254615784, Accuracy: 1.0, Computation time: 1.079930067062378\n",
      "Step: 6980, Loss: 0.9158322215080261, Accuracy: 1.0, Computation time: 0.9716670513153076\n",
      "Step: 6981, Loss: 0.9375815391540527, Accuracy: 0.96875, Computation time: 0.963517427444458\n",
      "Step: 6982, Loss: 0.9375429749488831, Accuracy: 0.96875, Computation time: 1.0905609130859375\n",
      "Step: 6983, Loss: 0.9158387184143066, Accuracy: 1.0, Computation time: 1.0109515190124512\n",
      "Step: 6984, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 0.9360685348510742\n",
      "Step: 6985, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.008415937423706\n",
      "Step: 6986, Loss: 0.9158356785774231, Accuracy: 1.0, Computation time: 1.134939432144165\n",
      "Step: 6987, Loss: 0.9160647988319397, Accuracy: 1.0, Computation time: 1.4121630191802979\n",
      "Step: 6988, Loss: 0.9374955892562866, Accuracy: 0.96875, Computation time: 1.2053430080413818\n",
      "Step: 6989, Loss: 0.9158348441123962, Accuracy: 1.0, Computation time: 1.2410273551940918\n",
      "Step: 6990, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 0.9604911804199219\n",
      "Step: 6991, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.1122379302978516\n",
      "Step: 6992, Loss: 0.9158381819725037, Accuracy: 1.0, Computation time: 1.0472242832183838\n",
      "Step: 6993, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.115356206893921\n",
      "Step: 6994, Loss: 0.921047568321228, Accuracy: 1.0, Computation time: 1.522104263305664\n",
      "Step: 6995, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 0.957531213760376\n",
      "Step: 6996, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 0.9900445938110352\n",
      "Step: 6997, Loss: 0.9160783886909485, Accuracy: 1.0, Computation time: 0.9981369972229004\n",
      "Step: 6998, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.3095123767852783\n",
      "Step: 6999, Loss: 0.916313886642456, Accuracy: 1.0, Computation time: 1.002760887145996\n",
      "Step: 7000, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.0997300148010254\n",
      "Step: 7001, Loss: 0.9378425478935242, Accuracy: 0.96875, Computation time: 1.2578222751617432\n",
      "Step: 7002, Loss: 0.9159099459648132, Accuracy: 1.0, Computation time: 1.1691501140594482\n",
      "Step: 7003, Loss: 0.9160789251327515, Accuracy: 1.0, Computation time: 1.4568819999694824\n",
      "Step: 7004, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.1915984153747559\n",
      "Step: 7005, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 1.012775182723999\n",
      "Step: 7006, Loss: 0.9544298052787781, Accuracy: 0.9375, Computation time: 1.2742750644683838\n",
      "Step: 7007, Loss: 0.9159169793128967, Accuracy: 1.0, Computation time: 1.1907634735107422\n",
      "Step: 7008, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 0.9623591899871826\n",
      "Step: 7009, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.1756634712219238\n",
      "Step: 7010, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 1.1208348274230957\n",
      "Step: 7011, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.0975539684295654\n",
      "Step: 7012, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 0.9897000789642334\n",
      "Step: 7013, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 0.9506542682647705\n",
      "Step: 7014, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.1200001239776611\n",
      "Step: 7015, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 1.3091144561767578\n",
      "Step: 7016, Loss: 0.916091799736023, Accuracy: 1.0, Computation time: 1.0681238174438477\n",
      "Step: 7017, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.0390565395355225\n",
      "Step: 7018, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.2702479362487793\n",
      "Step: 7019, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.2244038581848145\n",
      "Step: 7020, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 0.9468915462493896\n",
      "Step: 7021, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.8939917087554932\n",
      "Step: 7022, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.1170330047607422\n",
      "Step: 7023, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 1.1523966789245605\n",
      "Step: 7024, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 1.115494728088379\n",
      "Step: 7025, Loss: 0.9233027100563049, Accuracy: 1.0, Computation time: 2.6423556804656982\n",
      "Step: 7026, Loss: 0.9159333109855652, Accuracy: 1.0, Computation time: 0.9903526306152344\n",
      "Step: 7027, Loss: 0.9159553050994873, Accuracy: 1.0, Computation time: 0.9284555912017822\n",
      "Step: 7028, Loss: 0.9160847663879395, Accuracy: 1.0, Computation time: 1.1725661754608154\n",
      "Step: 7029, Loss: 0.9162513613700867, Accuracy: 1.0, Computation time: 1.1669411659240723\n",
      "Step: 7030, Loss: 0.9160513877868652, Accuracy: 1.0, Computation time: 1.170408010482788\n",
      "Step: 7031, Loss: 0.9162716269493103, Accuracy: 1.0, Computation time: 0.9694786071777344\n",
      "Step: 7032, Loss: 0.9377405643463135, Accuracy: 0.96875, Computation time: 1.0756738185882568\n",
      "Step: 7033, Loss: 0.9159331321716309, Accuracy: 1.0, Computation time: 0.8969881534576416\n",
      "Step: 7034, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 0.7991347312927246\n",
      "Step: 7035, Loss: 0.9161065220832825, Accuracy: 1.0, Computation time: 0.8407468795776367\n",
      "Step: 7036, Loss: 0.9209126234054565, Accuracy: 1.0, Computation time: 0.7836503982543945\n",
      "Step: 7037, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 0.7721579074859619\n",
      "Step: 7038, Loss: 0.9161069989204407, Accuracy: 1.0, Computation time: 1.3789284229278564\n",
      "Step: 7039, Loss: 0.9162094593048096, Accuracy: 1.0, Computation time: 0.778632640838623\n",
      "Step: 7040, Loss: 0.9166474938392639, Accuracy: 1.0, Computation time: 1.302823543548584\n",
      "Step: 7041, Loss: 0.9162807464599609, Accuracy: 1.0, Computation time: 0.7419078350067139\n",
      "Step: 7042, Loss: 0.9170137643814087, Accuracy: 1.0, Computation time: 1.0181105136871338\n",
      "Step: 7043, Loss: 0.9380945563316345, Accuracy: 0.96875, Computation time: 0.8647651672363281\n",
      "Step: 7044, Loss: 0.9161282777786255, Accuracy: 1.0, Computation time: 0.7873244285583496\n",
      "Step: 7045, Loss: 0.9159826040267944, Accuracy: 1.0, Computation time: 0.8269970417022705\n",
      "Step: 7046, Loss: 0.9380788803100586, Accuracy: 0.96875, Computation time: 1.1950087547302246\n",
      "Step: 7047, Loss: 0.9593816995620728, Accuracy: 0.9375, Computation time: 0.8367235660552979\n",
      "Step: 7048, Loss: 0.9159682393074036, Accuracy: 1.0, Computation time: 1.039473056793213\n",
      "Step: 7049, Loss: 0.9159554839134216, Accuracy: 1.0, Computation time: 1.1239495277404785\n",
      "Step: 7050, Loss: 0.9161386489868164, Accuracy: 1.0, Computation time: 0.8053808212280273\n",
      "Step: 7051, Loss: 0.9159506559371948, Accuracy: 1.0, Computation time: 0.8116116523742676\n",
      "Step: 7052, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 0.903362512588501\n",
      "Step: 7053, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 0.8761413097381592\n",
      "Step: 7054, Loss: 0.9226763844490051, Accuracy: 1.0, Computation time: 1.26027250289917\n",
      "Step: 7055, Loss: 0.9162067770957947, Accuracy: 1.0, Computation time: 1.1106021404266357\n",
      "Step: 7056, Loss: 0.9161725044250488, Accuracy: 1.0, Computation time: 0.7384262084960938\n",
      "Step: 7057, Loss: 0.9162500500679016, Accuracy: 1.0, Computation time: 0.7520694732666016\n",
      "Step: 7058, Loss: 0.9379380345344543, Accuracy: 0.96875, Computation time: 0.9509613513946533\n",
      "Step: 7059, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 0.9838626384735107\n",
      "Step: 7060, Loss: 0.9374762773513794, Accuracy: 0.96875, Computation time: 1.0234076976776123\n",
      "Step: 7061, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 1.2720513343811035\n",
      "Step: 7062, Loss: 0.9159587025642395, Accuracy: 1.0, Computation time: 1.0107038021087646\n",
      "Step: 7063, Loss: 0.9159796237945557, Accuracy: 1.0, Computation time: 1.1566693782806396\n",
      "Step: 7064, Loss: 0.9161352515220642, Accuracy: 1.0, Computation time: 0.9973742961883545\n",
      "Step: 7065, Loss: 0.9160423278808594, Accuracy: 1.0, Computation time: 0.8789479732513428\n",
      "Step: 7066, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 0.8587653636932373\n",
      "Step: 7067, Loss: 0.9158936738967896, Accuracy: 1.0, Computation time: 0.9196031093597412\n",
      "Step: 7068, Loss: 0.9159622192382812, Accuracy: 1.0, Computation time: 1.0596795082092285\n",
      "Step: 7069, Loss: 0.91584312915802, Accuracy: 1.0, Computation time: 0.8634943962097168\n",
      "Step: 7070, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 0.988792896270752\n",
      "Step: 7071, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 0.882465124130249\n",
      "Step: 7072, Loss: 0.9277209043502808, Accuracy: 0.96875, Computation time: 1.1008424758911133\n",
      "Step: 7073, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 1.0015878677368164\n",
      "Step: 7074, Loss: 0.9160163402557373, Accuracy: 1.0, Computation time: 0.9740548133850098\n",
      "Step: 7075, Loss: 0.9163832664489746, Accuracy: 1.0, Computation time: 1.4263403415679932\n",
      "Step: 7076, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 1.1243467330932617\n",
      "Step: 7077, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.235332727432251\n",
      "Step: 7078, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.162196397781372\n",
      "Step: 7079, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.115123987197876\n",
      "Step: 7080, Loss: 0.9172524213790894, Accuracy: 1.0, Computation time: 1.177779197692871\n",
      "Step: 7081, Loss: 0.9167253971099854, Accuracy: 1.0, Computation time: 1.2110984325408936\n",
      "Step: 7082, Loss: 0.9176629185676575, Accuracy: 1.0, Computation time: 1.2268640995025635\n",
      "Step: 7083, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 0.9307787418365479\n",
      "Step: 7084, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 0.9627704620361328\n",
      "Step: 7085, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 0.9045290946960449\n",
      "Test loss: 1.0694255828857422, Test Accuracy: 0.7769156098365784\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2028b0-553c-4c6b-9405-4e6ee3cb705a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
