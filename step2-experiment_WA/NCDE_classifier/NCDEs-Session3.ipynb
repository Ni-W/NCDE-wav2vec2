{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc730092-2d61-4d9a-acb0-26870cbac0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d95f677-439c-49ed-bc58-dc75c8087061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b5324c-2040-4e58-8632-6eae2dcd2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4380, 256, 768) (4380,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last2, wav2vec_last4, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last2,label_last4,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3069199c-f12b-4605-aae5-6f20713391dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed25523-11d9-4dd5-8812-478ea9ce2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07832cf0-d7e2-4f1f-8f1e-c8dc04b5349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        #先将数据流降维再放入模型中训练\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0) \n",
    "            pred_var=prediction.var(axis=0) \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)\n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872948e5-5baf-4173-95f1-4b40f8ca83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0618f74e-1cc8-4b15-9444-5a2d256a4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last3], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last3\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4093697b-9d1e-449f-a3e8-a7fe321960ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bced6cb-0423-4b28-9ab6-d8f217a39dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49e47df-8db1-4c87-9efe-d1ac8c80ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4380,\n",
    "    dataset_test_size=1151,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=3589,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        total_acc = 0\n",
    "        total_num = total_size\n",
    "        predicted_class = jnp.argmax(pred, axis=1)\n",
    "        total_acc += jnp.sum(predicted_class == label_i)\n",
    "        return total_acc / total_num\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9113d6d-4e5a-4f02-8b68-601d16bdb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.482541561126709, Accuracy: 0.1875, Computation time: 13.662938833236694\n",
      "Step: 1, Loss: 1.336632251739502, Accuracy: 0.46875, Computation time: 3.025475263595581\n",
      "Step: 2, Loss: 1.387138843536377, Accuracy: 0.125, Computation time: 3.3664939403533936\n",
      "Step: 3, Loss: 1.3594563007354736, Accuracy: 0.40625, Computation time: 3.0489325523376465\n",
      "Step: 4, Loss: 1.3694967031478882, Accuracy: 0.3125, Computation time: 3.031599283218384\n",
      "Step: 5, Loss: 1.3349233865737915, Accuracy: 0.375, Computation time: 3.2432217597961426\n",
      "Step: 6, Loss: 1.3896957635879517, Accuracy: 0.21875, Computation time: 3.1269495487213135\n",
      "Step: 7, Loss: 1.3872665166854858, Accuracy: 0.21875, Computation time: 2.8043148517608643\n",
      "Step: 8, Loss: 1.343180775642395, Accuracy: 0.59375, Computation time: 3.492328405380249\n",
      "Step: 9, Loss: 1.2640554904937744, Accuracy: 0.71875, Computation time: 2.79123854637146\n",
      "Step: 10, Loss: 1.2838479280471802, Accuracy: 0.40625, Computation time: 3.4542460441589355\n",
      "Step: 11, Loss: 1.2759734392166138, Accuracy: 0.46875, Computation time: 2.921839952468872\n",
      "Step: 12, Loss: 1.2829928398132324, Accuracy: 0.34375, Computation time: 4.092084646224976\n",
      "Step: 13, Loss: 1.2429401874542236, Accuracy: 0.71875, Computation time: 3.340653896331787\n",
      "Step: 14, Loss: 1.3409616947174072, Accuracy: 0.21875, Computation time: 2.773303747177124\n",
      "Step: 15, Loss: 1.230762243270874, Accuracy: 0.6875, Computation time: 2.8146097660064697\n",
      "Step: 16, Loss: 1.2780333757400513, Accuracy: 0.5, Computation time: 2.814976692199707\n",
      "Step: 17, Loss: 1.2615073919296265, Accuracy: 0.59375, Computation time: 3.16304087638855\n",
      "Step: 18, Loss: 1.2082648277282715, Accuracy: 0.59375, Computation time: 3.6075615882873535\n",
      "Step: 19, Loss: 1.3366751670837402, Accuracy: 0.375, Computation time: 3.5880606174468994\n",
      "Step: 20, Loss: 1.1516070365905762, Accuracy: 0.65625, Computation time: 3.128552198410034\n",
      "Step: 21, Loss: 1.1619739532470703, Accuracy: 0.625, Computation time: 3.3616092205047607\n",
      "Step: 22, Loss: 1.1810506582260132, Accuracy: 0.5625, Computation time: 3.306831121444702\n",
      "Step: 23, Loss: 1.0250232219696045, Accuracy: 0.875, Computation time: 3.2159080505371094\n",
      "Step: 24, Loss: 1.088196873664856, Accuracy: 0.8125, Computation time: 3.524406671524048\n",
      "Step: 25, Loss: 1.1000889539718628, Accuracy: 0.84375, Computation time: 3.0077104568481445\n",
      "Step: 26, Loss: 1.1189935207366943, Accuracy: 0.71875, Computation time: 3.9472742080688477\n",
      "Step: 27, Loss: 1.0910834074020386, Accuracy: 0.78125, Computation time: 3.3943119049072266\n",
      "Step: 28, Loss: 1.0439412593841553, Accuracy: 0.84375, Computation time: 3.61504864692688\n",
      "Step: 29, Loss: 1.000319480895996, Accuracy: 0.90625, Computation time: 2.6409895420074463\n",
      "Step: 30, Loss: 1.0229276418685913, Accuracy: 0.875, Computation time: 3.0547871589660645\n",
      "Step: 31, Loss: 1.0375874042510986, Accuracy: 0.84375, Computation time: 3.1856257915496826\n",
      "Step: 32, Loss: 0.9959996938705444, Accuracy: 0.90625, Computation time: 2.685626745223999\n",
      "Step: 33, Loss: 1.0647251605987549, Accuracy: 0.78125, Computation time: 2.9329380989074707\n",
      "Step: 34, Loss: 1.0241447687149048, Accuracy: 0.84375, Computation time: 2.6599204540252686\n",
      "Step: 35, Loss: 1.1000487804412842, Accuracy: 0.6875, Computation time: 2.422912359237671\n",
      "Step: 36, Loss: 1.0196528434753418, Accuracy: 0.84375, Computation time: 2.0071771144866943\n",
      "Step: 37, Loss: 1.022692322731018, Accuracy: 0.8125, Computation time: 2.485934019088745\n",
      "Step: 38, Loss: 0.9908656477928162, Accuracy: 0.90625, Computation time: 1.8967723846435547\n",
      "Step: 39, Loss: 1.0083070993423462, Accuracy: 0.90625, Computation time: 2.299497127532959\n",
      "Step: 40, Loss: 1.0096228122711182, Accuracy: 0.9375, Computation time: 1.9108586311340332\n",
      "Step: 41, Loss: 1.0124180316925049, Accuracy: 0.90625, Computation time: 2.2862918376922607\n",
      "Step: 42, Loss: 0.9756044149398804, Accuracy: 0.96875, Computation time: 2.0499138832092285\n",
      "Step: 43, Loss: 1.0000208616256714, Accuracy: 0.96875, Computation time: 2.5864601135253906\n",
      "Step: 44, Loss: 0.979939341545105, Accuracy: 0.96875, Computation time: 2.7339956760406494\n",
      "Step: 45, Loss: 0.9705935716629028, Accuracy: 0.96875, Computation time: 2.3340485095977783\n",
      "Step: 46, Loss: 0.9437046647071838, Accuracy: 1.0, Computation time: 2.4768221378326416\n",
      "Step: 47, Loss: 0.9629572033882141, Accuracy: 0.96875, Computation time: 2.689032793045044\n",
      "Step: 48, Loss: 0.9509057998657227, Accuracy: 0.96875, Computation time: 2.5126428604125977\n",
      "Step: 49, Loss: 0.9653864502906799, Accuracy: 0.9375, Computation time: 2.151686191558838\n",
      "Step: 50, Loss: 0.939893901348114, Accuracy: 1.0, Computation time: 2.3551108837127686\n",
      "Step: 51, Loss: 0.9299142360687256, Accuracy: 1.0, Computation time: 1.8780012130737305\n",
      "Step: 52, Loss: 0.943126380443573, Accuracy: 0.96875, Computation time: 2.1173043251037598\n",
      "Step: 53, Loss: 0.938965380191803, Accuracy: 0.96875, Computation time: 1.9638869762420654\n",
      "Step: 54, Loss: 0.9246754050254822, Accuracy: 1.0, Computation time: 2.0607047080993652\n",
      "Step: 55, Loss: 0.9286789298057556, Accuracy: 1.0, Computation time: 2.3915631771087646\n",
      "Step: 56, Loss: 0.9247764945030212, Accuracy: 1.0, Computation time: 2.188467502593994\n",
      "Step: 57, Loss: 0.9524368047714233, Accuracy: 0.96875, Computation time: 2.195941925048828\n",
      "Step: 58, Loss: 0.9409248232841492, Accuracy: 0.96875, Computation time: 1.805448055267334\n",
      "Step: 59, Loss: 0.9230908155441284, Accuracy: 1.0, Computation time: 1.8818597793579102\n",
      "Step: 60, Loss: 0.9284551739692688, Accuracy: 1.0, Computation time: 2.0860214233398438\n",
      "Step: 61, Loss: 0.9277498722076416, Accuracy: 1.0, Computation time: 1.8731727600097656\n",
      "Step: 62, Loss: 0.9272891283035278, Accuracy: 1.0, Computation time: 2.393203020095825\n",
      "Step: 63, Loss: 0.9485363364219666, Accuracy: 0.96875, Computation time: 2.1786627769470215\n",
      "Step: 64, Loss: 0.9280819892883301, Accuracy: 1.0, Computation time: 1.638718605041504\n",
      "Step: 65, Loss: 0.9863929748535156, Accuracy: 0.90625, Computation time: 1.80057692527771\n",
      "Step: 66, Loss: 0.935941219329834, Accuracy: 0.96875, Computation time: 2.138049364089966\n",
      "Step: 67, Loss: 0.9586775302886963, Accuracy: 0.9375, Computation time: 1.9167609214782715\n",
      "Step: 68, Loss: 0.918816864490509, Accuracy: 1.0, Computation time: 1.8241291046142578\n",
      "Step: 69, Loss: 0.920897364616394, Accuracy: 1.0, Computation time: 1.9405858516693115\n",
      "Step: 70, Loss: 0.9414060115814209, Accuracy: 0.96875, Computation time: 1.934171438217163\n",
      "Step: 71, Loss: 0.9200237989425659, Accuracy: 1.0, Computation time: 1.9558145999908447\n",
      "Step: 72, Loss: 0.9183034300804138, Accuracy: 1.0, Computation time: 2.1592278480529785\n",
      "Step: 73, Loss: 0.9329354166984558, Accuracy: 0.96875, Computation time: 1.6925110816955566\n",
      "Step: 74, Loss: 0.9311253428459167, Accuracy: 0.96875, Computation time: 1.6183745861053467\n",
      "Step: 75, Loss: 0.9186509847640991, Accuracy: 1.0, Computation time: 1.6159119606018066\n",
      "Step: 76, Loss: 0.9187827110290527, Accuracy: 1.0, Computation time: 2.166919231414795\n",
      "Step: 77, Loss: 0.9263570308685303, Accuracy: 1.0, Computation time: 1.967214584350586\n",
      "Step: 78, Loss: 0.917161226272583, Accuracy: 1.0, Computation time: 2.010906934738159\n",
      "Step: 79, Loss: 0.9183424115180969, Accuracy: 1.0, Computation time: 1.9719874858856201\n",
      "Step: 80, Loss: 0.9342431426048279, Accuracy: 0.96875, Computation time: 2.1268787384033203\n",
      "Step: 81, Loss: 0.9210231304168701, Accuracy: 1.0, Computation time: 1.6916658878326416\n",
      "Step: 82, Loss: 0.9233955144882202, Accuracy: 1.0, Computation time: 2.049241542816162\n",
      "Step: 83, Loss: 0.9177454710006714, Accuracy: 1.0, Computation time: 1.6962263584136963\n",
      "Step: 84, Loss: 0.9175012111663818, Accuracy: 1.0, Computation time: 2.150341033935547\n",
      "Step: 85, Loss: 0.9431314468383789, Accuracy: 0.96875, Computation time: 2.1104578971862793\n",
      "Step: 86, Loss: 0.9304038286209106, Accuracy: 0.96875, Computation time: 1.929487943649292\n",
      "Step: 87, Loss: 0.9201871156692505, Accuracy: 1.0, Computation time: 2.4197144508361816\n",
      "Step: 88, Loss: 0.9362295269966125, Accuracy: 0.96875, Computation time: 2.2773618698120117\n",
      "Step: 89, Loss: 0.9202607274055481, Accuracy: 1.0, Computation time: 1.9333131313323975\n",
      "Step: 90, Loss: 0.9174221754074097, Accuracy: 1.0, Computation time: 1.6196813583374023\n",
      "Step: 91, Loss: 0.9410818219184875, Accuracy: 0.96875, Computation time: 2.0414552688598633\n",
      "Step: 92, Loss: 0.9190001487731934, Accuracy: 1.0, Computation time: 1.7669174671173096\n",
      "Step: 93, Loss: 0.9531247019767761, Accuracy: 0.9375, Computation time: 2.192840337753296\n",
      "Step: 94, Loss: 0.9230396747589111, Accuracy: 1.0, Computation time: 3.3947229385375977\n",
      "Step: 95, Loss: 0.9300398826599121, Accuracy: 1.0, Computation time: 2.0556623935699463\n",
      "Step: 96, Loss: 0.9342831969261169, Accuracy: 0.96875, Computation time: 2.0592527389526367\n",
      "Step: 97, Loss: 0.9196441173553467, Accuracy: 1.0, Computation time: 2.417973279953003\n",
      "Step: 98, Loss: 0.9183624982833862, Accuracy: 1.0, Computation time: 1.8787779808044434\n",
      "Step: 99, Loss: 0.9167484045028687, Accuracy: 1.0, Computation time: 1.878910779953003\n",
      "Step: 100, Loss: 0.9582870006561279, Accuracy: 0.9375, Computation time: 2.2774765491485596\n",
      "Step: 101, Loss: 0.9176947474479675, Accuracy: 1.0, Computation time: 1.6694581508636475\n",
      "Step: 102, Loss: 0.9666261672973633, Accuracy: 0.9375, Computation time: 1.9291679859161377\n",
      "Step: 103, Loss: 0.9195420145988464, Accuracy: 1.0, Computation time: 1.921903133392334\n",
      "Step: 104, Loss: 0.9236598610877991, Accuracy: 1.0, Computation time: 1.9595143795013428\n",
      "Step: 105, Loss: 0.9179787039756775, Accuracy: 1.0, Computation time: 2.318490982055664\n",
      "Step: 106, Loss: 0.9204744100570679, Accuracy: 1.0, Computation time: 1.7362141609191895\n",
      "Step: 107, Loss: 0.9412598609924316, Accuracy: 0.96875, Computation time: 1.7949180603027344\n",
      "Step: 108, Loss: 0.9178813695907593, Accuracy: 1.0, Computation time: 1.8809309005737305\n",
      "Step: 109, Loss: 0.920566737651825, Accuracy: 1.0, Computation time: 1.7265655994415283\n",
      "Step: 110, Loss: 0.9274548292160034, Accuracy: 0.96875, Computation time: 1.8782751560211182\n",
      "Step: 111, Loss: 0.9188852906227112, Accuracy: 1.0, Computation time: 2.27978777885437\n",
      "Step: 112, Loss: 0.9178197383880615, Accuracy: 1.0, Computation time: 2.086383104324341\n",
      "Step: 113, Loss: 0.9170242547988892, Accuracy: 1.0, Computation time: 2.324740171432495\n",
      "Step: 114, Loss: 0.9166264533996582, Accuracy: 1.0, Computation time: 1.596276044845581\n",
      "Step: 115, Loss: 0.9163033962249756, Accuracy: 1.0, Computation time: 1.846980094909668\n",
      "Step: 116, Loss: 0.9385136961936951, Accuracy: 0.96875, Computation time: 1.8743581771850586\n",
      "Step: 117, Loss: 0.9202554821968079, Accuracy: 1.0, Computation time: 2.010885238647461\n",
      "Step: 118, Loss: 0.9207559823989868, Accuracy: 1.0, Computation time: 1.8195843696594238\n",
      "Step: 119, Loss: 0.9249511361122131, Accuracy: 1.0, Computation time: 2.111612319946289\n",
      "Step: 120, Loss: 0.9205899238586426, Accuracy: 1.0, Computation time: 1.9119622707366943\n",
      "Step: 121, Loss: 0.9190732836723328, Accuracy: 1.0, Computation time: 1.7177205085754395\n",
      "Step: 122, Loss: 0.9175671935081482, Accuracy: 1.0, Computation time: 2.6318721771240234\n",
      "Step: 123, Loss: 0.9243199825286865, Accuracy: 1.0, Computation time: 2.877894639968872\n",
      "Step: 124, Loss: 0.9210999608039856, Accuracy: 1.0, Computation time: 2.2576591968536377\n",
      "Step: 125, Loss: 0.926415741443634, Accuracy: 0.96875, Computation time: 2.002493381500244\n",
      "Step: 126, Loss: 0.922581136226654, Accuracy: 1.0, Computation time: 2.029923439025879\n",
      "Step: 127, Loss: 0.9378479719161987, Accuracy: 0.96875, Computation time: 1.9657936096191406\n",
      "Step: 128, Loss: 0.9257369041442871, Accuracy: 1.0, Computation time: 1.828338861465454\n",
      "Step: 129, Loss: 0.9409233927726746, Accuracy: 0.96875, Computation time: 1.5798170566558838\n",
      "Step: 130, Loss: 0.9167134165763855, Accuracy: 1.0, Computation time: 2.1974236965179443\n",
      "Step: 131, Loss: 0.9180938601493835, Accuracy: 1.0, Computation time: 1.8235576152801514\n",
      "Step: 132, Loss: 0.9171932339668274, Accuracy: 1.0, Computation time: 2.1131973266601562\n",
      "Step: 133, Loss: 0.9179980754852295, Accuracy: 1.0, Computation time: 1.6988623142242432\n",
      "Step: 134, Loss: 0.9178918600082397, Accuracy: 1.0, Computation time: 1.6444737911224365\n",
      "Step: 135, Loss: 0.9215931296348572, Accuracy: 1.0, Computation time: 1.8895163536071777\n",
      "Step: 177, Loss: 0.9162988662719727, Accuracy: 1.0, Computation time: 2.1343133449554443\n",
      "Step: 178, Loss: 0.9393138289451599, Accuracy: 0.96875, Computation time: 2.6521880626678467\n",
      "Step: 179, Loss: 0.9165844917297363, Accuracy: 1.0, Computation time: 2.217433452606201\n",
      "Step: 180, Loss: 0.916719377040863, Accuracy: 1.0, Computation time: 1.7298195362091064\n",
      "Step: 181, Loss: 0.9501742720603943, Accuracy: 0.9375, Computation time: 2.215766429901123\n",
      "Step: 182, Loss: 0.9380212426185608, Accuracy: 0.96875, Computation time: 1.9388558864593506\n",
      "Step: 183, Loss: 0.921689510345459, Accuracy: 1.0, Computation time: 1.9048783779144287\n",
      "Step: 184, Loss: 0.9162865877151489, Accuracy: 1.0, Computation time: 2.2441864013671875\n",
      "Step: 185, Loss: 0.9379404187202454, Accuracy: 0.96875, Computation time: 2.285897970199585\n",
      "Step: 186, Loss: 0.9166756868362427, Accuracy: 1.0, Computation time: 1.7162327766418457\n",
      "Step: 187, Loss: 0.9172763824462891, Accuracy: 1.0, Computation time: 2.2100839614868164\n",
      "Step: 188, Loss: 0.916155219078064, Accuracy: 1.0, Computation time: 2.1107449531555176\n",
      "Step: 189, Loss: 0.9185525178909302, Accuracy: 1.0, Computation time: 1.7751896381378174\n",
      "Step: 190, Loss: 0.9172240495681763, Accuracy: 1.0, Computation time: 1.9610660076141357\n",
      "Step: 191, Loss: 0.9173401594161987, Accuracy: 1.0, Computation time: 2.1914117336273193\n",
      "Step: 192, Loss: 0.9552525877952576, Accuracy: 0.9375, Computation time: 1.983186960220337\n",
      "Step: 193, Loss: 0.916656494140625, Accuracy: 1.0, Computation time: 1.942007303237915\n",
      "Step: 194, Loss: 0.9162697792053223, Accuracy: 1.0, Computation time: 1.7607941627502441\n",
      "Step: 195, Loss: 0.9164198637008667, Accuracy: 1.0, Computation time: 2.1183876991271973\n",
      "Step: 196, Loss: 0.9164943695068359, Accuracy: 1.0, Computation time: 2.029343366622925\n",
      "Step: 197, Loss: 0.9238086938858032, Accuracy: 1.0, Computation time: 1.9814119338989258\n",
      "Step: 198, Loss: 0.9241786599159241, Accuracy: 1.0, Computation time: 1.9177806377410889\n",
      "Step: 199, Loss: 0.9162057638168335, Accuracy: 1.0, Computation time: 1.780404806137085\n",
      "Step: 200, Loss: 0.918436586856842, Accuracy: 1.0, Computation time: 3.0720760822296143\n",
      "Step: 201, Loss: 0.9169975519180298, Accuracy: 1.0, Computation time: 1.9707720279693604\n",
      "Step: 202, Loss: 0.9406565427780151, Accuracy: 0.96875, Computation time: 2.0053927898406982\n",
      "Step: 203, Loss: 0.9166715741157532, Accuracy: 1.0, Computation time: 2.0066776275634766\n",
      "Step: 204, Loss: 0.9172492623329163, Accuracy: 1.0, Computation time: 1.8009095191955566\n",
      "Step: 205, Loss: 0.9377011060714722, Accuracy: 0.96875, Computation time: 2.156156063079834\n",
      "Step: 206, Loss: 0.9388042688369751, Accuracy: 0.96875, Computation time: 2.714698076248169\n",
      "Step: 207, Loss: 0.9177751541137695, Accuracy: 1.0, Computation time: 2.0960679054260254\n",
      "Step: 208, Loss: 0.9246476888656616, Accuracy: 1.0, Computation time: 1.9647135734558105\n",
      "Step: 209, Loss: 0.9234252572059631, Accuracy: 1.0, Computation time: 2.2121193408966064\n",
      "Step: 210, Loss: 0.917066216468811, Accuracy: 1.0, Computation time: 2.2819745540618896\n",
      "Step: 211, Loss: 0.9198669195175171, Accuracy: 1.0, Computation time: 2.528040647506714\n",
      "Step: 212, Loss: 0.9166568517684937, Accuracy: 1.0, Computation time: 1.8759808540344238\n",
      "Step: 213, Loss: 0.9166678786277771, Accuracy: 1.0, Computation time: 2.707947015762329\n",
      "Step: 214, Loss: 0.9162768125534058, Accuracy: 1.0, Computation time: 1.7465996742248535\n",
      "Step: 215, Loss: 0.9398952126502991, Accuracy: 0.96875, Computation time: 2.2242534160614014\n",
      "Step: 216, Loss: 0.9379909634590149, Accuracy: 0.96875, Computation time: 2.283442497253418\n",
      "Step: 217, Loss: 0.950520396232605, Accuracy: 0.9375, Computation time: 2.21173357963562\n",
      "Step: 218, Loss: 0.9164834022521973, Accuracy: 1.0, Computation time: 1.8533899784088135\n",
      "Step: 219, Loss: 0.9173839092254639, Accuracy: 1.0, Computation time: 2.239081859588623\n",
      "Step: 220, Loss: 0.9339479207992554, Accuracy: 0.96875, Computation time: 2.116382122039795\n",
      "Step: 221, Loss: 0.9166072010993958, Accuracy: 1.0, Computation time: 2.0201303958892822\n",
      "Step: 222, Loss: 0.9207472205162048, Accuracy: 1.0, Computation time: 2.112635374069214\n",
      "Step: 223, Loss: 0.9376294016838074, Accuracy: 0.96875, Computation time: 1.93180513381958\n",
      "Step: 224, Loss: 0.9172024726867676, Accuracy: 1.0, Computation time: 1.8729357719421387\n",
      "Step: 225, Loss: 0.9161013960838318, Accuracy: 1.0, Computation time: 1.8837437629699707\n",
      "Step: 226, Loss: 0.9265496730804443, Accuracy: 0.96875, Computation time: 1.7747018337249756\n",
      "Step: 227, Loss: 0.9353838562965393, Accuracy: 0.96875, Computation time: 2.1364660263061523\n",
      "Step: 228, Loss: 0.9163493514060974, Accuracy: 1.0, Computation time: 1.8950743675231934\n",
      "Step: 229, Loss: 0.9399652481079102, Accuracy: 0.96875, Computation time: 2.218113422393799\n",
      "Step: 230, Loss: 0.9180113673210144, Accuracy: 1.0, Computation time: 2.477250814437866\n",
      "Step: 231, Loss: 0.9179736971855164, Accuracy: 1.0, Computation time: 2.123445749282837\n",
      "Step: 232, Loss: 0.9371594786643982, Accuracy: 0.96875, Computation time: 2.116961717605591\n",
      "Step: 233, Loss: 0.9163714647293091, Accuracy: 1.0, Computation time: 2.3281400203704834\n",
      "Step: 234, Loss: 0.9161575436592102, Accuracy: 1.0, Computation time: 2.120206594467163\n",
      "Step: 235, Loss: 0.9358697533607483, Accuracy: 0.96875, Computation time: 2.050015687942505\n",
      "Step: 236, Loss: 0.9167271852493286, Accuracy: 1.0, Computation time: 2.027493476867676\n",
      "Step: 237, Loss: 0.9161247611045837, Accuracy: 1.0, Computation time: 2.0416810512542725\n",
      "Step: 238, Loss: 0.9162257313728333, Accuracy: 1.0, Computation time: 1.9631006717681885\n",
      "Step: 239, Loss: 0.943378210067749, Accuracy: 0.96875, Computation time: 2.689068078994751\n",
      "Step: 240, Loss: 0.9166867733001709, Accuracy: 1.0, Computation time: 1.8857924938201904\n",
      "Step: 241, Loss: 0.9163219928741455, Accuracy: 1.0, Computation time: 2.3945555686950684\n",
      "Step: 242, Loss: 0.916908323764801, Accuracy: 1.0, Computation time: 1.961817979812622\n",
      "Step: 243, Loss: 0.9471968412399292, Accuracy: 0.9375, Computation time: 2.4195892810821533\n",
      "Step: 244, Loss: 0.9194899797439575, Accuracy: 1.0, Computation time: 2.6724960803985596\n",
      "Step: 245, Loss: 0.9161215424537659, Accuracy: 1.0, Computation time: 2.0635087490081787\n",
      "Step: 246, Loss: 0.9219188094139099, Accuracy: 1.0, Computation time: 2.2973885536193848\n",
      "Step: 247, Loss: 0.9164441823959351, Accuracy: 1.0, Computation time: 1.974306344985962\n",
      "Step: 248, Loss: 0.9379321336746216, Accuracy: 0.96875, Computation time: 2.40042781829834\n",
      "Step: 249, Loss: 0.9164654016494751, Accuracy: 1.0, Computation time: 2.2375950813293457\n",
      "Step: 250, Loss: 0.916768491268158, Accuracy: 1.0, Computation time: 1.9168753623962402\n",
      "Step: 251, Loss: 0.9168627858161926, Accuracy: 1.0, Computation time: 1.8852319717407227\n",
      "Step: 252, Loss: 0.9163392782211304, Accuracy: 1.0, Computation time: 2.6355252265930176\n",
      "Step: 253, Loss: 0.918734610080719, Accuracy: 1.0, Computation time: 2.595247745513916\n",
      "Step: 254, Loss: 0.9211986660957336, Accuracy: 1.0, Computation time: 2.729503870010376\n",
      "Step: 255, Loss: 0.9161880612373352, Accuracy: 1.0, Computation time: 2.15563702583313\n",
      "Step: 256, Loss: 0.9166960716247559, Accuracy: 1.0, Computation time: 2.03898286819458\n",
      "Step: 257, Loss: 0.9373694062232971, Accuracy: 0.96875, Computation time: 2.2648842334747314\n",
      "Step: 258, Loss: 0.9170302748680115, Accuracy: 1.0, Computation time: 2.1687707901000977\n",
      "Step: 259, Loss: 0.9162667393684387, Accuracy: 1.0, Computation time: 1.9981989860534668\n",
      "Step: 260, Loss: 0.9233246445655823, Accuracy: 1.0, Computation time: 2.0872318744659424\n",
      "Step: 261, Loss: 0.9188977479934692, Accuracy: 1.0, Computation time: 1.693770170211792\n",
      "Step: 262, Loss: 0.917950451374054, Accuracy: 1.0, Computation time: 2.280815601348877\n",
      "Step: 263, Loss: 0.9188677072525024, Accuracy: 1.0, Computation time: 2.3201634883880615\n",
      "Step: 264, Loss: 0.9163233637809753, Accuracy: 1.0, Computation time: 2.109806537628174\n",
      "Step: 265, Loss: 0.9354357123374939, Accuracy: 0.96875, Computation time: 2.751356363296509\n",
      "Step: 266, Loss: 0.9177774786949158, Accuracy: 1.0, Computation time: 1.8453130722045898\n",
      "Step: 267, Loss: 0.9351698756217957, Accuracy: 0.96875, Computation time: 2.239511251449585\n",
      "Step: 268, Loss: 0.9166094660758972, Accuracy: 1.0, Computation time: 2.0108745098114014\n",
      "Step: 269, Loss: 0.93630450963974, Accuracy: 0.96875, Computation time: 2.2307395935058594\n",
      "Step: 270, Loss: 0.9164524674415588, Accuracy: 1.0, Computation time: 1.9159929752349854\n",
      "Step: 271, Loss: 0.9382448792457581, Accuracy: 0.96875, Computation time: 2.1185591220855713\n",
      "Step: 272, Loss: 0.9160301685333252, Accuracy: 1.0, Computation time: 2.028400182723999\n",
      "Step: 273, Loss: 0.937941312789917, Accuracy: 0.96875, Computation time: 2.8914833068847656\n",
      "Step: 274, Loss: 0.9161432981491089, Accuracy: 1.0, Computation time: 2.170072317123413\n",
      "Step: 275, Loss: 0.9544599056243896, Accuracy: 0.9375, Computation time: 2.4622645378112793\n",
      "Step: 276, Loss: 0.9175550937652588, Accuracy: 1.0, Computation time: 2.212716579437256\n",
      "Step: 277, Loss: 0.9161022305488586, Accuracy: 1.0, Computation time: 2.0240657329559326\n",
      "Step: 278, Loss: 0.9162895679473877, Accuracy: 1.0, Computation time: 2.0507566928863525\n",
      "########################\n",
      "Test loss: 1.1124478578567505, Test Accuracy_epoch2: 0.7098175883293152\n",
      "########################\n",
      "Step: 279, Loss: 0.9159867167472839, Accuracy: 1.0, Computation time: 1.7710926532745361\n",
      "Step: 280, Loss: 0.9378159642219543, Accuracy: 0.96875, Computation time: 2.329601526260376\n",
      "Step: 281, Loss: 0.9160839915275574, Accuracy: 1.0, Computation time: 1.9361293315887451\n",
      "Step: 282, Loss: 0.9168856143951416, Accuracy: 1.0, Computation time: 1.9866437911987305\n",
      "Step: 283, Loss: 0.9160910248756409, Accuracy: 1.0, Computation time: 2.244372606277466\n",
      "Step: 284, Loss: 0.917872965335846, Accuracy: 1.0, Computation time: 2.0434207916259766\n",
      "Step: 285, Loss: 0.9172693490982056, Accuracy: 1.0, Computation time: 2.271116018295288\n",
      "Step: 286, Loss: 0.9199475049972534, Accuracy: 1.0, Computation time: 2.0862061977386475\n",
      "Step: 287, Loss: 0.9163283109664917, Accuracy: 1.0, Computation time: 2.1320602893829346\n",
      "Step: 288, Loss: 0.9210612773895264, Accuracy: 1.0, Computation time: 2.95002818107605\n",
      "Step: 289, Loss: 0.9166268706321716, Accuracy: 1.0, Computation time: 2.014455795288086\n",
      "Step: 290, Loss: 0.9161165952682495, Accuracy: 1.0, Computation time: 2.331219434738159\n",
      "Step: 291, Loss: 0.9162797331809998, Accuracy: 1.0, Computation time: 2.0804898738861084\n",
      "Step: 292, Loss: 0.9162805676460266, Accuracy: 1.0, Computation time: 1.8498127460479736\n",
      "Step: 293, Loss: 0.9162889122962952, Accuracy: 1.0, Computation time: 2.272371292114258\n",
      "Step: 294, Loss: 0.9184966087341309, Accuracy: 1.0, Computation time: 2.2945287227630615\n",
      "Step: 295, Loss: 0.9161779880523682, Accuracy: 1.0, Computation time: 2.26065731048584\n",
      "Step: 296, Loss: 0.9165592789649963, Accuracy: 1.0, Computation time: 1.8955128192901611\n",
      "Step: 297, Loss: 0.9179195165634155, Accuracy: 1.0, Computation time: 2.1197707653045654\n",
      "Step: 298, Loss: 0.9291793704032898, Accuracy: 0.96875, Computation time: 1.931640625\n",
      "Step: 299, Loss: 0.916402280330658, Accuracy: 1.0, Computation time: 2.172456979751587\n",
      "Step: 300, Loss: 0.938439667224884, Accuracy: 0.96875, Computation time: 2.160315990447998\n",
      "Step: 301, Loss: 0.9162970781326294, Accuracy: 1.0, Computation time: 2.4339137077331543\n",
      "Step: 302, Loss: 0.9382299780845642, Accuracy: 0.96875, Computation time: 2.015181064605713\n",
      "Step: 303, Loss: 0.916218638420105, Accuracy: 1.0, Computation time: 1.7537736892700195\n",
      "Step: 304, Loss: 0.9349416494369507, Accuracy: 0.96875, Computation time: 2.0781893730163574\n",
      "Step: 305, Loss: 0.9377902746200562, Accuracy: 0.96875, Computation time: 2.342331647872925\n",
      "Step: 306, Loss: 0.937494158744812, Accuracy: 0.96875, Computation time: 2.0139660835266113\n",
      "Step: 307, Loss: 0.9163725972175598, Accuracy: 1.0, Computation time: 2.0032336711883545\n",
      "Step: 308, Loss: 0.9381924271583557, Accuracy: 0.96875, Computation time: 2.061908006668091\n",
      "Step: 309, Loss: 0.9162417054176331, Accuracy: 1.0, Computation time: 1.7991435527801514\n",
      "Step: 310, Loss: 0.9168020486831665, Accuracy: 1.0, Computation time: 2.495298147201538\n",
      "Step: 311, Loss: 0.9277275204658508, Accuracy: 0.96875, Computation time: 2.4042625427246094\n",
      "Step: 312, Loss: 0.9163848161697388, Accuracy: 1.0, Computation time: 2.3235511779785156\n",
      "Step: 313, Loss: 0.9263455867767334, Accuracy: 0.96875, Computation time: 2.0047740936279297\n",
      "Step: 314, Loss: 0.9382973909378052, Accuracy: 0.96875, Computation time: 2.1687562465667725\n",
      "Step: 315, Loss: 0.9164991974830627, Accuracy: 1.0, Computation time: 2.107545852661133\n",
      "Step: 316, Loss: 0.916160523891449, Accuracy: 1.0, Computation time: 1.871201515197754\n",
      "Step: 317, Loss: 0.9163833856582642, Accuracy: 1.0, Computation time: 2.2114877700805664\n",
      "Step: 318, Loss: 0.9177020788192749, Accuracy: 1.0, Computation time: 1.9052019119262695\n",
      "Step: 319, Loss: 0.916178822517395, Accuracy: 1.0, Computation time: 1.9404659271240234\n",
      "Step: 320, Loss: 0.9369552135467529, Accuracy: 0.96875, Computation time: 2.077956199645996\n",
      "Step: 321, Loss: 0.9161581993103027, Accuracy: 1.0, Computation time: 2.1801886558532715\n",
      "Step: 322, Loss: 0.9162431955337524, Accuracy: 1.0, Computation time: 1.7838013172149658\n",
      "Step: 323, Loss: 0.9165651202201843, Accuracy: 1.0, Computation time: 1.9186608791351318\n",
      "Step: 324, Loss: 0.9507912993431091, Accuracy: 0.9375, Computation time: 1.8166463375091553\n",
      "Step: 325, Loss: 0.9264271259307861, Accuracy: 0.96875, Computation time: 2.2962894439697266\n",
      "Step: 326, Loss: 0.9368091821670532, Accuracy: 0.96875, Computation time: 1.941659927368164\n",
      "Step: 327, Loss: 0.9208555221557617, Accuracy: 1.0, Computation time: 2.039642572402954\n",
      "Step: 328, Loss: 0.916460394859314, Accuracy: 1.0, Computation time: 2.5221786499023438\n",
      "Step: 329, Loss: 0.9167726635932922, Accuracy: 1.0, Computation time: 2.9154605865478516\n",
      "Step: 330, Loss: 0.9190261363983154, Accuracy: 1.0, Computation time: 2.252126455307007\n",
      "Step: 331, Loss: 0.9208471179008484, Accuracy: 1.0, Computation time: 2.020963430404663\n",
      "Step: 332, Loss: 0.9162752628326416, Accuracy: 1.0, Computation time: 2.213916540145874\n",
      "Step: 333, Loss: 0.916157066822052, Accuracy: 1.0, Computation time: 2.277952194213867\n",
      "Step: 334, Loss: 0.9273014068603516, Accuracy: 0.96875, Computation time: 2.445885181427002\n",
      "Step: 335, Loss: 0.9160417914390564, Accuracy: 1.0, Computation time: 1.9020733833312988\n",
      "Step: 336, Loss: 0.9236375689506531, Accuracy: 1.0, Computation time: 2.6062183380126953\n",
      "Step: 337, Loss: 0.9168261289596558, Accuracy: 1.0, Computation time: 2.457876443862915\n",
      "Step: 338, Loss: 0.9381516575813293, Accuracy: 0.96875, Computation time: 2.280543565750122\n",
      "Step: 339, Loss: 0.9378225803375244, Accuracy: 0.96875, Computation time: 2.8924267292022705\n",
      "Step: 340, Loss: 0.9227719306945801, Accuracy: 1.0, Computation time: 2.5040781497955322\n",
      "Step: 341, Loss: 0.9330291152000427, Accuracy: 0.96875, Computation time: 2.1435787677764893\n",
      "Step: 342, Loss: 0.9161516427993774, Accuracy: 1.0, Computation time: 2.1305925846099854\n",
      "Step: 343, Loss: 0.9383100271224976, Accuracy: 0.96875, Computation time: 4.109261512756348\n",
      "Step: 344, Loss: 0.9182150363922119, Accuracy: 1.0, Computation time: 2.163878917694092\n",
      "Step: 345, Loss: 0.9592794179916382, Accuracy: 0.9375, Computation time: 1.9196746349334717\n",
      "Step: 346, Loss: 0.938440203666687, Accuracy: 0.96875, Computation time: 2.8745808601379395\n",
      "Step: 347, Loss: 0.9162125587463379, Accuracy: 1.0, Computation time: 2.5202155113220215\n",
      "Step: 348, Loss: 0.9170548319816589, Accuracy: 1.0, Computation time: 2.9983246326446533\n",
      "Step: 349, Loss: 0.9159597754478455, Accuracy: 1.0, Computation time: 2.160037040710449\n",
      "Step: 350, Loss: 0.9161500334739685, Accuracy: 1.0, Computation time: 1.9354782104492188\n",
      "Step: 351, Loss: 0.9160833358764648, Accuracy: 1.0, Computation time: 2.2137839794158936\n",
      "Step: 352, Loss: 0.9161108136177063, Accuracy: 1.0, Computation time: 1.9558236598968506\n",
      "Step: 353, Loss: 0.9160996675491333, Accuracy: 1.0, Computation time: 1.898083209991455\n",
      "Step: 354, Loss: 0.9378225207328796, Accuracy: 0.96875, Computation time: 1.8844614028930664\n",
      "Step: 355, Loss: 0.9443694353103638, Accuracy: 0.9375, Computation time: 2.2322797775268555\n",
      "Step: 356, Loss: 0.9161133766174316, Accuracy: 1.0, Computation time: 1.9316301345825195\n",
      "Step: 357, Loss: 0.9330360293388367, Accuracy: 0.96875, Computation time: 2.137573003768921\n",
      "Step: 358, Loss: 0.9164061546325684, Accuracy: 1.0, Computation time: 2.0983498096466064\n",
      "Step: 359, Loss: 0.9258080720901489, Accuracy: 1.0, Computation time: 2.077125310897827\n",
      "Step: 360, Loss: 0.9383008480072021, Accuracy: 0.96875, Computation time: 1.7578186988830566\n",
      "Step: 361, Loss: 0.9189064502716064, Accuracy: 1.0, Computation time: 2.698416233062744\n",
      "Step: 362, Loss: 0.9177006483078003, Accuracy: 1.0, Computation time: 3.0121147632598877\n",
      "Step: 363, Loss: 0.9177131652832031, Accuracy: 1.0, Computation time: 1.9559030532836914\n",
      "Step: 364, Loss: 0.9477871060371399, Accuracy: 0.9375, Computation time: 2.0796091556549072\n",
      "Step: 365, Loss: 0.9160817861557007, Accuracy: 1.0, Computation time: 2.374514579772949\n",
      "Step: 366, Loss: 0.9161607623100281, Accuracy: 1.0, Computation time: 2.277688503265381\n",
      "Step: 367, Loss: 0.9166625142097473, Accuracy: 1.0, Computation time: 2.516319990158081\n",
      "Step: 368, Loss: 0.9161879420280457, Accuracy: 1.0, Computation time: 2.2730352878570557\n",
      "Step: 369, Loss: 0.9239418506622314, Accuracy: 1.0, Computation time: 2.511446714401245\n",
      "Step: 370, Loss: 0.9165238738059998, Accuracy: 1.0, Computation time: 2.2444448471069336\n",
      "Step: 371, Loss: 0.9164723753929138, Accuracy: 1.0, Computation time: 2.1931638717651367\n",
      "Step: 372, Loss: 0.9165366888046265, Accuracy: 1.0, Computation time: 2.333284616470337\n",
      "Step: 373, Loss: 0.9185460805892944, Accuracy: 1.0, Computation time: 2.7601535320281982\n",
      "Step: 374, Loss: 0.9371439218521118, Accuracy: 0.96875, Computation time: 2.0835278034210205\n",
      "Step: 375, Loss: 0.9166450500488281, Accuracy: 1.0, Computation time: 1.9229092597961426\n",
      "Step: 376, Loss: 0.916231095790863, Accuracy: 1.0, Computation time: 2.180957555770874\n",
      "Step: 377, Loss: 0.9165331721305847, Accuracy: 1.0, Computation time: 2.1368401050567627\n",
      "Step: 378, Loss: 0.9165733456611633, Accuracy: 1.0, Computation time: 2.215895652770996\n",
      "Step: 379, Loss: 0.9165228009223938, Accuracy: 1.0, Computation time: 2.996138334274292\n",
      "Step: 380, Loss: 0.9162335991859436, Accuracy: 1.0, Computation time: 2.2357969284057617\n",
      "Step: 381, Loss: 0.9339114427566528, Accuracy: 0.96875, Computation time: 2.127453327178955\n",
      "Step: 382, Loss: 0.9161223769187927, Accuracy: 1.0, Computation time: 2.0272409915924072\n",
      "Step: 383, Loss: 0.9162503480911255, Accuracy: 1.0, Computation time: 1.8370614051818848\n",
      "Step: 384, Loss: 0.9340059161186218, Accuracy: 0.96875, Computation time: 2.0177853107452393\n",
      "Step: 385, Loss: 0.9173101186752319, Accuracy: 1.0, Computation time: 1.8936681747436523\n",
      "Step: 386, Loss: 0.917048454284668, Accuracy: 1.0, Computation time: 2.1093082427978516\n",
      "Step: 387, Loss: 0.9205441474914551, Accuracy: 1.0, Computation time: 3.4592232704162598\n",
      "Step: 388, Loss: 0.9169526696205139, Accuracy: 1.0, Computation time: 2.4766104221343994\n",
      "Step: 389, Loss: 0.9381791353225708, Accuracy: 0.96875, Computation time: 2.3158457279205322\n",
      "Step: 390, Loss: 0.9161845445632935, Accuracy: 1.0, Computation time: 2.0311472415924072\n",
      "Step: 391, Loss: 0.9167384505271912, Accuracy: 1.0, Computation time: 2.4364190101623535\n",
      "Step: 392, Loss: 0.9334464073181152, Accuracy: 0.96875, Computation time: 2.0968477725982666\n",
      "Step: 393, Loss: 0.9169822931289673, Accuracy: 1.0, Computation time: 2.150627374649048\n",
      "Step: 394, Loss: 0.9485414028167725, Accuracy: 0.9375, Computation time: 2.971914529800415\n",
      "Step: 395, Loss: 0.9166607856750488, Accuracy: 1.0, Computation time: 2.2947075366973877\n",
      "Step: 396, Loss: 0.9379240870475769, Accuracy: 0.96875, Computation time: 2.9480926990509033\n",
      "Step: 397, Loss: 0.9162110686302185, Accuracy: 1.0, Computation time: 2.132650136947632\n",
      "Step: 398, Loss: 0.927617609500885, Accuracy: 0.96875, Computation time: 3.1288695335388184\n",
      "Step: 399, Loss: 0.9281015992164612, Accuracy: 0.96875, Computation time: 2.2962543964385986\n",
      "Step: 400, Loss: 0.9169533848762512, Accuracy: 1.0, Computation time: 2.375274181365967\n",
      "Step: 401, Loss: 0.9244081377983093, Accuracy: 1.0, Computation time: 2.550023317337036\n",
      "Step: 402, Loss: 0.9354870915412903, Accuracy: 0.96875, Computation time: 2.65280818939209\n",
      "Step: 403, Loss: 0.9319905042648315, Accuracy: 0.96875, Computation time: 2.428307294845581\n",
      "Step: 404, Loss: 0.9408976435661316, Accuracy: 0.96875, Computation time: 2.1434459686279297\n",
      "Step: 405, Loss: 0.9300802946090698, Accuracy: 0.96875, Computation time: 2.5490012168884277\n",
      "Step: 406, Loss: 0.9161681532859802, Accuracy: 1.0, Computation time: 2.0889155864715576\n",
      "Step: 407, Loss: 0.9383689165115356, Accuracy: 0.96875, Computation time: 2.0783157348632812\n",
      "Step: 408, Loss: 0.9340405464172363, Accuracy: 0.96875, Computation time: 3.351257085800171\n",
      "Step: 409, Loss: 0.9162507653236389, Accuracy: 1.0, Computation time: 2.160527467727661\n",
      "Step: 410, Loss: 0.931263267993927, Accuracy: 0.96875, Computation time: 2.3805060386657715\n",
      "Step: 411, Loss: 0.9207023978233337, Accuracy: 1.0, Computation time: 2.564396381378174\n",
      "Step: 412, Loss: 0.9165320992469788, Accuracy: 1.0, Computation time: 1.9996232986450195\n",
      "Step: 413, Loss: 0.918955385684967, Accuracy: 1.0, Computation time: 2.5815725326538086\n",
      "Step: 414, Loss: 0.916442334651947, Accuracy: 1.0, Computation time: 2.1552934646606445\n",
      "Step: 415, Loss: 0.9163961410522461, Accuracy: 1.0, Computation time: 2.0389575958251953\n",
      "Step: 416, Loss: 0.917858898639679, Accuracy: 1.0, Computation time: 2.016327381134033\n",
      "Step: 417, Loss: 0.9163256287574768, Accuracy: 1.0, Computation time: 2.152869701385498\n",
      "########################\n",
      "Test loss: 1.115471601486206, Test Accuracy_epoch3: 0.706342339515686\n",
      "########################\n",
      "Step: 418, Loss: 0.9610899686813354, Accuracy: 0.9375, Computation time: 2.0244970321655273\n",
      "Step: 419, Loss: 0.9162366390228271, Accuracy: 1.0, Computation time: 2.0201687812805176\n",
      "Step: 420, Loss: 0.915954053401947, Accuracy: 1.0, Computation time: 2.334608316421509\n",
      "Step: 421, Loss: 0.9378030300140381, Accuracy: 0.96875, Computation time: 2.4266598224639893\n",
      "Step: 422, Loss: 0.9165557026863098, Accuracy: 1.0, Computation time: 2.2511513233184814\n",
      "Step: 423, Loss: 0.9262815117835999, Accuracy: 0.96875, Computation time: 2.136882781982422\n",
      "Step: 424, Loss: 0.9161731600761414, Accuracy: 1.0, Computation time: 2.4617676734924316\n",
      "Step: 425, Loss: 0.9161701202392578, Accuracy: 1.0, Computation time: 2.762627124786377\n",
      "Step: 426, Loss: 0.9163383841514587, Accuracy: 1.0, Computation time: 2.5921905040740967\n",
      "Step: 427, Loss: 0.9161581993103027, Accuracy: 1.0, Computation time: 1.6766295433044434\n",
      "Step: 428, Loss: 0.9247632622718811, Accuracy: 1.0, Computation time: 2.724942207336426\n",
      "Step: 429, Loss: 0.9433737397193909, Accuracy: 0.96875, Computation time: 1.9488434791564941\n",
      "Step: 430, Loss: 0.9171408414840698, Accuracy: 1.0, Computation time: 2.22072172164917\n",
      "Step: 431, Loss: 0.9170897006988525, Accuracy: 1.0, Computation time: 2.839796543121338\n",
      "Step: 432, Loss: 0.9236271977424622, Accuracy: 1.0, Computation time: 2.5399436950683594\n",
      "Step: 433, Loss: 0.916074275970459, Accuracy: 1.0, Computation time: 3.041822910308838\n",
      "Step: 434, Loss: 0.9207710027694702, Accuracy: 1.0, Computation time: 2.818413257598877\n",
      "Step: 435, Loss: 0.9599340558052063, Accuracy: 0.9375, Computation time: 2.3216285705566406\n",
      "Step: 436, Loss: 0.9164841175079346, Accuracy: 1.0, Computation time: 2.251988649368286\n",
      "Step: 437, Loss: 0.9190416932106018, Accuracy: 1.0, Computation time: 2.2814810276031494\n",
      "Step: 438, Loss: 0.9392842054367065, Accuracy: 0.96875, Computation time: 2.7163050174713135\n",
      "Step: 439, Loss: 0.9164980053901672, Accuracy: 1.0, Computation time: 2.2870137691497803\n",
      "Step: 440, Loss: 0.9163002967834473, Accuracy: 1.0, Computation time: 2.645003318786621\n",
      "Step: 441, Loss: 0.9181845784187317, Accuracy: 1.0, Computation time: 2.87935733795166\n",
      "Step: 442, Loss: 0.916115403175354, Accuracy: 1.0, Computation time: 1.9693291187286377\n",
      "Step: 443, Loss: 0.9394749999046326, Accuracy: 0.96875, Computation time: 3.2097017765045166\n",
      "Step: 444, Loss: 0.9161763191223145, Accuracy: 1.0, Computation time: 2.4384284019470215\n",
      "Step: 445, Loss: 0.9161190390586853, Accuracy: 1.0, Computation time: 2.190359115600586\n",
      "Step: 446, Loss: 0.9187394380569458, Accuracy: 1.0, Computation time: 2.271026134490967\n",
      "Step: 447, Loss: 0.9346543550491333, Accuracy: 0.96875, Computation time: 2.733947992324829\n",
      "Step: 448, Loss: 0.9347764253616333, Accuracy: 0.96875, Computation time: 1.9540724754333496\n",
      "Step: 449, Loss: 0.9162644743919373, Accuracy: 1.0, Computation time: 2.0464277267456055\n",
      "Step: 450, Loss: 0.918009877204895, Accuracy: 1.0, Computation time: 2.0054736137390137\n",
      "Step: 451, Loss: 0.923026978969574, Accuracy: 1.0, Computation time: 2.2371160984039307\n",
      "Step: 452, Loss: 0.9381301999092102, Accuracy: 0.96875, Computation time: 2.4538533687591553\n",
      "Step: 453, Loss: 0.9164937734603882, Accuracy: 1.0, Computation time: 2.1011695861816406\n",
      "Step: 454, Loss: 0.9241151213645935, Accuracy: 1.0, Computation time: 2.5306544303894043\n",
      "Step: 455, Loss: 0.9160083532333374, Accuracy: 1.0, Computation time: 1.9055147171020508\n",
      "Step: 456, Loss: 0.9159642457962036, Accuracy: 1.0, Computation time: 1.9761011600494385\n",
      "Step: 457, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 2.3195271492004395\n",
      "Step: 458, Loss: 0.9161527752876282, Accuracy: 1.0, Computation time: 2.2234013080596924\n",
      "Step: 459, Loss: 0.9162807464599609, Accuracy: 1.0, Computation time: 2.0718719959259033\n",
      "Step: 460, Loss: 0.9382269382476807, Accuracy: 0.96875, Computation time: 2.300912857055664\n",
      "Step: 461, Loss: 0.9160218834877014, Accuracy: 1.0, Computation time: 1.8671901226043701\n",
      "Step: 462, Loss: 0.9161729216575623, Accuracy: 1.0, Computation time: 1.9650580883026123\n",
      "Step: 463, Loss: 0.9160308837890625, Accuracy: 1.0, Computation time: 2.722412109375\n",
      "Step: 464, Loss: 0.9610586762428284, Accuracy: 0.9375, Computation time: 2.4427919387817383\n",
      "Step: 465, Loss: 0.9522444009780884, Accuracy: 0.9375, Computation time: 2.4873135089874268\n",
      "Step: 466, Loss: 0.9218423366546631, Accuracy: 1.0, Computation time: 3.0100345611572266\n",
      "Step: 467, Loss: 0.9161792993545532, Accuracy: 1.0, Computation time: 2.2961292266845703\n",
      "Step: 468, Loss: 0.923764705657959, Accuracy: 1.0, Computation time: 2.2779271602630615\n",
      "Step: 469, Loss: 0.9179138541221619, Accuracy: 1.0, Computation time: 2.3017420768737793\n",
      "Step: 470, Loss: 0.9167117476463318, Accuracy: 1.0, Computation time: 3.400278329849243\n",
      "Step: 471, Loss: 0.9164524078369141, Accuracy: 1.0, Computation time: 2.5432522296905518\n",
      "Step: 472, Loss: 0.9175789952278137, Accuracy: 1.0, Computation time: 2.201735496520996\n",
      "Step: 473, Loss: 0.9165993332862854, Accuracy: 1.0, Computation time: 2.259671926498413\n",
      "Step: 474, Loss: 0.917761504650116, Accuracy: 1.0, Computation time: 2.665728807449341\n",
      "Step: 475, Loss: 0.916344940662384, Accuracy: 1.0, Computation time: 2.220715045928955\n",
      "Step: 476, Loss: 0.9159366488456726, Accuracy: 1.0, Computation time: 2.320516347885132\n",
      "Step: 477, Loss: 0.9161336421966553, Accuracy: 1.0, Computation time: 2.1878936290740967\n",
      "Step: 478, Loss: 0.9314315319061279, Accuracy: 0.96875, Computation time: 2.214916944503784\n",
      "Step: 479, Loss: 0.9162289500236511, Accuracy: 1.0, Computation time: 2.865443229675293\n",
      "Step: 480, Loss: 0.9161339998245239, Accuracy: 1.0, Computation time: 2.4729514122009277\n",
      "Step: 481, Loss: 0.9371635317802429, Accuracy: 0.96875, Computation time: 2.2483739852905273\n",
      "Step: 482, Loss: 0.9160369634628296, Accuracy: 1.0, Computation time: 2.0151216983795166\n",
      "Step: 483, Loss: 0.92991703748703, Accuracy: 0.96875, Computation time: 3.25557017326355\n",
      "Step: 484, Loss: 0.9161343574523926, Accuracy: 1.0, Computation time: 1.9032814502716064\n",
      "Step: 485, Loss: 0.9378420114517212, Accuracy: 0.96875, Computation time: 1.8307130336761475\n",
      "Step: 486, Loss: 0.9161480665206909, Accuracy: 1.0, Computation time: 1.9897210597991943\n",
      "Step: 487, Loss: 0.9311883449554443, Accuracy: 0.96875, Computation time: 2.123037576675415\n",
      "Step: 488, Loss: 0.9162882566452026, Accuracy: 1.0, Computation time: 1.8171584606170654\n",
      "Step: 489, Loss: 0.9379717707633972, Accuracy: 0.96875, Computation time: 3.203159809112549\n",
      "Step: 490, Loss: 0.9181324243545532, Accuracy: 1.0, Computation time: 3.0913963317871094\n",
      "Step: 491, Loss: 0.9421089887619019, Accuracy: 0.9375, Computation time: 2.2686827182769775\n",
      "Step: 492, Loss: 0.9160373210906982, Accuracy: 1.0, Computation time: 2.398615837097168\n",
      "Step: 493, Loss: 0.9160062074661255, Accuracy: 1.0, Computation time: 1.7839064598083496\n",
      "Step: 494, Loss: 0.9165180921554565, Accuracy: 1.0, Computation time: 2.841937780380249\n",
      "Step: 495, Loss: 0.9165345430374146, Accuracy: 1.0, Computation time: 2.0746676921844482\n",
      "Step: 496, Loss: 0.9256405830383301, Accuracy: 0.96875, Computation time: 2.0578560829162598\n",
      "Step: 497, Loss: 0.9164882898330688, Accuracy: 1.0, Computation time: 2.619727373123169\n",
      "Step: 498, Loss: 0.9160021543502808, Accuracy: 1.0, Computation time: 2.132858991622925\n",
      "Step: 499, Loss: 0.9164883494377136, Accuracy: 1.0, Computation time: 2.4825196266174316\n",
      "Step: 500, Loss: 0.9161608219146729, Accuracy: 1.0, Computation time: 2.478816032409668\n",
      "Step: 501, Loss: 0.9385243058204651, Accuracy: 0.96875, Computation time: 2.69606614112854\n",
      "Step: 502, Loss: 0.936820387840271, Accuracy: 0.96875, Computation time: 2.6509816646575928\n",
      "Step: 503, Loss: 0.9166409969329834, Accuracy: 1.0, Computation time: 2.1167726516723633\n",
      "Step: 504, Loss: 0.9168385863304138, Accuracy: 1.0, Computation time: 2.0345587730407715\n",
      "Step: 505, Loss: 0.9764499068260193, Accuracy: 0.90625, Computation time: 2.542128086090088\n",
      "Step: 506, Loss: 0.9160234332084656, Accuracy: 1.0, Computation time: 1.7818517684936523\n",
      "Step: 507, Loss: 0.9161186218261719, Accuracy: 1.0, Computation time: 2.0840048789978027\n",
      "Step: 508, Loss: 0.9160269498825073, Accuracy: 1.0, Computation time: 1.913111925125122\n",
      "Step: 509, Loss: 0.9416641592979431, Accuracy: 0.96875, Computation time: 2.931530237197876\n",
      "Step: 510, Loss: 0.9282084703445435, Accuracy: 0.96875, Computation time: 2.463491678237915\n",
      "Step: 511, Loss: 0.9203618764877319, Accuracy: 1.0, Computation time: 1.8439967632293701\n",
      "Step: 512, Loss: 0.9159867167472839, Accuracy: 1.0, Computation time: 2.0283963680267334\n",
      "Step: 513, Loss: 0.9346740245819092, Accuracy: 0.96875, Computation time: 2.1786930561065674\n",
      "Step: 514, Loss: 0.9163786768913269, Accuracy: 1.0, Computation time: 2.1448776721954346\n",
      "Step: 515, Loss: 0.916206419467926, Accuracy: 1.0, Computation time: 2.734954595565796\n",
      "Step: 516, Loss: 0.916308581829071, Accuracy: 1.0, Computation time: 2.1645500659942627\n",
      "Step: 517, Loss: 0.9162183403968811, Accuracy: 1.0, Computation time: 2.269270896911621\n",
      "Step: 518, Loss: 0.9163933992385864, Accuracy: 1.0, Computation time: 2.1418678760528564\n",
      "Step: 519, Loss: 0.9163040518760681, Accuracy: 1.0, Computation time: 2.379450559616089\n",
      "Step: 520, Loss: 0.9475787878036499, Accuracy: 0.9375, Computation time: 2.631042718887329\n",
      "Step: 521, Loss: 0.9168853163719177, Accuracy: 1.0, Computation time: 2.482123374938965\n",
      "Step: 522, Loss: 0.9366677403450012, Accuracy: 0.96875, Computation time: 2.4771571159362793\n",
      "Step: 523, Loss: 0.9160226583480835, Accuracy: 1.0, Computation time: 1.9562616348266602\n",
      "Step: 524, Loss: 0.9160246849060059, Accuracy: 1.0, Computation time: 2.0639185905456543\n",
      "Step: 525, Loss: 0.9160789847373962, Accuracy: 1.0, Computation time: 1.906367301940918\n",
      "Step: 526, Loss: 0.9159762263298035, Accuracy: 1.0, Computation time: 2.4946141242980957\n",
      "Step: 527, Loss: 0.9376789331436157, Accuracy: 0.96875, Computation time: 1.9801948070526123\n",
      "Step: 528, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 2.609337568283081\n",
      "Step: 529, Loss: 0.925650417804718, Accuracy: 0.96875, Computation time: 1.9259674549102783\n",
      "Step: 530, Loss: 0.9163315296173096, Accuracy: 1.0, Computation time: 2.4412481784820557\n",
      "Step: 531, Loss: 0.9159968495368958, Accuracy: 1.0, Computation time: 1.8932750225067139\n",
      "Step: 532, Loss: 0.9159846305847168, Accuracy: 1.0, Computation time: 1.9568030834197998\n",
      "Step: 533, Loss: 0.91603022813797, Accuracy: 1.0, Computation time: 1.7540299892425537\n",
      "Step: 534, Loss: 0.9220236539840698, Accuracy: 1.0, Computation time: 2.0839388370513916\n",
      "Step: 535, Loss: 0.9162871837615967, Accuracy: 1.0, Computation time: 1.9740281105041504\n",
      "Step: 536, Loss: 0.9161891341209412, Accuracy: 1.0, Computation time: 2.6417176723480225\n",
      "Step: 537, Loss: 0.9366863369941711, Accuracy: 0.96875, Computation time: 2.4070441722869873\n",
      "Step: 538, Loss: 0.9221388697624207, Accuracy: 1.0, Computation time: 2.736741542816162\n",
      "Step: 539, Loss: 0.9161716103553772, Accuracy: 1.0, Computation time: 1.9803290367126465\n",
      "Step: 540, Loss: 0.9172355532646179, Accuracy: 1.0, Computation time: 2.4448025226593018\n",
      "Step: 541, Loss: 0.9171648025512695, Accuracy: 1.0, Computation time: 2.3416659832000732\n",
      "Step: 542, Loss: 0.9231697916984558, Accuracy: 1.0, Computation time: 2.449596643447876\n",
      "Step: 543, Loss: 0.9167294502258301, Accuracy: 1.0, Computation time: 2.0973331928253174\n",
      "Step: 544, Loss: 0.9162753820419312, Accuracy: 1.0, Computation time: 2.5059032440185547\n",
      "Step: 545, Loss: 0.9167133569717407, Accuracy: 1.0, Computation time: 2.042241334915161\n",
      "Step: 546, Loss: 0.9162638783454895, Accuracy: 1.0, Computation time: 2.0877208709716797\n",
      "Step: 547, Loss: 0.916513204574585, Accuracy: 1.0, Computation time: 2.3755130767822266\n",
      "Step: 548, Loss: 0.9164024591445923, Accuracy: 1.0, Computation time: 2.2580301761627197\n",
      "Step: 549, Loss: 0.9160184860229492, Accuracy: 1.0, Computation time: 1.9666166305541992\n",
      "Step: 550, Loss: 0.9375842809677124, Accuracy: 0.96875, Computation time: 2.211108922958374\n",
      "Step: 551, Loss: 0.9162629246711731, Accuracy: 1.0, Computation time: 2.333329439163208\n",
      "Step: 552, Loss: 0.9342222809791565, Accuracy: 0.96875, Computation time: 2.3329885005950928\n",
      "Step: 553, Loss: 0.9160119295120239, Accuracy: 1.0, Computation time: 2.267242670059204\n",
      "Step: 554, Loss: 0.973892331123352, Accuracy: 0.90625, Computation time: 2.2190749645233154\n",
      "Step: 555, Loss: 0.9170588254928589, Accuracy: 1.0, Computation time: 2.529566526412964\n",
      "Step: 556, Loss: 0.9318753480911255, Accuracy: 0.96875, Computation time: 2.2420458793640137\n",
      "########################\n",
      "Test loss: 1.1110953092575073, Test Accuracy_epoch4: 0.7132928371429443\n",
      "########################\n",
      "Step: 557, Loss: 0.9161349534988403, Accuracy: 1.0, Computation time: 2.0487098693847656\n",
      "Step: 558, Loss: 0.9300611615180969, Accuracy: 0.96875, Computation time: 2.9141690731048584\n",
      "Step: 559, Loss: 0.9165412783622742, Accuracy: 1.0, Computation time: 2.2583324909210205\n",
      "Step: 560, Loss: 0.917143702507019, Accuracy: 1.0, Computation time: 2.006958246231079\n",
      "Step: 561, Loss: 0.9170911312103271, Accuracy: 1.0, Computation time: 2.1327836513519287\n",
      "Step: 562, Loss: 0.9377672672271729, Accuracy: 0.96875, Computation time: 2.716797113418579\n",
      "Step: 563, Loss: 0.929611325263977, Accuracy: 0.96875, Computation time: 2.058997392654419\n",
      "Step: 564, Loss: 0.9166448712348938, Accuracy: 1.0, Computation time: 2.031264305114746\n",
      "Step: 565, Loss: 0.9161064624786377, Accuracy: 1.0, Computation time: 1.9763739109039307\n",
      "Step: 566, Loss: 0.9164177775382996, Accuracy: 1.0, Computation time: 2.0913937091827393\n",
      "Step: 567, Loss: 0.9383834004402161, Accuracy: 0.96875, Computation time: 3.2748382091522217\n",
      "Step: 568, Loss: 0.9167094230651855, Accuracy: 1.0, Computation time: 2.3444223403930664\n",
      "Step: 569, Loss: 0.9170206785202026, Accuracy: 1.0, Computation time: 2.9451887607574463\n",
      "Step: 570, Loss: 0.9342050552368164, Accuracy: 0.96875, Computation time: 2.094597578048706\n",
      "Step: 571, Loss: 0.9161383509635925, Accuracy: 1.0, Computation time: 1.9011433124542236\n",
      "Step: 572, Loss: 0.9166156649589539, Accuracy: 1.0, Computation time: 3.239614486694336\n",
      "Step: 573, Loss: 0.9196295738220215, Accuracy: 1.0, Computation time: 2.1259965896606445\n",
      "Step: 574, Loss: 0.9160071015357971, Accuracy: 1.0, Computation time: 1.7633719444274902\n",
      "Step: 575, Loss: 0.9160595536231995, Accuracy: 1.0, Computation time: 2.451343536376953\n",
      "Step: 576, Loss: 0.9159368276596069, Accuracy: 1.0, Computation time: 2.0706562995910645\n",
      "Step: 577, Loss: 0.9220232963562012, Accuracy: 1.0, Computation time: 3.0717904567718506\n",
      "Step: 578, Loss: 0.9165410399436951, Accuracy: 1.0, Computation time: 2.575843095779419\n",
      "Step: 579, Loss: 0.9368687272071838, Accuracy: 0.96875, Computation time: 1.8918395042419434\n",
      "Step: 580, Loss: 0.9161243438720703, Accuracy: 1.0, Computation time: 1.968273401260376\n",
      "Step: 581, Loss: 0.9382015466690063, Accuracy: 0.96875, Computation time: 2.131134033203125\n",
      "Step: 582, Loss: 0.9160399436950684, Accuracy: 1.0, Computation time: 1.7645881175994873\n",
      "Step: 583, Loss: 0.9360852837562561, Accuracy: 0.96875, Computation time: 2.733323574066162\n",
      "Step: 584, Loss: 0.9160748720169067, Accuracy: 1.0, Computation time: 1.6711235046386719\n",
      "Step: 585, Loss: 0.9160224795341492, Accuracy: 1.0, Computation time: 2.196779489517212\n",
      "Step: 586, Loss: 0.9174798727035522, Accuracy: 1.0, Computation time: 2.2625885009765625\n",
      "Step: 587, Loss: 0.9377241134643555, Accuracy: 0.96875, Computation time: 2.5841331481933594\n",
      "Step: 588, Loss: 0.916144073009491, Accuracy: 1.0, Computation time: 2.4121296405792236\n",
      "Step: 589, Loss: 0.9377754926681519, Accuracy: 0.96875, Computation time: 1.7488715648651123\n",
      "Step: 590, Loss: 0.9160259366035461, Accuracy: 1.0, Computation time: 2.1575841903686523\n",
      "Step: 591, Loss: 0.934322714805603, Accuracy: 0.96875, Computation time: 2.1401305198669434\n",
      "Step: 592, Loss: 0.916215181350708, Accuracy: 1.0, Computation time: 1.9035861492156982\n",
      "Step: 593, Loss: 0.9160197973251343, Accuracy: 1.0, Computation time: 2.141537666320801\n",
      "Step: 594, Loss: 0.9166164398193359, Accuracy: 1.0, Computation time: 2.4952621459960938\n",
      "Step: 595, Loss: 0.916106104850769, Accuracy: 1.0, Computation time: 2.1011605262756348\n",
      "Step: 596, Loss: 0.9377398490905762, Accuracy: 0.96875, Computation time: 2.250857353210449\n",
      "Step: 597, Loss: 0.9161245822906494, Accuracy: 1.0, Computation time: 2.150987386703491\n",
      "Step: 598, Loss: 0.9373465776443481, Accuracy: 0.96875, Computation time: 1.950197458267212\n",
      "Step: 599, Loss: 0.9160163402557373, Accuracy: 1.0, Computation time: 2.2716100215911865\n",
      "Step: 600, Loss: 0.931714653968811, Accuracy: 0.96875, Computation time: 1.8965048789978027\n",
      "Step: 601, Loss: 0.9160926938056946, Accuracy: 1.0, Computation time: 2.4999537467956543\n",
      "Step: 602, Loss: 0.9377507567405701, Accuracy: 0.96875, Computation time: 2.5180447101593018\n",
      "Step: 603, Loss: 0.9375225901603699, Accuracy: 0.96875, Computation time: 1.768171787261963\n",
      "Step: 604, Loss: 0.9159447550773621, Accuracy: 1.0, Computation time: 1.9652612209320068\n",
      "Step: 605, Loss: 0.9164972305297852, Accuracy: 1.0, Computation time: 2.6104984283447266\n",
      "Step: 606, Loss: 0.9167423248291016, Accuracy: 1.0, Computation time: 2.542367696762085\n",
      "Step: 607, Loss: 0.9377546310424805, Accuracy: 0.96875, Computation time: 2.3468124866485596\n",
      "Step: 608, Loss: 0.9160658717155457, Accuracy: 1.0, Computation time: 1.792367935180664\n",
      "Step: 609, Loss: 0.9162459373474121, Accuracy: 1.0, Computation time: 2.138758659362793\n",
      "Step: 610, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.9805421829223633\n",
      "Step: 611, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.709702491760254\n",
      "Step: 612, Loss: 0.9232238531112671, Accuracy: 1.0, Computation time: 2.545990228652954\n",
      "Step: 613, Loss: 0.91773521900177, Accuracy: 1.0, Computation time: 2.103672504425049\n",
      "Step: 614, Loss: 0.9431458711624146, Accuracy: 0.96875, Computation time: 2.7122251987457275\n",
      "Step: 615, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 2.0636208057403564\n",
      "Step: 616, Loss: 0.9160026907920837, Accuracy: 1.0, Computation time: 2.156299591064453\n",
      "Step: 617, Loss: 0.9312401413917542, Accuracy: 0.96875, Computation time: 2.493600845336914\n",
      "Step: 618, Loss: 0.9361811280250549, Accuracy: 0.96875, Computation time: 2.3844635486602783\n",
      "Step: 619, Loss: 0.9160388708114624, Accuracy: 1.0, Computation time: 2.2462410926818848\n",
      "Step: 620, Loss: 0.93963223695755, Accuracy: 0.96875, Computation time: 2.7167186737060547\n",
      "Step: 621, Loss: 0.9249733686447144, Accuracy: 1.0, Computation time: 2.6054975986480713\n",
      "Step: 622, Loss: 0.9379751682281494, Accuracy: 0.96875, Computation time: 2.6674013137817383\n",
      "Step: 623, Loss: 0.9164891839027405, Accuracy: 1.0, Computation time: 2.2152562141418457\n",
      "Step: 624, Loss: 0.9168819785118103, Accuracy: 1.0, Computation time: 2.814737558364868\n",
      "Step: 625, Loss: 0.9162631630897522, Accuracy: 1.0, Computation time: 2.6589460372924805\n",
      "Step: 626, Loss: 0.9379074573516846, Accuracy: 0.96875, Computation time: 2.2928507328033447\n",
      "Step: 627, Loss: 0.9172137379646301, Accuracy: 1.0, Computation time: 1.9812054634094238\n",
      "Step: 628, Loss: 0.9246295094490051, Accuracy: 1.0, Computation time: 3.1313953399658203\n",
      "Step: 629, Loss: 0.9161357283592224, Accuracy: 1.0, Computation time: 2.3896749019622803\n",
      "Step: 630, Loss: 0.9164572954177856, Accuracy: 1.0, Computation time: 2.753227949142456\n",
      "Step: 631, Loss: 0.9163017272949219, Accuracy: 1.0, Computation time: 2.24700665473938\n",
      "Step: 632, Loss: 0.9382221102714539, Accuracy: 0.96875, Computation time: 2.690910816192627\n",
      "Step: 633, Loss: 0.916045069694519, Accuracy: 1.0, Computation time: 2.2817656993865967\n",
      "Step: 634, Loss: 0.9162279963493347, Accuracy: 1.0, Computation time: 2.733705759048462\n",
      "Step: 635, Loss: 0.9163838028907776, Accuracy: 1.0, Computation time: 3.096130609512329\n",
      "Step: 636, Loss: 0.9166619777679443, Accuracy: 1.0, Computation time: 2.5357892513275146\n",
      "Step: 637, Loss: 0.9162218570709229, Accuracy: 1.0, Computation time: 2.3059117794036865\n",
      "Step: 638, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 2.0549731254577637\n",
      "Step: 639, Loss: 0.9165527820587158, Accuracy: 1.0, Computation time: 2.4119114875793457\n",
      "Step: 640, Loss: 0.9160452485084534, Accuracy: 1.0, Computation time: 2.4772567749023438\n",
      "Step: 641, Loss: 0.9204650521278381, Accuracy: 1.0, Computation time: 2.369871139526367\n",
      "Step: 642, Loss: 0.9160012006759644, Accuracy: 1.0, Computation time: 2.2431023120880127\n",
      "Step: 643, Loss: 0.9160778522491455, Accuracy: 1.0, Computation time: 2.2767696380615234\n",
      "Step: 644, Loss: 0.9160526990890503, Accuracy: 1.0, Computation time: 2.491669178009033\n",
      "Step: 645, Loss: 0.916015088558197, Accuracy: 1.0, Computation time: 2.2834880352020264\n",
      "Step: 646, Loss: 0.9375660419464111, Accuracy: 0.96875, Computation time: 2.2758235931396484\n",
      "Step: 647, Loss: 0.9373713731765747, Accuracy: 0.96875, Computation time: 2.6491832733154297\n",
      "Step: 648, Loss: 0.916056752204895, Accuracy: 1.0, Computation time: 2.373441219329834\n",
      "Step: 649, Loss: 0.9380020499229431, Accuracy: 0.96875, Computation time: 2.3400871753692627\n",
      "Step: 650, Loss: 0.9161540865898132, Accuracy: 1.0, Computation time: 2.9120447635650635\n",
      "Step: 651, Loss: 0.9217463731765747, Accuracy: 1.0, Computation time: 2.893810749053955\n",
      "Step: 652, Loss: 0.9160099625587463, Accuracy: 1.0, Computation time: 2.4600605964660645\n",
      "Step: 653, Loss: 0.9164328575134277, Accuracy: 1.0, Computation time: 2.7312941551208496\n",
      "Step: 654, Loss: 0.9200619459152222, Accuracy: 1.0, Computation time: 2.265747547149658\n",
      "Step: 655, Loss: 0.9160848259925842, Accuracy: 1.0, Computation time: 2.5146584510803223\n",
      "Step: 656, Loss: 0.9160065054893494, Accuracy: 1.0, Computation time: 2.1992979049682617\n",
      "Step: 657, Loss: 0.9159661531448364, Accuracy: 1.0, Computation time: 2.3219220638275146\n",
      "Step: 658, Loss: 0.9159709811210632, Accuracy: 1.0, Computation time: 3.3053202629089355\n",
      "Step: 659, Loss: 0.916221559047699, Accuracy: 1.0, Computation time: 2.6240434646606445\n",
      "Step: 660, Loss: 0.9304355978965759, Accuracy: 0.96875, Computation time: 2.472862482070923\n",
      "Step: 661, Loss: 0.937965452671051, Accuracy: 0.96875, Computation time: 2.25044322013855\n",
      "Step: 662, Loss: 0.9159459471702576, Accuracy: 1.0, Computation time: 2.278062343597412\n",
      "Step: 663, Loss: 0.916043758392334, Accuracy: 1.0, Computation time: 2.84635329246521\n",
      "Step: 664, Loss: 0.9182297587394714, Accuracy: 1.0, Computation time: 2.3781280517578125\n",
      "Step: 665, Loss: 0.9161631464958191, Accuracy: 1.0, Computation time: 1.9014265537261963\n",
      "Step: 666, Loss: 0.9563977122306824, Accuracy: 0.9375, Computation time: 2.3669240474700928\n",
      "Step: 667, Loss: 0.9223943948745728, Accuracy: 1.0, Computation time: 2.5080504417419434\n",
      "Step: 668, Loss: 0.9159616231918335, Accuracy: 1.0, Computation time: 2.0638861656188965\n",
      "Step: 669, Loss: 0.9160100221633911, Accuracy: 1.0, Computation time: 2.201000690460205\n",
      "Step: 670, Loss: 0.9161829352378845, Accuracy: 1.0, Computation time: 2.1135330200195312\n",
      "Step: 671, Loss: 0.9162759780883789, Accuracy: 1.0, Computation time: 2.497481346130371\n",
      "Step: 672, Loss: 0.9162664413452148, Accuracy: 1.0, Computation time: 1.9623355865478516\n",
      "Step: 673, Loss: 0.9162329435348511, Accuracy: 1.0, Computation time: 2.5324912071228027\n",
      "Step: 674, Loss: 0.9188041090965271, Accuracy: 1.0, Computation time: 2.4623260498046875\n",
      "Step: 675, Loss: 0.9160380363464355, Accuracy: 1.0, Computation time: 2.254082202911377\n",
      "Step: 676, Loss: 0.9163346290588379, Accuracy: 1.0, Computation time: 2.320103645324707\n",
      "Step: 677, Loss: 0.9159653186798096, Accuracy: 1.0, Computation time: 2.0838541984558105\n",
      "Step: 678, Loss: 0.9160149097442627, Accuracy: 1.0, Computation time: 1.9070403575897217\n",
      "Step: 679, Loss: 0.9159911870956421, Accuracy: 1.0, Computation time: 1.6891319751739502\n",
      "Step: 680, Loss: 0.9371108412742615, Accuracy: 0.96875, Computation time: 2.3980026245117188\n",
      "Step: 681, Loss: 0.9178282618522644, Accuracy: 1.0, Computation time: 3.030329704284668\n",
      "Step: 682, Loss: 0.9263303279876709, Accuracy: 0.96875, Computation time: 3.056062698364258\n",
      "Step: 683, Loss: 0.9160737991333008, Accuracy: 1.0, Computation time: 2.4974749088287354\n",
      "Step: 684, Loss: 0.916178286075592, Accuracy: 1.0, Computation time: 2.0346555709838867\n",
      "Step: 685, Loss: 0.9162939190864563, Accuracy: 1.0, Computation time: 2.626254081726074\n",
      "Step: 686, Loss: 0.9160988330841064, Accuracy: 1.0, Computation time: 2.716240167617798\n",
      "Step: 687, Loss: 0.9160722494125366, Accuracy: 1.0, Computation time: 2.1005499362945557\n",
      "Step: 688, Loss: 0.9167937636375427, Accuracy: 1.0, Computation time: 2.318159580230713\n",
      "Step: 689, Loss: 0.9218007922172546, Accuracy: 1.0, Computation time: 2.710916519165039\n",
      "Step: 690, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 2.2659552097320557\n",
      "Step: 691, Loss: 0.9196866750717163, Accuracy: 1.0, Computation time: 2.1921932697296143\n",
      "Step: 692, Loss: 0.9377234578132629, Accuracy: 0.96875, Computation time: 2.2274410724639893\n",
      "Step: 693, Loss: 0.9179675579071045, Accuracy: 1.0, Computation time: 2.455338954925537\n",
      "Step: 694, Loss: 0.9161535501480103, Accuracy: 1.0, Computation time: 2.348773717880249\n",
      "Step: 695, Loss: 0.937966525554657, Accuracy: 0.96875, Computation time: 2.0108916759490967\n",
      "########################\n",
      "Test loss: 1.1172685623168945, Test Accuracy_epoch5: 0.7054735422134399\n",
      "########################\n",
      "Step: 696, Loss: 0.9160717725753784, Accuracy: 1.0, Computation time: 2.012756109237671\n",
      "Step: 697, Loss: 0.9165500402450562, Accuracy: 1.0, Computation time: 2.521097183227539\n",
      "Step: 698, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 2.070096254348755\n",
      "Step: 699, Loss: 0.9168240427970886, Accuracy: 1.0, Computation time: 2.1395201683044434\n",
      "Step: 700, Loss: 0.9159895777702332, Accuracy: 1.0, Computation time: 1.6176297664642334\n",
      "Step: 701, Loss: 0.9159994125366211, Accuracy: 1.0, Computation time: 2.497729778289795\n",
      "Step: 702, Loss: 0.9436354041099548, Accuracy: 0.96875, Computation time: 2.5558836460113525\n",
      "Step: 703, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 2.067105770111084\n",
      "Step: 704, Loss: 0.9159759283065796, Accuracy: 1.0, Computation time: 2.1917080879211426\n",
      "Step: 705, Loss: 0.9377672076225281, Accuracy: 0.96875, Computation time: 1.801285982131958\n",
      "Step: 706, Loss: 0.9161027669906616, Accuracy: 1.0, Computation time: 1.985328197479248\n",
      "Step: 707, Loss: 0.9162214398384094, Accuracy: 1.0, Computation time: 2.440463066101074\n",
      "Step: 708, Loss: 0.9353876709938049, Accuracy: 0.96875, Computation time: 3.119206666946411\n",
      "Step: 709, Loss: 0.9161575436592102, Accuracy: 1.0, Computation time: 2.3789780139923096\n",
      "Step: 710, Loss: 0.9159847497940063, Accuracy: 1.0, Computation time: 2.283029556274414\n",
      "Step: 711, Loss: 0.9160414338111877, Accuracy: 1.0, Computation time: 2.7548563480377197\n",
      "Step: 712, Loss: 0.9584486484527588, Accuracy: 0.9375, Computation time: 2.729377508163452\n",
      "Step: 713, Loss: 0.9163647890090942, Accuracy: 1.0, Computation time: 2.6600027084350586\n",
      "Step: 714, Loss: 0.9161718487739563, Accuracy: 1.0, Computation time: 2.442500352859497\n",
      "Step: 715, Loss: 0.9161508083343506, Accuracy: 1.0, Computation time: 2.3631858825683594\n",
      "Step: 716, Loss: 0.9184898734092712, Accuracy: 1.0, Computation time: 3.153041124343872\n",
      "Step: 717, Loss: 0.9185861349105835, Accuracy: 1.0, Computation time: 2.4552645683288574\n",
      "Step: 718, Loss: 0.9167543053627014, Accuracy: 1.0, Computation time: 2.034651279449463\n",
      "Step: 719, Loss: 0.916176438331604, Accuracy: 1.0, Computation time: 2.3246076107025146\n",
      "Step: 720, Loss: 0.9163894653320312, Accuracy: 1.0, Computation time: 2.370436906814575\n",
      "Step: 721, Loss: 0.9162066578865051, Accuracy: 1.0, Computation time: 2.211813449859619\n",
      "Step: 722, Loss: 0.9167469143867493, Accuracy: 1.0, Computation time: 3.4872994422912598\n",
      "Step: 723, Loss: 0.9302634596824646, Accuracy: 0.96875, Computation time: 3.02902889251709\n",
      "Step: 724, Loss: 0.916324257850647, Accuracy: 1.0, Computation time: 2.225093364715576\n",
      "Step: 725, Loss: 0.9760855436325073, Accuracy: 0.90625, Computation time: 2.1494908332824707\n",
      "Step: 726, Loss: 0.916083037853241, Accuracy: 1.0, Computation time: 1.7879140377044678\n",
      "Step: 727, Loss: 0.9160251617431641, Accuracy: 1.0, Computation time: 2.3662407398223877\n",
      "Step: 728, Loss: 0.9455966353416443, Accuracy: 0.96875, Computation time: 2.2270348072052\n",
      "Step: 729, Loss: 0.9166483283042908, Accuracy: 1.0, Computation time: 2.6287295818328857\n",
      "Step: 730, Loss: 0.9508577585220337, Accuracy: 0.9375, Computation time: 2.092259645462036\n",
      "Step: 731, Loss: 0.9161260724067688, Accuracy: 1.0, Computation time: 2.421776294708252\n",
      "Step: 732, Loss: 0.9162243008613586, Accuracy: 1.0, Computation time: 2.3043105602264404\n",
      "Step: 733, Loss: 0.9162769913673401, Accuracy: 1.0, Computation time: 2.5790693759918213\n",
      "Step: 734, Loss: 0.9193946719169617, Accuracy: 1.0, Computation time: 2.0406408309936523\n",
      "Step: 735, Loss: 0.9162139296531677, Accuracy: 1.0, Computation time: 2.141014337539673\n",
      "Step: 736, Loss: 0.9350892305374146, Accuracy: 0.96875, Computation time: 3.3628973960876465\n",
      "Step: 737, Loss: 0.9326676726341248, Accuracy: 0.96875, Computation time: 2.9985060691833496\n",
      "Step: 738, Loss: 0.9160995483398438, Accuracy: 1.0, Computation time: 2.078364849090576\n",
      "Step: 739, Loss: 0.9205712676048279, Accuracy: 1.0, Computation time: 2.038374185562134\n",
      "Step: 740, Loss: 0.9370540976524353, Accuracy: 0.96875, Computation time: 2.173259973526001\n",
      "Step: 741, Loss: 0.9163253903388977, Accuracy: 1.0, Computation time: 2.3813693523406982\n",
      "Step: 742, Loss: 0.9162057638168335, Accuracy: 1.0, Computation time: 2.173322916030884\n",
      "Step: 743, Loss: 0.9333713054656982, Accuracy: 0.96875, Computation time: 1.9122569561004639\n",
      "Step: 744, Loss: 0.9162177443504333, Accuracy: 1.0, Computation time: 2.0567715167999268\n",
      "Step: 745, Loss: 0.9384656548500061, Accuracy: 0.96875, Computation time: 2.5813398361206055\n",
      "Step: 746, Loss: 0.9377299547195435, Accuracy: 0.96875, Computation time: 2.125175714492798\n",
      "Step: 747, Loss: 0.9161604046821594, Accuracy: 1.0, Computation time: 2.452606201171875\n",
      "Step: 748, Loss: 0.9166210889816284, Accuracy: 1.0, Computation time: 2.088341236114502\n",
      "Step: 749, Loss: 0.9169530272483826, Accuracy: 1.0, Computation time: 1.9258382320404053\n",
      "Step: 750, Loss: 0.9159810543060303, Accuracy: 1.0, Computation time: 2.067568778991699\n",
      "Step: 751, Loss: 0.9378028512001038, Accuracy: 0.96875, Computation time: 2.0359764099121094\n",
      "Step: 752, Loss: 0.9168239235877991, Accuracy: 1.0, Computation time: 2.5370945930480957\n",
      "Step: 753, Loss: 0.9162758588790894, Accuracy: 1.0, Computation time: 1.9113695621490479\n",
      "Step: 754, Loss: 0.9159268140792847, Accuracy: 1.0, Computation time: 2.350571632385254\n",
      "Step: 755, Loss: 0.9166080951690674, Accuracy: 1.0, Computation time: 1.9605932235717773\n",
      "Step: 756, Loss: 0.938141942024231, Accuracy: 0.96875, Computation time: 2.285977363586426\n",
      "Step: 757, Loss: 0.9162030816078186, Accuracy: 1.0, Computation time: 2.4032669067382812\n",
      "Step: 758, Loss: 0.9161092042922974, Accuracy: 1.0, Computation time: 1.8886334896087646\n",
      "Step: 759, Loss: 0.9160535335540771, Accuracy: 1.0, Computation time: 1.929680585861206\n",
      "Step: 760, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 1.9250538349151611\n",
      "Step: 761, Loss: 0.9160769581794739, Accuracy: 1.0, Computation time: 2.2828330993652344\n",
      "Step: 762, Loss: 0.9160151481628418, Accuracy: 1.0, Computation time: 2.603755235671997\n",
      "Step: 763, Loss: 0.915982186794281, Accuracy: 1.0, Computation time: 2.4347641468048096\n",
      "Step: 764, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 2.2147490978240967\n",
      "Step: 765, Loss: 0.9376625418663025, Accuracy: 0.96875, Computation time: 2.4930758476257324\n",
      "Step: 766, Loss: 0.9159387350082397, Accuracy: 1.0, Computation time: 2.4084959030151367\n",
      "Step: 767, Loss: 0.9159544706344604, Accuracy: 1.0, Computation time: 2.37384295463562\n",
      "Step: 768, Loss: 0.9159942865371704, Accuracy: 1.0, Computation time: 1.7751965522766113\n",
      "Step: 769, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 2.3040716648101807\n",
      "Step: 770, Loss: 0.9161757230758667, Accuracy: 1.0, Computation time: 2.5335378646850586\n",
      "Step: 771, Loss: 0.9195927381515503, Accuracy: 1.0, Computation time: 2.2214763164520264\n",
      "Step: 772, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 2.584502696990967\n",
      "Step: 773, Loss: 0.9159798622131348, Accuracy: 1.0, Computation time: 2.702432155609131\n",
      "Step: 774, Loss: 0.9159405827522278, Accuracy: 1.0, Computation time: 2.1501641273498535\n",
      "Step: 775, Loss: 0.9356017708778381, Accuracy: 0.96875, Computation time: 2.216855764389038\n",
      "Step: 776, Loss: 0.9162036180496216, Accuracy: 1.0, Computation time: 2.764859676361084\n",
      "Step: 777, Loss: 0.9160187840461731, Accuracy: 1.0, Computation time: 2.587491750717163\n",
      "Step: 778, Loss: 0.9161421060562134, Accuracy: 1.0, Computation time: 2.2844855785369873\n",
      "Step: 779, Loss: 0.9159954190254211, Accuracy: 1.0, Computation time: 2.2632696628570557\n",
      "Step: 780, Loss: 0.9211809635162354, Accuracy: 1.0, Computation time: 2.7045986652374268\n",
      "Step: 781, Loss: 0.9283760190010071, Accuracy: 0.96875, Computation time: 4.552796125411987\n",
      "Step: 782, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 2.1689577102661133\n",
      "Step: 783, Loss: 0.9375301003456116, Accuracy: 0.96875, Computation time: 2.7065584659576416\n",
      "Step: 784, Loss: 0.916019856929779, Accuracy: 1.0, Computation time: 2.413241147994995\n",
      "Step: 785, Loss: 0.9161251187324524, Accuracy: 1.0, Computation time: 1.9902315139770508\n",
      "Step: 786, Loss: 0.9162622094154358, Accuracy: 1.0, Computation time: 2.531719923019409\n",
      "Step: 787, Loss: 0.9363722801208496, Accuracy: 0.96875, Computation time: 2.495922327041626\n",
      "Step: 788, Loss: 0.9161728024482727, Accuracy: 1.0, Computation time: 2.2798943519592285\n",
      "Step: 789, Loss: 0.9171537160873413, Accuracy: 1.0, Computation time: 2.849003791809082\n",
      "Step: 790, Loss: 0.9161965250968933, Accuracy: 1.0, Computation time: 1.9697277545928955\n",
      "Step: 791, Loss: 0.915995717048645, Accuracy: 1.0, Computation time: 2.437014579772949\n",
      "Step: 792, Loss: 0.9377056956291199, Accuracy: 0.96875, Computation time: 2.3839662075042725\n",
      "Step: 793, Loss: 0.9196656346321106, Accuracy: 1.0, Computation time: 1.9920201301574707\n",
      "Step: 794, Loss: 0.9594147801399231, Accuracy: 0.9375, Computation time: 2.8989648818969727\n",
      "Step: 795, Loss: 0.9733830094337463, Accuracy: 0.90625, Computation time: 3.044917583465576\n",
      "Step: 796, Loss: 0.917828381061554, Accuracy: 1.0, Computation time: 2.264651298522949\n",
      "Step: 797, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 2.6193292140960693\n",
      "Step: 798, Loss: 0.9374560117721558, Accuracy: 0.96875, Computation time: 2.1538150310516357\n",
      "Step: 799, Loss: 0.9160219430923462, Accuracy: 1.0, Computation time: 2.381420612335205\n",
      "Step: 800, Loss: 0.9314379096031189, Accuracy: 0.96875, Computation time: 2.228137731552124\n",
      "Step: 801, Loss: 0.9160952568054199, Accuracy: 1.0, Computation time: 2.429544448852539\n",
      "Step: 802, Loss: 0.9161405563354492, Accuracy: 1.0, Computation time: 2.305266857147217\n",
      "Step: 803, Loss: 0.9163066744804382, Accuracy: 1.0, Computation time: 2.1127572059631348\n",
      "Step: 804, Loss: 0.9161070585250854, Accuracy: 1.0, Computation time: 2.3057448863983154\n",
      "Step: 805, Loss: 0.9168822169303894, Accuracy: 1.0, Computation time: 2.32295823097229\n",
      "Step: 806, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 2.4682576656341553\n",
      "Step: 807, Loss: 0.9390611052513123, Accuracy: 0.96875, Computation time: 2.578822135925293\n",
      "Step: 808, Loss: 0.919360876083374, Accuracy: 1.0, Computation time: 2.5254111289978027\n",
      "Step: 809, Loss: 0.9172871112823486, Accuracy: 1.0, Computation time: 2.583925247192383\n",
      "Step: 810, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 2.1455471515655518\n",
      "Step: 811, Loss: 0.9161379933357239, Accuracy: 1.0, Computation time: 2.2689669132232666\n",
      "Step: 812, Loss: 0.9163166284561157, Accuracy: 1.0, Computation time: 2.8175930976867676\n",
      "Step: 813, Loss: 0.916388988494873, Accuracy: 1.0, Computation time: 2.189624547958374\n",
      "Step: 814, Loss: 0.9265873432159424, Accuracy: 1.0, Computation time: 2.4321835041046143\n",
      "Step: 815, Loss: 0.9161902070045471, Accuracy: 1.0, Computation time: 2.0699961185455322\n",
      "Step: 816, Loss: 0.9306920766830444, Accuracy: 0.96875, Computation time: 2.330573558807373\n",
      "Step: 817, Loss: 0.9188460111618042, Accuracy: 1.0, Computation time: 2.1895453929901123\n",
      "Step: 818, Loss: 0.9166445732116699, Accuracy: 1.0, Computation time: 2.454141855239868\n",
      "Step: 819, Loss: 0.9163317084312439, Accuracy: 1.0, Computation time: 2.228684186935425\n",
      "Step: 820, Loss: 0.9161890149116516, Accuracy: 1.0, Computation time: 2.014052152633667\n",
      "Step: 821, Loss: 0.9177566170692444, Accuracy: 1.0, Computation time: 3.3111095428466797\n",
      "Step: 822, Loss: 0.9160611033439636, Accuracy: 1.0, Computation time: 1.9037399291992188\n",
      "Step: 823, Loss: 0.9160177111625671, Accuracy: 1.0, Computation time: 2.1110527515411377\n",
      "Step: 824, Loss: 0.9161182045936584, Accuracy: 1.0, Computation time: 2.112422227859497\n",
      "Step: 825, Loss: 0.9395586848258972, Accuracy: 0.96875, Computation time: 3.1169087886810303\n",
      "Step: 826, Loss: 0.9160988926887512, Accuracy: 1.0, Computation time: 2.1219098567962646\n",
      "Step: 827, Loss: 0.9160336256027222, Accuracy: 1.0, Computation time: 2.1144635677337646\n",
      "Step: 828, Loss: 0.9159582257270813, Accuracy: 1.0, Computation time: 2.1680140495300293\n",
      "Step: 829, Loss: 0.9163893461227417, Accuracy: 1.0, Computation time: 2.50555419921875\n",
      "Step: 830, Loss: 0.934635579586029, Accuracy: 0.96875, Computation time: 3.211935520172119\n",
      "Step: 831, Loss: 0.9160060286521912, Accuracy: 1.0, Computation time: 2.420989513397217\n",
      "Step: 832, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 2.808159112930298\n",
      "Step: 833, Loss: 0.9201912879943848, Accuracy: 1.0, Computation time: 2.1304891109466553\n",
      "Step: 834, Loss: 0.9374759793281555, Accuracy: 0.96875, Computation time: 2.81231427192688\n",
      "########################\n",
      "Test loss: 1.1244020462036133, Test Accuracy_epoch6: 0.6967854499816895\n",
      "########################\n",
      "Step: 835, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 2.5957040786743164\n",
      "Step: 836, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 1.8418092727661133\n",
      "Step: 837, Loss: 0.915949285030365, Accuracy: 1.0, Computation time: 2.209566831588745\n",
      "Step: 838, Loss: 0.9405832886695862, Accuracy: 0.96875, Computation time: 2.7846059799194336\n",
      "Step: 839, Loss: 0.9375942945480347, Accuracy: 0.96875, Computation time: 2.1754605770111084\n",
      "Step: 840, Loss: 0.9160282611846924, Accuracy: 1.0, Computation time: 2.7195873260498047\n",
      "Step: 841, Loss: 0.9375115633010864, Accuracy: 0.96875, Computation time: 1.9012477397918701\n",
      "Step: 842, Loss: 0.9185347557067871, Accuracy: 1.0, Computation time: 2.326087474822998\n",
      "Step: 843, Loss: 0.9361788630485535, Accuracy: 0.96875, Computation time: 3.383145332336426\n",
      "Step: 844, Loss: 0.9160680770874023, Accuracy: 1.0, Computation time: 2.325589418411255\n",
      "Step: 845, Loss: 0.9282423853874207, Accuracy: 0.96875, Computation time: 3.2802305221557617\n",
      "Step: 846, Loss: 0.9339110851287842, Accuracy: 0.96875, Computation time: 1.935093879699707\n",
      "Step: 847, Loss: 0.9161359071731567, Accuracy: 1.0, Computation time: 1.9892971515655518\n",
      "Step: 848, Loss: 0.9161568284034729, Accuracy: 1.0, Computation time: 2.1635091304779053\n",
      "Step: 849, Loss: 0.9162055253982544, Accuracy: 1.0, Computation time: 2.1788129806518555\n",
      "Step: 850, Loss: 0.9161819219589233, Accuracy: 1.0, Computation time: 2.182116985321045\n",
      "Step: 851, Loss: 0.9168659448623657, Accuracy: 1.0, Computation time: 2.427929401397705\n",
      "Step: 852, Loss: 0.9162541627883911, Accuracy: 1.0, Computation time: 2.5679538249969482\n",
      "Step: 853, Loss: 0.916208028793335, Accuracy: 1.0, Computation time: 1.976055383682251\n",
      "Step: 854, Loss: 0.9162909388542175, Accuracy: 1.0, Computation time: 2.215808629989624\n",
      "Step: 855, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 2.529946804046631\n",
      "Step: 856, Loss: 0.9159141778945923, Accuracy: 1.0, Computation time: 2.0288524627685547\n",
      "Step: 857, Loss: 0.9159610271453857, Accuracy: 1.0, Computation time: 2.6792075634002686\n",
      "Step: 858, Loss: 0.9365596771240234, Accuracy: 0.96875, Computation time: 2.5668556690216064\n",
      "Step: 859, Loss: 0.91605144739151, Accuracy: 1.0, Computation time: 2.1007423400878906\n",
      "Step: 860, Loss: 0.92854243516922, Accuracy: 0.96875, Computation time: 3.093592405319214\n",
      "Step: 861, Loss: 0.9160290360450745, Accuracy: 1.0, Computation time: 2.3882687091827393\n",
      "Step: 862, Loss: 0.9374566078186035, Accuracy: 0.96875, Computation time: 2.1785526275634766\n",
      "Step: 863, Loss: 0.9177383184432983, Accuracy: 1.0, Computation time: 2.5513572692871094\n",
      "Step: 864, Loss: 0.9369809627532959, Accuracy: 0.96875, Computation time: 2.7211666107177734\n",
      "Step: 865, Loss: 0.9201083183288574, Accuracy: 1.0, Computation time: 2.4261083602905273\n",
      "Step: 866, Loss: 0.923001766204834, Accuracy: 1.0, Computation time: 2.4786181449890137\n",
      "Step: 867, Loss: 0.9161181449890137, Accuracy: 1.0, Computation time: 2.2251880168914795\n",
      "Step: 868, Loss: 0.9160283803939819, Accuracy: 1.0, Computation time: 2.304483652114868\n",
      "Step: 869, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 2.0242419242858887\n",
      "Step: 870, Loss: 0.9160214066505432, Accuracy: 1.0, Computation time: 2.2473156452178955\n",
      "Step: 871, Loss: 0.9385652542114258, Accuracy: 0.96875, Computation time: 3.792426109313965\n",
      "Step: 872, Loss: 0.9160446524620056, Accuracy: 1.0, Computation time: 2.067253351211548\n",
      "Step: 873, Loss: 0.9384866952896118, Accuracy: 0.96875, Computation time: 2.2486236095428467\n",
      "Step: 874, Loss: 0.9161521196365356, Accuracy: 1.0, Computation time: 1.9975485801696777\n",
      "Step: 875, Loss: 0.9385722875595093, Accuracy: 0.96875, Computation time: 2.1894192695617676\n",
      "Step: 876, Loss: 0.9162335991859436, Accuracy: 1.0, Computation time: 1.9188005924224854\n",
      "Step: 877, Loss: 0.9296209812164307, Accuracy: 0.96875, Computation time: 1.9997880458831787\n",
      "Step: 878, Loss: 0.93683260679245, Accuracy: 0.96875, Computation time: 2.1985116004943848\n",
      "Step: 879, Loss: 0.9160250425338745, Accuracy: 1.0, Computation time: 1.8933510780334473\n",
      "Step: 880, Loss: 0.9287163615226746, Accuracy: 0.96875, Computation time: 2.1633291244506836\n",
      "Step: 881, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.7417399883270264\n",
      "Step: 882, Loss: 0.937695324420929, Accuracy: 0.96875, Computation time: 1.907932996749878\n",
      "Step: 883, Loss: 0.9324483871459961, Accuracy: 0.96875, Computation time: 2.5017590522766113\n",
      "Step: 884, Loss: 0.9341070055961609, Accuracy: 0.96875, Computation time: 2.4327657222747803\n",
      "Step: 885, Loss: 0.9166657328605652, Accuracy: 1.0, Computation time: 1.9114890098571777\n",
      "Step: 886, Loss: 0.9165335893630981, Accuracy: 1.0, Computation time: 1.9054608345031738\n",
      "Step: 887, Loss: 0.9562197327613831, Accuracy: 0.9375, Computation time: 2.0793519020080566\n",
      "Step: 888, Loss: 0.9339778423309326, Accuracy: 0.96875, Computation time: 2.5414843559265137\n",
      "Step: 889, Loss: 0.923146665096283, Accuracy: 1.0, Computation time: 1.976576805114746\n",
      "Step: 890, Loss: 0.9160735607147217, Accuracy: 1.0, Computation time: 2.402290105819702\n",
      "Step: 891, Loss: 0.9380203485488892, Accuracy: 0.96875, Computation time: 2.526076316833496\n",
      "Step: 892, Loss: 0.9161028265953064, Accuracy: 1.0, Computation time: 1.9691905975341797\n",
      "Step: 893, Loss: 0.9161325693130493, Accuracy: 1.0, Computation time: 1.6100831031799316\n",
      "Step: 894, Loss: 0.9182263612747192, Accuracy: 1.0, Computation time: 2.152162551879883\n",
      "Step: 895, Loss: 0.9161112308502197, Accuracy: 1.0, Computation time: 1.9619333744049072\n",
      "Step: 896, Loss: 0.9163119792938232, Accuracy: 1.0, Computation time: 2.0551223754882812\n",
      "Step: 897, Loss: 0.9160757064819336, Accuracy: 1.0, Computation time: 2.049377918243408\n",
      "Step: 898, Loss: 0.9162470102310181, Accuracy: 1.0, Computation time: 1.6403138637542725\n",
      "Step: 899, Loss: 0.9304153323173523, Accuracy: 0.96875, Computation time: 1.8980035781860352\n",
      "Step: 900, Loss: 0.9159456491470337, Accuracy: 1.0, Computation time: 1.85331130027771\n",
      "Step: 901, Loss: 0.9379859566688538, Accuracy: 0.96875, Computation time: 1.7391483783721924\n",
      "Step: 902, Loss: 0.9374428391456604, Accuracy: 0.96875, Computation time: 2.2734363079071045\n",
      "Step: 903, Loss: 0.9172583222389221, Accuracy: 1.0, Computation time: 1.8539237976074219\n",
      "Step: 904, Loss: 0.9161131381988525, Accuracy: 1.0, Computation time: 1.8909716606140137\n",
      "Step: 905, Loss: 0.9160472750663757, Accuracy: 1.0, Computation time: 2.3857333660125732\n",
      "Step: 906, Loss: 0.9378641843795776, Accuracy: 0.96875, Computation time: 1.9749302864074707\n",
      "Step: 907, Loss: 0.9309869408607483, Accuracy: 0.96875, Computation time: 2.407972812652588\n",
      "Step: 908, Loss: 0.9160980582237244, Accuracy: 1.0, Computation time: 1.985595464706421\n",
      "Step: 909, Loss: 0.9161041975021362, Accuracy: 1.0, Computation time: 2.157236337661743\n",
      "Step: 910, Loss: 0.9160448908805847, Accuracy: 1.0, Computation time: 2.1466574668884277\n",
      "Step: 911, Loss: 0.9159989356994629, Accuracy: 1.0, Computation time: 1.8840484619140625\n",
      "Step: 912, Loss: 0.9370853900909424, Accuracy: 0.96875, Computation time: 1.9556944370269775\n",
      "Step: 913, Loss: 0.9206550121307373, Accuracy: 1.0, Computation time: 1.9075639247894287\n",
      "Step: 914, Loss: 0.9543899893760681, Accuracy: 0.9375, Computation time: 2.315861701965332\n",
      "Step: 915, Loss: 0.9160634279251099, Accuracy: 1.0, Computation time: 2.0349369049072266\n",
      "Step: 916, Loss: 0.9590294361114502, Accuracy: 0.9375, Computation time: 1.8523893356323242\n",
      "Step: 917, Loss: 0.9160028100013733, Accuracy: 1.0, Computation time: 1.8236236572265625\n",
      "Step: 918, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 1.7482726573944092\n",
      "Step: 919, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.9323999881744385\n",
      "Step: 920, Loss: 0.9210850596427917, Accuracy: 1.0, Computation time: 2.03657603263855\n",
      "Step: 921, Loss: 0.9162607192993164, Accuracy: 1.0, Computation time: 1.602219581604004\n",
      "Step: 922, Loss: 0.9167964458465576, Accuracy: 1.0, Computation time: 2.055077075958252\n",
      "Step: 923, Loss: 0.9261893630027771, Accuracy: 0.96875, Computation time: 2.901845693588257\n",
      "Step: 924, Loss: 0.916214644908905, Accuracy: 1.0, Computation time: 1.8673934936523438\n",
      "Step: 925, Loss: 0.9159959554672241, Accuracy: 1.0, Computation time: 1.804548978805542\n",
      "Step: 926, Loss: 0.9159988760948181, Accuracy: 1.0, Computation time: 2.0696704387664795\n",
      "Step: 927, Loss: 0.9159769415855408, Accuracy: 1.0, Computation time: 2.441333293914795\n",
      "Step: 928, Loss: 0.9164332151412964, Accuracy: 1.0, Computation time: 2.2806332111358643\n",
      "Step: 929, Loss: 0.9163864850997925, Accuracy: 1.0, Computation time: 1.6789100170135498\n",
      "Step: 930, Loss: 0.937502384185791, Accuracy: 0.96875, Computation time: 1.6582307815551758\n",
      "Step: 931, Loss: 0.9159815907478333, Accuracy: 1.0, Computation time: 3.0421595573425293\n",
      "Step: 932, Loss: 0.9159988760948181, Accuracy: 1.0, Computation time: 1.6976616382598877\n",
      "Step: 933, Loss: 0.9163817763328552, Accuracy: 1.0, Computation time: 1.874068260192871\n",
      "Step: 934, Loss: 0.9160535335540771, Accuracy: 1.0, Computation time: 2.2005622386932373\n",
      "Step: 935, Loss: 0.9160914421081543, Accuracy: 1.0, Computation time: 2.0798180103302\n",
      "Step: 936, Loss: 0.916042685508728, Accuracy: 1.0, Computation time: 1.8409466743469238\n",
      "Step: 937, Loss: 0.9163848757743835, Accuracy: 1.0, Computation time: 1.9439728260040283\n",
      "Step: 938, Loss: 0.9162788987159729, Accuracy: 1.0, Computation time: 1.8807132244110107\n",
      "Step: 939, Loss: 0.9162118434906006, Accuracy: 1.0, Computation time: 2.2167887687683105\n",
      "Step: 940, Loss: 0.9161410927772522, Accuracy: 1.0, Computation time: 2.491842031478882\n",
      "Step: 941, Loss: 0.9198200702667236, Accuracy: 1.0, Computation time: 2.3731043338775635\n",
      "Step: 942, Loss: 0.9166128635406494, Accuracy: 1.0, Computation time: 2.071035861968994\n",
      "Step: 943, Loss: 0.9180508852005005, Accuracy: 1.0, Computation time: 2.420349597930908\n",
      "Step: 944, Loss: 0.9335567951202393, Accuracy: 0.96875, Computation time: 1.9299325942993164\n",
      "Step: 945, Loss: 0.9221764206886292, Accuracy: 1.0, Computation time: 1.9434022903442383\n",
      "Step: 946, Loss: 0.9159324765205383, Accuracy: 1.0, Computation time: 1.6122794151306152\n",
      "Step: 947, Loss: 0.9166010022163391, Accuracy: 1.0, Computation time: 2.1583659648895264\n",
      "Step: 948, Loss: 0.9376696944236755, Accuracy: 0.96875, Computation time: 2.176325798034668\n",
      "Step: 949, Loss: 0.918595016002655, Accuracy: 1.0, Computation time: 1.9502818584442139\n",
      "Step: 950, Loss: 0.9159565567970276, Accuracy: 1.0, Computation time: 1.9509706497192383\n",
      "Step: 951, Loss: 0.9159714579582214, Accuracy: 1.0, Computation time: 1.7598180770874023\n",
      "Step: 952, Loss: 0.9204717874526978, Accuracy: 1.0, Computation time: 1.7836880683898926\n",
      "Step: 953, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 1.5752179622650146\n",
      "Step: 954, Loss: 0.9160223007202148, Accuracy: 1.0, Computation time: 1.7041833400726318\n",
      "Step: 955, Loss: 0.9169430732727051, Accuracy: 1.0, Computation time: 1.6860454082489014\n",
      "Step: 956, Loss: 0.92316734790802, Accuracy: 1.0, Computation time: 1.6487338542938232\n",
      "Step: 957, Loss: 0.9179143905639648, Accuracy: 1.0, Computation time: 2.1892151832580566\n",
      "Step: 958, Loss: 0.9169723391532898, Accuracy: 1.0, Computation time: 1.7911596298217773\n",
      "Step: 959, Loss: 0.9160796403884888, Accuracy: 1.0, Computation time: 1.7372920513153076\n",
      "Step: 960, Loss: 0.9162217378616333, Accuracy: 1.0, Computation time: 1.7047231197357178\n",
      "Step: 961, Loss: 0.916313648223877, Accuracy: 1.0, Computation time: 1.9398162364959717\n",
      "Step: 962, Loss: 0.916007936000824, Accuracy: 1.0, Computation time: 1.6663947105407715\n",
      "Step: 963, Loss: 0.9371746778488159, Accuracy: 0.96875, Computation time: 1.6446046829223633\n",
      "Step: 964, Loss: 0.9159641861915588, Accuracy: 1.0, Computation time: 1.8672850131988525\n",
      "Step: 965, Loss: 0.9159717559814453, Accuracy: 1.0, Computation time: 1.6847050189971924\n",
      "Step: 966, Loss: 0.9160178303718567, Accuracy: 1.0, Computation time: 1.6767351627349854\n",
      "Step: 967, Loss: 0.9159718155860901, Accuracy: 1.0, Computation time: 2.163461208343506\n",
      "Step: 968, Loss: 0.9159801006317139, Accuracy: 1.0, Computation time: 1.7203989028930664\n",
      "Step: 969, Loss: 0.9162405729293823, Accuracy: 1.0, Computation time: 1.7946388721466064\n",
      "Step: 970, Loss: 0.9377224445343018, Accuracy: 0.96875, Computation time: 1.7698369026184082\n",
      "Step: 971, Loss: 0.9198229312896729, Accuracy: 1.0, Computation time: 2.1666696071624756\n",
      "Step: 972, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.5736775398254395\n",
      "Step: 973, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.5221424102783203\n",
      "########################\n",
      "Test loss: 1.1079453229904175, Test Accuracy_epoch7: 0.7176368832588196\n",
      "########################\n",
      "Step: 974, Loss: 0.9160138964653015, Accuracy: 1.0, Computation time: 1.652130126953125\n",
      "Step: 975, Loss: 0.9167437553405762, Accuracy: 1.0, Computation time: 2.335775136947632\n",
      "Step: 976, Loss: 0.9163121581077576, Accuracy: 1.0, Computation time: 1.6806621551513672\n",
      "Step: 977, Loss: 0.9375088214874268, Accuracy: 0.96875, Computation time: 1.7835404872894287\n",
      "Step: 978, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 1.5188915729522705\n",
      "Step: 979, Loss: 0.9160507917404175, Accuracy: 1.0, Computation time: 2.4048666954040527\n",
      "Step: 980, Loss: 0.9161173701286316, Accuracy: 1.0, Computation time: 2.1734507083892822\n",
      "Step: 981, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 1.4289803504943848\n",
      "Step: 982, Loss: 0.9176347255706787, Accuracy: 1.0, Computation time: 1.633735179901123\n",
      "Step: 983, Loss: 0.9159683585166931, Accuracy: 1.0, Computation time: 1.7889540195465088\n",
      "Step: 984, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.7774808406829834\n",
      "Step: 985, Loss: 0.916058361530304, Accuracy: 1.0, Computation time: 1.7869048118591309\n",
      "Step: 986, Loss: 0.9160750508308411, Accuracy: 1.0, Computation time: 1.478513240814209\n",
      "Step: 987, Loss: 0.9160098433494568, Accuracy: 1.0, Computation time: 1.9562840461730957\n",
      "Step: 988, Loss: 0.9167930483818054, Accuracy: 1.0, Computation time: 2.191800832748413\n",
      "Step: 989, Loss: 0.9289577603340149, Accuracy: 0.96875, Computation time: 2.1626172065734863\n",
      "Step: 990, Loss: 0.9372503161430359, Accuracy: 0.96875, Computation time: 1.868769884109497\n",
      "Step: 991, Loss: 0.9375646710395813, Accuracy: 0.96875, Computation time: 1.6630232334136963\n",
      "Step: 992, Loss: 0.9159867763519287, Accuracy: 1.0, Computation time: 1.649648904800415\n",
      "Step: 993, Loss: 0.9160459041595459, Accuracy: 1.0, Computation time: 2.201322317123413\n",
      "Step: 994, Loss: 0.9373818635940552, Accuracy: 0.96875, Computation time: 2.100947856903076\n",
      "Step: 995, Loss: 0.9328502416610718, Accuracy: 0.96875, Computation time: 2.1503961086273193\n",
      "Step: 996, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 1.5497972965240479\n",
      "Step: 997, Loss: 0.92696613073349, Accuracy: 0.96875, Computation time: 2.108921766281128\n",
      "Step: 998, Loss: 0.9169735312461853, Accuracy: 1.0, Computation time: 2.0047099590301514\n",
      "Step: 999, Loss: 0.9173616170883179, Accuracy: 1.0, Computation time: 1.5779883861541748\n",
      "Step: 1000, Loss: 0.930687427520752, Accuracy: 0.96875, Computation time: 1.7545733451843262\n",
      "Step: 1001, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.5922727584838867\n",
      "Step: 1002, Loss: 0.9162741899490356, Accuracy: 1.0, Computation time: 2.0309791564941406\n",
      "Step: 1003, Loss: 0.9160814881324768, Accuracy: 1.0, Computation time: 1.815387487411499\n",
      "Step: 1004, Loss: 0.9164746403694153, Accuracy: 1.0, Computation time: 1.7971827983856201\n",
      "Step: 1005, Loss: 0.9161993265151978, Accuracy: 1.0, Computation time: 1.6915318965911865\n",
      "Step: 1006, Loss: 0.9242894649505615, Accuracy: 1.0, Computation time: 2.1945278644561768\n",
      "Step: 1007, Loss: 0.9162821173667908, Accuracy: 1.0, Computation time: 2.073148727416992\n",
      "Step: 1008, Loss: 0.9164595007896423, Accuracy: 1.0, Computation time: 1.9758009910583496\n",
      "Step: 1009, Loss: 0.9263232350349426, Accuracy: 0.96875, Computation time: 2.5723073482513428\n",
      "Step: 1010, Loss: 0.916255533695221, Accuracy: 1.0, Computation time: 1.740936279296875\n",
      "Step: 1011, Loss: 0.9170587658882141, Accuracy: 1.0, Computation time: 1.802189588546753\n",
      "Step: 1012, Loss: 0.9169345498085022, Accuracy: 1.0, Computation time: 2.020177125930786\n",
      "Step: 1013, Loss: 0.9253766536712646, Accuracy: 1.0, Computation time: 2.7333686351776123\n",
      "Step: 1014, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 1.947350025177002\n",
      "Step: 1015, Loss: 0.9160860180854797, Accuracy: 1.0, Computation time: 2.1798641681671143\n",
      "Step: 1016, Loss: 0.9161458015441895, Accuracy: 1.0, Computation time: 2.0951945781707764\n",
      "Step: 1017, Loss: 0.9386599659919739, Accuracy: 0.96875, Computation time: 2.5145013332366943\n",
      "Step: 1018, Loss: 0.9318680763244629, Accuracy: 0.96875, Computation time: 2.191316843032837\n",
      "Step: 1019, Loss: 0.9598482251167297, Accuracy: 0.9375, Computation time: 1.7119479179382324\n",
      "Step: 1020, Loss: 0.9160842895507812, Accuracy: 1.0, Computation time: 1.9001564979553223\n",
      "Step: 1021, Loss: 0.9337718486785889, Accuracy: 0.96875, Computation time: 2.1768100261688232\n",
      "Step: 1022, Loss: 0.9165332317352295, Accuracy: 1.0, Computation time: 2.05006742477417\n",
      "Step: 1023, Loss: 0.9172380566596985, Accuracy: 1.0, Computation time: 1.9878840446472168\n",
      "Step: 1024, Loss: 0.9162766337394714, Accuracy: 1.0, Computation time: 1.9868919849395752\n",
      "Step: 1025, Loss: 0.9164243936538696, Accuracy: 1.0, Computation time: 2.0567994117736816\n",
      "Step: 1026, Loss: 0.9233153462409973, Accuracy: 1.0, Computation time: 1.7988426685333252\n",
      "Step: 1027, Loss: 0.9160729050636292, Accuracy: 1.0, Computation time: 1.984938621520996\n",
      "Step: 1028, Loss: 0.9161015152931213, Accuracy: 1.0, Computation time: 2.0442564487457275\n",
      "Step: 1029, Loss: 0.9159895181655884, Accuracy: 1.0, Computation time: 2.108750104904175\n",
      "Step: 1030, Loss: 0.9171488881111145, Accuracy: 1.0, Computation time: 4.739271402359009\n",
      "Step: 1031, Loss: 0.9162363409996033, Accuracy: 1.0, Computation time: 1.6203935146331787\n",
      "Step: 1032, Loss: 0.9160711765289307, Accuracy: 1.0, Computation time: 1.6552481651306152\n",
      "Step: 1033, Loss: 0.9160534143447876, Accuracy: 1.0, Computation time: 1.6828675270080566\n",
      "Step: 1034, Loss: 0.9161447286605835, Accuracy: 1.0, Computation time: 1.9928581714630127\n",
      "Step: 1035, Loss: 0.9409730434417725, Accuracy: 0.96875, Computation time: 2.3354010581970215\n",
      "Step: 1036, Loss: 0.9468153119087219, Accuracy: 0.96875, Computation time: 2.814436435699463\n",
      "Step: 1037, Loss: 0.9377337694168091, Accuracy: 0.96875, Computation time: 1.913437843322754\n",
      "Step: 1038, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.7340526580810547\n",
      "Step: 1039, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.944765567779541\n",
      "Step: 1040, Loss: 0.9159805774688721, Accuracy: 1.0, Computation time: 1.8467469215393066\n",
      "Step: 1041, Loss: 0.9266790151596069, Accuracy: 0.96875, Computation time: 1.5579195022583008\n",
      "Step: 1042, Loss: 0.9162281155586243, Accuracy: 1.0, Computation time: 1.7688000202178955\n",
      "Step: 1043, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 1.5975897312164307\n",
      "Step: 1044, Loss: 0.9160320162773132, Accuracy: 1.0, Computation time: 2.3577170372009277\n",
      "Step: 1045, Loss: 0.9160977602005005, Accuracy: 1.0, Computation time: 1.6794679164886475\n",
      "Step: 1046, Loss: 0.9160782694816589, Accuracy: 1.0, Computation time: 2.0080783367156982\n",
      "Step: 1047, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 1.8369312286376953\n",
      "Step: 1048, Loss: 0.9162043929100037, Accuracy: 1.0, Computation time: 1.5716824531555176\n",
      "Step: 1049, Loss: 0.9172400832176208, Accuracy: 1.0, Computation time: 1.8261151313781738\n",
      "Step: 1050, Loss: 0.9582775831222534, Accuracy: 0.9375, Computation time: 2.0234792232513428\n",
      "Step: 1051, Loss: 0.9373371601104736, Accuracy: 0.96875, Computation time: 1.560281753540039\n",
      "Step: 1052, Loss: 0.9377284049987793, Accuracy: 0.96875, Computation time: 1.7613332271575928\n",
      "Step: 1053, Loss: 0.9159535765647888, Accuracy: 1.0, Computation time: 1.814382553100586\n",
      "Step: 1054, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 1.8462088108062744\n",
      "Step: 1055, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 2.0565953254699707\n",
      "Step: 1056, Loss: 0.9159379005432129, Accuracy: 1.0, Computation time: 1.4040021896362305\n",
      "Step: 1057, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.5526175498962402\n",
      "Step: 1058, Loss: 0.9160515069961548, Accuracy: 1.0, Computation time: 1.9232041835784912\n",
      "Step: 1059, Loss: 0.936912477016449, Accuracy: 0.96875, Computation time: 2.119636058807373\n",
      "Step: 1060, Loss: 0.9163637757301331, Accuracy: 1.0, Computation time: 2.076819658279419\n",
      "Step: 1061, Loss: 0.9160743951797485, Accuracy: 1.0, Computation time: 1.8267288208007812\n",
      "Step: 1062, Loss: 0.9168142080307007, Accuracy: 1.0, Computation time: 1.551952600479126\n",
      "Step: 1063, Loss: 0.9376150369644165, Accuracy: 0.96875, Computation time: 1.9530630111694336\n",
      "Step: 1064, Loss: 0.9375324249267578, Accuracy: 0.96875, Computation time: 1.8522202968597412\n",
      "Step: 1065, Loss: 0.9468512535095215, Accuracy: 0.96875, Computation time: 2.0117082595825195\n",
      "Step: 1066, Loss: 0.9160593748092651, Accuracy: 1.0, Computation time: 2.20786190032959\n",
      "Step: 1067, Loss: 0.9162377715110779, Accuracy: 1.0, Computation time: 1.9398391246795654\n",
      "Step: 1068, Loss: 0.9535273313522339, Accuracy: 0.9375, Computation time: 2.535980224609375\n",
      "Step: 1069, Loss: 0.9377456903457642, Accuracy: 0.96875, Computation time: 2.0966243743896484\n",
      "Step: 1070, Loss: 0.9164182543754578, Accuracy: 1.0, Computation time: 1.7596657276153564\n",
      "Step: 1071, Loss: 0.9163608551025391, Accuracy: 1.0, Computation time: 2.1805315017700195\n",
      "Step: 1072, Loss: 0.9162642359733582, Accuracy: 1.0, Computation time: 1.5171136856079102\n",
      "Step: 1073, Loss: 0.9176287055015564, Accuracy: 1.0, Computation time: 2.429746389389038\n",
      "Step: 1074, Loss: 0.9160457253456116, Accuracy: 1.0, Computation time: 1.7183427810668945\n",
      "Step: 1075, Loss: 0.916010320186615, Accuracy: 1.0, Computation time: 1.7924847602844238\n",
      "Step: 1076, Loss: 0.9159999489784241, Accuracy: 1.0, Computation time: 1.9204158782958984\n",
      "Step: 1077, Loss: 0.959760844707489, Accuracy: 0.9375, Computation time: 1.393953800201416\n",
      "Step: 1078, Loss: 0.9160722494125366, Accuracy: 1.0, Computation time: 2.2091140747070312\n",
      "Step: 1079, Loss: 0.916111171245575, Accuracy: 1.0, Computation time: 1.7545337677001953\n",
      "Step: 1080, Loss: 0.9161472916603088, Accuracy: 1.0, Computation time: 1.6832706928253174\n",
      "Step: 1081, Loss: 0.9160901308059692, Accuracy: 1.0, Computation time: 1.8256866931915283\n",
      "Step: 1082, Loss: 0.9181491136550903, Accuracy: 1.0, Computation time: 2.0092573165893555\n",
      "Step: 1083, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 2.155510425567627\n",
      "Step: 1084, Loss: 0.9160119295120239, Accuracy: 1.0, Computation time: 1.920011281967163\n",
      "Step: 1085, Loss: 0.9358322024345398, Accuracy: 0.96875, Computation time: 2.962768316268921\n",
      "Step: 1086, Loss: 0.9378527402877808, Accuracy: 0.96875, Computation time: 1.9599254131317139\n",
      "Step: 1087, Loss: 0.9338349103927612, Accuracy: 0.96875, Computation time: 2.159475803375244\n",
      "Step: 1088, Loss: 0.9159534573554993, Accuracy: 1.0, Computation time: 2.0792033672332764\n",
      "Step: 1089, Loss: 0.9349545240402222, Accuracy: 0.96875, Computation time: 2.2380263805389404\n",
      "Step: 1090, Loss: 0.9414334893226624, Accuracy: 0.96875, Computation time: 1.9701032638549805\n",
      "Step: 1091, Loss: 0.9160244464874268, Accuracy: 1.0, Computation time: 1.8716630935668945\n",
      "Step: 1092, Loss: 0.9278799891471863, Accuracy: 0.96875, Computation time: 2.3886332511901855\n",
      "Step: 1093, Loss: 0.9169616103172302, Accuracy: 1.0, Computation time: 3.0495266914367676\n",
      "Step: 1094, Loss: 0.916335940361023, Accuracy: 1.0, Computation time: 2.096386194229126\n",
      "Step: 1095, Loss: 0.9161546230316162, Accuracy: 1.0, Computation time: 2.080279588699341\n",
      "Step: 1096, Loss: 0.959663450717926, Accuracy: 0.9375, Computation time: 2.362779140472412\n",
      "Step: 1097, Loss: 0.9166315197944641, Accuracy: 1.0, Computation time: 2.118312358856201\n",
      "Step: 1098, Loss: 0.9346120953559875, Accuracy: 0.96875, Computation time: 1.948070764541626\n",
      "Step: 1099, Loss: 0.9160783290863037, Accuracy: 1.0, Computation time: 2.1782748699188232\n",
      "Step: 1100, Loss: 0.9171098470687866, Accuracy: 1.0, Computation time: 2.0282647609710693\n",
      "Step: 1101, Loss: 0.9163655638694763, Accuracy: 1.0, Computation time: 2.1405978202819824\n",
      "Step: 1102, Loss: 0.9395829439163208, Accuracy: 0.96875, Computation time: 2.7254343032836914\n",
      "Step: 1103, Loss: 0.9161245822906494, Accuracy: 1.0, Computation time: 1.9582905769348145\n",
      "Step: 1104, Loss: 0.9162322282791138, Accuracy: 1.0, Computation time: 1.99806547164917\n",
      "Step: 1105, Loss: 0.9161167144775391, Accuracy: 1.0, Computation time: 1.8798367977142334\n",
      "Step: 1106, Loss: 0.9161116480827332, Accuracy: 1.0, Computation time: 2.098947048187256\n",
      "Step: 1107, Loss: 0.9160542488098145, Accuracy: 1.0, Computation time: 2.4345250129699707\n",
      "Step: 1108, Loss: 0.9159817695617676, Accuracy: 1.0, Computation time: 1.7610266208648682\n",
      "Step: 1109, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 2.568793773651123\n",
      "Step: 1110, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.9283623695373535\n",
      "Step: 1111, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.6521632671356201\n",
      "Step: 1112, Loss: 0.9199590682983398, Accuracy: 1.0, Computation time: 2.302795171737671\n",
      "########################\n",
      "Test loss: 1.1172666549682617, Test Accuracy_epoch8: 0.7054735422134399\n",
      "########################\n",
      "Step: 1113, Loss: 0.9479886293411255, Accuracy: 0.9375, Computation time: 2.639793634414673\n",
      "Step: 1114, Loss: 0.9159892797470093, Accuracy: 1.0, Computation time: 1.6383826732635498\n",
      "Step: 1115, Loss: 0.9277109503746033, Accuracy: 1.0, Computation time: 2.848137617111206\n",
      "Step: 1116, Loss: 0.9373914003372192, Accuracy: 0.96875, Computation time: 1.6047461032867432\n",
      "Step: 1117, Loss: 0.9162677526473999, Accuracy: 1.0, Computation time: 1.4646191596984863\n",
      "Step: 1118, Loss: 0.9260827302932739, Accuracy: 0.96875, Computation time: 2.4256253242492676\n",
      "Step: 1119, Loss: 0.9384785890579224, Accuracy: 0.96875, Computation time: 2.511667013168335\n",
      "Step: 1120, Loss: 0.9162037968635559, Accuracy: 1.0, Computation time: 1.8390657901763916\n",
      "Step: 1121, Loss: 0.9161590337753296, Accuracy: 1.0, Computation time: 1.7177274227142334\n",
      "Step: 1122, Loss: 0.916115403175354, Accuracy: 1.0, Computation time: 1.5796194076538086\n",
      "Step: 1123, Loss: 0.9164294004440308, Accuracy: 1.0, Computation time: 1.6794922351837158\n",
      "Step: 1124, Loss: 0.9203858971595764, Accuracy: 1.0, Computation time: 1.9779179096221924\n",
      "Step: 1125, Loss: 0.9159973859786987, Accuracy: 1.0, Computation time: 1.634256362915039\n",
      "Step: 1126, Loss: 0.916023313999176, Accuracy: 1.0, Computation time: 1.93308424949646\n",
      "Step: 1127, Loss: 0.9237289428710938, Accuracy: 1.0, Computation time: 1.967841625213623\n",
      "Step: 1128, Loss: 0.9161016941070557, Accuracy: 1.0, Computation time: 1.8654119968414307\n",
      "Step: 1129, Loss: 0.9538712501525879, Accuracy: 0.9375, Computation time: 3.024221897125244\n",
      "Step: 1130, Loss: 0.9451256394386292, Accuracy: 0.9375, Computation time: 2.2500808238983154\n",
      "Step: 1131, Loss: 0.938014030456543, Accuracy: 0.96875, Computation time: 2.1032166481018066\n",
      "Step: 1132, Loss: 0.9317833781242371, Accuracy: 0.96875, Computation time: 1.969996452331543\n",
      "Step: 1133, Loss: 0.9161741137504578, Accuracy: 1.0, Computation time: 1.5619196891784668\n",
      "Step: 1134, Loss: 0.9163910746574402, Accuracy: 1.0, Computation time: 2.12325119972229\n",
      "Step: 1135, Loss: 0.916405439376831, Accuracy: 1.0, Computation time: 1.948587417602539\n",
      "Step: 1136, Loss: 0.9166967272758484, Accuracy: 1.0, Computation time: 1.9791946411132812\n",
      "Step: 1137, Loss: 0.9161595106124878, Accuracy: 1.0, Computation time: 2.122851848602295\n",
      "Step: 1138, Loss: 0.9163581132888794, Accuracy: 1.0, Computation time: 2.001627206802368\n",
      "Step: 1139, Loss: 0.9185504913330078, Accuracy: 1.0, Computation time: 2.6890053749084473\n",
      "Step: 1140, Loss: 0.9190579652786255, Accuracy: 1.0, Computation time: 2.3687167167663574\n",
      "Step: 1141, Loss: 0.9358513355255127, Accuracy: 0.96875, Computation time: 1.9939954280853271\n",
      "Step: 1142, Loss: 0.9161615967750549, Accuracy: 1.0, Computation time: 2.1007778644561768\n",
      "Step: 1143, Loss: 0.9171479940414429, Accuracy: 1.0, Computation time: 1.9390318393707275\n",
      "Step: 1144, Loss: 0.9161256551742554, Accuracy: 1.0, Computation time: 1.9114015102386475\n",
      "Step: 1145, Loss: 0.9467014670372009, Accuracy: 0.96875, Computation time: 2.9765756130218506\n",
      "Step: 1146, Loss: 0.9282562732696533, Accuracy: 0.96875, Computation time: 2.5373950004577637\n",
      "Step: 1147, Loss: 0.9186999201774597, Accuracy: 1.0, Computation time: 2.0175490379333496\n",
      "Step: 1148, Loss: 0.9159820675849915, Accuracy: 1.0, Computation time: 1.9109315872192383\n",
      "Step: 1149, Loss: 0.9159560799598694, Accuracy: 1.0, Computation time: 1.5853302478790283\n",
      "Step: 1150, Loss: 0.9377083778381348, Accuracy: 0.96875, Computation time: 2.2981910705566406\n",
      "Step: 1151, Loss: 0.9161679148674011, Accuracy: 1.0, Computation time: 1.9865672588348389\n",
      "Step: 1152, Loss: 0.9162712097167969, Accuracy: 1.0, Computation time: 2.395233631134033\n",
      "Step: 1153, Loss: 0.9217756986618042, Accuracy: 1.0, Computation time: 2.1673898696899414\n",
      "Step: 1154, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 2.126878261566162\n",
      "Step: 1155, Loss: 0.9163609147071838, Accuracy: 1.0, Computation time: 2.0316250324249268\n",
      "Step: 1156, Loss: 0.9160349369049072, Accuracy: 1.0, Computation time: 1.8373010158538818\n",
      "Step: 1157, Loss: 0.9162220358848572, Accuracy: 1.0, Computation time: 2.6224217414855957\n",
      "Step: 1158, Loss: 0.9160478711128235, Accuracy: 1.0, Computation time: 2.1095423698425293\n",
      "Step: 1159, Loss: 0.9159901142120361, Accuracy: 1.0, Computation time: 2.110201597213745\n",
      "Step: 1160, Loss: 0.9343545436859131, Accuracy: 0.96875, Computation time: 2.3456101417541504\n",
      "Step: 1161, Loss: 0.9161232709884644, Accuracy: 1.0, Computation time: 2.099942684173584\n",
      "Step: 1162, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 1.9961903095245361\n",
      "Step: 1163, Loss: 0.9160122275352478, Accuracy: 1.0, Computation time: 2.7460436820983887\n",
      "Step: 1164, Loss: 0.9368458986282349, Accuracy: 0.96875, Computation time: 3.0748660564422607\n",
      "Step: 1165, Loss: 0.9486597180366516, Accuracy: 0.9375, Computation time: 1.8585498332977295\n",
      "Step: 1166, Loss: 0.934536337852478, Accuracy: 0.96875, Computation time: 1.8782787322998047\n",
      "Step: 1167, Loss: 0.9374803900718689, Accuracy: 0.96875, Computation time: 2.0490870475769043\n",
      "Step: 1168, Loss: 0.9380850791931152, Accuracy: 0.96875, Computation time: 2.064729928970337\n",
      "Step: 1169, Loss: 0.916339099407196, Accuracy: 1.0, Computation time: 1.6455597877502441\n",
      "Step: 1170, Loss: 0.9585601091384888, Accuracy: 0.9375, Computation time: 2.1901700496673584\n",
      "Step: 1171, Loss: 0.9163437485694885, Accuracy: 1.0, Computation time: 1.6639981269836426\n",
      "Step: 1172, Loss: 0.9161608815193176, Accuracy: 1.0, Computation time: 4.2004969120025635\n",
      "Step: 1173, Loss: 0.9160633683204651, Accuracy: 1.0, Computation time: 1.7208106517791748\n",
      "Step: 1174, Loss: 0.9159865975379944, Accuracy: 1.0, Computation time: 1.5666959285736084\n",
      "Step: 1175, Loss: 0.9225583672523499, Accuracy: 1.0, Computation time: 2.011813163757324\n",
      "Step: 1176, Loss: 0.9160951972007751, Accuracy: 1.0, Computation time: 1.6795926094055176\n",
      "Step: 1177, Loss: 0.9195590019226074, Accuracy: 1.0, Computation time: 1.573152780532837\n",
      "Step: 1178, Loss: 0.9161720275878906, Accuracy: 1.0, Computation time: 1.761528491973877\n",
      "Step: 1179, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 1.599616289138794\n",
      "Step: 1180, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 1.936375617980957\n",
      "Step: 1181, Loss: 0.9176986813545227, Accuracy: 1.0, Computation time: 2.053576707839966\n",
      "Step: 1182, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.5007622241973877\n",
      "Step: 1183, Loss: 0.9162755012512207, Accuracy: 1.0, Computation time: 1.7594387531280518\n",
      "Step: 1184, Loss: 0.9159642457962036, Accuracy: 1.0, Computation time: 1.3202605247497559\n",
      "Step: 1185, Loss: 0.9166104793548584, Accuracy: 1.0, Computation time: 1.8535537719726562\n",
      "Step: 1186, Loss: 0.9159330725669861, Accuracy: 1.0, Computation time: 1.7135910987854004\n",
      "Step: 1187, Loss: 0.9159729480743408, Accuracy: 1.0, Computation time: 1.5603179931640625\n",
      "Step: 1188, Loss: 0.9160599112510681, Accuracy: 1.0, Computation time: 2.048496723175049\n",
      "Step: 1189, Loss: 0.9160321354866028, Accuracy: 1.0, Computation time: 1.672443151473999\n",
      "Step: 1190, Loss: 0.9184823036193848, Accuracy: 1.0, Computation time: 1.88236403465271\n",
      "Step: 1191, Loss: 0.9169626235961914, Accuracy: 1.0, Computation time: 2.039433717727661\n",
      "Step: 1192, Loss: 0.9163692593574524, Accuracy: 1.0, Computation time: 1.8427228927612305\n",
      "Step: 1193, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.6677532196044922\n",
      "Step: 1194, Loss: 0.9161495566368103, Accuracy: 1.0, Computation time: 1.9116771221160889\n",
      "Step: 1195, Loss: 0.9161975979804993, Accuracy: 1.0, Computation time: 1.5989561080932617\n",
      "Step: 1196, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 1.6976335048675537\n",
      "Step: 1197, Loss: 0.9375308752059937, Accuracy: 0.96875, Computation time: 1.6588554382324219\n",
      "Step: 1198, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 1.810912847518921\n",
      "Step: 1199, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 1.8160436153411865\n",
      "Step: 1200, Loss: 0.958561360836029, Accuracy: 0.9375, Computation time: 1.5827171802520752\n",
      "Step: 1201, Loss: 0.915997326374054, Accuracy: 1.0, Computation time: 1.730870008468628\n",
      "Step: 1202, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 1.9036800861358643\n",
      "Step: 1203, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 1.5904850959777832\n",
      "Step: 1204, Loss: 0.93768310546875, Accuracy: 0.96875, Computation time: 1.7324976921081543\n",
      "Step: 1205, Loss: 0.9162304401397705, Accuracy: 1.0, Computation time: 1.8226861953735352\n",
      "Step: 1206, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.4184203147888184\n",
      "Step: 1207, Loss: 0.9451469779014587, Accuracy: 0.96875, Computation time: 1.7363693714141846\n",
      "Step: 1208, Loss: 0.9403902888298035, Accuracy: 0.96875, Computation time: 1.9174551963806152\n",
      "Step: 1209, Loss: 0.915986180305481, Accuracy: 1.0, Computation time: 1.7487781047821045\n",
      "Step: 1210, Loss: 0.9375509023666382, Accuracy: 0.96875, Computation time: 1.7465522289276123\n",
      "Step: 1211, Loss: 0.9160958528518677, Accuracy: 1.0, Computation time: 1.5025112628936768\n",
      "Step: 1212, Loss: 0.9379064440727234, Accuracy: 0.96875, Computation time: 2.5652544498443604\n",
      "Step: 1213, Loss: 0.9160464406013489, Accuracy: 1.0, Computation time: 1.3441071510314941\n",
      "Step: 1214, Loss: 0.9160847663879395, Accuracy: 1.0, Computation time: 1.4527866840362549\n",
      "Step: 1215, Loss: 0.9170687198638916, Accuracy: 1.0, Computation time: 1.6155714988708496\n",
      "Step: 1216, Loss: 0.9160454273223877, Accuracy: 1.0, Computation time: 1.6020030975341797\n",
      "Step: 1217, Loss: 0.9159839749336243, Accuracy: 1.0, Computation time: 1.8832030296325684\n",
      "Step: 1218, Loss: 0.9377593398094177, Accuracy: 0.96875, Computation time: 1.8771800994873047\n",
      "Step: 1219, Loss: 0.9163601398468018, Accuracy: 1.0, Computation time: 2.324871063232422\n",
      "Step: 1220, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.5355243682861328\n",
      "Step: 1221, Loss: 0.9279037117958069, Accuracy: 0.96875, Computation time: 1.6554439067840576\n",
      "Step: 1222, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.8824844360351562\n",
      "Step: 1223, Loss: 0.927808403968811, Accuracy: 1.0, Computation time: 1.7585976123809814\n",
      "Step: 1224, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 1.823671579360962\n",
      "Step: 1225, Loss: 0.9160651564598083, Accuracy: 1.0, Computation time: 1.6517577171325684\n",
      "Step: 1226, Loss: 0.9160715937614441, Accuracy: 1.0, Computation time: 1.8042912483215332\n",
      "Step: 1227, Loss: 0.9162978529930115, Accuracy: 1.0, Computation time: 2.031046152114868\n",
      "Step: 1228, Loss: 0.9377397298812866, Accuracy: 0.96875, Computation time: 1.7986433506011963\n",
      "Step: 1229, Loss: 0.9161990284919739, Accuracy: 1.0, Computation time: 1.5837905406951904\n",
      "Step: 1230, Loss: 0.9359758496284485, Accuracy: 0.96875, Computation time: 1.9825243949890137\n",
      "Step: 1231, Loss: 0.9375583529472351, Accuracy: 0.96875, Computation time: 1.7486553192138672\n",
      "Step: 1232, Loss: 0.916067361831665, Accuracy: 1.0, Computation time: 1.6688568592071533\n",
      "Step: 1233, Loss: 0.9161006212234497, Accuracy: 1.0, Computation time: 1.3158605098724365\n",
      "Step: 1234, Loss: 0.915957510471344, Accuracy: 1.0, Computation time: 1.7559418678283691\n",
      "Step: 1235, Loss: 0.9161017537117004, Accuracy: 1.0, Computation time: 1.7218923568725586\n",
      "Step: 1236, Loss: 0.9372146725654602, Accuracy: 0.96875, Computation time: 2.118603467941284\n",
      "Step: 1237, Loss: 0.916024923324585, Accuracy: 1.0, Computation time: 1.8511006832122803\n",
      "Step: 1238, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.2540569305419922\n",
      "Step: 1239, Loss: 0.9208052754402161, Accuracy: 1.0, Computation time: 2.2901358604431152\n",
      "Step: 1240, Loss: 0.916118323802948, Accuracy: 1.0, Computation time: 2.1344354152679443\n",
      "Step: 1241, Loss: 0.9164553284645081, Accuracy: 1.0, Computation time: 1.4625048637390137\n",
      "Step: 1242, Loss: 0.9161601662635803, Accuracy: 1.0, Computation time: 1.524707555770874\n",
      "Step: 1243, Loss: 0.9162383675575256, Accuracy: 1.0, Computation time: 1.694645643234253\n",
      "Step: 1244, Loss: 0.9353153705596924, Accuracy: 0.96875, Computation time: 1.7225284576416016\n",
      "Step: 1245, Loss: 0.9195476770401001, Accuracy: 1.0, Computation time: 1.6165406703948975\n",
      "Step: 1246, Loss: 0.9167504906654358, Accuracy: 1.0, Computation time: 2.002540349960327\n",
      "Step: 1247, Loss: 0.9160200953483582, Accuracy: 1.0, Computation time: 2.0330440998077393\n",
      "Step: 1248, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 1.6908180713653564\n",
      "Step: 1249, Loss: 0.915958821773529, Accuracy: 1.0, Computation time: 1.649810791015625\n",
      "Step: 1250, Loss: 0.9159574508666992, Accuracy: 1.0, Computation time: 1.6644556522369385\n",
      "Step: 1251, Loss: 0.9377298951148987, Accuracy: 0.96875, Computation time: 1.676424264907837\n",
      "########################\n",
      "Test loss: 1.123146414756775, Test Accuracy_epoch9: 0.6976542472839355\n",
      "########################\n",
      "Step: 1252, Loss: 0.9385972619056702, Accuracy: 0.96875, Computation time: 3.1051931381225586\n",
      "Step: 1253, Loss: 0.9159700870513916, Accuracy: 1.0, Computation time: 1.8666794300079346\n",
      "Step: 1254, Loss: 0.9168925881385803, Accuracy: 1.0, Computation time: 1.8027923107147217\n",
      "Step: 1255, Loss: 0.9159523844718933, Accuracy: 1.0, Computation time: 1.4795360565185547\n",
      "Step: 1256, Loss: 0.9162768721580505, Accuracy: 1.0, Computation time: 1.7104463577270508\n",
      "Step: 1257, Loss: 0.9207357168197632, Accuracy: 1.0, Computation time: 2.0670762062072754\n",
      "Step: 1258, Loss: 0.9376381635665894, Accuracy: 0.96875, Computation time: 2.2055857181549072\n",
      "Step: 1259, Loss: 0.9332280158996582, Accuracy: 0.96875, Computation time: 2.3762941360473633\n",
      "Step: 1260, Loss: 0.9160571098327637, Accuracy: 1.0, Computation time: 1.5025367736816406\n",
      "Step: 1261, Loss: 0.9161606431007385, Accuracy: 1.0, Computation time: 1.817922830581665\n",
      "Step: 1262, Loss: 0.9574975967407227, Accuracy: 0.9375, Computation time: 2.068455219268799\n",
      "Step: 1263, Loss: 0.9160462021827698, Accuracy: 1.0, Computation time: 1.5726819038391113\n",
      "Step: 1264, Loss: 0.9160126447677612, Accuracy: 1.0, Computation time: 1.7038946151733398\n",
      "Step: 1265, Loss: 0.9160446524620056, Accuracy: 1.0, Computation time: 1.6901509761810303\n",
      "Step: 1266, Loss: 0.9377862811088562, Accuracy: 0.96875, Computation time: 1.7017738819122314\n",
      "Step: 1267, Loss: 0.917374312877655, Accuracy: 1.0, Computation time: 1.6012768745422363\n",
      "Step: 1268, Loss: 0.9159548878669739, Accuracy: 1.0, Computation time: 1.721895694732666\n",
      "Step: 1269, Loss: 0.9160686731338501, Accuracy: 1.0, Computation time: 1.4734776020050049\n",
      "Step: 1270, Loss: 0.9159844517707825, Accuracy: 1.0, Computation time: 1.5792620182037354\n",
      "Step: 1271, Loss: 0.9159954190254211, Accuracy: 1.0, Computation time: 1.7654762268066406\n",
      "Step: 1272, Loss: 0.916140079498291, Accuracy: 1.0, Computation time: 1.6494102478027344\n",
      "Step: 1273, Loss: 0.9160122871398926, Accuracy: 1.0, Computation time: 1.7543253898620605\n",
      "Step: 1274, Loss: 0.9159994721412659, Accuracy: 1.0, Computation time: 1.710096836090088\n",
      "Step: 1275, Loss: 0.9379363059997559, Accuracy: 0.96875, Computation time: 1.9086451530456543\n",
      "Step: 1276, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 1.5804805755615234\n",
      "Step: 1277, Loss: 0.9162678122520447, Accuracy: 1.0, Computation time: 1.640394926071167\n",
      "Step: 1278, Loss: 0.9336603879928589, Accuracy: 0.96875, Computation time: 2.2407262325286865\n",
      "Step: 1279, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.5768489837646484\n",
      "Step: 1280, Loss: 0.9422296285629272, Accuracy: 0.96875, Computation time: 1.9337704181671143\n",
      "Step: 1281, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 1.4280171394348145\n",
      "Step: 1282, Loss: 0.9249053001403809, Accuracy: 1.0, Computation time: 1.7428102493286133\n",
      "Step: 1283, Loss: 0.9352220296859741, Accuracy: 0.96875, Computation time: 2.354173183441162\n",
      "Step: 1284, Loss: 0.9165356159210205, Accuracy: 1.0, Computation time: 1.5730338096618652\n",
      "Step: 1285, Loss: 0.9161883592605591, Accuracy: 1.0, Computation time: 1.3405067920684814\n",
      "Step: 1286, Loss: 0.9382187724113464, Accuracy: 0.96875, Computation time: 2.0508248805999756\n",
      "Step: 1287, Loss: 0.9160476326942444, Accuracy: 1.0, Computation time: 1.3032562732696533\n",
      "Step: 1288, Loss: 0.9372273683547974, Accuracy: 0.96875, Computation time: 1.6621344089508057\n",
      "Step: 1289, Loss: 0.9160453081130981, Accuracy: 1.0, Computation time: 1.4744515419006348\n",
      "Step: 1290, Loss: 0.9159666895866394, Accuracy: 1.0, Computation time: 1.5894782543182373\n",
      "Step: 1291, Loss: 0.9160481095314026, Accuracy: 1.0, Computation time: 1.7031059265136719\n",
      "Step: 1292, Loss: 0.9159723520278931, Accuracy: 1.0, Computation time: 1.558887243270874\n",
      "Step: 1293, Loss: 0.9160230159759521, Accuracy: 1.0, Computation time: 1.5919795036315918\n",
      "Step: 1294, Loss: 0.9376316070556641, Accuracy: 0.96875, Computation time: 1.6594717502593994\n",
      "Step: 1295, Loss: 0.9168660640716553, Accuracy: 1.0, Computation time: 1.8194313049316406\n",
      "Step: 1296, Loss: 0.9370260834693909, Accuracy: 0.96875, Computation time: 2.5833184719085693\n",
      "Step: 1297, Loss: 0.9159996509552002, Accuracy: 1.0, Computation time: 1.7949731349945068\n",
      "Step: 1298, Loss: 0.9195910692214966, Accuracy: 1.0, Computation time: 1.582106351852417\n",
      "Step: 1299, Loss: 0.9374390244483948, Accuracy: 0.96875, Computation time: 1.697129249572754\n",
      "Step: 1300, Loss: 0.9250332117080688, Accuracy: 1.0, Computation time: 2.3114707469940186\n",
      "Step: 1301, Loss: 0.9159765839576721, Accuracy: 1.0, Computation time: 1.8442604541778564\n",
      "Step: 1302, Loss: 0.9191097617149353, Accuracy: 1.0, Computation time: 2.20255184173584\n",
      "Step: 1303, Loss: 0.9160140752792358, Accuracy: 1.0, Computation time: 1.716416835784912\n",
      "Step: 1304, Loss: 0.916024923324585, Accuracy: 1.0, Computation time: 1.8119566440582275\n",
      "Step: 1305, Loss: 0.9160410165786743, Accuracy: 1.0, Computation time: 2.033724308013916\n",
      "Step: 1306, Loss: 0.9377690553665161, Accuracy: 0.96875, Computation time: 2.0598044395446777\n",
      "Step: 1307, Loss: 0.9160215854644775, Accuracy: 1.0, Computation time: 1.8134515285491943\n",
      "Step: 1308, Loss: 0.9377045631408691, Accuracy: 0.96875, Computation time: 1.9853665828704834\n",
      "Step: 1309, Loss: 0.9159557223320007, Accuracy: 1.0, Computation time: 2.127044677734375\n",
      "Step: 1310, Loss: 0.9159997701644897, Accuracy: 1.0, Computation time: 2.3462774753570557\n",
      "Step: 1311, Loss: 0.9162159562110901, Accuracy: 1.0, Computation time: 1.9952750205993652\n",
      "Step: 1312, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 1.9322960376739502\n",
      "Step: 1313, Loss: 0.9160990715026855, Accuracy: 1.0, Computation time: 2.0450713634490967\n",
      "Step: 1314, Loss: 0.9459405541419983, Accuracy: 0.96875, Computation time: 2.0783867835998535\n",
      "Step: 1315, Loss: 0.9159506559371948, Accuracy: 1.0, Computation time: 1.9180636405944824\n",
      "Step: 1316, Loss: 0.9356319308280945, Accuracy: 0.96875, Computation time: 1.9603221416473389\n",
      "Step: 1317, Loss: 0.9159858822822571, Accuracy: 1.0, Computation time: 2.02940034866333\n",
      "Step: 1318, Loss: 0.9160376787185669, Accuracy: 1.0, Computation time: 1.8158018589019775\n",
      "Step: 1319, Loss: 0.9385201334953308, Accuracy: 0.96875, Computation time: 2.1844851970672607\n",
      "Step: 1320, Loss: 0.9160363674163818, Accuracy: 1.0, Computation time: 1.578739881515503\n",
      "Step: 1321, Loss: 0.9160287380218506, Accuracy: 1.0, Computation time: 1.7612357139587402\n",
      "Step: 1322, Loss: 0.9368392825126648, Accuracy: 0.96875, Computation time: 1.8459844589233398\n",
      "Step: 1323, Loss: 0.9376450777053833, Accuracy: 0.96875, Computation time: 1.776369571685791\n",
      "Step: 1324, Loss: 0.9375289082527161, Accuracy: 0.96875, Computation time: 1.7995617389678955\n",
      "Step: 1325, Loss: 0.9159737825393677, Accuracy: 1.0, Computation time: 2.007438898086548\n",
      "Step: 1326, Loss: 0.9160629510879517, Accuracy: 1.0, Computation time: 2.0461208820343018\n",
      "Step: 1327, Loss: 0.9160671234130859, Accuracy: 1.0, Computation time: 1.4624929428100586\n",
      "Step: 1328, Loss: 0.9357395172119141, Accuracy: 0.96875, Computation time: 1.8802120685577393\n",
      "Step: 1329, Loss: 0.9377919435501099, Accuracy: 0.96875, Computation time: 1.7939043045043945\n",
      "Step: 1330, Loss: 0.9160997867584229, Accuracy: 1.0, Computation time: 1.719038963317871\n",
      "Step: 1331, Loss: 0.9159885048866272, Accuracy: 1.0, Computation time: 1.589339017868042\n",
      "Step: 1332, Loss: 0.9254100918769836, Accuracy: 0.96875, Computation time: 2.0702781677246094\n",
      "Step: 1333, Loss: 0.9196563363075256, Accuracy: 1.0, Computation time: 1.8608767986297607\n",
      "Step: 1334, Loss: 0.916006863117218, Accuracy: 1.0, Computation time: 1.8075895309448242\n",
      "Step: 1335, Loss: 0.9352390766143799, Accuracy: 0.96875, Computation time: 1.6849360466003418\n",
      "Step: 1336, Loss: 0.9159799218177795, Accuracy: 1.0, Computation time: 1.5367517471313477\n",
      "Step: 1337, Loss: 0.9359325170516968, Accuracy: 0.96875, Computation time: 2.8334665298461914\n",
      "Step: 1338, Loss: 0.9159972071647644, Accuracy: 1.0, Computation time: 1.2611513137817383\n",
      "Step: 1339, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 2.1256935596466064\n",
      "Step: 1340, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.5776114463806152\n",
      "Step: 1341, Loss: 0.9162167310714722, Accuracy: 1.0, Computation time: 1.7404470443725586\n",
      "Step: 1342, Loss: 0.9161192178726196, Accuracy: 1.0, Computation time: 2.910149574279785\n",
      "Step: 1343, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.602525234222412\n",
      "Step: 1344, Loss: 0.9214177131652832, Accuracy: 1.0, Computation time: 1.8789935111999512\n",
      "Step: 1345, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 1.4803290367126465\n",
      "Step: 1346, Loss: 0.9377743005752563, Accuracy: 0.96875, Computation time: 1.7869577407836914\n",
      "Step: 1347, Loss: 0.9160388708114624, Accuracy: 1.0, Computation time: 1.8504140377044678\n",
      "Step: 1348, Loss: 0.9159976840019226, Accuracy: 1.0, Computation time: 1.7465307712554932\n",
      "Step: 1349, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 1.8233673572540283\n",
      "Step: 1350, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.6240870952606201\n",
      "Step: 1351, Loss: 0.9160001873970032, Accuracy: 1.0, Computation time: 1.7560882568359375\n",
      "Step: 1352, Loss: 0.9161425232887268, Accuracy: 1.0, Computation time: 1.627737283706665\n",
      "Step: 1353, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.7646915912628174\n",
      "Step: 1354, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 2.1094889640808105\n",
      "Step: 1355, Loss: 0.9161454439163208, Accuracy: 1.0, Computation time: 1.9731409549713135\n",
      "Step: 1356, Loss: 0.9327917695045471, Accuracy: 1.0, Computation time: 1.9521288871765137\n",
      "Step: 1357, Loss: 0.916233479976654, Accuracy: 1.0, Computation time: 1.8903789520263672\n",
      "Step: 1358, Loss: 0.9160721898078918, Accuracy: 1.0, Computation time: 1.5428242683410645\n",
      "Step: 1359, Loss: 0.9160590171813965, Accuracy: 1.0, Computation time: 1.5666813850402832\n",
      "Step: 1360, Loss: 0.9391326308250427, Accuracy: 0.96875, Computation time: 1.8362953662872314\n",
      "Step: 1361, Loss: 0.9372824430465698, Accuracy: 0.96875, Computation time: 1.5538489818572998\n",
      "Step: 1362, Loss: 0.9160051345825195, Accuracy: 1.0, Computation time: 1.84773588180542\n",
      "Step: 1363, Loss: 0.9160001277923584, Accuracy: 1.0, Computation time: 1.5645744800567627\n",
      "Step: 1364, Loss: 0.9423643350601196, Accuracy: 0.96875, Computation time: 2.1697237491607666\n",
      "Step: 1365, Loss: 0.9166489243507385, Accuracy: 1.0, Computation time: 1.994605541229248\n",
      "Step: 1366, Loss: 0.9160007238388062, Accuracy: 1.0, Computation time: 1.5871460437774658\n",
      "Step: 1367, Loss: 0.9563220739364624, Accuracy: 0.9375, Computation time: 1.7504725456237793\n",
      "Step: 1368, Loss: 0.9160902500152588, Accuracy: 1.0, Computation time: 1.7352817058563232\n",
      "Step: 1369, Loss: 0.9162156581878662, Accuracy: 1.0, Computation time: 1.5592365264892578\n",
      "Step: 1370, Loss: 0.9160812497138977, Accuracy: 1.0, Computation time: 1.5337538719177246\n",
      "Step: 1371, Loss: 0.9160842895507812, Accuracy: 1.0, Computation time: 1.9088234901428223\n",
      "Step: 1372, Loss: 0.9160390496253967, Accuracy: 1.0, Computation time: 1.6583077907562256\n",
      "Step: 1373, Loss: 0.9167386293411255, Accuracy: 1.0, Computation time: 2.201690196990967\n",
      "Step: 1374, Loss: 0.9159950613975525, Accuracy: 1.0, Computation time: 2.1822218894958496\n",
      "Step: 1375, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 1.5367391109466553\n",
      "Step: 1376, Loss: 0.9162626266479492, Accuracy: 1.0, Computation time: 1.52329683303833\n",
      "Step: 1377, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.534867286682129\n",
      "Step: 1378, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.6358110904693604\n",
      "Step: 1379, Loss: 0.9304666519165039, Accuracy: 0.96875, Computation time: 2.3069067001342773\n",
      "Step: 1380, Loss: 0.9159995913505554, Accuracy: 1.0, Computation time: 1.7644577026367188\n",
      "Step: 1381, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.8046891689300537\n",
      "Step: 1382, Loss: 0.937554657459259, Accuracy: 0.96875, Computation time: 1.7599413394927979\n",
      "Step: 1383, Loss: 0.9197086095809937, Accuracy: 1.0, Computation time: 1.8983922004699707\n",
      "Step: 1384, Loss: 0.9159908890724182, Accuracy: 1.0, Computation time: 1.5476205348968506\n",
      "Step: 1385, Loss: 0.949455738067627, Accuracy: 0.9375, Computation time: 1.9416062831878662\n",
      "Step: 1386, Loss: 0.9166256189346313, Accuracy: 1.0, Computation time: 1.7734925746917725\n",
      "Step: 1387, Loss: 0.9391142129898071, Accuracy: 0.96875, Computation time: 2.0910792350769043\n",
      "Step: 1388, Loss: 0.9297203421592712, Accuracy: 0.96875, Computation time: 2.4075000286102295\n",
      "Step: 1389, Loss: 0.9160577058792114, Accuracy: 1.0, Computation time: 1.4000961780548096\n",
      "Step: 1390, Loss: 0.9593732357025146, Accuracy: 0.9375, Computation time: 2.4802613258361816\n",
      "########################\n",
      "Test loss: 1.1204633712768555, Test Accuracy_epoch10: 0.7011294960975647\n",
      "########################\n",
      "Step: 1391, Loss: 0.9161516427993774, Accuracy: 1.0, Computation time: 1.7359707355499268\n",
      "Step: 1392, Loss: 0.9441261887550354, Accuracy: 0.96875, Computation time: 1.681305170059204\n",
      "Step: 1393, Loss: 0.9162715673446655, Accuracy: 1.0, Computation time: 1.4954721927642822\n",
      "Step: 1394, Loss: 0.916248619556427, Accuracy: 1.0, Computation time: 1.3148202896118164\n",
      "Step: 1395, Loss: 0.9162583351135254, Accuracy: 1.0, Computation time: 1.4481470584869385\n",
      "Step: 1396, Loss: 0.9161003232002258, Accuracy: 1.0, Computation time: 1.5347192287445068\n",
      "Step: 1397, Loss: 0.9160272479057312, Accuracy: 1.0, Computation time: 1.5110511779785156\n",
      "Step: 1398, Loss: 0.9160091280937195, Accuracy: 1.0, Computation time: 1.7017998695373535\n",
      "Step: 1399, Loss: 0.9160199761390686, Accuracy: 1.0, Computation time: 1.624211311340332\n",
      "Step: 1400, Loss: 0.916082501411438, Accuracy: 1.0, Computation time: 1.7666919231414795\n",
      "Step: 1401, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 1.7726027965545654\n",
      "Step: 1402, Loss: 0.9369937181472778, Accuracy: 0.96875, Computation time: 1.7034916877746582\n",
      "Step: 1403, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.7807440757751465\n",
      "Step: 1404, Loss: 0.9159886837005615, Accuracy: 1.0, Computation time: 2.0775985717773438\n",
      "Step: 1405, Loss: 0.9256398677825928, Accuracy: 0.96875, Computation time: 2.5408477783203125\n",
      "Step: 1406, Loss: 0.9164048433303833, Accuracy: 1.0, Computation time: 1.453366994857788\n",
      "Step: 1407, Loss: 0.916063129901886, Accuracy: 1.0, Computation time: 1.6386997699737549\n",
      "Step: 1408, Loss: 0.9376630783081055, Accuracy: 0.96875, Computation time: 1.5618512630462646\n",
      "Step: 1409, Loss: 0.9160645008087158, Accuracy: 1.0, Computation time: 1.7453153133392334\n",
      "Step: 1410, Loss: 0.9160362482070923, Accuracy: 1.0, Computation time: 1.6484603881835938\n",
      "Step: 1411, Loss: 0.9595358967781067, Accuracy: 0.9375, Computation time: 1.5790936946868896\n",
      "Step: 1412, Loss: 0.918941855430603, Accuracy: 1.0, Computation time: 2.1815779209136963\n",
      "Step: 1413, Loss: 0.9160040020942688, Accuracy: 1.0, Computation time: 1.8936982154846191\n",
      "Step: 1414, Loss: 0.937686562538147, Accuracy: 0.96875, Computation time: 1.8017776012420654\n",
      "Step: 1415, Loss: 0.9160158634185791, Accuracy: 1.0, Computation time: 2.0036392211914062\n",
      "Step: 1416, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.7119615077972412\n",
      "Step: 1417, Loss: 0.9600359201431274, Accuracy: 0.9375, Computation time: 1.7367196083068848\n",
      "Step: 1418, Loss: 0.9162260890007019, Accuracy: 1.0, Computation time: 1.763927936553955\n",
      "Step: 1419, Loss: 0.9369982481002808, Accuracy: 0.96875, Computation time: 2.3163509368896484\n",
      "Step: 1420, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.7276606559753418\n",
      "Step: 1421, Loss: 0.916323184967041, Accuracy: 1.0, Computation time: 2.0257554054260254\n",
      "Step: 1422, Loss: 0.9350180625915527, Accuracy: 0.96875, Computation time: 1.813572645187378\n",
      "Step: 1423, Loss: 0.931891679763794, Accuracy: 0.96875, Computation time: 3.5166380405426025\n",
      "Step: 1424, Loss: 0.9161173701286316, Accuracy: 1.0, Computation time: 1.6594212055206299\n",
      "Step: 1425, Loss: 0.9160264730453491, Accuracy: 1.0, Computation time: 1.7586545944213867\n",
      "Step: 1426, Loss: 0.9417122006416321, Accuracy: 0.96875, Computation time: 2.5159201622009277\n",
      "Step: 1427, Loss: 0.9159925580024719, Accuracy: 1.0, Computation time: 1.9147017002105713\n",
      "Step: 1428, Loss: 0.9160276055335999, Accuracy: 1.0, Computation time: 1.7603182792663574\n",
      "Step: 1429, Loss: 0.9161352515220642, Accuracy: 1.0, Computation time: 1.6398162841796875\n",
      "Step: 1430, Loss: 0.9160405993461609, Accuracy: 1.0, Computation time: 1.805013656616211\n",
      "Step: 1431, Loss: 0.9160568118095398, Accuracy: 1.0, Computation time: 1.8092842102050781\n",
      "Step: 1432, Loss: 0.9160335659980774, Accuracy: 1.0, Computation time: 1.7273240089416504\n",
      "Step: 1433, Loss: 0.9159901738166809, Accuracy: 1.0, Computation time: 1.4155793190002441\n",
      "Step: 1434, Loss: 0.9376801252365112, Accuracy: 0.96875, Computation time: 1.486710548400879\n",
      "Step: 1435, Loss: 0.9160418510437012, Accuracy: 1.0, Computation time: 1.764897346496582\n",
      "Step: 1436, Loss: 0.9159518480300903, Accuracy: 1.0, Computation time: 1.5930712223052979\n",
      "Step: 1437, Loss: 0.9201202392578125, Accuracy: 1.0, Computation time: 1.947429895401001\n",
      "Step: 1438, Loss: 0.91596519947052, Accuracy: 1.0, Computation time: 1.4452309608459473\n",
      "Step: 1439, Loss: 0.9159516096115112, Accuracy: 1.0, Computation time: 1.4283421039581299\n",
      "Step: 1440, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 1.596139669418335\n",
      "Step: 1441, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 1.3613553047180176\n",
      "Step: 1442, Loss: 0.9159756898880005, Accuracy: 1.0, Computation time: 1.5062594413757324\n",
      "Step: 1443, Loss: 0.9159433841705322, Accuracy: 1.0, Computation time: 1.4655654430389404\n",
      "Step: 1444, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 1.4411544799804688\n",
      "Step: 1445, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 1.883573055267334\n",
      "Step: 1446, Loss: 0.9167032837867737, Accuracy: 1.0, Computation time: 2.291367769241333\n",
      "Step: 1447, Loss: 0.9246503710746765, Accuracy: 1.0, Computation time: 1.8817944526672363\n",
      "Step: 1448, Loss: 0.9159739017486572, Accuracy: 1.0, Computation time: 1.430845022201538\n",
      "Step: 1449, Loss: 0.9160296320915222, Accuracy: 1.0, Computation time: 1.348823070526123\n",
      "Step: 1450, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 1.6246368885040283\n",
      "Step: 1451, Loss: 0.9162406921386719, Accuracy: 1.0, Computation time: 1.932647943496704\n",
      "Step: 1452, Loss: 0.9375032186508179, Accuracy: 0.96875, Computation time: 1.6624908447265625\n",
      "Step: 1453, Loss: 0.9374897480010986, Accuracy: 0.96875, Computation time: 1.628756046295166\n",
      "Step: 1454, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 1.7099053859710693\n",
      "Step: 1455, Loss: 0.9159315228462219, Accuracy: 1.0, Computation time: 1.396636724472046\n",
      "Step: 1456, Loss: 0.9382830858230591, Accuracy: 0.96875, Computation time: 2.1939780712127686\n",
      "Step: 1457, Loss: 0.9160934090614319, Accuracy: 1.0, Computation time: 1.7999775409698486\n",
      "Step: 1458, Loss: 0.9189090728759766, Accuracy: 1.0, Computation time: 1.9349172115325928\n",
      "Step: 1459, Loss: 0.9160483479499817, Accuracy: 1.0, Computation time: 1.720961332321167\n",
      "Step: 1460, Loss: 0.9160600900650024, Accuracy: 1.0, Computation time: 1.495110273361206\n",
      "Step: 1461, Loss: 0.9160209894180298, Accuracy: 1.0, Computation time: 1.6421349048614502\n",
      "Step: 1462, Loss: 0.9159145951271057, Accuracy: 1.0, Computation time: 1.5768768787384033\n",
      "Step: 1463, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 1.5670790672302246\n",
      "Step: 1464, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.9580702781677246\n",
      "Step: 1465, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 1.7422163486480713\n",
      "Step: 1466, Loss: 0.9159573316574097, Accuracy: 1.0, Computation time: 2.248660087585449\n",
      "Step: 1467, Loss: 0.9159267544746399, Accuracy: 1.0, Computation time: 1.5984408855438232\n",
      "Step: 1468, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.741257667541504\n",
      "Step: 1469, Loss: 0.9160061478614807, Accuracy: 1.0, Computation time: 1.5987284183502197\n",
      "Step: 1470, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.4742839336395264\n",
      "Step: 1471, Loss: 0.9162340760231018, Accuracy: 1.0, Computation time: 1.778224229812622\n",
      "Step: 1472, Loss: 0.9159635305404663, Accuracy: 1.0, Computation time: 1.6523091793060303\n",
      "Step: 1473, Loss: 0.9325791597366333, Accuracy: 0.96875, Computation time: 2.135744571685791\n",
      "Step: 1474, Loss: 0.9184973239898682, Accuracy: 1.0, Computation time: 2.0062029361724854\n",
      "Step: 1475, Loss: 0.9290502071380615, Accuracy: 0.96875, Computation time: 1.9368131160736084\n",
      "Step: 1476, Loss: 0.9159867167472839, Accuracy: 1.0, Computation time: 1.4757144451141357\n",
      "Step: 1477, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 1.805934190750122\n",
      "Step: 1478, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 1.5671985149383545\n",
      "Step: 1479, Loss: 0.915950357913971, Accuracy: 1.0, Computation time: 1.5354702472686768\n",
      "Step: 1480, Loss: 0.915957510471344, Accuracy: 1.0, Computation time: 1.3872087001800537\n",
      "Step: 1481, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.6078524589538574\n",
      "Step: 1482, Loss: 0.9164884686470032, Accuracy: 1.0, Computation time: 1.6840767860412598\n",
      "Step: 1483, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.2249374389648438\n",
      "Step: 1484, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.7701356410980225\n",
      "Step: 1485, Loss: 0.9160103797912598, Accuracy: 1.0, Computation time: 2.0729422569274902\n",
      "Step: 1486, Loss: 0.9173239469528198, Accuracy: 1.0, Computation time: 1.5334208011627197\n",
      "Step: 1487, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.5583999156951904\n",
      "Step: 1488, Loss: 0.9166976809501648, Accuracy: 1.0, Computation time: 1.504723310470581\n",
      "Step: 1489, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.2681074142456055\n",
      "Step: 1490, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.8298444747924805\n",
      "Step: 1491, Loss: 0.9160360097885132, Accuracy: 1.0, Computation time: 1.6944823265075684\n",
      "Step: 1492, Loss: 0.9370951652526855, Accuracy: 0.96875, Computation time: 1.76466965675354\n",
      "Step: 1493, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.6034128665924072\n",
      "Step: 1494, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.5512757301330566\n",
      "Step: 1495, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.642242193222046\n",
      "Step: 1496, Loss: 0.9362680315971375, Accuracy: 0.96875, Computation time: 2.0696516036987305\n",
      "Step: 1497, Loss: 0.9296115040779114, Accuracy: 0.96875, Computation time: 2.170917272567749\n",
      "Step: 1498, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 1.5845222473144531\n",
      "Step: 1499, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 1.711012363433838\n",
      "Step: 1500, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.3396055698394775\n",
      "Step: 1501, Loss: 0.9530933499336243, Accuracy: 0.9375, Computation time: 1.8604021072387695\n",
      "Step: 1502, Loss: 0.9375578165054321, Accuracy: 0.96875, Computation time: 1.4814059734344482\n",
      "Step: 1503, Loss: 0.9159649014472961, Accuracy: 1.0, Computation time: 1.6789801120758057\n",
      "Step: 1504, Loss: 0.9159795641899109, Accuracy: 1.0, Computation time: 1.6993017196655273\n",
      "Step: 1505, Loss: 0.937588095664978, Accuracy: 0.96875, Computation time: 1.3936293125152588\n",
      "Step: 1506, Loss: 0.9159356951713562, Accuracy: 1.0, Computation time: 1.3475956916809082\n",
      "Step: 1507, Loss: 0.915911078453064, Accuracy: 1.0, Computation time: 1.50531005859375\n",
      "Step: 1508, Loss: 0.9172471165657043, Accuracy: 1.0, Computation time: 1.7924253940582275\n",
      "Step: 1509, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.4983198642730713\n",
      "Step: 1510, Loss: 0.9158712029457092, Accuracy: 1.0, Computation time: 1.4108572006225586\n",
      "Step: 1511, Loss: 0.9449248313903809, Accuracy: 0.96875, Computation time: 2.157054901123047\n",
      "Step: 1512, Loss: 0.9160215258598328, Accuracy: 1.0, Computation time: 1.341031551361084\n",
      "Step: 1513, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.5854053497314453\n",
      "Step: 1514, Loss: 0.937615156173706, Accuracy: 0.96875, Computation time: 1.8069190979003906\n",
      "Step: 1515, Loss: 0.9160513281822205, Accuracy: 1.0, Computation time: 1.5164704322814941\n",
      "Step: 1516, Loss: 0.9159454107284546, Accuracy: 1.0, Computation time: 1.7547483444213867\n",
      "Step: 1517, Loss: 0.9206885695457458, Accuracy: 1.0, Computation time: 1.8549246788024902\n",
      "Step: 1518, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.4053940773010254\n",
      "Step: 1519, Loss: 0.9502055644989014, Accuracy: 0.96875, Computation time: 2.080828905105591\n",
      "Step: 1520, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.4860236644744873\n",
      "Step: 1521, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 2.320546865463257\n",
      "Step: 1522, Loss: 0.9333658814430237, Accuracy: 0.96875, Computation time: 1.7724549770355225\n",
      "Step: 1523, Loss: 0.9160828590393066, Accuracy: 1.0, Computation time: 2.3252439498901367\n",
      "Step: 1524, Loss: 0.9166592955589294, Accuracy: 1.0, Computation time: 1.8401281833648682\n",
      "Step: 1525, Loss: 0.9161416888237, Accuracy: 1.0, Computation time: 2.5426509380340576\n",
      "Step: 1526, Loss: 0.9160614013671875, Accuracy: 1.0, Computation time: 1.5453519821166992\n",
      "Step: 1527, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 1.6760754585266113\n",
      "Step: 1528, Loss: 0.9160167574882507, Accuracy: 1.0, Computation time: 1.5678339004516602\n",
      "Step: 1529, Loss: 0.9176918864250183, Accuracy: 1.0, Computation time: 1.9666917324066162\n",
      "########################\n",
      "Test loss: 1.1178429126739502, Test Accuracy_epoch11: 0.703735888004303\n",
      "########################\n",
      "Step: 1530, Loss: 0.9373410940170288, Accuracy: 0.96875, Computation time: 1.8471314907073975\n",
      "Step: 1531, Loss: 0.9161807894706726, Accuracy: 1.0, Computation time: 1.8025298118591309\n",
      "Step: 1532, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 1.6556775569915771\n",
      "Step: 1533, Loss: 0.9159743785858154, Accuracy: 1.0, Computation time: 1.76517653465271\n",
      "Step: 1534, Loss: 0.9323635697364807, Accuracy: 0.96875, Computation time: 1.8274259567260742\n",
      "Step: 1535, Loss: 0.9192858934402466, Accuracy: 1.0, Computation time: 1.7267048358917236\n",
      "Step: 1536, Loss: 0.9160639047622681, Accuracy: 1.0, Computation time: 1.423926830291748\n",
      "Step: 1537, Loss: 0.9160280227661133, Accuracy: 1.0, Computation time: 1.669119119644165\n",
      "Step: 1538, Loss: 0.9160063862800598, Accuracy: 1.0, Computation time: 1.537553071975708\n",
      "Step: 1539, Loss: 0.9163133502006531, Accuracy: 1.0, Computation time: 1.5473377704620361\n",
      "Step: 1540, Loss: 0.9159477949142456, Accuracy: 1.0, Computation time: 1.6608235836029053\n",
      "Step: 1541, Loss: 0.9159896969795227, Accuracy: 1.0, Computation time: 1.580716848373413\n",
      "Step: 1542, Loss: 0.9374099373817444, Accuracy: 0.96875, Computation time: 1.584198236465454\n",
      "Step: 1543, Loss: 0.9160553812980652, Accuracy: 1.0, Computation time: 1.9415769577026367\n",
      "Step: 1544, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.4535400867462158\n",
      "Step: 1545, Loss: 0.9159248471260071, Accuracy: 1.0, Computation time: 1.6257760524749756\n",
      "Step: 1546, Loss: 0.9578278660774231, Accuracy: 0.9375, Computation time: 2.023669481277466\n",
      "Step: 1547, Loss: 0.9159578084945679, Accuracy: 1.0, Computation time: 1.8500847816467285\n",
      "Step: 1548, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.6516273021697998\n",
      "Step: 1549, Loss: 0.9176715016365051, Accuracy: 1.0, Computation time: 1.857670545578003\n",
      "Step: 1550, Loss: 0.9335052371025085, Accuracy: 0.96875, Computation time: 2.2013425827026367\n",
      "Step: 1551, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 1.5212366580963135\n",
      "Step: 1552, Loss: 0.9385272860527039, Accuracy: 0.96875, Computation time: 1.769343376159668\n",
      "Step: 1553, Loss: 0.9380030035972595, Accuracy: 0.96875, Computation time: 1.7136218547821045\n",
      "Step: 1554, Loss: 0.9159483909606934, Accuracy: 1.0, Computation time: 1.549226999282837\n",
      "Step: 1555, Loss: 0.9159497022628784, Accuracy: 1.0, Computation time: 1.5920710563659668\n",
      "Step: 1556, Loss: 0.9159544706344604, Accuracy: 1.0, Computation time: 1.5344367027282715\n",
      "Step: 1557, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.7998709678649902\n",
      "Step: 1558, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 1.5866072177886963\n",
      "Step: 1559, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.6462962627410889\n",
      "Step: 1560, Loss: 0.9160830974578857, Accuracy: 1.0, Computation time: 1.8875648975372314\n",
      "Step: 1561, Loss: 0.9174608588218689, Accuracy: 1.0, Computation time: 1.7279284000396729\n",
      "Step: 1562, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.6930971145629883\n",
      "Step: 1563, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 1.6200649738311768\n",
      "Step: 1564, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 1.7613093852996826\n",
      "Step: 1565, Loss: 0.9160319566726685, Accuracy: 1.0, Computation time: 1.7161734104156494\n",
      "Step: 1566, Loss: 0.9221864342689514, Accuracy: 1.0, Computation time: 2.595757246017456\n",
      "Step: 1567, Loss: 0.9161350727081299, Accuracy: 1.0, Computation time: 1.802553653717041\n",
      "Step: 1568, Loss: 0.9172438383102417, Accuracy: 1.0, Computation time: 1.3686044216156006\n",
      "Step: 1569, Loss: 0.9162710905075073, Accuracy: 1.0, Computation time: 2.148043394088745\n",
      "Step: 1570, Loss: 0.9161784648895264, Accuracy: 1.0, Computation time: 1.520017385482788\n",
      "Step: 1571, Loss: 0.9360843300819397, Accuracy: 0.96875, Computation time: 1.8197927474975586\n",
      "Step: 1572, Loss: 0.919373095035553, Accuracy: 1.0, Computation time: 1.723454236984253\n",
      "Step: 1573, Loss: 0.9322525858879089, Accuracy: 0.96875, Computation time: 1.6923167705535889\n",
      "Step: 1574, Loss: 0.9379193782806396, Accuracy: 0.96875, Computation time: 2.0830142498016357\n",
      "Step: 1575, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 1.7035608291625977\n",
      "Step: 1576, Loss: 0.937217116355896, Accuracy: 0.96875, Computation time: 1.8587331771850586\n",
      "Step: 1577, Loss: 0.9161192178726196, Accuracy: 1.0, Computation time: 1.8809804916381836\n",
      "Step: 1578, Loss: 0.9161109328269958, Accuracy: 1.0, Computation time: 1.658017635345459\n",
      "Step: 1579, Loss: 0.9162110686302185, Accuracy: 1.0, Computation time: 1.6719274520874023\n",
      "Step: 1580, Loss: 0.9159666895866394, Accuracy: 1.0, Computation time: 1.7547972202301025\n",
      "Step: 1581, Loss: 0.9160510897636414, Accuracy: 1.0, Computation time: 2.0534465312957764\n",
      "Step: 1582, Loss: 0.9276867508888245, Accuracy: 0.96875, Computation time: 2.119778633117676\n",
      "Step: 1583, Loss: 0.9159879088401794, Accuracy: 1.0, Computation time: 1.9244120121002197\n",
      "Step: 1584, Loss: 0.9159708023071289, Accuracy: 1.0, Computation time: 1.7554662227630615\n",
      "Step: 1585, Loss: 0.9373595714569092, Accuracy: 0.96875, Computation time: 2.249345302581787\n",
      "Step: 1586, Loss: 0.916138768196106, Accuracy: 1.0, Computation time: 2.0040268898010254\n",
      "Step: 1587, Loss: 0.9375576972961426, Accuracy: 0.96875, Computation time: 1.5759074687957764\n",
      "Step: 1588, Loss: 0.9160813093185425, Accuracy: 1.0, Computation time: 1.8889386653900146\n",
      "Step: 1589, Loss: 0.9164181351661682, Accuracy: 1.0, Computation time: 1.771935224533081\n",
      "Step: 1590, Loss: 0.9160233736038208, Accuracy: 1.0, Computation time: 1.6624081134796143\n",
      "Step: 1591, Loss: 0.9161102175712585, Accuracy: 1.0, Computation time: 1.6259758472442627\n",
      "Step: 1592, Loss: 0.9159388542175293, Accuracy: 1.0, Computation time: 1.9959931373596191\n",
      "Step: 1593, Loss: 0.915956974029541, Accuracy: 1.0, Computation time: 1.8018066883087158\n",
      "Step: 1594, Loss: 0.9159988164901733, Accuracy: 1.0, Computation time: 2.1027607917785645\n",
      "Step: 1595, Loss: 0.9159364104270935, Accuracy: 1.0, Computation time: 2.0210158824920654\n",
      "Step: 1596, Loss: 0.9348278045654297, Accuracy: 0.96875, Computation time: 2.4405620098114014\n",
      "Step: 1597, Loss: 0.9375904202461243, Accuracy: 0.96875, Computation time: 1.827056884765625\n",
      "Step: 1598, Loss: 0.9169749021530151, Accuracy: 1.0, Computation time: 1.7571961879730225\n",
      "Step: 1599, Loss: 0.9160034656524658, Accuracy: 1.0, Computation time: 2.057284116744995\n",
      "Step: 1600, Loss: 0.9160720109939575, Accuracy: 1.0, Computation time: 1.9962489604949951\n",
      "Step: 1601, Loss: 0.9162461161613464, Accuracy: 1.0, Computation time: 1.6197843551635742\n",
      "Step: 1602, Loss: 0.9332935810089111, Accuracy: 0.96875, Computation time: 1.8547000885009766\n",
      "Step: 1603, Loss: 0.9160510897636414, Accuracy: 1.0, Computation time: 1.772965669631958\n",
      "Step: 1604, Loss: 0.9260010123252869, Accuracy: 0.96875, Computation time: 2.903862714767456\n",
      "Step: 1605, Loss: 0.9159548282623291, Accuracy: 1.0, Computation time: 1.6998345851898193\n",
      "Step: 1606, Loss: 0.9160544872283936, Accuracy: 1.0, Computation time: 2.180934429168701\n",
      "Step: 1607, Loss: 0.916023850440979, Accuracy: 1.0, Computation time: 1.7384169101715088\n",
      "Step: 1608, Loss: 0.9166426062583923, Accuracy: 1.0, Computation time: 2.454594135284424\n",
      "Step: 1609, Loss: 0.9168104529380798, Accuracy: 1.0, Computation time: 1.925570011138916\n",
      "Step: 1610, Loss: 0.9173566699028015, Accuracy: 1.0, Computation time: 2.5086424350738525\n",
      "Step: 1611, Loss: 0.9253545999526978, Accuracy: 0.96875, Computation time: 2.3074471950531006\n",
      "Step: 1612, Loss: 0.9207866787910461, Accuracy: 1.0, Computation time: 1.7602431774139404\n",
      "Step: 1613, Loss: 0.9159485697746277, Accuracy: 1.0, Computation time: 1.7885322570800781\n",
      "Step: 1614, Loss: 0.9160412549972534, Accuracy: 1.0, Computation time: 2.243964910507202\n",
      "Step: 1615, Loss: 0.9163401126861572, Accuracy: 1.0, Computation time: 1.844090223312378\n",
      "Step: 1616, Loss: 0.9165607690811157, Accuracy: 1.0, Computation time: 2.0506951808929443\n",
      "Step: 1617, Loss: 0.9398723840713501, Accuracy: 0.96875, Computation time: 1.809509515762329\n",
      "Step: 1618, Loss: 0.9162998795509338, Accuracy: 1.0, Computation time: 1.664738416671753\n",
      "Step: 1619, Loss: 0.9376202821731567, Accuracy: 0.96875, Computation time: 1.8852317333221436\n",
      "Step: 1620, Loss: 0.9159767627716064, Accuracy: 1.0, Computation time: 1.830143690109253\n",
      "Step: 1621, Loss: 0.9202873706817627, Accuracy: 1.0, Computation time: 2.325753927230835\n",
      "Step: 1622, Loss: 0.9376409649848938, Accuracy: 0.96875, Computation time: 1.8042707443237305\n",
      "Step: 1623, Loss: 0.9163782596588135, Accuracy: 1.0, Computation time: 1.933591604232788\n",
      "Step: 1624, Loss: 0.91603684425354, Accuracy: 1.0, Computation time: 1.9490017890930176\n",
      "Step: 1625, Loss: 0.916158139705658, Accuracy: 1.0, Computation time: 1.7834854125976562\n",
      "Step: 1626, Loss: 0.9160712957382202, Accuracy: 1.0, Computation time: 1.8337483406066895\n",
      "Step: 1627, Loss: 0.9203998446464539, Accuracy: 1.0, Computation time: 1.6322157382965088\n",
      "Step: 1628, Loss: 0.9160589575767517, Accuracy: 1.0, Computation time: 1.6672511100769043\n",
      "Step: 1629, Loss: 0.9160915017127991, Accuracy: 1.0, Computation time: 1.604252576828003\n",
      "Step: 1630, Loss: 0.9160119891166687, Accuracy: 1.0, Computation time: 1.5218861103057861\n",
      "Step: 1631, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.2752907276153564\n",
      "Step: 1632, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 1.8206779956817627\n",
      "Step: 1633, Loss: 0.9164097905158997, Accuracy: 1.0, Computation time: 1.4880475997924805\n",
      "Step: 1634, Loss: 0.9343782663345337, Accuracy: 0.96875, Computation time: 1.7076823711395264\n",
      "Step: 1635, Loss: 0.9162331819534302, Accuracy: 1.0, Computation time: 1.5535223484039307\n",
      "Step: 1636, Loss: 0.9161983132362366, Accuracy: 1.0, Computation time: 2.6680715084075928\n",
      "Step: 1637, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 1.2847402095794678\n",
      "Step: 1638, Loss: 0.9384301900863647, Accuracy: 0.96875, Computation time: 2.2697246074676514\n",
      "Step: 1639, Loss: 0.9159807562828064, Accuracy: 1.0, Computation time: 1.9226059913635254\n",
      "Step: 1640, Loss: 0.9164234399795532, Accuracy: 1.0, Computation time: 1.9038515090942383\n",
      "Step: 1641, Loss: 0.9159250855445862, Accuracy: 1.0, Computation time: 1.7440872192382812\n",
      "Step: 1642, Loss: 0.9375381469726562, Accuracy: 0.96875, Computation time: 1.497523307800293\n",
      "Step: 1643, Loss: 0.9375206232070923, Accuracy: 0.96875, Computation time: 1.6921429634094238\n",
      "Step: 1644, Loss: 0.9361741542816162, Accuracy: 0.96875, Computation time: 1.595702886581421\n",
      "Step: 1645, Loss: 0.916689932346344, Accuracy: 1.0, Computation time: 2.094372034072876\n",
      "Step: 1646, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.4175386428833008\n",
      "Step: 1647, Loss: 0.9373304843902588, Accuracy: 0.96875, Computation time: 2.5965664386749268\n",
      "Step: 1648, Loss: 0.9376610517501831, Accuracy: 0.96875, Computation time: 1.5040724277496338\n",
      "Step: 1649, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.6364407539367676\n",
      "Step: 1650, Loss: 0.9160469770431519, Accuracy: 1.0, Computation time: 1.7483313083648682\n",
      "Step: 1651, Loss: 0.9159421920776367, Accuracy: 1.0, Computation time: 2.0942189693450928\n",
      "Step: 1652, Loss: 0.9159144163131714, Accuracy: 1.0, Computation time: 1.7418715953826904\n",
      "Step: 1653, Loss: 0.9163440465927124, Accuracy: 1.0, Computation time: 1.382082462310791\n",
      "Step: 1654, Loss: 0.9359657764434814, Accuracy: 0.96875, Computation time: 1.5353624820709229\n",
      "Step: 1655, Loss: 0.9160041213035583, Accuracy: 1.0, Computation time: 1.5930306911468506\n",
      "Step: 1656, Loss: 0.9362351298332214, Accuracy: 0.96875, Computation time: 1.9177820682525635\n",
      "Step: 1657, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.7924175262451172\n",
      "Step: 1658, Loss: 0.9594277739524841, Accuracy: 0.9375, Computation time: 1.5404760837554932\n",
      "Step: 1659, Loss: 0.916088879108429, Accuracy: 1.0, Computation time: 1.5041136741638184\n",
      "Step: 1660, Loss: 0.9161292910575867, Accuracy: 1.0, Computation time: 1.8967182636260986\n",
      "Step: 1661, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 1.7071943283081055\n",
      "Step: 1662, Loss: 0.916029155254364, Accuracy: 1.0, Computation time: 1.4274754524230957\n",
      "Step: 1663, Loss: 0.9159786701202393, Accuracy: 1.0, Computation time: 1.9487829208374023\n",
      "Step: 1664, Loss: 0.943708062171936, Accuracy: 0.96875, Computation time: 2.0307326316833496\n",
      "Step: 1665, Loss: 0.91778564453125, Accuracy: 1.0, Computation time: 2.2045648097991943\n",
      "Step: 1666, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.3796887397766113\n",
      "Step: 1667, Loss: 0.9159261584281921, Accuracy: 1.0, Computation time: 1.6060740947723389\n",
      "Step: 1668, Loss: 0.9159853458404541, Accuracy: 1.0, Computation time: 1.9260106086730957\n",
      "########################\n",
      "Test loss: 1.111972451210022, Test Accuracy_epoch12: 0.7158992290496826\n",
      "########################\n",
      "Step: 1669, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 1.2886769771575928\n",
      "Step: 1670, Loss: 0.9167243242263794, Accuracy: 1.0, Computation time: 1.5930628776550293\n",
      "Step: 1671, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 1.3338513374328613\n",
      "Step: 1672, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.3606312274932861\n",
      "Step: 1673, Loss: 0.9159152507781982, Accuracy: 1.0, Computation time: 1.674513816833496\n",
      "Step: 1674, Loss: 0.9159286022186279, Accuracy: 1.0, Computation time: 2.304175853729248\n",
      "Step: 1675, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 1.5386629104614258\n",
      "Step: 1676, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.631087303161621\n",
      "Step: 1677, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.9871068000793457\n",
      "Step: 1678, Loss: 0.9364845156669617, Accuracy: 0.96875, Computation time: 1.7802152633666992\n",
      "Step: 1679, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.7107253074645996\n",
      "Step: 1680, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.6094963550567627\n",
      "Step: 1681, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.5876147747039795\n",
      "Step: 1682, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 1.72353196144104\n",
      "Step: 1683, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.676314115524292\n",
      "Step: 1684, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.574418067932129\n",
      "Step: 1685, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.6339409351348877\n",
      "Step: 1686, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.5659449100494385\n",
      "Step: 1687, Loss: 0.9166756868362427, Accuracy: 1.0, Computation time: 2.186450481414795\n",
      "Step: 1688, Loss: 0.917391836643219, Accuracy: 1.0, Computation time: 2.1971683502197266\n",
      "Step: 1689, Loss: 0.915984570980072, Accuracy: 1.0, Computation time: 1.5015294551849365\n",
      "Step: 1690, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 1.5872859954833984\n",
      "Step: 1691, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 2.0235726833343506\n",
      "Step: 1692, Loss: 0.91949462890625, Accuracy: 1.0, Computation time: 1.9092535972595215\n",
      "Step: 1693, Loss: 0.9375514984130859, Accuracy: 0.96875, Computation time: 1.76007080078125\n",
      "Step: 1694, Loss: 0.9159591794013977, Accuracy: 1.0, Computation time: 2.133636713027954\n",
      "Step: 1695, Loss: 0.9161698222160339, Accuracy: 1.0, Computation time: 1.7182469367980957\n",
      "Step: 1696, Loss: 0.9166591167449951, Accuracy: 1.0, Computation time: 1.7714908123016357\n",
      "Step: 1697, Loss: 0.9159839153289795, Accuracy: 1.0, Computation time: 1.7229299545288086\n",
      "Step: 1698, Loss: 0.9365823864936829, Accuracy: 0.96875, Computation time: 1.8021304607391357\n",
      "Step: 1699, Loss: 0.9425656795501709, Accuracy: 0.96875, Computation time: 1.9608070850372314\n",
      "Step: 1700, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.7584354877471924\n",
      "Step: 1701, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 2.0683727264404297\n",
      "Step: 1702, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.8827133178710938\n",
      "Step: 1703, Loss: 0.9171279668807983, Accuracy: 1.0, Computation time: 2.028481960296631\n",
      "Step: 1704, Loss: 0.9182732105255127, Accuracy: 1.0, Computation time: 2.0683937072753906\n",
      "Step: 1705, Loss: 0.9310439825057983, Accuracy: 0.96875, Computation time: 2.05776047706604\n",
      "Step: 1706, Loss: 0.9160190224647522, Accuracy: 1.0, Computation time: 1.8277490139007568\n",
      "Step: 1707, Loss: 0.9369522929191589, Accuracy: 0.96875, Computation time: 1.528446912765503\n",
      "Step: 1708, Loss: 0.9204273223876953, Accuracy: 1.0, Computation time: 1.67982816696167\n",
      "Step: 1709, Loss: 0.9377207159996033, Accuracy: 0.96875, Computation time: 1.654280185699463\n",
      "Step: 1710, Loss: 0.9160157442092896, Accuracy: 1.0, Computation time: 1.7624578475952148\n",
      "Step: 1711, Loss: 0.9160528779029846, Accuracy: 1.0, Computation time: 2.097348690032959\n",
      "Step: 1712, Loss: 0.9160768389701843, Accuracy: 1.0, Computation time: 1.930778980255127\n",
      "Step: 1713, Loss: 0.9162725806236267, Accuracy: 1.0, Computation time: 1.6175832748413086\n",
      "Step: 1714, Loss: 0.9160593152046204, Accuracy: 1.0, Computation time: 1.629194974899292\n",
      "Step: 1715, Loss: 0.9160521030426025, Accuracy: 1.0, Computation time: 1.932924509048462\n",
      "Step: 1716, Loss: 0.9169080257415771, Accuracy: 1.0, Computation time: 1.8335082530975342\n",
      "Step: 1717, Loss: 0.9374850988388062, Accuracy: 0.96875, Computation time: 2.0132858753204346\n",
      "Step: 1718, Loss: 0.9243202209472656, Accuracy: 1.0, Computation time: 2.240492343902588\n",
      "Step: 1719, Loss: 0.933925211429596, Accuracy: 0.96875, Computation time: 1.7634215354919434\n",
      "Step: 1720, Loss: 0.9378520846366882, Accuracy: 0.96875, Computation time: 1.8881816864013672\n",
      "Step: 1721, Loss: 0.9378236532211304, Accuracy: 0.96875, Computation time: 2.0460615158081055\n",
      "Step: 1722, Loss: 0.9437016844749451, Accuracy: 0.96875, Computation time: 2.430238723754883\n",
      "Step: 1723, Loss: 0.9161831140518188, Accuracy: 1.0, Computation time: 2.1042428016662598\n",
      "Step: 1724, Loss: 0.9192383289337158, Accuracy: 1.0, Computation time: 1.7989490032196045\n",
      "Step: 1725, Loss: 0.9365219473838806, Accuracy: 0.96875, Computation time: 2.527702569961548\n",
      "Step: 1726, Loss: 0.9161320924758911, Accuracy: 1.0, Computation time: 2.2849340438842773\n",
      "Step: 1727, Loss: 0.9160560965538025, Accuracy: 1.0, Computation time: 1.8192553520202637\n",
      "Step: 1728, Loss: 0.9160528182983398, Accuracy: 1.0, Computation time: 1.9707005023956299\n",
      "Step: 1729, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 1.942805528640747\n",
      "Step: 1730, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 1.936570405960083\n",
      "Step: 1731, Loss: 0.9306582808494568, Accuracy: 0.96875, Computation time: 2.066364288330078\n",
      "Step: 1732, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.921288013458252\n",
      "Step: 1733, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 1.7745821475982666\n",
      "Step: 1734, Loss: 0.9343111515045166, Accuracy: 0.96875, Computation time: 2.5966544151306152\n",
      "Step: 1735, Loss: 0.9160042405128479, Accuracy: 1.0, Computation time: 2.254500150680542\n",
      "Step: 1736, Loss: 0.9376866817474365, Accuracy: 0.96875, Computation time: 1.9332637786865234\n",
      "Step: 1737, Loss: 0.9160054922103882, Accuracy: 1.0, Computation time: 2.3309993743896484\n",
      "Step: 1738, Loss: 0.9161014556884766, Accuracy: 1.0, Computation time: 2.0165834426879883\n",
      "Step: 1739, Loss: 0.9159947037696838, Accuracy: 1.0, Computation time: 1.9534437656402588\n",
      "Step: 1740, Loss: 0.9161139130592346, Accuracy: 1.0, Computation time: 1.8187980651855469\n",
      "Step: 1741, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 1.936408281326294\n",
      "Step: 1742, Loss: 0.915919303894043, Accuracy: 1.0, Computation time: 2.0732553005218506\n",
      "Step: 1743, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.9001286029815674\n",
      "Step: 1744, Loss: 0.9184861779212952, Accuracy: 1.0, Computation time: 2.6750731468200684\n",
      "Step: 1745, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.744056224822998\n",
      "Step: 1746, Loss: 0.9159446954727173, Accuracy: 1.0, Computation time: 1.8956520557403564\n",
      "Step: 1747, Loss: 0.9159160852432251, Accuracy: 1.0, Computation time: 1.8491663932800293\n",
      "Step: 1748, Loss: 0.9159172177314758, Accuracy: 1.0, Computation time: 1.9609405994415283\n",
      "Step: 1749, Loss: 0.9375946521759033, Accuracy: 0.96875, Computation time: 2.473888635635376\n",
      "Step: 1750, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.8132143020629883\n",
      "Step: 1751, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 2.0011045932769775\n",
      "Step: 1752, Loss: 0.9588871002197266, Accuracy: 0.9375, Computation time: 2.242774724960327\n",
      "Step: 1753, Loss: 0.9504553079605103, Accuracy: 0.9375, Computation time: 2.1905713081359863\n",
      "Step: 1754, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 2.453672409057617\n",
      "Step: 1755, Loss: 0.9159928560256958, Accuracy: 1.0, Computation time: 1.9085595607757568\n",
      "Step: 1756, Loss: 0.915995717048645, Accuracy: 1.0, Computation time: 1.9166102409362793\n",
      "Step: 1757, Loss: 0.9162513017654419, Accuracy: 1.0, Computation time: 2.638986349105835\n",
      "Step: 1758, Loss: 0.9165852069854736, Accuracy: 1.0, Computation time: 2.369386672973633\n",
      "Step: 1759, Loss: 0.9160516262054443, Accuracy: 1.0, Computation time: 2.074324607849121\n",
      "Step: 1760, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 2.4966225624084473\n",
      "Step: 1761, Loss: 0.9166669249534607, Accuracy: 1.0, Computation time: 1.6415066719055176\n",
      "Step: 1762, Loss: 0.915901780128479, Accuracy: 1.0, Computation time: 1.8024861812591553\n",
      "Step: 1763, Loss: 0.9168667793273926, Accuracy: 1.0, Computation time: 1.7641606330871582\n",
      "Step: 1764, Loss: 0.9160237908363342, Accuracy: 1.0, Computation time: 1.8770265579223633\n",
      "Step: 1765, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.7955782413482666\n",
      "Step: 1766, Loss: 0.9162781834602356, Accuracy: 1.0, Computation time: 1.9023520946502686\n",
      "Step: 1767, Loss: 0.9159821271896362, Accuracy: 1.0, Computation time: 2.101898670196533\n",
      "Step: 1768, Loss: 0.9159448742866516, Accuracy: 1.0, Computation time: 1.588486909866333\n",
      "Step: 1769, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 2.0674636363983154\n",
      "Step: 1770, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 2.1507670879364014\n",
      "Step: 1771, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 2.080446243286133\n",
      "Step: 1772, Loss: 0.9165419340133667, Accuracy: 1.0, Computation time: 2.525897264480591\n",
      "Step: 1773, Loss: 0.9162390232086182, Accuracy: 1.0, Computation time: 1.969902753829956\n",
      "Step: 1774, Loss: 0.9368152022361755, Accuracy: 0.96875, Computation time: 1.816558599472046\n",
      "Step: 1775, Loss: 0.9159787893295288, Accuracy: 1.0, Computation time: 1.9780855178833008\n",
      "Step: 1776, Loss: 0.9162121415138245, Accuracy: 1.0, Computation time: 2.1837480068206787\n",
      "Step: 1777, Loss: 0.9375211000442505, Accuracy: 0.96875, Computation time: 1.98264741897583\n",
      "Step: 1778, Loss: 0.9262143969535828, Accuracy: 0.96875, Computation time: 1.7200031280517578\n",
      "Step: 1779, Loss: 0.9207264184951782, Accuracy: 1.0, Computation time: 1.5702416896820068\n",
      "Step: 1780, Loss: 0.9319973587989807, Accuracy: 0.96875, Computation time: 1.7573628425598145\n",
      "Step: 1781, Loss: 0.9159191250801086, Accuracy: 1.0, Computation time: 1.7721326351165771\n",
      "Step: 1782, Loss: 0.9376155138015747, Accuracy: 0.96875, Computation time: 1.6301884651184082\n",
      "Step: 1783, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.557502031326294\n",
      "Step: 1784, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 1.862365961074829\n",
      "Step: 1785, Loss: 0.9160369634628296, Accuracy: 1.0, Computation time: 1.7682075500488281\n",
      "Step: 1786, Loss: 0.915901780128479, Accuracy: 1.0, Computation time: 1.453923225402832\n",
      "Step: 1787, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 1.784816026687622\n",
      "Step: 1788, Loss: 0.9368829131126404, Accuracy: 0.96875, Computation time: 1.806504249572754\n",
      "Step: 1789, Loss: 0.9377121925354004, Accuracy: 0.96875, Computation time: 1.4659137725830078\n",
      "Step: 1790, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 1.895357370376587\n",
      "Step: 1791, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.494420051574707\n",
      "Step: 1792, Loss: 0.9337880611419678, Accuracy: 0.96875, Computation time: 2.162999153137207\n",
      "Step: 1793, Loss: 0.9161397218704224, Accuracy: 1.0, Computation time: 1.95638108253479\n",
      "Step: 1794, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 1.8777048587799072\n",
      "Step: 1795, Loss: 0.9159875512123108, Accuracy: 1.0, Computation time: 1.7110607624053955\n",
      "Step: 1796, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 1.4976465702056885\n",
      "Step: 1797, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.7028100490570068\n",
      "Step: 1798, Loss: 0.9159824848175049, Accuracy: 1.0, Computation time: 1.822667121887207\n",
      "Step: 1799, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 1.4321839809417725\n",
      "Step: 1800, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 2.3213798999786377\n",
      "Step: 1801, Loss: 0.9162353277206421, Accuracy: 1.0, Computation time: 2.0091733932495117\n",
      "Step: 1802, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 1.6060009002685547\n",
      "Step: 1803, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 1.5402374267578125\n",
      "Step: 1804, Loss: 0.9372620582580566, Accuracy: 0.96875, Computation time: 2.175560474395752\n",
      "Step: 1805, Loss: 0.9312655925750732, Accuracy: 0.96875, Computation time: 2.1366350650787354\n",
      "Step: 1806, Loss: 0.9373658895492554, Accuracy: 0.96875, Computation time: 1.9529130458831787\n",
      "Step: 1807, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.4610872268676758\n",
      "########################\n",
      "Test loss: 1.1175141334533691, Test Accuracy_epoch13: 0.703735888004303\n",
      "########################\n",
      "Step: 1808, Loss: 0.916009247303009, Accuracy: 1.0, Computation time: 1.820720911026001\n",
      "Step: 1809, Loss: 0.9170207977294922, Accuracy: 1.0, Computation time: 2.1790151596069336\n",
      "Step: 1810, Loss: 0.9376980066299438, Accuracy: 0.96875, Computation time: 2.0161218643188477\n",
      "Step: 1811, Loss: 0.9161461591720581, Accuracy: 1.0, Computation time: 1.506282091140747\n",
      "Step: 1812, Loss: 0.9377244710922241, Accuracy: 0.96875, Computation time: 1.6776378154754639\n",
      "Step: 1813, Loss: 0.9159337878227234, Accuracy: 1.0, Computation time: 1.4745068550109863\n",
      "Step: 1814, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.7410271167755127\n",
      "Step: 1815, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.575979232788086\n",
      "Step: 1816, Loss: 0.9159247279167175, Accuracy: 1.0, Computation time: 1.4178590774536133\n",
      "Step: 1817, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.4922411441802979\n",
      "Step: 1818, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 1.7915749549865723\n",
      "Step: 1819, Loss: 0.9159691333770752, Accuracy: 1.0, Computation time: 2.0418291091918945\n",
      "Step: 1820, Loss: 0.9173485040664673, Accuracy: 1.0, Computation time: 2.333266258239746\n",
      "Step: 1821, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 2.0572779178619385\n",
      "Step: 1822, Loss: 0.9160096645355225, Accuracy: 1.0, Computation time: 1.6273751258850098\n",
      "Step: 1823, Loss: 0.9581326246261597, Accuracy: 0.9375, Computation time: 1.8941097259521484\n",
      "Step: 1824, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.9086463451385498\n",
      "Step: 1825, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.7931745052337646\n",
      "Step: 1826, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.8411545753479004\n",
      "Step: 1827, Loss: 0.9373794198036194, Accuracy: 0.96875, Computation time: 1.6634852886199951\n",
      "Step: 1828, Loss: 0.9159561991691589, Accuracy: 1.0, Computation time: 1.7284600734710693\n",
      "Step: 1829, Loss: 0.936859130859375, Accuracy: 0.96875, Computation time: 2.2317709922790527\n",
      "Step: 1830, Loss: 0.9161415100097656, Accuracy: 1.0, Computation time: 1.8007020950317383\n",
      "Step: 1831, Loss: 0.9159569144248962, Accuracy: 1.0, Computation time: 1.8582186698913574\n",
      "Step: 1832, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.7518527507781982\n",
      "Step: 1833, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 1.5792808532714844\n",
      "Step: 1834, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.8373210430145264\n",
      "Step: 1835, Loss: 0.9241604804992676, Accuracy: 1.0, Computation time: 2.2595746517181396\n",
      "Step: 1836, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 2.0596041679382324\n",
      "Step: 1837, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.692380428314209\n",
      "Step: 1838, Loss: 0.9376195669174194, Accuracy: 0.96875, Computation time: 2.113441228866577\n",
      "Step: 1839, Loss: 0.9391681551933289, Accuracy: 0.96875, Computation time: 2.408351421356201\n",
      "Step: 1840, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.6152474880218506\n",
      "Step: 1841, Loss: 0.9160667657852173, Accuracy: 1.0, Computation time: 1.4921138286590576\n",
      "Step: 1842, Loss: 0.9160728454589844, Accuracy: 1.0, Computation time: 1.6582598686218262\n",
      "Step: 1843, Loss: 0.9375565648078918, Accuracy: 0.96875, Computation time: 1.9839143753051758\n",
      "Step: 1844, Loss: 0.9160161018371582, Accuracy: 1.0, Computation time: 2.0513105392456055\n",
      "Step: 1845, Loss: 0.9161827564239502, Accuracy: 1.0, Computation time: 2.55540132522583\n",
      "Step: 1846, Loss: 0.9160297513008118, Accuracy: 1.0, Computation time: 1.844858169555664\n",
      "Step: 1847, Loss: 0.9161462783813477, Accuracy: 1.0, Computation time: 1.7600162029266357\n",
      "Step: 1848, Loss: 0.9620434045791626, Accuracy: 0.9375, Computation time: 2.6609034538269043\n",
      "Step: 1849, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.9316339492797852\n",
      "Step: 1850, Loss: 0.916667103767395, Accuracy: 1.0, Computation time: 2.3961246013641357\n",
      "Step: 1851, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 2.134777784347534\n",
      "Step: 1852, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 2.038344621658325\n",
      "Step: 1853, Loss: 0.9159741997718811, Accuracy: 1.0, Computation time: 1.8555171489715576\n",
      "Step: 1854, Loss: 0.9159480333328247, Accuracy: 1.0, Computation time: 2.4841015338897705\n",
      "Step: 1855, Loss: 0.9261853694915771, Accuracy: 0.96875, Computation time: 2.669196844100952\n",
      "Step: 1856, Loss: 0.9469218254089355, Accuracy: 0.96875, Computation time: 1.9485187530517578\n",
      "Step: 1857, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.8831801414489746\n",
      "Step: 1858, Loss: 0.916118323802948, Accuracy: 1.0, Computation time: 2.0888137817382812\n",
      "Step: 1859, Loss: 0.9176391363143921, Accuracy: 1.0, Computation time: 1.9427294731140137\n",
      "Step: 1860, Loss: 0.9277352094650269, Accuracy: 0.96875, Computation time: 2.183901786804199\n",
      "Step: 1861, Loss: 0.9372081160545349, Accuracy: 0.96875, Computation time: 2.0350492000579834\n",
      "Step: 1862, Loss: 0.9159846305847168, Accuracy: 1.0, Computation time: 2.6906728744506836\n",
      "Step: 1863, Loss: 0.9390390515327454, Accuracy: 0.96875, Computation time: 2.414036989212036\n",
      "Step: 1864, Loss: 0.937684953212738, Accuracy: 0.96875, Computation time: 2.3376924991607666\n",
      "Step: 1865, Loss: 0.9173417687416077, Accuracy: 1.0, Computation time: 2.2379894256591797\n",
      "Step: 1866, Loss: 0.9164140820503235, Accuracy: 1.0, Computation time: 2.4621644020080566\n",
      "Step: 1867, Loss: 0.9162108898162842, Accuracy: 1.0, Computation time: 1.932117223739624\n",
      "Step: 1868, Loss: 0.9320046901702881, Accuracy: 0.96875, Computation time: 1.9636249542236328\n",
      "Step: 1869, Loss: 0.9376418590545654, Accuracy: 0.96875, Computation time: 1.7386724948883057\n",
      "Step: 1870, Loss: 0.9179831147193909, Accuracy: 1.0, Computation time: 2.2631189823150635\n",
      "Step: 1871, Loss: 0.9377883672714233, Accuracy: 0.96875, Computation time: 1.7302329540252686\n",
      "Step: 1872, Loss: 0.9160130023956299, Accuracy: 1.0, Computation time: 1.9953761100769043\n",
      "Step: 1873, Loss: 0.9162024259567261, Accuracy: 1.0, Computation time: 2.0074872970581055\n",
      "Step: 1874, Loss: 0.920248806476593, Accuracy: 1.0, Computation time: 3.8112261295318604\n",
      "Step: 1875, Loss: 0.9175205826759338, Accuracy: 1.0, Computation time: 2.3754184246063232\n",
      "Step: 1876, Loss: 0.9383060932159424, Accuracy: 0.96875, Computation time: 2.7240703105926514\n",
      "Step: 1877, Loss: 0.9163960218429565, Accuracy: 1.0, Computation time: 1.7628467082977295\n",
      "Step: 1878, Loss: 0.9161684513092041, Accuracy: 1.0, Computation time: 1.8744382858276367\n",
      "Step: 1879, Loss: 0.9226208329200745, Accuracy: 1.0, Computation time: 2.19687819480896\n",
      "Step: 1880, Loss: 0.924449622631073, Accuracy: 1.0, Computation time: 2.261822462081909\n",
      "Step: 1881, Loss: 0.9160482287406921, Accuracy: 1.0, Computation time: 1.699841022491455\n",
      "Step: 1882, Loss: 0.916694164276123, Accuracy: 1.0, Computation time: 1.9461498260498047\n",
      "Step: 1883, Loss: 0.9164361953735352, Accuracy: 1.0, Computation time: 2.0833539962768555\n",
      "Step: 1884, Loss: 0.9163278341293335, Accuracy: 1.0, Computation time: 1.6250081062316895\n",
      "Step: 1885, Loss: 0.9163175821304321, Accuracy: 1.0, Computation time: 1.6680290699005127\n",
      "Step: 1886, Loss: 0.9164516925811768, Accuracy: 1.0, Computation time: 1.949946403503418\n",
      "Step: 1887, Loss: 0.9339284896850586, Accuracy: 0.96875, Computation time: 2.202824831008911\n",
      "Step: 1888, Loss: 0.9161217212677002, Accuracy: 1.0, Computation time: 1.7951300144195557\n",
      "Step: 1889, Loss: 0.916351854801178, Accuracy: 1.0, Computation time: 2.6660125255584717\n",
      "Step: 1890, Loss: 0.916865348815918, Accuracy: 1.0, Computation time: 1.6094529628753662\n",
      "Step: 1891, Loss: 0.9160174131393433, Accuracy: 1.0, Computation time: 1.7442333698272705\n",
      "Step: 1892, Loss: 0.9201913475990295, Accuracy: 1.0, Computation time: 1.8342201709747314\n",
      "Step: 1893, Loss: 0.9159404039382935, Accuracy: 1.0, Computation time: 1.9770195484161377\n",
      "Step: 1894, Loss: 0.9424737691879272, Accuracy: 0.96875, Computation time: 1.9334027767181396\n",
      "Step: 1895, Loss: 0.9167551398277283, Accuracy: 1.0, Computation time: 1.6392884254455566\n",
      "Step: 1896, Loss: 0.9160698056221008, Accuracy: 1.0, Computation time: 1.991002082824707\n",
      "Step: 1897, Loss: 0.9162368774414062, Accuracy: 1.0, Computation time: 1.6660311222076416\n",
      "Step: 1898, Loss: 0.9159873723983765, Accuracy: 1.0, Computation time: 1.5771050453186035\n",
      "Step: 1899, Loss: 0.9160860776901245, Accuracy: 1.0, Computation time: 1.6245911121368408\n",
      "Step: 1900, Loss: 0.9159936904907227, Accuracy: 1.0, Computation time: 1.461338758468628\n",
      "Step: 1901, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.4266448020935059\n",
      "Step: 1902, Loss: 0.9160604476928711, Accuracy: 1.0, Computation time: 1.452345848083496\n",
      "Step: 1903, Loss: 0.9159470796585083, Accuracy: 1.0, Computation time: 1.483384609222412\n",
      "Step: 1904, Loss: 0.9379522800445557, Accuracy: 0.96875, Computation time: 2.00317120552063\n",
      "Step: 1905, Loss: 0.9159694314002991, Accuracy: 1.0, Computation time: 1.6912648677825928\n",
      "Step: 1906, Loss: 0.9159669876098633, Accuracy: 1.0, Computation time: 1.666382074356079\n",
      "Step: 1907, Loss: 0.9159356951713562, Accuracy: 1.0, Computation time: 2.2120652198791504\n",
      "Step: 1908, Loss: 0.9385863542556763, Accuracy: 0.96875, Computation time: 2.3652477264404297\n",
      "Step: 1909, Loss: 0.9161441922187805, Accuracy: 1.0, Computation time: 1.4368386268615723\n",
      "Step: 1910, Loss: 0.9316703677177429, Accuracy: 0.96875, Computation time: 2.006993293762207\n",
      "Step: 1911, Loss: 0.9173275232315063, Accuracy: 1.0, Computation time: 2.169698476791382\n",
      "Step: 1912, Loss: 0.915972113609314, Accuracy: 1.0, Computation time: 1.8196773529052734\n",
      "Step: 1913, Loss: 0.9364155530929565, Accuracy: 0.96875, Computation time: 1.8609752655029297\n",
      "Step: 1914, Loss: 0.9160782694816589, Accuracy: 1.0, Computation time: 1.8792173862457275\n",
      "Step: 1915, Loss: 0.9223051071166992, Accuracy: 1.0, Computation time: 1.771803855895996\n",
      "Step: 1916, Loss: 0.9160257577896118, Accuracy: 1.0, Computation time: 1.9237205982208252\n",
      "Step: 1917, Loss: 0.9170445203781128, Accuracy: 1.0, Computation time: 2.0511255264282227\n",
      "Step: 1918, Loss: 0.9159603714942932, Accuracy: 1.0, Computation time: 1.6874935626983643\n",
      "Step: 1919, Loss: 0.9249921441078186, Accuracy: 1.0, Computation time: 1.8273241519927979\n",
      "Step: 1920, Loss: 0.9161554574966431, Accuracy: 1.0, Computation time: 1.7259864807128906\n",
      "Step: 1921, Loss: 0.9169937968254089, Accuracy: 1.0, Computation time: 1.7106807231903076\n",
      "Step: 1922, Loss: 0.9348251223564148, Accuracy: 0.96875, Computation time: 2.50287127494812\n",
      "Step: 1923, Loss: 0.9378309845924377, Accuracy: 0.96875, Computation time: 1.5716431140899658\n",
      "Step: 1924, Loss: 0.9165362119674683, Accuracy: 1.0, Computation time: 1.826500415802002\n",
      "Step: 1925, Loss: 0.9166443943977356, Accuracy: 1.0, Computation time: 2.8084728717803955\n",
      "Step: 1926, Loss: 0.9168359637260437, Accuracy: 1.0, Computation time: 2.0065479278564453\n",
      "Step: 1927, Loss: 0.9170118570327759, Accuracy: 1.0, Computation time: 1.8913609981536865\n",
      "Step: 1928, Loss: 0.9227544665336609, Accuracy: 1.0, Computation time: 2.686361789703369\n",
      "Step: 1929, Loss: 0.9209848642349243, Accuracy: 1.0, Computation time: 1.9967305660247803\n",
      "Step: 1930, Loss: 0.9165773391723633, Accuracy: 1.0, Computation time: 2.316822052001953\n",
      "Step: 1931, Loss: 0.9163975119590759, Accuracy: 1.0, Computation time: 2.4501214027404785\n",
      "Step: 1932, Loss: 0.9162372946739197, Accuracy: 1.0, Computation time: 1.9180183410644531\n",
      "Step: 1933, Loss: 0.9161998629570007, Accuracy: 1.0, Computation time: 2.4696741104125977\n",
      "Step: 1934, Loss: 0.916215181350708, Accuracy: 1.0, Computation time: 2.3311500549316406\n",
      "Step: 1935, Loss: 0.9161651730537415, Accuracy: 1.0, Computation time: 2.323530435562134\n",
      "Step: 1936, Loss: 0.9381695985794067, Accuracy: 0.96875, Computation time: 2.167762041091919\n",
      "Step: 1937, Loss: 0.9374802708625793, Accuracy: 0.96875, Computation time: 2.2423508167266846\n",
      "Step: 1938, Loss: 0.916458010673523, Accuracy: 1.0, Computation time: 2.1965370178222656\n",
      "Step: 1939, Loss: 0.9163199067115784, Accuracy: 1.0, Computation time: 2.395918607711792\n",
      "Step: 1940, Loss: 0.9191394448280334, Accuracy: 1.0, Computation time: 2.3817570209503174\n",
      "Step: 1941, Loss: 0.9208701252937317, Accuracy: 1.0, Computation time: 1.994553804397583\n",
      "Step: 1942, Loss: 0.9163267612457275, Accuracy: 1.0, Computation time: 2.7948246002197266\n",
      "Step: 1943, Loss: 0.9164626002311707, Accuracy: 1.0, Computation time: 2.584467887878418\n",
      "Step: 1944, Loss: 0.9173694849014282, Accuracy: 1.0, Computation time: 2.3539414405822754\n",
      "Step: 1945, Loss: 0.9163180589675903, Accuracy: 1.0, Computation time: 2.2806594371795654\n",
      "Step: 1946, Loss: 0.9163260459899902, Accuracy: 1.0, Computation time: 2.0209109783172607\n",
      "########################\n",
      "Test loss: 1.113850474357605, Test Accuracy_epoch14: 0.7124239802360535\n",
      "########################\n",
      "Step: 1947, Loss: 0.9163875579833984, Accuracy: 1.0, Computation time: 2.224871873855591\n",
      "Step: 1948, Loss: 0.9162461757659912, Accuracy: 1.0, Computation time: 1.9489719867706299\n",
      "Step: 1949, Loss: 0.9160585999488831, Accuracy: 1.0, Computation time: 2.099592685699463\n",
      "Step: 1950, Loss: 0.9273767471313477, Accuracy: 0.96875, Computation time: 2.445558547973633\n",
      "Step: 1951, Loss: 0.9161484837532043, Accuracy: 1.0, Computation time: 2.1981921195983887\n",
      "Step: 1952, Loss: 0.9349586963653564, Accuracy: 0.96875, Computation time: 2.632758855819702\n",
      "Step: 1953, Loss: 0.9163962602615356, Accuracy: 1.0, Computation time: 2.8586597442626953\n",
      "Step: 1954, Loss: 0.9160075783729553, Accuracy: 1.0, Computation time: 1.9632680416107178\n",
      "Step: 1955, Loss: 0.9163082242012024, Accuracy: 1.0, Computation time: 1.9761426448822021\n",
      "Step: 1956, Loss: 0.9164583086967468, Accuracy: 1.0, Computation time: 2.7071428298950195\n",
      "Step: 1957, Loss: 0.9376517534255981, Accuracy: 0.96875, Computation time: 1.7525804042816162\n",
      "Step: 1958, Loss: 0.9375763535499573, Accuracy: 0.96875, Computation time: 2.178840398788452\n",
      "Step: 1959, Loss: 0.9159678816795349, Accuracy: 1.0, Computation time: 2.2708182334899902\n",
      "Step: 1960, Loss: 0.91609126329422, Accuracy: 1.0, Computation time: 2.0211565494537354\n",
      "Step: 1961, Loss: 0.9161953926086426, Accuracy: 1.0, Computation time: 2.1597366333007812\n",
      "Step: 1962, Loss: 0.9159862995147705, Accuracy: 1.0, Computation time: 1.8753368854522705\n",
      "Step: 1963, Loss: 0.9356011748313904, Accuracy: 0.96875, Computation time: 2.090794086456299\n",
      "Step: 1964, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 1.8341305255889893\n",
      "Step: 1965, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.9893782138824463\n",
      "Step: 1966, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 2.2566163539886475\n",
      "Step: 1967, Loss: 0.9375464916229248, Accuracy: 0.96875, Computation time: 2.1047987937927246\n",
      "Step: 1968, Loss: 0.9160346388816833, Accuracy: 1.0, Computation time: 2.772409439086914\n",
      "Step: 1969, Loss: 0.9159672856330872, Accuracy: 1.0, Computation time: 1.8700706958770752\n",
      "Step: 1970, Loss: 0.9159708023071289, Accuracy: 1.0, Computation time: 1.8286917209625244\n",
      "Step: 1971, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 1.9378690719604492\n",
      "Step: 1972, Loss: 0.9167649745941162, Accuracy: 1.0, Computation time: 1.8261127471923828\n",
      "Step: 1973, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.9867806434631348\n",
      "Step: 1974, Loss: 0.9375956058502197, Accuracy: 0.96875, Computation time: 1.8874783515930176\n",
      "Step: 1975, Loss: 0.9160383343696594, Accuracy: 1.0, Computation time: 1.6501529216766357\n",
      "Step: 1976, Loss: 0.9159404635429382, Accuracy: 1.0, Computation time: 2.0765628814697266\n",
      "Step: 1977, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.6117775440216064\n",
      "Step: 1978, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 1.8871335983276367\n",
      "Step: 1979, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 2.135138750076294\n",
      "Step: 1980, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.7106757164001465\n",
      "Step: 1981, Loss: 0.9291266798973083, Accuracy: 1.0, Computation time: 2.6221301555633545\n",
      "Step: 1982, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 2.0505263805389404\n",
      "Step: 1983, Loss: 0.9372552633285522, Accuracy: 0.96875, Computation time: 1.9193394184112549\n",
      "Step: 1984, Loss: 0.937365710735321, Accuracy: 0.96875, Computation time: 1.291391372680664\n",
      "Step: 1985, Loss: 0.937859058380127, Accuracy: 0.96875, Computation time: 1.6348822116851807\n",
      "Step: 1986, Loss: 0.9160280227661133, Accuracy: 1.0, Computation time: 1.711855411529541\n",
      "Step: 1987, Loss: 0.9342880249023438, Accuracy: 0.96875, Computation time: 1.7934086322784424\n",
      "Step: 1988, Loss: 0.9159613847732544, Accuracy: 1.0, Computation time: 2.2433841228485107\n",
      "Step: 1989, Loss: 0.9206053614616394, Accuracy: 1.0, Computation time: 2.2268989086151123\n",
      "Step: 1990, Loss: 0.919513463973999, Accuracy: 1.0, Computation time: 1.9118516445159912\n",
      "Step: 1991, Loss: 0.9383172988891602, Accuracy: 0.96875, Computation time: 2.323427438735962\n",
      "Step: 1992, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.7918519973754883\n",
      "Step: 1993, Loss: 0.9161151051521301, Accuracy: 1.0, Computation time: 2.2260260581970215\n",
      "Step: 1994, Loss: 0.9160748720169067, Accuracy: 1.0, Computation time: 1.8464763164520264\n",
      "Step: 1995, Loss: 0.9375389814376831, Accuracy: 0.96875, Computation time: 1.5507745742797852\n",
      "Step: 1996, Loss: 0.915984034538269, Accuracy: 1.0, Computation time: 1.620692253112793\n",
      "Step: 1997, Loss: 0.9177685976028442, Accuracy: 1.0, Computation time: 2.634465456008911\n",
      "Step: 1998, Loss: 0.938098669052124, Accuracy: 0.96875, Computation time: 2.3367350101470947\n",
      "Step: 1999, Loss: 0.9379943013191223, Accuracy: 0.96875, Computation time: 1.4791712760925293\n",
      "Step: 2000, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 1.5012445449829102\n",
      "Step: 2001, Loss: 0.916128396987915, Accuracy: 1.0, Computation time: 1.6901752948760986\n",
      "Step: 2002, Loss: 0.9219672083854675, Accuracy: 1.0, Computation time: 1.9005372524261475\n",
      "Step: 2003, Loss: 0.9160706996917725, Accuracy: 1.0, Computation time: 1.908663272857666\n",
      "Step: 2004, Loss: 0.9159813523292542, Accuracy: 1.0, Computation time: 2.2710061073303223\n",
      "Step: 2005, Loss: 0.916146457195282, Accuracy: 1.0, Computation time: 1.4509532451629639\n",
      "Step: 2006, Loss: 0.9160284399986267, Accuracy: 1.0, Computation time: 1.329005241394043\n",
      "Step: 2007, Loss: 0.9160122275352478, Accuracy: 1.0, Computation time: 1.6267175674438477\n",
      "Step: 2008, Loss: 0.9160347580909729, Accuracy: 1.0, Computation time: 1.6012256145477295\n",
      "Step: 2009, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 1.3594624996185303\n",
      "Step: 2010, Loss: 0.9160442352294922, Accuracy: 1.0, Computation time: 1.634807825088501\n",
      "Step: 2011, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 1.5923147201538086\n",
      "Step: 2012, Loss: 0.9342228174209595, Accuracy: 0.96875, Computation time: 2.185822010040283\n",
      "Step: 2013, Loss: 0.9344941973686218, Accuracy: 0.96875, Computation time: 2.6365363597869873\n",
      "Step: 2014, Loss: 0.9160556197166443, Accuracy: 1.0, Computation time: 1.5664153099060059\n",
      "Step: 2015, Loss: 0.9159258008003235, Accuracy: 1.0, Computation time: 1.487175703048706\n",
      "Step: 2016, Loss: 0.9162898063659668, Accuracy: 1.0, Computation time: 2.31463885307312\n",
      "Step: 2017, Loss: 0.9456543922424316, Accuracy: 0.9375, Computation time: 2.847194194793701\n",
      "Step: 2018, Loss: 0.9161441922187805, Accuracy: 1.0, Computation time: 1.4737071990966797\n",
      "Step: 2019, Loss: 0.9160700440406799, Accuracy: 1.0, Computation time: 1.3519070148468018\n",
      "Step: 2020, Loss: 0.9338930249214172, Accuracy: 0.96875, Computation time: 1.9823088645935059\n",
      "Step: 2021, Loss: 0.9161161780357361, Accuracy: 1.0, Computation time: 1.60028076171875\n",
      "Step: 2022, Loss: 0.9165682792663574, Accuracy: 1.0, Computation time: 1.6447572708129883\n",
      "Step: 2023, Loss: 0.9367667436599731, Accuracy: 0.96875, Computation time: 1.9703030586242676\n",
      "Step: 2024, Loss: 0.9160946011543274, Accuracy: 1.0, Computation time: 1.7167925834655762\n",
      "Step: 2025, Loss: 0.9160423874855042, Accuracy: 1.0, Computation time: 1.9576106071472168\n",
      "Step: 2026, Loss: 0.9160009026527405, Accuracy: 1.0, Computation time: 1.7195465564727783\n",
      "Step: 2027, Loss: 0.9159781336784363, Accuracy: 1.0, Computation time: 1.7738213539123535\n",
      "Step: 2028, Loss: 0.9161409735679626, Accuracy: 1.0, Computation time: 1.522263765335083\n",
      "Step: 2029, Loss: 0.9590585827827454, Accuracy: 0.9375, Computation time: 1.6208677291870117\n",
      "Step: 2030, Loss: 0.9275445342063904, Accuracy: 1.0, Computation time: 2.5622060298919678\n",
      "Step: 2031, Loss: 0.9269499778747559, Accuracy: 0.96875, Computation time: 2.1926567554473877\n",
      "Step: 2032, Loss: 0.9165481328964233, Accuracy: 1.0, Computation time: 1.4654045104980469\n",
      "Step: 2033, Loss: 0.9385842680931091, Accuracy: 0.96875, Computation time: 1.71964693069458\n",
      "Step: 2034, Loss: 0.9249745607376099, Accuracy: 1.0, Computation time: 2.0090949535369873\n",
      "Step: 2035, Loss: 0.9163850545883179, Accuracy: 1.0, Computation time: 1.621006965637207\n",
      "Step: 2036, Loss: 0.9163792133331299, Accuracy: 1.0, Computation time: 1.306556224822998\n",
      "Step: 2037, Loss: 0.9278061389923096, Accuracy: 0.96875, Computation time: 1.8590888977050781\n",
      "Step: 2038, Loss: 0.9185577630996704, Accuracy: 1.0, Computation time: 2.5141983032226562\n",
      "Step: 2039, Loss: 0.937609076499939, Accuracy: 0.96875, Computation time: 1.8807518482208252\n",
      "Step: 2040, Loss: 0.9160191416740417, Accuracy: 1.0, Computation time: 1.7369909286499023\n",
      "Step: 2041, Loss: 0.9375436902046204, Accuracy: 0.96875, Computation time: 1.9759464263916016\n",
      "Step: 2042, Loss: 0.9159731864929199, Accuracy: 1.0, Computation time: 1.6966352462768555\n",
      "Step: 2043, Loss: 0.9179242253303528, Accuracy: 1.0, Computation time: 2.1118178367614746\n",
      "Step: 2044, Loss: 0.91651850938797, Accuracy: 1.0, Computation time: 1.9391419887542725\n",
      "Step: 2045, Loss: 0.9161219596862793, Accuracy: 1.0, Computation time: 1.7977702617645264\n",
      "Step: 2046, Loss: 0.9161824584007263, Accuracy: 1.0, Computation time: 2.159238338470459\n",
      "Step: 2047, Loss: 0.9222817420959473, Accuracy: 1.0, Computation time: 1.8617444038391113\n",
      "Step: 2048, Loss: 0.9161553978919983, Accuracy: 1.0, Computation time: 1.7145795822143555\n",
      "Step: 2049, Loss: 0.9378198981285095, Accuracy: 0.96875, Computation time: 1.6973226070404053\n",
      "Step: 2050, Loss: 0.9332091808319092, Accuracy: 0.96875, Computation time: 2.0672266483306885\n",
      "Step: 2051, Loss: 0.916028618812561, Accuracy: 1.0, Computation time: 1.5086376667022705\n",
      "Step: 2052, Loss: 0.9749433398246765, Accuracy: 0.90625, Computation time: 2.070706605911255\n",
      "Step: 2053, Loss: 0.9159908890724182, Accuracy: 1.0, Computation time: 1.41290283203125\n",
      "Step: 2054, Loss: 0.9160314798355103, Accuracy: 1.0, Computation time: 1.9290213584899902\n",
      "Step: 2055, Loss: 0.915974497795105, Accuracy: 1.0, Computation time: 1.7353787422180176\n",
      "Step: 2056, Loss: 0.9378641247749329, Accuracy: 0.96875, Computation time: 2.9759979248046875\n",
      "Step: 2057, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.7134959697723389\n",
      "Step: 2058, Loss: 0.9161460399627686, Accuracy: 1.0, Computation time: 1.9147541522979736\n",
      "Step: 2059, Loss: 0.9209930300712585, Accuracy: 1.0, Computation time: 1.9924798011779785\n",
      "Step: 2060, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 1.9453926086425781\n",
      "Step: 2061, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 1.8374669551849365\n",
      "Step: 2062, Loss: 0.9163983464241028, Accuracy: 1.0, Computation time: 2.0023913383483887\n",
      "Step: 2063, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.6357030868530273\n",
      "Step: 2064, Loss: 0.9375683069229126, Accuracy: 0.96875, Computation time: 1.8552875518798828\n",
      "Step: 2065, Loss: 0.9160745143890381, Accuracy: 1.0, Computation time: 1.7874724864959717\n",
      "Step: 2066, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.5266215801239014\n",
      "Step: 2067, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.6296710968017578\n",
      "Step: 2068, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.7107887268066406\n",
      "Step: 2069, Loss: 0.9160019159317017, Accuracy: 1.0, Computation time: 1.5472328662872314\n",
      "Step: 2070, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 1.7900447845458984\n",
      "Step: 2071, Loss: 0.916016161441803, Accuracy: 1.0, Computation time: 1.4918954372406006\n",
      "Step: 2072, Loss: 0.9161449670791626, Accuracy: 1.0, Computation time: 1.6030058860778809\n",
      "Step: 2073, Loss: 0.9452783465385437, Accuracy: 0.96875, Computation time: 1.838538646697998\n",
      "Step: 2074, Loss: 0.9347644448280334, Accuracy: 0.96875, Computation time: 1.7531914710998535\n",
      "Step: 2075, Loss: 0.9160013794898987, Accuracy: 1.0, Computation time: 1.5465679168701172\n",
      "Step: 2076, Loss: 0.9160217642784119, Accuracy: 1.0, Computation time: 1.65720534324646\n",
      "Step: 2077, Loss: 0.9160321950912476, Accuracy: 1.0, Computation time: 2.0246851444244385\n",
      "Step: 2078, Loss: 0.916179895401001, Accuracy: 1.0, Computation time: 2.0078866481781006\n",
      "Step: 2079, Loss: 0.9162434935569763, Accuracy: 1.0, Computation time: 2.4110169410705566\n",
      "Step: 2080, Loss: 0.9161293506622314, Accuracy: 1.0, Computation time: 1.732701063156128\n",
      "Step: 2081, Loss: 0.9164702296257019, Accuracy: 1.0, Computation time: 2.0540082454681396\n",
      "Step: 2082, Loss: 0.9378088116645813, Accuracy: 0.96875, Computation time: 1.836216926574707\n",
      "Step: 2083, Loss: 0.9180600643157959, Accuracy: 1.0, Computation time: 1.77779221534729\n",
      "Step: 2084, Loss: 0.9373244047164917, Accuracy: 0.96875, Computation time: 1.7058131694793701\n",
      "Step: 2085, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.6851038932800293\n",
      "########################\n",
      "Test loss: 1.1202969551086426, Test Accuracy_epoch15: 0.703735888004303\n",
      "########################\n",
      "Step: 2086, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 1.6592748165130615\n",
      "Step: 2087, Loss: 0.9480041265487671, Accuracy: 0.9375, Computation time: 1.807919979095459\n",
      "Step: 2088, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.6380324363708496\n",
      "Step: 2089, Loss: 0.91595858335495, Accuracy: 1.0, Computation time: 1.6445262432098389\n",
      "Step: 2090, Loss: 0.917243242263794, Accuracy: 1.0, Computation time: 1.8984909057617188\n",
      "Step: 2091, Loss: 0.9159940481185913, Accuracy: 1.0, Computation time: 1.6707134246826172\n",
      "Step: 2092, Loss: 0.9159757494926453, Accuracy: 1.0, Computation time: 1.7650060653686523\n",
      "Step: 2093, Loss: 0.9167214035987854, Accuracy: 1.0, Computation time: 1.8349413871765137\n",
      "Step: 2094, Loss: 0.9159989953041077, Accuracy: 1.0, Computation time: 1.6845381259918213\n",
      "Step: 2095, Loss: 0.9159979820251465, Accuracy: 1.0, Computation time: 2.01118540763855\n",
      "Step: 2096, Loss: 0.9159456491470337, Accuracy: 1.0, Computation time: 1.9469459056854248\n",
      "Step: 2097, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 2.31170916557312\n",
      "Step: 2098, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 2.007591962814331\n",
      "Step: 2099, Loss: 0.9377034902572632, Accuracy: 0.96875, Computation time: 2.052280902862549\n",
      "Step: 2100, Loss: 0.9159582257270813, Accuracy: 1.0, Computation time: 1.8197083473205566\n",
      "Step: 2101, Loss: 0.9166865348815918, Accuracy: 1.0, Computation time: 1.9958090782165527\n",
      "Step: 2102, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.6610360145568848\n",
      "Step: 2103, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 1.8737573623657227\n",
      "Step: 2104, Loss: 0.9218912720680237, Accuracy: 1.0, Computation time: 2.259305000305176\n",
      "Step: 2105, Loss: 0.959272027015686, Accuracy: 0.9375, Computation time: 1.7205114364624023\n",
      "Step: 2106, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 2.0624289512634277\n",
      "Step: 2107, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.5261938571929932\n",
      "Step: 2108, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.7968244552612305\n",
      "Step: 2109, Loss: 0.9205417037010193, Accuracy: 1.0, Computation time: 2.4742465019226074\n",
      "Step: 2110, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 2.1783833503723145\n",
      "Step: 2111, Loss: 0.9160700440406799, Accuracy: 1.0, Computation time: 1.7716286182403564\n",
      "Step: 2112, Loss: 0.9374890923500061, Accuracy: 0.96875, Computation time: 2.145277261734009\n",
      "Step: 2113, Loss: 0.9161474108695984, Accuracy: 1.0, Computation time: 2.407559633255005\n",
      "Step: 2114, Loss: 0.9165357947349548, Accuracy: 1.0, Computation time: 1.675342082977295\n",
      "Step: 2115, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 1.561500072479248\n",
      "Step: 2116, Loss: 0.9366662502288818, Accuracy: 0.96875, Computation time: 1.407182216644287\n",
      "Step: 2117, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 1.8564412593841553\n",
      "Step: 2118, Loss: 0.9374196529388428, Accuracy: 0.96875, Computation time: 1.8993346691131592\n",
      "Step: 2119, Loss: 0.9166733622550964, Accuracy: 1.0, Computation time: 1.639073371887207\n",
      "Step: 2120, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.951812505722046\n",
      "Step: 2121, Loss: 0.9162136316299438, Accuracy: 1.0, Computation time: 1.7061042785644531\n",
      "Step: 2122, Loss: 0.9216185212135315, Accuracy: 1.0, Computation time: 1.8161871433258057\n",
      "Step: 2123, Loss: 0.9367853403091431, Accuracy: 0.96875, Computation time: 2.2138519287109375\n",
      "Step: 2124, Loss: 0.9377119541168213, Accuracy: 0.96875, Computation time: 1.5457782745361328\n",
      "Step: 2125, Loss: 0.9165002107620239, Accuracy: 1.0, Computation time: 1.7307255268096924\n",
      "Step: 2126, Loss: 0.9269341230392456, Accuracy: 0.96875, Computation time: 2.4238576889038086\n",
      "Step: 2127, Loss: 0.9165724515914917, Accuracy: 1.0, Computation time: 1.9665822982788086\n",
      "Step: 2128, Loss: 0.9377993941307068, Accuracy: 0.96875, Computation time: 1.9566502571105957\n",
      "Step: 2129, Loss: 0.9407546520233154, Accuracy: 0.96875, Computation time: 2.3107900619506836\n",
      "Step: 2130, Loss: 0.9159814119338989, Accuracy: 1.0, Computation time: 1.6124789714813232\n",
      "Step: 2131, Loss: 0.9219384789466858, Accuracy: 1.0, Computation time: 1.6567246913909912\n",
      "Step: 2132, Loss: 0.9161613583564758, Accuracy: 1.0, Computation time: 1.6435599327087402\n",
      "Step: 2133, Loss: 0.9162105321884155, Accuracy: 1.0, Computation time: 1.6629860401153564\n",
      "Step: 2134, Loss: 0.9374045133590698, Accuracy: 0.96875, Computation time: 1.8776557445526123\n",
      "Step: 2135, Loss: 0.9162315130233765, Accuracy: 1.0, Computation time: 1.748183250427246\n",
      "Step: 2136, Loss: 0.9161148071289062, Accuracy: 1.0, Computation time: 1.7218875885009766\n",
      "Step: 2137, Loss: 0.9159804582595825, Accuracy: 1.0, Computation time: 1.556887149810791\n",
      "Step: 2138, Loss: 0.915932297706604, Accuracy: 1.0, Computation time: 2.2837817668914795\n",
      "Step: 2139, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.5550727844238281\n",
      "Step: 2140, Loss: 0.9159336686134338, Accuracy: 1.0, Computation time: 1.5457673072814941\n",
      "Step: 2141, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.9823448657989502\n",
      "Step: 2142, Loss: 0.9227471351623535, Accuracy: 1.0, Computation time: 1.9888134002685547\n",
      "Step: 2143, Loss: 0.9367485046386719, Accuracy: 0.96875, Computation time: 1.635077714920044\n",
      "Step: 2144, Loss: 0.9372808337211609, Accuracy: 0.96875, Computation time: 1.714963674545288\n",
      "Step: 2145, Loss: 0.9159651398658752, Accuracy: 1.0, Computation time: 1.9009644985198975\n",
      "Step: 2146, Loss: 0.9377004504203796, Accuracy: 0.96875, Computation time: 2.4489500522613525\n",
      "Step: 2147, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 1.699739694595337\n",
      "Step: 2148, Loss: 0.9163274168968201, Accuracy: 1.0, Computation time: 1.700535774230957\n",
      "Step: 2149, Loss: 0.9175430536270142, Accuracy: 1.0, Computation time: 2.6312644481658936\n",
      "Step: 2150, Loss: 0.9380567669868469, Accuracy: 0.96875, Computation time: 1.8008224964141846\n",
      "Step: 2151, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 1.7127361297607422\n",
      "Step: 2152, Loss: 0.9163644909858704, Accuracy: 1.0, Computation time: 1.9129979610443115\n",
      "Step: 2153, Loss: 0.9375657439231873, Accuracy: 0.96875, Computation time: 1.7002956867218018\n",
      "Step: 2154, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.5439858436584473\n",
      "Step: 2155, Loss: 0.9162442684173584, Accuracy: 1.0, Computation time: 1.6434261798858643\n",
      "Step: 2156, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.554734706878662\n",
      "Step: 2157, Loss: 0.931898832321167, Accuracy: 0.96875, Computation time: 1.8958144187927246\n",
      "Step: 2158, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 1.8875324726104736\n",
      "Step: 2159, Loss: 0.9160152077674866, Accuracy: 1.0, Computation time: 1.7335305213928223\n",
      "Step: 2160, Loss: 0.9160283803939819, Accuracy: 1.0, Computation time: 1.5611259937286377\n",
      "Step: 2161, Loss: 0.9160965085029602, Accuracy: 1.0, Computation time: 1.4388504028320312\n",
      "Step: 2162, Loss: 0.9161527156829834, Accuracy: 1.0, Computation time: 1.8353817462921143\n",
      "Step: 2163, Loss: 0.9161385297775269, Accuracy: 1.0, Computation time: 2.416238784790039\n",
      "Step: 2164, Loss: 0.9377749562263489, Accuracy: 0.96875, Computation time: 1.6661090850830078\n",
      "Step: 2165, Loss: 0.9159070253372192, Accuracy: 1.0, Computation time: 1.5226261615753174\n",
      "Step: 2166, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 1.4307644367218018\n",
      "Step: 2167, Loss: 0.9159223437309265, Accuracy: 1.0, Computation time: 1.6725950241088867\n",
      "Step: 2168, Loss: 0.9372953772544861, Accuracy: 0.96875, Computation time: 1.9368305206298828\n",
      "Step: 2169, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 2.0902018547058105\n",
      "Step: 2170, Loss: 0.9159669876098633, Accuracy: 1.0, Computation time: 1.676236867904663\n",
      "Step: 2171, Loss: 0.9221459031105042, Accuracy: 1.0, Computation time: 1.888951063156128\n",
      "Step: 2172, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.6504449844360352\n",
      "Step: 2173, Loss: 0.9376259446144104, Accuracy: 0.96875, Computation time: 1.4761509895324707\n",
      "Step: 2174, Loss: 0.9160357713699341, Accuracy: 1.0, Computation time: 1.595621109008789\n",
      "Step: 2175, Loss: 0.9160542488098145, Accuracy: 1.0, Computation time: 1.6468191146850586\n",
      "Step: 2176, Loss: 0.9377793073654175, Accuracy: 0.96875, Computation time: 1.7331008911132812\n",
      "Step: 2177, Loss: 0.9376700520515442, Accuracy: 0.96875, Computation time: 1.6732232570648193\n",
      "Step: 2178, Loss: 0.9376723766326904, Accuracy: 0.96875, Computation time: 1.2958040237426758\n",
      "Step: 2179, Loss: 0.9159273505210876, Accuracy: 1.0, Computation time: 1.581279993057251\n",
      "Step: 2180, Loss: 0.9196991920471191, Accuracy: 1.0, Computation time: 1.5182437896728516\n",
      "Step: 2181, Loss: 0.9164187908172607, Accuracy: 1.0, Computation time: 1.5926544666290283\n",
      "Step: 2182, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 1.6811110973358154\n",
      "Step: 2183, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 1.4010567665100098\n",
      "Step: 2184, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 1.915102243423462\n",
      "Step: 2185, Loss: 0.9161053895950317, Accuracy: 1.0, Computation time: 1.8995740413665771\n",
      "Step: 2186, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 1.9959337711334229\n",
      "Step: 2187, Loss: 0.9161227345466614, Accuracy: 1.0, Computation time: 1.852478265762329\n",
      "Step: 2188, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.9415957927703857\n",
      "Step: 2189, Loss: 0.9380418062210083, Accuracy: 0.96875, Computation time: 2.5057528018951416\n",
      "Step: 2190, Loss: 0.9159565567970276, Accuracy: 1.0, Computation time: 1.8058786392211914\n",
      "Step: 2191, Loss: 0.9361851215362549, Accuracy: 0.96875, Computation time: 1.8564519882202148\n",
      "Step: 2192, Loss: 0.9159181714057922, Accuracy: 1.0, Computation time: 1.664940595626831\n",
      "Step: 2193, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.5054543018341064\n",
      "Step: 2194, Loss: 0.9166123867034912, Accuracy: 1.0, Computation time: 1.7369716167449951\n",
      "Step: 2195, Loss: 0.9350102543830872, Accuracy: 0.96875, Computation time: 1.5202531814575195\n",
      "Step: 2196, Loss: 0.9367612600326538, Accuracy: 0.96875, Computation time: 2.094203472137451\n",
      "Step: 2197, Loss: 0.9375355839729309, Accuracy: 0.96875, Computation time: 1.403860092163086\n",
      "Step: 2198, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.7503125667572021\n",
      "Step: 2199, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.554807186126709\n",
      "Step: 2200, Loss: 0.938174843788147, Accuracy: 0.96875, Computation time: 1.6015369892120361\n",
      "Step: 2201, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 1.7686386108398438\n",
      "Step: 2202, Loss: 0.9174486398696899, Accuracy: 1.0, Computation time: 1.56803560256958\n",
      "Step: 2203, Loss: 0.9158936738967896, Accuracy: 1.0, Computation time: 1.5855653285980225\n",
      "Step: 2204, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 1.5517759323120117\n",
      "Step: 2205, Loss: 0.9363331198692322, Accuracy: 0.96875, Computation time: 1.7921359539031982\n",
      "Step: 2206, Loss: 0.9376077651977539, Accuracy: 0.96875, Computation time: 1.5856537818908691\n",
      "Step: 2207, Loss: 0.9232375621795654, Accuracy: 1.0, Computation time: 1.920602798461914\n",
      "Step: 2208, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 1.84403657913208\n",
      "Step: 2209, Loss: 0.9353408813476562, Accuracy: 0.96875, Computation time: 1.8425679206848145\n",
      "Step: 2210, Loss: 0.9160653352737427, Accuracy: 1.0, Computation time: 1.6060254573822021\n",
      "Step: 2211, Loss: 0.9161399602890015, Accuracy: 1.0, Computation time: 1.887369155883789\n",
      "Step: 2212, Loss: 0.9208467602729797, Accuracy: 1.0, Computation time: 1.8725814819335938\n",
      "Step: 2213, Loss: 0.9160364866256714, Accuracy: 1.0, Computation time: 2.366145610809326\n",
      "Step: 2214, Loss: 0.9160478711128235, Accuracy: 1.0, Computation time: 1.5102767944335938\n",
      "Step: 2215, Loss: 0.9184860587120056, Accuracy: 1.0, Computation time: 1.9848332405090332\n",
      "Step: 2216, Loss: 0.9160909056663513, Accuracy: 1.0, Computation time: 1.9618480205535889\n",
      "Step: 2217, Loss: 0.9371787309646606, Accuracy: 0.96875, Computation time: 1.7378246784210205\n",
      "Step: 2218, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.7217254638671875\n",
      "Step: 2219, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.7106385231018066\n",
      "Step: 2220, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.5292766094207764\n",
      "Step: 2221, Loss: 0.9160392880439758, Accuracy: 1.0, Computation time: 1.8018672466278076\n",
      "Step: 2222, Loss: 0.9160543084144592, Accuracy: 1.0, Computation time: 1.561337947845459\n",
      "Step: 2223, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 1.5599613189697266\n",
      "########################\n",
      "Test loss: 1.1200801134109497, Test Accuracy_epoch16: 0.7002606391906738\n",
      "########################\n",
      "Step: 2224, Loss: 0.9159696102142334, Accuracy: 1.0, Computation time: 1.6750223636627197\n",
      "Step: 2225, Loss: 0.9507454037666321, Accuracy: 0.9375, Computation time: 2.3384125232696533\n",
      "Step: 2226, Loss: 0.9162361025810242, Accuracy: 1.0, Computation time: 2.208207368850708\n",
      "Step: 2227, Loss: 0.9160254001617432, Accuracy: 1.0, Computation time: 1.556344747543335\n",
      "Step: 2228, Loss: 0.9159692525863647, Accuracy: 1.0, Computation time: 1.7846481800079346\n",
      "Step: 2229, Loss: 0.9159892797470093, Accuracy: 1.0, Computation time: 1.2746682167053223\n",
      "Step: 2230, Loss: 0.9374104738235474, Accuracy: 0.96875, Computation time: 1.5270166397094727\n",
      "Step: 2231, Loss: 0.9159396886825562, Accuracy: 1.0, Computation time: 1.635481357574463\n",
      "Step: 2232, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 1.3972227573394775\n",
      "Step: 2233, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.6145269870758057\n",
      "Step: 2234, Loss: 0.9160991907119751, Accuracy: 1.0, Computation time: 1.5537669658660889\n",
      "Step: 2235, Loss: 0.9376183152198792, Accuracy: 0.96875, Computation time: 1.6926283836364746\n",
      "Step: 2236, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.8665752410888672\n",
      "Step: 2237, Loss: 0.9160643219947815, Accuracy: 1.0, Computation time: 1.618471384048462\n",
      "Step: 2238, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.674112319946289\n",
      "Step: 2239, Loss: 0.9159277081489563, Accuracy: 1.0, Computation time: 1.8448691368103027\n",
      "Step: 2240, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.695133924484253\n",
      "Step: 2241, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 1.6374685764312744\n",
      "Step: 2242, Loss: 0.9159346222877502, Accuracy: 1.0, Computation time: 1.5701847076416016\n",
      "Step: 2243, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 1.248739242553711\n",
      "Step: 2244, Loss: 0.9159946441650391, Accuracy: 1.0, Computation time: 1.6998059749603271\n",
      "Step: 2245, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 1.555159091949463\n",
      "Step: 2246, Loss: 0.9253620505332947, Accuracy: 0.96875, Computation time: 2.2452023029327393\n",
      "Step: 2247, Loss: 0.9359439015388489, Accuracy: 0.96875, Computation time: 1.6506903171539307\n",
      "Step: 2248, Loss: 0.9183605909347534, Accuracy: 1.0, Computation time: 2.834678888320923\n",
      "Step: 2249, Loss: 0.9162088632583618, Accuracy: 1.0, Computation time: 2.036858558654785\n",
      "Step: 2250, Loss: 0.9163549542427063, Accuracy: 1.0, Computation time: 1.5971803665161133\n",
      "Step: 2251, Loss: 0.9163182973861694, Accuracy: 1.0, Computation time: 2.1103949546813965\n",
      "Step: 2252, Loss: 0.9162728190422058, Accuracy: 1.0, Computation time: 1.9504868984222412\n",
      "Step: 2253, Loss: 0.937795877456665, Accuracy: 0.96875, Computation time: 1.5046958923339844\n",
      "Step: 2254, Loss: 0.9160113334655762, Accuracy: 1.0, Computation time: 1.5762357711791992\n",
      "Step: 2255, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.5325043201446533\n",
      "Step: 2256, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.6262285709381104\n",
      "Step: 2257, Loss: 0.9162038564682007, Accuracy: 1.0, Computation time: 1.4462530612945557\n",
      "Step: 2258, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 1.3945567607879639\n",
      "Step: 2259, Loss: 0.9160237312316895, Accuracy: 1.0, Computation time: 1.4442589282989502\n",
      "Step: 2260, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.5650789737701416\n",
      "Step: 2261, Loss: 0.9159334301948547, Accuracy: 1.0, Computation time: 1.2722322940826416\n",
      "Step: 2262, Loss: 0.9159854650497437, Accuracy: 1.0, Computation time: 1.5211076736450195\n",
      "Step: 2263, Loss: 0.9593941569328308, Accuracy: 0.9375, Computation time: 1.7611284255981445\n",
      "Step: 2264, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.4160621166229248\n",
      "Step: 2265, Loss: 0.9159231781959534, Accuracy: 1.0, Computation time: 2.5164318084716797\n",
      "Step: 2266, Loss: 0.9160012006759644, Accuracy: 1.0, Computation time: 1.4549200534820557\n",
      "Step: 2267, Loss: 0.9374072551727295, Accuracy: 0.96875, Computation time: 1.7163517475128174\n",
      "Step: 2268, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 1.3477144241333008\n",
      "Step: 2269, Loss: 0.9338273406028748, Accuracy: 0.96875, Computation time: 2.307190179824829\n",
      "Step: 2270, Loss: 0.9159324765205383, Accuracy: 1.0, Computation time: 1.6777632236480713\n",
      "Step: 2271, Loss: 0.9159960746765137, Accuracy: 1.0, Computation time: 1.8670542240142822\n",
      "Step: 2272, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.557352066040039\n",
      "Step: 2273, Loss: 0.9221500158309937, Accuracy: 1.0, Computation time: 1.6315736770629883\n",
      "Step: 2274, Loss: 0.9159409999847412, Accuracy: 1.0, Computation time: 1.4647629261016846\n",
      "Step: 2275, Loss: 0.915934145450592, Accuracy: 1.0, Computation time: 1.1899387836456299\n",
      "Step: 2276, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.712817907333374\n",
      "Step: 2277, Loss: 0.9159104228019714, Accuracy: 1.0, Computation time: 1.4563992023468018\n",
      "Step: 2278, Loss: 0.9160321950912476, Accuracy: 1.0, Computation time: 1.4444880485534668\n",
      "Step: 2279, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 1.8843715190887451\n",
      "Step: 2280, Loss: 0.916662871837616, Accuracy: 1.0, Computation time: 1.5474615097045898\n",
      "Step: 2281, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.6029939651489258\n",
      "Step: 2282, Loss: 0.9367455840110779, Accuracy: 0.96875, Computation time: 1.691039800643921\n",
      "Step: 2283, Loss: 0.9368554949760437, Accuracy: 0.96875, Computation time: 1.22135329246521\n",
      "Step: 2284, Loss: 0.9375309348106384, Accuracy: 0.96875, Computation time: 1.815371036529541\n",
      "Step: 2285, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.715047836303711\n",
      "Step: 2286, Loss: 0.9160322546958923, Accuracy: 1.0, Computation time: 1.3979172706604004\n",
      "Step: 2287, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.5717966556549072\n",
      "Step: 2288, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 1.4926674365997314\n",
      "Step: 2289, Loss: 0.9162896871566772, Accuracy: 1.0, Computation time: 1.574704647064209\n",
      "Step: 2290, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.5197501182556152\n",
      "Step: 2291, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.4532520771026611\n",
      "Step: 2292, Loss: 0.9165340662002563, Accuracy: 1.0, Computation time: 1.8090555667877197\n",
      "Step: 2293, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.793980598449707\n",
      "Step: 2294, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.7095458507537842\n",
      "Step: 2295, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 1.564244031906128\n",
      "Step: 2296, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.623600959777832\n",
      "Step: 2297, Loss: 0.9375893473625183, Accuracy: 0.96875, Computation time: 2.322969913482666\n",
      "Step: 2298, Loss: 0.919560432434082, Accuracy: 1.0, Computation time: 1.7606582641601562\n",
      "Step: 2299, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.4629147052764893\n",
      "Step: 2300, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.6977627277374268\n",
      "Step: 2301, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.3537614345550537\n",
      "Step: 2302, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 1.5137553215026855\n",
      "Step: 2303, Loss: 0.9159199595451355, Accuracy: 1.0, Computation time: 1.74180006980896\n",
      "Step: 2304, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.4891254901885986\n",
      "Step: 2305, Loss: 0.937079668045044, Accuracy: 0.96875, Computation time: 2.048288106918335\n",
      "Step: 2306, Loss: 0.9372934699058533, Accuracy: 0.96875, Computation time: 1.3248059749603271\n",
      "Step: 2307, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 1.612877368927002\n",
      "Step: 2308, Loss: 0.9276343584060669, Accuracy: 0.96875, Computation time: 1.6076226234436035\n",
      "Step: 2309, Loss: 0.9163088202476501, Accuracy: 1.0, Computation time: 2.285078287124634\n",
      "Step: 2310, Loss: 0.9166041612625122, Accuracy: 1.0, Computation time: 2.2333672046661377\n",
      "Step: 2311, Loss: 0.9161804914474487, Accuracy: 1.0, Computation time: 2.491550922393799\n",
      "Step: 2312, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.9328668117523193\n",
      "Step: 2313, Loss: 0.9159321784973145, Accuracy: 1.0, Computation time: 1.6177997589111328\n",
      "Step: 2314, Loss: 0.9376159310340881, Accuracy: 0.96875, Computation time: 1.6165904998779297\n",
      "Step: 2315, Loss: 0.9386538863182068, Accuracy: 0.96875, Computation time: 2.40706729888916\n",
      "Step: 2316, Loss: 0.9159477353096008, Accuracy: 1.0, Computation time: 1.689772605895996\n",
      "Step: 2317, Loss: 0.9159249663352966, Accuracy: 1.0, Computation time: 1.8439302444458008\n",
      "Step: 2318, Loss: 0.9159172773361206, Accuracy: 1.0, Computation time: 2.002257823944092\n",
      "Step: 2319, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.6014831066131592\n",
      "Step: 2320, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.395462989807129\n",
      "Step: 2321, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 1.5145041942596436\n",
      "Step: 2322, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.5268497467041016\n",
      "Step: 2323, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.6321723461151123\n",
      "Step: 2324, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.6628048419952393\n",
      "Step: 2325, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.7903151512145996\n",
      "Step: 2326, Loss: 0.937503457069397, Accuracy: 0.96875, Computation time: 1.6156530380249023\n",
      "Step: 2327, Loss: 0.9368940591812134, Accuracy: 0.96875, Computation time: 1.5733213424682617\n",
      "Step: 2328, Loss: 0.937623143196106, Accuracy: 0.96875, Computation time: 1.6351215839385986\n",
      "Step: 2329, Loss: 0.9375678300857544, Accuracy: 0.96875, Computation time: 1.6932117938995361\n",
      "Step: 2330, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.5489630699157715\n",
      "Step: 2331, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.848851203918457\n",
      "Step: 2332, Loss: 0.9376242756843567, Accuracy: 0.96875, Computation time: 1.3797361850738525\n",
      "Step: 2333, Loss: 0.9372076988220215, Accuracy: 0.96875, Computation time: 1.7407026290893555\n",
      "Step: 2334, Loss: 0.9165114164352417, Accuracy: 1.0, Computation time: 1.531987190246582\n",
      "Step: 2335, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.9254803657531738\n",
      "Step: 2336, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.5760045051574707\n",
      "Step: 2337, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.562943696975708\n",
      "Step: 2338, Loss: 0.9319084882736206, Accuracy: 0.96875, Computation time: 1.9224343299865723\n",
      "Step: 2339, Loss: 0.9198674559593201, Accuracy: 1.0, Computation time: 1.7625863552093506\n",
      "Step: 2340, Loss: 0.9224066734313965, Accuracy: 1.0, Computation time: 2.333684206008911\n",
      "Step: 2341, Loss: 0.9165067076683044, Accuracy: 1.0, Computation time: 1.8003599643707275\n",
      "Step: 2342, Loss: 0.9159631133079529, Accuracy: 1.0, Computation time: 2.007711887359619\n",
      "Step: 2343, Loss: 0.9160236120223999, Accuracy: 1.0, Computation time: 1.9466874599456787\n",
      "Step: 2344, Loss: 0.9375488758087158, Accuracy: 0.96875, Computation time: 2.206242322921753\n",
      "Step: 2345, Loss: 0.9161237478256226, Accuracy: 1.0, Computation time: 1.6753544807434082\n",
      "Step: 2346, Loss: 0.9371829628944397, Accuracy: 0.96875, Computation time: 2.359480142593384\n",
      "Step: 2347, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.534466028213501\n",
      "Step: 2348, Loss: 0.9159160852432251, Accuracy: 1.0, Computation time: 1.893617868423462\n",
      "Step: 2349, Loss: 0.937552273273468, Accuracy: 0.96875, Computation time: 1.5722582340240479\n",
      "Step: 2350, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 1.7142698764801025\n",
      "Step: 2351, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.904207468032837\n",
      "Step: 2352, Loss: 0.9159451723098755, Accuracy: 1.0, Computation time: 1.5976991653442383\n",
      "Step: 2353, Loss: 0.9375852346420288, Accuracy: 0.96875, Computation time: 1.8114056587219238\n",
      "Step: 2354, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.6948144435882568\n",
      "Step: 2355, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 2.4624643325805664\n",
      "Step: 2356, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.9733362197875977\n",
      "Step: 2357, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 2.1479368209838867\n",
      "Step: 2358, Loss: 0.9350001215934753, Accuracy: 0.96875, Computation time: 1.8032984733581543\n",
      "Step: 2359, Loss: 0.9339035153388977, Accuracy: 0.96875, Computation time: 1.708775281906128\n",
      "Step: 2360, Loss: 0.9357672333717346, Accuracy: 0.96875, Computation time: 2.115708827972412\n",
      "Step: 2361, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.7257766723632812\n",
      "Step: 2362, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.6533536911010742\n",
      "########################\n",
      "Test loss: 1.1174097061157227, Test Accuracy_epoch17: 0.7019982933998108\n",
      "########################\n",
      "Step: 2363, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.8469336032867432\n",
      "Step: 2364, Loss: 0.9281520843505859, Accuracy: 1.0, Computation time: 2.0573365688323975\n",
      "Step: 2365, Loss: 0.9160897731781006, Accuracy: 1.0, Computation time: 1.7724933624267578\n",
      "Step: 2366, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.7128238677978516\n",
      "Step: 2367, Loss: 0.9202185869216919, Accuracy: 1.0, Computation time: 1.6123614311218262\n",
      "Step: 2368, Loss: 0.9161412715911865, Accuracy: 1.0, Computation time: 1.9040324687957764\n",
      "Step: 2369, Loss: 0.9242274761199951, Accuracy: 1.0, Computation time: 1.5091323852539062\n",
      "Step: 2370, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.670325756072998\n",
      "Step: 2371, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 1.604504108428955\n",
      "Step: 2372, Loss: 0.9160220623016357, Accuracy: 1.0, Computation time: 2.3059282302856445\n",
      "Step: 2373, Loss: 0.9316904544830322, Accuracy: 0.96875, Computation time: 1.8631582260131836\n",
      "Step: 2374, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 1.7169768810272217\n",
      "Step: 2375, Loss: 0.9161149859428406, Accuracy: 1.0, Computation time: 1.4974300861358643\n",
      "Step: 2376, Loss: 0.9190792441368103, Accuracy: 1.0, Computation time: 1.926506519317627\n",
      "Step: 2377, Loss: 0.9159455895423889, Accuracy: 1.0, Computation time: 1.3736543655395508\n",
      "Step: 2378, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 1.288623332977295\n",
      "Step: 2379, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.998732328414917\n",
      "Step: 2380, Loss: 0.937615692615509, Accuracy: 0.96875, Computation time: 1.8447067737579346\n",
      "Step: 2381, Loss: 0.9376565217971802, Accuracy: 0.96875, Computation time: 1.7116913795471191\n",
      "Step: 2382, Loss: 0.9383180737495422, Accuracy: 0.96875, Computation time: 1.3990111351013184\n",
      "Step: 2383, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 1.5638782978057861\n",
      "Step: 2384, Loss: 0.915963351726532, Accuracy: 1.0, Computation time: 1.3770711421966553\n",
      "Step: 2385, Loss: 0.9161458015441895, Accuracy: 1.0, Computation time: 1.7345778942108154\n",
      "Step: 2386, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 1.2916600704193115\n",
      "Step: 2387, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.3524789810180664\n",
      "Step: 2388, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.6591055393218994\n",
      "Step: 2389, Loss: 0.9164901375770569, Accuracy: 1.0, Computation time: 1.8163652420043945\n",
      "Step: 2390, Loss: 0.9374958276748657, Accuracy: 0.96875, Computation time: 1.8648664951324463\n",
      "Step: 2391, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.9162960052490234\n",
      "Step: 2392, Loss: 0.9379231929779053, Accuracy: 0.96875, Computation time: 1.6715598106384277\n",
      "Step: 2393, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.7028048038482666\n",
      "Step: 2394, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.6991770267486572\n",
      "Step: 2395, Loss: 0.9372289776802063, Accuracy: 0.96875, Computation time: 2.2316555976867676\n",
      "Step: 2396, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.5302245616912842\n",
      "Step: 2397, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.850675344467163\n",
      "Step: 2398, Loss: 0.9160510897636414, Accuracy: 1.0, Computation time: 2.016484498977661\n",
      "Step: 2399, Loss: 0.9159824848175049, Accuracy: 1.0, Computation time: 1.9059967994689941\n",
      "Step: 2400, Loss: 0.9166260361671448, Accuracy: 1.0, Computation time: 1.3100848197937012\n",
      "Step: 2401, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 1.5542011260986328\n",
      "Step: 2402, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.505220651626587\n",
      "Step: 2403, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.7262468338012695\n",
      "Step: 2404, Loss: 0.9370474219322205, Accuracy: 0.96875, Computation time: 1.4189720153808594\n",
      "Step: 2405, Loss: 0.920647144317627, Accuracy: 1.0, Computation time: 2.0976362228393555\n",
      "Step: 2406, Loss: 0.9342801570892334, Accuracy: 0.96875, Computation time: 1.7449820041656494\n",
      "Step: 2407, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.37465238571167\n",
      "Step: 2408, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.6664137840270996\n",
      "Step: 2409, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.7486755847930908\n",
      "Step: 2410, Loss: 0.9160251617431641, Accuracy: 1.0, Computation time: 1.7148680686950684\n",
      "Step: 2411, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.442687749862671\n",
      "Step: 2412, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.7199110984802246\n",
      "Step: 2413, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.7633743286132812\n",
      "Step: 2414, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.4952092170715332\n",
      "Step: 2415, Loss: 0.9160213470458984, Accuracy: 1.0, Computation time: 1.5241203308105469\n",
      "Step: 2416, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.604210376739502\n",
      "Step: 2417, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 1.839332103729248\n",
      "Step: 2418, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 1.5144743919372559\n",
      "Step: 2419, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.7820959091186523\n",
      "Step: 2420, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 2.108907461166382\n",
      "Step: 2421, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.9499526023864746\n",
      "Step: 2422, Loss: 0.9159708023071289, Accuracy: 1.0, Computation time: 1.5731379985809326\n",
      "Step: 2423, Loss: 0.9297363758087158, Accuracy: 0.96875, Computation time: 1.6695337295532227\n",
      "Step: 2424, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 2.384446859359741\n",
      "Step: 2425, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.806006908416748\n",
      "Step: 2426, Loss: 0.9160243272781372, Accuracy: 1.0, Computation time: 1.6319732666015625\n",
      "Step: 2427, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.5693867206573486\n",
      "Step: 2428, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 2.145956039428711\n",
      "Step: 2429, Loss: 0.9160009622573853, Accuracy: 1.0, Computation time: 1.8473930358886719\n",
      "Step: 2430, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.7851402759552002\n",
      "Step: 2431, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.6851751804351807\n",
      "Step: 2432, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.4534919261932373\n",
      "Step: 2433, Loss: 0.9160172939300537, Accuracy: 1.0, Computation time: 1.466719388961792\n",
      "Step: 2434, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.7138643264770508\n",
      "Step: 2435, Loss: 0.9313588738441467, Accuracy: 0.96875, Computation time: 2.2101149559020996\n",
      "Step: 2436, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 1.687715768814087\n",
      "Step: 2437, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.6389446258544922\n",
      "Step: 2438, Loss: 0.9162351489067078, Accuracy: 1.0, Computation time: 2.4061355590820312\n",
      "Step: 2439, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.6169712543487549\n",
      "Step: 2440, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.988525152206421\n",
      "Step: 2441, Loss: 0.9182354211807251, Accuracy: 1.0, Computation time: 1.502122163772583\n",
      "Step: 2442, Loss: 0.9160364866256714, Accuracy: 1.0, Computation time: 1.817521095275879\n",
      "Step: 2443, Loss: 0.9374099969863892, Accuracy: 0.96875, Computation time: 1.7874445915222168\n",
      "Step: 2444, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.725938081741333\n",
      "Step: 2445, Loss: 0.9376016855239868, Accuracy: 0.96875, Computation time: 1.4247910976409912\n",
      "Step: 2446, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 1.5848474502563477\n",
      "Step: 2447, Loss: 0.9375893473625183, Accuracy: 0.96875, Computation time: 1.5599267482757568\n",
      "Step: 2448, Loss: 0.9160402417182922, Accuracy: 1.0, Computation time: 2.2473793029785156\n",
      "Step: 2449, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.6530473232269287\n",
      "Step: 2450, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.805448055267334\n",
      "Step: 2451, Loss: 0.925902247428894, Accuracy: 0.96875, Computation time: 2.1707072257995605\n",
      "Step: 2452, Loss: 0.9375677108764648, Accuracy: 0.96875, Computation time: 1.6266586780548096\n",
      "Step: 2453, Loss: 0.9294803738594055, Accuracy: 0.96875, Computation time: 1.6082298755645752\n",
      "Step: 2454, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.5203373432159424\n",
      "Step: 2455, Loss: 0.9160107970237732, Accuracy: 1.0, Computation time: 1.8047447204589844\n",
      "Step: 2456, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.6839725971221924\n",
      "Step: 2457, Loss: 0.9160228371620178, Accuracy: 1.0, Computation time: 1.742147445678711\n",
      "Step: 2458, Loss: 0.916064977645874, Accuracy: 1.0, Computation time: 1.645214319229126\n",
      "Step: 2459, Loss: 0.9159916043281555, Accuracy: 1.0, Computation time: 1.5453970432281494\n",
      "Step: 2460, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 1.717237949371338\n",
      "Step: 2461, Loss: 0.9344425201416016, Accuracy: 0.96875, Computation time: 2.588977813720703\n",
      "Step: 2462, Loss: 0.9375357031822205, Accuracy: 0.96875, Computation time: 1.435727596282959\n",
      "Step: 2463, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 1.6355488300323486\n",
      "Step: 2464, Loss: 0.9159685969352722, Accuracy: 1.0, Computation time: 1.5747711658477783\n",
      "Step: 2465, Loss: 0.9374418258666992, Accuracy: 0.96875, Computation time: 1.5489869117736816\n",
      "Step: 2466, Loss: 0.9159768223762512, Accuracy: 1.0, Computation time: 2.2349250316619873\n",
      "Step: 2467, Loss: 0.9159677624702454, Accuracy: 1.0, Computation time: 1.9364638328552246\n",
      "Step: 2468, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 2.472576141357422\n",
      "Step: 2469, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.9180102348327637\n",
      "Step: 2470, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 2.0533483028411865\n",
      "Step: 2471, Loss: 0.9159767627716064, Accuracy: 1.0, Computation time: 1.5579819679260254\n",
      "Step: 2472, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.5917384624481201\n",
      "Step: 2473, Loss: 0.916242778301239, Accuracy: 1.0, Computation time: 1.4262032508850098\n",
      "Step: 2474, Loss: 0.9371787905693054, Accuracy: 0.96875, Computation time: 1.754418134689331\n",
      "Step: 2475, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.8793487548828125\n",
      "Step: 2476, Loss: 0.9164589643478394, Accuracy: 1.0, Computation time: 1.9149889945983887\n",
      "Step: 2477, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 1.8408403396606445\n",
      "Step: 2478, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.4672534465789795\n",
      "Step: 2479, Loss: 0.9332253932952881, Accuracy: 0.96875, Computation time: 2.103407144546509\n",
      "Step: 2480, Loss: 0.9373911619186401, Accuracy: 0.96875, Computation time: 1.8023655414581299\n",
      "Step: 2481, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 1.5408446788787842\n",
      "Step: 2482, Loss: 0.9159484505653381, Accuracy: 1.0, Computation time: 1.2712526321411133\n",
      "Step: 2483, Loss: 0.9160358309745789, Accuracy: 1.0, Computation time: 1.6122634410858154\n",
      "Step: 2484, Loss: 0.9375064373016357, Accuracy: 0.96875, Computation time: 1.6444780826568604\n",
      "Step: 2485, Loss: 0.9160264134407043, Accuracy: 1.0, Computation time: 2.0030860900878906\n",
      "Step: 2486, Loss: 0.9375087022781372, Accuracy: 0.96875, Computation time: 1.6846096515655518\n",
      "Step: 2487, Loss: 0.9159495234489441, Accuracy: 1.0, Computation time: 1.8791024684906006\n",
      "Step: 2488, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.6121625900268555\n",
      "Step: 2489, Loss: 0.9159087538719177, Accuracy: 1.0, Computation time: 2.0159835815429688\n",
      "Step: 2490, Loss: 0.91591876745224, Accuracy: 1.0, Computation time: 1.6316075325012207\n",
      "Step: 2491, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.3676114082336426\n",
      "Step: 2492, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.7032599449157715\n",
      "Step: 2493, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.8929812908172607\n",
      "Step: 2494, Loss: 0.9321634769439697, Accuracy: 0.96875, Computation time: 1.945430040359497\n",
      "Step: 2495, Loss: 0.9160040616989136, Accuracy: 1.0, Computation time: 1.6400020122528076\n",
      "Step: 2496, Loss: 0.9174591898918152, Accuracy: 1.0, Computation time: 1.7233607769012451\n",
      "Step: 2497, Loss: 0.9375016689300537, Accuracy: 0.96875, Computation time: 2.0023791790008545\n",
      "Step: 2498, Loss: 0.9375940561294556, Accuracy: 0.96875, Computation time: 1.705324411392212\n",
      "Step: 2499, Loss: 0.9606921672821045, Accuracy: 0.9375, Computation time: 2.166654586791992\n",
      "Step: 2500, Loss: 0.9159371852874756, Accuracy: 1.0, Computation time: 1.7795970439910889\n",
      "Step: 2501, Loss: 0.9160060882568359, Accuracy: 1.0, Computation time: 1.5723848342895508\n",
      "########################\n",
      "Test loss: 1.1187853813171387, Test Accuracy_epoch18: 0.7046046853065491\n",
      "########################\n",
      "Step: 2502, Loss: 0.9161518812179565, Accuracy: 1.0, Computation time: 1.7004199028015137\n",
      "Step: 2503, Loss: 0.9159765839576721, Accuracy: 1.0, Computation time: 1.3141863346099854\n",
      "Step: 2504, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 1.6427946090698242\n",
      "Step: 2505, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.4935178756713867\n",
      "Step: 2506, Loss: 0.9159892797470093, Accuracy: 1.0, Computation time: 1.616370439529419\n",
      "Step: 2507, Loss: 0.915923535823822, Accuracy: 1.0, Computation time: 1.926276683807373\n",
      "Step: 2508, Loss: 0.9380266070365906, Accuracy: 0.96875, Computation time: 1.6206865310668945\n",
      "Step: 2509, Loss: 0.937624990940094, Accuracy: 0.96875, Computation time: 1.4773716926574707\n",
      "Step: 2510, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.3187575340270996\n",
      "Step: 2511, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 1.2969894409179688\n",
      "Step: 2512, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.3319671154022217\n",
      "Step: 2513, Loss: 0.9169528484344482, Accuracy: 1.0, Computation time: 2.2199630737304688\n",
      "Step: 2514, Loss: 0.9371415376663208, Accuracy: 0.96875, Computation time: 1.5845246315002441\n",
      "Step: 2515, Loss: 0.919802188873291, Accuracy: 1.0, Computation time: 1.7488393783569336\n",
      "Step: 2516, Loss: 0.9376451969146729, Accuracy: 0.96875, Computation time: 2.210068464279175\n",
      "Step: 2517, Loss: 0.9346614480018616, Accuracy: 0.96875, Computation time: 1.917513370513916\n",
      "Step: 2518, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 1.473564863204956\n",
      "Step: 2519, Loss: 0.9160557389259338, Accuracy: 1.0, Computation time: 2.353827476501465\n",
      "Step: 2520, Loss: 0.9259724617004395, Accuracy: 0.96875, Computation time: 2.104518175125122\n",
      "Step: 2521, Loss: 0.9160178899765015, Accuracy: 1.0, Computation time: 1.8956317901611328\n",
      "Step: 2522, Loss: 0.937625527381897, Accuracy: 0.96875, Computation time: 1.3525984287261963\n",
      "Step: 2523, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.4452288150787354\n",
      "Step: 2524, Loss: 0.9181392788887024, Accuracy: 1.0, Computation time: 2.0122392177581787\n",
      "Step: 2525, Loss: 0.9159680604934692, Accuracy: 1.0, Computation time: 1.5555689334869385\n",
      "Step: 2526, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 1.4953076839447021\n",
      "Step: 2527, Loss: 0.9160094857215881, Accuracy: 1.0, Computation time: 2.828386068344116\n",
      "Step: 2528, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.4238290786743164\n",
      "Step: 2529, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.2759997844696045\n",
      "Step: 2530, Loss: 0.9375483989715576, Accuracy: 0.96875, Computation time: 1.9283080101013184\n",
      "Step: 2531, Loss: 0.9177584648132324, Accuracy: 1.0, Computation time: 1.9740078449249268\n",
      "Step: 2532, Loss: 0.9159623980522156, Accuracy: 1.0, Computation time: 1.801220417022705\n",
      "Step: 2533, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.438960313796997\n",
      "Step: 2534, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.609898328781128\n",
      "Step: 2535, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.5146849155426025\n",
      "Step: 2536, Loss: 0.9561566710472107, Accuracy: 0.9375, Computation time: 1.6271040439605713\n",
      "Step: 2537, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 1.3667924404144287\n",
      "Step: 2538, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.4096386432647705\n",
      "Step: 2539, Loss: 0.924811840057373, Accuracy: 1.0, Computation time: 1.7577431201934814\n",
      "Step: 2540, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.6635372638702393\n",
      "Step: 2541, Loss: 0.9160240292549133, Accuracy: 1.0, Computation time: 1.824730396270752\n",
      "Step: 2542, Loss: 0.9363371133804321, Accuracy: 0.96875, Computation time: 1.7267606258392334\n",
      "Step: 2543, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 1.4148919582366943\n",
      "Step: 2544, Loss: 0.9174561500549316, Accuracy: 1.0, Computation time: 1.8354246616363525\n",
      "Step: 2545, Loss: 0.9160178899765015, Accuracy: 1.0, Computation time: 1.5674023628234863\n",
      "Step: 2546, Loss: 0.9332889914512634, Accuracy: 0.96875, Computation time: 1.4859225749969482\n",
      "Step: 2547, Loss: 0.935744047164917, Accuracy: 0.96875, Computation time: 1.4566020965576172\n",
      "Step: 2548, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 1.5401077270507812\n",
      "Step: 2549, Loss: 0.9160521030426025, Accuracy: 1.0, Computation time: 1.3620176315307617\n",
      "Step: 2550, Loss: 0.9159514904022217, Accuracy: 1.0, Computation time: 1.2344017028808594\n",
      "Step: 2551, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.556110143661499\n",
      "Step: 2552, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.334650993347168\n",
      "Step: 2553, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.620069980621338\n",
      "Step: 2554, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.6383233070373535\n",
      "Step: 2555, Loss: 0.950367271900177, Accuracy: 0.9375, Computation time: 1.8634004592895508\n",
      "Step: 2556, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.4036128520965576\n",
      "Step: 2557, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.27052903175354\n",
      "Step: 2558, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.2651443481445312\n",
      "Step: 2559, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.3627760410308838\n",
      "Step: 2560, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 1.3714005947113037\n",
      "Step: 2561, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.3658041954040527\n",
      "Step: 2562, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.190934419631958\n",
      "Step: 2563, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 1.3496453762054443\n",
      "Step: 2564, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.1686959266662598\n",
      "Step: 2565, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.2421743869781494\n",
      "Step: 2566, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.3829598426818848\n",
      "Step: 2567, Loss: 0.9372982978820801, Accuracy: 0.96875, Computation time: 1.107710361480713\n",
      "Step: 2568, Loss: 0.9163769483566284, Accuracy: 1.0, Computation time: 1.444929599761963\n",
      "Step: 2569, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 1.2416529655456543\n",
      "Step: 2570, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.6099989414215088\n",
      "Step: 2571, Loss: 0.9375752210617065, Accuracy: 0.96875, Computation time: 1.193169355392456\n",
      "Step: 2572, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.1875410079956055\n",
      "Step: 2573, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.2906577587127686\n",
      "Step: 2574, Loss: 0.9367629289627075, Accuracy: 0.96875, Computation time: 2.7853548526763916\n",
      "Step: 2575, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.0749289989471436\n",
      "Step: 2576, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.1193921566009521\n",
      "Step: 2577, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.0806636810302734\n",
      "Step: 2578, Loss: 0.9174533486366272, Accuracy: 1.0, Computation time: 1.7136244773864746\n",
      "Step: 2579, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.3661789894104004\n",
      "Step: 2580, Loss: 0.9274554252624512, Accuracy: 0.96875, Computation time: 2.211993932723999\n",
      "Step: 2581, Loss: 0.9159629344940186, Accuracy: 1.0, Computation time: 1.155653476715088\n",
      "Step: 2582, Loss: 0.91595059633255, Accuracy: 1.0, Computation time: 1.1024315357208252\n",
      "Step: 2583, Loss: 0.9159809947013855, Accuracy: 1.0, Computation time: 1.252206563949585\n",
      "Step: 2584, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.1065797805786133\n",
      "Step: 2585, Loss: 0.937497615814209, Accuracy: 0.96875, Computation time: 1.7995197772979736\n",
      "Step: 2586, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.2783210277557373\n",
      "Step: 2587, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.312770128250122\n",
      "Step: 2588, Loss: 0.9374550580978394, Accuracy: 0.96875, Computation time: 1.8862347602844238\n",
      "Step: 2589, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.6315362453460693\n",
      "Step: 2590, Loss: 0.9367288947105408, Accuracy: 0.96875, Computation time: 1.608734369277954\n",
      "Step: 2591, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.3860714435577393\n",
      "Step: 2592, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.2620792388916016\n",
      "Step: 2593, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.3544886112213135\n",
      "Step: 2594, Loss: 0.9159573912620544, Accuracy: 1.0, Computation time: 1.1806070804595947\n",
      "Step: 2595, Loss: 0.915948212146759, Accuracy: 1.0, Computation time: 1.1269927024841309\n",
      "Step: 2596, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 1.376563549041748\n",
      "Step: 2597, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 2.0145368576049805\n",
      "Step: 2598, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.2941913604736328\n",
      "Step: 2599, Loss: 0.935625433921814, Accuracy: 0.96875, Computation time: 1.7170443534851074\n",
      "Step: 2600, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 1.4391961097717285\n",
      "Step: 2601, Loss: 0.916172981262207, Accuracy: 1.0, Computation time: 1.7722856998443604\n",
      "Step: 2602, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.4786338806152344\n",
      "Step: 2603, Loss: 0.9376316070556641, Accuracy: 0.96875, Computation time: 1.4851932525634766\n",
      "Step: 2604, Loss: 0.9294061064720154, Accuracy: 0.96875, Computation time: 1.8136768341064453\n",
      "Step: 2605, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.2621402740478516\n",
      "Step: 2606, Loss: 0.9159994125366211, Accuracy: 1.0, Computation time: 1.7528808116912842\n",
      "Step: 2607, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 1.7455675601959229\n",
      "Step: 2608, Loss: 0.9159649610519409, Accuracy: 1.0, Computation time: 1.1947288513183594\n",
      "Step: 2609, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.2530932426452637\n",
      "Step: 2610, Loss: 0.9159959554672241, Accuracy: 1.0, Computation time: 1.6362805366516113\n",
      "Step: 2611, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.436584234237671\n",
      "Step: 2612, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 1.7114601135253906\n",
      "Step: 2613, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.3295602798461914\n",
      "Step: 2614, Loss: 0.9396209716796875, Accuracy: 0.96875, Computation time: 1.6943066120147705\n",
      "Step: 2615, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.3164875507354736\n",
      "Step: 2616, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.3367416858673096\n",
      "Step: 2617, Loss: 0.9371932744979858, Accuracy: 0.96875, Computation time: 1.551680326461792\n",
      "Step: 2618, Loss: 0.9159172773361206, Accuracy: 1.0, Computation time: 1.2404518127441406\n",
      "Step: 2619, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.686293601989746\n",
      "Step: 2620, Loss: 0.9385057091712952, Accuracy: 0.96875, Computation time: 2.4521260261535645\n",
      "Step: 2621, Loss: 0.9159326553344727, Accuracy: 1.0, Computation time: 1.545987844467163\n",
      "Step: 2622, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.6419322490692139\n",
      "Step: 2623, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 1.617994785308838\n",
      "Step: 2624, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 2.0010135173797607\n",
      "Step: 2625, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.6738483905792236\n",
      "Step: 2626, Loss: 0.9159521460533142, Accuracy: 1.0, Computation time: 1.9993934631347656\n",
      "Step: 2627, Loss: 0.9160904288291931, Accuracy: 1.0, Computation time: 1.7694995403289795\n",
      "Step: 2628, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.8086225986480713\n",
      "Step: 2629, Loss: 0.9159793257713318, Accuracy: 1.0, Computation time: 1.522085428237915\n",
      "Step: 2630, Loss: 0.9164304137229919, Accuracy: 1.0, Computation time: 1.8930871486663818\n",
      "Step: 2631, Loss: 0.9168820381164551, Accuracy: 1.0, Computation time: 1.5601072311401367\n",
      "Step: 2632, Loss: 0.9374803900718689, Accuracy: 0.96875, Computation time: 1.5612318515777588\n",
      "Step: 2633, Loss: 0.9160633087158203, Accuracy: 1.0, Computation time: 1.596576452255249\n",
      "Step: 2634, Loss: 0.9337040781974792, Accuracy: 0.96875, Computation time: 1.4880616664886475\n",
      "Step: 2635, Loss: 0.9160392880439758, Accuracy: 1.0, Computation time: 1.403106689453125\n",
      "Step: 2636, Loss: 0.9160171747207642, Accuracy: 1.0, Computation time: 1.3109650611877441\n",
      "Step: 2637, Loss: 0.9159926772117615, Accuracy: 1.0, Computation time: 1.6088428497314453\n",
      "Step: 2638, Loss: 0.9162947535514832, Accuracy: 1.0, Computation time: 1.6208970546722412\n",
      "Step: 2639, Loss: 0.9371923208236694, Accuracy: 0.96875, Computation time: 1.719031572341919\n",
      "Step: 2640, Loss: 0.9159613251686096, Accuracy: 1.0, Computation time: 1.6783168315887451\n",
      "########################\n",
      "Test loss: 1.116931676864624, Test Accuracy_epoch19: 0.7046046853065491\n",
      "########################\n",
      "Step: 2641, Loss: 0.9366907477378845, Accuracy: 0.96875, Computation time: 1.7208847999572754\n",
      "Step: 2642, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.3679883480072021\n",
      "Step: 2643, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.6598789691925049\n",
      "Step: 2644, Loss: 0.937190592288971, Accuracy: 0.96875, Computation time: 1.6726648807525635\n",
      "Step: 2645, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.3487298488616943\n",
      "Step: 2646, Loss: 0.9160451889038086, Accuracy: 1.0, Computation time: 1.4204797744750977\n",
      "Step: 2647, Loss: 0.916131317615509, Accuracy: 1.0, Computation time: 1.4771006107330322\n",
      "Step: 2648, Loss: 0.9183252453804016, Accuracy: 1.0, Computation time: 1.6931991577148438\n",
      "Step: 2649, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 1.2542295455932617\n",
      "Step: 2650, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.3640825748443604\n",
      "Step: 2651, Loss: 0.9179657101631165, Accuracy: 1.0, Computation time: 1.8332245349884033\n",
      "Step: 2652, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 1.4115285873413086\n",
      "Step: 2653, Loss: 0.9376096725463867, Accuracy: 0.96875, Computation time: 1.6857573986053467\n",
      "Step: 2654, Loss: 0.9375938177108765, Accuracy: 0.96875, Computation time: 1.8109309673309326\n",
      "Step: 2655, Loss: 0.9341710209846497, Accuracy: 0.96875, Computation time: 1.5876870155334473\n",
      "Step: 2656, Loss: 0.9161660671234131, Accuracy: 1.0, Computation time: 1.4034638404846191\n",
      "Step: 2657, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.362727165222168\n",
      "Step: 2658, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.5585038661956787\n",
      "Step: 2659, Loss: 0.9159523248672485, Accuracy: 1.0, Computation time: 1.3422138690948486\n",
      "Step: 2660, Loss: 0.9187635779380798, Accuracy: 1.0, Computation time: 1.4264168739318848\n",
      "Step: 2661, Loss: 0.9180614948272705, Accuracy: 1.0, Computation time: 1.619004487991333\n",
      "Step: 2662, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 1.9183306694030762\n",
      "Step: 2663, Loss: 0.9159761071205139, Accuracy: 1.0, Computation time: 1.6926460266113281\n",
      "Step: 2664, Loss: 0.9159577488899231, Accuracy: 1.0, Computation time: 1.2399687767028809\n",
      "Step: 2665, Loss: 0.9166691303253174, Accuracy: 1.0, Computation time: 1.2638914585113525\n",
      "Step: 2666, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.211090087890625\n",
      "Step: 2667, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.2901453971862793\n",
      "Step: 2668, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.2797152996063232\n",
      "Step: 2669, Loss: 0.9375618696212769, Accuracy: 0.96875, Computation time: 1.2649412155151367\n",
      "Step: 2670, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.2091269493103027\n",
      "Step: 2671, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.2852566242218018\n",
      "Step: 2672, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.5516777038574219\n",
      "Step: 2673, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.2252187728881836\n",
      "Step: 2674, Loss: 0.916155993938446, Accuracy: 1.0, Computation time: 1.494520902633667\n",
      "Step: 2675, Loss: 0.9362743496894836, Accuracy: 0.96875, Computation time: 1.4383468627929688\n",
      "Step: 2676, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.1920106410980225\n",
      "Step: 2677, Loss: 0.9159378409385681, Accuracy: 1.0, Computation time: 1.41707181930542\n",
      "Step: 2678, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 1.305210828781128\n",
      "Step: 2679, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 2.0405519008636475\n",
      "Step: 2680, Loss: 0.9376059770584106, Accuracy: 0.96875, Computation time: 1.3825342655181885\n",
      "Step: 2681, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.656914234161377\n",
      "Step: 2682, Loss: 0.9160205125808716, Accuracy: 1.0, Computation time: 1.4627025127410889\n",
      "Step: 2683, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 1.4234468936920166\n",
      "Step: 2684, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.5034573078155518\n",
      "Step: 2685, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.4493377208709717\n",
      "Step: 2686, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.4445297718048096\n",
      "Step: 2687, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.4636428356170654\n",
      "Step: 2688, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.4197967052459717\n",
      "Step: 2689, Loss: 0.9281862378120422, Accuracy: 0.96875, Computation time: 1.5268704891204834\n",
      "Step: 2690, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.3278615474700928\n",
      "Step: 2691, Loss: 0.9159038662910461, Accuracy: 1.0, Computation time: 1.4988102912902832\n",
      "Step: 2692, Loss: 0.9180924892425537, Accuracy: 1.0, Computation time: 1.4865858554840088\n",
      "Step: 2693, Loss: 0.915903627872467, Accuracy: 1.0, Computation time: 1.2001285552978516\n",
      "Step: 2694, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.436422348022461\n",
      "Step: 2695, Loss: 0.9212327003479004, Accuracy: 1.0, Computation time: 2.0812437534332275\n",
      "Step: 2696, Loss: 0.9534512758255005, Accuracy: 0.9375, Computation time: 1.9469292163848877\n",
      "Step: 2697, Loss: 0.9172709584236145, Accuracy: 1.0, Computation time: 1.6091701984405518\n",
      "Step: 2698, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 1.5995147228240967\n",
      "Step: 2699, Loss: 0.9392594695091248, Accuracy: 0.96875, Computation time: 1.8116111755371094\n",
      "Step: 2700, Loss: 0.9226054549217224, Accuracy: 1.0, Computation time: 1.4685454368591309\n",
      "Step: 2701, Loss: 0.916103184223175, Accuracy: 1.0, Computation time: 1.688777208328247\n",
      "Step: 2702, Loss: 0.9374032020568848, Accuracy: 0.96875, Computation time: 1.692455768585205\n",
      "Step: 2703, Loss: 0.9378078579902649, Accuracy: 0.96875, Computation time: 1.7597451210021973\n",
      "Step: 2704, Loss: 0.916114330291748, Accuracy: 1.0, Computation time: 1.922253131866455\n",
      "Step: 2705, Loss: 0.9161118268966675, Accuracy: 1.0, Computation time: 1.7653734683990479\n",
      "Step: 2706, Loss: 0.9161736965179443, Accuracy: 1.0, Computation time: 1.8762879371643066\n",
      "Step: 2707, Loss: 0.9192503094673157, Accuracy: 1.0, Computation time: 1.960700273513794\n",
      "Step: 2708, Loss: 0.9159730076789856, Accuracy: 1.0, Computation time: 1.647068738937378\n",
      "Step: 2709, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.3854084014892578\n",
      "Step: 2710, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 1.399568796157837\n",
      "Step: 2711, Loss: 0.9589618444442749, Accuracy: 0.9375, Computation time: 2.062896966934204\n",
      "Step: 2712, Loss: 0.9159790277481079, Accuracy: 1.0, Computation time: 1.2753918170928955\n",
      "Step: 2713, Loss: 0.915954053401947, Accuracy: 1.0, Computation time: 1.2278246879577637\n",
      "Step: 2714, Loss: 0.9376019835472107, Accuracy: 0.96875, Computation time: 1.348827838897705\n",
      "Step: 2715, Loss: 0.9377413392066956, Accuracy: 0.96875, Computation time: 1.672621726989746\n",
      "Step: 2716, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.2957267761230469\n",
      "Step: 2717, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.4901905059814453\n",
      "Step: 2718, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.235968828201294\n",
      "Step: 2719, Loss: 0.9163936972618103, Accuracy: 1.0, Computation time: 1.8620758056640625\n",
      "Step: 2720, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.4430413246154785\n",
      "Step: 2721, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.4532434940338135\n",
      "Step: 2722, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 1.4805939197540283\n",
      "Step: 2723, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.9671165943145752\n",
      "Step: 2724, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 1.53572416305542\n",
      "Step: 2725, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.4910171031951904\n",
      "Step: 2726, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 2.05448842048645\n",
      "Step: 2727, Loss: 0.916081964969635, Accuracy: 1.0, Computation time: 1.744610071182251\n",
      "Step: 2728, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.3852362632751465\n",
      "Step: 2729, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.5279803276062012\n",
      "Step: 2730, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.535627841949463\n",
      "Step: 2731, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.5835504531860352\n",
      "Step: 2732, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.6709156036376953\n",
      "Step: 2733, Loss: 0.9375226497650146, Accuracy: 0.96875, Computation time: 1.7441918849945068\n",
      "Step: 2734, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.7864100933074951\n",
      "Step: 2735, Loss: 0.9162641763687134, Accuracy: 1.0, Computation time: 1.6758043766021729\n",
      "Step: 2736, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 2.022840738296509\n",
      "Step: 2737, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.935697317123413\n",
      "Step: 2738, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 2.269160270690918\n",
      "Step: 2739, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.6469027996063232\n",
      "Step: 2740, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.5886569023132324\n",
      "Step: 2741, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.7049927711486816\n",
      "Step: 2742, Loss: 0.9193442463874817, Accuracy: 1.0, Computation time: 1.441159963607788\n",
      "Step: 2743, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.5960004329681396\n",
      "Step: 2744, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.811429500579834\n",
      "Step: 2745, Loss: 0.9376190900802612, Accuracy: 0.96875, Computation time: 1.8942701816558838\n",
      "Step: 2746, Loss: 0.9379908442497253, Accuracy: 0.96875, Computation time: 1.632624626159668\n",
      "Step: 2747, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.707179307937622\n",
      "Step: 2748, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.2910683155059814\n",
      "Step: 2749, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.464472770690918\n",
      "Step: 2750, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.9251534938812256\n",
      "Step: 2751, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.5579190254211426\n",
      "Step: 2752, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.8531274795532227\n",
      "Step: 2753, Loss: 0.9162535071372986, Accuracy: 1.0, Computation time: 1.5708141326904297\n",
      "Step: 2754, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.4968597888946533\n",
      "Step: 2755, Loss: 0.9163106083869934, Accuracy: 1.0, Computation time: 1.9504685401916504\n",
      "Step: 2756, Loss: 0.9159289002418518, Accuracy: 1.0, Computation time: 2.2376458644866943\n",
      "Step: 2757, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.445816993713379\n",
      "Step: 2758, Loss: 0.937559962272644, Accuracy: 0.96875, Computation time: 1.4927318096160889\n",
      "Step: 2759, Loss: 0.9377619624137878, Accuracy: 0.96875, Computation time: 1.6053955554962158\n",
      "Step: 2760, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.3785982131958008\n",
      "Step: 2761, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 1.6050608158111572\n",
      "Step: 2762, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.5556895732879639\n",
      "Step: 2763, Loss: 0.9376946091651917, Accuracy: 0.96875, Computation time: 1.6598880290985107\n",
      "Step: 2764, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.2816863059997559\n",
      "Step: 2765, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.7390072345733643\n",
      "Step: 2766, Loss: 0.9278731942176819, Accuracy: 0.96875, Computation time: 1.6456480026245117\n",
      "Step: 2767, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.4741666316986084\n",
      "Step: 2768, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.5917458534240723\n",
      "Step: 2769, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.5934231281280518\n",
      "Step: 2770, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.5280210971832275\n",
      "Step: 2771, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.3175890445709229\n",
      "Step: 2772, Loss: 0.9375309944152832, Accuracy: 0.96875, Computation time: 1.2573025226593018\n",
      "Step: 2773, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 1.8981142044067383\n",
      "Step: 2774, Loss: 0.9506796598434448, Accuracy: 0.9375, Computation time: 2.2463276386260986\n",
      "Step: 2775, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.6499812602996826\n",
      "Step: 2776, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 1.4494013786315918\n",
      "Step: 2777, Loss: 0.9375680088996887, Accuracy: 0.96875, Computation time: 1.794438362121582\n",
      "Step: 2778, Loss: 0.9160107374191284, Accuracy: 1.0, Computation time: 1.1963531970977783\n",
      "Step: 2779, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.3943097591400146\n",
      "########################\n",
      "Test loss: 1.117867350578308, Test Accuracy_epoch20: 0.7054735422134399\n",
      "########################\n",
      "Step: 2780, Loss: 0.9161700010299683, Accuracy: 1.0, Computation time: 2.570889949798584\n",
      "Step: 2781, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.5729353427886963\n",
      "Step: 2782, Loss: 0.9159768223762512, Accuracy: 1.0, Computation time: 1.77077054977417\n",
      "Step: 2783, Loss: 0.9161120653152466, Accuracy: 1.0, Computation time: 1.8633084297180176\n",
      "Step: 2784, Loss: 0.9284549951553345, Accuracy: 0.96875, Computation time: 2.3771045207977295\n",
      "Step: 2785, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.291520118713379\n",
      "Step: 2786, Loss: 0.9160219430923462, Accuracy: 1.0, Computation time: 1.6655406951904297\n",
      "Step: 2787, Loss: 0.9379915595054626, Accuracy: 0.96875, Computation time: 2.4071154594421387\n",
      "Step: 2788, Loss: 0.9162067174911499, Accuracy: 1.0, Computation time: 1.9537160396575928\n",
      "Step: 2789, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.695641279220581\n",
      "Step: 2790, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.409334659576416\n",
      "Step: 2791, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.319000244140625\n",
      "Step: 2792, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 1.3140246868133545\n",
      "Step: 2793, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.3357937335968018\n",
      "Step: 2794, Loss: 0.9165048003196716, Accuracy: 1.0, Computation time: 1.8857028484344482\n",
      "Step: 2795, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.452892780303955\n",
      "Step: 2796, Loss: 0.942197859287262, Accuracy: 0.96875, Computation time: 1.8552474975585938\n",
      "Step: 2797, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.4992694854736328\n",
      "Step: 2798, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 1.6232445240020752\n",
      "Step: 2799, Loss: 0.9159350991249084, Accuracy: 1.0, Computation time: 1.7479157447814941\n",
      "Step: 2800, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 2.094939708709717\n",
      "Step: 2801, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.960984230041504\n",
      "Step: 2802, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.3806297779083252\n",
      "Step: 2803, Loss: 0.9382997155189514, Accuracy: 0.96875, Computation time: 1.6078381538391113\n",
      "Step: 2804, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.342449426651001\n",
      "Step: 2805, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.510195016860962\n",
      "Step: 2806, Loss: 0.9376201629638672, Accuracy: 0.96875, Computation time: 1.5495765209197998\n",
      "Step: 2807, Loss: 0.9160071611404419, Accuracy: 1.0, Computation time: 1.9781885147094727\n",
      "Step: 2808, Loss: 0.9354982376098633, Accuracy: 0.96875, Computation time: 2.1174423694610596\n",
      "Step: 2809, Loss: 0.9187557697296143, Accuracy: 1.0, Computation time: 1.8402841091156006\n",
      "Step: 2810, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 1.3223047256469727\n",
      "Step: 2811, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 1.1789624691009521\n",
      "Step: 2812, Loss: 0.9375014901161194, Accuracy: 0.96875, Computation time: 1.5271053314208984\n",
      "Step: 2813, Loss: 0.9159873127937317, Accuracy: 1.0, Computation time: 1.4786951541900635\n",
      "Step: 2814, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 1.4404213428497314\n",
      "Step: 2815, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 1.5456335544586182\n",
      "Step: 2816, Loss: 0.9160174131393433, Accuracy: 1.0, Computation time: 1.6815361976623535\n",
      "Step: 2817, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.3455135822296143\n",
      "Step: 2818, Loss: 0.9379415512084961, Accuracy: 0.96875, Computation time: 1.7812995910644531\n",
      "Step: 2819, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 1.9569711685180664\n",
      "Step: 2820, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.5704033374786377\n",
      "Step: 2821, Loss: 0.9159666299819946, Accuracy: 1.0, Computation time: 1.4227845668792725\n",
      "Step: 2822, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.7495059967041016\n",
      "Step: 2823, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.4695031642913818\n",
      "Step: 2824, Loss: 0.926176905632019, Accuracy: 0.96875, Computation time: 1.7007701396942139\n",
      "Step: 2825, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.8159990310668945\n",
      "Step: 2826, Loss: 0.9377328753471375, Accuracy: 0.96875, Computation time: 1.7360200881958008\n",
      "Step: 2827, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 2.1566615104675293\n",
      "Step: 2828, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.7929465770721436\n",
      "Step: 2829, Loss: 0.9384016990661621, Accuracy: 0.96875, Computation time: 1.761561632156372\n",
      "Step: 2830, Loss: 0.9159843921661377, Accuracy: 1.0, Computation time: 1.257758378982544\n",
      "Step: 2831, Loss: 0.9159501194953918, Accuracy: 1.0, Computation time: 1.5669710636138916\n",
      "Step: 2832, Loss: 0.9191190004348755, Accuracy: 1.0, Computation time: 1.9361608028411865\n",
      "Step: 2833, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 1.4726648330688477\n",
      "Step: 2834, Loss: 0.915989339351654, Accuracy: 1.0, Computation time: 2.219507932662964\n",
      "Step: 2835, Loss: 0.9177382588386536, Accuracy: 1.0, Computation time: 1.6313836574554443\n",
      "Step: 2836, Loss: 0.9374276995658875, Accuracy: 0.96875, Computation time: 1.5592904090881348\n",
      "Step: 2837, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 1.4107775688171387\n",
      "Step: 2838, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.5180153846740723\n",
      "Step: 2839, Loss: 0.9377975463867188, Accuracy: 0.96875, Computation time: 1.4494240283966064\n",
      "Step: 2840, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.5364477634429932\n",
      "Step: 2841, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.2457129955291748\n",
      "Step: 2842, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.6104066371917725\n",
      "Step: 2843, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.5909218788146973\n",
      "Step: 2844, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.5994269847869873\n",
      "Step: 2845, Loss: 0.9388260245323181, Accuracy: 0.96875, Computation time: 2.0953190326690674\n",
      "Step: 2846, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.2708525657653809\n",
      "Step: 2847, Loss: 0.9162558913230896, Accuracy: 1.0, Computation time: 1.482924461364746\n",
      "Step: 2848, Loss: 0.9189157485961914, Accuracy: 1.0, Computation time: 2.2441036701202393\n",
      "Step: 2849, Loss: 0.9337915182113647, Accuracy: 0.96875, Computation time: 1.4260470867156982\n",
      "Step: 2850, Loss: 0.9279966354370117, Accuracy: 0.96875, Computation time: 1.5016837120056152\n",
      "Step: 2851, Loss: 0.9377048015594482, Accuracy: 0.96875, Computation time: 1.9728302955627441\n",
      "Step: 2852, Loss: 0.9160725474357605, Accuracy: 1.0, Computation time: 1.491448163986206\n",
      "Step: 2853, Loss: 0.9376266002655029, Accuracy: 0.96875, Computation time: 1.4968962669372559\n",
      "Step: 2854, Loss: 0.9594940543174744, Accuracy: 0.9375, Computation time: 1.845121145248413\n",
      "Step: 2855, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 1.3767788410186768\n",
      "Step: 2856, Loss: 0.9159388542175293, Accuracy: 1.0, Computation time: 1.5990595817565918\n",
      "Step: 2857, Loss: 0.9170872569084167, Accuracy: 1.0, Computation time: 2.3515355587005615\n",
      "Step: 2858, Loss: 0.915985643863678, Accuracy: 1.0, Computation time: 1.3538470268249512\n",
      "Step: 2859, Loss: 0.9160571098327637, Accuracy: 1.0, Computation time: 1.5260748863220215\n",
      "Step: 2860, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 1.5129432678222656\n",
      "Step: 2861, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.5095155239105225\n",
      "Step: 2862, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.4130496978759766\n",
      "Step: 2863, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.2691054344177246\n",
      "Step: 2864, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.3559811115264893\n",
      "Step: 2865, Loss: 0.9374710917472839, Accuracy: 0.96875, Computation time: 1.6583471298217773\n",
      "Step: 2866, Loss: 0.936491847038269, Accuracy: 0.96875, Computation time: 1.60888671875\n",
      "Step: 2867, Loss: 0.9530487656593323, Accuracy: 0.9375, Computation time: 2.1291921138763428\n",
      "Step: 2868, Loss: 0.9159263968467712, Accuracy: 1.0, Computation time: 1.2464168071746826\n",
      "Step: 2869, Loss: 0.9159919619560242, Accuracy: 1.0, Computation time: 1.2495880126953125\n",
      "Step: 2870, Loss: 0.9160487651824951, Accuracy: 1.0, Computation time: 1.411621332168579\n",
      "Step: 2871, Loss: 0.9160882830619812, Accuracy: 1.0, Computation time: 1.4509203433990479\n",
      "Step: 2872, Loss: 0.9162144660949707, Accuracy: 1.0, Computation time: 1.319075345993042\n",
      "Step: 2873, Loss: 0.917595386505127, Accuracy: 1.0, Computation time: 1.4361534118652344\n",
      "Step: 2874, Loss: 0.9274374842643738, Accuracy: 0.96875, Computation time: 1.874986171722412\n",
      "Step: 2875, Loss: 0.9373937249183655, Accuracy: 0.96875, Computation time: 1.8849804401397705\n",
      "Step: 2876, Loss: 0.9159421324729919, Accuracy: 1.0, Computation time: 1.4174156188964844\n",
      "Step: 2877, Loss: 0.9159461259841919, Accuracy: 1.0, Computation time: 1.520836591720581\n",
      "Step: 2878, Loss: 0.9376704096794128, Accuracy: 0.96875, Computation time: 1.317009687423706\n",
      "Step: 2879, Loss: 0.9159772992134094, Accuracy: 1.0, Computation time: 1.6150133609771729\n",
      "Step: 2880, Loss: 0.9376792311668396, Accuracy: 0.96875, Computation time: 1.4627456665039062\n",
      "Step: 2881, Loss: 0.9159697890281677, Accuracy: 1.0, Computation time: 1.3294954299926758\n",
      "Step: 2882, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 1.2855422496795654\n",
      "Step: 2883, Loss: 0.9373921155929565, Accuracy: 0.96875, Computation time: 1.6506953239440918\n",
      "Step: 2884, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.324995994567871\n",
      "Step: 2885, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.6050963401794434\n",
      "Step: 2886, Loss: 0.9160146713256836, Accuracy: 1.0, Computation time: 1.2709524631500244\n",
      "Step: 2887, Loss: 0.915995717048645, Accuracy: 1.0, Computation time: 1.7091796398162842\n",
      "Step: 2888, Loss: 0.9161000847816467, Accuracy: 1.0, Computation time: 1.5827617645263672\n",
      "Step: 2889, Loss: 0.9195403456687927, Accuracy: 1.0, Computation time: 1.4604747295379639\n",
      "Step: 2890, Loss: 0.9160161018371582, Accuracy: 1.0, Computation time: 1.7209086418151855\n",
      "Step: 2891, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.438066005706787\n",
      "Step: 2892, Loss: 0.9330978989601135, Accuracy: 0.96875, Computation time: 1.5466384887695312\n",
      "Step: 2893, Loss: 0.9165838360786438, Accuracy: 1.0, Computation time: 1.4297749996185303\n",
      "Step: 2894, Loss: 0.9161229133605957, Accuracy: 1.0, Computation time: 1.4649591445922852\n",
      "Step: 2895, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 1.5212385654449463\n",
      "Step: 2896, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.1949381828308105\n",
      "Step: 2897, Loss: 0.9376305341720581, Accuracy: 0.96875, Computation time: 1.36458420753479\n",
      "Step: 2898, Loss: 0.9164379239082336, Accuracy: 1.0, Computation time: 2.133793592453003\n",
      "Step: 2899, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.452115774154663\n",
      "Step: 2900, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.6073265075683594\n",
      "Step: 2901, Loss: 0.9376930594444275, Accuracy: 0.96875, Computation time: 1.7760286331176758\n",
      "Step: 2902, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 1.6045079231262207\n",
      "Step: 2903, Loss: 0.9160083532333374, Accuracy: 1.0, Computation time: 1.8449814319610596\n",
      "Step: 2904, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 1.4304771423339844\n",
      "Step: 2905, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 1.41347074508667\n",
      "Step: 2906, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.28737473487854\n",
      "Step: 2907, Loss: 0.9159082770347595, Accuracy: 1.0, Computation time: 1.4510669708251953\n",
      "Step: 2908, Loss: 0.9159348011016846, Accuracy: 1.0, Computation time: 1.2245235443115234\n",
      "Step: 2909, Loss: 0.9159213900566101, Accuracy: 1.0, Computation time: 1.2147486209869385\n",
      "Step: 2910, Loss: 0.9163192510604858, Accuracy: 1.0, Computation time: 1.5401725769042969\n",
      "Step: 2911, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.5975265502929688\n",
      "Step: 2912, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.3430545330047607\n",
      "Step: 2913, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.5918378829956055\n",
      "Step: 2914, Loss: 0.9159435033798218, Accuracy: 1.0, Computation time: 1.603426218032837\n",
      "Step: 2915, Loss: 0.9375825524330139, Accuracy: 0.96875, Computation time: 2.200334072113037\n",
      "Step: 2916, Loss: 0.9168277978897095, Accuracy: 1.0, Computation time: 1.207409143447876\n",
      "Step: 2917, Loss: 0.9448668360710144, Accuracy: 0.96875, Computation time: 2.024681806564331\n",
      "Step: 2918, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 2.032630205154419\n",
      "########################\n",
      "Test loss: 1.118383765220642, Test Accuracy_epoch21: 0.7054735422134399\n",
      "########################\n",
      "Step: 2919, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.6028633117675781\n",
      "Step: 2920, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.238814115524292\n",
      "Step: 2921, Loss: 0.9159759879112244, Accuracy: 1.0, Computation time: 1.4601433277130127\n",
      "Step: 2922, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.394531011581421\n",
      "Step: 2923, Loss: 0.9163433909416199, Accuracy: 1.0, Computation time: 1.4871196746826172\n",
      "Step: 2924, Loss: 0.9310719966888428, Accuracy: 0.96875, Computation time: 2.0546457767486572\n",
      "Step: 2925, Loss: 0.9159719944000244, Accuracy: 1.0, Computation time: 1.6005866527557373\n",
      "Step: 2926, Loss: 0.9161223769187927, Accuracy: 1.0, Computation time: 1.4220023155212402\n",
      "Step: 2927, Loss: 0.9164248704910278, Accuracy: 1.0, Computation time: 1.3884789943695068\n",
      "Step: 2928, Loss: 0.9162843227386475, Accuracy: 1.0, Computation time: 1.5467712879180908\n",
      "Step: 2929, Loss: 0.9197164177894592, Accuracy: 1.0, Computation time: 1.812958002090454\n",
      "Step: 2930, Loss: 0.9161062836647034, Accuracy: 1.0, Computation time: 1.663116216659546\n",
      "Step: 2931, Loss: 0.9163801074028015, Accuracy: 1.0, Computation time: 1.7439286708831787\n",
      "Step: 2932, Loss: 0.9158921837806702, Accuracy: 1.0, Computation time: 1.3820366859436035\n",
      "Step: 2933, Loss: 0.9375776052474976, Accuracy: 0.96875, Computation time: 1.46651029586792\n",
      "Step: 2934, Loss: 0.9161007404327393, Accuracy: 1.0, Computation time: 1.444615125656128\n",
      "Step: 2935, Loss: 0.9161316752433777, Accuracy: 1.0, Computation time: 1.53623628616333\n",
      "Step: 2936, Loss: 0.9161492586135864, Accuracy: 1.0, Computation time: 1.3186957836151123\n",
      "Step: 2937, Loss: 0.9161003828048706, Accuracy: 1.0, Computation time: 1.4913384914398193\n",
      "Step: 2938, Loss: 0.9161500930786133, Accuracy: 1.0, Computation time: 1.6678094863891602\n",
      "Step: 2939, Loss: 0.9313730001449585, Accuracy: 0.96875, Computation time: 1.9127795696258545\n",
      "Step: 2940, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 1.5643525123596191\n",
      "Step: 2941, Loss: 0.9159727096557617, Accuracy: 1.0, Computation time: 1.2940771579742432\n",
      "Step: 2942, Loss: 0.9159755706787109, Accuracy: 1.0, Computation time: 1.5364761352539062\n",
      "Step: 2943, Loss: 0.9377334713935852, Accuracy: 0.96875, Computation time: 1.4899406433105469\n",
      "Step: 2944, Loss: 0.9160653948783875, Accuracy: 1.0, Computation time: 1.543304681777954\n",
      "Step: 2945, Loss: 0.9159311056137085, Accuracy: 1.0, Computation time: 1.6044363975524902\n",
      "Step: 2946, Loss: 0.9161338210105896, Accuracy: 1.0, Computation time: 1.6832709312438965\n",
      "Step: 2947, Loss: 0.93756103515625, Accuracy: 0.96875, Computation time: 1.383329153060913\n",
      "Step: 2948, Loss: 0.9168198108673096, Accuracy: 1.0, Computation time: 1.3295841217041016\n",
      "Step: 2949, Loss: 0.916279673576355, Accuracy: 1.0, Computation time: 1.5872621536254883\n",
      "Step: 2950, Loss: 0.9165098667144775, Accuracy: 1.0, Computation time: 1.6470394134521484\n",
      "Step: 2951, Loss: 0.9160794615745544, Accuracy: 1.0, Computation time: 1.4174809455871582\n",
      "Step: 2952, Loss: 0.9163001179695129, Accuracy: 1.0, Computation time: 1.53127121925354\n",
      "Step: 2953, Loss: 0.9159598350524902, Accuracy: 1.0, Computation time: 1.569678783416748\n",
      "Step: 2954, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.3327217102050781\n",
      "Step: 2955, Loss: 0.9160064458847046, Accuracy: 1.0, Computation time: 1.222536325454712\n",
      "Step: 2956, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 1.1372065544128418\n",
      "Step: 2957, Loss: 0.9575194120407104, Accuracy: 0.9375, Computation time: 1.8235499858856201\n",
      "Step: 2958, Loss: 0.9375203251838684, Accuracy: 0.96875, Computation time: 1.4551115036010742\n",
      "Step: 2959, Loss: 0.9170230031013489, Accuracy: 1.0, Computation time: 1.7345197200775146\n",
      "Step: 2960, Loss: 0.9159307479858398, Accuracy: 1.0, Computation time: 1.9338688850402832\n",
      "Step: 2961, Loss: 0.9376266598701477, Accuracy: 0.96875, Computation time: 1.498467206954956\n",
      "Step: 2962, Loss: 0.9159653186798096, Accuracy: 1.0, Computation time: 1.5735061168670654\n",
      "Step: 2963, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.2695422172546387\n",
      "Step: 2964, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.47236967086792\n",
      "Step: 2965, Loss: 0.937485933303833, Accuracy: 0.96875, Computation time: 1.376366138458252\n",
      "Step: 2966, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 1.4129090309143066\n",
      "Step: 2967, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 1.370452880859375\n",
      "Step: 2968, Loss: 0.921323835849762, Accuracy: 1.0, Computation time: 1.5648822784423828\n",
      "Step: 2969, Loss: 0.9163323640823364, Accuracy: 1.0, Computation time: 1.7541096210479736\n",
      "Step: 2970, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.7587742805480957\n",
      "Step: 2971, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.9743578433990479\n",
      "Step: 2972, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.1709496974945068\n",
      "Step: 2973, Loss: 0.9375938177108765, Accuracy: 0.96875, Computation time: 1.2975616455078125\n",
      "Step: 2974, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.3041625022888184\n",
      "Step: 2975, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 1.6579198837280273\n",
      "Step: 2976, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.6215174198150635\n",
      "Step: 2977, Loss: 0.9376752376556396, Accuracy: 0.96875, Computation time: 2.2918994426727295\n",
      "Step: 2978, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 1.568415641784668\n",
      "Step: 2979, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.4375734329223633\n",
      "Step: 2980, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 1.3561139106750488\n",
      "Step: 2981, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.4748735427856445\n",
      "Step: 2982, Loss: 0.9326522946357727, Accuracy: 0.96875, Computation time: 2.1515285968780518\n",
      "Step: 2983, Loss: 0.9159553647041321, Accuracy: 1.0, Computation time: 1.3608665466308594\n",
      "Step: 2984, Loss: 0.9578126668930054, Accuracy: 0.9375, Computation time: 1.6245343685150146\n",
      "Step: 2985, Loss: 0.937515914440155, Accuracy: 0.96875, Computation time: 1.7857184410095215\n",
      "Step: 2986, Loss: 0.954193115234375, Accuracy: 0.9375, Computation time: 2.529289960861206\n",
      "Step: 2987, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.739720344543457\n",
      "Step: 2988, Loss: 0.9169307947158813, Accuracy: 1.0, Computation time: 1.3726422786712646\n",
      "Step: 2989, Loss: 0.9161537885665894, Accuracy: 1.0, Computation time: 1.3387658596038818\n",
      "Step: 2990, Loss: 0.9374241232872009, Accuracy: 0.96875, Computation time: 1.5817599296569824\n",
      "Step: 2991, Loss: 0.9375461339950562, Accuracy: 0.96875, Computation time: 1.3937067985534668\n",
      "Step: 2992, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 1.3112268447875977\n",
      "Step: 2993, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 2.3527793884277344\n",
      "Step: 2994, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.2507286071777344\n",
      "Step: 2995, Loss: 0.9165370464324951, Accuracy: 1.0, Computation time: 1.6603426933288574\n",
      "Step: 2996, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.7804787158966064\n",
      "Step: 2997, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 1.1718072891235352\n",
      "Step: 2998, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.2448203563690186\n",
      "Step: 2999, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.8051624298095703\n",
      "Step: 3000, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.2599551677703857\n",
      "Step: 3001, Loss: 0.9375690221786499, Accuracy: 0.96875, Computation time: 1.4493253231048584\n",
      "Step: 3002, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.2119932174682617\n",
      "Step: 3003, Loss: 0.9197695851325989, Accuracy: 1.0, Computation time: 2.0029666423797607\n",
      "Step: 3004, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.4003338813781738\n",
      "Step: 3005, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.9508912563323975\n",
      "Step: 3006, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.6161415576934814\n",
      "Step: 3007, Loss: 0.9375438094139099, Accuracy: 0.96875, Computation time: 1.4541730880737305\n",
      "Step: 3008, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.6207046508789062\n",
      "Step: 3009, Loss: 0.9159652590751648, Accuracy: 1.0, Computation time: 2.221285343170166\n",
      "Step: 3010, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 1.389495611190796\n",
      "Step: 3011, Loss: 0.9169080853462219, Accuracy: 1.0, Computation time: 1.602018117904663\n",
      "Step: 3012, Loss: 0.9371110200881958, Accuracy: 0.96875, Computation time: 2.5801315307617188\n",
      "Step: 3013, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.4665436744689941\n",
      "Step: 3014, Loss: 0.9159333109855652, Accuracy: 1.0, Computation time: 1.5341951847076416\n",
      "Step: 3015, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.3008556365966797\n",
      "Step: 3016, Loss: 0.9374192953109741, Accuracy: 0.96875, Computation time: 1.5702073574066162\n",
      "Step: 3017, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 2.24100661277771\n",
      "Step: 3018, Loss: 0.9169151186943054, Accuracy: 1.0, Computation time: 1.594534158706665\n",
      "Step: 3019, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 1.4118247032165527\n",
      "Step: 3020, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.4289214611053467\n",
      "Step: 3021, Loss: 0.9165629148483276, Accuracy: 1.0, Computation time: 1.6563999652862549\n",
      "Step: 3022, Loss: 0.937755823135376, Accuracy: 0.96875, Computation time: 1.5634417533874512\n",
      "Step: 3023, Loss: 0.9374890923500061, Accuracy: 0.96875, Computation time: 1.362823724746704\n",
      "Step: 3024, Loss: 0.9206107258796692, Accuracy: 1.0, Computation time: 1.717203140258789\n",
      "Step: 3025, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 1.6645641326904297\n",
      "Step: 3026, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.5441701412200928\n",
      "Step: 3027, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 1.4598519802093506\n",
      "Step: 3028, Loss: 0.937643826007843, Accuracy: 0.96875, Computation time: 1.3009653091430664\n",
      "Step: 3029, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 1.6552729606628418\n",
      "Step: 3030, Loss: 0.9295562505722046, Accuracy: 0.96875, Computation time: 2.1186373233795166\n",
      "Step: 3031, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.5721571445465088\n",
      "Step: 3032, Loss: 0.9375388622283936, Accuracy: 0.96875, Computation time: 1.3050527572631836\n",
      "Step: 3033, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 1.4386839866638184\n",
      "Step: 3034, Loss: 0.9249081611633301, Accuracy: 1.0, Computation time: 1.765498161315918\n",
      "Step: 3035, Loss: 0.9379028677940369, Accuracy: 0.96875, Computation time: 1.7567570209503174\n",
      "Step: 3036, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.6514267921447754\n",
      "Step: 3037, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 1.4676470756530762\n",
      "Step: 3038, Loss: 0.9355757832527161, Accuracy: 0.96875, Computation time: 2.243913173675537\n",
      "Step: 3039, Loss: 0.9160357713699341, Accuracy: 1.0, Computation time: 1.9320528507232666\n",
      "Step: 3040, Loss: 0.9160053730010986, Accuracy: 1.0, Computation time: 1.2201907634735107\n",
      "Step: 3041, Loss: 0.9160147309303284, Accuracy: 1.0, Computation time: 1.6017482280731201\n",
      "Step: 3042, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 1.4791834354400635\n",
      "Step: 3043, Loss: 0.9160473942756653, Accuracy: 1.0, Computation time: 1.8503613471984863\n",
      "Step: 3044, Loss: 0.9164828658103943, Accuracy: 1.0, Computation time: 1.869802474975586\n",
      "Step: 3045, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.565819501876831\n",
      "Step: 3046, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.515394687652588\n",
      "Step: 3047, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 1.447329044342041\n",
      "Step: 3048, Loss: 0.9376418590545654, Accuracy: 0.96875, Computation time: 1.382960557937622\n",
      "Step: 3049, Loss: 0.9376248121261597, Accuracy: 0.96875, Computation time: 1.479590892791748\n",
      "Step: 3050, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.4082796573638916\n",
      "Step: 3051, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.6320891380310059\n",
      "Step: 3052, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.2079060077667236\n",
      "Step: 3053, Loss: 0.9168664216995239, Accuracy: 1.0, Computation time: 1.408477544784546\n",
      "Step: 3054, Loss: 0.9593992829322815, Accuracy: 0.9375, Computation time: 1.4093503952026367\n",
      "Step: 3055, Loss: 0.937527596950531, Accuracy: 0.96875, Computation time: 1.3858036994934082\n",
      "Step: 3056, Loss: 0.9164563417434692, Accuracy: 1.0, Computation time: 2.431569814682007\n",
      "Step: 3057, Loss: 0.9159538149833679, Accuracy: 1.0, Computation time: 1.4132812023162842\n",
      "########################\n",
      "Test loss: 1.1236870288848877, Test Accuracy_epoch22: 0.6985230445861816\n",
      "########################\n",
      "Step: 3058, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 1.6901414394378662\n",
      "Step: 3059, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.942791223526001\n",
      "Step: 3060, Loss: 0.9164752960205078, Accuracy: 1.0, Computation time: 1.73590087890625\n",
      "Step: 3061, Loss: 0.9159023761749268, Accuracy: 1.0, Computation time: 1.3690226078033447\n",
      "Step: 3062, Loss: 0.9364855885505676, Accuracy: 0.96875, Computation time: 1.8456432819366455\n",
      "Step: 3063, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 1.391507625579834\n",
      "Step: 3064, Loss: 0.9253419637680054, Accuracy: 0.96875, Computation time: 1.760206937789917\n",
      "Step: 3065, Loss: 0.9307533502578735, Accuracy: 0.96875, Computation time: 1.6400461196899414\n",
      "Step: 3066, Loss: 0.9159770011901855, Accuracy: 1.0, Computation time: 1.4336864948272705\n",
      "Step: 3067, Loss: 0.9162560105323792, Accuracy: 1.0, Computation time: 1.6256513595581055\n",
      "Step: 3068, Loss: 0.922831654548645, Accuracy: 1.0, Computation time: 1.9387264251708984\n",
      "Step: 3069, Loss: 0.9161766767501831, Accuracy: 1.0, Computation time: 1.5854358673095703\n",
      "Step: 3070, Loss: 0.9162683486938477, Accuracy: 1.0, Computation time: 1.3432135581970215\n",
      "Step: 3071, Loss: 0.9161630272865295, Accuracy: 1.0, Computation time: 1.86409330368042\n",
      "Step: 3072, Loss: 0.9161140322685242, Accuracy: 1.0, Computation time: 1.3848772048950195\n",
      "Step: 3073, Loss: 0.9161474108695984, Accuracy: 1.0, Computation time: 1.5391383171081543\n",
      "Step: 3074, Loss: 0.915985643863678, Accuracy: 1.0, Computation time: 1.822566032409668\n",
      "Step: 3075, Loss: 0.9160016775131226, Accuracy: 1.0, Computation time: 1.501577615737915\n",
      "Step: 3076, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.3307464122772217\n",
      "Step: 3077, Loss: 0.9159695506095886, Accuracy: 1.0, Computation time: 1.6779696941375732\n",
      "Step: 3078, Loss: 0.9159420728683472, Accuracy: 1.0, Computation time: 1.7709369659423828\n",
      "Step: 3079, Loss: 0.9160440564155579, Accuracy: 1.0, Computation time: 1.7891218662261963\n",
      "Step: 3080, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 1.5650854110717773\n",
      "Step: 3081, Loss: 0.9159725308418274, Accuracy: 1.0, Computation time: 2.091106653213501\n",
      "Step: 3082, Loss: 0.9159594774246216, Accuracy: 1.0, Computation time: 1.4488515853881836\n",
      "Step: 3083, Loss: 0.9244970679283142, Accuracy: 1.0, Computation time: 1.6053211688995361\n",
      "Step: 3084, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.5003008842468262\n",
      "Step: 3085, Loss: 0.9159272313117981, Accuracy: 1.0, Computation time: 1.357753038406372\n",
      "Step: 3086, Loss: 0.9372180700302124, Accuracy: 0.96875, Computation time: 2.088550090789795\n",
      "Step: 3087, Loss: 0.9174166917800903, Accuracy: 1.0, Computation time: 1.4369397163391113\n",
      "Step: 3088, Loss: 0.9160603880882263, Accuracy: 1.0, Computation time: 1.5840425491333008\n",
      "Step: 3089, Loss: 0.9160304069519043, Accuracy: 1.0, Computation time: 1.2425248622894287\n",
      "Step: 3090, Loss: 0.9160134196281433, Accuracy: 1.0, Computation time: 1.4495046138763428\n",
      "Step: 3091, Loss: 0.9160895347595215, Accuracy: 1.0, Computation time: 1.2629599571228027\n",
      "Step: 3092, Loss: 0.9375880360603333, Accuracy: 0.96875, Computation time: 1.1267151832580566\n",
      "Step: 3093, Loss: 0.9375818371772766, Accuracy: 0.96875, Computation time: 1.3347952365875244\n",
      "Step: 3094, Loss: 0.9376625418663025, Accuracy: 0.96875, Computation time: 2.299759864807129\n",
      "Step: 3095, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.2081255912780762\n",
      "Step: 3096, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.4333300590515137\n",
      "Step: 3097, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 1.3404841423034668\n",
      "Step: 3098, Loss: 0.9159286618232727, Accuracy: 1.0, Computation time: 1.2292780876159668\n",
      "Step: 3099, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 1.3697221279144287\n",
      "Step: 3100, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 1.3376877307891846\n",
      "Step: 3101, Loss: 0.9161390662193298, Accuracy: 1.0, Computation time: 1.3507981300354004\n",
      "Step: 3102, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.3002419471740723\n",
      "Step: 3103, Loss: 0.9375742673873901, Accuracy: 0.96875, Computation time: 1.6318464279174805\n",
      "Step: 3104, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.409233808517456\n",
      "Step: 3105, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.4411377906799316\n",
      "Step: 3106, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.610806941986084\n",
      "Step: 3107, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 1.3232004642486572\n",
      "Step: 3108, Loss: 0.9365152716636658, Accuracy: 0.96875, Computation time: 1.3875470161437988\n",
      "Step: 3109, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.3741786479949951\n",
      "Step: 3110, Loss: 0.9160425066947937, Accuracy: 1.0, Computation time: 1.9967646598815918\n",
      "Step: 3111, Loss: 0.9373642802238464, Accuracy: 0.96875, Computation time: 1.8598599433898926\n",
      "Step: 3112, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.3223702907562256\n",
      "Step: 3113, Loss: 0.9190231561660767, Accuracy: 1.0, Computation time: 2.700193166732788\n",
      "Step: 3114, Loss: 0.9375523924827576, Accuracy: 0.96875, Computation time: 1.3962106704711914\n",
      "Step: 3115, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 1.4517042636871338\n",
      "Step: 3116, Loss: 0.9159688353538513, Accuracy: 1.0, Computation time: 1.2751929759979248\n",
      "Step: 3117, Loss: 0.9191544651985168, Accuracy: 1.0, Computation time: 1.6609797477722168\n",
      "Step: 3118, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.4930312633514404\n",
      "Step: 3119, Loss: 0.9160103797912598, Accuracy: 1.0, Computation time: 1.7599589824676514\n",
      "Step: 3120, Loss: 0.9476687908172607, Accuracy: 0.9375, Computation time: 2.4005134105682373\n",
      "Step: 3121, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.5271015167236328\n",
      "Step: 3122, Loss: 0.9160550832748413, Accuracy: 1.0, Computation time: 1.477271556854248\n",
      "Step: 3123, Loss: 0.9161020517349243, Accuracy: 1.0, Computation time: 1.4609901905059814\n",
      "Step: 3124, Loss: 0.9589067101478577, Accuracy: 0.9375, Computation time: 2.127955198287964\n",
      "Step: 3125, Loss: 0.9378069639205933, Accuracy: 0.96875, Computation time: 1.3417398929595947\n",
      "Step: 3126, Loss: 0.916006863117218, Accuracy: 1.0, Computation time: 1.7423443794250488\n",
      "Step: 3127, Loss: 0.9159898161888123, Accuracy: 1.0, Computation time: 1.3575801849365234\n",
      "Step: 3128, Loss: 0.916766881942749, Accuracy: 1.0, Computation time: 1.654165506362915\n",
      "Step: 3129, Loss: 0.9159626364707947, Accuracy: 1.0, Computation time: 1.7986230850219727\n",
      "Step: 3130, Loss: 0.9159095883369446, Accuracy: 1.0, Computation time: 1.4681625366210938\n",
      "Step: 3131, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.3246946334838867\n",
      "Step: 3132, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.430077075958252\n",
      "Step: 3133, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.345238447189331\n",
      "Step: 3134, Loss: 0.9159671664237976, Accuracy: 1.0, Computation time: 1.187187910079956\n",
      "Step: 3135, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.4189860820770264\n",
      "Step: 3136, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 1.629826545715332\n",
      "Step: 3137, Loss: 0.91606605052948, Accuracy: 1.0, Computation time: 1.2843761444091797\n",
      "Step: 3138, Loss: 0.9589921236038208, Accuracy: 0.9375, Computation time: 1.553389549255371\n",
      "Step: 3139, Loss: 0.9374144673347473, Accuracy: 0.96875, Computation time: 1.4593417644500732\n",
      "Step: 3140, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.5580813884735107\n",
      "Step: 3141, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 1.2652778625488281\n",
      "Step: 3142, Loss: 0.9159790873527527, Accuracy: 1.0, Computation time: 1.5655322074890137\n",
      "Step: 3143, Loss: 0.9180117249488831, Accuracy: 1.0, Computation time: 1.8337373733520508\n",
      "Step: 3144, Loss: 0.9181928634643555, Accuracy: 1.0, Computation time: 1.6607182025909424\n",
      "Step: 3145, Loss: 0.9160778522491455, Accuracy: 1.0, Computation time: 1.9143977165222168\n",
      "Step: 3146, Loss: 0.9160085916519165, Accuracy: 1.0, Computation time: 1.2791047096252441\n",
      "Step: 3147, Loss: 0.9159775972366333, Accuracy: 1.0, Computation time: 1.415477991104126\n",
      "Step: 3148, Loss: 0.916100800037384, Accuracy: 1.0, Computation time: 1.3483645915985107\n",
      "Step: 3149, Loss: 0.9352770447731018, Accuracy: 0.96875, Computation time: 1.990736961364746\n",
      "Step: 3150, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 1.467688798904419\n",
      "Step: 3151, Loss: 0.9165768623352051, Accuracy: 1.0, Computation time: 1.5100820064544678\n",
      "Step: 3152, Loss: 0.9159483313560486, Accuracy: 1.0, Computation time: 1.2608628273010254\n",
      "Step: 3153, Loss: 0.9371182322502136, Accuracy: 0.96875, Computation time: 1.4969518184661865\n",
      "Step: 3154, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 1.812645435333252\n",
      "Step: 3155, Loss: 0.9159618616104126, Accuracy: 1.0, Computation time: 1.582456350326538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5410, Loss: 0.9374254941940308, Accuracy: 0.96875, Computation time: 1.6823835372924805\n",
      "Step: 5411, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.3165574073791504\n",
      "Step: 5412, Loss: 0.9160820841789246, Accuracy: 1.0, Computation time: 1.274247646331787\n",
      "Step: 5413, Loss: 0.9374918341636658, Accuracy: 0.96875, Computation time: 1.7563416957855225\n",
      "Step: 5414, Loss: 0.944476306438446, Accuracy: 0.96875, Computation time: 1.5062346458435059\n",
      "Step: 5415, Loss: 0.91591477394104, Accuracy: 1.0, Computation time: 1.5913662910461426\n",
      "Step: 5416, Loss: 0.915944516658783, Accuracy: 1.0, Computation time: 1.5396370887756348\n",
      "Step: 5417, Loss: 0.9163374900817871, Accuracy: 1.0, Computation time: 1.2637181282043457\n",
      "Step: 5418, Loss: 0.9159455895423889, Accuracy: 1.0, Computation time: 1.4301233291625977\n",
      "Step: 5419, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 1.5557889938354492\n",
      "########################\n",
      "Test loss: 1.1172137260437012, Test Accuracy_epoch39: 0.7098175883293152\n",
      "########################\n",
      "Step: 5420, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.214010238647461\n",
      "Step: 5421, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.256507158279419\n",
      "Step: 5422, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.4455554485321045\n",
      "Step: 5423, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.5048637390136719\n",
      "Step: 5424, Loss: 0.9160448908805847, Accuracy: 1.0, Computation time: 1.6743695735931396\n",
      "Step: 5425, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.183903455734253\n",
      "Step: 5426, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.647608757019043\n",
      "Step: 5427, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 1.3609392642974854\n",
      "Step: 5428, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 1.3332257270812988\n",
      "Step: 5429, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.2074201107025146\n",
      "Step: 5430, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.561856985092163\n",
      "Step: 5431, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.2637076377868652\n",
      "Step: 5432, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.5924546718597412\n",
      "Step: 5433, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.2699611186981201\n",
      "Step: 5434, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.2427172660827637\n",
      "Step: 5435, Loss: 0.9162781834602356, Accuracy: 1.0, Computation time: 1.268066644668579\n",
      "Step: 5436, Loss: 0.937532365322113, Accuracy: 0.96875, Computation time: 1.7014126777648926\n",
      "Step: 5437, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.3559341430664062\n",
      "Step: 5438, Loss: 0.9375244379043579, Accuracy: 0.96875, Computation time: 1.3887598514556885\n",
      "Step: 5439, Loss: 0.9158340692520142, Accuracy: 1.0, Computation time: 0.9538378715515137\n",
      "Step: 5440, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.408323049545288\n",
      "Step: 5441, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.1183669567108154\n",
      "Step: 5442, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.426877498626709\n",
      "Step: 5443, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.1397769451141357\n",
      "Step: 5444, Loss: 0.915903627872467, Accuracy: 1.0, Computation time: 1.2744286060333252\n",
      "Step: 5445, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.1845967769622803\n",
      "Step: 5446, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.0057003498077393\n",
      "Step: 5447, Loss: 0.9159793257713318, Accuracy: 1.0, Computation time: 1.2481660842895508\n",
      "Step: 5448, Loss: 0.9373202323913574, Accuracy: 0.96875, Computation time: 1.5448122024536133\n",
      "Step: 5449, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 1.3244528770446777\n",
      "Step: 5450, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.3537213802337646\n",
      "Step: 5451, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.2657103538513184\n",
      "Step: 5452, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.5015380382537842\n",
      "Step: 5453, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.11845064163208\n",
      "Step: 5454, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.3941304683685303\n",
      "Step: 5455, Loss: 0.9161103367805481, Accuracy: 1.0, Computation time: 1.4524552822113037\n",
      "Step: 5456, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 1.109464168548584\n",
      "Step: 5457, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 1.4626884460449219\n",
      "Step: 5458, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.910222053527832\n",
      "Step: 5459, Loss: 0.9359441995620728, Accuracy: 0.96875, Computation time: 1.5678884983062744\n",
      "Step: 5460, Loss: 0.9375443458557129, Accuracy: 0.96875, Computation time: 1.573617935180664\n",
      "Step: 5461, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.3838777542114258\n",
      "Step: 5462, Loss: 0.9159397482872009, Accuracy: 1.0, Computation time: 1.2684149742126465\n",
      "Step: 5463, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.0995535850524902\n",
      "Step: 5464, Loss: 0.9374475479125977, Accuracy: 0.96875, Computation time: 1.4200997352600098\n",
      "Step: 5465, Loss: 0.9159387350082397, Accuracy: 1.0, Computation time: 1.3909316062927246\n",
      "Step: 5466, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.4874517917633057\n",
      "Step: 5467, Loss: 0.9368900656700134, Accuracy: 0.96875, Computation time: 1.2670273780822754\n",
      "Step: 5468, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.2633953094482422\n",
      "Step: 5469, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.547417163848877\n",
      "Step: 5470, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.4021403789520264\n",
      "Step: 5471, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.262497901916504\n",
      "Step: 5476, Loss: 0.9159637093544006, Accuracy: 1.0, Computation time: 1.3017292022705078\n",
      "Step: 5477, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.4146673679351807\n",
      "Step: 5478, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.1695282459259033\n",
      "Step: 5479, Loss: 0.9158401489257812, Accuracy: 1.0, Computation time: 1.342423677444458\n",
      "Step: 5480, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.1892528533935547\n",
      "Step: 5481, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.0296330451965332\n",
      "Step: 5482, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.230433702468872\n",
      "Step: 5483, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.1389069557189941\n",
      "Step: 5484, Loss: 0.9372890591621399, Accuracy: 0.96875, Computation time: 1.2528290748596191\n",
      "Step: 5485, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.3509297370910645\n",
      "Step: 5486, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.3304193019866943\n",
      "Step: 5487, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.094233751296997\n",
      "Step: 5488, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.3123722076416016\n",
      "Step: 5489, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.2202088832855225\n",
      "Step: 5490, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.931962490081787\n",
      "Step: 5491, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.3326008319854736\n",
      "Step: 5492, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 1.161827802658081\n",
      "Step: 5493, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 1.1587607860565186\n",
      "Step: 5494, Loss: 0.9372269511222839, Accuracy: 0.96875, Computation time: 1.3030338287353516\n",
      "Step: 5495, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 1.2408154010772705\n",
      "Step: 5496, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 1.3489983081817627\n",
      "Step: 5497, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.4111707210540771\n",
      "Step: 5498, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.437225341796875\n",
      "Step: 5499, Loss: 0.9163306355476379, Accuracy: 1.0, Computation time: 1.5797276496887207\n",
      "Step: 5500, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.2380340099334717\n",
      "Step: 5501, Loss: 0.9158362746238708, Accuracy: 1.0, Computation time: 1.1877577304840088\n",
      "Step: 5502, Loss: 0.9375414848327637, Accuracy: 0.96875, Computation time: 1.4676201343536377\n",
      "Step: 5503, Loss: 0.9234299063682556, Accuracy: 1.0, Computation time: 1.311708927154541\n",
      "Step: 5504, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.527573823928833\n",
      "Step: 5505, Loss: 0.9159555435180664, Accuracy: 1.0, Computation time: 1.5114521980285645\n",
      "Step: 5506, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.1302623748779297\n",
      "Step: 5507, Loss: 0.9376211166381836, Accuracy: 0.96875, Computation time: 1.2291440963745117\n",
      "Step: 5508, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 1.1915230751037598\n",
      "Step: 5509, Loss: 0.9375330805778503, Accuracy: 0.96875, Computation time: 1.3460516929626465\n",
      "Step: 5510, Loss: 0.9592140913009644, Accuracy: 0.9375, Computation time: 1.3887836933135986\n",
      "Step: 5511, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.899003028869629\n",
      "Step: 5512, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 1.3269710540771484\n",
      "Step: 5513, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.3525819778442383\n",
      "Step: 5514, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.5364198684692383\n",
      "Step: 5515, Loss: 0.9213994145393372, Accuracy: 1.0, Computation time: 1.605895757675171\n",
      "Step: 5516, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.2855982780456543\n",
      "Step: 5517, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 1.6086115837097168\n",
      "Step: 5518, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.802955150604248\n",
      "Step: 5519, Loss: 0.9207062721252441, Accuracy: 1.0, Computation time: 2.16849422454834\n",
      "Step: 5520, Loss: 0.9376352429389954, Accuracy: 0.96875, Computation time: 1.521517276763916\n",
      "Step: 5521, Loss: 0.9159671068191528, Accuracy: 1.0, Computation time: 1.4010882377624512\n",
      "Step: 5522, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.3839454650878906\n",
      "Step: 5523, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.2661993503570557\n",
      "Step: 5524, Loss: 0.9589568376541138, Accuracy: 0.9375, Computation time: 1.7207622528076172\n",
      "Step: 5525, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.5570690631866455\n",
      "Step: 5526, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.2540569305419922\n",
      "Step: 5527, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 1.6040880680084229\n",
      "Step: 5528, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.293278455734253\n",
      "Step: 5529, Loss: 0.9375719428062439, Accuracy: 0.96875, Computation time: 1.5447447299957275\n",
      "Step: 5530, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.460472583770752\n",
      "Step: 5531, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.319751262664795\n",
      "Step: 5532, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.2164783477783203\n",
      "Step: 5533, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.628725528717041\n",
      "Step: 5534, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.401432991027832\n",
      "Step: 5535, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 1.426330327987671\n",
      "Step: 5536, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.7435541152954102\n",
      "Step: 5537, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.3732185363769531\n",
      "Step: 5538, Loss: 0.9374789595603943, Accuracy: 0.96875, Computation time: 1.3107995986938477\n",
      "Step: 5539, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.2387876510620117\n",
      "Step: 5540, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.5475759506225586\n",
      "Step: 5541, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.495962142944336\n",
      "Step: 5542, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.7122602462768555\n",
      "Step: 5543, Loss: 0.9303939342498779, Accuracy: 0.96875, Computation time: 1.8999555110931396\n",
      "Step: 5544, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.0706756114959717\n",
      "Step: 5545, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 1.5272908210754395\n",
      "Step: 5546, Loss: 0.9376620650291443, Accuracy: 0.96875, Computation time: 1.3004119396209717\n",
      "Step: 5547, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.388791799545288\n",
      "Step: 5548, Loss: 0.9376324415206909, Accuracy: 0.96875, Computation time: 1.728966474533081\n",
      "Step: 5549, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 1.3244612216949463\n",
      "Step: 5550, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.6699843406677246\n",
      "Step: 5551, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.3281536102294922\n",
      "Step: 5552, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.513106346130371\n",
      "Step: 5553, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 1.6422255039215088\n",
      "Step: 5554, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.4149470329284668\n",
      "Step: 5555, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 1.3439233303070068\n",
      "Step: 5556, Loss: 0.9158394932746887, Accuracy: 1.0, Computation time: 1.217696189880371\n",
      "Step: 5557, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.2532141208648682\n",
      "Step: 5558, Loss: 0.91584312915802, Accuracy: 1.0, Computation time: 1.671292781829834\n",
      "########################\n",
      "Test loss: 1.1181915998458862, Test Accuracy_epoch40: 0.706342339515686\n",
      "########################\n",
      "Step: 5559, Loss: 0.9159501194953918, Accuracy: 1.0, Computation time: 1.5690627098083496\n",
      "Step: 5560, Loss: 0.9158448576927185, Accuracy: 1.0, Computation time: 1.9055981636047363\n",
      "Step: 5561, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.3282175064086914\n",
      "Step: 5562, Loss: 0.9161519408226013, Accuracy: 1.0, Computation time: 2.171776294708252\n",
      "Step: 5563, Loss: 0.9331169128417969, Accuracy: 0.96875, Computation time: 1.5840024948120117\n",
      "Step: 5564, Loss: 0.933711051940918, Accuracy: 0.96875, Computation time: 1.778120517730713\n",
      "Step: 5565, Loss: 0.9374890327453613, Accuracy: 0.96875, Computation time: 1.2681324481964111\n",
      "Step: 5566, Loss: 0.9160709381103516, Accuracy: 1.0, Computation time: 1.5281119346618652\n",
      "Step: 5567, Loss: 0.916084885597229, Accuracy: 1.0, Computation time: 1.8375601768493652\n",
      "Step: 5568, Loss: 0.9169121980667114, Accuracy: 1.0, Computation time: 1.6461637020111084\n",
      "Step: 5569, Loss: 0.9377062320709229, Accuracy: 0.96875, Computation time: 1.6563208103179932\n",
      "Step: 5570, Loss: 0.9211592674255371, Accuracy: 1.0, Computation time: 1.6154494285583496\n",
      "Step: 5571, Loss: 0.9160730838775635, Accuracy: 1.0, Computation time: 2.3644015789031982\n",
      "Step: 5572, Loss: 0.937443733215332, Accuracy: 0.96875, Computation time: 1.3230493068695068\n",
      "Step: 5573, Loss: 0.9376703500747681, Accuracy: 0.96875, Computation time: 1.7001276016235352\n",
      "Step: 5574, Loss: 0.937725305557251, Accuracy: 0.96875, Computation time: 1.7204322814941406\n",
      "Step: 5575, Loss: 0.9160706996917725, Accuracy: 1.0, Computation time: 1.448608636856079\n",
      "Step: 5576, Loss: 0.9161216020584106, Accuracy: 1.0, Computation time: 1.605520486831665\n",
      "Step: 5577, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 1.7794289588928223\n",
      "Step: 5578, Loss: 0.9159144163131714, Accuracy: 1.0, Computation time: 1.4410457611083984\n",
      "Step: 5579, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.6773319244384766\n",
      "Step: 5580, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.6413280963897705\n",
      "Step: 5581, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.4176905155181885\n",
      "Step: 5582, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 1.5319921970367432\n",
      "Step: 5583, Loss: 0.9159554839134216, Accuracy: 1.0, Computation time: 1.5706915855407715\n",
      "Step: 5584, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 1.487743616104126\n",
      "Step: 5585, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.8889505863189697\n",
      "Step: 5586, Loss: 0.9375181198120117, Accuracy: 0.96875, Computation time: 1.7108125686645508\n",
      "Step: 5587, Loss: 0.9205816984176636, Accuracy: 1.0, Computation time: 1.8460123538970947\n",
      "Step: 5588, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.7749500274658203\n",
      "Step: 5589, Loss: 0.9158821105957031, Accuracy: 1.0, Computation time: 1.4292218685150146\n",
      "Step: 5590, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 1.6266074180603027\n",
      "Step: 5591, Loss: 0.9159172773361206, Accuracy: 1.0, Computation time: 1.5192599296569824\n",
      "Step: 5592, Loss: 0.915923535823822, Accuracy: 1.0, Computation time: 1.528515338897705\n",
      "Step: 5593, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.5209128856658936\n",
      "Step: 5594, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 2.0535707473754883\n",
      "Step: 5595, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.281097650527954\n",
      "Step: 5596, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 1.5885345935821533\n",
      "Step: 5597, Loss: 0.9183899760246277, Accuracy: 1.0, Computation time: 1.5687556266784668\n",
      "Step: 5598, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.7789855003356934\n",
      "Step: 5599, Loss: 0.931877851486206, Accuracy: 0.96875, Computation time: 1.6221673488616943\n",
      "Step: 5600, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.4327244758605957\n",
      "Step: 5601, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.5712389945983887\n",
      "Step: 5602, Loss: 0.9362093210220337, Accuracy: 0.96875, Computation time: 1.9434831142425537\n",
      "Step: 5603, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.208022117614746\n",
      "Step: 5604, Loss: 0.9159982204437256, Accuracy: 1.0, Computation time: 1.51625657081604\n",
      "Step: 5605, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.3685717582702637\n",
      "Step: 5606, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 1.3364074230194092\n",
      "Step: 5607, Loss: 0.9373875856399536, Accuracy: 0.96875, Computation time: 1.1215415000915527\n",
      "Step: 5608, Loss: 0.9365602731704712, Accuracy: 0.96875, Computation time: 1.3819513320922852\n",
      "Step: 5609, Loss: 0.916175127029419, Accuracy: 1.0, Computation time: 1.9067590236663818\n",
      "Step: 5610, Loss: 0.9158987402915955, Accuracy: 1.0, Computation time: 1.3124542236328125\n",
      "Step: 5611, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.4271130561828613\n",
      "Step: 5612, Loss: 0.9373603463172913, Accuracy: 0.96875, Computation time: 1.8062949180603027\n",
      "Step: 5613, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.6296985149383545\n",
      "Step: 5614, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.474064826965332\n",
      "Step: 5615, Loss: 0.9159231185913086, Accuracy: 1.0, Computation time: 1.8612627983093262\n",
      "Step: 5616, Loss: 0.9375410079956055, Accuracy: 0.96875, Computation time: 1.4790077209472656\n",
      "Step: 5617, Loss: 0.9373253583908081, Accuracy: 0.96875, Computation time: 1.7425682544708252\n",
      "Step: 5618, Loss: 0.9593461751937866, Accuracy: 0.9375, Computation time: 1.7160296440124512\n",
      "Step: 5619, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 1.6788723468780518\n",
      "Step: 5620, Loss: 0.9159033894538879, Accuracy: 1.0, Computation time: 1.4432988166809082\n",
      "Step: 5621, Loss: 0.9369192719459534, Accuracy: 0.96875, Computation time: 1.5895965099334717\n",
      "Step: 5622, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 1.365450143814087\n",
      "Step: 5623, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.6557574272155762\n",
      "Step: 5624, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.729231357574463\n",
      "Step: 5625, Loss: 0.9375128149986267, Accuracy: 0.96875, Computation time: 1.8256921768188477\n",
      "Step: 5626, Loss: 0.9221846461296082, Accuracy: 1.0, Computation time: 1.6203117370605469\n",
      "Step: 5627, Loss: 0.9163686633110046, Accuracy: 1.0, Computation time: 1.6101100444793701\n",
      "Step: 5628, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.458690881729126\n",
      "Step: 5629, Loss: 0.9163634777069092, Accuracy: 1.0, Computation time: 1.5207514762878418\n",
      "Step: 5630, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.4294953346252441\n",
      "Step: 5631, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 2.0542566776275635\n",
      "Step: 5632, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 1.6624741554260254\n",
      "Step: 5633, Loss: 0.916023850440979, Accuracy: 1.0, Computation time: 1.4249379634857178\n",
      "Step: 5634, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.3879303932189941\n",
      "Step: 5635, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.336778163909912\n",
      "Step: 5636, Loss: 0.937556803226471, Accuracy: 0.96875, Computation time: 1.6693310737609863\n",
      "Step: 5637, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.3950021266937256\n",
      "Step: 5638, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.3943004608154297\n",
      "Step: 5639, Loss: 0.916048526763916, Accuracy: 1.0, Computation time: 1.4361066818237305\n",
      "Step: 5640, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.4454705715179443\n",
      "Step: 5641, Loss: 0.9375381469726562, Accuracy: 0.96875, Computation time: 1.318525791168213\n",
      "Step: 5642, Loss: 0.9374219179153442, Accuracy: 0.96875, Computation time: 1.6374330520629883\n",
      "Step: 5643, Loss: 0.9162571430206299, Accuracy: 1.0, Computation time: 1.8424534797668457\n",
      "Step: 5644, Loss: 0.9375225901603699, Accuracy: 0.96875, Computation time: 1.6052730083465576\n",
      "Step: 5645, Loss: 0.9160571098327637, Accuracy: 1.0, Computation time: 1.5603160858154297\n",
      "Step: 5646, Loss: 0.9177665710449219, Accuracy: 1.0, Computation time: 1.9352097511291504\n",
      "Step: 5647, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 1.4768776893615723\n",
      "Step: 5648, Loss: 0.9159123301506042, Accuracy: 1.0, Computation time: 1.7426087856292725\n",
      "Step: 5649, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.4123802185058594\n",
      "Step: 5650, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.2874701023101807\n",
      "Step: 5651, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.3792078495025635\n",
      "Step: 5652, Loss: 0.9161924719810486, Accuracy: 1.0, Computation time: 1.3569693565368652\n",
      "Step: 5653, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.4241445064544678\n",
      "Step: 5654, Loss: 0.9351024031639099, Accuracy: 0.96875, Computation time: 2.0421524047851562\n",
      "Step: 5655, Loss: 0.9422855377197266, Accuracy: 0.96875, Computation time: 1.7123689651489258\n",
      "Step: 5656, Loss: 0.9159033894538879, Accuracy: 1.0, Computation time: 1.6095621585845947\n",
      "Step: 5657, Loss: 0.9173507690429688, Accuracy: 1.0, Computation time: 1.4999287128448486\n",
      "Step: 5658, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.3587825298309326\n",
      "Step: 5659, Loss: 0.9376445412635803, Accuracy: 0.96875, Computation time: 1.6934003829956055\n",
      "Step: 5660, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.7556371688842773\n",
      "Step: 5661, Loss: 0.9377400875091553, Accuracy: 0.96875, Computation time: 1.290889024734497\n",
      "Step: 5662, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.8453922271728516\n",
      "Step: 5663, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.7008881568908691\n",
      "Step: 5664, Loss: 0.9205656051635742, Accuracy: 1.0, Computation time: 1.8492999076843262\n",
      "Step: 5665, Loss: 0.9163510203361511, Accuracy: 1.0, Computation time: 1.4127881526947021\n",
      "Step: 5666, Loss: 0.9159385561943054, Accuracy: 1.0, Computation time: 2.1926498413085938\n",
      "Step: 5667, Loss: 0.9159860014915466, Accuracy: 1.0, Computation time: 1.6941719055175781\n",
      "Step: 5668, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.6373543739318848\n",
      "Step: 5669, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.4671919345855713\n",
      "Step: 5670, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.694464921951294\n",
      "Step: 5671, Loss: 0.9375166296958923, Accuracy: 0.96875, Computation time: 1.8276252746582031\n",
      "Step: 5672, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.4677133560180664\n",
      "Step: 5673, Loss: 0.9159675240516663, Accuracy: 1.0, Computation time: 1.4883043766021729\n",
      "Step: 5674, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.4873104095458984\n",
      "Step: 5675, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.6702871322631836\n",
      "Step: 5676, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 1.47218918800354\n",
      "Step: 5677, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.7822835445404053\n",
      "Step: 5678, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.209040880203247\n",
      "Step: 5679, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 2.067413568496704\n",
      "Step: 5680, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.8227927684783936\n",
      "Step: 5681, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.8943052291870117\n",
      "Step: 5682, Loss: 0.9372316598892212, Accuracy: 0.96875, Computation time: 1.4415626525878906\n",
      "Step: 5683, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.2493798732757568\n",
      "Step: 5684, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.5327599048614502\n",
      "Step: 5685, Loss: 0.9158380627632141, Accuracy: 1.0, Computation time: 1.8400700092315674\n",
      "Step: 5686, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 1.631272554397583\n",
      "Step: 5687, Loss: 0.9372901320457458, Accuracy: 0.96875, Computation time: 1.3799688816070557\n",
      "Step: 5688, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.3831408023834229\n",
      "Step: 5689, Loss: 0.917009711265564, Accuracy: 1.0, Computation time: 1.566694974899292\n",
      "Step: 5690, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.419262170791626\n",
      "Step: 5691, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.4825150966644287\n",
      "Step: 5692, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.337251901626587\n",
      "Step: 5693, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.048396348953247\n",
      "Step: 5694, Loss: 0.9375656247138977, Accuracy: 0.96875, Computation time: 1.8527450561523438\n",
      "Step: 5695, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 1.2731804847717285\n",
      "Step: 5696, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.368194818496704\n",
      "Step: 5697, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.4018628597259521\n",
      "########################\n",
      "Test loss: 1.1211676597595215, Test Accuracy_epoch41: 0.7011294960975647\n",
      "########################\n",
      "Step: 5698, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.249758005142212\n",
      "Step: 5699, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 1.8694658279418945\n",
      "Step: 5700, Loss: 0.920708179473877, Accuracy: 1.0, Computation time: 1.7047302722930908\n",
      "Step: 5701, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.2872850894927979\n",
      "Step: 5702, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.7792325019836426\n",
      "Step: 5703, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.569852590560913\n",
      "Step: 5704, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.7476873397827148\n",
      "Step: 5705, Loss: 0.9375583529472351, Accuracy: 0.96875, Computation time: 1.656064510345459\n",
      "Step: 5706, Loss: 0.9375237822532654, Accuracy: 0.96875, Computation time: 2.0101804733276367\n",
      "Step: 5707, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 1.5414667129516602\n",
      "Step: 5708, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.4936778545379639\n",
      "Step: 5709, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.459650993347168\n",
      "Step: 5710, Loss: 0.9159121513366699, Accuracy: 1.0, Computation time: 1.6694328784942627\n",
      "Step: 5711, Loss: 0.9197438955307007, Accuracy: 1.0, Computation time: 1.6747145652770996\n",
      "Step: 5712, Loss: 0.9201581478118896, Accuracy: 1.0, Computation time: 1.661177158355713\n",
      "Step: 5713, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.7415800094604492\n",
      "Step: 5714, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.3920631408691406\n",
      "Step: 5715, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 1.5224857330322266\n",
      "Step: 5716, Loss: 0.9375934600830078, Accuracy: 0.96875, Computation time: 1.4503917694091797\n",
      "Step: 5717, Loss: 0.9222919940948486, Accuracy: 1.0, Computation time: 1.9996790885925293\n",
      "Step: 5718, Loss: 0.916455090045929, Accuracy: 1.0, Computation time: 1.7658753395080566\n",
      "Step: 5719, Loss: 0.9159558415412903, Accuracy: 1.0, Computation time: 1.6482141017913818\n",
      "Step: 5720, Loss: 0.9159439206123352, Accuracy: 1.0, Computation time: 1.7919096946716309\n",
      "Step: 5721, Loss: 0.9307164549827576, Accuracy: 0.96875, Computation time: 1.689527988433838\n",
      "Step: 5722, Loss: 0.9161078333854675, Accuracy: 1.0, Computation time: 1.7695550918579102\n",
      "Step: 5723, Loss: 0.9161337614059448, Accuracy: 1.0, Computation time: 1.4868273735046387\n",
      "Step: 5724, Loss: 0.9161195158958435, Accuracy: 1.0, Computation time: 1.564180612564087\n",
      "Step: 5725, Loss: 0.915983259677887, Accuracy: 1.0, Computation time: 1.4648776054382324\n",
      "Step: 5726, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.665905237197876\n",
      "Step: 5727, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.3918678760528564\n",
      "Step: 5728, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.0613796710968018\n",
      "Step: 5729, Loss: 0.9375433921813965, Accuracy: 0.96875, Computation time: 1.144540786743164\n",
      "Step: 5730, Loss: 0.9233466386795044, Accuracy: 1.0, Computation time: 1.4081523418426514\n",
      "Step: 5731, Loss: 0.9159800410270691, Accuracy: 1.0, Computation time: 1.1624486446380615\n",
      "Step: 5732, Loss: 0.9162347316741943, Accuracy: 1.0, Computation time: 2.1532700061798096\n",
      "Step: 5733, Loss: 0.9161067008972168, Accuracy: 1.0, Computation time: 1.3329555988311768\n",
      "Step: 5734, Loss: 0.9159983396530151, Accuracy: 1.0, Computation time: 1.6332623958587646\n",
      "Step: 5735, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 1.6494741439819336\n",
      "Step: 5736, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.6787707805633545\n",
      "Step: 5737, Loss: 0.9592851996421814, Accuracy: 0.9375, Computation time: 1.518763542175293\n",
      "Step: 5738, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.8725483417510986\n",
      "Step: 5739, Loss: 0.9160166382789612, Accuracy: 1.0, Computation time: 1.4006681442260742\n",
      "Step: 5740, Loss: 0.9159913063049316, Accuracy: 1.0, Computation time: 1.3914098739624023\n",
      "Step: 5741, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 2.0112171173095703\n",
      "Step: 5742, Loss: 0.915978729724884, Accuracy: 1.0, Computation time: 1.3557648658752441\n",
      "Step: 5743, Loss: 0.9331129789352417, Accuracy: 0.96875, Computation time: 1.838620662689209\n",
      "Step: 5744, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.514411211013794\n",
      "Step: 5745, Loss: 0.9160196185112, Accuracy: 1.0, Computation time: 1.7840979099273682\n",
      "Step: 5746, Loss: 0.915952205657959, Accuracy: 1.0, Computation time: 2.1593565940856934\n",
      "Step: 5747, Loss: 0.9377012848854065, Accuracy: 0.96875, Computation time: 1.1388843059539795\n",
      "Step: 5748, Loss: 0.9159789681434631, Accuracy: 1.0, Computation time: 1.2600760459899902\n",
      "Step: 5749, Loss: 0.9160678386688232, Accuracy: 1.0, Computation time: 1.8298697471618652\n",
      "Step: 5750, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 1.0025649070739746\n",
      "Step: 5751, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.9667551517486572\n",
      "Step: 5752, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 1.9032340049743652\n",
      "Step: 5753, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.887171745300293\n",
      "Step: 5754, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 2.1532938480377197\n",
      "Step: 5755, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.1757502555847168\n",
      "Step: 5756, Loss: 0.9159724712371826, Accuracy: 1.0, Computation time: 1.0901165008544922\n",
      "Step: 5757, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 1.5126652717590332\n",
      "Step: 5758, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.2213411331176758\n",
      "Step: 5759, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.2002613544464111\n",
      "Step: 5760, Loss: 0.9160733222961426, Accuracy: 1.0, Computation time: 1.6819660663604736\n",
      "Step: 5761, Loss: 0.9159306287765503, Accuracy: 1.0, Computation time: 1.1553263664245605\n",
      "Step: 5762, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.2854576110839844\n",
      "Step: 5763, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.336914300918579\n",
      "Step: 5764, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 1.568814754486084\n",
      "Step: 5765, Loss: 0.9375448226928711, Accuracy: 0.96875, Computation time: 1.157578468322754\n",
      "Step: 5766, Loss: 0.9163144826889038, Accuracy: 1.0, Computation time: 1.6177732944488525\n",
      "Step: 5767, Loss: 0.9163422584533691, Accuracy: 1.0, Computation time: 1.390582799911499\n",
      "Step: 5768, Loss: 0.9159457087516785, Accuracy: 1.0, Computation time: 1.2270994186401367\n",
      "Step: 5769, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.0976250171661377\n",
      "Step: 5770, Loss: 0.9159918427467346, Accuracy: 1.0, Computation time: 1.1655898094177246\n",
      "Step: 5771, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 1.2652003765106201\n",
      "Step: 5772, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.259075403213501\n",
      "Step: 5773, Loss: 0.9375045895576477, Accuracy: 0.96875, Computation time: 1.06797456741333\n",
      "Step: 5774, Loss: 0.9184003472328186, Accuracy: 1.0, Computation time: 1.323559045791626\n",
      "Step: 5775, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.3089547157287598\n",
      "Step: 5776, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 1.3034088611602783\n",
      "Step: 5777, Loss: 0.917175829410553, Accuracy: 1.0, Computation time: 1.5627970695495605\n",
      "Step: 5778, Loss: 0.9159908294677734, Accuracy: 1.0, Computation time: 1.57279634475708\n",
      "Step: 5779, Loss: 0.9376336932182312, Accuracy: 0.96875, Computation time: 1.4645581245422363\n",
      "Step: 5780, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.337235927581787\n",
      "Step: 5781, Loss: 0.9375867247581482, Accuracy: 0.96875, Computation time: 1.4205267429351807\n",
      "Step: 5782, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.0704526901245117\n",
      "Step: 5783, Loss: 0.9568700194358826, Accuracy: 0.9375, Computation time: 2.226088285446167\n",
      "Step: 5784, Loss: 0.916034996509552, Accuracy: 1.0, Computation time: 1.242788314819336\n",
      "Step: 5785, Loss: 0.9159799814224243, Accuracy: 1.0, Computation time: 1.4133777618408203\n",
      "Step: 5786, Loss: 0.916107714176178, Accuracy: 1.0, Computation time: 1.244001865386963\n",
      "Step: 5787, Loss: 0.915946364402771, Accuracy: 1.0, Computation time: 1.372147798538208\n",
      "Step: 5788, Loss: 0.9159579873085022, Accuracy: 1.0, Computation time: 1.0919485092163086\n",
      "Step: 5789, Loss: 0.9375373125076294, Accuracy: 0.96875, Computation time: 1.299757719039917\n",
      "Step: 5790, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 2.3682119846343994\n",
      "Step: 5791, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.200049877166748\n",
      "Step: 5792, Loss: 0.9160540103912354, Accuracy: 1.0, Computation time: 1.4029104709625244\n",
      "Step: 5793, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.9968700408935547\n",
      "Step: 5794, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.0753648281097412\n",
      "Step: 5795, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.242527723312378\n",
      "Step: 5796, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.4616544246673584\n",
      "Step: 5797, Loss: 0.9160069823265076, Accuracy: 1.0, Computation time: 1.6504113674163818\n",
      "Step: 5798, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.40476393699646\n",
      "Step: 5799, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.7111389636993408\n",
      "Step: 5800, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.3791110515594482\n",
      "Step: 5801, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.1486868858337402\n",
      "Step: 5802, Loss: 0.9161970615386963, Accuracy: 1.0, Computation time: 1.7568471431732178\n",
      "Step: 5803, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 1.8258721828460693\n",
      "Step: 5804, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.241426944732666\n",
      "Step: 5805, Loss: 0.937292218208313, Accuracy: 0.96875, Computation time: 1.0909063816070557\n",
      "Step: 5806, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.4244167804718018\n",
      "Step: 5807, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.542966604232788\n",
      "Step: 5808, Loss: 0.9160232543945312, Accuracy: 1.0, Computation time: 1.5464682579040527\n",
      "Step: 5809, Loss: 0.9381762742996216, Accuracy: 0.96875, Computation time: 1.6359224319458008\n",
      "Step: 5810, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.248652696609497\n",
      "Step: 5811, Loss: 0.9374903440475464, Accuracy: 0.96875, Computation time: 1.5446946620941162\n",
      "Step: 5812, Loss: 0.9168462157249451, Accuracy: 1.0, Computation time: 1.609527587890625\n",
      "Step: 5813, Loss: 0.9159704446792603, Accuracy: 1.0, Computation time: 2.370039463043213\n",
      "Step: 5814, Loss: 0.9160142540931702, Accuracy: 1.0, Computation time: 1.528472900390625\n",
      "Step: 5815, Loss: 0.937616765499115, Accuracy: 0.96875, Computation time: 1.5640790462493896\n",
      "Step: 5816, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.4008705615997314\n",
      "Step: 5817, Loss: 0.9375417828559875, Accuracy: 0.96875, Computation time: 1.1265642642974854\n",
      "Step: 5818, Loss: 0.9366206526756287, Accuracy: 0.96875, Computation time: 1.4177706241607666\n",
      "Step: 5819, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.3546926975250244\n",
      "Step: 5820, Loss: 0.9159289002418518, Accuracy: 1.0, Computation time: 1.1665091514587402\n",
      "Step: 5821, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.1563231945037842\n",
      "Step: 5822, Loss: 0.9375909566879272, Accuracy: 0.96875, Computation time: 1.240466833114624\n",
      "Step: 5823, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 1.265833854675293\n",
      "Step: 5824, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.398818016052246\n",
      "Step: 5825, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.25477933883667\n",
      "Step: 5826, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 1.749885082244873\n",
      "Step: 5827, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.316596269607544\n",
      "Step: 5828, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 1.1790711879730225\n",
      "Step: 5829, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.16841459274292\n",
      "Step: 5830, Loss: 0.937548041343689, Accuracy: 0.96875, Computation time: 1.3412821292877197\n",
      "Step: 5831, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.0777428150177002\n",
      "Step: 5832, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.1850559711456299\n",
      "Step: 5833, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.236609935760498\n",
      "Step: 5834, Loss: 0.9374747276306152, Accuracy: 0.96875, Computation time: 1.343407392501831\n",
      "Step: 5835, Loss: 0.9373582005500793, Accuracy: 0.96875, Computation time: 2.236179828643799\n",
      "Step: 5836, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.4787883758544922\n",
      "########################\n",
      "Test loss: 1.115606427192688, Test Accuracy_epoch42: 0.7098175883293152\n",
      "########################\n",
      "Step: 5837, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.863914966583252\n",
      "Step: 5838, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 1.438422441482544\n",
      "Step: 5839, Loss: 0.9353062510490417, Accuracy: 0.96875, Computation time: 1.5645568370819092\n",
      "Step: 5840, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.5202887058258057\n",
      "Step: 5841, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.4767653942108154\n",
      "Step: 5842, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.0965995788574219\n",
      "Step: 5843, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.2986969947814941\n",
      "Step: 5844, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.6379661560058594\n",
      "Step: 5845, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.2027184963226318\n",
      "Step: 5846, Loss: 0.9537872672080994, Accuracy: 0.9375, Computation time: 1.652611255645752\n",
      "Step: 5847, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.3883299827575684\n",
      "Step: 5848, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.3331255912780762\n",
      "Step: 5849, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 1.411778450012207\n",
      "Step: 5850, Loss: 0.9163837432861328, Accuracy: 1.0, Computation time: 1.9720637798309326\n",
      "Step: 5851, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.3517487049102783\n",
      "Step: 5852, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 1.6519513130187988\n",
      "Step: 5853, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.143296241760254\n",
      "Step: 5854, Loss: 0.9165005683898926, Accuracy: 1.0, Computation time: 1.6784305572509766\n",
      "Step: 5855, Loss: 0.9158928394317627, Accuracy: 1.0, Computation time: 1.4778087139129639\n",
      "Step: 5856, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.4925110340118408\n",
      "Step: 5857, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.1650152206420898\n",
      "Step: 5858, Loss: 0.9374991059303284, Accuracy: 0.96875, Computation time: 1.282952070236206\n",
      "Step: 5859, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.4897949695587158\n",
      "Step: 5860, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 1.3982298374176025\n",
      "Step: 5861, Loss: 0.9160667657852173, Accuracy: 1.0, Computation time: 1.2715630531311035\n",
      "Step: 5862, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 1.7010600566864014\n",
      "Step: 5863, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.4510135650634766\n",
      "Step: 5864, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 1.6602296829223633\n",
      "Step: 5865, Loss: 0.9166350960731506, Accuracy: 1.0, Computation time: 1.6356136798858643\n",
      "Step: 5866, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.7239863872528076\n",
      "Step: 5867, Loss: 0.937536358833313, Accuracy: 0.96875, Computation time: 1.4014275074005127\n",
      "Step: 5868, Loss: 0.9176090359687805, Accuracy: 1.0, Computation time: 1.9901971817016602\n",
      "Step: 5869, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 1.6923036575317383\n",
      "Step: 5870, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.585320234298706\n",
      "Step: 5871, Loss: 0.916002631187439, Accuracy: 1.0, Computation time: 1.5569064617156982\n",
      "Step: 5872, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.3233280181884766\n",
      "Step: 5873, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.4472851753234863\n",
      "Step: 5874, Loss: 0.9168210029602051, Accuracy: 1.0, Computation time: 1.5499541759490967\n",
      "Step: 5875, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.541309118270874\n",
      "Step: 5876, Loss: 0.915967583656311, Accuracy: 1.0, Computation time: 1.3804535865783691\n",
      "Step: 5877, Loss: 0.9159380197525024, Accuracy: 1.0, Computation time: 1.4584259986877441\n",
      "Step: 5878, Loss: 0.9389028549194336, Accuracy: 0.96875, Computation time: 1.714592695236206\n",
      "Step: 5879, Loss: 0.9160614013671875, Accuracy: 1.0, Computation time: 1.5007808208465576\n",
      "Step: 5880, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 1.9882423877716064\n",
      "Step: 5881, Loss: 0.93780517578125, Accuracy: 0.96875, Computation time: 1.5594582557678223\n",
      "Step: 5882, Loss: 0.9161179661750793, Accuracy: 1.0, Computation time: 1.5964746475219727\n",
      "Step: 5883, Loss: 0.9159572124481201, Accuracy: 1.0, Computation time: 1.6898777484893799\n",
      "Step: 5884, Loss: 0.91619473695755, Accuracy: 1.0, Computation time: 2.04508376121521\n",
      "Step: 5885, Loss: 0.916803777217865, Accuracy: 1.0, Computation time: 2.086627960205078\n",
      "Step: 5886, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.5245881080627441\n",
      "Step: 5887, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.5989100933074951\n",
      "Step: 5888, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.5227525234222412\n",
      "Step: 5889, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 1.4582195281982422\n",
      "Step: 5890, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.298635721206665\n",
      "Step: 5891, Loss: 0.9374668002128601, Accuracy: 0.96875, Computation time: 1.575230360031128\n",
      "Step: 5892, Loss: 0.9375706911087036, Accuracy: 0.96875, Computation time: 1.4169180393218994\n",
      "Step: 5893, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.2881417274475098\n",
      "Step: 5894, Loss: 0.9226256012916565, Accuracy: 1.0, Computation time: 2.401949644088745\n",
      "Step: 5895, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.3968696594238281\n",
      "Step: 5896, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.848778247833252\n",
      "Step: 5897, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.6412322521209717\n",
      "Step: 5898, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.4324750900268555\n",
      "Step: 5899, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.6647920608520508\n",
      "Step: 5900, Loss: 0.9374613761901855, Accuracy: 0.96875, Computation time: 1.289534330368042\n",
      "Step: 5901, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.6350243091583252\n",
      "Step: 5902, Loss: 0.9173563718795776, Accuracy: 1.0, Computation time: 1.3661129474639893\n",
      "Step: 5903, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.5703339576721191\n",
      "Step: 5904, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.2665090560913086\n",
      "Step: 5905, Loss: 0.9158603549003601, Accuracy: 1.0, Computation time: 1.4239788055419922\n",
      "Step: 5906, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.5401816368103027\n",
      "Step: 5907, Loss: 0.9168400764465332, Accuracy: 1.0, Computation time: 2.308030605316162\n",
      "Step: 5908, Loss: 0.9375796318054199, Accuracy: 0.96875, Computation time: 1.7783069610595703\n",
      "Step: 5909, Loss: 0.9165303111076355, Accuracy: 1.0, Computation time: 1.3825771808624268\n",
      "Step: 5910, Loss: 0.9159160852432251, Accuracy: 1.0, Computation time: 1.2636051177978516\n",
      "Step: 5911, Loss: 0.9375431537628174, Accuracy: 0.96875, Computation time: 1.6930344104766846\n",
      "Step: 5912, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.8165466785430908\n",
      "Step: 5913, Loss: 0.9375494122505188, Accuracy: 0.96875, Computation time: 1.6794250011444092\n",
      "Step: 5914, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 1.3837015628814697\n",
      "Step: 5915, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.276374101638794\n",
      "Step: 5916, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.2580687999725342\n",
      "Step: 5917, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.3316848278045654\n",
      "Step: 5918, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.250880241394043\n",
      "Step: 5919, Loss: 0.9376070499420166, Accuracy: 0.96875, Computation time: 1.4705455303192139\n",
      "Step: 5920, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.419485092163086\n",
      "Step: 5921, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 1.3803460597991943\n",
      "Step: 5922, Loss: 0.9373806715011597, Accuracy: 0.96875, Computation time: 2.0387003421783447\n",
      "Step: 5923, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.565019130706787\n",
      "Step: 5924, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.5479094982147217\n",
      "Step: 5925, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.2253406047821045\n",
      "Step: 5926, Loss: 0.9160655736923218, Accuracy: 1.0, Computation time: 1.6795451641082764\n",
      "Step: 5927, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.089350938796997\n",
      "Step: 5928, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.2523705959320068\n",
      "Step: 5929, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.13665771484375\n",
      "Step: 5930, Loss: 0.9158384799957275, Accuracy: 1.0, Computation time: 1.1487421989440918\n",
      "Step: 5931, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.1708171367645264\n",
      "Step: 5932, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.4162194728851318\n",
      "Step: 5933, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.6686553955078125\n",
      "Step: 5934, Loss: 0.9374898672103882, Accuracy: 0.96875, Computation time: 1.341078519821167\n",
      "Step: 5935, Loss: 0.9379218220710754, Accuracy: 0.96875, Computation time: 1.3376998901367188\n",
      "Step: 5936, Loss: 0.937444269657135, Accuracy: 0.96875, Computation time: 1.188798427581787\n",
      "Step: 5937, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.5087215900421143\n",
      "Step: 5938, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.2957336902618408\n",
      "Step: 5939, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.2462563514709473\n",
      "Step: 5940, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.5412468910217285\n",
      "Step: 5941, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.73390793800354\n",
      "Step: 5942, Loss: 0.9374552965164185, Accuracy: 0.96875, Computation time: 1.351665735244751\n",
      "Step: 5943, Loss: 0.9159771800041199, Accuracy: 1.0, Computation time: 1.2620861530303955\n",
      "Step: 5944, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.4472260475158691\n",
      "Step: 5945, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.2515287399291992\n",
      "Step: 5946, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.5117108821868896\n",
      "Step: 5947, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 1.4463438987731934\n",
      "Step: 5948, Loss: 0.9399079084396362, Accuracy: 0.96875, Computation time: 1.4971966743469238\n",
      "Step: 5949, Loss: 0.9375566840171814, Accuracy: 0.96875, Computation time: 1.548060655593872\n",
      "Step: 5950, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.2965571880340576\n",
      "Step: 5951, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.3381507396697998\n",
      "Step: 5952, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.2168693542480469\n",
      "Step: 5953, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.402184009552002\n",
      "Step: 5954, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.5355889797210693\n",
      "Step: 5955, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.3773338794708252\n",
      "Step: 5956, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.425558090209961\n",
      "Step: 5957, Loss: 0.9158360958099365, Accuracy: 1.0, Computation time: 1.5857675075531006\n",
      "Step: 5958, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 1.3483524322509766\n",
      "Step: 5959, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 1.2070271968841553\n",
      "Step: 5960, Loss: 0.9341592192649841, Accuracy: 0.96875, Computation time: 1.2697725296020508\n",
      "Step: 5961, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.4659645557403564\n",
      "Step: 5962, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 1.2376461029052734\n",
      "Step: 5963, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.3230359554290771\n",
      "Step: 5964, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.3306500911712646\n",
      "Step: 5965, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.2160813808441162\n",
      "Step: 5966, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.345149040222168\n",
      "Step: 5967, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.7245142459869385\n",
      "Step: 5968, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.2249622344970703\n",
      "Step: 5969, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 1.5570783615112305\n",
      "Step: 5970, Loss: 0.9176449775695801, Accuracy: 1.0, Computation time: 1.7869915962219238\n",
      "Step: 5971, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.2947697639465332\n",
      "Step: 5972, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.7400484085083008\n",
      "Step: 5973, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.3037068843841553\n",
      "Step: 5974, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.1076300144195557\n",
      "Step: 5975, Loss: 0.9158755540847778, Accuracy: 1.0, Computation time: 1.2759697437286377\n",
      "########################\n",
      "Test loss: 1.1151126623153687, Test Accuracy_epoch43: 0.7089487910270691\n",
      "########################\n",
      "Step: 5976, Loss: 0.9373863935470581, Accuracy: 0.96875, Computation time: 1.5763182640075684\n",
      "Step: 5977, Loss: 0.9374938011169434, Accuracy: 0.96875, Computation time: 1.1024110317230225\n",
      "Step: 5978, Loss: 0.9163989424705505, Accuracy: 1.0, Computation time: 1.446214199066162\n",
      "Step: 5979, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.0270237922668457\n",
      "Step: 5980, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.556304931640625\n",
      "Step: 5981, Loss: 0.9592218399047852, Accuracy: 0.9375, Computation time: 1.6829984188079834\n",
      "Step: 5982, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 1.4090471267700195\n",
      "Step: 5983, Loss: 0.9375379085540771, Accuracy: 0.96875, Computation time: 1.3173277378082275\n",
      "Step: 5984, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.2576212882995605\n",
      "Step: 5985, Loss: 0.9375817179679871, Accuracy: 0.96875, Computation time: 1.789154291152954\n",
      "Step: 5986, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 1.5550575256347656\n",
      "Step: 5987, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.3376796245574951\n",
      "Step: 5988, Loss: 0.9375767111778259, Accuracy: 0.96875, Computation time: 1.6573634147644043\n",
      "Step: 5989, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.282468557357788\n",
      "Step: 5990, Loss: 0.9372711777687073, Accuracy: 0.96875, Computation time: 1.616518259048462\n",
      "Step: 5991, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.2597379684448242\n",
      "Step: 5992, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.4141387939453125\n",
      "Step: 5993, Loss: 0.915995717048645, Accuracy: 1.0, Computation time: 1.5411419868469238\n",
      "Step: 5994, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.2801659107208252\n",
      "Step: 5995, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.4718332290649414\n",
      "Step: 5996, Loss: 0.9370901584625244, Accuracy: 0.96875, Computation time: 1.1007025241851807\n",
      "Step: 5997, Loss: 0.9159607291221619, Accuracy: 1.0, Computation time: 1.3025720119476318\n",
      "Step: 5998, Loss: 0.9371365904808044, Accuracy: 0.96875, Computation time: 1.668799877166748\n",
      "Step: 5999, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.4922964572906494\n",
      "Step: 6000, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.8396432399749756\n",
      "Step: 6001, Loss: 0.9375484585762024, Accuracy: 0.96875, Computation time: 1.2149572372436523\n",
      "Step: 6002, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 1.5324902534484863\n",
      "Step: 6003, Loss: 0.915835976600647, Accuracy: 1.0, Computation time: 1.195347547531128\n",
      "Step: 6004, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.7194135189056396\n",
      "Step: 6005, Loss: 0.9377682209014893, Accuracy: 0.96875, Computation time: 1.371689796447754\n",
      "Step: 6006, Loss: 0.9181324243545532, Accuracy: 1.0, Computation time: 1.2232015132904053\n",
      "Step: 6007, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.136293888092041\n",
      "Step: 6008, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.330127239227295\n",
      "Step: 6009, Loss: 0.9375434517860413, Accuracy: 0.96875, Computation time: 1.7443690299987793\n",
      "Step: 6010, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 1.0873656272888184\n",
      "Step: 6011, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.2772760391235352\n",
      "Step: 6012, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 1.3529539108276367\n",
      "Step: 6013, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.4679720401763916\n",
      "Step: 6014, Loss: 0.9591585397720337, Accuracy: 0.9375, Computation time: 1.3461287021636963\n",
      "Step: 6015, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.1825087070465088\n",
      "Step: 6016, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.1280889511108398\n",
      "Step: 6017, Loss: 0.9158390760421753, Accuracy: 1.0, Computation time: 1.3155076503753662\n",
      "Step: 6018, Loss: 0.9167128205299377, Accuracy: 1.0, Computation time: 1.368912935256958\n",
      "Step: 6019, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 1.4049065113067627\n",
      "Step: 6020, Loss: 0.9373644590377808, Accuracy: 0.96875, Computation time: 1.0530591011047363\n",
      "Step: 6021, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.1246888637542725\n",
      "Step: 6022, Loss: 0.9161953330039978, Accuracy: 1.0, Computation time: 1.330430030822754\n",
      "Step: 6023, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.2680144309997559\n",
      "Step: 6024, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.5005836486816406\n",
      "Step: 6025, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.289994716644287\n",
      "Step: 6026, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.4527027606964111\n",
      "Step: 6027, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.2827427387237549\n",
      "Step: 6028, Loss: 0.9159491062164307, Accuracy: 1.0, Computation time: 1.1226990222930908\n",
      "Step: 6029, Loss: 0.9374955296516418, Accuracy: 0.96875, Computation time: 1.5727083683013916\n",
      "Step: 6030, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.3703649044036865\n",
      "Step: 6031, Loss: 0.9158341884613037, Accuracy: 1.0, Computation time: 1.5132169723510742\n",
      "Step: 6032, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 1.3641905784606934\n",
      "Step: 6033, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.536893606185913\n",
      "Step: 6034, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.1920039653778076\n",
      "Step: 6035, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.2367594242095947\n",
      "Step: 6036, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 1.1554770469665527\n",
      "Step: 6037, Loss: 0.9253196120262146, Accuracy: 0.96875, Computation time: 1.2355232238769531\n",
      "Step: 6038, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 1.2642462253570557\n",
      "Step: 6039, Loss: 0.9374527335166931, Accuracy: 0.96875, Computation time: 1.7881443500518799\n",
      "Step: 6040, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.187830924987793\n",
      "Step: 6041, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.8075182437896729\n",
      "Step: 6042, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.0295283794403076\n",
      "Step: 6043, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.4990196228027344\n",
      "Step: 6044, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.3031761646270752\n",
      "Step: 6045, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 1.2987244129180908\n",
      "Step: 6046, Loss: 0.9340039491653442, Accuracy: 0.96875, Computation time: 1.18253493309021\n",
      "Step: 6047, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.1464207172393799\n",
      "Step: 6048, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.6107823848724365\n",
      "Step: 6049, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.1987214088439941\n",
      "Step: 6050, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.3099884986877441\n",
      "Step: 6051, Loss: 0.9374988079071045, Accuracy: 0.96875, Computation time: 1.5908143520355225\n",
      "Step: 6052, Loss: 0.9375889897346497, Accuracy: 0.96875, Computation time: 1.28302001953125\n",
      "Step: 6053, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.2575139999389648\n",
      "Step: 6054, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.1612484455108643\n",
      "Step: 6055, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.4461259841918945\n",
      "Step: 6056, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 1.3849027156829834\n",
      "Step: 6057, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.1868786811828613\n",
      "Step: 6058, Loss: 0.9163179993629456, Accuracy: 1.0, Computation time: 1.5963611602783203\n",
      "Step: 6059, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 1.417457103729248\n",
      "Step: 6060, Loss: 0.9159544110298157, Accuracy: 1.0, Computation time: 1.4954938888549805\n",
      "Step: 6061, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.1568050384521484\n",
      "Step: 6062, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 1.583693027496338\n",
      "Step: 6063, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.395552158355713\n",
      "Step: 6064, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.3552272319793701\n",
      "Step: 6065, Loss: 0.9207892417907715, Accuracy: 1.0, Computation time: 1.8004143238067627\n",
      "Step: 6066, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.3683338165283203\n",
      "Step: 6067, Loss: 0.9158753156661987, Accuracy: 1.0, Computation time: 1.481968641281128\n",
      "Step: 6068, Loss: 0.9160205721855164, Accuracy: 1.0, Computation time: 1.75913667678833\n",
      "Step: 6069, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 1.2174766063690186\n",
      "Step: 6070, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.5054099559783936\n",
      "Step: 6071, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.7471091747283936\n",
      "Step: 6072, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.4689960479736328\n",
      "Step: 6073, Loss: 0.9158409833908081, Accuracy: 1.0, Computation time: 1.731931447982788\n",
      "Step: 6074, Loss: 0.9375141263008118, Accuracy: 0.96875, Computation time: 1.722046136856079\n",
      "Step: 6075, Loss: 0.9376091957092285, Accuracy: 0.96875, Computation time: 1.4581892490386963\n",
      "Step: 6076, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.7779784202575684\n",
      "Step: 6077, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.5693755149841309\n",
      "Step: 6078, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.6116209030151367\n",
      "Step: 6079, Loss: 0.9377673268318176, Accuracy: 0.96875, Computation time: 2.0895168781280518\n",
      "Step: 6080, Loss: 0.9375024437904358, Accuracy: 0.96875, Computation time: 1.4746944904327393\n",
      "Step: 6081, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.6711227893829346\n",
      "Step: 6082, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.6085691452026367\n",
      "Step: 6083, Loss: 0.9317516088485718, Accuracy: 0.96875, Computation time: 1.9865834712982178\n",
      "Step: 6084, Loss: 0.959166407585144, Accuracy: 0.9375, Computation time: 1.5970211029052734\n",
      "Step: 6085, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.670684814453125\n",
      "Step: 6086, Loss: 0.916498064994812, Accuracy: 1.0, Computation time: 1.2844176292419434\n",
      "Step: 6087, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 1.190124273300171\n",
      "Step: 6088, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.1856205463409424\n",
      "Step: 6089, Loss: 0.9159322381019592, Accuracy: 1.0, Computation time: 1.5561678409576416\n",
      "Step: 6090, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 1.19297456741333\n",
      "Step: 6091, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.4304189682006836\n",
      "Step: 6092, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.0789854526519775\n",
      "Step: 6093, Loss: 0.9383110404014587, Accuracy: 0.96875, Computation time: 2.035381555557251\n",
      "Step: 6094, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.1961464881896973\n",
      "Step: 6095, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.3440818786621094\n",
      "Step: 6096, Loss: 0.9375540018081665, Accuracy: 0.96875, Computation time: 1.2550461292266846\n",
      "Step: 6097, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.105210304260254\n",
      "Step: 6098, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.2213294506072998\n",
      "Step: 6099, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.305717945098877\n",
      "Step: 6100, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.2853775024414062\n",
      "Step: 6101, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 0.999669075012207\n",
      "Step: 6102, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.2955708503723145\n",
      "Step: 6103, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 1.158318042755127\n",
      "Step: 6104, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.2746481895446777\n",
      "Step: 6105, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.9263043403625488\n",
      "Step: 6106, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.4348952770233154\n",
      "Step: 6107, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.2166037559509277\n",
      "Step: 6108, Loss: 0.9375479817390442, Accuracy: 0.96875, Computation time: 1.1618282794952393\n",
      "Step: 6109, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.0877819061279297\n",
      "Step: 6110, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.1307268142700195\n",
      "Step: 6111, Loss: 0.9377211928367615, Accuracy: 0.96875, Computation time: 1.1653027534484863\n",
      "Step: 6112, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.0829474925994873\n",
      "Step: 6113, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 1.192662239074707\n",
      "Step: 6114, Loss: 0.915916383266449, Accuracy: 1.0, Computation time: 1.1931734085083008\n",
      "########################\n",
      "Test loss: 1.1189088821411133, Test Accuracy_epoch44: 0.703735888004303\n",
      "########################\n",
      "Step: 6115, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.1432294845581055\n",
      "Step: 6116, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.1872296333312988\n",
      "Step: 6117, Loss: 0.9159699082374573, Accuracy: 1.0, Computation time: 1.5071773529052734\n",
      "Step: 6118, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.1193656921386719\n",
      "Step: 6119, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 1.148752212524414\n",
      "Step: 6120, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 0.9845795631408691\n",
      "Step: 6121, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 0.9695143699645996\n",
      "Step: 6122, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.1243362426757812\n",
      "Step: 6123, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.0953969955444336\n",
      "Step: 6124, Loss: 0.9375500679016113, Accuracy: 0.96875, Computation time: 1.1770787239074707\n",
      "Step: 6125, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.172783374786377\n",
      "Step: 6126, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 1.12599778175354\n",
      "Step: 6127, Loss: 0.9158366918563843, Accuracy: 1.0, Computation time: 1.3041129112243652\n",
      "Step: 6128, Loss: 0.9158350825309753, Accuracy: 1.0, Computation time: 1.4945099353790283\n",
      "Step: 6129, Loss: 0.9158406257629395, Accuracy: 1.0, Computation time: 1.415665626525879\n",
      "Step: 6130, Loss: 0.9158322811126709, Accuracy: 1.0, Computation time: 1.0428290367126465\n",
      "Step: 6131, Loss: 0.9158340692520142, Accuracy: 1.0, Computation time: 1.1304633617401123\n",
      "Step: 6132, Loss: 0.9158357381820679, Accuracy: 1.0, Computation time: 1.0533535480499268\n",
      "Step: 6133, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.1328401565551758\n",
      "Step: 6134, Loss: 0.9158307313919067, Accuracy: 1.0, Computation time: 1.1136276721954346\n",
      "Step: 6135, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 1.3786299228668213\n",
      "Step: 6136, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.1303801536560059\n",
      "Step: 6137, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.606684684753418\n",
      "Step: 6138, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.9876704216003418\n",
      "Step: 6139, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.4718835353851318\n",
      "Step: 6140, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 1.0702862739562988\n",
      "Step: 6141, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 0.9974348545074463\n",
      "Step: 6142, Loss: 0.9375145435333252, Accuracy: 0.96875, Computation time: 1.305593729019165\n",
      "Step: 6143, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.1778457164764404\n",
      "Step: 6144, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 1.1163711547851562\n",
      "Step: 6145, Loss: 0.9370419383049011, Accuracy: 0.96875, Computation time: 1.5330002307891846\n",
      "Step: 6146, Loss: 0.9373974800109863, Accuracy: 0.96875, Computation time: 1.3809747695922852\n",
      "Step: 6147, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.1885910034179688\n",
      "Step: 6148, Loss: 0.9164106845855713, Accuracy: 1.0, Computation time: 1.2168843746185303\n",
      "Step: 6149, Loss: 0.9165151119232178, Accuracy: 1.0, Computation time: 1.3018052577972412\n",
      "Step: 6150, Loss: 0.9374903440475464, Accuracy: 0.96875, Computation time: 1.4026436805725098\n",
      "Step: 6151, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.2072134017944336\n",
      "Step: 6152, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.0111322402954102\n",
      "Step: 6153, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.1839711666107178\n",
      "Step: 6154, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.2883634567260742\n",
      "Step: 6155, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.3587591648101807\n",
      "Step: 6156, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.2509448528289795\n",
      "Step: 6157, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 1.3829267024993896\n",
      "Step: 6158, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.366976261138916\n",
      "Step: 6159, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 1.21799635887146\n",
      "Step: 6160, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 1.5466556549072266\n",
      "Step: 6161, Loss: 0.9158357381820679, Accuracy: 1.0, Computation time: 1.4136979579925537\n",
      "Step: 6162, Loss: 0.9158373475074768, Accuracy: 1.0, Computation time: 1.1535425186157227\n",
      "Step: 6163, Loss: 0.9158397316932678, Accuracy: 1.0, Computation time: 1.3110125064849854\n",
      "Step: 6164, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 1.0227174758911133\n",
      "Step: 6165, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 1.1149380207061768\n",
      "Step: 6166, Loss: 0.9158406257629395, Accuracy: 1.0, Computation time: 1.2066493034362793\n",
      "Step: 6167, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 1.1206893920898438\n",
      "Step: 6168, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 1.245332956314087\n",
      "Step: 6169, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 1.4819908142089844\n",
      "Step: 6170, Loss: 0.9586352705955505, Accuracy: 0.9375, Computation time: 1.292301893234253\n",
      "Step: 6171, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.2605953216552734\n",
      "Step: 6172, Loss: 0.915840208530426, Accuracy: 1.0, Computation time: 1.1577301025390625\n",
      "Step: 6173, Loss: 0.9158453941345215, Accuracy: 1.0, Computation time: 1.136904001235962\n",
      "Step: 6174, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.1463534832000732\n",
      "Step: 6175, Loss: 0.9374865293502808, Accuracy: 0.96875, Computation time: 1.3354649543762207\n",
      "Step: 6176, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.4658102989196777\n",
      "Step: 6177, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.2423269748687744\n",
      "Step: 6178, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 1.717801570892334\n",
      "Step: 6179, Loss: 0.9158344268798828, Accuracy: 1.0, Computation time: 1.3128058910369873\n",
      "Step: 6180, Loss: 0.9375096559524536, Accuracy: 0.96875, Computation time: 1.128812551498413\n",
      "Step: 6181, Loss: 0.9158321022987366, Accuracy: 1.0, Computation time: 1.6126179695129395\n",
      "Step: 6182, Loss: 0.9374831914901733, Accuracy: 0.96875, Computation time: 1.2497138977050781\n",
      "Step: 6183, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 1.3770723342895508\n",
      "Step: 6184, Loss: 0.9591379761695862, Accuracy: 0.9375, Computation time: 1.6085560321807861\n",
      "Step: 6185, Loss: 0.915838897228241, Accuracy: 1.0, Computation time: 1.150954008102417\n",
      "Step: 6186, Loss: 0.9211179614067078, Accuracy: 1.0, Computation time: 1.2351608276367188\n",
      "Step: 6187, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.4364452362060547\n",
      "Step: 6188, Loss: 0.9159844517707825, Accuracy: 1.0, Computation time: 1.4280388355255127\n",
      "Step: 6189, Loss: 0.9403092861175537, Accuracy: 0.96875, Computation time: 1.9824268817901611\n",
      "Step: 6190, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 1.286585807800293\n",
      "Step: 6191, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.7013359069824219\n",
      "Step: 6192, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.1924974918365479\n",
      "Step: 6193, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.1142327785491943\n",
      "Step: 6194, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 1.052725076675415\n",
      "Step: 6195, Loss: 0.9158753156661987, Accuracy: 1.0, Computation time: 1.365807294845581\n",
      "Step: 6196, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.2408161163330078\n",
      "Step: 6197, Loss: 0.915882408618927, Accuracy: 1.0, Computation time: 1.1317157745361328\n",
      "Step: 6198, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.174107551574707\n",
      "Step: 6199, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 1.2948992252349854\n",
      "Step: 6200, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 1.3499948978424072\n",
      "Step: 6201, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.2518589496612549\n",
      "Step: 6202, Loss: 0.9353679418563843, Accuracy: 0.96875, Computation time: 1.0716712474822998\n",
      "Step: 6203, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.7739276885986328\n",
      "Step: 6204, Loss: 0.9161384701728821, Accuracy: 1.0, Computation time: 1.8875527381896973\n",
      "Step: 6205, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.1541638374328613\n",
      "Step: 6206, Loss: 0.9511371850967407, Accuracy: 0.9375, Computation time: 2.067819356918335\n",
      "Step: 6207, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.1657655239105225\n",
      "Step: 6208, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.7142956256866455\n",
      "Step: 6209, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.0596888065338135\n",
      "Step: 6210, Loss: 0.9163512587547302, Accuracy: 1.0, Computation time: 1.5025110244750977\n",
      "Step: 6211, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 1.4723436832427979\n",
      "Step: 6212, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.2094783782958984\n",
      "Step: 6213, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.3824491500854492\n",
      "Step: 6214, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.1305546760559082\n",
      "Step: 6215, Loss: 0.9163388609886169, Accuracy: 1.0, Computation time: 1.4936869144439697\n",
      "Step: 6216, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.340897798538208\n",
      "Step: 6217, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.5071475505828857\n",
      "Step: 6218, Loss: 0.9591853618621826, Accuracy: 0.9375, Computation time: 1.4842169284820557\n",
      "Step: 6219, Loss: 0.9183352589607239, Accuracy: 1.0, Computation time: 1.470184087753296\n",
      "Step: 6220, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.2057137489318848\n",
      "Step: 6221, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 1.2255749702453613\n",
      "Step: 6222, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.2513911724090576\n",
      "Step: 6223, Loss: 0.9352558851242065, Accuracy: 0.96875, Computation time: 2.3324973583221436\n",
      "Step: 6224, Loss: 0.9303519129753113, Accuracy: 0.96875, Computation time: 1.5496149063110352\n",
      "Step: 6225, Loss: 0.9160376787185669, Accuracy: 1.0, Computation time: 1.0450575351715088\n",
      "Step: 6226, Loss: 0.916090190410614, Accuracy: 1.0, Computation time: 1.3279697895050049\n",
      "Step: 6227, Loss: 0.916192352771759, Accuracy: 1.0, Computation time: 1.686891794204712\n",
      "Step: 6228, Loss: 0.9162102341651917, Accuracy: 1.0, Computation time: 1.1107990741729736\n",
      "Step: 6229, Loss: 0.9159917831420898, Accuracy: 1.0, Computation time: 1.0315470695495605\n",
      "Step: 6230, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 1.0928401947021484\n",
      "Step: 6231, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.1790945529937744\n",
      "Step: 6232, Loss: 0.9159393906593323, Accuracy: 1.0, Computation time: 1.2309107780456543\n",
      "Step: 6233, Loss: 0.9159581661224365, Accuracy: 1.0, Computation time: 1.078751564025879\n",
      "Step: 6234, Loss: 0.9160649180412292, Accuracy: 1.0, Computation time: 1.1758339405059814\n",
      "Step: 6235, Loss: 0.9381535053253174, Accuracy: 0.96875, Computation time: 1.2122197151184082\n",
      "Step: 6236, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 1.176382303237915\n",
      "Step: 6237, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.1965537071228027\n",
      "Step: 6238, Loss: 0.9159291982650757, Accuracy: 1.0, Computation time: 1.5757420063018799\n",
      "Step: 6239, Loss: 0.9159139394760132, Accuracy: 1.0, Computation time: 1.1759405136108398\n",
      "Step: 6240, Loss: 0.9254404306411743, Accuracy: 0.96875, Computation time: 1.7157490253448486\n",
      "Step: 6241, Loss: 0.9376037120819092, Accuracy: 0.96875, Computation time: 1.3010585308074951\n",
      "Step: 6242, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.3084874153137207\n",
      "Step: 6243, Loss: 0.9594595432281494, Accuracy: 0.9375, Computation time: 1.7712116241455078\n",
      "Step: 6244, Loss: 0.9159805178642273, Accuracy: 1.0, Computation time: 1.659494161605835\n",
      "Step: 6245, Loss: 0.9371638894081116, Accuracy: 0.96875, Computation time: 1.3174030780792236\n",
      "Step: 6246, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.4473466873168945\n",
      "Step: 6247, Loss: 0.9375524520874023, Accuracy: 0.96875, Computation time: 1.125821590423584\n",
      "Step: 6248, Loss: 0.9588332176208496, Accuracy: 0.9375, Computation time: 1.3142125606536865\n",
      "Step: 6249, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 0.9389004707336426\n",
      "Step: 6250, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 1.1198103427886963\n",
      "Step: 6251, Loss: 0.91596919298172, Accuracy: 1.0, Computation time: 1.2779943943023682\n",
      "Step: 6252, Loss: 0.9160049557685852, Accuracy: 1.0, Computation time: 1.7311985492706299\n",
      "Step: 6253, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 1.4677836894989014\n",
      "########################\n",
      "Test loss: 1.1188727617263794, Test Accuracy_epoch45: 0.703735888004303\n",
      "########################\n",
      "Step: 6254, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.394378662109375\n",
      "Step: 6255, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.294555425643921\n",
      "Step: 6256, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.2042691707611084\n",
      "Step: 6257, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.1745128631591797\n",
      "Step: 6258, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.0834310054779053\n",
      "Step: 6259, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.3980183601379395\n",
      "Step: 6260, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.2407052516937256\n",
      "Step: 6261, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 1.4005732536315918\n",
      "Step: 6262, Loss: 0.9378998279571533, Accuracy: 0.96875, Computation time: 1.502324104309082\n",
      "Step: 6263, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 1.213089942932129\n",
      "Step: 6264, Loss: 0.9375, Accuracy: 0.96875, Computation time: 1.6192660331726074\n",
      "Step: 6265, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 1.4134149551391602\n",
      "Step: 6266, Loss: 0.9162285923957825, Accuracy: 1.0, Computation time: 2.3611950874328613\n",
      "Step: 6267, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.3327975273132324\n",
      "Step: 6268, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 1.0344176292419434\n",
      "Step: 6269, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.1710824966430664\n",
      "Step: 6270, Loss: 0.9375768899917603, Accuracy: 0.96875, Computation time: 1.2497830390930176\n",
      "Step: 6271, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.4150667190551758\n",
      "Step: 6272, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.526196002960205\n",
      "Step: 6273, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 1.5752630233764648\n",
      "Step: 6274, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.6596405506134033\n",
      "Step: 6275, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.4805371761322021\n",
      "Step: 6276, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.3840928077697754\n",
      "Step: 6277, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.3711585998535156\n",
      "Step: 6278, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.115011215209961\n",
      "Step: 6279, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.8065006732940674\n",
      "Step: 6280, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.1932477951049805\n",
      "Step: 6281, Loss: 0.9370969533920288, Accuracy: 0.96875, Computation time: 1.2835605144500732\n",
      "Step: 6282, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.08738374710083\n",
      "Step: 6283, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.211181879043579\n",
      "Step: 6284, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.2790286540985107\n",
      "Step: 6285, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 1.24153470993042\n",
      "Step: 6286, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 1.5859858989715576\n",
      "Step: 6287, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.311056137084961\n",
      "Step: 6288, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 1.1405203342437744\n",
      "Step: 6289, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 1.1888535022735596\n",
      "Step: 6290, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.1969077587127686\n",
      "Step: 6291, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 0.9960453510284424\n",
      "Step: 6292, Loss: 0.9158398509025574, Accuracy: 1.0, Computation time: 1.0813605785369873\n",
      "Step: 6293, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.8401963710784912\n",
      "Step: 6294, Loss: 0.937534749507904, Accuracy: 0.96875, Computation time: 1.5602898597717285\n",
      "Step: 6295, Loss: 0.9375303387641907, Accuracy: 0.96875, Computation time: 1.4623668193817139\n",
      "Step: 6296, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 1.383859395980835\n",
      "Step: 6297, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 1.2702717781066895\n",
      "Step: 6298, Loss: 0.9159204363822937, Accuracy: 1.0, Computation time: 2.1723036766052246\n",
      "Step: 6299, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.2476680278778076\n",
      "Step: 6300, Loss: 0.9160189032554626, Accuracy: 1.0, Computation time: 1.7653229236602783\n",
      "Step: 6301, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.4866416454315186\n",
      "Step: 6302, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.2608063220977783\n",
      "Step: 6303, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 1.4877417087554932\n",
      "Step: 6304, Loss: 0.9375219345092773, Accuracy: 0.96875, Computation time: 1.2803900241851807\n",
      "Step: 6305, Loss: 0.936003565788269, Accuracy: 0.96875, Computation time: 1.6438095569610596\n",
      "Step: 6306, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.8400335311889648\n",
      "Step: 6307, Loss: 0.9371199607849121, Accuracy: 0.96875, Computation time: 1.6439540386199951\n",
      "Step: 6308, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 1.4838411808013916\n",
      "Step: 6309, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.2872369289398193\n",
      "Step: 6310, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.6769869327545166\n",
      "Step: 6311, Loss: 0.9174748659133911, Accuracy: 1.0, Computation time: 1.6690349578857422\n",
      "Step: 6312, Loss: 0.9375280141830444, Accuracy: 0.96875, Computation time: 1.2744104862213135\n",
      "Step: 6313, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 1.673598051071167\n",
      "Step: 6314, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.4484663009643555\n",
      "Step: 6315, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.171257734298706\n",
      "Step: 6316, Loss: 0.9375458359718323, Accuracy: 0.96875, Computation time: 1.648836374282837\n",
      "Step: 6317, Loss: 0.9159935712814331, Accuracy: 1.0, Computation time: 1.854032039642334\n",
      "Step: 6318, Loss: 0.9158371090888977, Accuracy: 1.0, Computation time: 1.6125338077545166\n",
      "Step: 6319, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 1.386620044708252\n",
      "Step: 6320, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.484527587890625\n",
      "Step: 6321, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.196028232574463\n",
      "Step: 6322, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.3261778354644775\n",
      "Step: 6323, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.6041362285614014\n",
      "Step: 6324, Loss: 0.9217026829719543, Accuracy: 1.0, Computation time: 1.6427175998687744\n",
      "Step: 6325, Loss: 0.937503457069397, Accuracy: 0.96875, Computation time: 1.187570333480835\n",
      "Step: 6326, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 1.4955639839172363\n",
      "Step: 6327, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.158785104751587\n",
      "Step: 6328, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.3147828578948975\n",
      "Step: 6329, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.5319640636444092\n",
      "Step: 6330, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.1640827655792236\n",
      "Step: 6331, Loss: 0.937471330165863, Accuracy: 0.96875, Computation time: 0.9783926010131836\n",
      "Step: 6332, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 1.6599102020263672\n",
      "Step: 6333, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 1.2822883129119873\n",
      "Step: 6334, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.8114955425262451\n",
      "Step: 6335, Loss: 0.9374644160270691, Accuracy: 0.96875, Computation time: 1.0883777141571045\n",
      "Step: 6336, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.2567014694213867\n",
      "Step: 6337, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 1.0765116214752197\n",
      "Step: 6338, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.2506256103515625\n",
      "Step: 6339, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.607675313949585\n",
      "Step: 6340, Loss: 0.9368971586227417, Accuracy: 0.96875, Computation time: 0.9893519878387451\n",
      "Step: 6341, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.2341749668121338\n",
      "Step: 6342, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.3901550769805908\n",
      "Step: 6343, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.712996244430542\n",
      "Step: 6344, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.1631953716278076\n",
      "Step: 6345, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 1.1120717525482178\n",
      "Step: 6346, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.250347375869751\n",
      "Step: 6347, Loss: 0.9374527335166931, Accuracy: 0.96875, Computation time: 1.3892707824707031\n",
      "Step: 6348, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.303396463394165\n",
      "Step: 6349, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.696509838104248\n",
      "Step: 6350, Loss: 0.9375249147415161, Accuracy: 0.96875, Computation time: 1.2108433246612549\n",
      "Step: 6351, Loss: 0.9375978708267212, Accuracy: 0.96875, Computation time: 1.1698570251464844\n",
      "Step: 6352, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.1680476665496826\n",
      "Step: 6353, Loss: 0.9318529367446899, Accuracy: 0.96875, Computation time: 1.9034466743469238\n",
      "Step: 6354, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.2729694843292236\n",
      "Step: 6355, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 1.5915167331695557\n",
      "Step: 6356, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.292898178100586\n",
      "Step: 6357, Loss: 0.9396600127220154, Accuracy: 0.96875, Computation time: 1.661836862564087\n",
      "Step: 6358, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.0579261779785156\n",
      "Step: 6359, Loss: 0.937545120716095, Accuracy: 0.96875, Computation time: 1.4335417747497559\n",
      "Step: 6360, Loss: 0.9375241994857788, Accuracy: 0.96875, Computation time: 1.0284078121185303\n",
      "Step: 6361, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.1187856197357178\n",
      "Step: 6362, Loss: 0.9160889983177185, Accuracy: 1.0, Computation time: 1.326991319656372\n",
      "Step: 6363, Loss: 0.9375250935554504, Accuracy: 0.96875, Computation time: 1.5456900596618652\n",
      "Step: 6364, Loss: 0.9161716103553772, Accuracy: 1.0, Computation time: 1.1471402645111084\n",
      "Step: 6365, Loss: 0.9374580979347229, Accuracy: 0.96875, Computation time: 1.195422649383545\n",
      "Step: 6366, Loss: 0.9160187840461731, Accuracy: 1.0, Computation time: 1.3014819622039795\n",
      "Step: 6367, Loss: 0.9160553216934204, Accuracy: 1.0, Computation time: 1.8259625434875488\n",
      "Step: 6368, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.290726900100708\n",
      "Step: 6369, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 0.9957170486450195\n",
      "Step: 6370, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.1915037631988525\n",
      "Step: 6371, Loss: 0.9162676334381104, Accuracy: 1.0, Computation time: 1.4160823822021484\n",
      "Step: 6372, Loss: 0.9163059592247009, Accuracy: 1.0, Computation time: 1.4439911842346191\n",
      "Step: 6373, Loss: 0.9159457683563232, Accuracy: 1.0, Computation time: 1.211874008178711\n",
      "Step: 6374, Loss: 0.9159499406814575, Accuracy: 1.0, Computation time: 1.2738258838653564\n",
      "Step: 6375, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.2697157859802246\n",
      "Step: 6376, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 1.2682030200958252\n",
      "Step: 6377, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.2510991096496582\n",
      "Step: 6378, Loss: 0.9171333312988281, Accuracy: 1.0, Computation time: 1.3727238178253174\n",
      "Step: 6379, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.2705202102661133\n",
      "Step: 6380, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.2861406803131104\n",
      "Step: 6381, Loss: 0.9374286532402039, Accuracy: 0.96875, Computation time: 1.8194947242736816\n",
      "Step: 6382, Loss: 0.9159916043281555, Accuracy: 1.0, Computation time: 1.2378900051116943\n",
      "Step: 6383, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.2034389972686768\n",
      "Step: 6384, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.40370774269104\n",
      "Step: 6385, Loss: 0.9159145951271057, Accuracy: 1.0, Computation time: 1.3652536869049072\n",
      "Step: 6386, Loss: 0.9159459471702576, Accuracy: 1.0, Computation time: 1.1861121654510498\n",
      "Step: 6387, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.327331781387329\n",
      "Step: 6388, Loss: 0.9165592789649963, Accuracy: 1.0, Computation time: 1.4899797439575195\n",
      "Step: 6389, Loss: 0.9366661310195923, Accuracy: 0.96875, Computation time: 1.270063877105713\n",
      "Step: 6390, Loss: 0.9160497188568115, Accuracy: 1.0, Computation time: 1.4123823642730713\n",
      "Step: 6391, Loss: 0.916001558303833, Accuracy: 1.0, Computation time: 1.2747745513916016\n",
      "Step: 6392, Loss: 0.9376791715621948, Accuracy: 0.96875, Computation time: 1.4913411140441895\n",
      "########################\n",
      "Test loss: 1.117591142654419, Test Accuracy_epoch46: 0.706342339515686\n",
      "########################\n",
      "Step: 6393, Loss: 0.9160786867141724, Accuracy: 1.0, Computation time: 1.1200804710388184\n",
      "Step: 6394, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 1.3156259059906006\n",
      "Step: 6395, Loss: 0.9162368178367615, Accuracy: 1.0, Computation time: 1.642150640487671\n",
      "Step: 6396, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.1879043579101562\n",
      "Step: 6397, Loss: 0.9358997344970703, Accuracy: 0.96875, Computation time: 1.251251220703125\n",
      "Step: 6398, Loss: 0.9159261584281921, Accuracy: 1.0, Computation time: 1.3508639335632324\n",
      "Step: 6399, Loss: 0.9160134792327881, Accuracy: 1.0, Computation time: 1.2152154445648193\n",
      "Step: 6400, Loss: 0.9370437264442444, Accuracy: 0.96875, Computation time: 1.3017568588256836\n",
      "Step: 6401, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.750178575515747\n",
      "Step: 6402, Loss: 0.9160078167915344, Accuracy: 1.0, Computation time: 1.1821703910827637\n",
      "Step: 6403, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.1582210063934326\n",
      "Step: 6404, Loss: 0.9160650372505188, Accuracy: 1.0, Computation time: 1.5379202365875244\n",
      "Step: 6405, Loss: 0.9161710143089294, Accuracy: 1.0, Computation time: 1.8849351406097412\n",
      "Step: 6406, Loss: 0.9164237380027771, Accuracy: 1.0, Computation time: 1.4391720294952393\n",
      "Step: 6407, Loss: 0.9285590648651123, Accuracy: 0.96875, Computation time: 1.8357617855072021\n",
      "Step: 6408, Loss: 0.9159986972808838, Accuracy: 1.0, Computation time: 1.2633802890777588\n",
      "Step: 6409, Loss: 0.9592302441596985, Accuracy: 0.9375, Computation time: 1.3811042308807373\n",
      "Step: 6410, Loss: 0.9160754680633545, Accuracy: 1.0, Computation time: 1.2724053859710693\n",
      "Step: 6411, Loss: 0.9160459637641907, Accuracy: 1.0, Computation time: 0.987727165222168\n",
      "Step: 6412, Loss: 0.9163033962249756, Accuracy: 1.0, Computation time: 1.1646597385406494\n",
      "Step: 6413, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.1543762683868408\n",
      "Step: 6414, Loss: 0.9369572997093201, Accuracy: 0.96875, Computation time: 1.3676886558532715\n",
      "Step: 6415, Loss: 0.9378637671470642, Accuracy: 0.96875, Computation time: 1.3772516250610352\n",
      "Step: 6416, Loss: 0.9160038232803345, Accuracy: 1.0, Computation time: 1.4145584106445312\n",
      "Step: 6417, Loss: 0.9382364749908447, Accuracy: 0.96875, Computation time: 1.4232985973358154\n",
      "Step: 6418, Loss: 0.9376389980316162, Accuracy: 0.96875, Computation time: 1.3002338409423828\n",
      "Step: 6419, Loss: 0.9177196025848389, Accuracy: 1.0, Computation time: 1.4187726974487305\n",
      "Step: 6420, Loss: 0.9170103073120117, Accuracy: 1.0, Computation time: 1.6100437641143799\n",
      "Step: 6421, Loss: 0.9163850545883179, Accuracy: 1.0, Computation time: 1.24346923828125\n",
      "Step: 6422, Loss: 0.9163942337036133, Accuracy: 1.0, Computation time: 1.3632409572601318\n",
      "Step: 6423, Loss: 0.9379122853279114, Accuracy: 0.96875, Computation time: 1.8298542499542236\n",
      "Step: 6424, Loss: 0.9308005571365356, Accuracy: 0.96875, Computation time: 1.3883166313171387\n",
      "Step: 6425, Loss: 0.9166556000709534, Accuracy: 1.0, Computation time: 1.3502388000488281\n",
      "Step: 6426, Loss: 0.9161533117294312, Accuracy: 1.0, Computation time: 1.3794143199920654\n",
      "Step: 6427, Loss: 0.9160616993904114, Accuracy: 1.0, Computation time: 1.4185705184936523\n",
      "Step: 6428, Loss: 0.9160158634185791, Accuracy: 1.0, Computation time: 1.1977646350860596\n",
      "Step: 6429, Loss: 0.9160082340240479, Accuracy: 1.0, Computation time: 1.0989329814910889\n",
      "Step: 6430, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 1.2580313682556152\n",
      "Step: 6431, Loss: 0.9160230755805969, Accuracy: 1.0, Computation time: 1.7240941524505615\n",
      "Step: 6432, Loss: 0.9159602522850037, Accuracy: 1.0, Computation time: 1.3579092025756836\n",
      "Step: 6433, Loss: 0.9159416556358337, Accuracy: 1.0, Computation time: 1.1398332118988037\n",
      "Step: 6434, Loss: 0.933907687664032, Accuracy: 0.96875, Computation time: 1.691603660583496\n",
      "Step: 6435, Loss: 0.915976345539093, Accuracy: 1.0, Computation time: 1.3950603008270264\n",
      "Step: 6436, Loss: 0.9159557223320007, Accuracy: 1.0, Computation time: 1.1835308074951172\n",
      "Step: 6437, Loss: 0.9179590344429016, Accuracy: 1.0, Computation time: 1.3495912551879883\n",
      "Step: 6438, Loss: 0.9159376621246338, Accuracy: 1.0, Computation time: 1.1811566352844238\n",
      "Step: 6439, Loss: 0.9159477949142456, Accuracy: 1.0, Computation time: 1.103853464126587\n",
      "Step: 6440, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.3143243789672852\n",
      "Step: 6441, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.307861328125\n",
      "Step: 6442, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.1869335174560547\n",
      "Step: 6443, Loss: 0.9159119129180908, Accuracy: 1.0, Computation time: 1.1879994869232178\n",
      "Step: 6444, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.1988787651062012\n",
      "Step: 6445, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.178896427154541\n",
      "Step: 6446, Loss: 0.9375719428062439, Accuracy: 0.96875, Computation time: 1.278029441833496\n",
      "Step: 6447, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 0.9404704570770264\n",
      "Step: 6448, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.2006220817565918\n",
      "Step: 6449, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.5201122760772705\n",
      "Step: 6450, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.2667415142059326\n",
      "Step: 6451, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.0445430278778076\n",
      "Step: 6452, Loss: 0.9190205335617065, Accuracy: 1.0, Computation time: 1.7662489414215088\n",
      "Step: 6453, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.9141860008239746\n",
      "Step: 6454, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.1997463703155518\n",
      "Step: 6455, Loss: 0.9159548878669739, Accuracy: 1.0, Computation time: 1.2011396884918213\n",
      "Step: 6456, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 1.1743426322937012\n",
      "Step: 6457, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 1.5556399822235107\n",
      "Step: 6458, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.307999849319458\n",
      "Step: 6459, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 1.2258796691894531\n",
      "Step: 6460, Loss: 0.937491774559021, Accuracy: 0.96875, Computation time: 1.2743103504180908\n",
      "Step: 6461, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.197878122329712\n",
      "Step: 6462, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.1501476764678955\n",
      "Step: 6463, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 1.1051726341247559\n",
      "Step: 6464, Loss: 0.9375597834587097, Accuracy: 0.96875, Computation time: 1.0405497550964355\n",
      "Step: 6465, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 1.0921916961669922\n",
      "Step: 6466, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.2048349380493164\n",
      "Step: 6467, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.0804297924041748\n",
      "Step: 6468, Loss: 0.9374091625213623, Accuracy: 0.96875, Computation time: 1.1762113571166992\n",
      "Step: 6469, Loss: 0.9167695641517639, Accuracy: 1.0, Computation time: 1.4931282997131348\n",
      "Step: 6470, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.0634310245513916\n",
      "Step: 6471, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.7812645435333252\n",
      "Step: 6472, Loss: 0.9375454783439636, Accuracy: 0.96875, Computation time: 1.1482973098754883\n",
      "Step: 6473, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 1.250992774963379\n",
      "Step: 6474, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 1.1360201835632324\n",
      "Step: 6475, Loss: 0.937647819519043, Accuracy: 0.96875, Computation time: 1.4544458389282227\n",
      "Step: 6476, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.448620319366455\n",
      "Step: 6477, Loss: 0.9164703488349915, Accuracy: 1.0, Computation time: 1.4402472972869873\n",
      "Step: 6478, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.1254658699035645\n",
      "Step: 6479, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 1.0574769973754883\n",
      "Step: 6480, Loss: 0.9373483061790466, Accuracy: 0.96875, Computation time: 1.1290535926818848\n",
      "Step: 6481, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.116098165512085\n",
      "Step: 6482, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 1.2097909450531006\n",
      "Step: 6483, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.015528678894043\n",
      "Step: 6484, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.0411639213562012\n",
      "Step: 6485, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.1034283638000488\n",
      "Step: 6486, Loss: 0.9375154376029968, Accuracy: 0.96875, Computation time: 1.2909045219421387\n",
      "Step: 6487, Loss: 0.9374121427536011, Accuracy: 0.96875, Computation time: 1.2230241298675537\n",
      "Step: 6488, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.052347183227539\n",
      "Step: 6489, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 0.9355349540710449\n",
      "Step: 6490, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.5733387470245361\n",
      "Step: 6491, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.1899840831756592\n",
      "Step: 6492, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.0878121852874756\n",
      "Step: 6493, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.1614704132080078\n",
      "Step: 6494, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.055354118347168\n",
      "Step: 6495, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.214449167251587\n",
      "Step: 6496, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.7729387283325195\n",
      "Step: 6497, Loss: 0.9284816384315491, Accuracy: 0.96875, Computation time: 1.4533970355987549\n",
      "Step: 6498, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.2104988098144531\n",
      "Step: 6499, Loss: 0.9159936904907227, Accuracy: 1.0, Computation time: 1.4288761615753174\n",
      "Step: 6500, Loss: 0.9160021543502808, Accuracy: 1.0, Computation time: 1.2043464183807373\n",
      "Step: 6501, Loss: 0.9376593828201294, Accuracy: 0.96875, Computation time: 1.5075724124908447\n",
      "Step: 6502, Loss: 0.9359738826751709, Accuracy: 0.96875, Computation time: 1.1593585014343262\n",
      "Step: 6503, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 1.0513942241668701\n",
      "Step: 6504, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.3255584239959717\n",
      "Step: 6505, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.1502540111541748\n",
      "Step: 6506, Loss: 0.9375898241996765, Accuracy: 0.96875, Computation time: 1.2962825298309326\n",
      "Step: 6507, Loss: 0.9160563349723816, Accuracy: 1.0, Computation time: 1.337376594543457\n",
      "Step: 6508, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 1.1285924911499023\n",
      "Step: 6509, Loss: 0.9162044525146484, Accuracy: 1.0, Computation time: 1.2311649322509766\n",
      "Step: 6510, Loss: 0.9168883562088013, Accuracy: 1.0, Computation time: 1.1669490337371826\n",
      "Step: 6511, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.2033002376556396\n",
      "Step: 6512, Loss: 0.9160900115966797, Accuracy: 1.0, Computation time: 1.5212321281433105\n",
      "Step: 6513, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 1.1832311153411865\n",
      "Step: 6514, Loss: 0.9370105862617493, Accuracy: 0.96875, Computation time: 1.1835763454437256\n",
      "Step: 6515, Loss: 0.9370851516723633, Accuracy: 0.96875, Computation time: 1.134397029876709\n",
      "Step: 6516, Loss: 0.9159703254699707, Accuracy: 1.0, Computation time: 1.2042641639709473\n",
      "Step: 6517, Loss: 0.932579755783081, Accuracy: 0.96875, Computation time: 1.219778060913086\n",
      "Step: 6518, Loss: 0.937560498714447, Accuracy: 0.96875, Computation time: 1.2444496154785156\n",
      "Step: 6519, Loss: 0.9366956353187561, Accuracy: 0.96875, Computation time: 1.3691513538360596\n",
      "Step: 6520, Loss: 0.9162473082542419, Accuracy: 1.0, Computation time: 1.2406306266784668\n",
      "Step: 6521, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 1.1848983764648438\n",
      "Step: 6522, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 1.4967827796936035\n",
      "Step: 6523, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 1.309654951095581\n",
      "Step: 6524, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.7406342029571533\n",
      "Step: 6525, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.2359364032745361\n",
      "Step: 6526, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.0650899410247803\n",
      "Step: 6527, Loss: 0.9257892370223999, Accuracy: 0.96875, Computation time: 1.383138656616211\n",
      "Step: 6528, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.9366974830627441\n",
      "Step: 6529, Loss: 0.915958821773529, Accuracy: 1.0, Computation time: 1.3302006721496582\n",
      "Step: 6530, Loss: 0.9606247544288635, Accuracy: 0.9375, Computation time: 1.650404930114746\n",
      "Step: 6531, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 1.0923752784729004\n",
      "########################\n",
      "Test loss: 1.1156553030014038, Test Accuracy_epoch47: 0.7089487910270691\n",
      "########################\n",
      "Step: 6532, Loss: 0.9160476922988892, Accuracy: 1.0, Computation time: 1.0707757472991943\n",
      "Step: 6533, Loss: 0.9160242080688477, Accuracy: 1.0, Computation time: 1.0273330211639404\n",
      "Step: 6534, Loss: 0.9164853692054749, Accuracy: 1.0, Computation time: 1.2794816493988037\n",
      "Step: 6535, Loss: 0.928353488445282, Accuracy: 0.96875, Computation time: 1.8181231021881104\n",
      "Step: 6536, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 1.1170499324798584\n",
      "Step: 6537, Loss: 0.9158987402915955, Accuracy: 1.0, Computation time: 1.2843525409698486\n",
      "Step: 6538, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 1.5627532005310059\n",
      "Step: 6539, Loss: 0.915977418422699, Accuracy: 1.0, Computation time: 1.5341265201568604\n",
      "Step: 6540, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.0067036151885986\n",
      "Step: 6541, Loss: 0.9159389734268188, Accuracy: 1.0, Computation time: 1.210538387298584\n",
      "Step: 6542, Loss: 0.9375231266021729, Accuracy: 0.96875, Computation time: 1.105360984802246\n",
      "Step: 6543, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 1.0281825065612793\n",
      "Step: 6544, Loss: 0.9159691333770752, Accuracy: 1.0, Computation time: 1.0869898796081543\n",
      "Step: 6545, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.1901922225952148\n",
      "Step: 6546, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.0364582538604736\n",
      "Step: 6547, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.1335749626159668\n",
      "Step: 6548, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.0006752014160156\n",
      "Step: 6549, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.0159544944763184\n",
      "Step: 6550, Loss: 0.9374850392341614, Accuracy: 0.96875, Computation time: 0.8663487434387207\n",
      "Step: 6551, Loss: 0.9375919103622437, Accuracy: 0.96875, Computation time: 1.2262523174285889\n",
      "Step: 6552, Loss: 0.9160007238388062, Accuracy: 1.0, Computation time: 0.9776453971862793\n",
      "Step: 6553, Loss: 0.9159277081489563, Accuracy: 1.0, Computation time: 0.9535753726959229\n",
      "Step: 6554, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 0.9586091041564941\n",
      "Step: 6555, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 1.243173599243164\n",
      "Step: 6556, Loss: 0.916152834892273, Accuracy: 1.0, Computation time: 1.5675384998321533\n",
      "Step: 6557, Loss: 0.9375115633010864, Accuracy: 0.96875, Computation time: 0.9693946838378906\n",
      "Step: 6558, Loss: 0.9184651374816895, Accuracy: 1.0, Computation time: 1.1107738018035889\n",
      "Step: 6559, Loss: 0.9161024689674377, Accuracy: 1.0, Computation time: 1.0950884819030762\n",
      "Step: 6560, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 0.9469888210296631\n",
      "Step: 6561, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 0.9925761222839355\n",
      "Step: 6562, Loss: 0.9159337282180786, Accuracy: 1.0, Computation time: 0.9648056030273438\n",
      "Step: 6563, Loss: 0.9178956747055054, Accuracy: 1.0, Computation time: 1.1263318061828613\n",
      "Step: 6564, Loss: 0.91688472032547, Accuracy: 1.0, Computation time: 1.0764248371124268\n",
      "Step: 6565, Loss: 0.9159683585166931, Accuracy: 1.0, Computation time: 0.9615421295166016\n",
      "Step: 6566, Loss: 0.9367913603782654, Accuracy: 0.96875, Computation time: 1.075610637664795\n",
      "Step: 6567, Loss: 0.9159446358680725, Accuracy: 1.0, Computation time: 0.9390411376953125\n",
      "Step: 6568, Loss: 0.9160345196723938, Accuracy: 1.0, Computation time: 1.462308645248413\n",
      "Step: 6569, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.0317885875701904\n",
      "Step: 6570, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.2473535537719727\n",
      "Step: 6571, Loss: 0.9375062584877014, Accuracy: 0.96875, Computation time: 1.2662158012390137\n",
      "Step: 6572, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 1.3455555438995361\n",
      "Step: 6573, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 0.9208173751831055\n",
      "Step: 6574, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 1.0809166431427002\n",
      "Step: 6575, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.120804786682129\n",
      "Step: 6576, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.8659727573394775\n",
      "Step: 6577, Loss: 0.9206231832504272, Accuracy: 1.0, Computation time: 1.3490276336669922\n",
      "Step: 6578, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.1401376724243164\n",
      "Step: 6579, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 0.9117074012756348\n",
      "Step: 6580, Loss: 0.9375155568122864, Accuracy: 0.96875, Computation time: 1.2299525737762451\n",
      "Step: 6581, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.0282349586486816\n",
      "Step: 6582, Loss: 0.9371902346611023, Accuracy: 0.96875, Computation time: 1.0366876125335693\n",
      "Step: 6583, Loss: 0.9430803656578064, Accuracy: 0.96875, Computation time: 1.4084620475769043\n",
      "Step: 6584, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.3975129127502441\n",
      "Step: 6585, Loss: 0.9368598461151123, Accuracy: 0.96875, Computation time: 1.120455265045166\n",
      "Step: 6586, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 0.8839998245239258\n",
      "Step: 6587, Loss: 0.9163123965263367, Accuracy: 1.0, Computation time: 1.1787371635437012\n",
      "Step: 6588, Loss: 0.9159805774688721, Accuracy: 1.0, Computation time: 1.3511407375335693\n",
      "Step: 6589, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.150712251663208\n",
      "Step: 6590, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 1.0938570499420166\n",
      "Step: 6591, Loss: 0.9159442782402039, Accuracy: 1.0, Computation time: 1.038696527481079\n",
      "Step: 6592, Loss: 0.9173916578292847, Accuracy: 1.0, Computation time: 1.047055959701538\n",
      "Step: 6593, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.015681505203247\n",
      "Step: 6594, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 1.036975383758545\n",
      "Step: 6595, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.4011926651000977\n",
      "Step: 6596, Loss: 0.9164950251579285, Accuracy: 1.0, Computation time: 1.3979942798614502\n",
      "Step: 6597, Loss: 0.9159018993377686, Accuracy: 1.0, Computation time: 1.120448112487793\n",
      "Step: 6598, Loss: 0.9159534573554993, Accuracy: 1.0, Computation time: 1.0765924453735352\n",
      "Step: 6599, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.0216197967529297\n",
      "Step: 6600, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.046750545501709\n",
      "Step: 6601, Loss: 0.9159122109413147, Accuracy: 1.0, Computation time: 1.2274627685546875\n",
      "Step: 6602, Loss: 0.9159075617790222, Accuracy: 1.0, Computation time: 0.9662346839904785\n",
      "Step: 6603, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.1916728019714355\n",
      "Step: 6604, Loss: 0.9162446856498718, Accuracy: 1.0, Computation time: 1.4496006965637207\n",
      "Step: 6605, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.191230058670044\n",
      "Step: 6606, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.3511672019958496\n",
      "Step: 6607, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.2567505836486816\n",
      "Step: 6608, Loss: 0.9232975244522095, Accuracy: 1.0, Computation time: 1.9325511455535889\n",
      "Step: 6609, Loss: 0.9375009536743164, Accuracy: 0.96875, Computation time: 1.10931396484375\n",
      "Step: 6610, Loss: 0.9377055168151855, Accuracy: 0.96875, Computation time: 1.7445197105407715\n",
      "Step: 6611, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.3950703144073486\n",
      "Step: 6612, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 0.9875955581665039\n",
      "Step: 6613, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.2594380378723145\n",
      "Step: 6614, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 0.9948079586029053\n",
      "Step: 6615, Loss: 0.9378196001052856, Accuracy: 0.96875, Computation time: 1.0931971073150635\n",
      "Step: 6616, Loss: 0.9159518480300903, Accuracy: 1.0, Computation time: 1.0409657955169678\n",
      "Step: 6617, Loss: 0.9229598045349121, Accuracy: 1.0, Computation time: 1.5185918807983398\n",
      "Step: 6618, Loss: 0.9161311388015747, Accuracy: 1.0, Computation time: 1.1707735061645508\n",
      "Step: 6619, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 0.8764023780822754\n",
      "Step: 6620, Loss: 0.916053295135498, Accuracy: 1.0, Computation time: 1.2330241203308105\n",
      "Step: 6621, Loss: 0.9159709215164185, Accuracy: 1.0, Computation time: 1.0677075386047363\n",
      "Step: 6622, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 1.2224087715148926\n",
      "Step: 6623, Loss: 0.9374279975891113, Accuracy: 0.96875, Computation time: 0.957334041595459\n",
      "Step: 6624, Loss: 0.9320843815803528, Accuracy: 0.96875, Computation time: 1.5428106784820557\n",
      "Step: 6625, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 0.8354425430297852\n",
      "Step: 6626, Loss: 0.9159122109413147, Accuracy: 1.0, Computation time: 1.110755443572998\n",
      "Step: 6627, Loss: 0.9591981768608093, Accuracy: 0.9375, Computation time: 1.1789674758911133\n",
      "Step: 6628, Loss: 0.9374684691429138, Accuracy: 0.96875, Computation time: 1.088512659072876\n",
      "Step: 6629, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 0.9279217720031738\n",
      "Step: 6630, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.0928068161010742\n",
      "Step: 6631, Loss: 0.9198483824729919, Accuracy: 1.0, Computation time: 0.9712138175964355\n",
      "Step: 6632, Loss: 0.9376031756401062, Accuracy: 0.96875, Computation time: 0.9302046298980713\n",
      "Step: 6633, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 1.2432448863983154\n",
      "Step: 6634, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 0.9539730548858643\n",
      "Step: 6635, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 0.9868056774139404\n",
      "Step: 6636, Loss: 0.9168709516525269, Accuracy: 1.0, Computation time: 1.3267109394073486\n",
      "Step: 6637, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.0349302291870117\n",
      "Step: 6638, Loss: 0.919929563999176, Accuracy: 1.0, Computation time: 1.1976912021636963\n",
      "Step: 6639, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.2388174533843994\n",
      "Step: 6640, Loss: 0.9376106262207031, Accuracy: 0.96875, Computation time: 1.128345251083374\n",
      "Step: 6641, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 0.9468369483947754\n",
      "Step: 6642, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.1317200660705566\n",
      "Step: 6643, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 1.0568270683288574\n",
      "Step: 6644, Loss: 0.9286682605743408, Accuracy: 0.96875, Computation time: 1.4910645484924316\n",
      "Step: 6645, Loss: 0.9374932646751404, Accuracy: 0.96875, Computation time: 1.0343267917633057\n",
      "Step: 6646, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.155951738357544\n",
      "Step: 6647, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.3174550533294678\n",
      "Step: 6648, Loss: 0.9374760389328003, Accuracy: 0.96875, Computation time: 1.1483023166656494\n",
      "Step: 6649, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.8513522148132324\n",
      "Step: 6650, Loss: 0.9158843159675598, Accuracy: 1.0, Computation time: 1.32218337059021\n",
      "Step: 6651, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.1491656303405762\n",
      "Step: 6652, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.16182541847229\n",
      "Step: 6653, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.0798184871673584\n",
      "Step: 6654, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 1.2918610572814941\n",
      "Step: 6655, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.2064225673675537\n",
      "Step: 6656, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 1.5103259086608887\n",
      "Step: 6657, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.3361256122589111\n",
      "Step: 6658, Loss: 0.9159537553787231, Accuracy: 1.0, Computation time: 1.122955083847046\n",
      "Step: 6659, Loss: 0.9160332083702087, Accuracy: 1.0, Computation time: 1.2749011516571045\n",
      "Step: 6660, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.7163269519805908\n",
      "Step: 6661, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.1113812923431396\n",
      "Step: 6662, Loss: 0.9220651984214783, Accuracy: 1.0, Computation time: 1.6316354274749756\n",
      "Step: 6663, Loss: 0.937490701675415, Accuracy: 0.96875, Computation time: 1.2091410160064697\n",
      "Step: 6664, Loss: 0.9160131216049194, Accuracy: 1.0, Computation time: 1.2076709270477295\n",
      "Step: 6665, Loss: 0.9377050399780273, Accuracy: 0.96875, Computation time: 1.3125441074371338\n",
      "Step: 6666, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 1.0760350227355957\n",
      "Step: 6667, Loss: 0.9159123301506042, Accuracy: 1.0, Computation time: 1.243004560470581\n",
      "Step: 6668, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.0474803447723389\n",
      "Step: 6669, Loss: 0.9377391934394836, Accuracy: 0.96875, Computation time: 1.1623456478118896\n",
      "########################\n",
      "Test loss: 1.1192291975021362, Test Accuracy_epoch48: 0.706342339515686\n",
      "########################\n",
      "Step: 6670, Loss: 0.9162004590034485, Accuracy: 1.0, Computation time: 1.2108080387115479\n",
      "Step: 6671, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.2646570205688477\n",
      "Step: 6672, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.215855360031128\n",
      "Step: 6673, Loss: 0.915985107421875, Accuracy: 1.0, Computation time: 1.4926519393920898\n",
      "Step: 6674, Loss: 0.9160475134849548, Accuracy: 1.0, Computation time: 1.2692298889160156\n",
      "Step: 6675, Loss: 0.9375288486480713, Accuracy: 0.96875, Computation time: 1.2894995212554932\n",
      "Step: 6676, Loss: 0.9164087772369385, Accuracy: 1.0, Computation time: 1.274500846862793\n",
      "Step: 6677, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.2678136825561523\n",
      "Step: 6678, Loss: 0.9159628748893738, Accuracy: 1.0, Computation time: 1.267268419265747\n",
      "Step: 6679, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.0169730186462402\n",
      "Step: 6680, Loss: 0.9159789681434631, Accuracy: 1.0, Computation time: 1.1477229595184326\n",
      "Step: 6681, Loss: 0.9164304733276367, Accuracy: 1.0, Computation time: 1.5031840801239014\n",
      "Step: 6682, Loss: 0.937972903251648, Accuracy: 0.96875, Computation time: 1.5046460628509521\n",
      "Step: 6683, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 1.6501586437225342\n",
      "Step: 6684, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.024911642074585\n",
      "Step: 6685, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.2077324390411377\n",
      "Step: 6686, Loss: 0.9591811895370483, Accuracy: 0.9375, Computation time: 1.2674009799957275\n",
      "Step: 6687, Loss: 0.915956974029541, Accuracy: 1.0, Computation time: 1.4875714778900146\n",
      "Step: 6688, Loss: 0.915941596031189, Accuracy: 1.0, Computation time: 1.129920244216919\n",
      "Step: 6689, Loss: 0.9161763191223145, Accuracy: 1.0, Computation time: 1.4579529762268066\n",
      "Step: 6690, Loss: 0.9376043081283569, Accuracy: 0.96875, Computation time: 1.4724557399749756\n",
      "Step: 6691, Loss: 0.9348618984222412, Accuracy: 0.96875, Computation time: 1.6619033813476562\n",
      "Step: 6692, Loss: 0.9160061478614807, Accuracy: 1.0, Computation time: 1.2220561504364014\n",
      "Step: 6693, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.5816259384155273\n",
      "Step: 6694, Loss: 0.9365013837814331, Accuracy: 0.96875, Computation time: 2.141131639480591\n",
      "Step: 6695, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 1.1969828605651855\n",
      "Step: 6696, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.3230185508728027\n",
      "Step: 6697, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.1132473945617676\n",
      "Step: 6698, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.4901280403137207\n",
      "Step: 6699, Loss: 0.9348310828208923, Accuracy: 0.96875, Computation time: 1.5958278179168701\n",
      "Step: 6700, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.5992281436920166\n",
      "Step: 6701, Loss: 0.9309822916984558, Accuracy: 0.96875, Computation time: 1.4344217777252197\n",
      "Step: 6702, Loss: 0.9366132020950317, Accuracy: 0.96875, Computation time: 1.4069077968597412\n",
      "Step: 6703, Loss: 0.9159820675849915, Accuracy: 1.0, Computation time: 1.3187901973724365\n",
      "Step: 6704, Loss: 0.9160756468772888, Accuracy: 1.0, Computation time: 1.2430851459503174\n",
      "Step: 6705, Loss: 0.9159567952156067, Accuracy: 1.0, Computation time: 1.2420525550842285\n",
      "Step: 6706, Loss: 0.9159809947013855, Accuracy: 1.0, Computation time: 1.1748950481414795\n",
      "Step: 6707, Loss: 0.9242434501647949, Accuracy: 1.0, Computation time: 1.9661614894866943\n",
      "Step: 6708, Loss: 0.9171156883239746, Accuracy: 1.0, Computation time: 1.8387112617492676\n",
      "Step: 6709, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 1.1781425476074219\n",
      "Step: 6710, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.2847366333007812\n",
      "Step: 6711, Loss: 0.9236695766448975, Accuracy: 1.0, Computation time: 1.196054458618164\n",
      "Step: 6712, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.0814237594604492\n",
      "Step: 6713, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.0854136943817139\n",
      "Step: 6714, Loss: 0.9159471392631531, Accuracy: 1.0, Computation time: 1.0695288181304932\n",
      "Step: 6715, Loss: 0.9160071611404419, Accuracy: 1.0, Computation time: 1.074659824371338\n",
      "Step: 6716, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.2548792362213135\n",
      "Step: 6717, Loss: 0.9376218914985657, Accuracy: 0.96875, Computation time: 1.3052566051483154\n",
      "Step: 6718, Loss: 0.9159650802612305, Accuracy: 1.0, Computation time: 1.2331950664520264\n",
      "Step: 6719, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 1.5113341808319092\n",
      "Step: 6720, Loss: 0.9160088896751404, Accuracy: 1.0, Computation time: 1.2583563327789307\n",
      "Step: 6721, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 1.2076144218444824\n",
      "Step: 6722, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.421004295349121\n",
      "Step: 6723, Loss: 0.9375879764556885, Accuracy: 0.96875, Computation time: 1.1741840839385986\n",
      "Step: 6724, Loss: 0.9376062154769897, Accuracy: 0.96875, Computation time: 1.0294029712677002\n",
      "Step: 6725, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.5274765491485596\n",
      "Step: 6726, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.38038969039917\n",
      "Step: 6727, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 1.1730928421020508\n",
      "Step: 6728, Loss: 0.9159393310546875, Accuracy: 1.0, Computation time: 1.1562824249267578\n",
      "Step: 6729, Loss: 0.9228714108467102, Accuracy: 1.0, Computation time: 1.1895420551300049\n",
      "Step: 6730, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 1.234464168548584\n",
      "Step: 6731, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.3010947704315186\n",
      "Step: 6732, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.3155322074890137\n",
      "Step: 6733, Loss: 0.915890634059906, Accuracy: 1.0, Computation time: 1.2375338077545166\n",
      "Step: 6734, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 1.046682596206665\n",
      "Step: 6735, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 1.0725812911987305\n",
      "Step: 6736, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.2493863105773926\n",
      "Step: 6737, Loss: 0.9159016013145447, Accuracy: 1.0, Computation time: 1.3182101249694824\n",
      "Step: 6738, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.1113905906677246\n",
      "Step: 6739, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.0517487525939941\n",
      "Step: 6740, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.1335067749023438\n",
      "Step: 6741, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.1550889015197754\n",
      "Step: 6742, Loss: 0.9162565469741821, Accuracy: 1.0, Computation time: 1.1656534671783447\n",
      "Step: 6743, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.0675771236419678\n",
      "Step: 6744, Loss: 0.9158453941345215, Accuracy: 1.0, Computation time: 1.402782678604126\n",
      "Step: 6745, Loss: 0.937497079372406, Accuracy: 0.96875, Computation time: 1.1891703605651855\n",
      "Step: 6746, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.30263090133667\n",
      "Step: 6747, Loss: 0.9375823736190796, Accuracy: 0.96875, Computation time: 1.2604122161865234\n",
      "Step: 6748, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.1424596309661865\n",
      "Step: 6749, Loss: 0.9160383939743042, Accuracy: 1.0, Computation time: 1.5746581554412842\n",
      "Step: 6750, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.4854254722595215\n",
      "Step: 6751, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.1086781024932861\n",
      "Step: 6752, Loss: 0.9376382827758789, Accuracy: 0.96875, Computation time: 1.6469476222991943\n",
      "Step: 6753, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.494692325592041\n",
      "Step: 6754, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.2055704593658447\n",
      "Step: 6755, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.360534906387329\n",
      "Step: 6756, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 1.0703651905059814\n",
      "Step: 6757, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 2.010051965713501\n",
      "Step: 6758, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.1936657428741455\n",
      "Step: 6759, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.161663293838501\n",
      "Step: 6760, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.444166898727417\n",
      "Step: 6761, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.273158311843872\n",
      "Step: 6762, Loss: 0.9298510551452637, Accuracy: 0.96875, Computation time: 1.2382442951202393\n",
      "Step: 6763, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.3015587329864502\n",
      "Step: 6764, Loss: 0.9378503561019897, Accuracy: 0.96875, Computation time: 1.4594345092773438\n",
      "Step: 6765, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.1849088668823242\n",
      "Step: 6766, Loss: 0.9375255107879639, Accuracy: 0.96875, Computation time: 0.9369916915893555\n",
      "Step: 6767, Loss: 0.9376657009124756, Accuracy: 0.96875, Computation time: 1.608602523803711\n",
      "Step: 6768, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 0.9414732456207275\n",
      "Step: 6769, Loss: 0.9161943197250366, Accuracy: 1.0, Computation time: 1.3066010475158691\n",
      "Step: 6770, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 1.3446097373962402\n",
      "Step: 6771, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.254800796508789\n",
      "Step: 6772, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 0.8839449882507324\n",
      "Step: 6773, Loss: 0.9374717473983765, Accuracy: 0.96875, Computation time: 1.0976800918579102\n",
      "Step: 6774, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 0.9883542060852051\n",
      "Step: 6775, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 0.9455296993255615\n",
      "Step: 6776, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.3377878665924072\n",
      "Step: 6777, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.1780803203582764\n",
      "Step: 6778, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.9297776222229004\n",
      "Step: 6779, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 0.9289143085479736\n",
      "Step: 6780, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.2171909809112549\n",
      "Step: 6781, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.0860145092010498\n",
      "Step: 6782, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.1693685054779053\n",
      "Step: 6783, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.107816457748413\n",
      "Step: 6784, Loss: 0.9162874221801758, Accuracy: 1.0, Computation time: 1.1223628520965576\n",
      "Step: 6785, Loss: 0.9168060421943665, Accuracy: 1.0, Computation time: 1.1616013050079346\n",
      "Step: 6786, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.2222745418548584\n",
      "Step: 6787, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.1911399364471436\n",
      "Step: 6788, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.3493657112121582\n",
      "Step: 6789, Loss: 0.91595458984375, Accuracy: 1.0, Computation time: 1.2310190200805664\n",
      "Step: 6790, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 0.8886826038360596\n",
      "Step: 6791, Loss: 0.9359225630760193, Accuracy: 0.96875, Computation time: 1.4275870323181152\n",
      "Step: 6792, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 0.970477819442749\n",
      "Step: 6793, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.5337696075439453\n",
      "Step: 6794, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.26316499710083\n",
      "Step: 6795, Loss: 0.9375523924827576, Accuracy: 0.96875, Computation time: 1.1496093273162842\n",
      "Step: 6796, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.5089046955108643\n",
      "Step: 6797, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 0.9231493473052979\n",
      "Step: 6798, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.327528953552246\n",
      "Step: 6799, Loss: 0.9375947117805481, Accuracy: 0.96875, Computation time: 1.3076605796813965\n",
      "Step: 6800, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.186854362487793\n",
      "Step: 6801, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.160834550857544\n",
      "Step: 6802, Loss: 0.916161060333252, Accuracy: 1.0, Computation time: 1.316476583480835\n",
      "Step: 6803, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 1.0264451503753662\n",
      "Step: 6804, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 0.9654936790466309\n",
      "Step: 6805, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.3606441020965576\n",
      "Step: 6806, Loss: 0.937539279460907, Accuracy: 0.96875, Computation time: 1.580343246459961\n",
      "Step: 6807, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.0446746349334717\n",
      "Step: 6808, Loss: 0.9180282354354858, Accuracy: 1.0, Computation time: 1.1990869045257568\n",
      "########################\n",
      "Test loss: 1.1181056499481201, Test Accuracy_epoch49: 0.7046046853065491\n",
      "########################\n",
      "Step: 6809, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.0988881587982178\n",
      "Step: 6810, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.2619354724884033\n",
      "Step: 6811, Loss: 0.9359511733055115, Accuracy: 0.96875, Computation time: 1.4967584609985352\n",
      "Step: 6812, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.1182241439819336\n",
      "Step: 6813, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.0993473529815674\n",
      "Step: 6814, Loss: 0.9491258263587952, Accuracy: 0.9375, Computation time: 1.654264211654663\n",
      "Step: 6815, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.2882764339447021\n",
      "Step: 6816, Loss: 0.937522828578949, Accuracy: 0.96875, Computation time: 1.6710550785064697\n",
      "Step: 6817, Loss: 0.9376167058944702, Accuracy: 0.96875, Computation time: 1.251192331314087\n",
      "Step: 6818, Loss: 0.9375811815261841, Accuracy: 0.96875, Computation time: 1.290719985961914\n",
      "Step: 6819, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 1.2264621257781982\n",
      "Step: 6820, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.0203638076782227\n",
      "Step: 6821, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.2090320587158203\n",
      "Step: 6822, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.0046894550323486\n",
      "Step: 6823, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.0737650394439697\n",
      "Step: 6824, Loss: 0.9158667922019958, Accuracy: 1.0, Computation time: 1.1580708026885986\n",
      "Step: 6825, Loss: 0.9160446524620056, Accuracy: 1.0, Computation time: 1.1578481197357178\n",
      "Step: 6826, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.2874293327331543\n",
      "Step: 6827, Loss: 0.9375467300415039, Accuracy: 0.96875, Computation time: 1.1452159881591797\n",
      "Step: 6828, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 1.2208600044250488\n",
      "Step: 6829, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.3845820426940918\n",
      "Step: 6830, Loss: 0.9436684846878052, Accuracy: 0.96875, Computation time: 1.5550806522369385\n",
      "Step: 6831, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.038841724395752\n",
      "Step: 6832, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.2164244651794434\n",
      "Step: 6833, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 0.9288656711578369\n",
      "Step: 6834, Loss: 0.9160758852958679, Accuracy: 1.0, Computation time: 1.3848893642425537\n",
      "Step: 6835, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.0293595790863037\n",
      "Step: 6836, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 0.9886054992675781\n",
      "Step: 6837, Loss: 0.9374977946281433, Accuracy: 0.96875, Computation time: 0.9935441017150879\n",
      "Step: 6838, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.1079390048980713\n",
      "Step: 6839, Loss: 0.9159877896308899, Accuracy: 1.0, Computation time: 1.0055456161499023\n",
      "Step: 6840, Loss: 0.920337438583374, Accuracy: 1.0, Computation time: 1.4881389141082764\n",
      "Step: 6841, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 1.03448486328125\n",
      "Step: 6842, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 0.9596982002258301\n",
      "Step: 6843, Loss: 0.9376363754272461, Accuracy: 0.96875, Computation time: 1.0117075443267822\n",
      "Step: 6844, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.0866518020629883\n",
      "Step: 6845, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 1.1160686016082764\n",
      "Step: 6846, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.3253872394561768\n",
      "Step: 6847, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.250624656677246\n",
      "Step: 6848, Loss: 0.9372994899749756, Accuracy: 0.96875, Computation time: 1.2144038677215576\n",
      "Step: 6849, Loss: 0.9159280061721802, Accuracy: 1.0, Computation time: 1.356022834777832\n",
      "Step: 6850, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.0996341705322266\n",
      "Step: 6851, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.2833483219146729\n",
      "Step: 6852, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 1.0616743564605713\n",
      "Step: 6853, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.0120081901550293\n",
      "Step: 6854, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.5592050552368164\n",
      "Step: 6855, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.2039639949798584\n",
      "Step: 6856, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.217674732208252\n",
      "Step: 6857, Loss: 0.9449399709701538, Accuracy: 0.96875, Computation time: 1.6872966289520264\n",
      "Step: 6858, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.0713684558868408\n",
      "Step: 6859, Loss: 0.9374808073043823, Accuracy: 0.96875, Computation time: 1.2257108688354492\n",
      "Step: 6860, Loss: 0.9159939885139465, Accuracy: 1.0, Computation time: 0.9122576713562012\n",
      "Step: 6861, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 1.1568872928619385\n",
      "Step: 6862, Loss: 0.9161337614059448, Accuracy: 1.0, Computation time: 1.3028767108917236\n",
      "Step: 6863, Loss: 0.916045606136322, Accuracy: 1.0, Computation time: 1.01173996925354\n",
      "Step: 6864, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 1.0758144855499268\n",
      "Step: 6865, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.6318392753601074\n",
      "Step: 6866, Loss: 0.9160869121551514, Accuracy: 1.0, Computation time: 1.0460474491119385\n",
      "Step: 6867, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 1.1326534748077393\n",
      "Step: 6868, Loss: 0.916080892086029, Accuracy: 1.0, Computation time: 0.9770224094390869\n",
      "Step: 6869, Loss: 0.9160226583480835, Accuracy: 1.0, Computation time: 1.017777919769287\n",
      "Step: 6870, Loss: 0.9218472838401794, Accuracy: 1.0, Computation time: 1.2658183574676514\n",
      "Step: 6871, Loss: 0.9159802198410034, Accuracy: 1.0, Computation time: 1.4119737148284912\n",
      "Step: 6872, Loss: 0.9378578662872314, Accuracy: 0.96875, Computation time: 1.5866870880126953\n",
      "Step: 6873, Loss: 0.9160448908805847, Accuracy: 1.0, Computation time: 1.3655035495758057\n",
      "Step: 6874, Loss: 0.9361456632614136, Accuracy: 0.96875, Computation time: 1.2852387428283691\n",
      "Step: 6875, Loss: 0.9160722494125366, Accuracy: 1.0, Computation time: 1.0729923248291016\n",
      "Step: 6876, Loss: 0.9161625504493713, Accuracy: 1.0, Computation time: 1.3930599689483643\n",
      "Step: 6877, Loss: 0.9378055334091187, Accuracy: 0.96875, Computation time: 1.6205723285675049\n",
      "Step: 6878, Loss: 0.9159371256828308, Accuracy: 1.0, Computation time: 1.1740081310272217\n",
      "Step: 6879, Loss: 0.9159458875656128, Accuracy: 1.0, Computation time: 1.122309923171997\n",
      "Step: 6880, Loss: 0.9159905314445496, Accuracy: 1.0, Computation time: 1.157263994216919\n",
      "Step: 6881, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.051759958267212\n",
      "Step: 6882, Loss: 0.9376266002655029, Accuracy: 0.96875, Computation time: 0.9363927841186523\n",
      "Step: 6883, Loss: 0.9161109924316406, Accuracy: 1.0, Computation time: 1.1377010345458984\n",
      "Step: 6884, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 0.9945573806762695\n",
      "Step: 6885, Loss: 0.9159339666366577, Accuracy: 1.0, Computation time: 1.3213920593261719\n",
      "Step: 6886, Loss: 0.937218427658081, Accuracy: 0.96875, Computation time: 1.0014777183532715\n",
      "Step: 6887, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.2803950309753418\n",
      "Step: 6888, Loss: 0.9394536018371582, Accuracy: 0.96875, Computation time: 1.1564953327178955\n",
      "Step: 6889, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.0476715564727783\n",
      "Step: 6890, Loss: 0.915938675403595, Accuracy: 1.0, Computation time: 1.0181653499603271\n",
      "Step: 6891, Loss: 0.9159355759620667, Accuracy: 1.0, Computation time: 1.028745412826538\n",
      "Step: 6892, Loss: 0.9375362992286682, Accuracy: 0.96875, Computation time: 1.0693354606628418\n",
      "Step: 6893, Loss: 0.916020929813385, Accuracy: 1.0, Computation time: 1.0570335388183594\n",
      "Step: 6894, Loss: 0.9159574508666992, Accuracy: 1.0, Computation time: 1.2331576347351074\n",
      "Step: 6895, Loss: 0.9159601330757141, Accuracy: 1.0, Computation time: 1.1222965717315674\n",
      "Step: 6896, Loss: 0.915935754776001, Accuracy: 1.0, Computation time: 1.1745738983154297\n",
      "Step: 6897, Loss: 0.9160007834434509, Accuracy: 1.0, Computation time: 1.3902664184570312\n",
      "Step: 6898, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 0.9360451698303223\n",
      "Step: 6899, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.156754493713379\n",
      "Step: 6900, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 1.1448726654052734\n",
      "Step: 6901, Loss: 0.9618514180183411, Accuracy: 0.9375, Computation time: 1.6486732959747314\n",
      "Step: 6902, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.0184288024902344\n",
      "Step: 6903, Loss: 0.937872052192688, Accuracy: 0.96875, Computation time: 1.2391166687011719\n",
      "Step: 6904, Loss: 0.9159657955169678, Accuracy: 1.0, Computation time: 1.474621057510376\n",
      "Step: 6905, Loss: 0.922735333442688, Accuracy: 1.0, Computation time: 1.757227897644043\n",
      "Step: 6906, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 1.4685955047607422\n",
      "Step: 6907, Loss: 0.9159346222877502, Accuracy: 1.0, Computation time: 1.070091962814331\n",
      "Step: 6908, Loss: 0.9159396290779114, Accuracy: 1.0, Computation time: 1.4468612670898438\n",
      "Step: 6909, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.706364631652832\n",
      "Step: 6910, Loss: 0.9550502896308899, Accuracy: 0.9375, Computation time: 1.4629004001617432\n",
      "Step: 6911, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 0.8889861106872559\n",
      "Step: 6912, Loss: 0.9159318208694458, Accuracy: 1.0, Computation time: 1.4505360126495361\n",
      "Step: 6913, Loss: 0.9158980846405029, Accuracy: 1.0, Computation time: 1.005828857421875\n",
      "Step: 6914, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 1.7831947803497314\n",
      "Step: 6915, Loss: 0.9159197211265564, Accuracy: 1.0, Computation time: 1.6340839862823486\n",
      "Step: 6916, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.4890365600585938\n",
      "Step: 6917, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.502460241317749\n",
      "Step: 6918, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.3040416240692139\n",
      "Step: 6919, Loss: 0.9438518285751343, Accuracy: 0.96875, Computation time: 1.6242973804473877\n",
      "Step: 6920, Loss: 0.915908694267273, Accuracy: 1.0, Computation time: 1.2567121982574463\n",
      "Step: 6921, Loss: 0.9160671830177307, Accuracy: 1.0, Computation time: 1.5844995975494385\n",
      "Step: 6922, Loss: 0.916019856929779, Accuracy: 1.0, Computation time: 1.4099843502044678\n",
      "Step: 6923, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 0.9122176170349121\n",
      "Step: 6924, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 1.2277429103851318\n",
      "Step: 6925, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 1.5415434837341309\n",
      "Step: 6926, Loss: 0.9375877976417542, Accuracy: 0.96875, Computation time: 1.1067700386047363\n",
      "Step: 6927, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.0518214702606201\n",
      "Step: 6928, Loss: 0.9160062670707703, Accuracy: 1.0, Computation time: 1.255791187286377\n",
      "Step: 6929, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 1.1678051948547363\n",
      "Step: 6930, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.2468457221984863\n",
      "Step: 6931, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.0629596710205078\n",
      "Step: 6932, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.199965000152588\n",
      "Step: 6933, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.149031639099121\n",
      "Step: 6934, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 1.6131951808929443\n",
      "Step: 6935, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.501288890838623\n",
      "Step: 6936, Loss: 0.937415599822998, Accuracy: 0.96875, Computation time: 1.3533337116241455\n",
      "Step: 6937, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.1229233741760254\n",
      "Step: 6938, Loss: 0.9373955130577087, Accuracy: 0.96875, Computation time: 1.2017295360565186\n",
      "Step: 6939, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.1627662181854248\n",
      "Step: 6940, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.3166694641113281\n",
      "Step: 6941, Loss: 0.9374955892562866, Accuracy: 0.96875, Computation time: 1.112438440322876\n",
      "Step: 6942, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.2383956909179688\n",
      "Step: 6943, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 1.704211950302124\n",
      "Step: 6944, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 2.1682140827178955\n",
      "Step: 6945, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.5482375621795654\n",
      "Step: 6946, Loss: 0.9374845623970032, Accuracy: 0.96875, Computation time: 1.2984652519226074\n",
      "Step: 6947, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.320232629776001\n",
      "########################\n",
      "Test loss: 1.1189316511154175, Test Accuracy_epoch50: 0.7080799341201782\n",
      "########################\n",
      "Step: 6948, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.3450510501861572\n",
      "Step: 6949, Loss: 0.9376034140586853, Accuracy: 0.96875, Computation time: 1.5302929878234863\n",
      "Step: 6950, Loss: 0.9375239014625549, Accuracy: 0.96875, Computation time: 1.2498362064361572\n",
      "Step: 6951, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.4406633377075195\n",
      "Step: 6952, Loss: 0.9369052052497864, Accuracy: 0.96875, Computation time: 1.2279136180877686\n",
      "Step: 6953, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.2970623970031738\n",
      "Step: 6954, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.3859517574310303\n",
      "Step: 6955, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.2667262554168701\n",
      "Step: 6956, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.0749187469482422\n",
      "Step: 6957, Loss: 0.9160383939743042, Accuracy: 1.0, Computation time: 1.4131858348846436\n",
      "Step: 6958, Loss: 0.91584312915802, Accuracy: 1.0, Computation time: 1.0505115985870361\n",
      "Step: 6959, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.1540822982788086\n",
      "Step: 6960, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 1.215273380279541\n",
      "Step: 6961, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.6510488986968994\n",
      "Step: 6962, Loss: 0.9374958276748657, Accuracy: 0.96875, Computation time: 1.4463567733764648\n",
      "Step: 6963, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.2229387760162354\n",
      "Step: 6964, Loss: 0.9158400297164917, Accuracy: 1.0, Computation time: 1.187171220779419\n",
      "Step: 6965, Loss: 0.9375107884407043, Accuracy: 0.96875, Computation time: 1.2382266521453857\n",
      "Step: 6966, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.4312939643859863\n",
      "Step: 6967, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 1.096522569656372\n",
      "Step: 6968, Loss: 0.9374645352363586, Accuracy: 0.96875, Computation time: 1.2162556648254395\n",
      "Step: 6969, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.2295658588409424\n",
      "Step: 6970, Loss: 0.9158448576927185, Accuracy: 1.0, Computation time: 1.3736317157745361\n",
      "Step: 6971, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.4907352924346924\n",
      "Step: 6972, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.1986055374145508\n",
      "Step: 6973, Loss: 0.9158364534378052, Accuracy: 1.0, Computation time: 1.0283844470977783\n",
      "Step: 6974, Loss: 0.9158727526664734, Accuracy: 1.0, Computation time: 1.3740761280059814\n",
      "Step: 6975, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.3142163753509521\n",
      "Step: 6976, Loss: 0.9159887433052063, Accuracy: 1.0, Computation time: 1.1876416206359863\n",
      "Step: 6977, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.2941415309906006\n",
      "Step: 6978, Loss: 0.9162195324897766, Accuracy: 1.0, Computation time: 1.328073501586914\n",
      "Step: 6979, Loss: 0.9158361554145813, Accuracy: 1.0, Computation time: 1.0516316890716553\n",
      "Step: 6980, Loss: 0.9375461935997009, Accuracy: 0.96875, Computation time: 1.1287941932678223\n",
      "Step: 6981, Loss: 0.9160641431808472, Accuracy: 1.0, Computation time: 1.343355417251587\n",
      "Step: 6982, Loss: 0.9202983379364014, Accuracy: 1.0, Computation time: 1.3794059753417969\n",
      "Step: 6983, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 1.3821587562561035\n",
      "Step: 6984, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 1.1927211284637451\n",
      "Step: 6985, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.2616841793060303\n",
      "Step: 6986, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.2223474979400635\n",
      "Step: 6987, Loss: 0.9374889731407166, Accuracy: 0.96875, Computation time: 1.4221570491790771\n",
      "Step: 6988, Loss: 0.9375070333480835, Accuracy: 0.96875, Computation time: 1.2568535804748535\n",
      "Step: 6989, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.2356386184692383\n",
      "Step: 6990, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.4751911163330078\n",
      "Step: 6991, Loss: 0.9160925149917603, Accuracy: 1.0, Computation time: 1.713733434677124\n",
      "Step: 6992, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.2112901210784912\n",
      "Step: 6993, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.1085448265075684\n",
      "Step: 6994, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.307018756866455\n",
      "Step: 6995, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.1267967224121094\n",
      "Step: 6996, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.1068577766418457\n",
      "Step: 6997, Loss: 0.9165453910827637, Accuracy: 1.0, Computation time: 1.780238389968872\n",
      "Step: 6998, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.3390777111053467\n",
      "Step: 6999, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.4759604930877686\n",
      "Step: 7000, Loss: 0.9375588893890381, Accuracy: 0.96875, Computation time: 1.2877657413482666\n",
      "Step: 7001, Loss: 0.9295393228530884, Accuracy: 0.96875, Computation time: 1.4977803230285645\n",
      "Step: 7002, Loss: 0.9160142540931702, Accuracy: 1.0, Computation time: 1.2458550930023193\n",
      "Step: 7003, Loss: 0.9375950694084167, Accuracy: 0.96875, Computation time: 1.4966909885406494\n",
      "Step: 7004, Loss: 0.9158875346183777, Accuracy: 1.0, Computation time: 1.3389263153076172\n",
      "Step: 7005, Loss: 0.933058500289917, Accuracy: 0.96875, Computation time: 1.448904275894165\n",
      "Step: 7006, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.1870512962341309\n",
      "Step: 7007, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 1.5942494869232178\n",
      "Step: 7008, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.2757019996643066\n",
      "Step: 7009, Loss: 0.9159939885139465, Accuracy: 1.0, Computation time: 1.2705674171447754\n",
      "Step: 7010, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 1.2481400966644287\n",
      "Step: 7011, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.7652921676635742\n",
      "Step: 7012, Loss: 0.9159969091415405, Accuracy: 1.0, Computation time: 1.5743255615234375\n",
      "Step: 7013, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.3195009231567383\n",
      "Step: 7014, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.439589500427246\n",
      "Step: 7015, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.6266956329345703\n",
      "Step: 7016, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 1.1830604076385498\n",
      "Step: 7017, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.6443760395050049\n",
      "Step: 7018, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.3573355674743652\n",
      "Step: 7019, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.2227058410644531\n",
      "Step: 7020, Loss: 0.9354750514030457, Accuracy: 0.96875, Computation time: 1.3261363506317139\n",
      "Step: 7021, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.2763941287994385\n",
      "Step: 7022, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.297757625579834\n",
      "Step: 7023, Loss: 0.9178563952445984, Accuracy: 1.0, Computation time: 1.2336418628692627\n",
      "Step: 7024, Loss: 0.9250679016113281, Accuracy: 1.0, Computation time: 1.2548580169677734\n",
      "Step: 7025, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.427950382232666\n",
      "Step: 7026, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.4644176959991455\n",
      "Step: 7027, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.5439884662628174\n",
      "Step: 7028, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.7751190662384033\n",
      "Step: 7029, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.3885953426361084\n",
      "Step: 7030, Loss: 0.9371474385261536, Accuracy: 0.96875, Computation time: 2.6785428524017334\n",
      "Step: 7031, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 1.2615525722503662\n",
      "Step: 7032, Loss: 0.9170181751251221, Accuracy: 1.0, Computation time: 1.408299207687378\n",
      "Step: 7033, Loss: 0.9160600900650024, Accuracy: 1.0, Computation time: 1.2579095363616943\n",
      "Step: 7034, Loss: 0.916228711605072, Accuracy: 1.0, Computation time: 1.6922078132629395\n",
      "Step: 7035, Loss: 0.9161525368690491, Accuracy: 1.0, Computation time: 1.586881399154663\n",
      "Step: 7036, Loss: 0.9160327911376953, Accuracy: 1.0, Computation time: 1.4002878665924072\n",
      "Step: 7037, Loss: 0.9165000319480896, Accuracy: 1.0, Computation time: 1.57594633102417\n",
      "Step: 7038, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 1.1413140296936035\n",
      "Step: 7039, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.0520038604736328\n",
      "Step: 7040, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 1.2061219215393066\n",
      "Step: 7041, Loss: 0.9160974025726318, Accuracy: 1.0, Computation time: 1.331590175628662\n",
      "Step: 7042, Loss: 0.9161012768745422, Accuracy: 1.0, Computation time: 1.1865003108978271\n",
      "Step: 7043, Loss: 0.91694176197052, Accuracy: 1.0, Computation time: 1.2182223796844482\n",
      "Step: 7044, Loss: 0.9160958528518677, Accuracy: 1.0, Computation time: 1.4398939609527588\n",
      "Step: 7045, Loss: 0.9159483909606934, Accuracy: 1.0, Computation time: 1.2376015186309814\n",
      "Step: 7046, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.0592074394226074\n",
      "Step: 7047, Loss: 0.9159914255142212, Accuracy: 1.0, Computation time: 1.0749711990356445\n",
      "Step: 7048, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 0.8966124057769775\n",
      "Step: 7049, Loss: 0.9377477169036865, Accuracy: 0.96875, Computation time: 1.17842435836792\n",
      "Step: 7050, Loss: 0.9160531163215637, Accuracy: 1.0, Computation time: 1.2517447471618652\n",
      "Step: 7051, Loss: 0.9318493604660034, Accuracy: 0.96875, Computation time: 1.1447575092315674\n",
      "Step: 7052, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 1.0774023532867432\n",
      "Step: 7053, Loss: 0.9172912836074829, Accuracy: 1.0, Computation time: 2.7302680015563965\n",
      "Step: 7054, Loss: 0.9159204363822937, Accuracy: 1.0, Computation time: 1.1297986507415771\n",
      "Step: 7055, Loss: 0.9375827312469482, Accuracy: 0.96875, Computation time: 1.1084563732147217\n",
      "Step: 7056, Loss: 0.9160294532775879, Accuracy: 1.0, Computation time: 1.111213207244873\n",
      "Step: 7057, Loss: 0.9375988841056824, Accuracy: 0.96875, Computation time: 1.2029337882995605\n",
      "Step: 7058, Loss: 0.9161130785942078, Accuracy: 1.0, Computation time: 1.099463939666748\n",
      "Step: 7059, Loss: 0.9160456657409668, Accuracy: 1.0, Computation time: 1.6655974388122559\n",
      "Step: 7060, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.3168702125549316\n",
      "Step: 7061, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 1.740968942642212\n",
      "Step: 7062, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.5229010581970215\n",
      "Step: 7063, Loss: 0.9158954620361328, Accuracy: 1.0, Computation time: 1.0661520957946777\n",
      "Step: 7064, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.228440761566162\n",
      "Step: 7065, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 1.0637505054473877\n",
      "Step: 7066, Loss: 0.9161258935928345, Accuracy: 1.0, Computation time: 1.2234199047088623\n",
      "Step: 7067, Loss: 0.9158967137336731, Accuracy: 1.0, Computation time: 1.2770581245422363\n",
      "Step: 7068, Loss: 0.9373894333839417, Accuracy: 0.96875, Computation time: 1.0064432621002197\n",
      "Step: 7069, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.0861730575561523\n",
      "Step: 7070, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.344564437866211\n",
      "Step: 7071, Loss: 0.915882408618927, Accuracy: 1.0, Computation time: 1.2456655502319336\n",
      "Step: 7072, Loss: 0.9159756302833557, Accuracy: 1.0, Computation time: 1.1940734386444092\n",
      "Step: 7073, Loss: 0.9176212549209595, Accuracy: 1.0, Computation time: 1.022228479385376\n",
      "Step: 7074, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.2229793071746826\n",
      "Step: 7075, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.9751112461090088\n",
      "Step: 7076, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.099947214126587\n",
      "Step: 7077, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.137864351272583\n",
      "Step: 7078, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 0.9916520118713379\n",
      "Step: 7079, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 0.9814853668212891\n",
      "Step: 7080, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 0.9247636795043945\n",
      "Step: 7081, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.2016174793243408\n",
      "Step: 7082, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.0301930904388428\n",
      "Step: 7083, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 0.8723385334014893\n",
      "Step: 7084, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.043165922164917\n",
      "Step: 7085, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.056539535522461\n",
      "Test loss: 1.1183063983917236, Test Accuracy: 0.706342339515686\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adebef9-5daa-4fa2-96da-3c80a30ea1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
