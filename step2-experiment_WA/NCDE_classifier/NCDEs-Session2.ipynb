{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03dd99a-1b06-4175-808d-ac7ee5794a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7309689-332e-4c97-a7e5-e4704b60fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cb5853-13b0-4421-98a7-01f16a843bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4508, 256, 768) (4508,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last3, wav2vec_last4, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last3,label_last4,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f98d91f7-ca18-4d34-b493-7d199e0dd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec6881a-a7ee-4e45-8f52-9ec0adca387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e86696-9945-4967-b1ed-b5478f8ee866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        #先将数据流降维再放入模型中训练\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0)  \n",
    "            pred_var=prediction.var(axis=0)  \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)     \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1e78bb6-7817-4c06-b522-ba252e479211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c883fb56-167f-4afd-9054-ea8b59e3f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last2], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last2\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945278a5-ea19-4f3c-9071-1efdd710212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445dcd30-7a02-4c29-8b3f-d3600d3a36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a08e7c-9681-488f-b50b-38bf06d2bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4508,\n",
    "    dataset_test_size=1023,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=3234,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        total_acc = 0\n",
    "        total_num = total_size\n",
    "        predicted_class = jnp.argmax(pred, axis=1)\n",
    "        total_acc += jnp.sum(predicted_class == label_i)\n",
    "        return total_acc / total_num\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.array(pred)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d4fe85-4688-446a-9883-69af9c33cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.3735839128494263, Accuracy: 0.34375, Computation time: 12.146655082702637\n",
      "Step: 1, Loss: 1.3841294050216675, Accuracy: 0.28125, Computation time: 1.9635345935821533\n",
      "Step: 2, Loss: 1.3726595640182495, Accuracy: 0.34375, Computation time: 1.64892578125\n",
      "Step: 3, Loss: 1.3718698024749756, Accuracy: 0.34375, Computation time: 1.6442804336547852\n",
      "Step: 4, Loss: 1.4508860111236572, Accuracy: 0.1875, Computation time: 1.5641417503356934\n",
      "Step: 5, Loss: 1.3574392795562744, Accuracy: 0.3125, Computation time: 1.727430820465088\n",
      "Step: 6, Loss: 1.4151957035064697, Accuracy: 0.21875, Computation time: 1.7483091354370117\n",
      "Step: 7, Loss: 1.3437983989715576, Accuracy: 0.375, Computation time: 1.8177804946899414\n",
      "Step: 8, Loss: 1.3317105770111084, Accuracy: 0.375, Computation time: 1.633110761642456\n",
      "Step: 9, Loss: 1.3745570182800293, Accuracy: 0.21875, Computation time: 2.0104403495788574\n",
      "Step: 10, Loss: 1.3631460666656494, Accuracy: 0.4375, Computation time: 1.7557618618011475\n",
      "Step: 11, Loss: 1.3563069105148315, Accuracy: 0.34375, Computation time: 1.6251201629638672\n",
      "Step: 12, Loss: 1.3395047187805176, Accuracy: 0.46875, Computation time: 1.782163143157959\n",
      "Step: 13, Loss: 1.340790033340454, Accuracy: 0.28125, Computation time: 1.643134593963623\n",
      "Step: 14, Loss: 1.4190843105316162, Accuracy: 0.15625, Computation time: 1.683744192123413\n",
      "Step: 15, Loss: 1.3777469396591187, Accuracy: 0.25, Computation time: 1.5990850925445557\n",
      "Step: 16, Loss: 1.3873814344406128, Accuracy: 0.21875, Computation time: 1.6478323936462402\n",
      "Step: 17, Loss: 1.3416786193847656, Accuracy: 0.5, Computation time: 1.7838497161865234\n",
      "Step: 18, Loss: 1.3429137468338013, Accuracy: 0.3125, Computation time: 1.5364456176757812\n",
      "Step: 19, Loss: 1.2947192192077637, Accuracy: 0.5625, Computation time: 1.5449886322021484\n",
      "Step: 20, Loss: 1.3175474405288696, Accuracy: 0.53125, Computation time: 1.4034380912780762\n",
      "Step: 21, Loss: 1.2664092779159546, Accuracy: 0.625, Computation time: 1.5873489379882812\n",
      "Step: 22, Loss: 1.2514227628707886, Accuracy: 0.59375, Computation time: 1.6852562427520752\n",
      "Step: 23, Loss: 1.260236144065857, Accuracy: 0.5, Computation time: 1.416032314300537\n",
      "Step: 24, Loss: 1.3122217655181885, Accuracy: 0.4375, Computation time: 1.3665337562561035\n",
      "Step: 25, Loss: 1.2361047267913818, Accuracy: 0.53125, Computation time: 1.565485954284668\n",
      "Step: 26, Loss: 1.2443175315856934, Accuracy: 0.5, Computation time: 1.4315783977508545\n",
      "Step: 27, Loss: 1.2042373418807983, Accuracy: 0.5625, Computation time: 1.6363046169281006\n",
      "Step: 28, Loss: 1.201107382774353, Accuracy: 0.53125, Computation time: 1.8188636302947998\n",
      "Step: 29, Loss: 1.0811718702316284, Accuracy: 0.71875, Computation time: 1.429365634918213\n",
      "Step: 30, Loss: 1.0790979862213135, Accuracy: 0.84375, Computation time: 1.6725358963012695\n",
      "Step: 31, Loss: 1.0586727857589722, Accuracy: 0.96875, Computation time: 1.2406082153320312\n",
      "Step: 32, Loss: 1.024915337562561, Accuracy: 1.0, Computation time: 1.4582204818725586\n",
      "Step: 33, Loss: 1.0139116048812866, Accuracy: 0.96875, Computation time: 1.2845077514648438\n",
      "Step: 34, Loss: 1.0180916786193848, Accuracy: 0.96875, Computation time: 1.3053810596466064\n",
      "Step: 35, Loss: 1.039082407951355, Accuracy: 0.96875, Computation time: 1.3825197219848633\n",
      "Step: 36, Loss: 1.0439385175704956, Accuracy: 0.9375, Computation time: 1.2901241779327393\n",
      "Step: 37, Loss: 1.0233349800109863, Accuracy: 1.0, Computation time: 1.2852106094360352\n",
      "Step: 38, Loss: 1.0143678188323975, Accuracy: 1.0, Computation time: 1.1225287914276123\n",
      "Step: 39, Loss: 1.0033637285232544, Accuracy: 0.96875, Computation time: 1.1176061630249023\n",
      "Step: 40, Loss: 0.9873425960540771, Accuracy: 1.0, Computation time: 1.3113408088684082\n",
      "Step: 41, Loss: 0.9754310846328735, Accuracy: 1.0, Computation time: 0.9860517978668213\n",
      "Step: 42, Loss: 1.0002264976501465, Accuracy: 0.9375, Computation time: 1.2427711486816406\n",
      "Step: 43, Loss: 0.9526439905166626, Accuracy: 1.0, Computation time: 1.1336991786956787\n",
      "Step: 44, Loss: 0.9688305258750916, Accuracy: 1.0, Computation time: 1.2419137954711914\n",
      "Step: 45, Loss: 0.9722122550010681, Accuracy: 0.96875, Computation time: 1.4101824760437012\n",
      "Step: 46, Loss: 0.948371946811676, Accuracy: 1.0, Computation time: 1.8610949516296387\n",
      "Step: 47, Loss: 0.9429156184196472, Accuracy: 1.0, Computation time: 1.6949608325958252\n",
      "Step: 48, Loss: 0.9554315805435181, Accuracy: 0.96875, Computation time: 1.2881124019622803\n",
      "Step: 49, Loss: 0.9304906129837036, Accuracy: 1.0, Computation time: 1.7691161632537842\n",
      "Step: 50, Loss: 0.9355940818786621, Accuracy: 1.0, Computation time: 1.9542934894561768\n",
      "Step: 51, Loss: 0.9373161196708679, Accuracy: 0.96875, Computation time: 1.3643717765808105\n",
      "Step: 52, Loss: 0.9245401620864868, Accuracy: 1.0, Computation time: 1.2793817520141602\n",
      "Step: 53, Loss: 0.9469953775405884, Accuracy: 0.96875, Computation time: 1.2957191467285156\n",
      "Step: 54, Loss: 0.9213749170303345, Accuracy: 1.0, Computation time: 1.1477386951446533\n",
      "Step: 55, Loss: 0.921017587184906, Accuracy: 1.0, Computation time: 1.0988531112670898\n",
      "Step: 56, Loss: 0.9265434741973877, Accuracy: 1.0, Computation time: 1.171220064163208\n",
      "Step: 57, Loss: 0.9479512572288513, Accuracy: 0.96875, Computation time: 1.2408978939056396\n",
      "Step: 58, Loss: 0.939324140548706, Accuracy: 0.96875, Computation time: 1.3114416599273682\n",
      "Step: 59, Loss: 0.9261294007301331, Accuracy: 1.0, Computation time: 1.4739859104156494\n",
      "Step: 60, Loss: 0.9209132790565491, Accuracy: 1.0, Computation time: 1.519120216369629\n",
      "Step: 61, Loss: 0.9419445395469666, Accuracy: 0.96875, Computation time: 1.4380078315734863\n",
      "Step: 62, Loss: 0.9449718594551086, Accuracy: 1.0, Computation time: 1.982541799545288\n",
      "Step: 63, Loss: 0.936609148979187, Accuracy: 1.0, Computation time: 1.2509169578552246\n",
      "Step: 64, Loss: 0.9199815392494202, Accuracy: 1.0, Computation time: 1.1813983917236328\n",
      "Step: 65, Loss: 0.9197956323623657, Accuracy: 1.0, Computation time: 1.1824281215667725\n",
      "Step: 66, Loss: 0.9194412231445312, Accuracy: 1.0, Computation time: 1.1357412338256836\n",
      "Step: 67, Loss: 0.9204003214836121, Accuracy: 1.0, Computation time: 1.1353230476379395\n",
      "Step: 68, Loss: 0.9191350340843201, Accuracy: 1.0, Computation time: 1.1445362567901611\n",
      "Step: 69, Loss: 0.9663673043251038, Accuracy: 0.9375, Computation time: 1.2533490657806396\n",
      "Step: 70, Loss: 0.9177266359329224, Accuracy: 1.0, Computation time: 1.1704282760620117\n",
      "Step: 71, Loss: 0.9175752401351929, Accuracy: 1.0, Computation time: 1.388185977935791\n",
      "Step: 72, Loss: 0.9329670071601868, Accuracy: 0.96875, Computation time: 1.5767195224761963\n",
      "Step: 73, Loss: 0.9384137988090515, Accuracy: 0.96875, Computation time: 1.4523179531097412\n",
      "Step: 74, Loss: 0.9426669478416443, Accuracy: 0.96875, Computation time: 1.2472281455993652\n",
      "Step: 75, Loss: 0.9339272379875183, Accuracy: 0.96875, Computation time: 1.2188503742218018\n",
      "Step: 76, Loss: 0.919815182685852, Accuracy: 1.0, Computation time: 0.9672536849975586\n",
      "Step: 77, Loss: 0.9172815084457397, Accuracy: 1.0, Computation time: 1.154249668121338\n",
      "Step: 78, Loss: 0.920145571231842, Accuracy: 1.0, Computation time: 1.0702025890350342\n",
      "Step: 79, Loss: 0.9178507924079895, Accuracy: 1.0, Computation time: 1.7077910900115967\n",
      "Step: 80, Loss: 0.9190700054168701, Accuracy: 1.0, Computation time: 1.6857523918151855\n",
      "Step: 81, Loss: 0.9215027093887329, Accuracy: 1.0, Computation time: 1.5242652893066406\n",
      "Step: 82, Loss: 0.9179390668869019, Accuracy: 1.0, Computation time: 1.4434902667999268\n",
      "Step: 83, Loss: 0.9169867634773254, Accuracy: 1.0, Computation time: 1.979905605316162\n",
      "Step: 84, Loss: 0.9215283989906311, Accuracy: 1.0, Computation time: 1.3757641315460205\n",
      "Step: 85, Loss: 0.924808144569397, Accuracy: 1.0, Computation time: 1.4198615550994873\n",
      "Step: 86, Loss: 0.921112596988678, Accuracy: 1.0, Computation time: 1.5258760452270508\n",
      "Step: 87, Loss: 0.918010413646698, Accuracy: 1.0, Computation time: 1.2806708812713623\n",
      "Step: 88, Loss: 0.9203901290893555, Accuracy: 1.0, Computation time: 1.3078277111053467\n",
      "Step: 89, Loss: 0.9188581109046936, Accuracy: 1.0, Computation time: 1.1813721656799316\n",
      "Step: 90, Loss: 0.9184114933013916, Accuracy: 1.0, Computation time: 1.1045410633087158\n",
      "Step: 91, Loss: 0.9296870231628418, Accuracy: 0.96875, Computation time: 1.9646546840667725\n",
      "Step: 92, Loss: 0.9298271536827087, Accuracy: 0.96875, Computation time: 1.2311041355133057\n",
      "Step: 93, Loss: 0.9389888048171997, Accuracy: 0.96875, Computation time: 1.308584213256836\n",
      "Step: 94, Loss: 0.9208583235740662, Accuracy: 1.0, Computation time: 1.0034549236297607\n",
      "Step: 95, Loss: 0.9174643754959106, Accuracy: 1.0, Computation time: 1.326827049255371\n",
      "Step: 96, Loss: 0.9371457695960999, Accuracy: 0.96875, Computation time: 1.0508899688720703\n",
      "Step: 97, Loss: 0.97320556640625, Accuracy: 0.90625, Computation time: 1.1984765529632568\n",
      "Step: 98, Loss: 0.9251203536987305, Accuracy: 1.0, Computation time: 1.2825326919555664\n",
      "Step: 99, Loss: 0.9167296886444092, Accuracy: 1.0, Computation time: 1.2326436042785645\n",
      "Step: 100, Loss: 0.9169149398803711, Accuracy: 1.0, Computation time: 1.4885060787200928\n",
      "Step: 101, Loss: 0.9228370189666748, Accuracy: 1.0, Computation time: 1.2672123908996582\n",
      "Step: 102, Loss: 0.9181455969810486, Accuracy: 1.0, Computation time: 1.7928619384765625\n",
      "Step: 103, Loss: 0.9171330332756042, Accuracy: 1.0, Computation time: 1.3799843788146973\n",
      "Step: 104, Loss: 0.916909396648407, Accuracy: 1.0, Computation time: 1.9292793273925781\n",
      "Step: 105, Loss: 0.9425872564315796, Accuracy: 0.96875, Computation time: 1.2183268070220947\n",
      "Step: 106, Loss: 0.9235750436782837, Accuracy: 1.0, Computation time: 1.1585898399353027\n",
      "Step: 107, Loss: 0.9170926213264465, Accuracy: 1.0, Computation time: 1.1879432201385498\n",
      "Step: 108, Loss: 0.9195316433906555, Accuracy: 1.0, Computation time: 0.9870026111602783\n",
      "Step: 109, Loss: 0.9211786985397339, Accuracy: 1.0, Computation time: 1.4161219596862793\n",
      "Step: 110, Loss: 0.9180223941802979, Accuracy: 1.0, Computation time: 1.2083611488342285\n",
      "Step: 111, Loss: 0.9181974530220032, Accuracy: 1.0, Computation time: 1.2081115245819092\n",
      "Step: 112, Loss: 0.9254310131072998, Accuracy: 1.0, Computation time: 1.3609282970428467\n",
      "Step: 113, Loss: 0.9209957122802734, Accuracy: 1.0, Computation time: 1.2177577018737793\n",
      "Step: 114, Loss: 0.9222533106803894, Accuracy: 1.0, Computation time: 0.9297053813934326\n",
      "Step: 115, Loss: 0.9455543756484985, Accuracy: 0.96875, Computation time: 1.3109636306762695\n",
      "Step: 116, Loss: 0.9173762798309326, Accuracy: 1.0, Computation time: 1.2812657356262207\n",
      "Step: 117, Loss: 0.917910635471344, Accuracy: 1.0, Computation time: 1.5468461513519287\n",
      "Step: 118, Loss: 0.9188450574874878, Accuracy: 1.0, Computation time: 1.648024320602417\n",
      "Step: 119, Loss: 0.9170143008232117, Accuracy: 1.0, Computation time: 1.3143301010131836\n",
      "Step: 120, Loss: 0.9166391491889954, Accuracy: 1.0, Computation time: 1.584129810333252\n",
      "Step: 121, Loss: 0.9356341361999512, Accuracy: 0.96875, Computation time: 1.1696360111236572\n",
      "Step: 122, Loss: 0.9390283823013306, Accuracy: 0.96875, Computation time: 1.0269041061401367\n",
      "Step: 123, Loss: 0.9164490699768066, Accuracy: 1.0, Computation time: 1.3839712142944336\n",
      "Step: 124, Loss: 0.9390559792518616, Accuracy: 0.96875, Computation time: 1.137528419494629\n",
      "Step: 125, Loss: 0.9200780987739563, Accuracy: 1.0, Computation time: 1.0526607036590576\n",
      "Step: 126, Loss: 0.9171319007873535, Accuracy: 1.0, Computation time: 1.2720234394073486\n",
      "Step: 127, Loss: 0.9217729568481445, Accuracy: 1.0, Computation time: 1.171645164489746\n",
      "Step: 128, Loss: 0.9174169301986694, Accuracy: 1.0, Computation time: 1.186112880706787\n",
      "Step: 129, Loss: 0.9300591349601746, Accuracy: 0.96875, Computation time: 1.0069396495819092\n",
      "Step: 130, Loss: 0.916252851486206, Accuracy: 1.0, Computation time: 1.0807688236236572\n",
      "Step: 131, Loss: 0.9166808128356934, Accuracy: 1.0, Computation time: 1.1954951286315918\n",
      "Step: 132, Loss: 0.926004946231842, Accuracy: 1.0, Computation time: 1.0923912525177002\n",
      "Step: 133, Loss: 0.9164520502090454, Accuracy: 1.0, Computation time: 0.968043327331543\n",
      "Step: 134, Loss: 0.9173808097839355, Accuracy: 1.0, Computation time: 1.148177146911621\n",
      "Step: 135, Loss: 0.9174042344093323, Accuracy: 1.0, Computation time: 0.9742386341094971\n",
      "Step: 136, Loss: 0.9314356446266174, Accuracy: 0.96875, Computation time: 1.238468885421753\n",
      "Step: 137, Loss: 0.9256191849708557, Accuracy: 1.0, Computation time: 1.2879266738891602\n",
      "Step: 138, Loss: 0.9170472025871277, Accuracy: 1.0, Computation time: 1.1103401184082031\n",
      "Step: 139, Loss: 0.9172921776771545, Accuracy: 1.0, Computation time: 1.437720537185669\n",
      "########################\n",
      "Test loss: 1.0664174556732178, Test Accuracy_epoch1: 0.7790811061859131\n",
      "########################\n",
      "Step: 140, Loss: 0.9169039726257324, Accuracy: 1.0, Computation time: 1.088294506072998\n",
      "Step: 141, Loss: 0.9175635576248169, Accuracy: 1.0, Computation time: 1.185450792312622\n",
      "Step: 142, Loss: 0.9168070554733276, Accuracy: 1.0, Computation time: 1.1423537731170654\n",
      "Step: 143, Loss: 0.9176562428474426, Accuracy: 1.0, Computation time: 1.0856070518493652\n",
      "Step: 144, Loss: 0.916187047958374, Accuracy: 1.0, Computation time: 1.5213501453399658\n",
      "Step: 145, Loss: 0.9165312051773071, Accuracy: 1.0, Computation time: 1.0087544918060303\n",
      "Step: 146, Loss: 0.918345034122467, Accuracy: 1.0, Computation time: 1.3841919898986816\n",
      "Step: 147, Loss: 0.9178935885429382, Accuracy: 1.0, Computation time: 1.3465754985809326\n",
      "Step: 148, Loss: 0.916194498538971, Accuracy: 1.0, Computation time: 1.6690740585327148\n",
      "Step: 149, Loss: 0.9162960648536682, Accuracy: 1.0, Computation time: 1.435605764389038\n",
      "Step: 150, Loss: 0.9164203405380249, Accuracy: 1.0, Computation time: 1.3163998126983643\n",
      "Step: 151, Loss: 0.9585210084915161, Accuracy: 0.9375, Computation time: 1.1848807334899902\n",
      "Step: 152, Loss: 0.9160776138305664, Accuracy: 1.0, Computation time: 1.094513177871704\n",
      "Step: 153, Loss: 0.9267173409461975, Accuracy: 0.96875, Computation time: 1.7456374168395996\n",
      "Step: 154, Loss: 0.9214716553688049, Accuracy: 1.0, Computation time: 1.7010917663574219\n",
      "Step: 155, Loss: 0.9164823293685913, Accuracy: 1.0, Computation time: 1.1651074886322021\n",
      "Step: 156, Loss: 0.9171719551086426, Accuracy: 1.0, Computation time: 1.1304118633270264\n",
      "Step: 157, Loss: 0.9176733493804932, Accuracy: 1.0, Computation time: 1.0987460613250732\n",
      "Step: 158, Loss: 0.9160445332527161, Accuracy: 1.0, Computation time: 1.3597941398620605\n",
      "Step: 159, Loss: 0.939337432384491, Accuracy: 0.96875, Computation time: 1.2569506168365479\n",
      "Step: 160, Loss: 0.9171273708343506, Accuracy: 1.0, Computation time: 1.2235820293426514\n",
      "Step: 161, Loss: 0.9380072951316833, Accuracy: 0.96875, Computation time: 1.399129867553711\n",
      "Step: 162, Loss: 0.9173318147659302, Accuracy: 1.0, Computation time: 1.3067502975463867\n",
      "Step: 163, Loss: 0.9586601257324219, Accuracy: 0.9375, Computation time: 1.1384308338165283\n",
      "Step: 164, Loss: 0.9232532978057861, Accuracy: 1.0, Computation time: 0.9950203895568848\n",
      "Step: 165, Loss: 0.9163074493408203, Accuracy: 1.0, Computation time: 1.0396287441253662\n",
      "Step: 166, Loss: 0.916155993938446, Accuracy: 1.0, Computation time: 1.184373140335083\n",
      "Step: 167, Loss: 0.9221319556236267, Accuracy: 1.0, Computation time: 1.679081678390503\n",
      "Step: 168, Loss: 0.9165027737617493, Accuracy: 1.0, Computation time: 1.0063459873199463\n",
      "Step: 169, Loss: 0.9167233109474182, Accuracy: 1.0, Computation time: 1.3700921535491943\n",
      "Step: 170, Loss: 0.916412353515625, Accuracy: 1.0, Computation time: 1.4231679439544678\n",
      "Step: 171, Loss: 0.9166243076324463, Accuracy: 1.0, Computation time: 0.9950182437896729\n",
      "Step: 172, Loss: 0.9164857864379883, Accuracy: 1.0, Computation time: 1.3302147388458252\n",
      "Step: 173, Loss: 0.9162396192550659, Accuracy: 1.0, Computation time: 1.3735463619232178\n",
      "Step: 174, Loss: 0.928511917591095, Accuracy: 1.0, Computation time: 1.1193575859069824\n",
      "Step: 175, Loss: 0.9183262586593628, Accuracy: 1.0, Computation time: 1.7744712829589844\n",
      "Step: 176, Loss: 0.9378722310066223, Accuracy: 0.96875, Computation time: 1.365368366241455\n",
      "Step: 177, Loss: 0.9353577494621277, Accuracy: 0.96875, Computation time: 1.2552237510681152\n",
      "Step: 178, Loss: 0.9313989877700806, Accuracy: 0.96875, Computation time: 1.1569433212280273\n",
      "Step: 179, Loss: 0.937960147857666, Accuracy: 0.96875, Computation time: 1.0602412223815918\n",
      "Step: 180, Loss: 0.9168020486831665, Accuracy: 1.0, Computation time: 1.1679344177246094\n",
      "Step: 181, Loss: 0.9168627262115479, Accuracy: 1.0, Computation time: 1.570981502532959\n",
      "Step: 182, Loss: 0.9178192615509033, Accuracy: 1.0, Computation time: 0.9730584621429443\n",
      "Step: 183, Loss: 0.9162313342094421, Accuracy: 1.0, Computation time: 1.2729828357696533\n",
      "Step: 184, Loss: 0.9163256287574768, Accuracy: 1.0, Computation time: 0.9436378479003906\n",
      "Step: 185, Loss: 0.9191402196884155, Accuracy: 1.0, Computation time: 1.2897660732269287\n",
      "Step: 186, Loss: 0.9166033864021301, Accuracy: 1.0, Computation time: 1.5044069290161133\n",
      "Step: 187, Loss: 0.926460325717926, Accuracy: 1.0, Computation time: 1.2013933658599854\n",
      "Step: 188, Loss: 0.9161959290504456, Accuracy: 1.0, Computation time: 1.0491976737976074\n",
      "Step: 189, Loss: 0.9182018637657166, Accuracy: 1.0, Computation time: 0.9240071773529053\n",
      "Step: 190, Loss: 0.9164839386940002, Accuracy: 1.0, Computation time: 1.1078979969024658\n",
      "Step: 191, Loss: 0.917120099067688, Accuracy: 1.0, Computation time: 1.3528037071228027\n",
      "Step: 192, Loss: 0.9161171913146973, Accuracy: 1.0, Computation time: 1.3165147304534912\n",
      "Step: 193, Loss: 0.9162427186965942, Accuracy: 1.0, Computation time: 0.971163272857666\n",
      "Step: 194, Loss: 0.9168338179588318, Accuracy: 1.0, Computation time: 1.2892138957977295\n",
      "Step: 195, Loss: 0.9192562103271484, Accuracy: 1.0, Computation time: 1.1277966499328613\n",
      "Step: 196, Loss: 0.9160434603691101, Accuracy: 1.0, Computation time: 1.092649221420288\n",
      "Step: 197, Loss: 0.9161867499351501, Accuracy: 1.0, Computation time: 1.1291289329528809\n",
      "Step: 198, Loss: 0.9161182045936584, Accuracy: 1.0, Computation time: 1.3822269439697266\n",
      "Step: 199, Loss: 0.9160529375076294, Accuracy: 1.0, Computation time: 1.1543614864349365\n",
      "Step: 200, Loss: 0.9161381125450134, Accuracy: 1.0, Computation time: 1.3379833698272705\n",
      "Step: 201, Loss: 0.9161484837532043, Accuracy: 1.0, Computation time: 1.5460841655731201\n",
      "Step: 202, Loss: 0.9306291937828064, Accuracy: 0.96875, Computation time: 1.5060515403747559\n",
      "Step: 203, Loss: 0.9380714297294617, Accuracy: 0.96875, Computation time: 1.0223941802978516\n",
      "Step: 204, Loss: 0.9426615834236145, Accuracy: 0.96875, Computation time: 1.183720350265503\n",
      "Step: 205, Loss: 0.9188874363899231, Accuracy: 1.0, Computation time: 1.126206636428833\n",
      "Step: 206, Loss: 0.9162709712982178, Accuracy: 1.0, Computation time: 1.3559441566467285\n",
      "Step: 207, Loss: 0.9167720675468445, Accuracy: 1.0, Computation time: 1.0558743476867676\n",
      "Step: 208, Loss: 0.9164089560508728, Accuracy: 1.0, Computation time: 1.2535741329193115\n",
      "Step: 209, Loss: 0.9166538119316101, Accuracy: 1.0, Computation time: 1.3064076900482178\n",
      "Step: 210, Loss: 0.9176040291786194, Accuracy: 1.0, Computation time: 1.1272540092468262\n",
      "Step: 211, Loss: 0.916134238243103, Accuracy: 1.0, Computation time: 1.1602449417114258\n",
      "Step: 212, Loss: 0.916256308555603, Accuracy: 1.0, Computation time: 1.2442827224731445\n",
      "Step: 213, Loss: 0.9367262125015259, Accuracy: 0.96875, Computation time: 1.3909106254577637\n",
      "Step: 214, Loss: 0.9373899102210999, Accuracy: 0.96875, Computation time: 1.1619195938110352\n",
      "Step: 215, Loss: 0.9231405854225159, Accuracy: 1.0, Computation time: 1.0886261463165283\n",
      "Step: 216, Loss: 0.9213672280311584, Accuracy: 1.0, Computation time: 1.481203317642212\n",
      "Step: 217, Loss: 0.9165055751800537, Accuracy: 1.0, Computation time: 1.1765952110290527\n",
      "Step: 218, Loss: 0.928716778755188, Accuracy: 0.96875, Computation time: 1.4535915851593018\n",
      "Step: 219, Loss: 0.9182701706886292, Accuracy: 1.0, Computation time: 1.3353633880615234\n",
      "Step: 220, Loss: 0.9419790506362915, Accuracy: 0.96875, Computation time: 1.147731065750122\n",
      "Step: 221, Loss: 0.937308132648468, Accuracy: 0.96875, Computation time: 1.0397248268127441\n",
      "Step: 222, Loss: 0.9172800779342651, Accuracy: 1.0, Computation time: 1.026425838470459\n",
      "Step: 223, Loss: 0.9367222785949707, Accuracy: 0.96875, Computation time: 1.0992932319641113\n",
      "Step: 224, Loss: 0.9184759259223938, Accuracy: 1.0, Computation time: 1.356879472732544\n",
      "Step: 225, Loss: 0.9164243340492249, Accuracy: 1.0, Computation time: 1.519144058227539\n",
      "Step: 226, Loss: 0.9162195920944214, Accuracy: 1.0, Computation time: 1.2225489616394043\n",
      "Step: 227, Loss: 0.9163611531257629, Accuracy: 1.0, Computation time: 1.0752546787261963\n",
      "Step: 228, Loss: 0.9161490201950073, Accuracy: 1.0, Computation time: 1.0216236114501953\n",
      "Step: 229, Loss: 0.9165124893188477, Accuracy: 1.0, Computation time: 0.9837436676025391\n",
      "Step: 230, Loss: 0.9228028059005737, Accuracy: 1.0, Computation time: 1.4693799018859863\n",
      "Step: 231, Loss: 0.9347921013832092, Accuracy: 0.96875, Computation time: 1.3991246223449707\n",
      "Step: 232, Loss: 0.9279310703277588, Accuracy: 0.96875, Computation time: 1.537471055984497\n",
      "Step: 233, Loss: 0.9183964729309082, Accuracy: 1.0, Computation time: 1.468017339706421\n",
      "Step: 234, Loss: 0.927840530872345, Accuracy: 0.96875, Computation time: 1.4740207195281982\n",
      "Step: 235, Loss: 0.9186878204345703, Accuracy: 1.0, Computation time: 1.342477798461914\n",
      "Step: 236, Loss: 0.9164839386940002, Accuracy: 1.0, Computation time: 1.0788373947143555\n",
      "Step: 237, Loss: 0.9179900288581848, Accuracy: 1.0, Computation time: 1.094029426574707\n",
      "Step: 238, Loss: 0.9190162420272827, Accuracy: 1.0, Computation time: 1.157846450805664\n",
      "Step: 239, Loss: 0.917937159538269, Accuracy: 1.0, Computation time: 1.214369297027588\n",
      "Step: 240, Loss: 0.9176450967788696, Accuracy: 1.0, Computation time: 1.446000576019287\n",
      "Step: 241, Loss: 0.9167664647102356, Accuracy: 1.0, Computation time: 1.2530107498168945\n",
      "Step: 242, Loss: 0.9380365610122681, Accuracy: 0.96875, Computation time: 1.032715082168579\n",
      "Step: 243, Loss: 0.9164483547210693, Accuracy: 1.0, Computation time: 1.2838420867919922\n",
      "Step: 244, Loss: 0.9166244864463806, Accuracy: 1.0, Computation time: 1.0308444499969482\n",
      "Step: 245, Loss: 0.9167793989181519, Accuracy: 1.0, Computation time: 1.0793476104736328\n",
      "Step: 246, Loss: 0.9162712693214417, Accuracy: 1.0, Computation time: 1.177008867263794\n",
      "Step: 247, Loss: 0.9163166880607605, Accuracy: 1.0, Computation time: 1.1907927989959717\n",
      "Step: 248, Loss: 0.9177584648132324, Accuracy: 1.0, Computation time: 1.1030635833740234\n",
      "Step: 249, Loss: 0.9229801893234253, Accuracy: 1.0, Computation time: 1.0969367027282715\n",
      "Step: 250, Loss: 0.9175011515617371, Accuracy: 1.0, Computation time: 1.3812618255615234\n",
      "Step: 251, Loss: 0.9196670055389404, Accuracy: 1.0, Computation time: 1.3423399925231934\n",
      "Step: 252, Loss: 0.9347291588783264, Accuracy: 0.96875, Computation time: 1.7674229145050049\n",
      "Step: 253, Loss: 0.9164100885391235, Accuracy: 1.0, Computation time: 1.073493480682373\n",
      "Step: 254, Loss: 0.9217415452003479, Accuracy: 1.0, Computation time: 1.1099512577056885\n",
      "Step: 255, Loss: 0.9166616797447205, Accuracy: 1.0, Computation time: 0.8932712078094482\n",
      "Step: 256, Loss: 0.9378811717033386, Accuracy: 0.96875, Computation time: 1.3199667930603027\n",
      "Step: 257, Loss: 0.9263930320739746, Accuracy: 0.96875, Computation time: 0.9668247699737549\n",
      "Step: 258, Loss: 0.9162333011627197, Accuracy: 1.0, Computation time: 1.3401684761047363\n",
      "Step: 259, Loss: 0.9351811408996582, Accuracy: 0.96875, Computation time: 1.3724358081817627\n",
      "Step: 260, Loss: 0.9189916849136353, Accuracy: 1.0, Computation time: 1.3148586750030518\n",
      "Step: 261, Loss: 0.9163234233856201, Accuracy: 1.0, Computation time: 1.0322303771972656\n",
      "Step: 262, Loss: 0.9383891820907593, Accuracy: 0.96875, Computation time: 1.1399400234222412\n",
      "Step: 263, Loss: 0.917087972164154, Accuracy: 1.0, Computation time: 1.4035727977752686\n",
      "Step: 264, Loss: 0.9391180872917175, Accuracy: 0.96875, Computation time: 1.1446545124053955\n",
      "Step: 265, Loss: 0.9161571860313416, Accuracy: 1.0, Computation time: 1.6777403354644775\n",
      "Step: 266, Loss: 0.9373956918716431, Accuracy: 0.96875, Computation time: 1.2523927688598633\n",
      "Step: 267, Loss: 0.9160481095314026, Accuracy: 1.0, Computation time: 1.1162371635437012\n",
      "Step: 268, Loss: 0.9166358113288879, Accuracy: 1.0, Computation time: 1.1214077472686768\n",
      "Step: 269, Loss: 0.9300099015235901, Accuracy: 0.96875, Computation time: 1.1464564800262451\n",
      "Step: 270, Loss: 0.9178510904312134, Accuracy: 1.0, Computation time: 1.0738039016723633\n",
      "Step: 271, Loss: 0.9162667393684387, Accuracy: 1.0, Computation time: 1.4917652606964111\n",
      "Step: 272, Loss: 0.916724681854248, Accuracy: 1.0, Computation time: 1.2128229141235352\n",
      "Step: 273, Loss: 0.9373699426651001, Accuracy: 0.96875, Computation time: 1.2431120872497559\n",
      "Step: 274, Loss: 0.9178497195243835, Accuracy: 1.0, Computation time: 1.3961427211761475\n",
      "Step: 275, Loss: 0.9161710739135742, Accuracy: 1.0, Computation time: 1.3314497470855713\n",
      "Step: 276, Loss: 0.9182976484298706, Accuracy: 1.0, Computation time: 1.0528671741485596\n",
      "Step: 277, Loss: 0.9170539975166321, Accuracy: 1.0, Computation time: 0.9431412220001221\n",
      "Step: 278, Loss: 0.9162299633026123, Accuracy: 1.0, Computation time: 0.9287810325622559\n",
      "########################\n",
      "Test loss: 1.0689436197280884, Test Accuracy_epoch2: 0.769305944442749\n",
      "########################\n",
      "Step: 279, Loss: 0.9336010813713074, Accuracy: 0.96875, Computation time: 1.2416877746582031\n",
      "Step: 280, Loss: 0.916233241558075, Accuracy: 1.0, Computation time: 1.4888570308685303\n",
      "Step: 281, Loss: 0.916385293006897, Accuracy: 1.0, Computation time: 1.1225202083587646\n",
      "Step: 282, Loss: 0.9167367815971375, Accuracy: 1.0, Computation time: 1.2710285186767578\n",
      "Step: 283, Loss: 0.9160318374633789, Accuracy: 1.0, Computation time: 1.0002343654632568\n",
      "Step: 284, Loss: 0.916165292263031, Accuracy: 1.0, Computation time: 1.0770273208618164\n",
      "Step: 285, Loss: 0.9165434837341309, Accuracy: 1.0, Computation time: 1.3171098232269287\n",
      "Step: 286, Loss: 0.9393901228904724, Accuracy: 0.96875, Computation time: 1.3752822875976562\n",
      "Step: 287, Loss: 0.9162541627883911, Accuracy: 1.0, Computation time: 1.1022040843963623\n",
      "Step: 288, Loss: 0.9162375926971436, Accuracy: 1.0, Computation time: 1.042104721069336\n",
      "Step: 289, Loss: 0.9161515235900879, Accuracy: 1.0, Computation time: 1.0147709846496582\n",
      "Step: 290, Loss: 0.9162493348121643, Accuracy: 1.0, Computation time: 1.12479829788208\n",
      "Step: 291, Loss: 0.9160202741622925, Accuracy: 1.0, Computation time: 1.1690473556518555\n",
      "Step: 292, Loss: 0.9180607199668884, Accuracy: 1.0, Computation time: 1.133474588394165\n",
      "Step: 293, Loss: 0.916305422782898, Accuracy: 1.0, Computation time: 0.9177031517028809\n",
      "Step: 294, Loss: 0.9160412549972534, Accuracy: 1.0, Computation time: 0.965815544128418\n",
      "Step: 295, Loss: 0.9170691967010498, Accuracy: 1.0, Computation time: 1.1117501258850098\n",
      "Step: 296, Loss: 0.9201829433441162, Accuracy: 1.0, Computation time: 1.413595199584961\n",
      "Step: 297, Loss: 0.9208095669746399, Accuracy: 1.0, Computation time: 1.1393978595733643\n",
      "Step: 298, Loss: 0.9166370034217834, Accuracy: 1.0, Computation time: 1.209562063217163\n",
      "Step: 299, Loss: 0.9161292314529419, Accuracy: 1.0, Computation time: 1.0960464477539062\n",
      "Step: 300, Loss: 0.9164561629295349, Accuracy: 1.0, Computation time: 1.1360676288604736\n",
      "Step: 301, Loss: 0.9164097905158997, Accuracy: 1.0, Computation time: 0.9504444599151611\n",
      "Step: 302, Loss: 0.9395142197608948, Accuracy: 0.96875, Computation time: 1.0959203243255615\n",
      "Step: 303, Loss: 0.9166784882545471, Accuracy: 1.0, Computation time: 1.4010851383209229\n",
      "Step: 304, Loss: 0.9177603721618652, Accuracy: 1.0, Computation time: 1.3080096244812012\n",
      "Step: 305, Loss: 0.9191033244132996, Accuracy: 1.0, Computation time: 1.0334954261779785\n",
      "Step: 306, Loss: 0.9161515235900879, Accuracy: 1.0, Computation time: 0.9932942390441895\n",
      "Step: 307, Loss: 0.9184703230857849, Accuracy: 1.0, Computation time: 1.9061768054962158\n",
      "Step: 308, Loss: 0.91667640209198, Accuracy: 1.0, Computation time: 0.9699962139129639\n",
      "Step: 309, Loss: 0.9162191152572632, Accuracy: 1.0, Computation time: 0.9971530437469482\n",
      "Step: 310, Loss: 0.9238361716270447, Accuracy: 1.0, Computation time: 1.1901514530181885\n",
      "Step: 311, Loss: 0.9377676844596863, Accuracy: 0.96875, Computation time: 1.0254838466644287\n",
      "Step: 312, Loss: 0.9283497333526611, Accuracy: 0.96875, Computation time: 1.16725492477417\n",
      "Step: 313, Loss: 0.924168050289154, Accuracy: 1.0, Computation time: 1.437138319015503\n",
      "Step: 314, Loss: 0.9200302958488464, Accuracy: 1.0, Computation time: 0.9590399265289307\n",
      "Step: 315, Loss: 0.9162245392799377, Accuracy: 1.0, Computation time: 1.1695010662078857\n",
      "Step: 316, Loss: 0.9587666988372803, Accuracy: 0.9375, Computation time: 1.1053969860076904\n",
      "Step: 317, Loss: 0.9176844358444214, Accuracy: 1.0, Computation time: 1.1180775165557861\n",
      "Step: 318, Loss: 0.9184436202049255, Accuracy: 1.0, Computation time: 1.1160855293273926\n",
      "Step: 319, Loss: 0.9166216254234314, Accuracy: 1.0, Computation time: 0.9433016777038574\n",
      "Step: 320, Loss: 0.9163780808448792, Accuracy: 1.0, Computation time: 0.8601059913635254\n",
      "Step: 321, Loss: 0.9378442168235779, Accuracy: 0.96875, Computation time: 1.4644315242767334\n",
      "Step: 322, Loss: 0.9314171671867371, Accuracy: 0.96875, Computation time: 1.0452423095703125\n",
      "Step: 323, Loss: 0.9166644811630249, Accuracy: 1.0, Computation time: 0.9862105846405029\n",
      "Step: 324, Loss: 0.9225648641586304, Accuracy: 1.0, Computation time: 1.0781810283660889\n",
      "Step: 325, Loss: 0.9180406332015991, Accuracy: 1.0, Computation time: 0.9407041072845459\n",
      "Step: 326, Loss: 0.9286220073699951, Accuracy: 0.96875, Computation time: 1.2831730842590332\n",
      "Step: 327, Loss: 0.916512668132782, Accuracy: 1.0, Computation time: 0.9832038879394531\n",
      "Step: 328, Loss: 0.9161645770072937, Accuracy: 1.0, Computation time: 1.256782054901123\n",
      "Step: 329, Loss: 0.916148841381073, Accuracy: 1.0, Computation time: 1.001007318496704\n",
      "Step: 330, Loss: 0.9363941550254822, Accuracy: 0.96875, Computation time: 1.073174238204956\n",
      "Step: 331, Loss: 0.9170099496841431, Accuracy: 1.0, Computation time: 1.1320281028747559\n",
      "Step: 332, Loss: 0.9167246222496033, Accuracy: 1.0, Computation time: 0.9760096073150635\n",
      "Step: 333, Loss: 0.9170151352882385, Accuracy: 1.0, Computation time: 1.416003704071045\n",
      "Step: 334, Loss: 0.916596531867981, Accuracy: 1.0, Computation time: 1.1503069400787354\n",
      "Step: 335, Loss: 0.9161365628242493, Accuracy: 1.0, Computation time: 1.0276405811309814\n",
      "Step: 336, Loss: 0.9369891881942749, Accuracy: 0.96875, Computation time: 1.2151179313659668\n",
      "Step: 337, Loss: 0.9162125587463379, Accuracy: 1.0, Computation time: 0.9444880485534668\n",
      "Step: 338, Loss: 0.9359755516052246, Accuracy: 0.96875, Computation time: 1.0294253826141357\n",
      "Step: 339, Loss: 0.9165027737617493, Accuracy: 1.0, Computation time: 0.9875550270080566\n",
      "Step: 340, Loss: 0.9164861440658569, Accuracy: 1.0, Computation time: 1.144463300704956\n",
      "Step: 341, Loss: 0.9160376191139221, Accuracy: 1.0, Computation time: 1.2518157958984375\n",
      "Step: 342, Loss: 0.9162047505378723, Accuracy: 1.0, Computation time: 1.226635217666626\n",
      "Step: 343, Loss: 0.9172067642211914, Accuracy: 1.0, Computation time: 1.0344188213348389\n",
      "Step: 344, Loss: 0.9162015914916992, Accuracy: 1.0, Computation time: 1.194883108139038\n",
      "Step: 345, Loss: 0.9161839485168457, Accuracy: 1.0, Computation time: 1.3243482112884521\n",
      "Step: 346, Loss: 0.9355521202087402, Accuracy: 0.96875, Computation time: 1.1880443096160889\n",
      "Step: 347, Loss: 0.916056215763092, Accuracy: 1.0, Computation time: 1.1018881797790527\n",
      "Step: 348, Loss: 0.9562669396400452, Accuracy: 0.9375, Computation time: 1.52524995803833\n",
      "Step: 349, Loss: 0.9160059094429016, Accuracy: 1.0, Computation time: 1.1957645416259766\n",
      "Step: 350, Loss: 0.9328231811523438, Accuracy: 0.96875, Computation time: 1.6234865188598633\n",
      "Step: 351, Loss: 0.9161221385002136, Accuracy: 1.0, Computation time: 1.3477027416229248\n",
      "Step: 352, Loss: 0.9163874387741089, Accuracy: 1.0, Computation time: 1.014333963394165\n",
      "Step: 353, Loss: 0.9159930944442749, Accuracy: 1.0, Computation time: 0.9538846015930176\n",
      "Step: 354, Loss: 0.9284339547157288, Accuracy: 0.96875, Computation time: 1.3602385520935059\n",
      "Step: 355, Loss: 0.9160424470901489, Accuracy: 1.0, Computation time: 1.0965323448181152\n",
      "Step: 356, Loss: 0.9248307943344116, Accuracy: 1.0, Computation time: 1.1541998386383057\n",
      "Step: 357, Loss: 0.9165729880332947, Accuracy: 1.0, Computation time: 1.3288040161132812\n",
      "Step: 358, Loss: 0.9164547920227051, Accuracy: 1.0, Computation time: 1.1214778423309326\n",
      "Step: 359, Loss: 0.9163756966590881, Accuracy: 1.0, Computation time: 1.3434789180755615\n",
      "Step: 360, Loss: 0.9162501692771912, Accuracy: 1.0, Computation time: 1.460913896560669\n",
      "Step: 361, Loss: 0.9283839464187622, Accuracy: 0.96875, Computation time: 1.1769959926605225\n",
      "Step: 362, Loss: 0.9384315013885498, Accuracy: 0.96875, Computation time: 1.1848976612091064\n",
      "Step: 363, Loss: 0.9254134893417358, Accuracy: 1.0, Computation time: 1.6451799869537354\n",
      "Step: 364, Loss: 0.9161782264709473, Accuracy: 1.0, Computation time: 1.3602066040039062\n",
      "Step: 365, Loss: 0.9236193895339966, Accuracy: 1.0, Computation time: 1.1682591438293457\n",
      "Step: 366, Loss: 0.9188868999481201, Accuracy: 1.0, Computation time: 1.2835402488708496\n",
      "Step: 367, Loss: 0.9166436195373535, Accuracy: 1.0, Computation time: 1.2392034530639648\n",
      "Step: 368, Loss: 0.9395288825035095, Accuracy: 0.96875, Computation time: 1.0353467464447021\n",
      "Step: 369, Loss: 0.9165710806846619, Accuracy: 1.0, Computation time: 1.1380341053009033\n",
      "Step: 370, Loss: 0.9389185905456543, Accuracy: 0.96875, Computation time: 1.0050034523010254\n",
      "Step: 371, Loss: 0.9405186772346497, Accuracy: 0.96875, Computation time: 0.9991281032562256\n",
      "Step: 372, Loss: 0.9165083169937134, Accuracy: 1.0, Computation time: 0.9948484897613525\n",
      "Step: 373, Loss: 0.9677280783653259, Accuracy: 0.90625, Computation time: 0.9650576114654541\n",
      "Step: 374, Loss: 0.9160453081130981, Accuracy: 1.0, Computation time: 1.197213888168335\n",
      "Step: 375, Loss: 0.938484787940979, Accuracy: 0.96875, Computation time: 0.978428840637207\n",
      "Step: 376, Loss: 0.9160304069519043, Accuracy: 1.0, Computation time: 1.800701379776001\n",
      "Step: 377, Loss: 0.916547417640686, Accuracy: 1.0, Computation time: 1.168457269668579\n",
      "Step: 378, Loss: 0.9165703654289246, Accuracy: 1.0, Computation time: 1.1311805248260498\n",
      "Step: 379, Loss: 0.9162278175354004, Accuracy: 1.0, Computation time: 1.2119994163513184\n",
      "Step: 380, Loss: 0.938630223274231, Accuracy: 0.96875, Computation time: 1.0061204433441162\n",
      "Step: 381, Loss: 0.9162448644638062, Accuracy: 1.0, Computation time: 1.2485709190368652\n",
      "Step: 382, Loss: 0.9163006544113159, Accuracy: 1.0, Computation time: 1.0473623275756836\n",
      "Step: 383, Loss: 0.9161592721939087, Accuracy: 1.0, Computation time: 1.1675310134887695\n",
      "Step: 384, Loss: 0.9376505613327026, Accuracy: 0.96875, Computation time: 1.1457154750823975\n",
      "Step: 385, Loss: 0.9419641494750977, Accuracy: 0.96875, Computation time: 1.5785160064697266\n",
      "Step: 386, Loss: 0.9176864624023438, Accuracy: 1.0, Computation time: 1.020310640335083\n",
      "Step: 387, Loss: 0.9260736703872681, Accuracy: 0.96875, Computation time: 1.3084261417388916\n",
      "Step: 388, Loss: 0.9176272749900818, Accuracy: 1.0, Computation time: 1.1865355968475342\n",
      "Step: 389, Loss: 0.9225636720657349, Accuracy: 1.0, Computation time: 0.9957876205444336\n",
      "Step: 390, Loss: 0.9164934158325195, Accuracy: 1.0, Computation time: 1.2840557098388672\n",
      "Step: 391, Loss: 0.9187508225440979, Accuracy: 1.0, Computation time: 1.3583602905273438\n",
      "Step: 392, Loss: 0.9165971875190735, Accuracy: 1.0, Computation time: 1.0302419662475586\n",
      "Step: 393, Loss: 0.9163137078285217, Accuracy: 1.0, Computation time: 1.034668207168579\n",
      "Step: 394, Loss: 0.9166712164878845, Accuracy: 1.0, Computation time: 1.027517557144165\n",
      "Step: 395, Loss: 0.9183366298675537, Accuracy: 1.0, Computation time: 1.1985342502593994\n",
      "Step: 396, Loss: 0.9168490767478943, Accuracy: 1.0, Computation time: 1.1224334239959717\n",
      "Step: 397, Loss: 0.916139543056488, Accuracy: 1.0, Computation time: 1.43745756149292\n",
      "Step: 398, Loss: 0.936029314994812, Accuracy: 0.96875, Computation time: 1.2300922870635986\n",
      "Step: 399, Loss: 0.9161292910575867, Accuracy: 1.0, Computation time: 1.3348491191864014\n",
      "Step: 400, Loss: 0.9279054403305054, Accuracy: 0.96875, Computation time: 1.103980541229248\n",
      "Step: 401, Loss: 0.9161099791526794, Accuracy: 1.0, Computation time: 0.9519522190093994\n",
      "Step: 402, Loss: 0.9160510897636414, Accuracy: 1.0, Computation time: 1.025813341140747\n",
      "Step: 403, Loss: 0.9160948991775513, Accuracy: 1.0, Computation time: 1.211240291595459\n",
      "Step: 404, Loss: 0.9381678700447083, Accuracy: 0.96875, Computation time: 1.2229740619659424\n",
      "Step: 405, Loss: 0.9163296818733215, Accuracy: 1.0, Computation time: 1.2514972686767578\n",
      "Step: 406, Loss: 0.9162315130233765, Accuracy: 1.0, Computation time: 1.323697566986084\n",
      "Step: 407, Loss: 0.9376689195632935, Accuracy: 0.96875, Computation time: 1.680464267730713\n",
      "Step: 408, Loss: 0.9165328741073608, Accuracy: 1.0, Computation time: 1.458855390548706\n",
      "Step: 409, Loss: 0.9162617921829224, Accuracy: 1.0, Computation time: 1.300245761871338\n",
      "Step: 410, Loss: 0.9163618683815002, Accuracy: 1.0, Computation time: 1.3016483783721924\n",
      "Step: 411, Loss: 0.9253761768341064, Accuracy: 1.0, Computation time: 1.042262315750122\n",
      "Step: 412, Loss: 0.916147768497467, Accuracy: 1.0, Computation time: 1.170076608657837\n",
      "Step: 413, Loss: 0.9478616714477539, Accuracy: 0.9375, Computation time: 1.4199111461639404\n",
      "Step: 414, Loss: 0.9322865009307861, Accuracy: 0.96875, Computation time: 1.0356457233428955\n",
      "Step: 415, Loss: 0.9161602258682251, Accuracy: 1.0, Computation time: 1.4742562770843506\n",
      "Step: 416, Loss: 0.9180484414100647, Accuracy: 1.0, Computation time: 1.2251770496368408\n",
      "Step: 417, Loss: 0.9170109629631042, Accuracy: 1.0, Computation time: 1.3444280624389648\n",
      "########################\n",
      "Test loss: 1.066329002380371, Test Accuracy_epoch3: 0.7790811061859131\n",
      "########################\n",
      "Step: 418, Loss: 0.9160646796226501, Accuracy: 1.0, Computation time: 1.137218713760376\n",
      "Step: 419, Loss: 0.916053831577301, Accuracy: 1.0, Computation time: 1.2326347827911377\n",
      "Step: 420, Loss: 0.9161691665649414, Accuracy: 1.0, Computation time: 1.1027588844299316\n",
      "Step: 421, Loss: 0.9166538119316101, Accuracy: 1.0, Computation time: 1.4398412704467773\n",
      "Step: 422, Loss: 0.9161256551742554, Accuracy: 1.0, Computation time: 0.901984453201294\n",
      "Step: 423, Loss: 0.9162745475769043, Accuracy: 1.0, Computation time: 1.588244915008545\n",
      "Step: 424, Loss: 0.9161083102226257, Accuracy: 1.0, Computation time: 1.2423861026763916\n",
      "Step: 425, Loss: 0.9160580635070801, Accuracy: 1.0, Computation time: 1.2081358432769775\n",
      "Step: 426, Loss: 0.9390998482704163, Accuracy: 0.96875, Computation time: 1.1909773349761963\n",
      "Step: 427, Loss: 0.9160456657409668, Accuracy: 1.0, Computation time: 1.3687784671783447\n",
      "Step: 428, Loss: 0.9373703002929688, Accuracy: 0.96875, Computation time: 1.587550401687622\n",
      "Step: 429, Loss: 0.9175453782081604, Accuracy: 1.0, Computation time: 1.2350127696990967\n",
      "Step: 430, Loss: 0.9162189960479736, Accuracy: 1.0, Computation time: 1.32269287109375\n",
      "Step: 431, Loss: 0.9178645014762878, Accuracy: 1.0, Computation time: 1.130624532699585\n",
      "Step: 432, Loss: 0.9379743337631226, Accuracy: 0.96875, Computation time: 0.9510791301727295\n",
      "Step: 433, Loss: 0.9264389872550964, Accuracy: 0.96875, Computation time: 1.4588351249694824\n",
      "Step: 434, Loss: 0.9159783124923706, Accuracy: 1.0, Computation time: 1.2780065536499023\n",
      "Step: 435, Loss: 0.9369595646858215, Accuracy: 0.96875, Computation time: 1.0696227550506592\n",
      "Step: 436, Loss: 0.9165118336677551, Accuracy: 1.0, Computation time: 1.2058210372924805\n",
      "Step: 437, Loss: 0.9178235530853271, Accuracy: 1.0, Computation time: 1.2195651531219482\n",
      "Step: 438, Loss: 0.917388916015625, Accuracy: 1.0, Computation time: 1.5906400680541992\n",
      "Step: 439, Loss: 0.9287411570549011, Accuracy: 0.96875, Computation time: 1.317805290222168\n",
      "Step: 440, Loss: 0.9359705448150635, Accuracy: 0.96875, Computation time: 1.019242525100708\n",
      "Step: 441, Loss: 0.9162169694900513, Accuracy: 1.0, Computation time: 1.434295892715454\n",
      "Step: 442, Loss: 0.916144073009491, Accuracy: 1.0, Computation time: 1.0250821113586426\n",
      "Step: 443, Loss: 0.9245419502258301, Accuracy: 1.0, Computation time: 1.2720332145690918\n",
      "Step: 444, Loss: 0.9164650440216064, Accuracy: 1.0, Computation time: 1.135819435119629\n",
      "Step: 445, Loss: 0.9160362482070923, Accuracy: 1.0, Computation time: 1.425480604171753\n",
      "Step: 446, Loss: 0.9305155277252197, Accuracy: 0.96875, Computation time: 1.6500797271728516\n",
      "Step: 447, Loss: 0.9162210822105408, Accuracy: 1.0, Computation time: 1.1985325813293457\n",
      "Step: 448, Loss: 0.9244200587272644, Accuracy: 1.0, Computation time: 1.6725780963897705\n",
      "Step: 449, Loss: 0.9580368399620056, Accuracy: 0.9375, Computation time: 1.176811933517456\n",
      "Step: 450, Loss: 0.9161200523376465, Accuracy: 1.0, Computation time: 1.4521605968475342\n",
      "Step: 451, Loss: 0.9164113998413086, Accuracy: 1.0, Computation time: 1.2262747287750244\n",
      "Step: 452, Loss: 0.9377481341362, Accuracy: 0.96875, Computation time: 1.08195161819458\n",
      "Step: 453, Loss: 0.9163742661476135, Accuracy: 1.0, Computation time: 1.4432215690612793\n",
      "Step: 454, Loss: 0.9194722175598145, Accuracy: 1.0, Computation time: 1.1766388416290283\n",
      "Step: 455, Loss: 0.9161258935928345, Accuracy: 1.0, Computation time: 1.2067985534667969\n",
      "Step: 456, Loss: 0.9179227948188782, Accuracy: 1.0, Computation time: 1.6522352695465088\n",
      "Step: 457, Loss: 0.9164771437644958, Accuracy: 1.0, Computation time: 1.4051728248596191\n",
      "Step: 458, Loss: 0.9161868691444397, Accuracy: 1.0, Computation time: 1.2353579998016357\n",
      "Step: 459, Loss: 0.9177791476249695, Accuracy: 1.0, Computation time: 1.456848382949829\n",
      "Step: 460, Loss: 0.9162771701812744, Accuracy: 1.0, Computation time: 0.982405424118042\n",
      "Step: 461, Loss: 0.916158139705658, Accuracy: 1.0, Computation time: 1.5117642879486084\n",
      "Step: 462, Loss: 0.9164137244224548, Accuracy: 1.0, Computation time: 1.224701166152954\n",
      "Step: 463, Loss: 0.916274905204773, Accuracy: 1.0, Computation time: 1.1366651058197021\n",
      "Step: 464, Loss: 0.9183604121208191, Accuracy: 1.0, Computation time: 1.324596881866455\n",
      "Step: 465, Loss: 0.9160869121551514, Accuracy: 1.0, Computation time: 1.293412208557129\n",
      "Step: 466, Loss: 0.9262344241142273, Accuracy: 0.96875, Computation time: 1.4816944599151611\n",
      "Step: 467, Loss: 0.9160897135734558, Accuracy: 1.0, Computation time: 1.2471754550933838\n",
      "Step: 468, Loss: 0.9161345958709717, Accuracy: 1.0, Computation time: 1.6468842029571533\n",
      "Step: 469, Loss: 0.9160696268081665, Accuracy: 1.0, Computation time: 1.2883739471435547\n",
      "Step: 470, Loss: 0.9363424777984619, Accuracy: 0.96875, Computation time: 1.0639064311981201\n",
      "Step: 471, Loss: 0.937523365020752, Accuracy: 0.96875, Computation time: 1.2121734619140625\n",
      "Step: 472, Loss: 0.9161564707756042, Accuracy: 1.0, Computation time: 1.2528736591339111\n",
      "Step: 473, Loss: 0.923531174659729, Accuracy: 1.0, Computation time: 1.2590832710266113\n",
      "Step: 474, Loss: 0.9188345670700073, Accuracy: 1.0, Computation time: 1.1879620552062988\n",
      "Step: 475, Loss: 0.9159783124923706, Accuracy: 1.0, Computation time: 1.3871872425079346\n",
      "Step: 476, Loss: 0.9161163568496704, Accuracy: 1.0, Computation time: 1.2113325595855713\n",
      "Step: 477, Loss: 0.9237619638442993, Accuracy: 1.0, Computation time: 1.1754908561706543\n",
      "Step: 478, Loss: 0.9377337694168091, Accuracy: 0.96875, Computation time: 1.0231306552886963\n",
      "Step: 479, Loss: 0.9167841076850891, Accuracy: 1.0, Computation time: 0.9839377403259277\n",
      "Step: 480, Loss: 0.9160516262054443, Accuracy: 1.0, Computation time: 1.0427160263061523\n",
      "Step: 481, Loss: 0.9268398284912109, Accuracy: 0.96875, Computation time: 1.034583330154419\n",
      "Step: 482, Loss: 0.916368305683136, Accuracy: 1.0, Computation time: 1.0128233432769775\n",
      "Step: 483, Loss: 0.9162212014198303, Accuracy: 1.0, Computation time: 0.9515986442565918\n",
      "Step: 484, Loss: 0.9162517786026001, Accuracy: 1.0, Computation time: 1.293083906173706\n",
      "Step: 485, Loss: 0.9160414338111877, Accuracy: 1.0, Computation time: 1.0423479080200195\n",
      "Step: 486, Loss: 0.9248210191726685, Accuracy: 1.0, Computation time: 0.9947645664215088\n",
      "Step: 487, Loss: 0.9436745047569275, Accuracy: 0.9375, Computation time: 1.1554620265960693\n",
      "Step: 488, Loss: 0.9242700338363647, Accuracy: 1.0, Computation time: 1.161628246307373\n",
      "Step: 489, Loss: 0.916068971157074, Accuracy: 1.0, Computation time: 1.1753044128417969\n",
      "Step: 490, Loss: 0.9375796914100647, Accuracy: 0.96875, Computation time: 1.2279577255249023\n",
      "Step: 491, Loss: 0.9188385009765625, Accuracy: 1.0, Computation time: 0.8830113410949707\n",
      "Step: 492, Loss: 0.9162659049034119, Accuracy: 1.0, Computation time: 0.9327378273010254\n",
      "Step: 493, Loss: 0.9166762232780457, Accuracy: 1.0, Computation time: 1.0787920951843262\n",
      "Step: 494, Loss: 0.9165461659431458, Accuracy: 1.0, Computation time: 0.9632060527801514\n",
      "Step: 495, Loss: 0.9214023947715759, Accuracy: 1.0, Computation time: 1.2012202739715576\n",
      "Step: 496, Loss: 0.9163550734519958, Accuracy: 1.0, Computation time: 0.9298787117004395\n",
      "Step: 497, Loss: 0.9162977933883667, Accuracy: 1.0, Computation time: 1.2241854667663574\n",
      "Step: 498, Loss: 0.9212194085121155, Accuracy: 1.0, Computation time: 1.6671526432037354\n",
      "Step: 499, Loss: 0.9162846803665161, Accuracy: 1.0, Computation time: 1.0702354907989502\n",
      "Step: 500, Loss: 0.916018009185791, Accuracy: 1.0, Computation time: 0.9572393894195557\n",
      "Step: 501, Loss: 0.9160439372062683, Accuracy: 1.0, Computation time: 1.4639017581939697\n",
      "Step: 502, Loss: 0.9172818064689636, Accuracy: 1.0, Computation time: 1.556182622909546\n",
      "Step: 503, Loss: 0.9249791502952576, Accuracy: 1.0, Computation time: 1.2920873165130615\n",
      "Step: 504, Loss: 0.9177464246749878, Accuracy: 1.0, Computation time: 1.053936243057251\n",
      "Step: 505, Loss: 0.9160676002502441, Accuracy: 1.0, Computation time: 1.1000556945800781\n",
      "Step: 506, Loss: 0.9378000497817993, Accuracy: 0.96875, Computation time: 1.1371347904205322\n",
      "Step: 507, Loss: 0.9161473512649536, Accuracy: 1.0, Computation time: 1.8510229587554932\n",
      "Step: 508, Loss: 0.9163211584091187, Accuracy: 1.0, Computation time: 1.0846233367919922\n",
      "Step: 509, Loss: 0.9165976643562317, Accuracy: 1.0, Computation time: 1.079514980316162\n",
      "Step: 510, Loss: 0.9162151217460632, Accuracy: 1.0, Computation time: 0.8663942813873291\n",
      "Step: 511, Loss: 0.9196408987045288, Accuracy: 1.0, Computation time: 1.2409286499023438\n",
      "Step: 512, Loss: 0.9357264637947083, Accuracy: 0.96875, Computation time: 1.1212208271026611\n",
      "Step: 513, Loss: 0.916145920753479, Accuracy: 1.0, Computation time: 1.3170666694641113\n",
      "Step: 514, Loss: 0.9160300493240356, Accuracy: 1.0, Computation time: 1.0490961074829102\n",
      "Step: 515, Loss: 0.9169828295707703, Accuracy: 1.0, Computation time: 1.0367140769958496\n",
      "Step: 516, Loss: 0.918272852897644, Accuracy: 1.0, Computation time: 1.009216070175171\n",
      "Step: 517, Loss: 0.91597980260849, Accuracy: 1.0, Computation time: 1.3802525997161865\n",
      "Step: 518, Loss: 0.9160112142562866, Accuracy: 1.0, Computation time: 1.0216896533966064\n",
      "Step: 519, Loss: 0.9160247445106506, Accuracy: 1.0, Computation time: 0.8469324111938477\n",
      "Step: 520, Loss: 0.9161133766174316, Accuracy: 1.0, Computation time: 0.9619686603546143\n",
      "Step: 521, Loss: 0.9161701202392578, Accuracy: 1.0, Computation time: 1.3575108051300049\n",
      "Step: 522, Loss: 0.9162417650222778, Accuracy: 1.0, Computation time: 0.955204963684082\n",
      "Step: 523, Loss: 0.9367439150810242, Accuracy: 0.96875, Computation time: 1.2422480583190918\n",
      "Step: 524, Loss: 0.9160396456718445, Accuracy: 1.0, Computation time: 1.060842752456665\n",
      "Step: 525, Loss: 0.9200841784477234, Accuracy: 1.0, Computation time: 1.0524694919586182\n",
      "Step: 526, Loss: 0.937546968460083, Accuracy: 0.96875, Computation time: 1.1743948459625244\n",
      "Step: 527, Loss: 0.9336099624633789, Accuracy: 0.96875, Computation time: 1.0732066631317139\n",
      "Step: 528, Loss: 0.916013240814209, Accuracy: 1.0, Computation time: 1.1299974918365479\n",
      "Step: 529, Loss: 0.9160316586494446, Accuracy: 1.0, Computation time: 1.1486554145812988\n",
      "Step: 530, Loss: 0.9161602854728699, Accuracy: 1.0, Computation time: 1.2064697742462158\n",
      "Step: 531, Loss: 0.9377252459526062, Accuracy: 0.96875, Computation time: 1.0613479614257812\n",
      "Step: 532, Loss: 0.9379308223724365, Accuracy: 0.96875, Computation time: 1.0499317646026611\n",
      "Step: 533, Loss: 0.9161879420280457, Accuracy: 1.0, Computation time: 1.1037678718566895\n",
      "Step: 534, Loss: 0.9163241386413574, Accuracy: 1.0, Computation time: 1.2019984722137451\n",
      "Step: 535, Loss: 0.9162750244140625, Accuracy: 1.0, Computation time: 1.2726576328277588\n",
      "Step: 536, Loss: 0.9160438179969788, Accuracy: 1.0, Computation time: 0.8866686820983887\n",
      "Step: 537, Loss: 0.9161545634269714, Accuracy: 1.0, Computation time: 0.9659895896911621\n",
      "Step: 538, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 1.5516843795776367\n",
      "Step: 539, Loss: 0.9179093241691589, Accuracy: 1.0, Computation time: 1.229966163635254\n",
      "Step: 540, Loss: 0.9376415610313416, Accuracy: 0.96875, Computation time: 1.1400301456451416\n",
      "Step: 541, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 1.4372382164001465\n",
      "Step: 542, Loss: 0.9202764630317688, Accuracy: 1.0, Computation time: 1.1962027549743652\n",
      "Step: 543, Loss: 0.9205460548400879, Accuracy: 1.0, Computation time: 1.298125982284546\n",
      "Step: 544, Loss: 0.9373080134391785, Accuracy: 0.96875, Computation time: 1.1637823581695557\n",
      "Step: 545, Loss: 0.9167770147323608, Accuracy: 1.0, Computation time: 1.248488426208496\n",
      "Step: 546, Loss: 0.9159988164901733, Accuracy: 1.0, Computation time: 1.344862937927246\n",
      "Step: 547, Loss: 0.9352544546127319, Accuracy: 0.96875, Computation time: 1.2370007038116455\n",
      "Step: 548, Loss: 0.9163535833358765, Accuracy: 1.0, Computation time: 1.1466035842895508\n",
      "Step: 549, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 1.289161205291748\n",
      "Step: 550, Loss: 0.9188507795333862, Accuracy: 1.0, Computation time: 1.3146374225616455\n",
      "Step: 551, Loss: 0.9160647988319397, Accuracy: 1.0, Computation time: 1.2573108673095703\n",
      "Step: 552, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 1.0229408740997314\n",
      "Step: 553, Loss: 0.937986433506012, Accuracy: 0.96875, Computation time: 1.3039658069610596\n",
      "Step: 554, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.1221837997436523\n",
      "Step: 555, Loss: 0.9163461923599243, Accuracy: 1.0, Computation time: 1.1325931549072266\n",
      "Step: 556, Loss: 0.9159354567527771, Accuracy: 1.0, Computation time: 0.9995236396789551\n",
      "########################\n",
      "Test loss: 1.0668119192123413, Test Accuracy_epoch4: 0.7761485576629639\n",
      "########################\n",
      "Step: 557, Loss: 0.9159700274467468, Accuracy: 1.0, Computation time: 1.2060647010803223\n",
      "Step: 558, Loss: 0.9159798622131348, Accuracy: 1.0, Computation time: 1.5936458110809326\n",
      "Step: 559, Loss: 0.9378340244293213, Accuracy: 0.96875, Computation time: 1.3093132972717285\n",
      "Step: 560, Loss: 0.9173280596733093, Accuracy: 1.0, Computation time: 1.2729415893554688\n",
      "Step: 561, Loss: 0.9163725972175598, Accuracy: 1.0, Computation time: 1.1299710273742676\n",
      "Step: 562, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.0680408477783203\n",
      "Step: 563, Loss: 0.9160611033439636, Accuracy: 1.0, Computation time: 1.1136343479156494\n",
      "Step: 564, Loss: 0.9314282536506653, Accuracy: 1.0, Computation time: 1.3024685382843018\n",
      "Step: 565, Loss: 0.9160988926887512, Accuracy: 1.0, Computation time: 1.154667615890503\n",
      "Step: 566, Loss: 0.9176661968231201, Accuracy: 1.0, Computation time: 1.3897275924682617\n",
      "Step: 567, Loss: 0.9161679148674011, Accuracy: 1.0, Computation time: 1.39266037940979\n",
      "Step: 568, Loss: 0.9178852438926697, Accuracy: 1.0, Computation time: 1.3146653175354004\n",
      "Step: 569, Loss: 0.9161310791969299, Accuracy: 1.0, Computation time: 1.1276922225952148\n",
      "Step: 570, Loss: 0.9368845820426941, Accuracy: 0.96875, Computation time: 1.4260351657867432\n",
      "Step: 571, Loss: 0.9172298908233643, Accuracy: 1.0, Computation time: 0.9741311073303223\n",
      "Step: 572, Loss: 0.9376768469810486, Accuracy: 0.96875, Computation time: 1.122586727142334\n",
      "Step: 573, Loss: 0.916002631187439, Accuracy: 1.0, Computation time: 0.856548547744751\n",
      "Step: 574, Loss: 0.9162271618843079, Accuracy: 1.0, Computation time: 1.2329096794128418\n",
      "Step: 575, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 1.1389036178588867\n",
      "Step: 576, Loss: 0.916144847869873, Accuracy: 1.0, Computation time: 1.0019946098327637\n",
      "Step: 577, Loss: 0.9186404943466187, Accuracy: 1.0, Computation time: 1.0066139698028564\n",
      "Step: 578, Loss: 0.931901752948761, Accuracy: 0.96875, Computation time: 1.053898572921753\n",
      "Step: 579, Loss: 0.915969729423523, Accuracy: 1.0, Computation time: 1.1126794815063477\n",
      "Step: 580, Loss: 0.9161916375160217, Accuracy: 1.0, Computation time: 1.0679900646209717\n",
      "Step: 581, Loss: 0.9161061644554138, Accuracy: 1.0, Computation time: 1.4245386123657227\n",
      "Step: 582, Loss: 0.9160517454147339, Accuracy: 1.0, Computation time: 1.0254533290863037\n",
      "Step: 583, Loss: 0.9160575270652771, Accuracy: 1.0, Computation time: 1.0380449295043945\n",
      "Step: 584, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 1.2139735221862793\n",
      "Step: 585, Loss: 0.9160498976707458, Accuracy: 1.0, Computation time: 1.0452220439910889\n",
      "Step: 586, Loss: 0.9161256551742554, Accuracy: 1.0, Computation time: 1.3686468601226807\n",
      "Step: 587, Loss: 0.9177771806716919, Accuracy: 1.0, Computation time: 1.2139396667480469\n",
      "Step: 588, Loss: 0.9160376787185669, Accuracy: 1.0, Computation time: 1.4048566818237305\n",
      "Step: 589, Loss: 0.9377427101135254, Accuracy: 0.96875, Computation time: 1.211766004562378\n",
      "Step: 590, Loss: 0.9159708619117737, Accuracy: 1.0, Computation time: 0.9925882816314697\n",
      "Step: 591, Loss: 0.916023850440979, Accuracy: 1.0, Computation time: 1.0589497089385986\n",
      "Step: 592, Loss: 0.9160237312316895, Accuracy: 1.0, Computation time: 1.2768733501434326\n",
      "Step: 593, Loss: 0.9169225096702576, Accuracy: 1.0, Computation time: 1.3501267433166504\n",
      "Step: 594, Loss: 0.9183292388916016, Accuracy: 1.0, Computation time: 1.20444917678833\n",
      "Step: 595, Loss: 0.9160137176513672, Accuracy: 1.0, Computation time: 1.011427879333496\n",
      "Step: 596, Loss: 0.9377938508987427, Accuracy: 0.96875, Computation time: 0.9507777690887451\n",
      "Step: 597, Loss: 0.9160369038581848, Accuracy: 1.0, Computation time: 1.3255550861358643\n",
      "Step: 598, Loss: 0.9160188436508179, Accuracy: 1.0, Computation time: 1.0881829261779785\n",
      "Step: 599, Loss: 0.916034460067749, Accuracy: 1.0, Computation time: 1.6336615085601807\n",
      "Step: 600, Loss: 0.9375227689743042, Accuracy: 0.96875, Computation time: 1.6155331134796143\n",
      "Step: 601, Loss: 0.9273107647895813, Accuracy: 0.96875, Computation time: 1.1335914134979248\n",
      "Step: 602, Loss: 0.9159391522407532, Accuracy: 1.0, Computation time: 1.2521882057189941\n",
      "Step: 603, Loss: 0.9159466624259949, Accuracy: 1.0, Computation time: 0.9868369102478027\n",
      "Step: 604, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 0.9663491249084473\n",
      "Step: 605, Loss: 0.9376842975616455, Accuracy: 0.96875, Computation time: 1.218790054321289\n",
      "Step: 606, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.0447371006011963\n",
      "Step: 607, Loss: 0.933933675289154, Accuracy: 0.96875, Computation time: 1.0327410697937012\n",
      "Step: 608, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 1.6753628253936768\n",
      "Step: 609, Loss: 0.9159756302833557, Accuracy: 1.0, Computation time: 1.1424098014831543\n",
      "Step: 610, Loss: 0.9159008264541626, Accuracy: 1.0, Computation time: 1.312629222869873\n",
      "Step: 611, Loss: 0.9162998795509338, Accuracy: 1.0, Computation time: 1.1794099807739258\n",
      "Step: 612, Loss: 0.9166423678398132, Accuracy: 1.0, Computation time: 0.951223611831665\n",
      "Step: 613, Loss: 0.9273513555526733, Accuracy: 0.96875, Computation time: 1.405808687210083\n",
      "Step: 614, Loss: 0.9381918907165527, Accuracy: 0.96875, Computation time: 0.9817368984222412\n",
      "Step: 615, Loss: 0.9159197807312012, Accuracy: 1.0, Computation time: 1.0526726245880127\n",
      "Step: 616, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.1840918064117432\n",
      "Step: 617, Loss: 0.916046679019928, Accuracy: 1.0, Computation time: 1.0409107208251953\n",
      "Step: 618, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 1.110435962677002\n",
      "Step: 619, Loss: 0.9175755381584167, Accuracy: 1.0, Computation time: 1.0727670192718506\n",
      "Step: 620, Loss: 0.9169678092002869, Accuracy: 1.0, Computation time: 1.055356740951538\n",
      "Step: 621, Loss: 0.9168418049812317, Accuracy: 1.0, Computation time: 1.1944677829742432\n",
      "Step: 622, Loss: 0.9159576892852783, Accuracy: 1.0, Computation time: 1.146531343460083\n",
      "Step: 623, Loss: 0.915939211845398, Accuracy: 1.0, Computation time: 1.039973497390747\n",
      "Step: 624, Loss: 0.9160255789756775, Accuracy: 1.0, Computation time: 1.1457724571228027\n",
      "Step: 625, Loss: 0.9512726068496704, Accuracy: 0.96875, Computation time: 1.262249231338501\n",
      "Step: 626, Loss: 0.937921941280365, Accuracy: 0.96875, Computation time: 0.92840576171875\n",
      "Step: 627, Loss: 0.916701078414917, Accuracy: 1.0, Computation time: 1.047468900680542\n",
      "Step: 628, Loss: 0.9160765409469604, Accuracy: 1.0, Computation time: 1.0656564235687256\n",
      "Step: 629, Loss: 0.9299217462539673, Accuracy: 0.96875, Computation time: 1.2393193244934082\n",
      "Step: 630, Loss: 0.9421428442001343, Accuracy: 0.9375, Computation time: 0.9667160511016846\n",
      "Step: 631, Loss: 0.9163126349449158, Accuracy: 1.0, Computation time: 1.176086187362671\n",
      "Step: 632, Loss: 0.9167734980583191, Accuracy: 1.0, Computation time: 1.0721673965454102\n",
      "Step: 633, Loss: 0.9174689650535583, Accuracy: 1.0, Computation time: 1.0639324188232422\n",
      "Step: 634, Loss: 0.9186052680015564, Accuracy: 1.0, Computation time: 1.0355055332183838\n",
      "Step: 635, Loss: 0.9180927872657776, Accuracy: 1.0, Computation time: 1.0268983840942383\n",
      "Step: 636, Loss: 0.918834924697876, Accuracy: 1.0, Computation time: 1.2975947856903076\n",
      "Step: 637, Loss: 0.9357245564460754, Accuracy: 0.96875, Computation time: 0.9851832389831543\n",
      "Step: 638, Loss: 0.9204435348510742, Accuracy: 1.0, Computation time: 1.542142391204834\n",
      "Step: 639, Loss: 0.9165834188461304, Accuracy: 1.0, Computation time: 1.1866655349731445\n",
      "Step: 640, Loss: 0.9162260293960571, Accuracy: 1.0, Computation time: 1.0471844673156738\n",
      "Step: 641, Loss: 0.9391758441925049, Accuracy: 0.96875, Computation time: 1.0623047351837158\n",
      "Step: 642, Loss: 0.9163389205932617, Accuracy: 1.0, Computation time: 0.9897685050964355\n",
      "Step: 643, Loss: 0.9168184995651245, Accuracy: 1.0, Computation time: 0.9251551628112793\n",
      "Step: 644, Loss: 0.9180040955543518, Accuracy: 1.0, Computation time: 1.104109287261963\n",
      "Step: 645, Loss: 0.9169145226478577, Accuracy: 1.0, Computation time: 0.9760332107543945\n",
      "Step: 646, Loss: 0.9491063356399536, Accuracy: 0.9375, Computation time: 1.0222766399383545\n",
      "Step: 647, Loss: 0.9171745181083679, Accuracy: 1.0, Computation time: 1.0213186740875244\n",
      "Step: 648, Loss: 0.9165562391281128, Accuracy: 1.0, Computation time: 1.2268192768096924\n",
      "Step: 649, Loss: 0.9163216948509216, Accuracy: 1.0, Computation time: 1.1199257373809814\n",
      "Step: 650, Loss: 0.9166622757911682, Accuracy: 1.0, Computation time: 1.1224579811096191\n",
      "Step: 651, Loss: 0.9376710653305054, Accuracy: 0.96875, Computation time: 1.0534539222717285\n",
      "Step: 652, Loss: 0.9374318718910217, Accuracy: 0.96875, Computation time: 1.2044870853424072\n",
      "Step: 653, Loss: 0.9315474033355713, Accuracy: 0.96875, Computation time: 1.1656672954559326\n",
      "Step: 654, Loss: 0.9377199411392212, Accuracy: 0.96875, Computation time: 1.146785020828247\n",
      "Step: 655, Loss: 0.9165872931480408, Accuracy: 1.0, Computation time: 1.059359073638916\n",
      "Step: 656, Loss: 0.9167953729629517, Accuracy: 1.0, Computation time: 1.1878187656402588\n",
      "Step: 657, Loss: 0.9379319548606873, Accuracy: 0.96875, Computation time: 1.29278564453125\n",
      "Step: 658, Loss: 0.929785430431366, Accuracy: 0.96875, Computation time: 1.2401440143585205\n",
      "Step: 659, Loss: 0.916106104850769, Accuracy: 1.0, Computation time: 1.1823368072509766\n",
      "Step: 660, Loss: 0.9161105155944824, Accuracy: 1.0, Computation time: 1.000868558883667\n",
      "Step: 661, Loss: 0.9162613749504089, Accuracy: 1.0, Computation time: 0.9584310054779053\n",
      "Step: 662, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 1.2339861392974854\n",
      "Step: 663, Loss: 0.9161497354507446, Accuracy: 1.0, Computation time: 1.1527600288391113\n",
      "Step: 664, Loss: 0.9168086051940918, Accuracy: 1.0, Computation time: 1.0848758220672607\n",
      "Step: 665, Loss: 0.9163042902946472, Accuracy: 1.0, Computation time: 0.9876232147216797\n",
      "Step: 666, Loss: 0.9159930348396301, Accuracy: 1.0, Computation time: 1.0053043365478516\n",
      "Step: 667, Loss: 0.9163293242454529, Accuracy: 1.0, Computation time: 1.0577635765075684\n",
      "Step: 668, Loss: 0.9162314534187317, Accuracy: 1.0, Computation time: 1.0884795188903809\n",
      "Step: 669, Loss: 0.9165772795677185, Accuracy: 1.0, Computation time: 0.972836971282959\n",
      "Step: 670, Loss: 0.937839686870575, Accuracy: 0.96875, Computation time: 1.0280094146728516\n",
      "Step: 671, Loss: 0.9163093566894531, Accuracy: 1.0, Computation time: 0.9681298732757568\n",
      "Step: 672, Loss: 0.9159972667694092, Accuracy: 1.0, Computation time: 1.0667366981506348\n",
      "Step: 673, Loss: 0.9335416555404663, Accuracy: 0.96875, Computation time: 1.1384797096252441\n",
      "Step: 674, Loss: 0.9171327948570251, Accuracy: 1.0, Computation time: 1.6422944068908691\n",
      "Step: 675, Loss: 0.9160521030426025, Accuracy: 1.0, Computation time: 0.8926959037780762\n",
      "Step: 676, Loss: 0.9375653266906738, Accuracy: 0.96875, Computation time: 1.0351002216339111\n",
      "Step: 677, Loss: 0.9159619212150574, Accuracy: 1.0, Computation time: 1.0180025100708008\n",
      "Step: 678, Loss: 0.9272114038467407, Accuracy: 0.96875, Computation time: 0.9317030906677246\n",
      "Step: 679, Loss: 0.9161026477813721, Accuracy: 1.0, Computation time: 0.9595978260040283\n",
      "Step: 680, Loss: 0.921711802482605, Accuracy: 1.0, Computation time: 1.0464000701904297\n",
      "Step: 681, Loss: 0.9161434173583984, Accuracy: 1.0, Computation time: 0.9257206916809082\n",
      "Step: 682, Loss: 0.9161304831504822, Accuracy: 1.0, Computation time: 0.8668003082275391\n",
      "Step: 683, Loss: 0.9161093235015869, Accuracy: 1.0, Computation time: 1.0359554290771484\n",
      "Step: 684, Loss: 0.924070417881012, Accuracy: 1.0, Computation time: 1.1321814060211182\n",
      "Step: 685, Loss: 0.9160498380661011, Accuracy: 1.0, Computation time: 1.0130205154418945\n",
      "Step: 686, Loss: 0.9417749643325806, Accuracy: 0.96875, Computation time: 1.2584056854248047\n",
      "Step: 687, Loss: 0.9160976409912109, Accuracy: 1.0, Computation time: 0.9478085041046143\n",
      "Step: 688, Loss: 0.9162594676017761, Accuracy: 1.0, Computation time: 0.9284491539001465\n",
      "Step: 689, Loss: 0.9165006279945374, Accuracy: 1.0, Computation time: 1.095219373703003\n",
      "Step: 690, Loss: 0.9537342190742493, Accuracy: 0.9375, Computation time: 0.9878113269805908\n",
      "Step: 691, Loss: 0.916226327419281, Accuracy: 1.0, Computation time: 0.8924455642700195\n",
      "Step: 692, Loss: 0.915998101234436, Accuracy: 1.0, Computation time: 0.9592175483703613\n",
      "Step: 693, Loss: 0.9162909984588623, Accuracy: 1.0, Computation time: 1.3059594631195068\n",
      "Step: 694, Loss: 0.9358371496200562, Accuracy: 0.96875, Computation time: 1.0672178268432617\n",
      "Step: 695, Loss: 0.937633216381073, Accuracy: 0.96875, Computation time: 1.2635374069213867\n",
      "########################\n",
      "Test loss: 1.0703614950180054, Test Accuracy_epoch5: 0.7751710414886475\n",
      "########################\n",
      "Step: 696, Loss: 0.9161025285720825, Accuracy: 1.0, Computation time: 0.9858293533325195\n",
      "Step: 697, Loss: 0.9193107485771179, Accuracy: 1.0, Computation time: 1.0093870162963867\n",
      "Step: 698, Loss: 0.9163408279418945, Accuracy: 1.0, Computation time: 1.2195417881011963\n",
      "Step: 699, Loss: 0.9162223935127258, Accuracy: 1.0, Computation time: 1.1366283893585205\n",
      "Step: 700, Loss: 0.9161530137062073, Accuracy: 1.0, Computation time: 1.010305404663086\n",
      "Step: 701, Loss: 0.9169601202011108, Accuracy: 1.0, Computation time: 1.0933983325958252\n",
      "Step: 702, Loss: 0.9372967481613159, Accuracy: 0.96875, Computation time: 1.1047439575195312\n",
      "Step: 703, Loss: 0.9195970892906189, Accuracy: 1.0, Computation time: 1.297252893447876\n",
      "Step: 704, Loss: 0.9160704612731934, Accuracy: 1.0, Computation time: 1.0240449905395508\n",
      "Step: 705, Loss: 0.916080117225647, Accuracy: 1.0, Computation time: 0.9970943927764893\n",
      "Step: 706, Loss: 0.9169352650642395, Accuracy: 1.0, Computation time: 0.9302847385406494\n",
      "Step: 707, Loss: 0.9161766767501831, Accuracy: 1.0, Computation time: 0.9402449131011963\n",
      "Step: 708, Loss: 0.9197608828544617, Accuracy: 1.0, Computation time: 1.0897009372711182\n",
      "Step: 709, Loss: 0.9159976243972778, Accuracy: 1.0, Computation time: 1.0083930492401123\n",
      "Step: 710, Loss: 0.9167057871818542, Accuracy: 1.0, Computation time: 0.9966607093811035\n",
      "Step: 711, Loss: 0.9176163077354431, Accuracy: 1.0, Computation time: 0.9451608657836914\n",
      "Step: 712, Loss: 0.9161912798881531, Accuracy: 1.0, Computation time: 0.9389955997467041\n",
      "Step: 713, Loss: 0.9376792311668396, Accuracy: 0.96875, Computation time: 0.9800734519958496\n",
      "Step: 714, Loss: 0.9160146117210388, Accuracy: 1.0, Computation time: 0.8411743640899658\n",
      "Step: 715, Loss: 0.9160138964653015, Accuracy: 1.0, Computation time: 0.9587340354919434\n",
      "Step: 716, Loss: 0.946511447429657, Accuracy: 0.9375, Computation time: 1.0917127132415771\n",
      "Step: 717, Loss: 0.9596266746520996, Accuracy: 0.9375, Computation time: 0.9682979583740234\n",
      "Step: 718, Loss: 0.9161986112594604, Accuracy: 1.0, Computation time: 0.8522439002990723\n",
      "Step: 719, Loss: 0.9162022471427917, Accuracy: 1.0, Computation time: 1.0621800422668457\n",
      "Step: 720, Loss: 0.9161630868911743, Accuracy: 1.0, Computation time: 0.8296308517456055\n",
      "Step: 721, Loss: 0.9162188172340393, Accuracy: 1.0, Computation time: 0.9846775531768799\n",
      "Step: 722, Loss: 0.9342132210731506, Accuracy: 0.96875, Computation time: 0.8823893070220947\n",
      "Step: 723, Loss: 0.9159997701644897, Accuracy: 1.0, Computation time: 1.308971643447876\n",
      "Step: 724, Loss: 0.9159505367279053, Accuracy: 1.0, Computation time: 0.8620767593383789\n",
      "Step: 725, Loss: 0.9170508980751038, Accuracy: 1.0, Computation time: 0.9347906112670898\n",
      "Step: 726, Loss: 0.9161012172698975, Accuracy: 1.0, Computation time: 0.8902637958526611\n",
      "Step: 727, Loss: 0.9160503149032593, Accuracy: 1.0, Computation time: 0.9101672172546387\n",
      "Step: 728, Loss: 0.916033923625946, Accuracy: 1.0, Computation time: 0.7906458377838135\n",
      "Step: 729, Loss: 0.9160385131835938, Accuracy: 1.0, Computation time: 0.8681449890136719\n",
      "Step: 730, Loss: 0.9161458015441895, Accuracy: 1.0, Computation time: 0.8732788562774658\n",
      "Step: 731, Loss: 0.9159969091415405, Accuracy: 1.0, Computation time: 0.8866739273071289\n",
      "Step: 732, Loss: 0.9594569802284241, Accuracy: 0.9375, Computation time: 1.0811314582824707\n",
      "Step: 733, Loss: 0.9367305040359497, Accuracy: 0.96875, Computation time: 0.9317286014556885\n",
      "Step: 734, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 0.8192591667175293\n",
      "Step: 735, Loss: 0.9204987287521362, Accuracy: 1.0, Computation time: 0.8430130481719971\n",
      "Step: 736, Loss: 0.9178347587585449, Accuracy: 1.0, Computation time: 0.8656725883483887\n",
      "Step: 737, Loss: 0.919538676738739, Accuracy: 1.0, Computation time: 0.9735250473022461\n",
      "Step: 738, Loss: 0.9161856770515442, Accuracy: 1.0, Computation time: 0.8518712520599365\n",
      "Step: 739, Loss: 0.9160783290863037, Accuracy: 1.0, Computation time: 0.8386681079864502\n",
      "Step: 740, Loss: 0.9160561561584473, Accuracy: 1.0, Computation time: 0.8733315467834473\n",
      "Step: 741, Loss: 0.916015625, Accuracy: 1.0, Computation time: 0.8603951930999756\n",
      "Step: 742, Loss: 0.9161313772201538, Accuracy: 1.0, Computation time: 0.8300464153289795\n",
      "Step: 743, Loss: 0.9191806316375732, Accuracy: 1.0, Computation time: 1.0933661460876465\n",
      "Step: 744, Loss: 0.9358269572257996, Accuracy: 0.96875, Computation time: 0.8790688514709473\n",
      "Step: 745, Loss: 0.9165138602256775, Accuracy: 1.0, Computation time: 0.8358030319213867\n",
      "Step: 746, Loss: 0.9356046319007874, Accuracy: 0.96875, Computation time: 0.9617629051208496\n",
      "Step: 747, Loss: 0.9363921284675598, Accuracy: 0.96875, Computation time: 0.9214372634887695\n",
      "Step: 748, Loss: 0.9164931774139404, Accuracy: 1.0, Computation time: 0.8335747718811035\n",
      "Step: 749, Loss: 0.9208114743232727, Accuracy: 1.0, Computation time: 0.8972134590148926\n",
      "Step: 750, Loss: 0.9160556197166443, Accuracy: 1.0, Computation time: 1.1984107494354248\n",
      "Step: 751, Loss: 0.9160270690917969, Accuracy: 1.0, Computation time: 0.765350341796875\n",
      "Step: 752, Loss: 0.9264241456985474, Accuracy: 1.0, Computation time: 0.8882312774658203\n",
      "Step: 753, Loss: 0.9159980416297913, Accuracy: 1.0, Computation time: 0.8251776695251465\n",
      "Step: 754, Loss: 0.9352051019668579, Accuracy: 0.96875, Computation time: 1.2633812427520752\n",
      "Step: 755, Loss: 0.9167625904083252, Accuracy: 1.0, Computation time: 0.8320364952087402\n",
      "Step: 756, Loss: 0.9594749808311462, Accuracy: 0.9375, Computation time: 0.8167154788970947\n",
      "Step: 757, Loss: 0.9160293340682983, Accuracy: 1.0, Computation time: 0.7577316761016846\n",
      "Step: 758, Loss: 0.9281910061836243, Accuracy: 0.96875, Computation time: 1.000580072402954\n",
      "Step: 759, Loss: 0.9160403609275818, Accuracy: 1.0, Computation time: 0.9146347045898438\n",
      "Step: 760, Loss: 0.9161080121994019, Accuracy: 1.0, Computation time: 0.8139991760253906\n",
      "Step: 761, Loss: 0.9162425398826599, Accuracy: 1.0, Computation time: 0.7749006748199463\n",
      "Step: 762, Loss: 0.937481164932251, Accuracy: 0.96875, Computation time: 0.8511955738067627\n",
      "Step: 763, Loss: 0.9341177344322205, Accuracy: 0.96875, Computation time: 0.8590435981750488\n",
      "Step: 764, Loss: 0.9160969257354736, Accuracy: 1.0, Computation time: 0.7896127700805664\n",
      "Step: 765, Loss: 0.9190577268600464, Accuracy: 1.0, Computation time: 0.9697082042694092\n",
      "Step: 766, Loss: 0.9160983562469482, Accuracy: 1.0, Computation time: 0.8245527744293213\n",
      "Step: 767, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 0.8058629035949707\n",
      "Step: 768, Loss: 0.9240806102752686, Accuracy: 1.0, Computation time: 0.8566622734069824\n",
      "Step: 769, Loss: 0.915947675704956, Accuracy: 1.0, Computation time: 0.7608509063720703\n",
      "Step: 770, Loss: 0.9160237312316895, Accuracy: 1.0, Computation time: 0.7399089336395264\n",
      "Step: 771, Loss: 0.9161301851272583, Accuracy: 1.0, Computation time: 0.7635049819946289\n",
      "Step: 772, Loss: 0.9374306797981262, Accuracy: 0.96875, Computation time: 0.8126635551452637\n",
      "Step: 773, Loss: 0.9159846305847168, Accuracy: 1.0, Computation time: 1.169320821762085\n",
      "Step: 774, Loss: 0.9375306963920593, Accuracy: 0.96875, Computation time: 0.8068501949310303\n",
      "Step: 775, Loss: 0.917351245880127, Accuracy: 1.0, Computation time: 0.7760231494903564\n",
      "Step: 776, Loss: 0.9160390496253967, Accuracy: 1.0, Computation time: 1.0875935554504395\n",
      "Step: 777, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 0.9514541625976562\n",
      "Step: 778, Loss: 0.9159843325614929, Accuracy: 1.0, Computation time: 1.2381246089935303\n",
      "Step: 779, Loss: 0.9379692673683167, Accuracy: 0.96875, Computation time: 0.7996313571929932\n",
      "Step: 780, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 0.7478303909301758\n",
      "Step: 781, Loss: 0.9174181222915649, Accuracy: 1.0, Computation time: 1.0157818794250488\n",
      "Step: 782, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 0.8086757659912109\n",
      "Step: 783, Loss: 0.9160889983177185, Accuracy: 1.0, Computation time: 0.7894999980926514\n",
      "Step: 784, Loss: 0.9342597126960754, Accuracy: 0.96875, Computation time: 0.9439091682434082\n",
      "Step: 785, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 0.7427842617034912\n",
      "Step: 786, Loss: 0.9159477353096008, Accuracy: 1.0, Computation time: 0.7633452415466309\n",
      "Step: 787, Loss: 0.9372552633285522, Accuracy: 0.96875, Computation time: 1.0550472736358643\n",
      "Step: 788, Loss: 0.9159560799598694, Accuracy: 1.0, Computation time: 0.7459375858306885\n",
      "Step: 789, Loss: 0.9208769202232361, Accuracy: 1.0, Computation time: 0.7914087772369385\n",
      "Step: 790, Loss: 0.9179286956787109, Accuracy: 1.0, Computation time: 0.768683671951294\n",
      "Step: 791, Loss: 0.9378729462623596, Accuracy: 0.96875, Computation time: 0.8155689239501953\n",
      "Step: 792, Loss: 0.9162412285804749, Accuracy: 1.0, Computation time: 0.8421874046325684\n",
      "Step: 793, Loss: 0.9175556898117065, Accuracy: 1.0, Computation time: 0.7556078433990479\n",
      "Step: 794, Loss: 0.9315689206123352, Accuracy: 0.96875, Computation time: 0.9742856025695801\n",
      "Step: 795, Loss: 0.9159340858459473, Accuracy: 1.0, Computation time: 0.6870853900909424\n",
      "Step: 796, Loss: 0.9355388283729553, Accuracy: 0.96875, Computation time: 0.685837984085083\n",
      "Step: 797, Loss: 0.916013777256012, Accuracy: 1.0, Computation time: 1.0426669120788574\n",
      "Step: 798, Loss: 0.9161040782928467, Accuracy: 1.0, Computation time: 0.7720794677734375\n",
      "Step: 799, Loss: 0.9160592555999756, Accuracy: 1.0, Computation time: 1.004018783569336\n",
      "Step: 800, Loss: 0.9164867401123047, Accuracy: 1.0, Computation time: 0.8385918140411377\n",
      "Step: 801, Loss: 0.9159983992576599, Accuracy: 1.0, Computation time: 0.8188345432281494\n",
      "Step: 802, Loss: 0.916032612323761, Accuracy: 1.0, Computation time: 0.7165787220001221\n",
      "Step: 803, Loss: 0.9159652590751648, Accuracy: 1.0, Computation time: 0.8740708827972412\n",
      "Step: 804, Loss: 0.9161402583122253, Accuracy: 1.0, Computation time: 0.7679810523986816\n",
      "Step: 805, Loss: 0.9197221994400024, Accuracy: 1.0, Computation time: 0.9205322265625\n",
      "Step: 806, Loss: 0.9161114692687988, Accuracy: 1.0, Computation time: 1.0551609992980957\n",
      "Step: 807, Loss: 0.9159330725669861, Accuracy: 1.0, Computation time: 0.8030593395233154\n",
      "Step: 808, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 0.7238919734954834\n",
      "Step: 809, Loss: 0.9230372309684753, Accuracy: 1.0, Computation time: 0.8097612857818604\n",
      "Step: 810, Loss: 0.9160576462745667, Accuracy: 1.0, Computation time: 0.8453879356384277\n",
      "Step: 811, Loss: 0.9160141944885254, Accuracy: 1.0, Computation time: 0.7906894683837891\n",
      "Step: 812, Loss: 0.9161323308944702, Accuracy: 1.0, Computation time: 0.8697249889373779\n",
      "Step: 813, Loss: 0.9159606099128723, Accuracy: 1.0, Computation time: 0.8501765727996826\n",
      "Step: 814, Loss: 0.915984034538269, Accuracy: 1.0, Computation time: 0.7651271820068359\n",
      "Step: 815, Loss: 0.9160330295562744, Accuracy: 1.0, Computation time: 0.8746776580810547\n",
      "Step: 816, Loss: 0.9167640805244446, Accuracy: 1.0, Computation time: 0.7447731494903564\n",
      "Step: 817, Loss: 0.9374905824661255, Accuracy: 0.96875, Computation time: 0.7494986057281494\n",
      "Step: 818, Loss: 0.9160043001174927, Accuracy: 1.0, Computation time: 0.7971203327178955\n",
      "Step: 819, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 0.9013419151306152\n",
      "Step: 820, Loss: 0.9163868427276611, Accuracy: 1.0, Computation time: 0.8307638168334961\n",
      "Step: 821, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.1438758373260498\n",
      "Step: 822, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 0.7400426864624023\n",
      "Step: 823, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 0.7642762660980225\n",
      "Step: 824, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 0.7280337810516357\n",
      "Step: 825, Loss: 0.9352416396141052, Accuracy: 0.96875, Computation time: 0.817760705947876\n",
      "Step: 826, Loss: 0.9159100651741028, Accuracy: 1.0, Computation time: 0.8114311695098877\n",
      "Step: 827, Loss: 0.9161320924758911, Accuracy: 1.0, Computation time: 0.9960312843322754\n",
      "Step: 828, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 0.8074960708618164\n",
      "Step: 829, Loss: 0.9373809695243835, Accuracy: 0.96875, Computation time: 0.8014795780181885\n",
      "Step: 830, Loss: 0.9159471988677979, Accuracy: 1.0, Computation time: 0.9656422138214111\n",
      "Step: 831, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 0.7179522514343262\n",
      "Step: 832, Loss: 0.9283561110496521, Accuracy: 0.96875, Computation time: 0.8949079513549805\n",
      "Step: 833, Loss: 0.9375495910644531, Accuracy: 0.96875, Computation time: 0.8088119029998779\n",
      "Step: 834, Loss: 0.9166281223297119, Accuracy: 1.0, Computation time: 2.069317579269409\n",
      "########################\n",
      "Test loss: 1.072809100151062, Test Accuracy_epoch6: 0.7673509120941162\n",
      "########################\n",
      "Step: 835, Loss: 0.9367815852165222, Accuracy: 0.96875, Computation time: 0.8628242015838623\n",
      "Step: 836, Loss: 0.9161667227745056, Accuracy: 1.0, Computation time: 0.7735638618469238\n",
      "Step: 837, Loss: 0.9160048365592957, Accuracy: 1.0, Computation time: 1.2760915756225586\n",
      "Step: 838, Loss: 0.9161590337753296, Accuracy: 1.0, Computation time: 0.7860634326934814\n",
      "Step: 839, Loss: 0.916093111038208, Accuracy: 1.0, Computation time: 0.786055326461792\n",
      "Step: 840, Loss: 0.9374864101409912, Accuracy: 0.96875, Computation time: 0.7916440963745117\n",
      "Step: 841, Loss: 0.9159555435180664, Accuracy: 1.0, Computation time: 0.7685456275939941\n",
      "Step: 842, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 0.9175586700439453\n",
      "Step: 843, Loss: 0.9160361289978027, Accuracy: 1.0, Computation time: 0.8400380611419678\n",
      "Step: 844, Loss: 0.9298195838928223, Accuracy: 0.96875, Computation time: 0.778918981552124\n",
      "Step: 845, Loss: 0.9402591586112976, Accuracy: 0.96875, Computation time: 1.1032884120941162\n",
      "Step: 846, Loss: 0.9179372191429138, Accuracy: 1.0, Computation time: 0.9218881130218506\n",
      "Step: 847, Loss: 0.9162535667419434, Accuracy: 1.0, Computation time: 0.8731281757354736\n",
      "Step: 848, Loss: 0.9163669347763062, Accuracy: 1.0, Computation time: 0.8516902923583984\n",
      "Step: 849, Loss: 0.9163056015968323, Accuracy: 1.0, Computation time: 0.8548703193664551\n",
      "Step: 850, Loss: 0.9161093235015869, Accuracy: 1.0, Computation time: 0.8685085773468018\n",
      "Step: 851, Loss: 0.9161514639854431, Accuracy: 1.0, Computation time: 0.9300990104675293\n",
      "Step: 852, Loss: 0.927740752696991, Accuracy: 0.96875, Computation time: 1.0611529350280762\n",
      "Step: 853, Loss: 0.9163260459899902, Accuracy: 1.0, Computation time: 0.891826868057251\n",
      "Step: 854, Loss: 0.9159973859786987, Accuracy: 1.0, Computation time: 0.8406045436859131\n",
      "Step: 855, Loss: 0.9160294532775879, Accuracy: 1.0, Computation time: 0.8350083827972412\n",
      "Step: 856, Loss: 0.9378324151039124, Accuracy: 0.96875, Computation time: 1.2921411991119385\n",
      "Step: 857, Loss: 0.9160480499267578, Accuracy: 1.0, Computation time: 0.9115931987762451\n",
      "Step: 858, Loss: 0.9160155057907104, Accuracy: 1.0, Computation time: 0.9200549125671387\n",
      "Step: 859, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 0.8068997859954834\n",
      "Step: 860, Loss: 0.9159801006317139, Accuracy: 1.0, Computation time: 0.8770477771759033\n",
      "Step: 861, Loss: 0.9384297728538513, Accuracy: 0.96875, Computation time: 0.9269254207611084\n",
      "Step: 862, Loss: 0.9160818457603455, Accuracy: 1.0, Computation time: 1.0167086124420166\n",
      "Step: 863, Loss: 0.9160536527633667, Accuracy: 1.0, Computation time: 0.8433084487915039\n",
      "Step: 864, Loss: 0.9167994856834412, Accuracy: 1.0, Computation time: 1.3577558994293213\n",
      "Step: 865, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 0.8005144596099854\n",
      "Step: 866, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 0.8378355503082275\n",
      "Step: 867, Loss: 0.9160363674163818, Accuracy: 1.0, Computation time: 0.9123106002807617\n",
      "Step: 868, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 0.8177821636199951\n",
      "Step: 869, Loss: 0.9160005450248718, Accuracy: 1.0, Computation time: 1.0604994297027588\n",
      "Step: 870, Loss: 0.9177478551864624, Accuracy: 1.0, Computation time: 0.8350424766540527\n",
      "Step: 871, Loss: 0.9159441590309143, Accuracy: 1.0, Computation time: 0.7720026969909668\n",
      "Step: 872, Loss: 0.9159722328186035, Accuracy: 1.0, Computation time: 0.9164478778839111\n",
      "Step: 873, Loss: 0.9162986278533936, Accuracy: 1.0, Computation time: 0.8520216941833496\n",
      "Step: 874, Loss: 0.9159936308860779, Accuracy: 1.0, Computation time: 1.335625171661377\n",
      "Step: 875, Loss: 0.9160794615745544, Accuracy: 1.0, Computation time: 0.8607375621795654\n",
      "Step: 876, Loss: 0.9375936985015869, Accuracy: 0.96875, Computation time: 1.2216899394989014\n",
      "Step: 877, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 0.8084197044372559\n",
      "Step: 878, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.8616905212402344\n",
      "Step: 879, Loss: 0.9381763339042664, Accuracy: 0.96875, Computation time: 1.0451264381408691\n",
      "Step: 880, Loss: 0.9160614609718323, Accuracy: 1.0, Computation time: 0.8180453777313232\n",
      "Step: 881, Loss: 0.9373493790626526, Accuracy: 0.96875, Computation time: 1.2397398948669434\n",
      "Step: 882, Loss: 0.9199250936508179, Accuracy: 1.0, Computation time: 1.3215892314910889\n",
      "Step: 883, Loss: 0.9311268925666809, Accuracy: 0.96875, Computation time: 1.2023303508758545\n",
      "Step: 884, Loss: 0.9343754649162292, Accuracy: 0.96875, Computation time: 0.7963554859161377\n",
      "Step: 885, Loss: 0.9159631133079529, Accuracy: 1.0, Computation time: 0.8432209491729736\n",
      "Step: 886, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 0.8208212852478027\n",
      "Step: 887, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 0.850649356842041\n",
      "Step: 888, Loss: 0.9159338474273682, Accuracy: 1.0, Computation time: 1.0039124488830566\n",
      "Step: 889, Loss: 0.9159530997276306, Accuracy: 1.0, Computation time: 0.8925867080688477\n",
      "Step: 890, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 1.0163655281066895\n",
      "Step: 891, Loss: 0.9159326553344727, Accuracy: 1.0, Computation time: 0.9737231731414795\n",
      "Step: 892, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 0.7802956104278564\n",
      "Step: 893, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 0.7413582801818848\n",
      "Step: 894, Loss: 0.9375911951065063, Accuracy: 0.96875, Computation time: 0.9050421714782715\n",
      "Step: 895, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 0.862884521484375\n",
      "Step: 896, Loss: 0.9417710900306702, Accuracy: 0.96875, Computation time: 0.8227872848510742\n",
      "Step: 897, Loss: 0.9255591630935669, Accuracy: 0.96875, Computation time: 0.8195152282714844\n",
      "Step: 898, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 0.9238972663879395\n",
      "Step: 899, Loss: 0.9160003662109375, Accuracy: 1.0, Computation time: 0.8024957180023193\n",
      "Step: 900, Loss: 0.9370114803314209, Accuracy: 0.96875, Computation time: 0.85113525390625\n",
      "Step: 901, Loss: 0.9202726483345032, Accuracy: 1.0, Computation time: 0.9635977745056152\n",
      "Step: 902, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 0.8223519325256348\n",
      "Step: 903, Loss: 0.9376063346862793, Accuracy: 0.96875, Computation time: 0.951819658279419\n",
      "Step: 904, Loss: 0.9167335629463196, Accuracy: 1.0, Computation time: 0.8023266792297363\n",
      "Step: 905, Loss: 0.9160058498382568, Accuracy: 1.0, Computation time: 0.934901237487793\n",
      "Step: 906, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 0.8659083843231201\n",
      "Step: 907, Loss: 0.9161074161529541, Accuracy: 1.0, Computation time: 0.9420323371887207\n",
      "Step: 908, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 0.8683071136474609\n",
      "Step: 909, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 0.8346199989318848\n",
      "Step: 910, Loss: 0.9377487301826477, Accuracy: 0.96875, Computation time: 1.0609731674194336\n",
      "Step: 911, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 0.983783483505249\n",
      "Step: 912, Loss: 0.9159693121910095, Accuracy: 1.0, Computation time: 1.027115821838379\n",
      "Step: 913, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 0.842695951461792\n",
      "Step: 914, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 0.9833495616912842\n",
      "Step: 915, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 0.8245208263397217\n",
      "Step: 916, Loss: 0.9326480627059937, Accuracy: 0.96875, Computation time: 1.1548185348510742\n",
      "Step: 917, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 0.9293229579925537\n",
      "Step: 918, Loss: 0.9164100885391235, Accuracy: 1.0, Computation time: 0.9983243942260742\n",
      "Step: 919, Loss: 0.9159687757492065, Accuracy: 1.0, Computation time: 0.820734977722168\n",
      "Step: 920, Loss: 0.9179632663726807, Accuracy: 1.0, Computation time: 0.8123683929443359\n",
      "Step: 921, Loss: 0.9160813689231873, Accuracy: 1.0, Computation time: 0.81498122215271\n",
      "Step: 922, Loss: 0.9173120856285095, Accuracy: 1.0, Computation time: 1.1106536388397217\n",
      "Step: 923, Loss: 0.9371649622917175, Accuracy: 0.96875, Computation time: 0.8838901519775391\n",
      "Step: 924, Loss: 0.9161065816879272, Accuracy: 1.0, Computation time: 0.7417886257171631\n",
      "Step: 925, Loss: 0.9181530475616455, Accuracy: 1.0, Computation time: 1.0252091884613037\n",
      "Step: 926, Loss: 0.9167841076850891, Accuracy: 1.0, Computation time: 0.9059944152832031\n",
      "Step: 927, Loss: 0.9159451127052307, Accuracy: 1.0, Computation time: 0.8694272041320801\n",
      "Step: 928, Loss: 0.9159666299819946, Accuracy: 1.0, Computation time: 0.7902095317840576\n",
      "Step: 929, Loss: 0.9160487651824951, Accuracy: 1.0, Computation time: 0.8078014850616455\n",
      "Step: 930, Loss: 0.9162329435348511, Accuracy: 1.0, Computation time: 1.9034602642059326\n",
      "Step: 931, Loss: 0.9159377217292786, Accuracy: 1.0, Computation time: 0.9656202793121338\n",
      "Step: 932, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 0.8393526077270508\n",
      "Step: 933, Loss: 0.9161046147346497, Accuracy: 1.0, Computation time: 1.072629690170288\n",
      "Step: 934, Loss: 0.9510306715965271, Accuracy: 0.96875, Computation time: 1.2051777839660645\n",
      "Step: 935, Loss: 0.9161601662635803, Accuracy: 1.0, Computation time: 0.8400764465332031\n",
      "Step: 936, Loss: 0.916111946105957, Accuracy: 1.0, Computation time: 0.960101842880249\n",
      "Step: 937, Loss: 0.9321482181549072, Accuracy: 0.96875, Computation time: 0.9361004829406738\n",
      "Step: 938, Loss: 0.9165142774581909, Accuracy: 1.0, Computation time: 0.8447933197021484\n",
      "Step: 939, Loss: 0.9162736535072327, Accuracy: 1.0, Computation time: 1.0785636901855469\n",
      "Step: 940, Loss: 0.9219697713851929, Accuracy: 1.0, Computation time: 0.9782133102416992\n",
      "Step: 941, Loss: 0.9162513613700867, Accuracy: 1.0, Computation time: 0.850893497467041\n",
      "Step: 942, Loss: 0.9166879057884216, Accuracy: 1.0, Computation time: 1.0217852592468262\n",
      "Step: 943, Loss: 0.9175163507461548, Accuracy: 1.0, Computation time: 1.0030949115753174\n",
      "Step: 944, Loss: 0.9159213900566101, Accuracy: 1.0, Computation time: 1.2127983570098877\n",
      "Step: 945, Loss: 0.9159365296363831, Accuracy: 1.0, Computation time: 0.8972806930541992\n",
      "Step: 946, Loss: 0.9160254001617432, Accuracy: 1.0, Computation time: 0.8632912635803223\n",
      "Step: 947, Loss: 0.9159996509552002, Accuracy: 1.0, Computation time: 0.8091428279876709\n",
      "Step: 948, Loss: 0.9165720343589783, Accuracy: 1.0, Computation time: 0.9774117469787598\n",
      "Step: 949, Loss: 0.9160409569740295, Accuracy: 1.0, Computation time: 0.7910275459289551\n",
      "Step: 950, Loss: 0.9214791059494019, Accuracy: 1.0, Computation time: 0.7696623802185059\n",
      "Step: 951, Loss: 0.9166178703308105, Accuracy: 1.0, Computation time: 0.9426398277282715\n",
      "Step: 952, Loss: 0.9163235425949097, Accuracy: 1.0, Computation time: 0.8092622756958008\n",
      "Step: 953, Loss: 0.9371768832206726, Accuracy: 0.96875, Computation time: 0.9215950965881348\n",
      "Step: 954, Loss: 0.9466028809547424, Accuracy: 0.96875, Computation time: 1.0319409370422363\n",
      "Step: 955, Loss: 0.9160687923431396, Accuracy: 1.0, Computation time: 1.3750550746917725\n",
      "Step: 956, Loss: 0.9161717891693115, Accuracy: 1.0, Computation time: 0.8402860164642334\n",
      "Step: 957, Loss: 0.9186520576477051, Accuracy: 1.0, Computation time: 1.1823885440826416\n",
      "Step: 958, Loss: 0.9160865545272827, Accuracy: 1.0, Computation time: 0.8796038627624512\n",
      "Step: 959, Loss: 0.9161775708198547, Accuracy: 1.0, Computation time: 0.8236277103424072\n",
      "Step: 960, Loss: 0.9169357419013977, Accuracy: 1.0, Computation time: 1.0344676971435547\n",
      "Step: 961, Loss: 0.9170538187026978, Accuracy: 1.0, Computation time: 0.9698011875152588\n",
      "Step: 962, Loss: 0.9173184633255005, Accuracy: 1.0, Computation time: 0.9610207080841064\n",
      "Step: 963, Loss: 0.9159478545188904, Accuracy: 1.0, Computation time: 0.8600642681121826\n",
      "Step: 964, Loss: 0.9373123645782471, Accuracy: 0.96875, Computation time: 0.8962430953979492\n",
      "Step: 965, Loss: 0.9441430568695068, Accuracy: 0.96875, Computation time: 1.2048985958099365\n",
      "Step: 966, Loss: 0.9159322381019592, Accuracy: 1.0, Computation time: 1.0792655944824219\n",
      "Step: 967, Loss: 0.937562108039856, Accuracy: 0.96875, Computation time: 1.145998239517212\n",
      "Step: 968, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 1.0578148365020752\n",
      "Step: 969, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 0.953284740447998\n",
      "Step: 970, Loss: 0.9160165786743164, Accuracy: 1.0, Computation time: 1.4934108257293701\n",
      "Step: 971, Loss: 0.9160087704658508, Accuracy: 1.0, Computation time: 1.1154437065124512\n",
      "Step: 972, Loss: 0.9359390735626221, Accuracy: 0.96875, Computation time: 1.3338589668273926\n",
      "Step: 973, Loss: 0.9161115884780884, Accuracy: 1.0, Computation time: 0.9980888366699219\n",
      "########################\n",
      "Test loss: 1.0714462995529175, Test Accuracy_epoch7: 0.7722384929656982\n",
      "########################\n",
      "Step: 974, Loss: 0.9178252816200256, Accuracy: 1.0, Computation time: 1.0033323764801025\n",
      "Step: 975, Loss: 0.928412675857544, Accuracy: 0.96875, Computation time: 0.967238187789917\n",
      "Step: 976, Loss: 0.9160725474357605, Accuracy: 1.0, Computation time: 0.9625651836395264\n",
      "Step: 977, Loss: 0.9159725308418274, Accuracy: 1.0, Computation time: 0.8706080913543701\n",
      "Step: 978, Loss: 0.9159749746322632, Accuracy: 1.0, Computation time: 0.8801045417785645\n",
      "Step: 979, Loss: 0.9161064624786377, Accuracy: 1.0, Computation time: 1.0827722549438477\n",
      "Step: 980, Loss: 0.9160574674606323, Accuracy: 1.0, Computation time: 1.0318882465362549\n",
      "Step: 981, Loss: 0.9593114256858826, Accuracy: 0.9375, Computation time: 0.9256510734558105\n",
      "Step: 982, Loss: 0.9376925230026245, Accuracy: 0.96875, Computation time: 0.9653689861297607\n",
      "Step: 983, Loss: 0.9161359071731567, Accuracy: 1.0, Computation time: 1.094118356704712\n",
      "Step: 984, Loss: 0.9160574078559875, Accuracy: 1.0, Computation time: 0.945465087890625\n",
      "Step: 985, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 0.8047463893890381\n",
      "Step: 986, Loss: 0.9161857962608337, Accuracy: 1.0, Computation time: 1.2534301280975342\n",
      "Step: 987, Loss: 0.9159282445907593, Accuracy: 1.0, Computation time: 0.8483006954193115\n",
      "Step: 988, Loss: 0.9376199841499329, Accuracy: 0.96875, Computation time: 0.9527945518493652\n",
      "Step: 989, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.1341643333435059\n",
      "Step: 990, Loss: 0.9160786271095276, Accuracy: 1.0, Computation time: 0.9025804996490479\n",
      "Step: 991, Loss: 0.9320512413978577, Accuracy: 0.96875, Computation time: 1.2367231845855713\n",
      "Step: 992, Loss: 0.9160786867141724, Accuracy: 1.0, Computation time: 0.9390847682952881\n",
      "Step: 993, Loss: 0.9159886240959167, Accuracy: 1.0, Computation time: 0.8545804023742676\n",
      "Step: 994, Loss: 0.9218248724937439, Accuracy: 1.0, Computation time: 1.2002906799316406\n",
      "Step: 995, Loss: 0.916535496711731, Accuracy: 1.0, Computation time: 1.2872564792633057\n",
      "Step: 996, Loss: 0.9375313520431519, Accuracy: 0.96875, Computation time: 1.232041835784912\n",
      "Step: 997, Loss: 0.9376850724220276, Accuracy: 0.96875, Computation time: 0.9724931716918945\n",
      "Step: 998, Loss: 0.9161490797996521, Accuracy: 1.0, Computation time: 1.058617115020752\n",
      "Step: 999, Loss: 0.9368912577629089, Accuracy: 0.96875, Computation time: 0.9498202800750732\n",
      "Step: 1000, Loss: 0.9160492420196533, Accuracy: 1.0, Computation time: 1.0724403858184814\n",
      "Step: 1001, Loss: 0.9176176190376282, Accuracy: 1.0, Computation time: 1.0291061401367188\n",
      "Step: 1002, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.212883472442627\n",
      "Step: 1003, Loss: 0.931520402431488, Accuracy: 0.96875, Computation time: 0.9048843383789062\n",
      "Step: 1004, Loss: 0.925373375415802, Accuracy: 1.0, Computation time: 1.2519426345825195\n",
      "Step: 1005, Loss: 0.9170773029327393, Accuracy: 1.0, Computation time: 1.1576776504516602\n",
      "Step: 1006, Loss: 0.9171872735023499, Accuracy: 1.0, Computation time: 0.9845175743103027\n",
      "Step: 1007, Loss: 0.9162184000015259, Accuracy: 1.0, Computation time: 0.8206079006195068\n",
      "Step: 1008, Loss: 0.9164519309997559, Accuracy: 1.0, Computation time: 0.9342737197875977\n",
      "Step: 1009, Loss: 0.9383358955383301, Accuracy: 0.96875, Computation time: 1.0138905048370361\n",
      "Step: 1010, Loss: 0.9161797761917114, Accuracy: 1.0, Computation time: 1.0835177898406982\n",
      "Step: 1011, Loss: 0.9160214066505432, Accuracy: 1.0, Computation time: 0.9424459934234619\n",
      "Step: 1012, Loss: 0.9161208868026733, Accuracy: 1.0, Computation time: 0.9224889278411865\n",
      "Step: 1013, Loss: 0.9302628636360168, Accuracy: 0.96875, Computation time: 0.8479969501495361\n",
      "Step: 1014, Loss: 0.9161614179611206, Accuracy: 1.0, Computation time: 0.9254105091094971\n",
      "Step: 1015, Loss: 0.9159688353538513, Accuracy: 1.0, Computation time: 0.9647562503814697\n",
      "Step: 1016, Loss: 0.9160526394844055, Accuracy: 1.0, Computation time: 0.8379127979278564\n",
      "Step: 1017, Loss: 0.916114330291748, Accuracy: 1.0, Computation time: 0.9690873622894287\n",
      "Step: 1018, Loss: 0.9165851473808289, Accuracy: 1.0, Computation time: 1.100790023803711\n",
      "Step: 1019, Loss: 0.9160861968994141, Accuracy: 1.0, Computation time: 0.880913496017456\n",
      "Step: 1020, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 0.9393925666809082\n",
      "Step: 1021, Loss: 0.9377810955047607, Accuracy: 0.96875, Computation time: 0.8635220527648926\n",
      "Step: 1022, Loss: 0.9161746501922607, Accuracy: 1.0, Computation time: 0.9938998222351074\n",
      "Step: 1023, Loss: 0.9232593774795532, Accuracy: 1.0, Computation time: 1.2181973457336426\n",
      "Step: 1024, Loss: 0.9348108768463135, Accuracy: 0.96875, Computation time: 0.9585144519805908\n",
      "Step: 1025, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 0.8567774295806885\n",
      "Step: 1026, Loss: 0.9160295128822327, Accuracy: 1.0, Computation time: 0.8141493797302246\n",
      "Step: 1027, Loss: 0.916162371635437, Accuracy: 1.0, Computation time: 1.1248745918273926\n",
      "Step: 1028, Loss: 0.915962278842926, Accuracy: 1.0, Computation time: 0.9270262718200684\n",
      "Step: 1029, Loss: 0.9163872003555298, Accuracy: 1.0, Computation time: 0.9004347324371338\n",
      "Step: 1030, Loss: 0.9376929998397827, Accuracy: 0.96875, Computation time: 0.9898395538330078\n",
      "Step: 1031, Loss: 0.9366382956504822, Accuracy: 0.96875, Computation time: 0.8971724510192871\n",
      "Step: 1032, Loss: 0.9160122275352478, Accuracy: 1.0, Computation time: 0.9778859615325928\n",
      "Step: 1033, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 0.8473000526428223\n",
      "Step: 1034, Loss: 0.9375330209732056, Accuracy: 0.96875, Computation time: 0.8088903427124023\n",
      "Step: 1035, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 0.913607120513916\n",
      "Step: 1036, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 0.8306481838226318\n",
      "Step: 1037, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 0.8407566547393799\n",
      "Step: 1038, Loss: 0.9244354367256165, Accuracy: 1.0, Computation time: 1.3341922760009766\n",
      "Step: 1039, Loss: 0.9160259366035461, Accuracy: 1.0, Computation time: 1.0681281089782715\n",
      "Step: 1040, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.1270573139190674\n",
      "Step: 1041, Loss: 0.9159412980079651, Accuracy: 1.0, Computation time: 0.9087858200073242\n",
      "Step: 1042, Loss: 0.9161733388900757, Accuracy: 1.0, Computation time: 0.9582715034484863\n",
      "Step: 1043, Loss: 0.937564492225647, Accuracy: 0.96875, Computation time: 0.841912031173706\n",
      "Step: 1044, Loss: 0.9159637093544006, Accuracy: 1.0, Computation time: 0.9068446159362793\n",
      "Step: 1045, Loss: 0.9160837531089783, Accuracy: 1.0, Computation time: 1.0520319938659668\n",
      "Step: 1046, Loss: 0.9160239696502686, Accuracy: 1.0, Computation time: 1.1210393905639648\n",
      "Step: 1047, Loss: 0.9324644804000854, Accuracy: 0.96875, Computation time: 0.9394974708557129\n",
      "Step: 1048, Loss: 0.9195810556411743, Accuracy: 1.0, Computation time: 1.8203034400939941\n",
      "Step: 1049, Loss: 0.9249362945556641, Accuracy: 1.0, Computation time: 1.346996545791626\n",
      "Step: 1050, Loss: 0.9172507524490356, Accuracy: 1.0, Computation time: 1.9851841926574707\n",
      "Step: 1051, Loss: 0.917770266532898, Accuracy: 1.0, Computation time: 0.8746962547302246\n",
      "Step: 1052, Loss: 0.9207956194877625, Accuracy: 1.0, Computation time: 1.0713365077972412\n",
      "Step: 1053, Loss: 0.9163165092468262, Accuracy: 1.0, Computation time: 1.0986618995666504\n",
      "Step: 1054, Loss: 0.9164388179779053, Accuracy: 1.0, Computation time: 1.014836072921753\n",
      "Step: 1055, Loss: 0.9163877367973328, Accuracy: 1.0, Computation time: 1.0359134674072266\n",
      "Step: 1056, Loss: 0.9160726070404053, Accuracy: 1.0, Computation time: 1.1666698455810547\n",
      "Step: 1057, Loss: 0.9347588419914246, Accuracy: 0.96875, Computation time: 1.488373041152954\n",
      "Step: 1058, Loss: 0.9380098581314087, Accuracy: 0.96875, Computation time: 1.5461506843566895\n",
      "Step: 1059, Loss: 0.9163413643836975, Accuracy: 1.0, Computation time: 1.0568768978118896\n",
      "Step: 1060, Loss: 0.9160105586051941, Accuracy: 1.0, Computation time: 1.3333477973937988\n",
      "Step: 1061, Loss: 0.9160850048065186, Accuracy: 1.0, Computation time: 1.1193811893463135\n",
      "Step: 1062, Loss: 0.9162936210632324, Accuracy: 1.0, Computation time: 0.9092557430267334\n",
      "Step: 1063, Loss: 0.916081428527832, Accuracy: 1.0, Computation time: 1.4331653118133545\n",
      "Step: 1064, Loss: 0.9169933795928955, Accuracy: 1.0, Computation time: 1.187497615814209\n",
      "Step: 1065, Loss: 0.9202884435653687, Accuracy: 1.0, Computation time: 1.1285500526428223\n",
      "Step: 1066, Loss: 0.939410388469696, Accuracy: 0.96875, Computation time: 1.2911765575408936\n",
      "Step: 1067, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 1.454101800918579\n",
      "Step: 1068, Loss: 0.915931224822998, Accuracy: 1.0, Computation time: 1.168177604675293\n",
      "Step: 1069, Loss: 0.9163553714752197, Accuracy: 1.0, Computation time: 1.1636767387390137\n",
      "Step: 1070, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 1.2207496166229248\n",
      "Step: 1071, Loss: 0.9162095189094543, Accuracy: 1.0, Computation time: 1.0073161125183105\n",
      "Step: 1072, Loss: 0.9175189733505249, Accuracy: 1.0, Computation time: 1.2964792251586914\n",
      "Step: 1073, Loss: 0.9159392714500427, Accuracy: 1.0, Computation time: 1.0231263637542725\n",
      "Step: 1074, Loss: 0.9161533117294312, Accuracy: 1.0, Computation time: 1.3317830562591553\n",
      "Step: 1075, Loss: 0.9378697276115417, Accuracy: 0.96875, Computation time: 1.1609270572662354\n",
      "Step: 1076, Loss: 0.9161508083343506, Accuracy: 1.0, Computation time: 1.356243371963501\n",
      "Step: 1077, Loss: 0.9269800782203674, Accuracy: 0.96875, Computation time: 1.0044314861297607\n",
      "Step: 1078, Loss: 0.9161964058876038, Accuracy: 1.0, Computation time: 1.1869134902954102\n",
      "Step: 1079, Loss: 0.9163591861724854, Accuracy: 1.0, Computation time: 1.72621750831604\n",
      "Step: 1080, Loss: 0.9163128137588501, Accuracy: 1.0, Computation time: 1.2184243202209473\n",
      "Step: 1081, Loss: 0.9169656038284302, Accuracy: 1.0, Computation time: 1.2222917079925537\n",
      "Step: 1082, Loss: 0.9160852432250977, Accuracy: 1.0, Computation time: 1.0788123607635498\n",
      "Step: 1083, Loss: 0.9160767793655396, Accuracy: 1.0, Computation time: 1.4184784889221191\n",
      "Step: 1084, Loss: 0.9160628318786621, Accuracy: 1.0, Computation time: 0.8991341590881348\n",
      "Step: 1085, Loss: 0.9160520434379578, Accuracy: 1.0, Computation time: 0.9324774742126465\n",
      "Step: 1086, Loss: 0.9160062670707703, Accuracy: 1.0, Computation time: 1.6760308742523193\n",
      "Step: 1087, Loss: 0.9226303100585938, Accuracy: 1.0, Computation time: 1.1608986854553223\n",
      "Step: 1088, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 0.8625414371490479\n",
      "Step: 1089, Loss: 0.9160816669464111, Accuracy: 1.0, Computation time: 0.9384026527404785\n",
      "Step: 1090, Loss: 0.9378833770751953, Accuracy: 0.96875, Computation time: 1.064892292022705\n",
      "Step: 1091, Loss: 0.9162526726722717, Accuracy: 1.0, Computation time: 1.001751184463501\n",
      "Step: 1092, Loss: 0.9367758631706238, Accuracy: 0.96875, Computation time: 1.0835585594177246\n",
      "Step: 1093, Loss: 0.9162154197692871, Accuracy: 1.0, Computation time: 0.9367291927337646\n",
      "Step: 1094, Loss: 0.9271939992904663, Accuracy: 0.96875, Computation time: 1.06211256980896\n",
      "Step: 1095, Loss: 0.9159814119338989, Accuracy: 1.0, Computation time: 1.1432969570159912\n",
      "Step: 1096, Loss: 0.9291984438896179, Accuracy: 0.96875, Computation time: 1.0743186473846436\n",
      "Step: 1097, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 1.1525516510009766\n",
      "Step: 1098, Loss: 0.9160148501396179, Accuracy: 1.0, Computation time: 1.055971384048462\n",
      "Step: 1099, Loss: 0.916031002998352, Accuracy: 1.0, Computation time: 0.7699477672576904\n",
      "Step: 1100, Loss: 0.9161229729652405, Accuracy: 1.0, Computation time: 0.8078896999359131\n",
      "Step: 1101, Loss: 0.918274462223053, Accuracy: 1.0, Computation time: 1.0911977291107178\n",
      "Step: 1102, Loss: 0.9318314790725708, Accuracy: 0.96875, Computation time: 0.838167667388916\n",
      "Step: 1103, Loss: 0.9159806966781616, Accuracy: 1.0, Computation time: 0.9032104015350342\n",
      "Step: 1104, Loss: 0.9181252717971802, Accuracy: 1.0, Computation time: 0.9632980823516846\n",
      "Step: 1105, Loss: 0.9160234928131104, Accuracy: 1.0, Computation time: 1.0060148239135742\n",
      "Step: 1106, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 0.854144811630249\n",
      "Step: 1107, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 0.9234614372253418\n",
      "Step: 1108, Loss: 0.9317419528961182, Accuracy: 0.96875, Computation time: 0.9758377075195312\n",
      "Step: 1109, Loss: 0.9159632921218872, Accuracy: 1.0, Computation time: 0.7913498878479004\n",
      "Step: 1110, Loss: 0.9180493354797363, Accuracy: 1.0, Computation time: 1.1648218631744385\n",
      "Step: 1111, Loss: 0.916163444519043, Accuracy: 1.0, Computation time: 0.8903830051422119\n",
      "Step: 1112, Loss: 0.915946364402771, Accuracy: 1.0, Computation time: 0.763582706451416\n",
      "########################\n",
      "Test loss: 1.0632680654525757, Test Accuracy_epoch8: 0.7839687466621399\n",
      "########################\n",
      "Step: 1113, Loss: 0.9160143136978149, Accuracy: 1.0, Computation time: 0.7752113342285156\n",
      "Step: 1114, Loss: 0.9376465678215027, Accuracy: 0.96875, Computation time: 0.9490742683410645\n",
      "Step: 1115, Loss: 0.9160969257354736, Accuracy: 1.0, Computation time: 0.828066349029541\n",
      "Step: 1116, Loss: 0.9160188436508179, Accuracy: 1.0, Computation time: 0.81601881980896\n",
      "Step: 1117, Loss: 0.9167370200157166, Accuracy: 1.0, Computation time: 1.1198909282684326\n",
      "Step: 1118, Loss: 0.9160407781600952, Accuracy: 1.0, Computation time: 0.8668925762176514\n",
      "Step: 1119, Loss: 0.9159322381019592, Accuracy: 1.0, Computation time: 0.835845947265625\n",
      "Step: 1120, Loss: 0.9162279963493347, Accuracy: 1.0, Computation time: 0.957564115524292\n",
      "Step: 1121, Loss: 0.9377266764640808, Accuracy: 0.96875, Computation time: 0.8598799705505371\n",
      "Step: 1122, Loss: 0.9195524454116821, Accuracy: 1.0, Computation time: 0.8644008636474609\n",
      "Step: 1123, Loss: 0.9162172675132751, Accuracy: 1.0, Computation time: 1.3086762428283691\n",
      "Step: 1124, Loss: 0.9276083707809448, Accuracy: 0.96875, Computation time: 0.8162498474121094\n",
      "Step: 1125, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.338252067565918\n",
      "Step: 1126, Loss: 0.9167496562004089, Accuracy: 1.0, Computation time: 0.9096400737762451\n",
      "Step: 1127, Loss: 0.9160847663879395, Accuracy: 1.0, Computation time: 0.7923057079315186\n",
      "Step: 1128, Loss: 0.924684464931488, Accuracy: 1.0, Computation time: 1.2156364917755127\n",
      "Step: 1129, Loss: 0.9160604476928711, Accuracy: 1.0, Computation time: 0.8018767833709717\n",
      "Step: 1130, Loss: 0.9160929918289185, Accuracy: 1.0, Computation time: 0.7708568572998047\n",
      "Step: 1131, Loss: 0.9160732626914978, Accuracy: 1.0, Computation time: 0.8592031002044678\n",
      "Step: 1132, Loss: 0.9376878142356873, Accuracy: 0.96875, Computation time: 1.0501306056976318\n",
      "Step: 1133, Loss: 0.9266519546508789, Accuracy: 0.96875, Computation time: 0.8851995468139648\n",
      "Step: 1134, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 1.1270766258239746\n",
      "Step: 1135, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.1677508354187012\n",
      "Step: 1136, Loss: 0.9164308309555054, Accuracy: 1.0, Computation time: 1.091738224029541\n",
      "Step: 1137, Loss: 0.9160680174827576, Accuracy: 1.0, Computation time: 0.8490040302276611\n",
      "Step: 1138, Loss: 0.9161605834960938, Accuracy: 1.0, Computation time: 0.8332197666168213\n",
      "Step: 1139, Loss: 0.9161875247955322, Accuracy: 1.0, Computation time: 0.9588673114776611\n",
      "Step: 1140, Loss: 0.9161490797996521, Accuracy: 1.0, Computation time: 0.8810620307922363\n",
      "Step: 1141, Loss: 0.9258140325546265, Accuracy: 0.96875, Computation time: 1.0480647087097168\n",
      "Step: 1142, Loss: 0.916079580783844, Accuracy: 1.0, Computation time: 0.9260306358337402\n",
      "Step: 1143, Loss: 0.9160786271095276, Accuracy: 1.0, Computation time: 0.9003815650939941\n",
      "Step: 1144, Loss: 0.9160311818122864, Accuracy: 1.0, Computation time: 1.0306487083435059\n",
      "Step: 1145, Loss: 0.9159974455833435, Accuracy: 1.0, Computation time: 0.9004034996032715\n",
      "Step: 1146, Loss: 0.9184629321098328, Accuracy: 1.0, Computation time: 0.964047908782959\n",
      "Step: 1147, Loss: 0.9160634279251099, Accuracy: 1.0, Computation time: 1.2806313037872314\n",
      "Step: 1148, Loss: 0.9372181296348572, Accuracy: 0.96875, Computation time: 0.9225614070892334\n",
      "Step: 1149, Loss: 0.9159994721412659, Accuracy: 1.0, Computation time: 0.9164466857910156\n",
      "Step: 1150, Loss: 0.9256435632705688, Accuracy: 0.96875, Computation time: 0.9806194305419922\n",
      "Step: 1151, Loss: 0.9165583848953247, Accuracy: 1.0, Computation time: 0.9172427654266357\n",
      "Step: 1152, Loss: 0.9159371852874756, Accuracy: 1.0, Computation time: 1.0560357570648193\n",
      "Step: 1153, Loss: 0.9161252975463867, Accuracy: 1.0, Computation time: 0.9830164909362793\n",
      "Step: 1154, Loss: 0.9160775542259216, Accuracy: 1.0, Computation time: 0.9614331722259521\n",
      "Step: 1155, Loss: 0.9160346984863281, Accuracy: 1.0, Computation time: 1.0813660621643066\n",
      "Step: 1156, Loss: 0.9162241816520691, Accuracy: 1.0, Computation time: 0.9594967365264893\n",
      "Step: 1157, Loss: 0.9381255507469177, Accuracy: 0.96875, Computation time: 0.9415900707244873\n",
      "Step: 1158, Loss: 0.9384238719940186, Accuracy: 0.96875, Computation time: 1.3659389019012451\n",
      "Step: 1159, Loss: 0.91604083776474, Accuracy: 1.0, Computation time: 1.0624353885650635\n",
      "Step: 1160, Loss: 0.9159492254257202, Accuracy: 1.0, Computation time: 0.8682394027709961\n",
      "Step: 1161, Loss: 0.9163753390312195, Accuracy: 1.0, Computation time: 0.9079697132110596\n",
      "Step: 1162, Loss: 0.9376062154769897, Accuracy: 0.96875, Computation time: 0.9261934757232666\n",
      "Step: 1163, Loss: 0.9160678386688232, Accuracy: 1.0, Computation time: 0.9605298042297363\n",
      "Step: 1164, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 0.9136371612548828\n",
      "Step: 1165, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 1.188506841659546\n",
      "Step: 1166, Loss: 0.9159241318702698, Accuracy: 1.0, Computation time: 0.9435675144195557\n",
      "Step: 1167, Loss: 0.9160711765289307, Accuracy: 1.0, Computation time: 0.8654773235321045\n",
      "Step: 1168, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 0.8765459060668945\n",
      "Step: 1169, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.1830894947052002\n",
      "Step: 1170, Loss: 0.937446117401123, Accuracy: 0.96875, Computation time: 1.121199369430542\n",
      "Step: 1171, Loss: 0.9159018993377686, Accuracy: 1.0, Computation time: 0.8440392017364502\n",
      "Step: 1172, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.104982614517212\n",
      "Step: 1173, Loss: 0.9464321136474609, Accuracy: 0.96875, Computation time: 1.3134069442749023\n",
      "Step: 1174, Loss: 0.937519907951355, Accuracy: 0.96875, Computation time: 1.1312944889068604\n",
      "Step: 1175, Loss: 0.9170049428939819, Accuracy: 1.0, Computation time: 0.8221654891967773\n",
      "Step: 1176, Loss: 0.915986955165863, Accuracy: 1.0, Computation time: 0.9994633197784424\n",
      "Step: 1177, Loss: 0.9160157442092896, Accuracy: 1.0, Computation time: 0.9909391403198242\n",
      "Step: 1178, Loss: 0.915978193283081, Accuracy: 1.0, Computation time: 0.9404284954071045\n",
      "Step: 1179, Loss: 0.9159930944442749, Accuracy: 1.0, Computation time: 1.2039093971252441\n",
      "Step: 1180, Loss: 0.9579349160194397, Accuracy: 0.9375, Computation time: 1.1646521091461182\n",
      "Step: 1181, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 1.0862722396850586\n",
      "Step: 1182, Loss: 0.9160095453262329, Accuracy: 1.0, Computation time: 0.8989887237548828\n",
      "Step: 1183, Loss: 0.9226524233818054, Accuracy: 1.0, Computation time: 1.2675354480743408\n",
      "Step: 1184, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 1.128824234008789\n",
      "Step: 1185, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 0.953477144241333\n",
      "Step: 1186, Loss: 0.9181000590324402, Accuracy: 1.0, Computation time: 1.131819248199463\n",
      "Step: 1187, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 0.9396388530731201\n",
      "Step: 1188, Loss: 0.9159103631973267, Accuracy: 1.0, Computation time: 0.8890104293823242\n",
      "Step: 1189, Loss: 0.9159995317459106, Accuracy: 1.0, Computation time: 1.0325069427490234\n",
      "Step: 1190, Loss: 0.9166336059570312, Accuracy: 1.0, Computation time: 1.329526662826538\n",
      "Step: 1191, Loss: 0.9160400032997131, Accuracy: 1.0, Computation time: 1.0397679805755615\n",
      "Step: 1192, Loss: 0.9159693121910095, Accuracy: 1.0, Computation time: 0.9384641647338867\n",
      "Step: 1193, Loss: 0.9159629940986633, Accuracy: 1.0, Computation time: 1.0680515766143799\n",
      "Step: 1194, Loss: 0.9160259962081909, Accuracy: 1.0, Computation time: 1.091059923171997\n",
      "Step: 1195, Loss: 0.9161369204521179, Accuracy: 1.0, Computation time: 1.7213966846466064\n",
      "Step: 1196, Loss: 0.9163709282875061, Accuracy: 1.0, Computation time: 1.1592111587524414\n",
      "Step: 1197, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.1389238834381104\n",
      "Step: 1198, Loss: 0.9159972667694092, Accuracy: 1.0, Computation time: 0.9660344123840332\n",
      "Step: 1199, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.5270490646362305\n",
      "Step: 1200, Loss: 0.9219300150871277, Accuracy: 1.0, Computation time: 1.4416496753692627\n",
      "Step: 1201, Loss: 0.9193798303604126, Accuracy: 1.0, Computation time: 1.5263350009918213\n",
      "Step: 1202, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 0.971778392791748\n",
      "Step: 1203, Loss: 0.937393307685852, Accuracy: 0.96875, Computation time: 1.098466396331787\n",
      "Step: 1204, Loss: 0.9490513205528259, Accuracy: 0.9375, Computation time: 1.0337159633636475\n",
      "Step: 1205, Loss: 0.9393411874771118, Accuracy: 0.96875, Computation time: 1.0473570823669434\n",
      "Step: 1206, Loss: 0.9160360097885132, Accuracy: 1.0, Computation time: 1.0362377166748047\n",
      "Step: 1207, Loss: 0.9317378997802734, Accuracy: 0.96875, Computation time: 1.0121135711669922\n",
      "Step: 1208, Loss: 0.9160078167915344, Accuracy: 1.0, Computation time: 0.9945971965789795\n",
      "Step: 1209, Loss: 0.9162759780883789, Accuracy: 1.0, Computation time: 1.0190727710723877\n",
      "Step: 1210, Loss: 0.9163820743560791, Accuracy: 1.0, Computation time: 0.9624214172363281\n",
      "Step: 1211, Loss: 0.9164230823516846, Accuracy: 1.0, Computation time: 1.2523491382598877\n",
      "Step: 1212, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 0.9769246578216553\n",
      "Step: 1213, Loss: 0.9357486963272095, Accuracy: 0.96875, Computation time: 1.2270641326904297\n",
      "Step: 1214, Loss: 0.9159550666809082, Accuracy: 1.0, Computation time: 1.064955234527588\n",
      "Step: 1215, Loss: 0.9160057306289673, Accuracy: 1.0, Computation time: 0.9612228870391846\n",
      "Step: 1216, Loss: 0.9160236120223999, Accuracy: 1.0, Computation time: 0.8309922218322754\n",
      "Step: 1217, Loss: 0.9159931540489197, Accuracy: 1.0, Computation time: 0.8607597351074219\n",
      "Step: 1218, Loss: 0.9164000749588013, Accuracy: 1.0, Computation time: 1.5229172706604004\n",
      "Step: 1219, Loss: 0.9159293174743652, Accuracy: 1.0, Computation time: 0.8791322708129883\n",
      "Step: 1220, Loss: 0.916012704372406, Accuracy: 1.0, Computation time: 0.9068126678466797\n",
      "Step: 1221, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 0.9173052310943604\n",
      "Step: 1222, Loss: 0.9159401059150696, Accuracy: 1.0, Computation time: 0.8632607460021973\n",
      "Step: 1223, Loss: 0.9159368276596069, Accuracy: 1.0, Computation time: 0.8769493103027344\n",
      "Step: 1224, Loss: 0.9373095631599426, Accuracy: 0.96875, Computation time: 1.1473119258880615\n",
      "Step: 1225, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.0392003059387207\n",
      "Step: 1226, Loss: 0.9160091876983643, Accuracy: 1.0, Computation time: 0.8903796672821045\n",
      "Step: 1227, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.9465587139129639\n",
      "Step: 1228, Loss: 0.9371692538261414, Accuracy: 0.96875, Computation time: 1.2915136814117432\n",
      "Step: 1229, Loss: 0.9173368811607361, Accuracy: 1.0, Computation time: 0.9181175231933594\n",
      "Step: 1230, Loss: 0.9160714149475098, Accuracy: 1.0, Computation time: 0.9114749431610107\n",
      "Step: 1231, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.0281774997711182\n",
      "Step: 1232, Loss: 0.916132390499115, Accuracy: 1.0, Computation time: 0.9133398532867432\n",
      "Step: 1233, Loss: 0.9159327745437622, Accuracy: 1.0, Computation time: 0.9519703388214111\n",
      "Step: 1234, Loss: 0.9159705638885498, Accuracy: 1.0, Computation time: 0.8392751216888428\n",
      "Step: 1235, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 0.9099774360656738\n",
      "Step: 1236, Loss: 0.9377153515815735, Accuracy: 0.96875, Computation time: 0.9166865348815918\n",
      "Step: 1237, Loss: 0.9171196818351746, Accuracy: 1.0, Computation time: 0.8514676094055176\n",
      "Step: 1238, Loss: 0.9160066246986389, Accuracy: 1.0, Computation time: 1.0619628429412842\n",
      "Step: 1239, Loss: 0.9169518947601318, Accuracy: 1.0, Computation time: 0.874323844909668\n",
      "Step: 1240, Loss: 0.9159509539604187, Accuracy: 1.0, Computation time: 1.101330280303955\n",
      "Step: 1241, Loss: 0.9159848093986511, Accuracy: 1.0, Computation time: 0.9230365753173828\n",
      "Step: 1242, Loss: 0.9161622524261475, Accuracy: 1.0, Computation time: 1.1340694427490234\n",
      "Step: 1243, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 0.8371636867523193\n",
      "Step: 1244, Loss: 0.936837375164032, Accuracy: 0.96875, Computation time: 0.8738796710968018\n",
      "Step: 1245, Loss: 0.9163095951080322, Accuracy: 1.0, Computation time: 0.8314764499664307\n",
      "Step: 1246, Loss: 0.9162351489067078, Accuracy: 1.0, Computation time: 1.0953469276428223\n",
      "Step: 1247, Loss: 0.9161191582679749, Accuracy: 1.0, Computation time: 0.8675005435943604\n",
      "Step: 1248, Loss: 0.9159250855445862, Accuracy: 1.0, Computation time: 0.8553907871246338\n",
      "Step: 1249, Loss: 0.9159911274909973, Accuracy: 1.0, Computation time: 1.0562033653259277\n",
      "Step: 1250, Loss: 0.9352070093154907, Accuracy: 0.96875, Computation time: 0.8666262626647949\n",
      "Step: 1251, Loss: 0.9159073233604431, Accuracy: 1.0, Computation time: 0.9786791801452637\n",
      "########################\n",
      "Test loss: 1.0683645009994507, Test Accuracy_epoch9: 0.7771260738372803\n",
      "########################\n",
      "Step: 1252, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 0.9041702747344971\n",
      "Step: 1253, Loss: 0.9159553647041321, Accuracy: 1.0, Computation time: 0.8692190647125244\n",
      "Step: 1254, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 0.8936052322387695\n",
      "Step: 1255, Loss: 0.9159458875656128, Accuracy: 1.0, Computation time: 0.8413684368133545\n",
      "Step: 1256, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 0.9940791130065918\n",
      "Step: 1257, Loss: 0.9217303395271301, Accuracy: 1.0, Computation time: 1.1395771503448486\n",
      "Step: 1258, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 0.9699323177337646\n",
      "Step: 1259, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 0.960059642791748\n",
      "Step: 1260, Loss: 0.9164162278175354, Accuracy: 1.0, Computation time: 1.0995609760284424\n",
      "Step: 1261, Loss: 0.9159993529319763, Accuracy: 1.0, Computation time: 1.0405707359313965\n",
      "Step: 1262, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 0.8937323093414307\n",
      "Step: 1263, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.86763596534729\n",
      "Step: 1264, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 1.0569496154785156\n",
      "Step: 1265, Loss: 0.937423050403595, Accuracy: 0.96875, Computation time: 0.9236855506896973\n",
      "Step: 1266, Loss: 0.9160216450691223, Accuracy: 1.0, Computation time: 1.1146628856658936\n",
      "Step: 1267, Loss: 0.9372936487197876, Accuracy: 0.96875, Computation time: 0.9584105014801025\n",
      "Step: 1268, Loss: 0.9304014444351196, Accuracy: 0.96875, Computation time: 1.3014013767242432\n",
      "Step: 1269, Loss: 0.9375646710395813, Accuracy: 0.96875, Computation time: 1.0544607639312744\n",
      "Step: 1270, Loss: 0.9160361289978027, Accuracy: 1.0, Computation time: 1.2584857940673828\n",
      "Step: 1271, Loss: 0.9165925979614258, Accuracy: 1.0, Computation time: 1.1042461395263672\n",
      "Step: 1272, Loss: 0.9162759184837341, Accuracy: 1.0, Computation time: 0.9577522277832031\n",
      "Step: 1273, Loss: 0.9209966659545898, Accuracy: 1.0, Computation time: 1.7636210918426514\n",
      "Step: 1274, Loss: 0.9159952998161316, Accuracy: 1.0, Computation time: 1.2540085315704346\n",
      "Step: 1275, Loss: 0.9385584592819214, Accuracy: 0.96875, Computation time: 1.2463269233703613\n",
      "Step: 1276, Loss: 0.9585322737693787, Accuracy: 0.9375, Computation time: 1.0998518466949463\n",
      "Step: 1277, Loss: 0.9204726219177246, Accuracy: 1.0, Computation time: 0.9180715084075928\n",
      "Step: 1278, Loss: 0.916034996509552, Accuracy: 1.0, Computation time: 0.909034013748169\n",
      "Step: 1279, Loss: 0.9162262082099915, Accuracy: 1.0, Computation time: 1.0720434188842773\n",
      "Step: 1280, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 1.348038673400879\n",
      "Step: 1281, Loss: 0.9159920811653137, Accuracy: 1.0, Computation time: 0.8824272155761719\n",
      "Step: 1282, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 0.9414875507354736\n",
      "Step: 1283, Loss: 0.9159838557243347, Accuracy: 1.0, Computation time: 0.9420886039733887\n",
      "Step: 1284, Loss: 0.9159902930259705, Accuracy: 1.0, Computation time: 1.0876052379608154\n",
      "Step: 1285, Loss: 0.9159784913063049, Accuracy: 1.0, Computation time: 0.9185323715209961\n",
      "Step: 1286, Loss: 0.9171655774116516, Accuracy: 1.0, Computation time: 0.9357016086578369\n",
      "Step: 1287, Loss: 0.9159467220306396, Accuracy: 1.0, Computation time: 0.912534236907959\n",
      "Step: 1288, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.1073031425476074\n",
      "Step: 1289, Loss: 0.9159275889396667, Accuracy: 1.0, Computation time: 0.7978198528289795\n",
      "Step: 1290, Loss: 0.9245139956474304, Accuracy: 1.0, Computation time: 1.2109036445617676\n",
      "Step: 1291, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 0.8998427391052246\n",
      "Step: 1292, Loss: 0.917718231678009, Accuracy: 1.0, Computation time: 0.8897862434387207\n",
      "Step: 1293, Loss: 0.9159737825393677, Accuracy: 1.0, Computation time: 0.919499397277832\n",
      "Step: 1294, Loss: 0.9159876704216003, Accuracy: 1.0, Computation time: 1.08424973487854\n",
      "Step: 1295, Loss: 0.9178018569946289, Accuracy: 1.0, Computation time: 0.9466562271118164\n",
      "Step: 1296, Loss: 0.9159910082817078, Accuracy: 1.0, Computation time: 1.4160845279693604\n",
      "Step: 1297, Loss: 0.915999174118042, Accuracy: 1.0, Computation time: 1.0822744369506836\n",
      "Step: 1298, Loss: 0.9159485101699829, Accuracy: 1.0, Computation time: 0.8814711570739746\n",
      "Step: 1299, Loss: 0.9159917831420898, Accuracy: 1.0, Computation time: 0.9823830127716064\n",
      "Step: 1300, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 0.8562667369842529\n",
      "Step: 1301, Loss: 0.9160044193267822, Accuracy: 1.0, Computation time: 0.9441022872924805\n",
      "Step: 1302, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.082991600036621\n",
      "Step: 1303, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 0.8115131855010986\n",
      "Step: 1304, Loss: 0.916684627532959, Accuracy: 1.0, Computation time: 1.8382244110107422\n",
      "Step: 1305, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 1.1601088047027588\n",
      "Step: 1306, Loss: 0.9159420728683472, Accuracy: 1.0, Computation time: 0.8730587959289551\n",
      "Step: 1307, Loss: 0.9159300327301025, Accuracy: 1.0, Computation time: 1.2870571613311768\n",
      "Step: 1308, Loss: 0.9170590043067932, Accuracy: 1.0, Computation time: 1.0673794746398926\n",
      "Step: 1309, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 0.8896007537841797\n",
      "Step: 1310, Loss: 0.9175460934638977, Accuracy: 1.0, Computation time: 1.1903574466705322\n",
      "Step: 1311, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 0.8403582572937012\n",
      "Step: 1312, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 0.8346166610717773\n",
      "Step: 1313, Loss: 0.9159624576568604, Accuracy: 1.0, Computation time: 1.3958585262298584\n",
      "Step: 1314, Loss: 0.916098952293396, Accuracy: 1.0, Computation time: 1.0517511367797852\n",
      "Step: 1315, Loss: 0.9384013414382935, Accuracy: 0.96875, Computation time: 1.0447382926940918\n",
      "Step: 1316, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.1470489501953125\n",
      "Step: 1317, Loss: 0.9159340858459473, Accuracy: 1.0, Computation time: 0.891228437423706\n",
      "Step: 1318, Loss: 0.9159690141677856, Accuracy: 1.0, Computation time: 0.9616880416870117\n",
      "Step: 1319, Loss: 0.9182016849517822, Accuracy: 1.0, Computation time: 1.2149357795715332\n",
      "Step: 1320, Loss: 0.9375854730606079, Accuracy: 0.96875, Computation time: 1.20015549659729\n",
      "Step: 1321, Loss: 0.9163278341293335, Accuracy: 1.0, Computation time: 1.30218505859375\n",
      "Step: 1322, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 0.921165943145752\n",
      "Step: 1323, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 0.8574526309967041\n",
      "Step: 1324, Loss: 0.9161235094070435, Accuracy: 1.0, Computation time: 1.0714116096496582\n",
      "Step: 1325, Loss: 0.9159805774688721, Accuracy: 1.0, Computation time: 1.0708203315734863\n",
      "Step: 1326, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 1.0950274467468262\n",
      "Step: 1327, Loss: 0.916509747505188, Accuracy: 1.0, Computation time: 1.0544309616088867\n",
      "Step: 1328, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.2155945301055908\n",
      "Step: 1329, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.0405964851379395\n",
      "Step: 1330, Loss: 0.9160025119781494, Accuracy: 1.0, Computation time: 1.0915868282318115\n",
      "Step: 1331, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 0.9954354763031006\n",
      "Step: 1332, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.0493431091308594\n",
      "Step: 1333, Loss: 0.9320289492607117, Accuracy: 0.96875, Computation time: 1.0993635654449463\n",
      "Step: 1334, Loss: 0.9164174199104309, Accuracy: 1.0, Computation time: 0.9804284572601318\n",
      "Step: 1335, Loss: 0.9160204529762268, Accuracy: 1.0, Computation time: 1.2945048809051514\n",
      "Step: 1336, Loss: 0.9160106778144836, Accuracy: 1.0, Computation time: 1.0214483737945557\n",
      "Step: 1337, Loss: 0.9160064458847046, Accuracy: 1.0, Computation time: 0.9374361038208008\n",
      "Step: 1338, Loss: 0.9159941673278809, Accuracy: 1.0, Computation time: 0.9775960445404053\n",
      "Step: 1339, Loss: 0.9162174463272095, Accuracy: 1.0, Computation time: 0.9955995082855225\n",
      "Step: 1340, Loss: 0.9159712791442871, Accuracy: 1.0, Computation time: 1.0948219299316406\n",
      "Step: 1341, Loss: 0.9159106612205505, Accuracy: 1.0, Computation time: 0.9859054088592529\n",
      "Step: 1342, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.0969510078430176\n",
      "Step: 1343, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.0310354232788086\n",
      "Step: 1344, Loss: 0.9163147807121277, Accuracy: 1.0, Computation time: 1.2053868770599365\n",
      "Step: 1345, Loss: 0.9159137606620789, Accuracy: 1.0, Computation time: 0.905306339263916\n",
      "Step: 1346, Loss: 0.9178057312965393, Accuracy: 1.0, Computation time: 1.1158702373504639\n",
      "Step: 1347, Loss: 0.9159848093986511, Accuracy: 1.0, Computation time: 0.852294921875\n",
      "Step: 1348, Loss: 0.9159268140792847, Accuracy: 1.0, Computation time: 0.9297575950622559\n",
      "Step: 1349, Loss: 0.9159916043281555, Accuracy: 1.0, Computation time: 1.0510149002075195\n",
      "Step: 1350, Loss: 0.9340301752090454, Accuracy: 0.96875, Computation time: 0.9140384197235107\n",
      "Step: 1351, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.1883769035339355\n",
      "Step: 1352, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 0.9636690616607666\n",
      "Step: 1353, Loss: 0.9355589151382446, Accuracy: 0.96875, Computation time: 0.9725649356842041\n",
      "Step: 1354, Loss: 0.937126100063324, Accuracy: 0.96875, Computation time: 0.9990215301513672\n",
      "Step: 1355, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.0029065608978271\n",
      "Step: 1356, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 1.1829345226287842\n",
      "Step: 1357, Loss: 0.9317676424980164, Accuracy: 0.96875, Computation time: 0.917421817779541\n",
      "Step: 1358, Loss: 0.9377220869064331, Accuracy: 0.96875, Computation time: 1.013246774673462\n",
      "Step: 1359, Loss: 0.9161450862884521, Accuracy: 1.0, Computation time: 1.1159727573394775\n",
      "Step: 1360, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.0712296962738037\n",
      "Step: 1361, Loss: 0.9159935116767883, Accuracy: 1.0, Computation time: 1.080061435699463\n",
      "Step: 1362, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 0.9293427467346191\n",
      "Step: 1363, Loss: 0.9159232378005981, Accuracy: 1.0, Computation time: 0.8562800884246826\n",
      "Step: 1364, Loss: 0.916652500629425, Accuracy: 1.0, Computation time: 1.6804578304290771\n",
      "Step: 1365, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 0.9969689846038818\n",
      "Step: 1366, Loss: 0.9159596562385559, Accuracy: 1.0, Computation time: 1.1860620975494385\n",
      "Step: 1367, Loss: 0.9159553050994873, Accuracy: 1.0, Computation time: 1.0567736625671387\n",
      "Step: 1368, Loss: 0.9175788164138794, Accuracy: 1.0, Computation time: 1.0460472106933594\n",
      "Step: 1369, Loss: 0.9372854232788086, Accuracy: 0.96875, Computation time: 1.496964931488037\n",
      "Step: 1370, Loss: 0.93407142162323, Accuracy: 0.96875, Computation time: 1.111546516418457\n",
      "Step: 1371, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 0.8674876689910889\n",
      "Step: 1372, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.1466281414031982\n",
      "Step: 1373, Loss: 0.9177994132041931, Accuracy: 1.0, Computation time: 1.0952184200286865\n",
      "Step: 1374, Loss: 0.9374480247497559, Accuracy: 0.96875, Computation time: 1.0778751373291016\n",
      "Step: 1375, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 0.8290894031524658\n",
      "Step: 1376, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 0.8867654800415039\n",
      "Step: 1377, Loss: 0.9261434078216553, Accuracy: 0.96875, Computation time: 1.1556742191314697\n",
      "Step: 1378, Loss: 0.9200352430343628, Accuracy: 1.0, Computation time: 1.1218159198760986\n",
      "Step: 1379, Loss: 0.9159132242202759, Accuracy: 1.0, Computation time: 0.8711328506469727\n",
      "Step: 1380, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.0298590660095215\n",
      "Step: 1381, Loss: 0.9375474452972412, Accuracy: 0.96875, Computation time: 0.8146464824676514\n",
      "Step: 1382, Loss: 0.9161635041236877, Accuracy: 1.0, Computation time: 1.219254970550537\n",
      "Step: 1383, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 1.260207176208496\n",
      "Step: 1384, Loss: 0.9366400241851807, Accuracy: 0.96875, Computation time: 1.0152692794799805\n",
      "Step: 1385, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 1.139345645904541\n",
      "Step: 1386, Loss: 0.9158729910850525, Accuracy: 1.0, Computation time: 1.0013298988342285\n",
      "Step: 1387, Loss: 0.9159764051437378, Accuracy: 1.0, Computation time: 1.0150277614593506\n",
      "Step: 1388, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 0.9306092262268066\n",
      "Step: 1389, Loss: 0.9166107773780823, Accuracy: 1.0, Computation time: 1.2372796535491943\n",
      "Step: 1390, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 0.8773415088653564\n",
      "########################\n",
      "Test loss: 1.069342851638794, Test Accuracy_epoch10: 0.7751710414886475\n",
      "########################\n",
      "Step: 1391, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 0.9141099452972412\n",
      "Step: 1392, Loss: 0.9377618432044983, Accuracy: 0.96875, Computation time: 0.9573571681976318\n",
      "Step: 1393, Loss: 0.937027633190155, Accuracy: 0.96875, Computation time: 1.3737685680389404\n",
      "Step: 1394, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.024482011795044\n",
      "Step: 1395, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 0.9323217868804932\n",
      "Step: 1396, Loss: 0.9375060200691223, Accuracy: 0.96875, Computation time: 0.9284772872924805\n",
      "Step: 1397, Loss: 0.9192910194396973, Accuracy: 1.0, Computation time: 1.180741548538208\n",
      "Step: 1398, Loss: 0.9159393906593323, Accuracy: 1.0, Computation time: 1.0215039253234863\n",
      "Step: 1399, Loss: 0.937554121017456, Accuracy: 0.96875, Computation time: 1.0454034805297852\n",
      "Step: 1400, Loss: 0.9375661015510559, Accuracy: 0.96875, Computation time: 0.8983867168426514\n",
      "Step: 1401, Loss: 0.9158852696418762, Accuracy: 1.0, Computation time: 0.7917196750640869\n",
      "Step: 1402, Loss: 0.9734303951263428, Accuracy: 0.90625, Computation time: 1.4032313823699951\n",
      "Step: 1403, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.0353245735168457\n",
      "Step: 1404, Loss: 0.9160405397415161, Accuracy: 1.0, Computation time: 0.9375009536743164\n",
      "Step: 1405, Loss: 0.9441372752189636, Accuracy: 0.96875, Computation time: 0.9964518547058105\n",
      "Step: 1406, Loss: 0.9171497225761414, Accuracy: 1.0, Computation time: 0.9688382148742676\n",
      "Step: 1407, Loss: 0.9367122650146484, Accuracy: 0.96875, Computation time: 1.1557600498199463\n",
      "Step: 1408, Loss: 0.9374699592590332, Accuracy: 0.96875, Computation time: 1.0700337886810303\n",
      "Step: 1409, Loss: 0.9169772267341614, Accuracy: 1.0, Computation time: 1.9250216484069824\n",
      "Step: 1410, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 1.0863018035888672\n",
      "Step: 1411, Loss: 0.9161587953567505, Accuracy: 1.0, Computation time: 0.9443445205688477\n",
      "Step: 1412, Loss: 0.9370766282081604, Accuracy: 0.96875, Computation time: 1.2885289192199707\n",
      "Step: 1413, Loss: 0.9163665175437927, Accuracy: 1.0, Computation time: 0.9499061107635498\n",
      "Step: 1414, Loss: 0.9162337183952332, Accuracy: 1.0, Computation time: 1.1197292804718018\n",
      "Step: 1415, Loss: 0.9159534573554993, Accuracy: 1.0, Computation time: 0.9542512893676758\n",
      "Step: 1416, Loss: 0.9160082936286926, Accuracy: 1.0, Computation time: 0.9990360736846924\n",
      "Step: 1417, Loss: 0.9160560369491577, Accuracy: 1.0, Computation time: 1.2292375564575195\n",
      "Step: 1418, Loss: 0.9178339242935181, Accuracy: 1.0, Computation time: 1.1662542819976807\n",
      "Step: 1419, Loss: 0.9159990549087524, Accuracy: 1.0, Computation time: 0.9571654796600342\n",
      "Step: 1420, Loss: 0.9159408807754517, Accuracy: 1.0, Computation time: 1.2779645919799805\n",
      "Step: 1421, Loss: 0.9161303043365479, Accuracy: 1.0, Computation time: 1.0142090320587158\n",
      "Step: 1422, Loss: 0.915976881980896, Accuracy: 1.0, Computation time: 0.823237419128418\n",
      "Step: 1423, Loss: 0.9160150289535522, Accuracy: 1.0, Computation time: 0.8287777900695801\n",
      "Step: 1424, Loss: 0.9159849882125854, Accuracy: 1.0, Computation time: 1.0161237716674805\n",
      "Step: 1425, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 0.9553225040435791\n",
      "Step: 1426, Loss: 0.9377362132072449, Accuracy: 0.96875, Computation time: 0.8785076141357422\n",
      "Step: 1427, Loss: 0.9159442782402039, Accuracy: 1.0, Computation time: 0.8160138130187988\n",
      "Step: 1428, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 0.8838131427764893\n",
      "Step: 1429, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 0.946526288986206\n",
      "Step: 1430, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 0.9334716796875\n",
      "Step: 1431, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 0.8676271438598633\n",
      "Step: 1432, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 0.9218776226043701\n",
      "Step: 1433, Loss: 0.9461866021156311, Accuracy: 0.96875, Computation time: 1.0228118896484375\n",
      "Step: 1434, Loss: 0.9165863394737244, Accuracy: 1.0, Computation time: 1.182354211807251\n",
      "Step: 1435, Loss: 0.9254969358444214, Accuracy: 0.96875, Computation time: 1.2230632305145264\n",
      "Step: 1436, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 0.9711134433746338\n",
      "Step: 1437, Loss: 0.9160237312316895, Accuracy: 1.0, Computation time: 0.9257490634918213\n",
      "Step: 1438, Loss: 0.9161266088485718, Accuracy: 1.0, Computation time: 1.0380942821502686\n",
      "Step: 1439, Loss: 0.9164738059043884, Accuracy: 1.0, Computation time: 1.6047749519348145\n",
      "Step: 1440, Loss: 0.9362268447875977, Accuracy: 0.96875, Computation time: 0.9451143741607666\n",
      "Step: 1441, Loss: 0.930667519569397, Accuracy: 0.96875, Computation time: 1.5958409309387207\n",
      "Step: 1442, Loss: 0.9372977018356323, Accuracy: 0.96875, Computation time: 1.0963528156280518\n",
      "Step: 1443, Loss: 0.930047869682312, Accuracy: 0.96875, Computation time: 1.406996250152588\n",
      "Step: 1444, Loss: 0.9161329865455627, Accuracy: 1.0, Computation time: 1.575678825378418\n",
      "Step: 1445, Loss: 0.9161388278007507, Accuracy: 1.0, Computation time: 1.1321394443511963\n",
      "Step: 1446, Loss: 0.9161972403526306, Accuracy: 1.0, Computation time: 0.9725637435913086\n",
      "Step: 1447, Loss: 0.9162333607673645, Accuracy: 1.0, Computation time: 1.072857141494751\n",
      "Step: 1448, Loss: 0.9160842895507812, Accuracy: 1.0, Computation time: 0.9823825359344482\n",
      "Step: 1449, Loss: 0.9162107110023499, Accuracy: 1.0, Computation time: 0.9061617851257324\n",
      "Step: 1450, Loss: 0.9334977269172668, Accuracy: 0.96875, Computation time: 0.9869565963745117\n",
      "Step: 1451, Loss: 0.9159740209579468, Accuracy: 1.0, Computation time: 1.015310287475586\n",
      "Step: 1452, Loss: 0.9159699082374573, Accuracy: 1.0, Computation time: 0.8758835792541504\n",
      "Step: 1453, Loss: 0.9368691444396973, Accuracy: 0.96875, Computation time: 1.287384033203125\n",
      "Step: 1454, Loss: 0.9160692691802979, Accuracy: 1.0, Computation time: 1.0450677871704102\n",
      "Step: 1455, Loss: 0.921471118927002, Accuracy: 1.0, Computation time: 1.15834379196167\n",
      "Step: 1456, Loss: 0.916050374507904, Accuracy: 1.0, Computation time: 0.8828206062316895\n",
      "Step: 1457, Loss: 0.9164509773254395, Accuracy: 1.0, Computation time: 0.9466800689697266\n",
      "Step: 1458, Loss: 0.9373838901519775, Accuracy: 0.96875, Computation time: 0.9268360137939453\n",
      "Step: 1459, Loss: 0.916599452495575, Accuracy: 1.0, Computation time: 1.3954226970672607\n",
      "Step: 1460, Loss: 0.916104793548584, Accuracy: 1.0, Computation time: 0.8437237739562988\n",
      "Step: 1461, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 0.8131370544433594\n",
      "Step: 1462, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 0.7900762557983398\n",
      "Step: 1463, Loss: 0.9160749912261963, Accuracy: 1.0, Computation time: 1.138474702835083\n",
      "Step: 1464, Loss: 0.9159497022628784, Accuracy: 1.0, Computation time: 0.8320431709289551\n",
      "Step: 1465, Loss: 0.9160025119781494, Accuracy: 1.0, Computation time: 0.8394901752471924\n",
      "Step: 1466, Loss: 0.915935218334198, Accuracy: 1.0, Computation time: 0.9219117164611816\n",
      "Step: 1467, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 0.8280670642852783\n",
      "Step: 1468, Loss: 0.9319370985031128, Accuracy: 0.96875, Computation time: 1.0791943073272705\n",
      "Step: 1469, Loss: 0.9160313010215759, Accuracy: 1.0, Computation time: 1.1355836391448975\n",
      "Step: 1470, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 0.9994823932647705\n",
      "Step: 1471, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 1.0825862884521484\n",
      "Step: 1472, Loss: 0.916860044002533, Accuracy: 1.0, Computation time: 1.121342658996582\n",
      "Step: 1473, Loss: 0.9376852512359619, Accuracy: 0.96875, Computation time: 1.0736331939697266\n",
      "Step: 1474, Loss: 0.9159900546073914, Accuracy: 1.0, Computation time: 0.8440463542938232\n",
      "Step: 1475, Loss: 0.9302731156349182, Accuracy: 0.96875, Computation time: 1.3487062454223633\n",
      "Step: 1476, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 0.9161770343780518\n",
      "Step: 1477, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 1.084136724472046\n",
      "Step: 1478, Loss: 0.916032612323761, Accuracy: 1.0, Computation time: 0.8733737468719482\n",
      "Step: 1479, Loss: 0.9163753986358643, Accuracy: 1.0, Computation time: 0.9782180786132812\n",
      "Step: 1480, Loss: 0.9230201840400696, Accuracy: 1.0, Computation time: 0.8815784454345703\n",
      "Step: 1481, Loss: 0.916109561920166, Accuracy: 1.0, Computation time: 1.0043644905090332\n",
      "Step: 1482, Loss: 0.9160581231117249, Accuracy: 1.0, Computation time: 0.9703652858734131\n",
      "Step: 1483, Loss: 0.9163908362388611, Accuracy: 1.0, Computation time: 1.6679461002349854\n",
      "Step: 1484, Loss: 0.92188560962677, Accuracy: 1.0, Computation time: 1.2547903060913086\n",
      "Step: 1485, Loss: 0.9247945547103882, Accuracy: 1.0, Computation time: 1.0328130722045898\n",
      "Step: 1486, Loss: 0.9162182211875916, Accuracy: 1.0, Computation time: 0.9931845664978027\n",
      "Step: 1487, Loss: 0.9377398490905762, Accuracy: 0.96875, Computation time: 0.8900022506713867\n",
      "Step: 1488, Loss: 0.9159952998161316, Accuracy: 1.0, Computation time: 1.1021089553833008\n",
      "Step: 1489, Loss: 0.9159489870071411, Accuracy: 1.0, Computation time: 0.8168268203735352\n",
      "Step: 1490, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 0.9819200038909912\n",
      "Step: 1491, Loss: 0.9168340563774109, Accuracy: 1.0, Computation time: 1.0285158157348633\n",
      "Step: 1492, Loss: 0.9159793257713318, Accuracy: 1.0, Computation time: 0.9034724235534668\n",
      "Step: 1493, Loss: 0.9168049097061157, Accuracy: 1.0, Computation time: 1.2158312797546387\n",
      "Step: 1494, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 0.987095832824707\n",
      "Step: 1495, Loss: 0.9386669993400574, Accuracy: 0.96875, Computation time: 1.0795888900756836\n",
      "Step: 1496, Loss: 0.9160600900650024, Accuracy: 1.0, Computation time: 1.121760606765747\n",
      "Step: 1497, Loss: 0.9159665703773499, Accuracy: 1.0, Computation time: 1.033658504486084\n",
      "Step: 1498, Loss: 0.9160026907920837, Accuracy: 1.0, Computation time: 0.8561453819274902\n",
      "Step: 1499, Loss: 0.916002094745636, Accuracy: 1.0, Computation time: 1.0554254055023193\n",
      "Step: 1500, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.0666649341583252\n",
      "Step: 1501, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 0.8940784931182861\n",
      "Step: 1502, Loss: 0.9375522136688232, Accuracy: 0.96875, Computation time: 0.9202525615692139\n",
      "Step: 1503, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 0.9732282161712646\n",
      "Step: 1504, Loss: 0.9160840511322021, Accuracy: 1.0, Computation time: 0.8454737663269043\n",
      "Step: 1505, Loss: 0.9159786105155945, Accuracy: 1.0, Computation time: 1.9809811115264893\n",
      "Step: 1506, Loss: 0.9159956574440002, Accuracy: 1.0, Computation time: 0.9773480892181396\n",
      "Step: 1507, Loss: 0.9174623489379883, Accuracy: 1.0, Computation time: 0.9852414131164551\n",
      "Step: 1508, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 0.8050491809844971\n",
      "Step: 1509, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 1.0061919689178467\n",
      "Step: 1510, Loss: 0.9162985682487488, Accuracy: 1.0, Computation time: 1.0541107654571533\n",
      "Step: 1511, Loss: 0.9338842034339905, Accuracy: 0.96875, Computation time: 1.1384978294372559\n",
      "Step: 1512, Loss: 0.9377764463424683, Accuracy: 0.96875, Computation time: 1.1058402061462402\n",
      "Step: 1513, Loss: 0.9181879162788391, Accuracy: 1.0, Computation time: 1.0492451190948486\n",
      "Step: 1514, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 0.9557902812957764\n",
      "Step: 1515, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.060636281967163\n",
      "Step: 1516, Loss: 0.937624454498291, Accuracy: 0.96875, Computation time: 0.8559970855712891\n",
      "Step: 1517, Loss: 0.9173130989074707, Accuracy: 1.0, Computation time: 1.1151564121246338\n",
      "Step: 1518, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 1.031306266784668\n",
      "Step: 1519, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 0.91036057472229\n",
      "Step: 1520, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 0.9747769832611084\n",
      "Step: 1521, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 1.044694185256958\n",
      "Step: 1522, Loss: 0.91899174451828, Accuracy: 1.0, Computation time: 1.1658966541290283\n",
      "Step: 1523, Loss: 0.9159231781959534, Accuracy: 1.0, Computation time: 0.8210563659667969\n",
      "Step: 1524, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.410996913909912\n",
      "Step: 1525, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 0.9597127437591553\n",
      "Step: 1526, Loss: 0.91596919298172, Accuracy: 1.0, Computation time: 0.8825845718383789\n",
      "Step: 1527, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.0044453144073486\n",
      "Step: 1528, Loss: 0.9159104228019714, Accuracy: 1.0, Computation time: 1.0738658905029297\n",
      "Step: 1529, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 1.036376714706421\n",
      "########################\n",
      "Test loss: 1.066830039024353, Test Accuracy_epoch11: 0.7781035900115967\n",
      "########################\n",
      "Step: 1530, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 1.0968575477600098\n",
      "Step: 1531, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 0.9986395835876465\n",
      "Step: 1532, Loss: 0.9160105586051941, Accuracy: 1.0, Computation time: 0.9316320419311523\n",
      "Step: 1533, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.8818681240081787\n",
      "Step: 1534, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 0.9289031028747559\n",
      "Step: 1535, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.017592430114746\n",
      "Step: 1536, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 0.9841203689575195\n",
      "Step: 1537, Loss: 0.917316734790802, Accuracy: 1.0, Computation time: 1.0045490264892578\n",
      "Step: 1538, Loss: 0.9159547686576843, Accuracy: 1.0, Computation time: 0.9722940921783447\n",
      "Step: 1539, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 0.9858512878417969\n",
      "Step: 1540, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 1.003528356552124\n",
      "Step: 1541, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.5915899276733398\n",
      "Step: 1542, Loss: 0.9159294366836548, Accuracy: 1.0, Computation time: 1.5565447807312012\n",
      "Step: 1543, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 0.9593663215637207\n",
      "Step: 1544, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.0473966598510742\n",
      "Step: 1545, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 0.9390420913696289\n",
      "Step: 1546, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 0.8179855346679688\n",
      "Step: 1547, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 1.0057244300842285\n",
      "Step: 1548, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.087291955947876\n",
      "Step: 1549, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 0.9709007740020752\n",
      "Step: 1550, Loss: 0.9374381899833679, Accuracy: 0.96875, Computation time: 1.1384141445159912\n",
      "Step: 1551, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 1.0310297012329102\n",
      "Step: 1552, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 0.9598956108093262\n",
      "Step: 1553, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 0.9442248344421387\n",
      "Step: 1554, Loss: 0.9374934434890747, Accuracy: 0.96875, Computation time: 0.8451957702636719\n",
      "Step: 1555, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.3113183975219727\n",
      "Step: 1556, Loss: 0.9374932646751404, Accuracy: 0.96875, Computation time: 1.0141842365264893\n",
      "Step: 1557, Loss: 0.9165829420089722, Accuracy: 1.0, Computation time: 0.9273085594177246\n",
      "Step: 1558, Loss: 0.9169338941574097, Accuracy: 1.0, Computation time: 1.1632449626922607\n",
      "Step: 1559, Loss: 0.9168952107429504, Accuracy: 1.0, Computation time: 1.008162498474121\n",
      "Step: 1560, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.0255818367004395\n",
      "Step: 1561, Loss: 0.9282993674278259, Accuracy: 0.96875, Computation time: 1.1977512836456299\n",
      "Step: 1562, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.0387122631072998\n",
      "Step: 1563, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 0.8835020065307617\n",
      "Step: 1564, Loss: 0.9377599954605103, Accuracy: 0.96875, Computation time: 0.9174072742462158\n",
      "Step: 1565, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 0.9894821643829346\n",
      "Step: 1566, Loss: 0.9161235094070435, Accuracy: 1.0, Computation time: 1.005483627319336\n",
      "Step: 1567, Loss: 0.9375240802764893, Accuracy: 0.96875, Computation time: 1.1269171237945557\n",
      "Step: 1568, Loss: 0.9215220212936401, Accuracy: 1.0, Computation time: 1.0828373432159424\n",
      "Step: 1569, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.043665885925293\n",
      "Step: 1570, Loss: 0.9592128396034241, Accuracy: 0.9375, Computation time: 1.0760655403137207\n",
      "Step: 1571, Loss: 0.9160740971565247, Accuracy: 1.0, Computation time: 0.9509663581848145\n",
      "Step: 1572, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.9984500408172607\n",
      "Step: 1573, Loss: 0.9373862147331238, Accuracy: 0.96875, Computation time: 1.2034001350402832\n",
      "Step: 1574, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 0.9400601387023926\n",
      "Step: 1575, Loss: 0.9440447092056274, Accuracy: 0.96875, Computation time: 0.900519609451294\n",
      "Step: 1576, Loss: 0.91595059633255, Accuracy: 1.0, Computation time: 0.9083850383758545\n",
      "Step: 1577, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 0.8746259212493896\n",
      "Step: 1578, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.350243091583252\n",
      "Step: 1579, Loss: 0.9201029539108276, Accuracy: 1.0, Computation time: 0.9629871845245361\n",
      "Step: 1580, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 0.9985389709472656\n",
      "Step: 1581, Loss: 0.9323468804359436, Accuracy: 0.96875, Computation time: 1.1132264137268066\n",
      "Step: 1582, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.0176103115081787\n",
      "Step: 1583, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 0.9743783473968506\n",
      "Step: 1584, Loss: 0.9368380308151245, Accuracy: 0.96875, Computation time: 1.103714942932129\n",
      "Step: 1585, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 1.0484859943389893\n",
      "Step: 1586, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.2828710079193115\n",
      "Step: 1587, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 1.016918659210205\n",
      "Step: 1588, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 0.9000015258789062\n",
      "Step: 1589, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 0.8589999675750732\n",
      "Step: 1590, Loss: 0.916124701499939, Accuracy: 1.0, Computation time: 0.9792807102203369\n",
      "Step: 1591, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.1167693138122559\n",
      "Step: 1592, Loss: 0.9377257227897644, Accuracy: 0.96875, Computation time: 1.0835864543914795\n",
      "Step: 1593, Loss: 0.9161877632141113, Accuracy: 1.0, Computation time: 0.9410040378570557\n",
      "Step: 1594, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.0240533351898193\n",
      "Step: 1595, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.118298053741455\n",
      "Step: 1596, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 0.924769401550293\n",
      "Step: 1597, Loss: 0.9174760580062866, Accuracy: 1.0, Computation time: 1.2341091632843018\n",
      "Step: 1598, Loss: 0.9330507516860962, Accuracy: 0.96875, Computation time: 1.1751832962036133\n",
      "Step: 1599, Loss: 0.9375994205474854, Accuracy: 0.96875, Computation time: 1.0403192043304443\n",
      "Step: 1600, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.0234849452972412\n",
      "Step: 1601, Loss: 0.937666654586792, Accuracy: 0.96875, Computation time: 0.9758305549621582\n",
      "Step: 1602, Loss: 0.9375021457672119, Accuracy: 0.96875, Computation time: 0.9457790851593018\n",
      "Step: 1603, Loss: 0.9241893291473389, Accuracy: 1.0, Computation time: 0.928318977355957\n",
      "Step: 1604, Loss: 0.9159513711929321, Accuracy: 1.0, Computation time: 0.8141396045684814\n",
      "Step: 1605, Loss: 0.9161161184310913, Accuracy: 1.0, Computation time: 1.0924088954925537\n",
      "Step: 1606, Loss: 0.9331738352775574, Accuracy: 0.96875, Computation time: 1.0446667671203613\n",
      "Step: 1607, Loss: 0.916134238243103, Accuracy: 1.0, Computation time: 0.9522562026977539\n",
      "Step: 1608, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 0.9317505359649658\n",
      "Step: 1609, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.0614092350006104\n",
      "Step: 1610, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 0.8466668128967285\n",
      "Step: 1611, Loss: 0.9159378409385681, Accuracy: 1.0, Computation time: 0.9477827548980713\n",
      "Step: 1612, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 0.9815657138824463\n",
      "Step: 1613, Loss: 0.91595858335495, Accuracy: 1.0, Computation time: 1.372145175933838\n",
      "Step: 1614, Loss: 0.9162015914916992, Accuracy: 1.0, Computation time: 0.9954767227172852\n",
      "Step: 1615, Loss: 0.9164174795150757, Accuracy: 1.0, Computation time: 1.0044291019439697\n",
      "Step: 1616, Loss: 0.9159165024757385, Accuracy: 1.0, Computation time: 1.0091588497161865\n",
      "Step: 1617, Loss: 0.9159586429595947, Accuracy: 1.0, Computation time: 1.2997016906738281\n",
      "Step: 1618, Loss: 0.9160177707672119, Accuracy: 1.0, Computation time: 0.9777498245239258\n",
      "Step: 1619, Loss: 0.9163230061531067, Accuracy: 1.0, Computation time: 0.947988748550415\n",
      "Step: 1620, Loss: 0.9387537837028503, Accuracy: 0.96875, Computation time: 1.0464575290679932\n",
      "Step: 1621, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 0.9638330936431885\n",
      "Step: 1622, Loss: 0.9347634315490723, Accuracy: 0.96875, Computation time: 0.8467063903808594\n",
      "Step: 1623, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 0.8339111804962158\n",
      "Step: 1624, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 0.97257399559021\n",
      "Step: 1625, Loss: 0.9299523234367371, Accuracy: 0.96875, Computation time: 1.061784267425537\n",
      "Step: 1626, Loss: 0.9160483479499817, Accuracy: 1.0, Computation time: 1.0675029754638672\n",
      "Step: 1627, Loss: 0.9167183637619019, Accuracy: 1.0, Computation time: 1.0363280773162842\n",
      "Step: 1628, Loss: 0.9378276467323303, Accuracy: 0.96875, Computation time: 1.0094470977783203\n",
      "Step: 1629, Loss: 0.9159780740737915, Accuracy: 1.0, Computation time: 1.0568830966949463\n",
      "Step: 1630, Loss: 0.9183706045150757, Accuracy: 1.0, Computation time: 1.300313949584961\n",
      "Step: 1631, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 0.9080095291137695\n",
      "Step: 1632, Loss: 0.9169870018959045, Accuracy: 1.0, Computation time: 1.076195478439331\n",
      "Step: 1633, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 0.8625075817108154\n",
      "Step: 1634, Loss: 0.916068971157074, Accuracy: 1.0, Computation time: 0.8868095874786377\n",
      "Step: 1635, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 1.0286951065063477\n",
      "Step: 1636, Loss: 0.9357458353042603, Accuracy: 0.96875, Computation time: 0.9414858818054199\n",
      "Step: 1637, Loss: 0.9159684777259827, Accuracy: 1.0, Computation time: 0.8676760196685791\n",
      "Step: 1638, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 0.8732194900512695\n",
      "Step: 1639, Loss: 0.9159634709358215, Accuracy: 1.0, Computation time: 1.1712884902954102\n",
      "Step: 1640, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 0.8722255229949951\n",
      "Step: 1641, Loss: 0.9161491990089417, Accuracy: 1.0, Computation time: 0.8354783058166504\n",
      "Step: 1642, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 0.8143703937530518\n",
      "Step: 1643, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 0.9010450839996338\n",
      "Step: 1644, Loss: 0.9242196679115295, Accuracy: 1.0, Computation time: 1.0097362995147705\n",
      "Step: 1645, Loss: 0.9159973859786987, Accuracy: 1.0, Computation time: 0.9583618640899658\n",
      "Step: 1646, Loss: 0.9159371852874756, Accuracy: 1.0, Computation time: 0.9342048168182373\n",
      "Step: 1647, Loss: 0.91612708568573, Accuracy: 1.0, Computation time: 0.8621771335601807\n",
      "Step: 1648, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 0.993842601776123\n",
      "Step: 1649, Loss: 0.9159592390060425, Accuracy: 1.0, Computation time: 1.026151180267334\n",
      "Step: 1650, Loss: 0.9160193204879761, Accuracy: 1.0, Computation time: 0.9343178272247314\n",
      "Step: 1651, Loss: 0.9159297347068787, Accuracy: 1.0, Computation time: 0.8622736930847168\n",
      "Step: 1652, Loss: 0.9159374237060547, Accuracy: 1.0, Computation time: 1.1291356086730957\n",
      "Step: 1653, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 0.8168807029724121\n",
      "Step: 1654, Loss: 0.9159415364265442, Accuracy: 1.0, Computation time: 0.9798910617828369\n",
      "Step: 1655, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 0.9426023960113525\n",
      "Step: 1656, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.0564298629760742\n",
      "Step: 1657, Loss: 0.9159464836120605, Accuracy: 1.0, Computation time: 0.8622934818267822\n",
      "Step: 1658, Loss: 0.9373463988304138, Accuracy: 0.96875, Computation time: 1.0300707817077637\n",
      "Step: 1659, Loss: 0.917002260684967, Accuracy: 1.0, Computation time: 0.8405904769897461\n",
      "Step: 1660, Loss: 0.916065514087677, Accuracy: 1.0, Computation time: 1.0019598007202148\n",
      "Step: 1661, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 1.1218540668487549\n",
      "Step: 1662, Loss: 0.918123185634613, Accuracy: 1.0, Computation time: 1.018617868423462\n",
      "Step: 1663, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 0.8796098232269287\n",
      "Step: 1664, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 0.8844702243804932\n",
      "Step: 1665, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.4423930644989014\n",
      "Step: 1666, Loss: 0.9159336686134338, Accuracy: 1.0, Computation time: 1.450920820236206\n",
      "Step: 1667, Loss: 0.9164726734161377, Accuracy: 1.0, Computation time: 1.0097787380218506\n",
      "Step: 1668, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 0.8353192806243896\n",
      "########################\n",
      "Test loss: 1.068041443824768, Test Accuracy_epoch12: 0.7751710414886475\n",
      "########################\n",
      "Step: 1669, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 0.907806396484375\n",
      "Step: 1670, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.010991096496582\n",
      "Step: 1671, Loss: 0.9160676002502441, Accuracy: 1.0, Computation time: 1.1527812480926514\n",
      "Step: 1672, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 0.8883349895477295\n",
      "Step: 1673, Loss: 0.9162272214889526, Accuracy: 1.0, Computation time: 0.8506650924682617\n",
      "Step: 1674, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 0.998682975769043\n",
      "Step: 1675, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 1.5948748588562012\n",
      "Step: 1676, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 0.8878169059753418\n",
      "Step: 1677, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.0195586681365967\n",
      "Step: 1678, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.066178321838379\n",
      "Step: 1679, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 0.894721508026123\n",
      "Step: 1680, Loss: 0.9233263731002808, Accuracy: 1.0, Computation time: 1.1232802867889404\n",
      "Step: 1681, Loss: 0.9257692098617554, Accuracy: 0.96875, Computation time: 0.912663459777832\n",
      "Step: 1682, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 0.9762172698974609\n",
      "Step: 1683, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.0360634326934814\n",
      "Step: 1684, Loss: 0.9203199744224548, Accuracy: 1.0, Computation time: 1.2034480571746826\n",
      "Step: 1685, Loss: 0.9160488247871399, Accuracy: 1.0, Computation time: 0.9164390563964844\n",
      "Step: 1686, Loss: 0.9161545634269714, Accuracy: 1.0, Computation time: 1.010171890258789\n",
      "Step: 1687, Loss: 0.9171980619430542, Accuracy: 1.0, Computation time: 1.0249695777893066\n",
      "Step: 1688, Loss: 0.9316835999488831, Accuracy: 0.96875, Computation time: 1.0670804977416992\n",
      "Step: 1689, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 0.962059497833252\n",
      "Step: 1690, Loss: 0.9159312844276428, Accuracy: 1.0, Computation time: 1.105531930923462\n",
      "Step: 1691, Loss: 0.9214430451393127, Accuracy: 1.0, Computation time: 1.0115375518798828\n",
      "Step: 1692, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.2812721729278564\n",
      "Step: 1693, Loss: 0.9161717891693115, Accuracy: 1.0, Computation time: 0.8745732307434082\n",
      "Step: 1694, Loss: 0.9163676500320435, Accuracy: 1.0, Computation time: 1.357954502105713\n",
      "Step: 1695, Loss: 0.9162784218788147, Accuracy: 1.0, Computation time: 0.9140691757202148\n",
      "Step: 1696, Loss: 0.9248833060264587, Accuracy: 1.0, Computation time: 1.010312557220459\n",
      "Step: 1697, Loss: 0.9160490036010742, Accuracy: 1.0, Computation time: 1.0280494689941406\n",
      "Step: 1698, Loss: 0.9163849949836731, Accuracy: 1.0, Computation time: 1.6460833549499512\n",
      "Step: 1699, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.0907628536224365\n",
      "Step: 1700, Loss: 0.9486265182495117, Accuracy: 0.9375, Computation time: 0.9966561794281006\n",
      "Step: 1701, Loss: 0.9160884618759155, Accuracy: 1.0, Computation time: 1.3244993686676025\n",
      "Step: 1702, Loss: 0.9162277579307556, Accuracy: 1.0, Computation time: 1.2421362400054932\n",
      "Step: 1703, Loss: 0.9379661083221436, Accuracy: 0.96875, Computation time: 1.1366608142852783\n",
      "Step: 1704, Loss: 0.9160441160202026, Accuracy: 1.0, Computation time: 0.8950896263122559\n",
      "Step: 1705, Loss: 0.9169658422470093, Accuracy: 1.0, Computation time: 1.0481843948364258\n",
      "Step: 1706, Loss: 0.916045606136322, Accuracy: 1.0, Computation time: 1.036092758178711\n",
      "Step: 1707, Loss: 0.9170868992805481, Accuracy: 1.0, Computation time: 1.2484967708587646\n",
      "Step: 1708, Loss: 0.9173135161399841, Accuracy: 1.0, Computation time: 1.0668413639068604\n",
      "Step: 1709, Loss: 0.9162650108337402, Accuracy: 1.0, Computation time: 1.4371509552001953\n",
      "Step: 1710, Loss: 0.9367570877075195, Accuracy: 0.96875, Computation time: 0.944425106048584\n",
      "Step: 1711, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 1.0679209232330322\n",
      "Step: 1712, Loss: 0.9160378575325012, Accuracy: 1.0, Computation time: 1.2397549152374268\n",
      "Step: 1713, Loss: 0.9161201119422913, Accuracy: 1.0, Computation time: 1.070894479751587\n",
      "Step: 1714, Loss: 0.9162102937698364, Accuracy: 1.0, Computation time: 1.137566328048706\n",
      "Step: 1715, Loss: 0.9160540103912354, Accuracy: 1.0, Computation time: 1.1481144428253174\n",
      "Step: 1716, Loss: 0.9484779238700867, Accuracy: 0.9375, Computation time: 1.68715500831604\n",
      "Step: 1717, Loss: 0.9175757169723511, Accuracy: 1.0, Computation time: 1.1080968379974365\n",
      "Step: 1718, Loss: 0.917639434337616, Accuracy: 1.0, Computation time: 1.1013257503509521\n",
      "Step: 1719, Loss: 0.9406610727310181, Accuracy: 0.96875, Computation time: 1.116429328918457\n",
      "Step: 1720, Loss: 0.9323972463607788, Accuracy: 1.0, Computation time: 1.1594595909118652\n",
      "Step: 1721, Loss: 0.9258581399917603, Accuracy: 1.0, Computation time: 1.415649652481079\n",
      "Step: 1722, Loss: 0.9206693768501282, Accuracy: 1.0, Computation time: 1.1961185932159424\n",
      "Step: 1723, Loss: 0.9186499714851379, Accuracy: 1.0, Computation time: 1.3394713401794434\n",
      "Step: 1724, Loss: 0.9195464849472046, Accuracy: 1.0, Computation time: 1.3251874446868896\n",
      "Step: 1725, Loss: 0.9239736199378967, Accuracy: 1.0, Computation time: 1.3427395820617676\n",
      "Step: 1726, Loss: 0.9375816583633423, Accuracy: 0.96875, Computation time: 1.7370102405548096\n",
      "Step: 1727, Loss: 0.9395189881324768, Accuracy: 0.96875, Computation time: 1.5908477306365967\n",
      "Step: 1728, Loss: 0.92328941822052, Accuracy: 1.0, Computation time: 1.3707237243652344\n",
      "Step: 1729, Loss: 0.9208817481994629, Accuracy: 1.0, Computation time: 1.4215013980865479\n",
      "Step: 1730, Loss: 0.9232103824615479, Accuracy: 1.0, Computation time: 1.319957971572876\n",
      "Step: 1731, Loss: 0.9174772500991821, Accuracy: 1.0, Computation time: 1.2192928791046143\n",
      "Step: 1732, Loss: 0.9410883784294128, Accuracy: 0.96875, Computation time: 1.1276569366455078\n",
      "Step: 1733, Loss: 0.9181330800056458, Accuracy: 1.0, Computation time: 1.2931125164031982\n",
      "Step: 1734, Loss: 0.9187131524085999, Accuracy: 1.0, Computation time: 1.226698637008667\n",
      "Step: 1735, Loss: 0.923026442527771, Accuracy: 1.0, Computation time: 1.0420773029327393\n",
      "Step: 1736, Loss: 0.9172316193580627, Accuracy: 1.0, Computation time: 1.0759167671203613\n",
      "Step: 1737, Loss: 0.9430878758430481, Accuracy: 0.96875, Computation time: 1.268995761871338\n",
      "Step: 1738, Loss: 0.9206690788269043, Accuracy: 1.0, Computation time: 1.34576416015625\n",
      "Step: 1739, Loss: 0.9164515733718872, Accuracy: 1.0, Computation time: 1.1427390575408936\n",
      "Step: 1740, Loss: 0.9194163084030151, Accuracy: 1.0, Computation time: 1.3497774600982666\n",
      "Step: 1741, Loss: 0.9169327616691589, Accuracy: 1.0, Computation time: 1.0535633563995361\n",
      "Step: 1742, Loss: 0.9384617209434509, Accuracy: 0.96875, Computation time: 1.1629807949066162\n",
      "Step: 1743, Loss: 0.916570782661438, Accuracy: 1.0, Computation time: 0.9845912456512451\n",
      "Step: 1744, Loss: 0.9233402609825134, Accuracy: 1.0, Computation time: 1.260185718536377\n",
      "Step: 1745, Loss: 0.9385949373245239, Accuracy: 0.96875, Computation time: 0.9457576274871826\n",
      "Step: 1746, Loss: 0.9180048108100891, Accuracy: 1.0, Computation time: 1.1521191596984863\n",
      "Step: 1747, Loss: 0.9336420893669128, Accuracy: 0.96875, Computation time: 0.9689581394195557\n",
      "Step: 1748, Loss: 0.9183581471443176, Accuracy: 1.0, Computation time: 0.8904342651367188\n",
      "Step: 1749, Loss: 0.9300345182418823, Accuracy: 0.96875, Computation time: 1.176973581314087\n",
      "Step: 1750, Loss: 0.9165377616882324, Accuracy: 1.0, Computation time: 1.1791713237762451\n",
      "Step: 1751, Loss: 0.927429735660553, Accuracy: 0.96875, Computation time: 1.1507995128631592\n",
      "Step: 1752, Loss: 0.9170132279396057, Accuracy: 1.0, Computation time: 0.8614797592163086\n",
      "Step: 1753, Loss: 0.9210777878761292, Accuracy: 1.0, Computation time: 0.9813148975372314\n",
      "Step: 1754, Loss: 0.9170731902122498, Accuracy: 1.0, Computation time: 0.9020731449127197\n",
      "Step: 1755, Loss: 0.9599087834358215, Accuracy: 0.9375, Computation time: 1.0912680625915527\n",
      "Step: 1756, Loss: 0.9176166653633118, Accuracy: 1.0, Computation time: 0.9542026519775391\n",
      "Step: 1757, Loss: 0.917397141456604, Accuracy: 1.0, Computation time: 1.6786510944366455\n",
      "Step: 1758, Loss: 0.9381036758422852, Accuracy: 0.96875, Computation time: 0.9677860736846924\n",
      "Step: 1759, Loss: 0.9162688255310059, Accuracy: 1.0, Computation time: 0.9597890377044678\n",
      "Step: 1760, Loss: 0.9169713854789734, Accuracy: 1.0, Computation time: 0.9512102603912354\n",
      "Step: 1761, Loss: 0.9165313243865967, Accuracy: 1.0, Computation time: 0.9888331890106201\n",
      "Step: 1762, Loss: 0.9168568849563599, Accuracy: 1.0, Computation time: 0.9140946865081787\n",
      "Step: 1763, Loss: 0.9277098178863525, Accuracy: 0.96875, Computation time: 1.150599479675293\n",
      "Step: 1764, Loss: 0.9163650870323181, Accuracy: 1.0, Computation time: 0.8841962814331055\n",
      "Step: 1765, Loss: 0.929061770439148, Accuracy: 0.96875, Computation time: 0.9976406097412109\n",
      "Step: 1766, Loss: 0.9165468811988831, Accuracy: 1.0, Computation time: 1.0415453910827637\n",
      "Step: 1767, Loss: 0.9168936610221863, Accuracy: 1.0, Computation time: 0.8199212551116943\n",
      "Step: 1768, Loss: 0.9173386693000793, Accuracy: 1.0, Computation time: 1.1228599548339844\n",
      "Step: 1769, Loss: 0.916332483291626, Accuracy: 1.0, Computation time: 0.9966640472412109\n",
      "Step: 1770, Loss: 0.9170337319374084, Accuracy: 1.0, Computation time: 1.0399999618530273\n",
      "Step: 1771, Loss: 0.9161623120307922, Accuracy: 1.0, Computation time: 0.932509183883667\n",
      "Step: 1772, Loss: 0.9171959161758423, Accuracy: 1.0, Computation time: 1.0526485443115234\n",
      "Step: 1773, Loss: 0.9160194396972656, Accuracy: 1.0, Computation time: 0.8644025325775146\n",
      "Step: 1774, Loss: 0.9162188172340393, Accuracy: 1.0, Computation time: 0.9294319152832031\n",
      "Step: 1775, Loss: 0.9377501606941223, Accuracy: 0.96875, Computation time: 1.2762987613677979\n",
      "Step: 1776, Loss: 0.9176990985870361, Accuracy: 1.0, Computation time: 1.0505852699279785\n",
      "Step: 1777, Loss: 0.9160186052322388, Accuracy: 1.0, Computation time: 0.9095642566680908\n",
      "Step: 1778, Loss: 0.9160579442977905, Accuracy: 1.0, Computation time: 0.9512510299682617\n",
      "Step: 1779, Loss: 0.9536596536636353, Accuracy: 0.9375, Computation time: 1.1268675327301025\n",
      "Step: 1780, Loss: 0.9161673784255981, Accuracy: 1.0, Computation time: 0.9844763278961182\n",
      "Step: 1781, Loss: 0.9161577820777893, Accuracy: 1.0, Computation time: 1.0394246578216553\n",
      "Step: 1782, Loss: 0.9267942905426025, Accuracy: 0.96875, Computation time: 0.8774175643920898\n",
      "Step: 1783, Loss: 0.9376910924911499, Accuracy: 0.96875, Computation time: 1.008788824081421\n",
      "Step: 1784, Loss: 0.9161108136177063, Accuracy: 1.0, Computation time: 0.8826446533203125\n",
      "Step: 1785, Loss: 0.9160984754562378, Accuracy: 1.0, Computation time: 0.8902485370635986\n",
      "Step: 1786, Loss: 0.9162271618843079, Accuracy: 1.0, Computation time: 0.9471328258514404\n",
      "Step: 1787, Loss: 0.915944516658783, Accuracy: 1.0, Computation time: 0.9223403930664062\n",
      "Step: 1788, Loss: 0.916103184223175, Accuracy: 1.0, Computation time: 0.9187493324279785\n",
      "Step: 1789, Loss: 0.9160621166229248, Accuracy: 1.0, Computation time: 1.1635563373565674\n",
      "Step: 1790, Loss: 0.9160065650939941, Accuracy: 1.0, Computation time: 0.9417686462402344\n",
      "Step: 1791, Loss: 0.9162747263908386, Accuracy: 1.0, Computation time: 0.9684135913848877\n",
      "Step: 1792, Loss: 0.9160881638526917, Accuracy: 1.0, Computation time: 1.0131876468658447\n",
      "Step: 1793, Loss: 0.9161108136177063, Accuracy: 1.0, Computation time: 0.9216809272766113\n",
      "Step: 1794, Loss: 0.9162042140960693, Accuracy: 1.0, Computation time: 0.9333000183105469\n",
      "Step: 1795, Loss: 0.9170045852661133, Accuracy: 1.0, Computation time: 0.9409012794494629\n",
      "Step: 1796, Loss: 0.9159886240959167, Accuracy: 1.0, Computation time: 1.0820224285125732\n",
      "Step: 1797, Loss: 0.9160205125808716, Accuracy: 1.0, Computation time: 0.9121975898742676\n",
      "Step: 1798, Loss: 0.9159743785858154, Accuracy: 1.0, Computation time: 0.7828764915466309\n",
      "Step: 1799, Loss: 0.924613356590271, Accuracy: 1.0, Computation time: 0.9535689353942871\n",
      "Step: 1800, Loss: 0.916067898273468, Accuracy: 1.0, Computation time: 0.9881956577301025\n",
      "Step: 1801, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 0.899733304977417\n",
      "Step: 1802, Loss: 0.9164690971374512, Accuracy: 1.0, Computation time: 1.1503002643585205\n",
      "Step: 1803, Loss: 0.937509298324585, Accuracy: 0.96875, Computation time: 0.8403785228729248\n",
      "Step: 1804, Loss: 0.9160833358764648, Accuracy: 1.0, Computation time: 0.9750514030456543\n",
      "Step: 1805, Loss: 0.915958046913147, Accuracy: 1.0, Computation time: 0.9945769309997559\n",
      "Step: 1806, Loss: 0.9164431095123291, Accuracy: 1.0, Computation time: 1.1570539474487305\n",
      "Step: 1807, Loss: 0.9162600636482239, Accuracy: 1.0, Computation time: 0.9440486431121826\n",
      "########################\n",
      "Test loss: 1.06761634349823, Test Accuracy_epoch13: 0.7771260738372803\n",
      "########################\n",
      "Step: 1808, Loss: 0.9164515733718872, Accuracy: 1.0, Computation time: 1.1631276607513428\n",
      "Step: 1809, Loss: 0.9159197211265564, Accuracy: 1.0, Computation time: 0.8225500583648682\n",
      "Step: 1810, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 1.0678110122680664\n",
      "Step: 1811, Loss: 0.9375050663948059, Accuracy: 0.96875, Computation time: 0.9187722206115723\n",
      "Step: 1812, Loss: 0.9174128770828247, Accuracy: 1.0, Computation time: 1.008424997329712\n",
      "Step: 1813, Loss: 0.9162325859069824, Accuracy: 1.0, Computation time: 1.0901720523834229\n",
      "Step: 1814, Loss: 0.9159353971481323, Accuracy: 1.0, Computation time: 1.0016450881958008\n",
      "Step: 1815, Loss: 0.92142254114151, Accuracy: 1.0, Computation time: 0.9728970527648926\n",
      "Step: 1816, Loss: 0.9228527545928955, Accuracy: 1.0, Computation time: 1.0456969738006592\n",
      "Step: 1817, Loss: 0.9220863580703735, Accuracy: 1.0, Computation time: 0.8970742225646973\n",
      "Step: 1818, Loss: 0.9160510897636414, Accuracy: 1.0, Computation time: 0.9596290588378906\n",
      "Step: 1819, Loss: 0.9160440564155579, Accuracy: 1.0, Computation time: 0.9125063419342041\n",
      "Step: 1820, Loss: 0.9384294748306274, Accuracy: 0.96875, Computation time: 1.263507604598999\n",
      "Step: 1821, Loss: 0.9160730242729187, Accuracy: 1.0, Computation time: 0.9939315319061279\n",
      "Step: 1822, Loss: 0.9376637935638428, Accuracy: 0.96875, Computation time: 0.8616085052490234\n",
      "Step: 1823, Loss: 0.9378303289413452, Accuracy: 0.96875, Computation time: 1.1520664691925049\n",
      "Step: 1824, Loss: 0.9164730310440063, Accuracy: 1.0, Computation time: 0.8336474895477295\n",
      "Step: 1825, Loss: 0.9176925420761108, Accuracy: 1.0, Computation time: 1.0006318092346191\n",
      "Step: 1826, Loss: 0.9168117046356201, Accuracy: 1.0, Computation time: 0.9406769275665283\n",
      "Step: 1827, Loss: 0.9160999059677124, Accuracy: 1.0, Computation time: 1.1074073314666748\n",
      "Step: 1828, Loss: 0.9160880446434021, Accuracy: 1.0, Computation time: 0.9071872234344482\n",
      "Step: 1829, Loss: 0.9161755442619324, Accuracy: 1.0, Computation time: 1.1303884983062744\n",
      "Step: 1830, Loss: 0.9161056876182556, Accuracy: 1.0, Computation time: 1.4231717586517334\n",
      "Step: 1831, Loss: 0.934766948223114, Accuracy: 0.96875, Computation time: 0.9946727752685547\n",
      "Step: 1832, Loss: 0.9160848259925842, Accuracy: 1.0, Computation time: 1.020355463027954\n",
      "Step: 1833, Loss: 0.9280743598937988, Accuracy: 0.96875, Computation time: 0.9247703552246094\n",
      "Step: 1834, Loss: 0.9159828424453735, Accuracy: 1.0, Computation time: 0.8330755233764648\n",
      "Step: 1835, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 0.9518325328826904\n",
      "Step: 1836, Loss: 0.916305422782898, Accuracy: 1.0, Computation time: 0.8859450817108154\n",
      "Step: 1837, Loss: 0.9160745143890381, Accuracy: 1.0, Computation time: 0.9111568927764893\n",
      "Step: 1838, Loss: 0.937777042388916, Accuracy: 0.96875, Computation time: 1.2143187522888184\n",
      "Step: 1839, Loss: 0.9297621250152588, Accuracy: 0.96875, Computation time: 1.022911548614502\n",
      "Step: 1840, Loss: 0.9161897897720337, Accuracy: 1.0, Computation time: 0.9332187175750732\n",
      "Step: 1841, Loss: 0.9164013266563416, Accuracy: 1.0, Computation time: 0.8619873523712158\n",
      "Step: 1842, Loss: 0.9161672592163086, Accuracy: 1.0, Computation time: 0.8263564109802246\n",
      "Step: 1843, Loss: 0.9239936470985413, Accuracy: 1.0, Computation time: 0.9163355827331543\n",
      "Step: 1844, Loss: 0.9185957312583923, Accuracy: 1.0, Computation time: 0.9851582050323486\n",
      "Step: 1845, Loss: 0.9160030484199524, Accuracy: 1.0, Computation time: 0.9353725910186768\n",
      "Step: 1846, Loss: 0.9159862995147705, Accuracy: 1.0, Computation time: 1.0217835903167725\n",
      "Step: 1847, Loss: 0.9160909056663513, Accuracy: 1.0, Computation time: 1.2205724716186523\n",
      "Step: 1848, Loss: 0.9161427617073059, Accuracy: 1.0, Computation time: 0.9882774353027344\n",
      "Step: 1849, Loss: 0.9163496494293213, Accuracy: 1.0, Computation time: 0.9663856029510498\n",
      "Step: 1850, Loss: 0.919467031955719, Accuracy: 1.0, Computation time: 0.9687201976776123\n",
      "Step: 1851, Loss: 0.9162487983703613, Accuracy: 1.0, Computation time: 0.9887213706970215\n",
      "Step: 1852, Loss: 0.9161249399185181, Accuracy: 1.0, Computation time: 0.9170763492584229\n",
      "Step: 1853, Loss: 0.9160504937171936, Accuracy: 1.0, Computation time: 0.9561643600463867\n",
      "Step: 1854, Loss: 0.9160804748535156, Accuracy: 1.0, Computation time: 0.8055627346038818\n",
      "Step: 1855, Loss: 0.9159547686576843, Accuracy: 1.0, Computation time: 1.1173126697540283\n",
      "Step: 1856, Loss: 0.915947675704956, Accuracy: 1.0, Computation time: 1.6589713096618652\n",
      "Step: 1857, Loss: 0.9160601496696472, Accuracy: 1.0, Computation time: 0.9166176319122314\n",
      "Step: 1858, Loss: 0.9161732196807861, Accuracy: 1.0, Computation time: 1.0403704643249512\n",
      "Step: 1859, Loss: 0.9212967753410339, Accuracy: 1.0, Computation time: 1.493441104888916\n",
      "Step: 1860, Loss: 0.916054368019104, Accuracy: 1.0, Computation time: 0.8886582851409912\n",
      "Step: 1861, Loss: 0.9161141514778137, Accuracy: 1.0, Computation time: 1.0864982604980469\n",
      "Step: 1862, Loss: 0.957561731338501, Accuracy: 0.9375, Computation time: 1.106180191040039\n",
      "Step: 1863, Loss: 0.9161065816879272, Accuracy: 1.0, Computation time: 0.7911155223846436\n",
      "Step: 1864, Loss: 0.9161227345466614, Accuracy: 1.0, Computation time: 0.8895895481109619\n",
      "Step: 1865, Loss: 0.9162115454673767, Accuracy: 1.0, Computation time: 1.1234831809997559\n",
      "Step: 1866, Loss: 0.9163473844528198, Accuracy: 1.0, Computation time: 1.1244454383850098\n",
      "Step: 1867, Loss: 0.9161496162414551, Accuracy: 1.0, Computation time: 1.1810283660888672\n",
      "Step: 1868, Loss: 0.9376055598258972, Accuracy: 0.96875, Computation time: 0.9082000255584717\n",
      "Step: 1869, Loss: 0.9376064538955688, Accuracy: 0.96875, Computation time: 0.8751792907714844\n",
      "Step: 1870, Loss: 0.9159627556800842, Accuracy: 1.0, Computation time: 1.2255439758300781\n",
      "Step: 1871, Loss: 0.9159523844718933, Accuracy: 1.0, Computation time: 0.7906777858734131\n",
      "Step: 1872, Loss: 0.9160163998603821, Accuracy: 1.0, Computation time: 0.9778757095336914\n",
      "Step: 1873, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 0.9018831253051758\n",
      "Step: 1874, Loss: 0.9162057042121887, Accuracy: 1.0, Computation time: 1.1127989292144775\n",
      "Step: 1875, Loss: 0.9159364104270935, Accuracy: 1.0, Computation time: 0.8449816703796387\n",
      "Step: 1876, Loss: 0.9159983992576599, Accuracy: 1.0, Computation time: 1.1275606155395508\n",
      "Step: 1877, Loss: 0.9250118732452393, Accuracy: 1.0, Computation time: 0.8815279006958008\n",
      "Step: 1878, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.2470967769622803\n",
      "Step: 1879, Loss: 0.9160565137863159, Accuracy: 1.0, Computation time: 0.9532811641693115\n",
      "Step: 1880, Loss: 0.9216359257698059, Accuracy: 1.0, Computation time: 0.9253354072570801\n",
      "Step: 1881, Loss: 0.9160050749778748, Accuracy: 1.0, Computation time: 0.8578271865844727\n",
      "Step: 1882, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 0.9281148910522461\n",
      "Step: 1883, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 0.96970534324646\n",
      "Step: 1884, Loss: 0.9159126877784729, Accuracy: 1.0, Computation time: 0.713148832321167\n",
      "Step: 1885, Loss: 0.9169157147407532, Accuracy: 1.0, Computation time: 0.9791011810302734\n",
      "Step: 1886, Loss: 0.9374101758003235, Accuracy: 0.96875, Computation time: 0.9399247169494629\n",
      "Step: 1887, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.0602002143859863\n",
      "Step: 1888, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 0.8848726749420166\n",
      "Step: 1889, Loss: 0.9159514904022217, Accuracy: 1.0, Computation time: 1.3669800758361816\n",
      "Step: 1890, Loss: 0.9169838428497314, Accuracy: 1.0, Computation time: 0.9037768840789795\n",
      "Step: 1891, Loss: 0.9413981437683105, Accuracy: 0.96875, Computation time: 0.8764007091522217\n",
      "Step: 1892, Loss: 0.9160292148590088, Accuracy: 1.0, Computation time: 1.4883933067321777\n",
      "Step: 1893, Loss: 0.9162420034408569, Accuracy: 1.0, Computation time: 0.8630688190460205\n",
      "Step: 1894, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 0.8582632541656494\n",
      "Step: 1895, Loss: 0.9159587025642395, Accuracy: 1.0, Computation time: 0.8691227436065674\n",
      "Step: 1896, Loss: 0.9375233054161072, Accuracy: 0.96875, Computation time: 0.758136510848999\n",
      "Step: 1897, Loss: 0.9352074265480042, Accuracy: 0.96875, Computation time: 0.9141459465026855\n",
      "Step: 1898, Loss: 0.916161298751831, Accuracy: 1.0, Computation time: 1.0956902503967285\n",
      "Step: 1899, Loss: 0.9360585808753967, Accuracy: 0.96875, Computation time: 0.8012733459472656\n",
      "Step: 1900, Loss: 0.9372098445892334, Accuracy: 0.96875, Computation time: 0.8905551433563232\n",
      "Step: 1901, Loss: 0.933793842792511, Accuracy: 0.96875, Computation time: 0.9517886638641357\n",
      "Step: 1902, Loss: 0.9159809350967407, Accuracy: 1.0, Computation time: 0.8060479164123535\n",
      "Step: 1903, Loss: 0.9162549376487732, Accuracy: 1.0, Computation time: 0.894794225692749\n",
      "Step: 1904, Loss: 0.9161127805709839, Accuracy: 1.0, Computation time: 0.7527370452880859\n",
      "Step: 1905, Loss: 0.9159891605377197, Accuracy: 1.0, Computation time: 0.8075695037841797\n",
      "Step: 1906, Loss: 0.9375900030136108, Accuracy: 0.96875, Computation time: 1.1405413150787354\n",
      "Step: 1907, Loss: 0.9309513568878174, Accuracy: 0.96875, Computation time: 0.8248696327209473\n",
      "Step: 1908, Loss: 0.9288332462310791, Accuracy: 0.96875, Computation time: 1.0062799453735352\n",
      "Step: 1909, Loss: 0.9167834520339966, Accuracy: 1.0, Computation time: 1.1593167781829834\n",
      "Step: 1910, Loss: 0.9295853972434998, Accuracy: 0.96875, Computation time: 1.0390217304229736\n",
      "Step: 1911, Loss: 0.9376907348632812, Accuracy: 0.96875, Computation time: 0.8851833343505859\n",
      "Step: 1912, Loss: 0.9165377616882324, Accuracy: 1.0, Computation time: 0.8654029369354248\n",
      "Step: 1913, Loss: 0.9164515137672424, Accuracy: 1.0, Computation time: 1.0806796550750732\n",
      "Step: 1914, Loss: 0.943161129951477, Accuracy: 0.96875, Computation time: 1.2638180255889893\n",
      "Step: 1915, Loss: 0.9164717197418213, Accuracy: 1.0, Computation time: 0.8022112846374512\n",
      "Step: 1916, Loss: 0.9160962700843811, Accuracy: 1.0, Computation time: 1.0124895572662354\n",
      "Step: 1917, Loss: 0.9164136648178101, Accuracy: 1.0, Computation time: 0.9515731334686279\n",
      "Step: 1918, Loss: 0.916153609752655, Accuracy: 1.0, Computation time: 0.9783163070678711\n",
      "Step: 1919, Loss: 0.9161575436592102, Accuracy: 1.0, Computation time: 1.0902886390686035\n",
      "Step: 1920, Loss: 0.9159577488899231, Accuracy: 1.0, Computation time: 0.9106087684631348\n",
      "Step: 1921, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 1.0769846439361572\n",
      "Step: 1922, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.023813009262085\n",
      "Step: 1923, Loss: 0.9170297384262085, Accuracy: 1.0, Computation time: 0.9210729598999023\n",
      "Step: 1924, Loss: 0.9159387350082397, Accuracy: 1.0, Computation time: 1.1268949508666992\n",
      "Step: 1925, Loss: 0.9392669200897217, Accuracy: 0.96875, Computation time: 1.4187381267547607\n",
      "Step: 1926, Loss: 0.9161386489868164, Accuracy: 1.0, Computation time: 0.8501427173614502\n",
      "Step: 1927, Loss: 0.9160188436508179, Accuracy: 1.0, Computation time: 0.9684317111968994\n",
      "Step: 1928, Loss: 0.9159913063049316, Accuracy: 1.0, Computation time: 1.3559112548828125\n",
      "Step: 1929, Loss: 0.9165276288986206, Accuracy: 1.0, Computation time: 1.0282342433929443\n",
      "Step: 1930, Loss: 0.9197718501091003, Accuracy: 1.0, Computation time: 0.9327292442321777\n",
      "Step: 1931, Loss: 0.9160603284835815, Accuracy: 1.0, Computation time: 1.0012338161468506\n",
      "Step: 1932, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 1.1853435039520264\n",
      "Step: 1933, Loss: 0.9160274267196655, Accuracy: 1.0, Computation time: 1.0046825408935547\n",
      "Step: 1934, Loss: 0.9159634113311768, Accuracy: 1.0, Computation time: 0.8423678874969482\n",
      "Step: 1935, Loss: 0.9168128967285156, Accuracy: 1.0, Computation time: 1.0948545932769775\n",
      "Step: 1936, Loss: 0.9159575700759888, Accuracy: 1.0, Computation time: 0.985278844833374\n",
      "Step: 1937, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.1392881870269775\n",
      "Step: 1938, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 1.0086443424224854\n",
      "Step: 1939, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 0.91162109375\n",
      "Step: 1940, Loss: 0.9161365032196045, Accuracy: 1.0, Computation time: 0.9841077327728271\n",
      "Step: 1941, Loss: 0.9376097321510315, Accuracy: 0.96875, Computation time: 0.8855564594268799\n",
      "Step: 1942, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.0542285442352295\n",
      "Step: 1943, Loss: 0.9160746335983276, Accuracy: 1.0, Computation time: 1.0201027393341064\n",
      "Step: 1944, Loss: 0.9169259667396545, Accuracy: 1.0, Computation time: 1.294177770614624\n",
      "Step: 1945, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 0.9744167327880859\n",
      "Step: 1946, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.1214094161987305\n",
      "########################\n",
      "Test loss: 1.0735063552856445, Test Accuracy_epoch14: 0.7663733959197998\n",
      "########################\n",
      "Step: 1947, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.2656686305999756\n",
      "Step: 1948, Loss: 0.9162352681159973, Accuracy: 1.0, Computation time: 1.211573600769043\n",
      "Step: 1949, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 0.8985111713409424\n",
      "Step: 1950, Loss: 0.9160957932472229, Accuracy: 1.0, Computation time: 1.1039717197418213\n",
      "Step: 1951, Loss: 0.9161074757575989, Accuracy: 1.0, Computation time: 1.043137550354004\n",
      "Step: 1952, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 0.9973647594451904\n",
      "Step: 1953, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.201673984527588\n",
      "Step: 1954, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 0.8505599498748779\n",
      "Step: 1955, Loss: 0.9159126877784729, Accuracy: 1.0, Computation time: 0.9954559803009033\n",
      "Step: 1956, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 0.9152998924255371\n",
      "Step: 1957, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 1.0699241161346436\n",
      "Step: 1958, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 0.9146552085876465\n",
      "Step: 1959, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 0.9359922409057617\n",
      "Step: 1960, Loss: 0.9159771203994751, Accuracy: 1.0, Computation time: 0.9309375286102295\n",
      "Step: 1961, Loss: 0.9159860014915466, Accuracy: 1.0, Computation time: 1.1248984336853027\n",
      "Step: 1962, Loss: 0.9373584389686584, Accuracy: 0.96875, Computation time: 1.067274570465088\n",
      "Step: 1963, Loss: 0.9376188516616821, Accuracy: 0.96875, Computation time: 1.2376422882080078\n",
      "Step: 1964, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.002519130706787\n",
      "Step: 1965, Loss: 0.9203312397003174, Accuracy: 1.0, Computation time: 1.0336854457855225\n",
      "Step: 1966, Loss: 0.9158914685249329, Accuracy: 1.0, Computation time: 1.0264511108398438\n",
      "Step: 1967, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 0.9048352241516113\n",
      "Step: 1968, Loss: 0.9160217642784119, Accuracy: 1.0, Computation time: 0.78470778465271\n",
      "Step: 1969, Loss: 0.9158967137336731, Accuracy: 1.0, Computation time: 0.9312441349029541\n",
      "Step: 1970, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 0.958953857421875\n",
      "Step: 1971, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 0.9157314300537109\n",
      "Step: 1972, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.0699272155761719\n",
      "Step: 1973, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 0.9848685264587402\n",
      "Step: 1974, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.0714058876037598\n",
      "Step: 1975, Loss: 0.9160829186439514, Accuracy: 1.0, Computation time: 1.2834486961364746\n",
      "Step: 1976, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.522878885269165\n",
      "Step: 1977, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 0.9155385494232178\n",
      "Step: 1978, Loss: 0.9165327548980713, Accuracy: 1.0, Computation time: 0.8959639072418213\n",
      "Step: 1979, Loss: 0.9321819543838501, Accuracy: 0.96875, Computation time: 1.0280418395996094\n",
      "Step: 1980, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 0.834909200668335\n",
      "Step: 1981, Loss: 0.9159424901008606, Accuracy: 1.0, Computation time: 1.1146018505096436\n",
      "Step: 1982, Loss: 0.921671450138092, Accuracy: 1.0, Computation time: 0.8957958221435547\n",
      "Step: 1983, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.0320971012115479\n",
      "Step: 1984, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.066403865814209\n",
      "Step: 1985, Loss: 0.9273604154586792, Accuracy: 0.96875, Computation time: 1.169107437133789\n",
      "Step: 1986, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.034851312637329\n",
      "Step: 1987, Loss: 0.9159662127494812, Accuracy: 1.0, Computation time: 0.99117112159729\n",
      "Step: 1988, Loss: 0.9159855246543884, Accuracy: 1.0, Computation time: 1.1381375789642334\n",
      "Step: 1989, Loss: 0.9194654822349548, Accuracy: 1.0, Computation time: 1.0429778099060059\n",
      "Step: 1990, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.3360671997070312\n",
      "Step: 1991, Loss: 0.916325032711029, Accuracy: 1.0, Computation time: 1.0154621601104736\n",
      "Step: 1992, Loss: 0.9159268140792847, Accuracy: 1.0, Computation time: 1.0457618236541748\n",
      "Step: 1993, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 0.9546768665313721\n",
      "Step: 1994, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.1203086376190186\n",
      "Step: 1995, Loss: 0.9160183072090149, Accuracy: 1.0, Computation time: 0.9657680988311768\n",
      "Step: 1996, Loss: 0.9159624576568604, Accuracy: 1.0, Computation time: 1.5730767250061035\n",
      "Step: 1997, Loss: 0.9159008264541626, Accuracy: 1.0, Computation time: 0.8746626377105713\n",
      "Step: 1998, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.1208746433258057\n",
      "Step: 1999, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 0.8579835891723633\n",
      "Step: 2000, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 0.9726755619049072\n",
      "Step: 2001, Loss: 0.9158833026885986, Accuracy: 1.0, Computation time: 1.1514759063720703\n",
      "Step: 2002, Loss: 0.915903627872467, Accuracy: 1.0, Computation time: 0.8608739376068115\n",
      "Step: 2003, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 0.9021422863006592\n",
      "Step: 2004, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 0.92905592918396\n",
      "Step: 2005, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 0.8990836143493652\n",
      "Step: 2006, Loss: 0.915982723236084, Accuracy: 1.0, Computation time: 1.1165528297424316\n",
      "Step: 2007, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.0612702369689941\n",
      "Step: 2008, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.088707447052002\n",
      "Step: 2009, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 1.209946632385254\n",
      "Step: 2010, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 0.8085658550262451\n",
      "Step: 2011, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 0.8823344707489014\n",
      "Step: 2012, Loss: 0.9409586191177368, Accuracy: 0.96875, Computation time: 0.9168922901153564\n",
      "Step: 2013, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.1527187824249268\n",
      "Step: 2014, Loss: 0.9159986972808838, Accuracy: 1.0, Computation time: 0.8973479270935059\n",
      "Step: 2015, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 0.9516911506652832\n",
      "Step: 2016, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.1640429496765137\n",
      "Step: 2017, Loss: 0.9159870743751526, Accuracy: 1.0, Computation time: 0.9881112575531006\n",
      "Step: 2018, Loss: 0.9160654544830322, Accuracy: 1.0, Computation time: 0.9129750728607178\n",
      "Step: 2019, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 0.9257717132568359\n",
      "Step: 2020, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.0483276844024658\n",
      "Step: 2021, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 0.9277050495147705\n",
      "Step: 2022, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 0.9963831901550293\n",
      "Step: 2023, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.0396265983581543\n",
      "Step: 2024, Loss: 0.9161012768745422, Accuracy: 1.0, Computation time: 1.0979158878326416\n",
      "Step: 2025, Loss: 0.9162010550498962, Accuracy: 1.0, Computation time: 1.0164117813110352\n",
      "Step: 2026, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 0.9665448665618896\n",
      "Step: 2027, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 0.9954273700714111\n",
      "Step: 2028, Loss: 0.9163767695426941, Accuracy: 1.0, Computation time: 0.7810251712799072\n",
      "Step: 2029, Loss: 0.9375845193862915, Accuracy: 0.96875, Computation time: 1.1884551048278809\n",
      "Step: 2030, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 0.7905142307281494\n",
      "Step: 2031, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 0.9034092426300049\n",
      "Step: 2032, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 0.8935153484344482\n",
      "Step: 2033, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 0.8502953052520752\n",
      "Step: 2034, Loss: 0.9216409921646118, Accuracy: 1.0, Computation time: 0.8433041572570801\n",
      "Step: 2035, Loss: 0.9160225987434387, Accuracy: 1.0, Computation time: 0.9974281787872314\n",
      "Step: 2036, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 0.8891425132751465\n",
      "Step: 2037, Loss: 0.9168041944503784, Accuracy: 1.0, Computation time: 0.9708857536315918\n",
      "Step: 2038, Loss: 0.9184532165527344, Accuracy: 1.0, Computation time: 0.9565465450286865\n",
      "Step: 2039, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.1048283576965332\n",
      "Step: 2040, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 0.875098466873169\n",
      "Step: 2041, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 0.8875634670257568\n",
      "Step: 2042, Loss: 0.9375361800193787, Accuracy: 0.96875, Computation time: 1.130674123764038\n",
      "Step: 2043, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.06489896774292\n",
      "Step: 2044, Loss: 0.9369365572929382, Accuracy: 0.96875, Computation time: 0.7952268123626709\n",
      "Step: 2045, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 0.8979389667510986\n",
      "Step: 2046, Loss: 0.9160321950912476, Accuracy: 1.0, Computation time: 1.3306586742401123\n",
      "Step: 2047, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 0.8209502696990967\n",
      "Step: 2048, Loss: 0.9375487565994263, Accuracy: 0.96875, Computation time: 0.8023059368133545\n",
      "Step: 2049, Loss: 0.9161345958709717, Accuracy: 1.0, Computation time: 0.8502786159515381\n",
      "Step: 2050, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 0.9772739410400391\n",
      "Step: 2051, Loss: 0.9167750477790833, Accuracy: 1.0, Computation time: 1.0265142917633057\n",
      "Step: 2052, Loss: 0.9326008558273315, Accuracy: 0.96875, Computation time: 0.9843685626983643\n",
      "Step: 2053, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 0.8984966278076172\n",
      "Step: 2054, Loss: 0.9178342223167419, Accuracy: 1.0, Computation time: 0.9234488010406494\n",
      "Step: 2055, Loss: 0.9294952750205994, Accuracy: 0.96875, Computation time: 0.9388084411621094\n",
      "Step: 2056, Loss: 0.9236334562301636, Accuracy: 1.0, Computation time: 0.9511761665344238\n",
      "Step: 2057, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 0.9334061145782471\n",
      "Step: 2058, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 0.8423407077789307\n",
      "Step: 2059, Loss: 0.9172298908233643, Accuracy: 1.0, Computation time: 1.085664987564087\n",
      "Step: 2060, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 0.784013032913208\n",
      "Step: 2061, Loss: 0.9323394298553467, Accuracy: 0.96875, Computation time: 0.8283891677856445\n",
      "Step: 2062, Loss: 0.9377107620239258, Accuracy: 0.96875, Computation time: 0.8294305801391602\n",
      "Step: 2063, Loss: 0.917058527469635, Accuracy: 1.0, Computation time: 0.9829623699188232\n",
      "Step: 2064, Loss: 0.9159294366836548, Accuracy: 1.0, Computation time: 0.856137752532959\n",
      "Step: 2065, Loss: 0.9236947298049927, Accuracy: 1.0, Computation time: 0.9056088924407959\n",
      "Step: 2066, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 0.8313257694244385\n",
      "Step: 2067, Loss: 0.9594024419784546, Accuracy: 0.9375, Computation time: 0.8567886352539062\n",
      "Step: 2068, Loss: 0.9376088976860046, Accuracy: 0.96875, Computation time: 0.9217455387115479\n",
      "Step: 2069, Loss: 0.9162075519561768, Accuracy: 1.0, Computation time: 1.1895225048065186\n",
      "Step: 2070, Loss: 0.9159562587738037, Accuracy: 1.0, Computation time: 0.7797932624816895\n",
      "Step: 2071, Loss: 0.9159329533576965, Accuracy: 1.0, Computation time: 0.7157058715820312\n",
      "Step: 2072, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 0.7577438354492188\n",
      "Step: 2073, Loss: 0.9161744117736816, Accuracy: 1.0, Computation time: 0.8656768798828125\n",
      "Step: 2074, Loss: 0.9317930936813354, Accuracy: 0.96875, Computation time: 0.9854493141174316\n",
      "Step: 2075, Loss: 0.9160499572753906, Accuracy: 1.0, Computation time: 0.884113073348999\n",
      "Step: 2076, Loss: 0.9376572370529175, Accuracy: 0.96875, Computation time: 0.7587060928344727\n",
      "Step: 2077, Loss: 0.9160127639770508, Accuracy: 1.0, Computation time: 0.7996058464050293\n",
      "Step: 2078, Loss: 0.9160928726196289, Accuracy: 1.0, Computation time: 1.1644682884216309\n",
      "Step: 2079, Loss: 0.9159748554229736, Accuracy: 1.0, Computation time: 0.7643070220947266\n",
      "Step: 2080, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 0.806480884552002\n",
      "Step: 2081, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 0.7637262344360352\n",
      "Step: 2082, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.6026673316955566\n",
      "Step: 2083, Loss: 0.9165589809417725, Accuracy: 1.0, Computation time: 0.8552811145782471\n",
      "Step: 2084, Loss: 0.9581896662712097, Accuracy: 0.9375, Computation time: 0.7847781181335449\n",
      "Step: 2085, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 0.8488063812255859\n",
      "########################\n",
      "Test loss: 1.066009283065796, Test Accuracy_epoch15: 0.7820136547088623\n",
      "########################\n",
      "Step: 2086, Loss: 0.916038990020752, Accuracy: 1.0, Computation time: 1.1704251766204834\n",
      "Step: 2087, Loss: 0.9160919785499573, Accuracy: 1.0, Computation time: 1.0226612091064453\n",
      "Step: 2088, Loss: 0.9159364104270935, Accuracy: 1.0, Computation time: 0.7399799823760986\n",
      "Step: 2089, Loss: 0.9159554839134216, Accuracy: 1.0, Computation time: 1.1001017093658447\n",
      "Step: 2090, Loss: 0.9160807728767395, Accuracy: 1.0, Computation time: 1.0027308464050293\n",
      "Step: 2091, Loss: 0.9186184406280518, Accuracy: 1.0, Computation time: 0.7576754093170166\n",
      "Step: 2092, Loss: 0.9192683100700378, Accuracy: 1.0, Computation time: 0.9230995178222656\n",
      "Step: 2093, Loss: 0.9410273432731628, Accuracy: 0.96875, Computation time: 1.0807015895843506\n",
      "Step: 2094, Loss: 0.915971040725708, Accuracy: 1.0, Computation time: 0.9016427993774414\n",
      "Step: 2095, Loss: 0.9162294864654541, Accuracy: 1.0, Computation time: 0.9372162818908691\n",
      "Step: 2096, Loss: 0.9196311831474304, Accuracy: 1.0, Computation time: 1.1075439453125\n",
      "Step: 2097, Loss: 0.9161325097084045, Accuracy: 1.0, Computation time: 0.776275634765625\n",
      "Step: 2098, Loss: 0.916235625743866, Accuracy: 1.0, Computation time: 1.2644786834716797\n",
      "Step: 2099, Loss: 0.9170874357223511, Accuracy: 1.0, Computation time: 0.9179573059082031\n",
      "Step: 2100, Loss: 0.9160428047180176, Accuracy: 1.0, Computation time: 0.8834483623504639\n",
      "Step: 2101, Loss: 0.916121780872345, Accuracy: 1.0, Computation time: 0.9695322513580322\n",
      "Step: 2102, Loss: 0.9377776384353638, Accuracy: 0.96875, Computation time: 1.0138885974884033\n",
      "Step: 2103, Loss: 0.915979266166687, Accuracy: 1.0, Computation time: 0.8331148624420166\n",
      "Step: 2104, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 0.8111820220947266\n",
      "Step: 2105, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 0.8160579204559326\n",
      "Step: 2106, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 0.7497143745422363\n",
      "Step: 2107, Loss: 0.9161842465400696, Accuracy: 1.0, Computation time: 0.7411854267120361\n",
      "Step: 2108, Loss: 0.9159995913505554, Accuracy: 1.0, Computation time: 0.890129566192627\n",
      "Step: 2109, Loss: 0.9159746170043945, Accuracy: 1.0, Computation time: 0.8999907970428467\n",
      "Step: 2110, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 0.8399758338928223\n",
      "Step: 2111, Loss: 0.9376786947250366, Accuracy: 0.96875, Computation time: 1.1248366832733154\n",
      "Step: 2112, Loss: 0.9166642427444458, Accuracy: 1.0, Computation time: 1.2979068756103516\n",
      "Step: 2113, Loss: 0.9161957502365112, Accuracy: 1.0, Computation time: 0.862018346786499\n",
      "Step: 2114, Loss: 0.9376813769340515, Accuracy: 0.96875, Computation time: 0.8130402565002441\n",
      "Step: 2115, Loss: 0.9163026213645935, Accuracy: 1.0, Computation time: 0.87388014793396\n",
      "Step: 2116, Loss: 0.9162198305130005, Accuracy: 1.0, Computation time: 1.0212461948394775\n",
      "Step: 2117, Loss: 0.9162071347236633, Accuracy: 1.0, Computation time: 0.8662810325622559\n",
      "Step: 2118, Loss: 0.9175834059715271, Accuracy: 1.0, Computation time: 1.0187804698944092\n",
      "Step: 2119, Loss: 0.9159335494041443, Accuracy: 1.0, Computation time: 0.91754150390625\n",
      "Step: 2120, Loss: 0.9158755540847778, Accuracy: 1.0, Computation time: 0.8106856346130371\n",
      "Step: 2121, Loss: 0.9244062304496765, Accuracy: 1.0, Computation time: 0.9901659488677979\n",
      "Step: 2122, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 0.8997480869293213\n",
      "Step: 2123, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 0.8553383350372314\n",
      "Step: 2124, Loss: 0.9159628748893738, Accuracy: 1.0, Computation time: 0.9621493816375732\n",
      "Step: 2125, Loss: 0.9340121746063232, Accuracy: 0.96875, Computation time: 0.9047141075134277\n",
      "Step: 2126, Loss: 0.9375464916229248, Accuracy: 0.96875, Computation time: 1.053030252456665\n",
      "Step: 2127, Loss: 0.9160906672477722, Accuracy: 1.0, Computation time: 0.9542052745819092\n",
      "Step: 2128, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 0.8891730308532715\n",
      "Step: 2129, Loss: 0.9160346984863281, Accuracy: 1.0, Computation time: 0.9858665466308594\n",
      "Step: 2130, Loss: 0.9160089492797852, Accuracy: 1.0, Computation time: 1.1857883930206299\n",
      "Step: 2131, Loss: 0.9159511923789978, Accuracy: 1.0, Computation time: 0.8620262145996094\n",
      "Step: 2132, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 0.8448836803436279\n",
      "Step: 2133, Loss: 0.9163695573806763, Accuracy: 1.0, Computation time: 1.0682728290557861\n",
      "Step: 2134, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.0144782066345215\n",
      "Step: 2135, Loss: 0.9159846901893616, Accuracy: 1.0, Computation time: 1.2451698780059814\n",
      "Step: 2136, Loss: 0.9159623980522156, Accuracy: 1.0, Computation time: 0.9086053371429443\n",
      "Step: 2137, Loss: 0.9159507751464844, Accuracy: 1.0, Computation time: 0.9316525459289551\n",
      "Step: 2138, Loss: 0.9159622192382812, Accuracy: 1.0, Computation time: 0.9865615367889404\n",
      "Step: 2139, Loss: 0.9160089492797852, Accuracy: 1.0, Computation time: 1.105027437210083\n",
      "Step: 2140, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 0.8552374839782715\n",
      "Step: 2141, Loss: 0.9160287976264954, Accuracy: 1.0, Computation time: 1.4041295051574707\n",
      "Step: 2142, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 0.9251782894134521\n",
      "Step: 2143, Loss: 0.9347178339958191, Accuracy: 0.96875, Computation time: 0.8875536918640137\n",
      "Step: 2144, Loss: 0.9169773459434509, Accuracy: 1.0, Computation time: 1.1746137142181396\n",
      "Step: 2145, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.037458896636963\n",
      "Step: 2146, Loss: 0.9190505146980286, Accuracy: 1.0, Computation time: 0.9334044456481934\n",
      "Step: 2147, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 0.9756977558135986\n",
      "Step: 2148, Loss: 0.9379399418830872, Accuracy: 0.96875, Computation time: 0.9429991245269775\n",
      "Step: 2149, Loss: 0.9219490885734558, Accuracy: 1.0, Computation time: 0.9333093166351318\n",
      "Step: 2150, Loss: 0.9199703931808472, Accuracy: 1.0, Computation time: 0.9871401786804199\n",
      "Step: 2151, Loss: 0.9159223437309265, Accuracy: 1.0, Computation time: 1.045780897140503\n",
      "Step: 2152, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 0.9144690036773682\n",
      "Step: 2153, Loss: 0.9165764451026917, Accuracy: 1.0, Computation time: 1.0469601154327393\n",
      "Step: 2154, Loss: 0.915956974029541, Accuracy: 1.0, Computation time: 0.9244694709777832\n",
      "Step: 2155, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.0513930320739746\n",
      "Step: 2156, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.1163465976715088\n",
      "Step: 2157, Loss: 0.9162027835845947, Accuracy: 1.0, Computation time: 1.2548832893371582\n",
      "Step: 2158, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 0.9583339691162109\n",
      "Step: 2159, Loss: 0.9205853939056396, Accuracy: 1.0, Computation time: 0.9378523826599121\n",
      "Step: 2160, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 0.8865723609924316\n",
      "Step: 2161, Loss: 0.9367645978927612, Accuracy: 0.96875, Computation time: 1.3852808475494385\n",
      "Step: 2162, Loss: 0.917487382888794, Accuracy: 1.0, Computation time: 1.278759241104126\n",
      "Step: 2163, Loss: 0.9163758158683777, Accuracy: 1.0, Computation time: 0.8825216293334961\n",
      "Step: 2164, Loss: 0.9160056114196777, Accuracy: 1.0, Computation time: 0.8506486415863037\n",
      "Step: 2165, Loss: 0.9159644842147827, Accuracy: 1.0, Computation time: 1.0301308631896973\n",
      "Step: 2166, Loss: 0.91595059633255, Accuracy: 1.0, Computation time: 0.9245681762695312\n",
      "Step: 2167, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 1.0360603332519531\n",
      "Step: 2168, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.0417311191558838\n",
      "Step: 2169, Loss: 0.9159061312675476, Accuracy: 1.0, Computation time: 0.8620100021362305\n",
      "Step: 2170, Loss: 0.9160205125808716, Accuracy: 1.0, Computation time: 0.9641232490539551\n",
      "Step: 2171, Loss: 0.9342127442359924, Accuracy: 0.96875, Computation time: 0.9645442962646484\n",
      "Step: 2172, Loss: 0.9179968237876892, Accuracy: 1.0, Computation time: 0.9717075824737549\n",
      "Step: 2173, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 1.5701484680175781\n",
      "Step: 2174, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.1883175373077393\n",
      "Step: 2175, Loss: 0.9159860014915466, Accuracy: 1.0, Computation time: 0.8351705074310303\n",
      "Step: 2176, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 0.9109842777252197\n",
      "Step: 2177, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 0.9631357192993164\n",
      "Step: 2178, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.3024933338165283\n",
      "Step: 2179, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.9673242568969727\n",
      "Step: 2180, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 0.9221286773681641\n",
      "Step: 2181, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 0.9415078163146973\n",
      "Step: 2182, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 0.9155471324920654\n",
      "Step: 2183, Loss: 0.935789942741394, Accuracy: 0.96875, Computation time: 0.9308969974517822\n",
      "Step: 2184, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.5084161758422852\n",
      "Step: 2185, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.2329292297363281\n",
      "Step: 2186, Loss: 0.9160792827606201, Accuracy: 1.0, Computation time: 1.0671930313110352\n",
      "Step: 2187, Loss: 0.9159365296363831, Accuracy: 1.0, Computation time: 0.9115574359893799\n",
      "Step: 2188, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.1069509983062744\n",
      "Step: 2189, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.078803300857544\n",
      "Step: 2190, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 0.9798421859741211\n",
      "Step: 2191, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.108689546585083\n",
      "Step: 2192, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 0.9175488948822021\n",
      "Step: 2193, Loss: 0.9261529445648193, Accuracy: 0.96875, Computation time: 1.1314163208007812\n",
      "Step: 2194, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 0.8538343906402588\n",
      "Step: 2195, Loss: 0.9340516328811646, Accuracy: 0.96875, Computation time: 1.162672519683838\n",
      "Step: 2196, Loss: 0.9249643683433533, Accuracy: 1.0, Computation time: 0.9921610355377197\n",
      "Step: 2197, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.135094165802002\n",
      "Step: 2198, Loss: 0.9159340858459473, Accuracy: 1.0, Computation time: 0.8282430171966553\n",
      "Step: 2199, Loss: 0.9159870743751526, Accuracy: 1.0, Computation time: 1.0136184692382812\n",
      "Step: 2200, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 0.9769425392150879\n",
      "Step: 2201, Loss: 0.9183254837989807, Accuracy: 1.0, Computation time: 0.878124475479126\n",
      "Step: 2202, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 0.9802055358886719\n",
      "Step: 2203, Loss: 0.9375792145729065, Accuracy: 0.96875, Computation time: 0.8286852836608887\n",
      "Step: 2204, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 0.97623610496521\n",
      "Step: 2205, Loss: 0.9170522689819336, Accuracy: 1.0, Computation time: 1.2166056632995605\n",
      "Step: 2206, Loss: 0.9159756302833557, Accuracy: 1.0, Computation time: 0.8879134654998779\n",
      "Step: 2207, Loss: 0.9369568228721619, Accuracy: 0.96875, Computation time: 0.9814579486846924\n",
      "Step: 2208, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 0.800692081451416\n",
      "Step: 2209, Loss: 0.9375215768814087, Accuracy: 0.96875, Computation time: 0.8247129917144775\n",
      "Step: 2210, Loss: 0.920778214931488, Accuracy: 1.0, Computation time: 1.0306243896484375\n",
      "Step: 2211, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 0.8375895023345947\n",
      "Step: 2212, Loss: 0.9412052631378174, Accuracy: 0.96875, Computation time: 1.188786268234253\n",
      "Step: 2213, Loss: 0.9159393906593323, Accuracy: 1.0, Computation time: 0.8193962574005127\n",
      "Step: 2214, Loss: 0.9159130454063416, Accuracy: 1.0, Computation time: 0.8873343467712402\n",
      "Step: 2215, Loss: 0.9376436471939087, Accuracy: 0.96875, Computation time: 0.9600241184234619\n",
      "Step: 2216, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 0.8333921432495117\n",
      "Step: 2217, Loss: 0.935218095779419, Accuracy: 0.96875, Computation time: 1.2724425792694092\n",
      "Step: 2218, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 0.9095499515533447\n",
      "Step: 2219, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 0.894169807434082\n",
      "Step: 2220, Loss: 0.9161416292190552, Accuracy: 1.0, Computation time: 0.9429478645324707\n",
      "Step: 2221, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 0.8226242065429688\n",
      "Step: 2222, Loss: 0.9160065054893494, Accuracy: 1.0, Computation time: 0.954890251159668\n",
      "Step: 2223, Loss: 0.9377392530441284, Accuracy: 0.96875, Computation time: 0.8413076400756836\n",
      "########################\n",
      "Test loss: 1.0674335956573486, Test Accuracy_epoch16: 0.7771260738372803\n",
      "########################\n",
      "Step: 2224, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 0.7533361911773682\n",
      "Step: 2225, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 0.867713212966919\n",
      "Step: 2226, Loss: 0.9159094095230103, Accuracy: 1.0, Computation time: 0.9730162620544434\n",
      "Step: 2227, Loss: 0.9371380805969238, Accuracy: 0.96875, Computation time: 0.8440582752227783\n",
      "Step: 2228, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 0.8482077121734619\n",
      "Step: 2229, Loss: 0.9158821105957031, Accuracy: 1.0, Computation time: 0.8118886947631836\n",
      "Step: 2230, Loss: 0.9159038662910461, Accuracy: 1.0, Computation time: 0.8767297267913818\n",
      "Step: 2231, Loss: 0.9167699217796326, Accuracy: 1.0, Computation time: 0.9319479465484619\n",
      "Step: 2232, Loss: 0.9162256717681885, Accuracy: 1.0, Computation time: 0.942258358001709\n",
      "Step: 2233, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 0.8322322368621826\n",
      "Step: 2234, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.8759171962738037\n",
      "Step: 2235, Loss: 0.9375240206718445, Accuracy: 0.96875, Computation time: 1.0078752040863037\n",
      "Step: 2236, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 0.8633220195770264\n",
      "Step: 2237, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 0.9273195266723633\n",
      "Step: 2238, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 0.9518792629241943\n",
      "Step: 2239, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 0.7950913906097412\n",
      "Step: 2240, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 0.8301327228546143\n",
      "Step: 2241, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 0.926109790802002\n",
      "Step: 2242, Loss: 0.928492546081543, Accuracy: 0.96875, Computation time: 1.1006591320037842\n",
      "Step: 2243, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 0.78310227394104\n",
      "Step: 2244, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 0.8206474781036377\n",
      "Step: 2245, Loss: 0.9159612655639648, Accuracy: 1.0, Computation time: 0.8377664089202881\n",
      "Step: 2246, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 0.8328640460968018\n",
      "Step: 2247, Loss: 0.9159979224205017, Accuracy: 1.0, Computation time: 0.9755003452301025\n",
      "Step: 2248, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 0.817357063293457\n",
      "Step: 2249, Loss: 0.9159645438194275, Accuracy: 1.0, Computation time: 0.8652467727661133\n",
      "Step: 2250, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.1179778575897217\n",
      "Step: 2251, Loss: 0.9487051367759705, Accuracy: 0.96875, Computation time: 1.1000053882598877\n",
      "Step: 2252, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 0.8888709545135498\n",
      "Step: 2253, Loss: 0.9162971377372742, Accuracy: 1.0, Computation time: 0.9092180728912354\n",
      "Step: 2254, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 0.8078277111053467\n",
      "Step: 2255, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 0.8495705127716064\n",
      "Step: 2256, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.0288362503051758\n",
      "Step: 2257, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 0.7731702327728271\n",
      "Step: 2258, Loss: 0.9159157872200012, Accuracy: 1.0, Computation time: 0.9675548076629639\n",
      "Step: 2259, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 0.6741843223571777\n",
      "Step: 2260, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 0.7685568332672119\n",
      "Step: 2261, Loss: 0.9427416920661926, Accuracy: 0.96875, Computation time: 0.8663032054901123\n",
      "Step: 2262, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 0.8159832954406738\n",
      "Step: 2263, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 0.8169863224029541\n",
      "Step: 2264, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 0.8186333179473877\n",
      "Step: 2265, Loss: 0.9370484352111816, Accuracy: 0.96875, Computation time: 0.9200084209442139\n",
      "Step: 2266, Loss: 0.9375488758087158, Accuracy: 0.96875, Computation time: 0.8957560062408447\n",
      "Step: 2267, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 0.9365925788879395\n",
      "Step: 2268, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 0.8091394901275635\n",
      "Step: 2269, Loss: 0.9159754514694214, Accuracy: 1.0, Computation time: 1.018538236618042\n",
      "Step: 2270, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 0.9096822738647461\n",
      "Step: 2271, Loss: 0.9348042011260986, Accuracy: 0.96875, Computation time: 0.9778604507446289\n",
      "Step: 2272, Loss: 0.9159056544303894, Accuracy: 1.0, Computation time: 0.9666876792907715\n",
      "Step: 2273, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 0.854780912399292\n",
      "Step: 2274, Loss: 0.9320794343948364, Accuracy: 0.96875, Computation time: 0.8504183292388916\n",
      "Step: 2275, Loss: 0.936797022819519, Accuracy: 0.96875, Computation time: 0.8662488460540771\n",
      "Step: 2276, Loss: 0.9159132242202759, Accuracy: 1.0, Computation time: 0.8281292915344238\n",
      "Step: 2277, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 0.8817470073699951\n",
      "Step: 2278, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 0.9495885372161865\n",
      "Step: 2279, Loss: 0.9160361289978027, Accuracy: 1.0, Computation time: 1.353330135345459\n",
      "Step: 2280, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 0.7484309673309326\n",
      "Step: 2281, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 0.8556928634643555\n",
      "Step: 2282, Loss: 0.9158852696418762, Accuracy: 1.0, Computation time: 1.0416290760040283\n",
      "Step: 2283, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 0.9312641620635986\n",
      "Step: 2284, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 0.8243799209594727\n",
      "Step: 2285, Loss: 0.9374637603759766, Accuracy: 0.96875, Computation time: 0.8941495418548584\n",
      "Step: 2286, Loss: 0.915958821773529, Accuracy: 1.0, Computation time: 1.079554796218872\n",
      "Step: 2287, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 0.8004224300384521\n",
      "Step: 2288, Loss: 0.915941596031189, Accuracy: 1.0, Computation time: 0.8328917026519775\n",
      "Step: 2289, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 0.9313051700592041\n",
      "Step: 2290, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 0.776993989944458\n",
      "Step: 2291, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 0.8996484279632568\n",
      "Step: 2292, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 0.9252512454986572\n",
      "Step: 2293, Loss: 0.9222246408462524, Accuracy: 1.0, Computation time: 0.7743346691131592\n",
      "Step: 2294, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 0.8706910610198975\n",
      "Step: 2295, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 0.7895073890686035\n",
      "Step: 2296, Loss: 0.9171534180641174, Accuracy: 1.0, Computation time: 0.7575819492340088\n",
      "Step: 2297, Loss: 0.9244728088378906, Accuracy: 1.0, Computation time: 1.3294782638549805\n",
      "Step: 2298, Loss: 0.9374301433563232, Accuracy: 0.96875, Computation time: 0.925776481628418\n",
      "Step: 2299, Loss: 0.9159871339797974, Accuracy: 1.0, Computation time: 1.0090012550354004\n",
      "Step: 2300, Loss: 0.9172433614730835, Accuracy: 1.0, Computation time: 1.201979160308838\n",
      "Step: 2301, Loss: 0.9160143733024597, Accuracy: 1.0, Computation time: 0.8553760051727295\n",
      "Step: 2302, Loss: 0.9163013100624084, Accuracy: 1.0, Computation time: 0.9395794868469238\n",
      "Step: 2303, Loss: 0.916050374507904, Accuracy: 1.0, Computation time: 0.8825955390930176\n",
      "Step: 2304, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 1.4202899932861328\n",
      "Step: 2305, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 1.0160951614379883\n",
      "Step: 2306, Loss: 0.9425699710845947, Accuracy: 0.96875, Computation time: 1.6947898864746094\n",
      "Step: 2307, Loss: 0.9311919808387756, Accuracy: 0.96875, Computation time: 1.2651915550231934\n",
      "Step: 2308, Loss: 0.916409432888031, Accuracy: 1.0, Computation time: 0.9110274314880371\n",
      "Step: 2309, Loss: 0.9160011410713196, Accuracy: 1.0, Computation time: 0.8121685981750488\n",
      "Step: 2310, Loss: 0.9162665009498596, Accuracy: 1.0, Computation time: 1.051239252090454\n",
      "Step: 2311, Loss: 0.9159912467002869, Accuracy: 1.0, Computation time: 0.9322302341461182\n",
      "Step: 2312, Loss: 0.9159318208694458, Accuracy: 1.0, Computation time: 1.1847996711730957\n",
      "Step: 2313, Loss: 0.9376856088638306, Accuracy: 0.96875, Computation time: 0.9852814674377441\n",
      "Step: 2314, Loss: 0.9192954897880554, Accuracy: 1.0, Computation time: 1.338223934173584\n",
      "Step: 2315, Loss: 0.916098415851593, Accuracy: 1.0, Computation time: 0.9972443580627441\n",
      "Step: 2316, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 0.9238815307617188\n",
      "Step: 2317, Loss: 0.916134774684906, Accuracy: 1.0, Computation time: 0.971118688583374\n",
      "Step: 2318, Loss: 0.916061520576477, Accuracy: 1.0, Computation time: 1.0602796077728271\n",
      "Step: 2319, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 0.9798941612243652\n",
      "Step: 2320, Loss: 0.915970504283905, Accuracy: 1.0, Computation time: 1.1350376605987549\n",
      "Step: 2321, Loss: 0.9554962515830994, Accuracy: 0.9375, Computation time: 1.3458881378173828\n",
      "Step: 2322, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.011085033416748\n",
      "Step: 2323, Loss: 0.9161571264266968, Accuracy: 1.0, Computation time: 1.022965669631958\n",
      "Step: 2324, Loss: 0.9159448146820068, Accuracy: 1.0, Computation time: 0.8990097045898438\n",
      "Step: 2325, Loss: 0.9161273241043091, Accuracy: 1.0, Computation time: 0.9846432209014893\n",
      "Step: 2326, Loss: 0.9161987900733948, Accuracy: 1.0, Computation time: 1.1753418445587158\n",
      "Step: 2327, Loss: 0.9160257577896118, Accuracy: 1.0, Computation time: 0.8876476287841797\n",
      "Step: 2328, Loss: 0.9162761569023132, Accuracy: 1.0, Computation time: 0.8982200622558594\n",
      "Step: 2329, Loss: 0.9159751534461975, Accuracy: 1.0, Computation time: 1.3808519840240479\n",
      "Step: 2330, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 0.9658141136169434\n",
      "Step: 2331, Loss: 0.9376989603042603, Accuracy: 0.96875, Computation time: 1.2558798789978027\n",
      "Step: 2332, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.0796899795532227\n",
      "Step: 2333, Loss: 0.9159553050994873, Accuracy: 1.0, Computation time: 0.9372551441192627\n",
      "Step: 2334, Loss: 0.9170610904693604, Accuracy: 1.0, Computation time: 1.0687110424041748\n",
      "Step: 2335, Loss: 0.9161614179611206, Accuracy: 1.0, Computation time: 0.9427645206451416\n",
      "Step: 2336, Loss: 0.9159736633300781, Accuracy: 1.0, Computation time: 1.1618292331695557\n",
      "Step: 2337, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 0.9749999046325684\n",
      "Step: 2338, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 1.0434393882751465\n",
      "Step: 2339, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 0.9695465564727783\n",
      "Step: 2340, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 0.9026103019714355\n",
      "Step: 2341, Loss: 0.9382303953170776, Accuracy: 0.96875, Computation time: 1.5585739612579346\n",
      "Step: 2342, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 1.3373281955718994\n",
      "Step: 2343, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.1561532020568848\n",
      "Step: 2344, Loss: 0.9161310195922852, Accuracy: 1.0, Computation time: 1.2365143299102783\n",
      "Step: 2345, Loss: 0.9160281419754028, Accuracy: 1.0, Computation time: 1.0842773914337158\n",
      "Step: 2346, Loss: 0.9161639213562012, Accuracy: 1.0, Computation time: 1.0403690338134766\n",
      "Step: 2347, Loss: 0.9356479644775391, Accuracy: 0.96875, Computation time: 0.9854168891906738\n",
      "Step: 2348, Loss: 0.9591838121414185, Accuracy: 0.9375, Computation time: 0.9910004138946533\n",
      "Step: 2349, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 0.9577851295471191\n",
      "Step: 2350, Loss: 0.91800856590271, Accuracy: 1.0, Computation time: 1.1314620971679688\n",
      "Step: 2351, Loss: 0.9173002243041992, Accuracy: 1.0, Computation time: 1.1833455562591553\n",
      "Step: 2352, Loss: 0.9160006046295166, Accuracy: 1.0, Computation time: 1.0066683292388916\n",
      "Step: 2353, Loss: 0.9219313263893127, Accuracy: 1.0, Computation time: 0.9799220561981201\n",
      "Step: 2354, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 0.9938411712646484\n",
      "Step: 2355, Loss: 0.9375707507133484, Accuracy: 0.96875, Computation time: 0.9098289012908936\n",
      "Step: 2356, Loss: 0.9375315308570862, Accuracy: 0.96875, Computation time: 0.8921177387237549\n",
      "Step: 2357, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.1128871440887451\n",
      "Step: 2358, Loss: 0.9160338044166565, Accuracy: 1.0, Computation time: 1.024183750152588\n",
      "Step: 2359, Loss: 0.9158998727798462, Accuracy: 1.0, Computation time: 1.1674141883850098\n",
      "Step: 2360, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.1360318660736084\n",
      "Step: 2361, Loss: 0.937613308429718, Accuracy: 0.96875, Computation time: 0.8495512008666992\n",
      "Step: 2362, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.1148617267608643\n",
      "########################\n",
      "Test loss: 1.068727731704712, Test Accuracy_epoch17: 0.774193525314331\n",
      "########################\n",
      "Step: 2363, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.4441959857940674\n",
      "Step: 2364, Loss: 0.9165205955505371, Accuracy: 1.0, Computation time: 1.0318949222564697\n",
      "Step: 2365, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 0.9608616828918457\n",
      "Step: 2366, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.0278339385986328\n",
      "Step: 2367, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.0687227249145508\n",
      "Step: 2368, Loss: 0.9160657525062561, Accuracy: 1.0, Computation time: 1.0306487083435059\n",
      "Step: 2369, Loss: 0.9166964292526245, Accuracy: 1.0, Computation time: 1.1949336528778076\n",
      "Step: 2370, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 1.1264526844024658\n",
      "Step: 2371, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.0528321266174316\n",
      "Step: 2372, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.04095458984375\n",
      "Step: 2373, Loss: 0.9375275373458862, Accuracy: 0.96875, Computation time: 1.044133186340332\n",
      "Step: 2374, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.0480782985687256\n",
      "Step: 2375, Loss: 0.9160826206207275, Accuracy: 1.0, Computation time: 1.337010383605957\n",
      "Step: 2376, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.1737451553344727\n",
      "Step: 2377, Loss: 0.9162248373031616, Accuracy: 1.0, Computation time: 1.1256563663482666\n",
      "Step: 2378, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 0.8171107769012451\n",
      "Step: 2379, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 0.9415316581726074\n",
      "Step: 2380, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 0.9612329006195068\n",
      "Step: 2381, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 0.9282796382904053\n",
      "Step: 2382, Loss: 0.9160556793212891, Accuracy: 1.0, Computation time: 0.9129276275634766\n",
      "Step: 2383, Loss: 0.9341530203819275, Accuracy: 0.96875, Computation time: 1.0679421424865723\n",
      "Step: 2384, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 0.9544820785522461\n",
      "Step: 2385, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 0.9891073703765869\n",
      "Step: 2386, Loss: 0.9587178230285645, Accuracy: 0.9375, Computation time: 1.0129480361938477\n",
      "Step: 2387, Loss: 0.9370160102844238, Accuracy: 0.96875, Computation time: 1.1388862133026123\n",
      "Step: 2388, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 0.9786162376403809\n",
      "Step: 2389, Loss: 0.9159280061721802, Accuracy: 1.0, Computation time: 0.9027504920959473\n",
      "Step: 2390, Loss: 0.9159142374992371, Accuracy: 1.0, Computation time: 1.119067907333374\n",
      "Step: 2391, Loss: 0.9180504083633423, Accuracy: 1.0, Computation time: 0.9370472431182861\n",
      "Step: 2392, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 0.9957592487335205\n",
      "Step: 2393, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.0302057266235352\n",
      "Step: 2394, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 1.2899246215820312\n",
      "Step: 2395, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.226097822189331\n",
      "Step: 2396, Loss: 0.9415874481201172, Accuracy: 0.96875, Computation time: 0.9442944526672363\n",
      "Step: 2397, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.0594589710235596\n",
      "Step: 2398, Loss: 0.9164725542068481, Accuracy: 1.0, Computation time: 1.3185019493103027\n",
      "Step: 2399, Loss: 0.9161150455474854, Accuracy: 1.0, Computation time: 1.4316296577453613\n",
      "Step: 2400, Loss: 0.9202459454536438, Accuracy: 1.0, Computation time: 1.1763484477996826\n",
      "Step: 2401, Loss: 0.9159178137779236, Accuracy: 1.0, Computation time: 0.9731252193450928\n",
      "Step: 2402, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 1.0497865676879883\n",
      "Step: 2403, Loss: 0.9163888096809387, Accuracy: 1.0, Computation time: 0.9653453826904297\n",
      "Step: 2404, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.0185089111328125\n",
      "Step: 2405, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.0705363750457764\n",
      "Step: 2406, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 1.011554479598999\n",
      "Step: 2407, Loss: 0.9158950448036194, Accuracy: 1.0, Computation time: 0.8568077087402344\n",
      "Step: 2408, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 1.0121009349822998\n",
      "Step: 2409, Loss: 0.9179420471191406, Accuracy: 1.0, Computation time: 1.2675526142120361\n",
      "Step: 2410, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.4054698944091797\n",
      "Step: 2411, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 1.539586067199707\n",
      "Step: 2412, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 0.9839255809783936\n",
      "Step: 2413, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.112894058227539\n",
      "Step: 2414, Loss: 0.9177994728088379, Accuracy: 1.0, Computation time: 1.075178861618042\n",
      "Step: 2415, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 0.8843305110931396\n",
      "Step: 2416, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 0.9884686470031738\n",
      "Step: 2417, Loss: 0.9210489988327026, Accuracy: 1.0, Computation time: 0.9714112281799316\n",
      "Step: 2418, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 1.0381150245666504\n",
      "Step: 2419, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 1.2053782939910889\n",
      "Step: 2420, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 1.0644314289093018\n",
      "Step: 2421, Loss: 0.9160828590393066, Accuracy: 1.0, Computation time: 1.026336908340454\n",
      "Step: 2422, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 0.8088626861572266\n",
      "Step: 2423, Loss: 0.9159349203109741, Accuracy: 1.0, Computation time: 1.1517486572265625\n",
      "Step: 2424, Loss: 0.9382377862930298, Accuracy: 0.96875, Computation time: 1.2731051445007324\n",
      "Step: 2425, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 0.7796931266784668\n",
      "Step: 2426, Loss: 0.9159621596336365, Accuracy: 1.0, Computation time: 1.5304985046386719\n",
      "Step: 2427, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.015625\n",
      "Step: 2428, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 0.9574902057647705\n",
      "Step: 2429, Loss: 0.9375775456428528, Accuracy: 0.96875, Computation time: 0.8989417552947998\n",
      "Step: 2430, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 0.9734268188476562\n",
      "Step: 2431, Loss: 0.9174228310585022, Accuracy: 1.0, Computation time: 1.0106201171875\n",
      "Step: 2432, Loss: 0.9371457695960999, Accuracy: 0.96875, Computation time: 0.9378950595855713\n",
      "Step: 2433, Loss: 0.9230014085769653, Accuracy: 1.0, Computation time: 0.9342095851898193\n",
      "Step: 2434, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 0.7908251285552979\n",
      "Step: 2435, Loss: 0.9197686314582825, Accuracy: 1.0, Computation time: 1.0159432888031006\n",
      "Step: 2436, Loss: 0.9160717129707336, Accuracy: 1.0, Computation time: 0.9291965961456299\n",
      "Step: 2437, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 0.9434075355529785\n",
      "Step: 2438, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 0.8933560848236084\n",
      "Step: 2439, Loss: 0.9162007570266724, Accuracy: 1.0, Computation time: 0.8938708305358887\n",
      "Step: 2440, Loss: 0.916605532169342, Accuracy: 1.0, Computation time: 1.0765399932861328\n",
      "Step: 2441, Loss: 0.9159495234489441, Accuracy: 1.0, Computation time: 0.7778713703155518\n",
      "Step: 2442, Loss: 0.9159671664237976, Accuracy: 1.0, Computation time: 1.1709842681884766\n",
      "Step: 2443, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 0.7965867519378662\n",
      "Step: 2444, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 1.0139641761779785\n",
      "Step: 2445, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.789970874786377\n",
      "Step: 2446, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 0.9889419078826904\n",
      "Step: 2447, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.0970022678375244\n",
      "Step: 2448, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.0438287258148193\n",
      "Step: 2449, Loss: 0.9317125082015991, Accuracy: 0.96875, Computation time: 1.1910576820373535\n",
      "Step: 2450, Loss: 0.9377889037132263, Accuracy: 0.96875, Computation time: 0.997528076171875\n",
      "Step: 2451, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 0.8785886764526367\n",
      "Step: 2452, Loss: 0.9375207424163818, Accuracy: 0.96875, Computation time: 0.8732047080993652\n",
      "Step: 2453, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.0526518821716309\n",
      "Step: 2454, Loss: 0.9162311553955078, Accuracy: 1.0, Computation time: 0.9788289070129395\n",
      "Step: 2455, Loss: 0.9159371256828308, Accuracy: 1.0, Computation time: 1.128981590270996\n",
      "Step: 2456, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 1.2935636043548584\n",
      "Step: 2457, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 1.0895898342132568\n",
      "Step: 2458, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 1.2798852920532227\n",
      "Step: 2459, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.0104601383209229\n",
      "Step: 2460, Loss: 0.9562633037567139, Accuracy: 0.9375, Computation time: 0.982921838760376\n",
      "Step: 2461, Loss: 0.9376295804977417, Accuracy: 0.96875, Computation time: 1.0101001262664795\n",
      "Step: 2462, Loss: 0.9159855246543884, Accuracy: 1.0, Computation time: 1.4461922645568848\n",
      "Step: 2463, Loss: 0.9158936738967896, Accuracy: 1.0, Computation time: 0.7901465892791748\n",
      "Step: 2464, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.0653488636016846\n",
      "Step: 2465, Loss: 0.915971577167511, Accuracy: 1.0, Computation time: 0.8673877716064453\n",
      "Step: 2466, Loss: 0.9159324765205383, Accuracy: 1.0, Computation time: 0.867504358291626\n",
      "Step: 2467, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 0.8119208812713623\n",
      "Step: 2468, Loss: 0.9160719513893127, Accuracy: 1.0, Computation time: 0.899714469909668\n",
      "Step: 2469, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.15802001953125\n",
      "Step: 2470, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 0.8917582035064697\n",
      "Step: 2471, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 0.8914051055908203\n",
      "Step: 2472, Loss: 0.9235455989837646, Accuracy: 1.0, Computation time: 1.0880849361419678\n",
      "Step: 2473, Loss: 0.9379152655601501, Accuracy: 0.96875, Computation time: 0.9434306621551514\n",
      "Step: 2474, Loss: 0.9159643650054932, Accuracy: 1.0, Computation time: 1.0920941829681396\n",
      "Step: 2475, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 0.9272270202636719\n",
      "Step: 2476, Loss: 0.9159707427024841, Accuracy: 1.0, Computation time: 0.9188704490661621\n",
      "Step: 2477, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 0.923499345779419\n",
      "Step: 2478, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 0.8683292865753174\n",
      "Step: 2479, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 0.9062180519104004\n",
      "Step: 2480, Loss: 0.9161983728408813, Accuracy: 1.0, Computation time: 1.0927631855010986\n",
      "Step: 2481, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.2497444152832031\n",
      "Step: 2482, Loss: 0.916450023651123, Accuracy: 1.0, Computation time: 1.7728843688964844\n",
      "Step: 2483, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 0.8757836818695068\n",
      "Step: 2484, Loss: 0.915988028049469, Accuracy: 1.0, Computation time: 1.1913244724273682\n",
      "Step: 2485, Loss: 0.9159383773803711, Accuracy: 1.0, Computation time: 1.0749833583831787\n",
      "Step: 2486, Loss: 0.9159606695175171, Accuracy: 1.0, Computation time: 0.878678560256958\n",
      "Step: 2487, Loss: 0.9159342050552368, Accuracy: 1.0, Computation time: 1.2386410236358643\n",
      "Step: 2488, Loss: 0.9570067524909973, Accuracy: 0.9375, Computation time: 1.0591673851013184\n",
      "Step: 2489, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 1.029888391494751\n",
      "Step: 2490, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 1.0937004089355469\n",
      "Step: 2491, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.9598174095153809\n",
      "Step: 2492, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.0296564102172852\n",
      "Step: 2493, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 0.7562248706817627\n",
      "Step: 2494, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 0.856297492980957\n",
      "Step: 2495, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 0.9547920227050781\n",
      "Step: 2496, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 1.1327378749847412\n",
      "Step: 2497, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 0.932762861251831\n",
      "Step: 2498, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.1925199031829834\n",
      "Step: 2499, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.117384433746338\n",
      "Step: 2500, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 1.0433828830718994\n",
      "Step: 2501, Loss: 0.9165481925010681, Accuracy: 1.0, Computation time: 1.0328891277313232\n",
      "########################\n",
      "Test loss: 1.0659042596817017, Test Accuracy_epoch18: 0.7790811061859131\n",
      "########################\n",
      "Step: 2502, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.1440000534057617\n",
      "Step: 2503, Loss: 0.938417375087738, Accuracy: 0.96875, Computation time: 1.0040404796600342\n",
      "Step: 2504, Loss: 0.9379681944847107, Accuracy: 0.96875, Computation time: 1.0002436637878418\n",
      "Step: 2505, Loss: 0.9158975481987, Accuracy: 1.0, Computation time: 0.7832036018371582\n",
      "Step: 2506, Loss: 0.915951132774353, Accuracy: 1.0, Computation time: 0.8030102252960205\n",
      "Step: 2507, Loss: 0.9160022139549255, Accuracy: 1.0, Computation time: 0.9288907051086426\n",
      "Step: 2508, Loss: 0.9159955382347107, Accuracy: 1.0, Computation time: 0.9478733539581299\n",
      "Step: 2509, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 0.8815591335296631\n",
      "Step: 2510, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 0.8385562896728516\n",
      "Step: 2511, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 0.8936362266540527\n",
      "Step: 2512, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 1.0098273754119873\n",
      "Step: 2513, Loss: 0.9206261038780212, Accuracy: 1.0, Computation time: 0.8559937477111816\n",
      "Step: 2514, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 0.9499003887176514\n",
      "Step: 2515, Loss: 0.9159342050552368, Accuracy: 1.0, Computation time: 0.8654923439025879\n",
      "Step: 2516, Loss: 0.9332159161567688, Accuracy: 0.96875, Computation time: 0.8430583477020264\n",
      "Step: 2517, Loss: 0.9159894585609436, Accuracy: 1.0, Computation time: 0.9393348693847656\n",
      "Step: 2518, Loss: 0.9159935712814331, Accuracy: 1.0, Computation time: 0.8767004013061523\n",
      "Step: 2519, Loss: 0.937830924987793, Accuracy: 0.96875, Computation time: 1.0457301139831543\n",
      "Step: 2520, Loss: 0.9159404635429382, Accuracy: 1.0, Computation time: 1.1562933921813965\n",
      "Step: 2521, Loss: 0.9277685880661011, Accuracy: 0.96875, Computation time: 1.2401280403137207\n",
      "Step: 2522, Loss: 0.9377133250236511, Accuracy: 0.96875, Computation time: 0.9981307983398438\n",
      "Step: 2523, Loss: 0.9161579012870789, Accuracy: 1.0, Computation time: 0.9263429641723633\n",
      "Step: 2524, Loss: 0.9239916801452637, Accuracy: 1.0, Computation time: 0.9918215274810791\n",
      "Step: 2525, Loss: 0.9160084128379822, Accuracy: 1.0, Computation time: 0.842583417892456\n",
      "Step: 2526, Loss: 0.9160725474357605, Accuracy: 1.0, Computation time: 0.8500151634216309\n",
      "Step: 2527, Loss: 0.9160292148590088, Accuracy: 1.0, Computation time: 0.8383824825286865\n",
      "Step: 2528, Loss: 0.9339544177055359, Accuracy: 0.96875, Computation time: 0.8505394458770752\n",
      "Step: 2529, Loss: 0.9310300350189209, Accuracy: 0.96875, Computation time: 0.9090688228607178\n",
      "Step: 2530, Loss: 0.9194662570953369, Accuracy: 1.0, Computation time: 0.8734261989593506\n",
      "Step: 2531, Loss: 0.9159828424453735, Accuracy: 1.0, Computation time: 0.8583724498748779\n",
      "Step: 2532, Loss: 0.9226621389389038, Accuracy: 1.0, Computation time: 1.0535812377929688\n",
      "Step: 2533, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 1.094388484954834\n",
      "Step: 2534, Loss: 0.9163269400596619, Accuracy: 1.0, Computation time: 1.1290669441223145\n",
      "Step: 2535, Loss: 0.9377703666687012, Accuracy: 0.96875, Computation time: 0.9869749546051025\n",
      "Step: 2536, Loss: 0.9171732068061829, Accuracy: 1.0, Computation time: 0.9397950172424316\n",
      "Step: 2537, Loss: 0.9166231155395508, Accuracy: 1.0, Computation time: 0.9722752571105957\n",
      "Step: 2538, Loss: 0.9180689454078674, Accuracy: 1.0, Computation time: 1.3978512287139893\n",
      "Step: 2539, Loss: 0.9164706468582153, Accuracy: 1.0, Computation time: 0.9987938404083252\n",
      "Step: 2540, Loss: 0.9227546453475952, Accuracy: 1.0, Computation time: 0.9601836204528809\n",
      "Step: 2541, Loss: 0.9159814119338989, Accuracy: 1.0, Computation time: 0.8885717391967773\n",
      "Step: 2542, Loss: 0.9297550320625305, Accuracy: 0.96875, Computation time: 1.0081350803375244\n",
      "Step: 2543, Loss: 0.9160500168800354, Accuracy: 1.0, Computation time: 0.8917853832244873\n",
      "Step: 2544, Loss: 0.9160082936286926, Accuracy: 1.0, Computation time: 1.266150951385498\n",
      "Step: 2545, Loss: 0.915958821773529, Accuracy: 1.0, Computation time: 0.8494532108306885\n",
      "Step: 2546, Loss: 0.9164187908172607, Accuracy: 1.0, Computation time: 0.9784247875213623\n",
      "Step: 2547, Loss: 0.917201578617096, Accuracy: 1.0, Computation time: 1.0918734073638916\n",
      "Step: 2548, Loss: 0.9162518978118896, Accuracy: 1.0, Computation time: 0.8835880756378174\n",
      "Step: 2549, Loss: 0.9213237762451172, Accuracy: 1.0, Computation time: 1.045701265335083\n",
      "Step: 2550, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.091961145401001\n",
      "Step: 2551, Loss: 0.9159658551216125, Accuracy: 1.0, Computation time: 1.056469440460205\n",
      "Step: 2552, Loss: 0.9160538911819458, Accuracy: 1.0, Computation time: 0.9644467830657959\n",
      "Step: 2553, Loss: 0.937682032585144, Accuracy: 0.96875, Computation time: 1.2427928447723389\n",
      "Step: 2554, Loss: 0.9159901738166809, Accuracy: 1.0, Computation time: 1.1525824069976807\n",
      "Step: 2555, Loss: 0.9160107374191284, Accuracy: 1.0, Computation time: 1.0976169109344482\n",
      "Step: 2556, Loss: 0.9224440455436707, Accuracy: 1.0, Computation time: 0.9972684383392334\n",
      "Step: 2557, Loss: 0.9167997241020203, Accuracy: 1.0, Computation time: 0.9048223495483398\n",
      "Step: 2558, Loss: 0.9311061501502991, Accuracy: 0.96875, Computation time: 1.1613376140594482\n",
      "Step: 2559, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.1179139614105225\n",
      "Step: 2560, Loss: 0.9369432330131531, Accuracy: 0.96875, Computation time: 1.1088752746582031\n",
      "Step: 2561, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 1.027405023574829\n",
      "Step: 2562, Loss: 0.9163326025009155, Accuracy: 1.0, Computation time: 1.2180256843566895\n",
      "Step: 2563, Loss: 0.9166070222854614, Accuracy: 1.0, Computation time: 1.091972827911377\n",
      "Step: 2564, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.1021785736083984\n",
      "Step: 2565, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.1658830642700195\n",
      "Step: 2566, Loss: 0.915901780128479, Accuracy: 1.0, Computation time: 0.8846213817596436\n",
      "Step: 2567, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 0.8751134872436523\n",
      "Step: 2568, Loss: 0.9161240458488464, Accuracy: 1.0, Computation time: 1.1334559917449951\n",
      "Step: 2569, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 0.8709616661071777\n",
      "Step: 2570, Loss: 0.93747878074646, Accuracy: 0.96875, Computation time: 0.839205265045166\n",
      "Step: 2571, Loss: 0.9159218668937683, Accuracy: 1.0, Computation time: 0.8114674091339111\n",
      "Step: 2572, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.0511322021484375\n",
      "Step: 2573, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.093907117843628\n",
      "Step: 2574, Loss: 0.9377467036247253, Accuracy: 0.96875, Computation time: 0.7865190505981445\n",
      "Step: 2575, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 0.9918687343597412\n",
      "Step: 2576, Loss: 0.9159513711929321, Accuracy: 1.0, Computation time: 1.0918664932250977\n",
      "Step: 2577, Loss: 0.9212185144424438, Accuracy: 1.0, Computation time: 0.8840072154998779\n",
      "Step: 2578, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 0.8502991199493408\n",
      "Step: 2579, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 0.7663006782531738\n",
      "Step: 2580, Loss: 0.9165427684783936, Accuracy: 1.0, Computation time: 1.5933008193969727\n",
      "Step: 2581, Loss: 0.9159265756607056, Accuracy: 1.0, Computation time: 0.9823319911956787\n",
      "Step: 2582, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 0.955430269241333\n",
      "Step: 2583, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 0.9756255149841309\n",
      "Step: 2584, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 0.9415626525878906\n",
      "Step: 2585, Loss: 0.9375177621841431, Accuracy: 0.96875, Computation time: 0.888268232345581\n",
      "Step: 2586, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 0.9985980987548828\n",
      "Step: 2587, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 0.8998563289642334\n",
      "Step: 2588, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.0744097232818604\n",
      "Step: 2589, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.032768726348877\n",
      "Step: 2590, Loss: 0.9163187742233276, Accuracy: 1.0, Computation time: 1.2281723022460938\n",
      "Step: 2591, Loss: 0.9158755540847778, Accuracy: 1.0, Computation time: 0.8870158195495605\n",
      "Step: 2592, Loss: 0.9286486506462097, Accuracy: 0.96875, Computation time: 1.032538652420044\n",
      "Step: 2593, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 0.8992059230804443\n",
      "Step: 2594, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 0.8941187858581543\n",
      "Step: 2595, Loss: 0.9159421920776367, Accuracy: 1.0, Computation time: 0.8559021949768066\n",
      "Step: 2596, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 0.9346349239349365\n",
      "Step: 2597, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.0160737037658691\n",
      "Step: 2598, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 0.888453483581543\n",
      "Step: 2599, Loss: 0.9159306287765503, Accuracy: 1.0, Computation time: 1.0965368747711182\n",
      "Step: 2600, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.147876262664795\n",
      "Step: 2601, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.047708511352539\n",
      "Step: 2602, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.0930869579315186\n",
      "Step: 2603, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.0713317394256592\n",
      "Step: 2604, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 0.9314417839050293\n",
      "Step: 2605, Loss: 0.915890634059906, Accuracy: 1.0, Computation time: 1.1996474266052246\n",
      "Step: 2606, Loss: 0.9161598682403564, Accuracy: 1.0, Computation time: 1.0415847301483154\n",
      "Step: 2607, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.012627124786377\n",
      "Step: 2608, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 0.8976688385009766\n",
      "Step: 2609, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 0.8748438358306885\n",
      "Step: 2610, Loss: 0.9374608397483826, Accuracy: 0.96875, Computation time: 0.8331663608551025\n",
      "Step: 2611, Loss: 0.9377710223197937, Accuracy: 0.96875, Computation time: 0.8740663528442383\n",
      "Step: 2612, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 0.954242467880249\n",
      "Step: 2613, Loss: 0.9160536527633667, Accuracy: 1.0, Computation time: 0.829082727432251\n",
      "Step: 2614, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 0.8504819869995117\n",
      "Step: 2615, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.0488693714141846\n",
      "Step: 2616, Loss: 0.9348784685134888, Accuracy: 0.96875, Computation time: 0.9379520416259766\n",
      "Step: 2617, Loss: 0.9479778409004211, Accuracy: 0.9375, Computation time: 0.9275801181793213\n",
      "Step: 2618, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 0.856691837310791\n",
      "Step: 2619, Loss: 0.9240637421607971, Accuracy: 1.0, Computation time: 1.231067419052124\n",
      "Step: 2620, Loss: 0.9168959856033325, Accuracy: 1.0, Computation time: 0.8551416397094727\n",
      "Step: 2621, Loss: 0.9160140156745911, Accuracy: 1.0, Computation time: 0.9517030715942383\n",
      "Step: 2622, Loss: 0.9159858822822571, Accuracy: 1.0, Computation time: 1.288257360458374\n",
      "Step: 2623, Loss: 0.9160311222076416, Accuracy: 1.0, Computation time: 0.9860804080963135\n",
      "Step: 2624, Loss: 0.9159416556358337, Accuracy: 1.0, Computation time: 0.9259989261627197\n",
      "Step: 2625, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 0.9541451930999756\n",
      "Step: 2626, Loss: 0.9159338474273682, Accuracy: 1.0, Computation time: 0.9549734592437744\n",
      "Step: 2627, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.0222008228302002\n",
      "Step: 2628, Loss: 0.9369389414787292, Accuracy: 0.96875, Computation time: 1.5609257221221924\n",
      "Step: 2629, Loss: 0.9375259876251221, Accuracy: 0.96875, Computation time: 0.8569598197937012\n",
      "Step: 2630, Loss: 0.9159911274909973, Accuracy: 1.0, Computation time: 1.0849394798278809\n",
      "Step: 2631, Loss: 0.9161232113838196, Accuracy: 1.0, Computation time: 0.8790037631988525\n",
      "Step: 2632, Loss: 0.9158936738967896, Accuracy: 1.0, Computation time: 0.9500977993011475\n",
      "Step: 2633, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 0.8921434879302979\n",
      "Step: 2634, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 1.0215036869049072\n",
      "Step: 2635, Loss: 0.935476541519165, Accuracy: 0.96875, Computation time: 1.183894395828247\n",
      "Step: 2636, Loss: 0.9342392683029175, Accuracy: 0.96875, Computation time: 1.0094196796417236\n",
      "Step: 2637, Loss: 0.9159406423568726, Accuracy: 1.0, Computation time: 0.9973552227020264\n",
      "Step: 2638, Loss: 0.9575259685516357, Accuracy: 0.9375, Computation time: 0.9929196834564209\n",
      "Step: 2639, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.0225203037261963\n",
      "Step: 2640, Loss: 0.9162327647209167, Accuracy: 1.0, Computation time: 1.0586020946502686\n",
      "########################\n",
      "Test loss: 1.0736815929412842, Test Accuracy_epoch19: 0.7702834606170654\n",
      "########################\n",
      "Step: 2641, Loss: 0.9162158966064453, Accuracy: 1.0, Computation time: 1.0029289722442627\n",
      "Step: 2642, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 0.9511966705322266\n",
      "Step: 2643, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 1.0663399696350098\n",
      "Step: 2644, Loss: 0.9159718155860901, Accuracy: 1.0, Computation time: 0.9290056228637695\n",
      "Step: 2645, Loss: 0.9349813461303711, Accuracy: 0.96875, Computation time: 1.1993858814239502\n",
      "Step: 2646, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 1.2821481227874756\n",
      "Step: 2647, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.1373040676116943\n",
      "Step: 2648, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 1.182429552078247\n",
      "Step: 2649, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.0671255588531494\n",
      "Step: 2650, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 0.9773972034454346\n",
      "Step: 2651, Loss: 0.9163410067558289, Accuracy: 1.0, Computation time: 0.9024240970611572\n",
      "Step: 2652, Loss: 0.9160672426223755, Accuracy: 1.0, Computation time: 1.0818803310394287\n",
      "Step: 2653, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.071854829788208\n",
      "Step: 2654, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.276846170425415\n",
      "Step: 2655, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.024463415145874\n",
      "Step: 2656, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 0.9940621852874756\n",
      "Step: 2657, Loss: 0.916027307510376, Accuracy: 1.0, Computation time: 1.3352017402648926\n",
      "Step: 2658, Loss: 0.9161105155944824, Accuracy: 1.0, Computation time: 1.2620604038238525\n",
      "Step: 2659, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.0397276878356934\n",
      "Step: 2660, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 0.9672820568084717\n",
      "Step: 2661, Loss: 0.9161304235458374, Accuracy: 1.0, Computation time: 0.9249136447906494\n",
      "Step: 2662, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 1.3833534717559814\n",
      "Step: 2663, Loss: 0.9159421324729919, Accuracy: 1.0, Computation time: 0.9683218002319336\n",
      "Step: 2664, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 0.8904850482940674\n",
      "Step: 2665, Loss: 0.9159427285194397, Accuracy: 1.0, Computation time: 1.0949108600616455\n",
      "Step: 2666, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.5150914192199707\n",
      "Step: 2667, Loss: 0.9195303320884705, Accuracy: 1.0, Computation time: 1.1735808849334717\n",
      "Step: 2668, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.3000547885894775\n",
      "Step: 2669, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.2203872203826904\n",
      "Step: 2670, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.3826537132263184\n",
      "Step: 2671, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.1821506023406982\n",
      "Step: 2672, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.2895283699035645\n",
      "Step: 2673, Loss: 0.9296072125434875, Accuracy: 0.96875, Computation time: 1.1646153926849365\n",
      "Step: 2674, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.214082956314087\n",
      "Step: 2675, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 0.8488481044769287\n",
      "Step: 2676, Loss: 0.9374545812606812, Accuracy: 0.96875, Computation time: 1.0099983215332031\n",
      "Step: 2677, Loss: 0.928041934967041, Accuracy: 0.96875, Computation time: 0.9681100845336914\n",
      "Step: 2678, Loss: 0.9159329533576965, Accuracy: 1.0, Computation time: 1.2942867279052734\n",
      "Step: 2679, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.2204468250274658\n",
      "Step: 2680, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 0.8500101566314697\n",
      "Step: 2681, Loss: 0.9160594940185547, Accuracy: 1.0, Computation time: 1.4011311531066895\n",
      "Step: 2682, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 0.9341480731964111\n",
      "Step: 2683, Loss: 0.9183444976806641, Accuracy: 1.0, Computation time: 0.9844884872436523\n",
      "Step: 2684, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 0.9438154697418213\n",
      "Step: 2685, Loss: 0.9162286520004272, Accuracy: 1.0, Computation time: 0.8532097339630127\n",
      "Step: 2686, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.7589964866638184\n",
      "Step: 2687, Loss: 0.918121874332428, Accuracy: 1.0, Computation time: 1.0946240425109863\n",
      "Step: 2688, Loss: 0.9339653253555298, Accuracy: 0.96875, Computation time: 1.0560460090637207\n",
      "Step: 2689, Loss: 0.9162635207176208, Accuracy: 1.0, Computation time: 1.0234768390655518\n",
      "Step: 2690, Loss: 0.9377018213272095, Accuracy: 0.96875, Computation time: 0.9813680648803711\n",
      "Step: 2691, Loss: 0.9159618020057678, Accuracy: 1.0, Computation time: 0.7718648910522461\n",
      "Step: 2692, Loss: 0.937549889087677, Accuracy: 0.96875, Computation time: 0.8399343490600586\n",
      "Step: 2693, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 0.8485825061798096\n",
      "Step: 2694, Loss: 0.9159365296363831, Accuracy: 1.0, Computation time: 0.8562526702880859\n",
      "Step: 2695, Loss: 0.915935218334198, Accuracy: 1.0, Computation time: 0.9896209239959717\n",
      "Step: 2696, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.019944429397583\n",
      "Step: 2697, Loss: 0.915988564491272, Accuracy: 1.0, Computation time: 0.9647557735443115\n",
      "Step: 2698, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.9348771572113037\n",
      "Step: 2699, Loss: 0.9159141778945923, Accuracy: 1.0, Computation time: 0.8258867263793945\n",
      "Step: 2700, Loss: 0.937596321105957, Accuracy: 0.96875, Computation time: 0.8405413627624512\n",
      "Step: 2701, Loss: 0.9404632449150085, Accuracy: 0.96875, Computation time: 1.029317855834961\n",
      "Step: 2702, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 0.7931516170501709\n",
      "Step: 2703, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 0.9022953510284424\n",
      "Step: 2704, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.4131073951721191\n",
      "Step: 2705, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 1.226006031036377\n",
      "Step: 2706, Loss: 0.9166485667228699, Accuracy: 1.0, Computation time: 0.9396533966064453\n",
      "Step: 2707, Loss: 0.9163682460784912, Accuracy: 1.0, Computation time: 1.033935546875\n",
      "Step: 2708, Loss: 0.9159725308418274, Accuracy: 1.0, Computation time: 0.9614129066467285\n",
      "Step: 2709, Loss: 0.922482967376709, Accuracy: 1.0, Computation time: 0.8632864952087402\n",
      "Step: 2710, Loss: 0.9159888029098511, Accuracy: 1.0, Computation time: 0.9655883312225342\n",
      "Step: 2711, Loss: 0.931825578212738, Accuracy: 0.96875, Computation time: 0.819622278213501\n",
      "Step: 2712, Loss: 0.9160058498382568, Accuracy: 1.0, Computation time: 0.7753658294677734\n",
      "Step: 2713, Loss: 0.9160352349281311, Accuracy: 1.0, Computation time: 0.9582171440124512\n",
      "Step: 2714, Loss: 0.9166082143783569, Accuracy: 1.0, Computation time: 0.9657635688781738\n",
      "Step: 2715, Loss: 0.9160728454589844, Accuracy: 1.0, Computation time: 1.1654002666473389\n",
      "Step: 2716, Loss: 0.916220486164093, Accuracy: 1.0, Computation time: 1.0100293159484863\n",
      "Step: 2717, Loss: 0.9375712275505066, Accuracy: 0.96875, Computation time: 1.0065762996673584\n",
      "Step: 2718, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.0267581939697266\n",
      "Step: 2719, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 0.7259140014648438\n",
      "Step: 2720, Loss: 0.9160556197166443, Accuracy: 1.0, Computation time: 0.9850094318389893\n",
      "Step: 2721, Loss: 0.9160014986991882, Accuracy: 1.0, Computation time: 0.8492550849914551\n",
      "Step: 2722, Loss: 0.916435182094574, Accuracy: 1.0, Computation time: 1.059079885482788\n",
      "Step: 2723, Loss: 0.9159146547317505, Accuracy: 1.0, Computation time: 0.9894566535949707\n",
      "Step: 2724, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 0.9301505088806152\n",
      "Step: 2725, Loss: 0.9159110188484192, Accuracy: 1.0, Computation time: 0.8650631904602051\n",
      "Step: 2726, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 0.8399746417999268\n",
      "Step: 2727, Loss: 0.9159800410270691, Accuracy: 1.0, Computation time: 1.1418805122375488\n",
      "Step: 2728, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 1.2071201801300049\n",
      "Step: 2729, Loss: 0.9159321784973145, Accuracy: 1.0, Computation time: 1.1170406341552734\n",
      "Step: 2730, Loss: 0.9308940768241882, Accuracy: 0.96875, Computation time: 0.88529372215271\n",
      "Step: 2731, Loss: 0.917198121547699, Accuracy: 1.0, Computation time: 0.7802412509918213\n",
      "Step: 2732, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 0.7783360481262207\n",
      "Step: 2733, Loss: 0.9161874055862427, Accuracy: 1.0, Computation time: 0.9417150020599365\n",
      "Step: 2734, Loss: 0.9160456657409668, Accuracy: 1.0, Computation time: 1.2013375759124756\n",
      "Step: 2735, Loss: 0.9164021015167236, Accuracy: 1.0, Computation time: 0.8775010108947754\n",
      "Step: 2736, Loss: 0.9160972237586975, Accuracy: 1.0, Computation time: 0.9424467086791992\n",
      "Step: 2737, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 0.9809625148773193\n",
      "Step: 2738, Loss: 0.9365240335464478, Accuracy: 0.96875, Computation time: 1.7716569900512695\n",
      "Step: 2739, Loss: 0.9375402927398682, Accuracy: 0.96875, Computation time: 0.9921190738677979\n",
      "Step: 2740, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.1417887210845947\n",
      "Step: 2741, Loss: 0.9161627888679504, Accuracy: 1.0, Computation time: 0.9680931568145752\n",
      "Step: 2742, Loss: 0.9224246144294739, Accuracy: 1.0, Computation time: 0.9607710838317871\n",
      "Step: 2743, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 0.9304723739624023\n",
      "Step: 2744, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 0.8882651329040527\n",
      "Step: 2745, Loss: 0.9160898327827454, Accuracy: 1.0, Computation time: 0.9617035388946533\n",
      "Step: 2746, Loss: 0.9160444140434265, Accuracy: 1.0, Computation time: 1.0040972232818604\n",
      "Step: 2747, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 1.0568208694458008\n",
      "Step: 2748, Loss: 0.9166386723518372, Accuracy: 1.0, Computation time: 1.0116503238677979\n",
      "Step: 2749, Loss: 0.9331941604614258, Accuracy: 0.96875, Computation time: 1.0757825374603271\n",
      "Step: 2750, Loss: 0.9159263968467712, Accuracy: 1.0, Computation time: 0.9182424545288086\n",
      "Step: 2751, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 0.8595414161682129\n",
      "Step: 2752, Loss: 0.9159205555915833, Accuracy: 1.0, Computation time: 0.8507978916168213\n",
      "Step: 2753, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.293755054473877\n",
      "Step: 2754, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 0.9388983249664307\n",
      "Step: 2755, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.0558526515960693\n",
      "Step: 2756, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.1235973834991455\n",
      "Step: 2757, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 0.8848981857299805\n",
      "Step: 2758, Loss: 0.9322725534439087, Accuracy: 0.96875, Computation time: 0.9118812084197998\n",
      "Step: 2759, Loss: 0.9158967137336731, Accuracy: 1.0, Computation time: 0.7923698425292969\n",
      "Step: 2760, Loss: 0.9161001443862915, Accuracy: 1.0, Computation time: 0.9779281616210938\n",
      "Step: 2761, Loss: 0.9160411357879639, Accuracy: 1.0, Computation time: 0.841073751449585\n",
      "Step: 2762, Loss: 0.9163012504577637, Accuracy: 1.0, Computation time: 0.8931910991668701\n",
      "Step: 2763, Loss: 0.9201036095619202, Accuracy: 1.0, Computation time: 1.0303831100463867\n",
      "Step: 2764, Loss: 0.929217517375946, Accuracy: 0.96875, Computation time: 0.9165918827056885\n",
      "Step: 2765, Loss: 0.9209352731704712, Accuracy: 1.0, Computation time: 1.1943705081939697\n",
      "Step: 2766, Loss: 0.916164219379425, Accuracy: 1.0, Computation time: 0.8850860595703125\n",
      "Step: 2767, Loss: 0.9162428975105286, Accuracy: 1.0, Computation time: 0.9686970710754395\n",
      "Step: 2768, Loss: 0.9165180325508118, Accuracy: 1.0, Computation time: 1.3661072254180908\n",
      "Step: 2769, Loss: 0.9375622272491455, Accuracy: 0.96875, Computation time: 1.1776304244995117\n",
      "Step: 2770, Loss: 0.9163022041320801, Accuracy: 1.0, Computation time: 1.1640150547027588\n",
      "Step: 2771, Loss: 0.9276591539382935, Accuracy: 0.96875, Computation time: 1.0286812782287598\n",
      "Step: 2772, Loss: 0.9159945249557495, Accuracy: 1.0, Computation time: 1.0274713039398193\n",
      "Step: 2773, Loss: 0.9160301685333252, Accuracy: 1.0, Computation time: 0.8475263118743896\n",
      "Step: 2774, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 1.0363678932189941\n",
      "Step: 2775, Loss: 0.9160301685333252, Accuracy: 1.0, Computation time: 1.0069406032562256\n",
      "Step: 2776, Loss: 0.9160070419311523, Accuracy: 1.0, Computation time: 0.8941209316253662\n",
      "Step: 2777, Loss: 0.9160316586494446, Accuracy: 1.0, Computation time: 0.9183063507080078\n",
      "Step: 2778, Loss: 0.9159456491470337, Accuracy: 1.0, Computation time: 0.9178848266601562\n",
      "Step: 2779, Loss: 0.9236615896224976, Accuracy: 1.0, Computation time: 0.8646390438079834\n",
      "########################\n",
      "Test loss: 1.0715782642364502, Test Accuracy_epoch20: 0.7702834606170654\n",
      "########################\n",
      "Step: 2780, Loss: 0.915936291217804, Accuracy: 1.0, Computation time: 0.7655913829803467\n",
      "Step: 2781, Loss: 0.9160463213920593, Accuracy: 1.0, Computation time: 0.8130652904510498\n",
      "Step: 2782, Loss: 0.9172563552856445, Accuracy: 1.0, Computation time: 1.056307077407837\n",
      "Step: 2783, Loss: 0.9162160754203796, Accuracy: 1.0, Computation time: 0.9715962409973145\n",
      "Step: 2784, Loss: 0.9161102175712585, Accuracy: 1.0, Computation time: 1.0430116653442383\n",
      "Step: 2785, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 0.9773674011230469\n",
      "Step: 2786, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 0.7508935928344727\n",
      "Step: 2787, Loss: 0.9159708619117737, Accuracy: 1.0, Computation time: 1.1013174057006836\n",
      "Step: 2788, Loss: 0.9377571940422058, Accuracy: 0.96875, Computation time: 1.1666429042816162\n",
      "Step: 2789, Loss: 0.9190770983695984, Accuracy: 1.0, Computation time: 0.9921722412109375\n",
      "Step: 2790, Loss: 0.9159008264541626, Accuracy: 1.0, Computation time: 0.85575270652771\n",
      "Step: 2791, Loss: 0.9376335144042969, Accuracy: 0.96875, Computation time: 0.9649600982666016\n",
      "Step: 2792, Loss: 0.9159652590751648, Accuracy: 1.0, Computation time: 1.1912713050842285\n",
      "Step: 2793, Loss: 0.9376519322395325, Accuracy: 0.96875, Computation time: 0.8982579708099365\n",
      "Step: 2794, Loss: 0.93765789270401, Accuracy: 0.96875, Computation time: 1.4131548404693604\n",
      "Step: 2795, Loss: 0.9159248471260071, Accuracy: 1.0, Computation time: 1.013991355895996\n",
      "Step: 2796, Loss: 0.9366403818130493, Accuracy: 0.96875, Computation time: 1.121885061264038\n",
      "Step: 2797, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 0.788766622543335\n",
      "Step: 2798, Loss: 0.9166995882987976, Accuracy: 1.0, Computation time: 0.7962181568145752\n",
      "Step: 2799, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.031224250793457\n",
      "Step: 2800, Loss: 0.9163521528244019, Accuracy: 1.0, Computation time: 1.4578328132629395\n",
      "Step: 2801, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 0.8657028675079346\n",
      "Step: 2802, Loss: 0.9160088896751404, Accuracy: 1.0, Computation time: 1.0607640743255615\n",
      "Step: 2803, Loss: 0.9314253330230713, Accuracy: 0.96875, Computation time: 1.3201653957366943\n",
      "Step: 2804, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 0.8764626979827881\n",
      "Step: 2805, Loss: 0.916579008102417, Accuracy: 1.0, Computation time: 1.0081210136413574\n",
      "Step: 2806, Loss: 0.9159939885139465, Accuracy: 1.0, Computation time: 1.1165671348571777\n",
      "Step: 2807, Loss: 0.9366022348403931, Accuracy: 0.96875, Computation time: 0.8329024314880371\n",
      "Step: 2808, Loss: 0.9161940217018127, Accuracy: 1.0, Computation time: 0.8598456382751465\n",
      "Step: 2809, Loss: 0.9159746766090393, Accuracy: 1.0, Computation time: 0.8428714275360107\n",
      "Step: 2810, Loss: 0.9377377033233643, Accuracy: 0.96875, Computation time: 0.77207350730896\n",
      "Step: 2811, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 0.8214888572692871\n",
      "Step: 2812, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 0.8185379505157471\n",
      "Step: 2813, Loss: 0.9242927432060242, Accuracy: 1.0, Computation time: 1.3247230052947998\n",
      "Step: 2814, Loss: 0.9160138368606567, Accuracy: 1.0, Computation time: 1.168675422668457\n",
      "Step: 2815, Loss: 0.916319727897644, Accuracy: 1.0, Computation time: 0.9213564395904541\n",
      "Step: 2816, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 0.9005029201507568\n",
      "Step: 2817, Loss: 0.9159767031669617, Accuracy: 1.0, Computation time: 0.7834451198577881\n",
      "Step: 2818, Loss: 0.9159249663352966, Accuracy: 1.0, Computation time: 0.8351073265075684\n",
      "Step: 2819, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 0.7991533279418945\n",
      "Step: 2820, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 0.8545711040496826\n",
      "Step: 2821, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.0636847019195557\n",
      "Step: 2822, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 0.8674938678741455\n",
      "Step: 2823, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 0.8273894786834717\n",
      "Step: 2824, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 0.9681456089019775\n",
      "Step: 2825, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 0.9137299060821533\n",
      "Step: 2826, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 0.7871015071868896\n",
      "Step: 2827, Loss: 0.9297479391098022, Accuracy: 0.96875, Computation time: 1.0553710460662842\n",
      "Step: 2828, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 0.784590482711792\n",
      "Step: 2829, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 0.7570512294769287\n",
      "Step: 2830, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 0.792278528213501\n",
      "Step: 2831, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 0.9868853092193604\n",
      "Step: 2832, Loss: 0.9159699082374573, Accuracy: 1.0, Computation time: 1.0770013332366943\n",
      "Step: 2833, Loss: 0.9159907698631287, Accuracy: 1.0, Computation time: 0.8242166042327881\n",
      "Step: 2834, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 0.7844388484954834\n",
      "Step: 2835, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 0.8748748302459717\n",
      "Step: 2836, Loss: 0.9159404635429382, Accuracy: 1.0, Computation time: 1.115067958831787\n",
      "Step: 2837, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 0.7975077629089355\n",
      "Step: 2838, Loss: 0.9161992073059082, Accuracy: 1.0, Computation time: 0.9252841472625732\n",
      "Step: 2839, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 0.8011643886566162\n",
      "Step: 2840, Loss: 0.9557147026062012, Accuracy: 0.9375, Computation time: 0.9150950908660889\n",
      "Step: 2841, Loss: 0.9160866141319275, Accuracy: 1.0, Computation time: 1.0125865936279297\n",
      "Step: 2842, Loss: 0.9159508347511292, Accuracy: 1.0, Computation time: 0.9299092292785645\n",
      "Step: 2843, Loss: 0.9160405397415161, Accuracy: 1.0, Computation time: 1.2772936820983887\n",
      "Step: 2844, Loss: 0.9159465432167053, Accuracy: 1.0, Computation time: 0.8312232494354248\n",
      "Step: 2845, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 0.862069845199585\n",
      "Step: 2846, Loss: 0.9160001277923584, Accuracy: 1.0, Computation time: 0.8198423385620117\n",
      "Step: 2847, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 0.8933238983154297\n",
      "Step: 2848, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 0.9016091823577881\n",
      "Step: 2849, Loss: 0.9165741205215454, Accuracy: 1.0, Computation time: 0.9327185153961182\n",
      "Step: 2850, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 0.8588912487030029\n",
      "Step: 2851, Loss: 0.9169555306434631, Accuracy: 1.0, Computation time: 0.7511796951293945\n",
      "Step: 2852, Loss: 0.9169070720672607, Accuracy: 1.0, Computation time: 1.2467079162597656\n",
      "Step: 2853, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.0066299438476562\n",
      "Step: 2854, Loss: 0.9207308292388916, Accuracy: 1.0, Computation time: 0.8801524639129639\n",
      "Step: 2855, Loss: 0.9159446954727173, Accuracy: 1.0, Computation time: 0.7792782783508301\n",
      "Step: 2856, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 0.9024112224578857\n",
      "Step: 2857, Loss: 0.9160676002502441, Accuracy: 1.0, Computation time: 1.1627697944641113\n",
      "Step: 2858, Loss: 0.9161145687103271, Accuracy: 1.0, Computation time: 0.830019474029541\n",
      "Step: 2859, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.9164049625396729\n",
      "Step: 2860, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 0.7668509483337402\n",
      "Step: 2861, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 0.9118056297302246\n",
      "Step: 2862, Loss: 0.9159618616104126, Accuracy: 1.0, Computation time: 0.720513105392456\n",
      "Step: 2863, Loss: 0.9376586079597473, Accuracy: 0.96875, Computation time: 0.7520468235015869\n",
      "Step: 2864, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 0.9152801036834717\n",
      "Step: 2865, Loss: 0.9294257760047913, Accuracy: 0.96875, Computation time: 0.9381580352783203\n",
      "Step: 2866, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 0.8425140380859375\n",
      "Step: 2867, Loss: 0.9313125610351562, Accuracy: 0.96875, Computation time: 0.9873833656311035\n",
      "Step: 2868, Loss: 0.9181525111198425, Accuracy: 1.0, Computation time: 0.8388457298278809\n",
      "Step: 2869, Loss: 0.9371597766876221, Accuracy: 0.96875, Computation time: 1.0181570053100586\n",
      "Step: 2870, Loss: 0.9159834980964661, Accuracy: 1.0, Computation time: 0.9122731685638428\n",
      "Step: 2871, Loss: 0.9159608483314514, Accuracy: 1.0, Computation time: 0.7612466812133789\n",
      "Step: 2872, Loss: 0.9454972147941589, Accuracy: 0.96875, Computation time: 1.137542724609375\n",
      "Step: 2873, Loss: 0.9160172939300537, Accuracy: 1.0, Computation time: 0.9481759071350098\n",
      "Step: 2874, Loss: 0.916069507598877, Accuracy: 1.0, Computation time: 0.9784977436065674\n",
      "Step: 2875, Loss: 0.9160332083702087, Accuracy: 1.0, Computation time: 0.8878214359283447\n",
      "Step: 2876, Loss: 0.9159711599349976, Accuracy: 1.0, Computation time: 0.8739144802093506\n",
      "Step: 2877, Loss: 0.9373860955238342, Accuracy: 0.96875, Computation time: 1.2797958850860596\n",
      "Step: 2878, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 0.8413004875183105\n",
      "Step: 2879, Loss: 0.9375324249267578, Accuracy: 0.96875, Computation time: 0.7621047496795654\n",
      "Step: 2880, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 0.7824993133544922\n",
      "Step: 2881, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 0.8763830661773682\n",
      "Step: 2882, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 0.8715353012084961\n",
      "Step: 2883, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.1557133197784424\n",
      "Step: 2884, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.7631123065948486\n",
      "Step: 2885, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.3255789279937744\n",
      "Step: 2886, Loss: 0.9205493927001953, Accuracy: 1.0, Computation time: 0.8915190696716309\n",
      "Step: 2887, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.8459064960479736\n",
      "Step: 2888, Loss: 0.9376389384269714, Accuracy: 0.96875, Computation time: 0.7417123317718506\n",
      "Step: 2889, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 0.88299560546875\n",
      "Step: 2890, Loss: 0.9159886240959167, Accuracy: 1.0, Computation time: 0.8588788509368896\n",
      "Step: 2891, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 0.8349614143371582\n",
      "Step: 2892, Loss: 0.9160898327827454, Accuracy: 1.0, Computation time: 0.9521751403808594\n",
      "Step: 2893, Loss: 0.9159113764762878, Accuracy: 1.0, Computation time: 0.7841401100158691\n",
      "Step: 2894, Loss: 0.9158954620361328, Accuracy: 1.0, Computation time: 1.3716015815734863\n",
      "Step: 2895, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 0.7785072326660156\n",
      "Step: 2896, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 0.8799459934234619\n",
      "Step: 2897, Loss: 0.9163353443145752, Accuracy: 1.0, Computation time: 0.8496432304382324\n",
      "Step: 2898, Loss: 0.915911078453064, Accuracy: 1.0, Computation time: 0.8703863620758057\n",
      "Step: 2899, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 0.7299237251281738\n",
      "Step: 2900, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.203852653503418\n",
      "Step: 2901, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.8978016376495361\n",
      "Step: 2902, Loss: 0.9165493845939636, Accuracy: 1.0, Computation time: 1.0548052787780762\n",
      "Step: 2903, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 0.8078548908233643\n",
      "Step: 2904, Loss: 0.9167614579200745, Accuracy: 1.0, Computation time: 1.1818921566009521\n",
      "Step: 2905, Loss: 0.9159267544746399, Accuracy: 1.0, Computation time: 0.9867289066314697\n",
      "Step: 2906, Loss: 0.9374587535858154, Accuracy: 0.96875, Computation time: 0.9844598770141602\n",
      "Step: 2907, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 0.751884937286377\n",
      "Step: 2908, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.0447940826416016\n",
      "Step: 2909, Loss: 0.9353014230728149, Accuracy: 0.96875, Computation time: 0.9500415325164795\n",
      "Step: 2910, Loss: 0.9316919445991516, Accuracy: 0.96875, Computation time: 1.111332654953003\n",
      "Step: 2911, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 1.2347898483276367\n",
      "Step: 2912, Loss: 0.9159769415855408, Accuracy: 1.0, Computation time: 0.9819374084472656\n",
      "Step: 2913, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 0.9008336067199707\n",
      "Step: 2914, Loss: 0.916000485420227, Accuracy: 1.0, Computation time: 0.7314395904541016\n",
      "Step: 2915, Loss: 0.9380896091461182, Accuracy: 0.96875, Computation time: 1.0125391483306885\n",
      "Step: 2916, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 0.9846456050872803\n",
      "Step: 2917, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.8810787200927734\n",
      "Step: 2918, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 0.9063858985900879\n",
      "########################\n",
      "Test loss: 1.065879464149475, Test Accuracy_epoch21: 0.7781035900115967\n",
      "########################\n",
      "Step: 2919, Loss: 0.9253061413764954, Accuracy: 0.96875, Computation time: 1.1173028945922852\n",
      "Step: 2920, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 0.8949706554412842\n",
      "Step: 2921, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 0.7322123050689697\n",
      "Step: 2922, Loss: 0.9288711547851562, Accuracy: 0.96875, Computation time: 0.8647096157073975\n",
      "Step: 2923, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 0.862762451171875\n",
      "Step: 2924, Loss: 0.9591522812843323, Accuracy: 0.9375, Computation time: 0.9539647102355957\n",
      "Step: 2925, Loss: 0.9159387946128845, Accuracy: 1.0, Computation time: 0.904768705368042\n",
      "Step: 2926, Loss: 0.9159137606620789, Accuracy: 1.0, Computation time: 0.8907158374786377\n",
      "Step: 2927, Loss: 0.9189643859863281, Accuracy: 1.0, Computation time: 0.7962737083435059\n",
      "Step: 2928, Loss: 0.9159508347511292, Accuracy: 1.0, Computation time: 0.8459587097167969\n",
      "Step: 2929, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 0.9767303466796875\n",
      "Step: 2930, Loss: 0.9159411787986755, Accuracy: 1.0, Computation time: 1.146592617034912\n",
      "Step: 2931, Loss: 0.9159659147262573, Accuracy: 1.0, Computation time: 0.9720790386199951\n",
      "Step: 2932, Loss: 0.9341224431991577, Accuracy: 0.96875, Computation time: 0.8203291893005371\n",
      "Step: 2933, Loss: 0.9373666644096375, Accuracy: 0.96875, Computation time: 1.0294826030731201\n",
      "Step: 2934, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.1831238269805908\n",
      "Step: 2935, Loss: 0.9159939289093018, Accuracy: 1.0, Computation time: 0.8948733806610107\n",
      "Step: 2936, Loss: 0.9363716840744019, Accuracy: 0.96875, Computation time: 0.9315760135650635\n",
      "Step: 2937, Loss: 0.9159842133522034, Accuracy: 1.0, Computation time: 1.0173566341400146\n",
      "Step: 2938, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 0.8302004337310791\n",
      "Step: 2939, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 0.9334616661071777\n",
      "Step: 2940, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 0.9579641819000244\n",
      "Step: 2941, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.0855720043182373\n",
      "Step: 2942, Loss: 0.9159319400787354, Accuracy: 1.0, Computation time: 1.0777103900909424\n",
      "Step: 2943, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 0.8965058326721191\n",
      "Step: 2944, Loss: 0.9159950613975525, Accuracy: 1.0, Computation time: 1.0458261966705322\n",
      "Step: 2945, Loss: 0.9375728368759155, Accuracy: 0.96875, Computation time: 1.3310537338256836\n",
      "Step: 2946, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 0.8768329620361328\n",
      "Step: 2947, Loss: 0.924985408782959, Accuracy: 1.0, Computation time: 1.0428755283355713\n",
      "Step: 2948, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 0.9767916202545166\n",
      "Step: 2949, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 1.008897066116333\n",
      "Step: 2950, Loss: 0.9159551858901978, Accuracy: 1.0, Computation time: 1.7106904983520508\n",
      "Step: 2951, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 0.917241096496582\n",
      "Step: 2952, Loss: 0.915939211845398, Accuracy: 1.0, Computation time: 1.172302484512329\n",
      "Step: 2953, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.0158052444458008\n",
      "Step: 2954, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 0.811098575592041\n",
      "Step: 2955, Loss: 0.916349470615387, Accuracy: 1.0, Computation time: 0.7830281257629395\n",
      "Step: 2956, Loss: 0.9179978370666504, Accuracy: 1.0, Computation time: 0.843041181564331\n",
      "Step: 2957, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 0.9178388118743896\n",
      "Step: 2958, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 0.9875340461730957\n",
      "Step: 2959, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 0.9084489345550537\n",
      "Step: 2960, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 0.9424483776092529\n",
      "Step: 2961, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.00278639793396\n",
      "Step: 2962, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.0553038120269775\n",
      "Step: 2963, Loss: 0.929007351398468, Accuracy: 0.96875, Computation time: 1.071770429611206\n",
      "Step: 2964, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 0.8759748935699463\n",
      "Step: 2965, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 0.8196344375610352\n",
      "Step: 2966, Loss: 0.9165229797363281, Accuracy: 1.0, Computation time: 0.9641175270080566\n",
      "Step: 2967, Loss: 0.926970899105072, Accuracy: 0.96875, Computation time: 1.015854835510254\n",
      "Step: 2968, Loss: 0.9159423112869263, Accuracy: 1.0, Computation time: 0.9525346755981445\n",
      "Step: 2969, Loss: 0.9159746170043945, Accuracy: 1.0, Computation time: 0.93886399269104\n",
      "Step: 2970, Loss: 0.9375261664390564, Accuracy: 0.96875, Computation time: 0.8234894275665283\n",
      "Step: 2971, Loss: 0.9159888029098511, Accuracy: 1.0, Computation time: 1.0278406143188477\n",
      "Step: 2972, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 1.155623197555542\n",
      "Step: 2973, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 0.8848884105682373\n",
      "Step: 2974, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 0.8212172985076904\n",
      "Step: 2975, Loss: 0.9355720281600952, Accuracy: 0.96875, Computation time: 1.1060643196105957\n",
      "Step: 2976, Loss: 0.9333686828613281, Accuracy: 0.96875, Computation time: 1.1887943744659424\n",
      "Step: 2977, Loss: 0.928057849407196, Accuracy: 0.96875, Computation time: 0.7865710258483887\n",
      "Step: 2978, Loss: 0.9159224629402161, Accuracy: 1.0, Computation time: 0.8114190101623535\n",
      "Step: 2979, Loss: 0.9159691333770752, Accuracy: 1.0, Computation time: 0.8029773235321045\n",
      "Step: 2980, Loss: 0.9161198735237122, Accuracy: 1.0, Computation time: 1.072329044342041\n",
      "Step: 2981, Loss: 0.9377490878105164, Accuracy: 0.96875, Computation time: 0.9188554286956787\n",
      "Step: 2982, Loss: 0.916583240032196, Accuracy: 1.0, Computation time: 0.8661603927612305\n",
      "Step: 2983, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 0.9891862869262695\n",
      "Step: 2984, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 0.9431424140930176\n",
      "Step: 2985, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 0.8468124866485596\n",
      "Step: 2986, Loss: 0.9162169098854065, Accuracy: 1.0, Computation time: 0.8215939998626709\n",
      "Step: 2987, Loss: 0.9159548282623291, Accuracy: 1.0, Computation time: 0.7642767429351807\n",
      "Step: 2988, Loss: 0.9376533031463623, Accuracy: 0.96875, Computation time: 0.942634105682373\n",
      "Step: 2989, Loss: 0.9166516065597534, Accuracy: 1.0, Computation time: 0.7943735122680664\n",
      "Step: 2990, Loss: 0.9231382608413696, Accuracy: 1.0, Computation time: 0.7844016551971436\n",
      "Step: 2991, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.059272289276123\n",
      "Step: 2992, Loss: 0.9396277070045471, Accuracy: 0.96875, Computation time: 0.860724687576294\n",
      "Step: 2993, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 0.9848215579986572\n",
      "Step: 2994, Loss: 0.9162822365760803, Accuracy: 1.0, Computation time: 1.3031795024871826\n",
      "Step: 2995, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 0.7710235118865967\n",
      "Step: 2996, Loss: 0.9159905314445496, Accuracy: 1.0, Computation time: 0.878495454788208\n",
      "Step: 2997, Loss: 0.9159707427024841, Accuracy: 1.0, Computation time: 0.7593827247619629\n",
      "Step: 2998, Loss: 0.9229292273521423, Accuracy: 1.0, Computation time: 0.7365336418151855\n",
      "Step: 2999, Loss: 0.9159908294677734, Accuracy: 1.0, Computation time: 0.9431741237640381\n",
      "Step: 3000, Loss: 0.9494922757148743, Accuracy: 0.9375, Computation time: 0.8849115371704102\n",
      "Step: 3001, Loss: 0.9160543084144592, Accuracy: 1.0, Computation time: 0.7927165031433105\n",
      "Step: 3002, Loss: 0.9160796999931335, Accuracy: 1.0, Computation time: 0.9918665885925293\n",
      "Step: 3003, Loss: 0.9162401556968689, Accuracy: 1.0, Computation time: 0.8812642097473145\n",
      "Step: 3004, Loss: 0.9160793423652649, Accuracy: 1.0, Computation time: 0.9104599952697754\n",
      "Step: 3005, Loss: 0.9160200357437134, Accuracy: 1.0, Computation time: 0.8765914440155029\n",
      "Step: 3006, Loss: 0.9159606099128723, Accuracy: 1.0, Computation time: 0.9152777194976807\n",
      "Step: 3007, Loss: 0.9159637689590454, Accuracy: 1.0, Computation time: 1.006446361541748\n",
      "Step: 3008, Loss: 0.937637209892273, Accuracy: 0.96875, Computation time: 1.0665535926818848\n",
      "Step: 3009, Loss: 0.918842613697052, Accuracy: 1.0, Computation time: 0.8275425434112549\n",
      "Step: 3010, Loss: 0.9161325097084045, Accuracy: 1.0, Computation time: 1.6618766784667969\n",
      "Step: 3011, Loss: 0.9159668684005737, Accuracy: 1.0, Computation time: 0.9956388473510742\n",
      "Step: 3012, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 0.7454040050506592\n",
      "Step: 3013, Loss: 0.9376553297042847, Accuracy: 0.96875, Computation time: 0.900538444519043\n",
      "Step: 3014, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 0.8876771926879883\n",
      "Step: 3015, Loss: 0.9380236268043518, Accuracy: 0.96875, Computation time: 0.9306409358978271\n",
      "Step: 3016, Loss: 0.9159521460533142, Accuracy: 1.0, Computation time: 0.8636729717254639\n",
      "Step: 3017, Loss: 0.937594473361969, Accuracy: 0.96875, Computation time: 0.9411320686340332\n",
      "Step: 3018, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 0.8818602561950684\n",
      "Step: 3019, Loss: 0.9179344177246094, Accuracy: 1.0, Computation time: 0.8261094093322754\n",
      "Step: 3020, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 0.7545344829559326\n",
      "Step: 3021, Loss: 0.9162834882736206, Accuracy: 1.0, Computation time: 1.1793348789215088\n",
      "Step: 3022, Loss: 0.9158900380134583, Accuracy: 1.0, Computation time: 1.000605821609497\n",
      "Step: 3023, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.0076308250427246\n",
      "Step: 3024, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.0651524066925049\n",
      "Step: 3025, Loss: 0.9159359335899353, Accuracy: 1.0, Computation time: 0.9210624694824219\n",
      "Step: 3026, Loss: 0.9165859222412109, Accuracy: 1.0, Computation time: 1.172027826309204\n",
      "Step: 3027, Loss: 0.9190780520439148, Accuracy: 1.0, Computation time: 0.9411466121673584\n",
      "Step: 3028, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 0.9974298477172852\n",
      "Step: 3029, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.0052452087402344\n",
      "Step: 3030, Loss: 0.9458955526351929, Accuracy: 0.96875, Computation time: 0.9692041873931885\n",
      "Step: 3031, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 0.9551372528076172\n",
      "Step: 3032, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 1.0275602340698242\n",
      "Step: 3033, Loss: 0.9159294962882996, Accuracy: 1.0, Computation time: 1.1099271774291992\n",
      "Step: 3034, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 0.8822617530822754\n",
      "Step: 3035, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 0.9582006931304932\n",
      "Step: 3036, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 0.8487546443939209\n",
      "Step: 3037, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 0.79341721534729\n",
      "Step: 3038, Loss: 0.917651355266571, Accuracy: 1.0, Computation time: 1.3295934200286865\n",
      "Step: 3039, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 0.9408607482910156\n",
      "Step: 3040, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 0.9351513385772705\n",
      "Step: 3041, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 0.8803682327270508\n",
      "Step: 3042, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 0.9487490653991699\n",
      "Step: 3043, Loss: 0.9169369339942932, Accuracy: 1.0, Computation time: 1.0511794090270996\n",
      "Step: 3044, Loss: 0.9374964237213135, Accuracy: 0.96875, Computation time: 1.0187506675720215\n",
      "Step: 3045, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 0.8580870628356934\n",
      "Step: 3046, Loss: 0.9184108972549438, Accuracy: 1.0, Computation time: 1.2972466945648193\n",
      "Step: 3047, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 0.9441041946411133\n",
      "Step: 3048, Loss: 0.9160951972007751, Accuracy: 1.0, Computation time: 0.9425530433654785\n",
      "Step: 3049, Loss: 0.9161055088043213, Accuracy: 1.0, Computation time: 1.4560563564300537\n",
      "Step: 3050, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.0393860340118408\n",
      "Step: 3051, Loss: 0.9363973140716553, Accuracy: 0.96875, Computation time: 1.5038201808929443\n",
      "Step: 3052, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.1887836456298828\n",
      "Step: 3053, Loss: 0.9314123392105103, Accuracy: 0.96875, Computation time: 1.0282700061798096\n",
      "Step: 3054, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 1.1009962558746338\n",
      "Step: 3055, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.0605905055999756\n",
      "Step: 3056, Loss: 0.9375038743019104, Accuracy: 0.96875, Computation time: 0.9819512367248535\n",
      "Step: 3057, Loss: 0.9374323487281799, Accuracy: 0.96875, Computation time: 1.2537317276000977\n",
      "########################\n",
      "Test loss: 1.074059009552002, Test Accuracy_epoch22: 0.7683284282684326\n",
      "########################\n",
      "Step: 3058, Loss: 0.937631368637085, Accuracy: 0.96875, Computation time: 0.9918057918548584\n",
      "Step: 3059, Loss: 0.9376426935195923, Accuracy: 0.96875, Computation time: 1.127777099609375\n",
      "Step: 3060, Loss: 0.9167091250419617, Accuracy: 1.0, Computation time: 1.0203967094421387\n",
      "Step: 3061, Loss: 0.9161579012870789, Accuracy: 1.0, Computation time: 0.8823192119598389\n",
      "Step: 3062, Loss: 0.920683741569519, Accuracy: 1.0, Computation time: 0.9087369441986084\n",
      "Step: 3063, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 0.9408121109008789\n",
      "Step: 3064, Loss: 0.9159334897994995, Accuracy: 1.0, Computation time: 0.8787014484405518\n",
      "Step: 3065, Loss: 0.9373443722724915, Accuracy: 0.96875, Computation time: 0.8949315547943115\n",
      "Step: 3066, Loss: 0.9161121845245361, Accuracy: 1.0, Computation time: 0.9350371360778809\n",
      "Step: 3067, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 0.8514888286590576\n",
      "Step: 3068, Loss: 0.9160301685333252, Accuracy: 1.0, Computation time: 0.9417760372161865\n",
      "Step: 3069, Loss: 0.9161123037338257, Accuracy: 1.0, Computation time: 1.1286647319793701\n",
      "Step: 3070, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 1.1092560291290283\n",
      "Step: 3071, Loss: 0.9249557256698608, Accuracy: 1.0, Computation time: 1.009939432144165\n",
      "Step: 3072, Loss: 0.9162607789039612, Accuracy: 1.0, Computation time: 1.009352207183838\n",
      "Step: 3073, Loss: 0.9545279741287231, Accuracy: 0.9375, Computation time: 0.9498996734619141\n",
      "Step: 3074, Loss: 0.916001558303833, Accuracy: 1.0, Computation time: 1.140601396560669\n",
      "Step: 3075, Loss: 0.9160827994346619, Accuracy: 1.0, Computation time: 1.0145955085754395\n",
      "Step: 3076, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 0.9683840274810791\n",
      "Step: 3077, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.1195039749145508\n",
      "Step: 3078, Loss: 0.9180864691734314, Accuracy: 1.0, Computation time: 1.2126026153564453\n",
      "Step: 3079, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 0.7394511699676514\n",
      "Step: 3080, Loss: 0.9377002716064453, Accuracy: 0.96875, Computation time: 0.8545575141906738\n",
      "Step: 3081, Loss: 0.915932834148407, Accuracy: 1.0, Computation time: 0.8216457366943359\n",
      "Step: 3082, Loss: 0.9169019460678101, Accuracy: 1.0, Computation time: 1.018697738647461\n",
      "Step: 3083, Loss: 0.9159355759620667, Accuracy: 1.0, Computation time: 0.8562695980072021\n",
      "Step: 3084, Loss: 0.9168500304222107, Accuracy: 1.0, Computation time: 1.3910534381866455\n",
      "Step: 3085, Loss: 0.9159521460533142, Accuracy: 1.0, Computation time: 0.834221363067627\n",
      "Step: 3086, Loss: 0.9196485877037048, Accuracy: 1.0, Computation time: 0.8896043300628662\n",
      "Step: 3087, Loss: 0.9561758637428284, Accuracy: 0.9375, Computation time: 1.1471412181854248\n",
      "Step: 3088, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.9008073806762695\n",
      "Step: 3089, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 0.9364759922027588\n",
      "Step: 3090, Loss: 0.9159488081932068, Accuracy: 1.0, Computation time: 0.7304508686065674\n",
      "Step: 3091, Loss: 0.9162058234214783, Accuracy: 1.0, Computation time: 1.0240020751953125\n",
      "Step: 3092, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 0.8601350784301758\n",
      "Step: 3093, Loss: 0.9295023679733276, Accuracy: 0.96875, Computation time: 1.2211365699768066\n",
      "Step: 3094, Loss: 0.9591706991195679, Accuracy: 0.9375, Computation time: 0.7594046592712402\n",
      "Step: 3095, Loss: 0.9327624440193176, Accuracy: 0.96875, Computation time: 0.9586460590362549\n",
      "Step: 3096, Loss: 0.9163073897361755, Accuracy: 1.0, Computation time: 0.8376801013946533\n",
      "Step: 3097, Loss: 0.9161710143089294, Accuracy: 1.0, Computation time: 1.0921807289123535\n",
      "Step: 3098, Loss: 0.9159560203552246, Accuracy: 1.0, Computation time: 0.840850830078125\n",
      "Step: 3099, Loss: 0.9162919521331787, Accuracy: 1.0, Computation time: 0.9740111827850342\n",
      "Step: 3100, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 0.7444267272949219\n",
      "Step: 3101, Loss: 0.9201290607452393, Accuracy: 1.0, Computation time: 1.0264592170715332\n",
      "Step: 3102, Loss: 0.9376933574676514, Accuracy: 0.96875, Computation time: 0.751779317855835\n",
      "Step: 3103, Loss: 0.9159480333328247, Accuracy: 1.0, Computation time: 0.9416382312774658\n",
      "Step: 3104, Loss: 0.9159784913063049, Accuracy: 1.0, Computation time: 1.0141212940216064\n",
      "Step: 3105, Loss: 0.9159400463104248, Accuracy: 1.0, Computation time: 0.851060152053833\n",
      "Step: 3106, Loss: 0.9159634709358215, Accuracy: 1.0, Computation time: 1.053189992904663\n",
      "Step: 3107, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 1.1511268615722656\n",
      "Step: 3108, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.3235859870910645\n",
      "Step: 3109, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.1189672946929932\n",
      "Step: 3110, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 0.8506851196289062\n",
      "Step: 3111, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.0188848972320557\n",
      "Step: 3112, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 0.9966259002685547\n",
      "Step: 3113, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 0.7535226345062256\n",
      "Step: 3114, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.215585470199585\n",
      "Step: 3115, Loss: 0.9158753156661987, Accuracy: 1.0, Computation time: 0.8441193103790283\n",
      "Step: 3116, Loss: 0.9159421324729919, Accuracy: 1.0, Computation time: 0.9861841201782227\n",
      "Step: 3117, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 0.997178316116333\n",
      "Step: 3118, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 0.88667893409729\n",
      "Step: 3119, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.9605216979980469\n",
      "Step: 3120, Loss: 0.9373845458030701, Accuracy: 0.96875, Computation time: 0.8111350536346436\n",
      "Step: 3121, Loss: 0.9164324998855591, Accuracy: 1.0, Computation time: 1.160400390625\n",
      "Step: 3122, Loss: 0.9161458015441895, Accuracy: 1.0, Computation time: 1.3175792694091797\n",
      "Step: 3123, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 0.9697442054748535\n",
      "Step: 3124, Loss: 0.9159607291221619, Accuracy: 1.0, Computation time: 0.9908359050750732\n",
      "Step: 3125, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 0.9795923233032227\n",
      "Step: 3126, Loss: 0.937305212020874, Accuracy: 0.96875, Computation time: 1.7328975200653076\n",
      "Step: 3127, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 0.9981575012207031\n",
      "Step: 3128, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.0412101745605469\n",
      "Step: 3129, Loss: 0.919914186000824, Accuracy: 1.0, Computation time: 1.025947093963623\n",
      "Step: 3130, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 0.9079403877258301\n",
      "Step: 3131, Loss: 0.9340790510177612, Accuracy: 0.96875, Computation time: 0.8376634120941162\n",
      "Step: 3132, Loss: 0.9159008264541626, Accuracy: 1.0, Computation time: 0.9028451442718506\n",
      "Step: 3133, Loss: 0.9253548383712769, Accuracy: 0.96875, Computation time: 0.9715521335601807\n",
      "Step: 3134, Loss: 0.9244478344917297, Accuracy: 1.0, Computation time: 1.0190165042877197\n",
      "Step: 3135, Loss: 0.9159929752349854, Accuracy: 1.0, Computation time: 1.221771001815796\n",
      "Step: 3136, Loss: 0.9159356355667114, Accuracy: 1.0, Computation time: 0.8525948524475098\n",
      "Step: 3137, Loss: 0.9219115972518921, Accuracy: 1.0, Computation time: 1.2481718063354492\n",
      "Step: 3138, Loss: 0.9160088300704956, Accuracy: 1.0, Computation time: 1.3814027309417725\n",
      "Step: 3139, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 0.8729376792907715\n",
      "Step: 3140, Loss: 0.9376718997955322, Accuracy: 0.96875, Computation time: 0.983191728591919\n",
      "Step: 3141, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 0.9923567771911621\n",
      "Step: 3142, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 0.8795814514160156\n",
      "Step: 3143, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 0.7357227802276611\n",
      "Step: 3144, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 0.9237136840820312\n",
      "Step: 3145, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 0.7782034873962402\n",
      "Step: 3146, Loss: 0.9591951966285706, Accuracy: 0.9375, Computation time: 0.8897109031677246\n",
      "Step: 3147, Loss: 0.9163543581962585, Accuracy: 1.0, Computation time: 1.1007554531097412\n",
      "Step: 3148, Loss: 0.9163656234741211, Accuracy: 1.0, Computation time: 0.7323431968688965\n",
      "Step: 3149, Loss: 0.9331708550453186, Accuracy: 0.96875, Computation time: 0.8118572235107422\n",
      "Step: 3150, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.287297010421753\n",
      "Step: 3151, Loss: 0.9160398840904236, Accuracy: 1.0, Computation time: 0.9860787391662598\n",
      "Step: 3152, Loss: 0.9159998893737793, Accuracy: 1.0, Computation time: 0.9437129497528076\n",
      "Step: 3153, Loss: 0.9159904718399048, Accuracy: 1.0, Computation time: 0.9035465717315674\n",
      "Step: 3154, Loss: 0.9159374833106995, Accuracy: 1.0, Computation time: 0.8837399482727051\n",
      "Step: 3155, Loss: 0.9204133152961731, Accuracy: 1.0, Computation time: 1.0696742534637451\n",
      "Step: 3156, Loss: 0.9159917235374451, Accuracy: 1.0, Computation time: 1.0639488697052002\n",
      "Step: 3157, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 0.8510637283325195\n",
      "Step: 3158, Loss: 0.9162460565567017, Accuracy: 1.0, Computation time: 0.9046707153320312\n",
      "Step: 3159, Loss: 0.9160090088844299, Accuracy: 1.0, Computation time: 0.9043247699737549\n",
      "Step: 3160, Loss: 0.9159249663352966, Accuracy: 1.0, Computation time: 0.8694334030151367\n",
      "Step: 3161, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.106907844543457\n",
      "Step: 3162, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 0.870934247970581\n",
      "Step: 3163, Loss: 0.9159324765205383, Accuracy: 1.0, Computation time: 1.6627998352050781\n",
      "Step: 3164, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 0.8117077350616455\n",
      "Step: 3165, Loss: 0.916976809501648, Accuracy: 1.0, Computation time: 0.876502513885498\n",
      "Step: 3166, Loss: 0.9159095883369446, Accuracy: 1.0, Computation time: 0.8137521743774414\n",
      "Step: 3167, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 0.8931708335876465\n",
      "Step: 3168, Loss: 0.9377002120018005, Accuracy: 0.96875, Computation time: 0.8642916679382324\n",
      "Step: 3169, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 0.8230171203613281\n",
      "Step: 3170, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 0.7912535667419434\n",
      "Step: 3171, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 0.8837099075317383\n",
      "Step: 3172, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 0.8926815986633301\n",
      "Step: 3173, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 0.887864351272583\n",
      "Step: 3174, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.1655466556549072\n",
      "Step: 3175, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 0.9725930690765381\n",
      "Step: 3176, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 0.9204905033111572\n",
      "Step: 3177, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 0.870741605758667\n",
      "Step: 3178, Loss: 0.9160667657852173, Accuracy: 1.0, Computation time: 0.8635280132293701\n",
      "Step: 3179, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 0.7771265506744385\n",
      "Step: 3180, Loss: 0.9177758693695068, Accuracy: 1.0, Computation time: 0.931969404220581\n",
      "Step: 3181, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 0.8063359260559082\n",
      "Step: 3182, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 0.8651080131530762\n",
      "Step: 3183, Loss: 0.9160351753234863, Accuracy: 1.0, Computation time: 1.3524346351623535\n",
      "Step: 3184, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 1.1885695457458496\n",
      "Step: 3185, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 0.9897909164428711\n",
      "Step: 3186, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 0.8101236820220947\n",
      "Step: 3187, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.0186254978179932\n",
      "Step: 3188, Loss: 0.934419572353363, Accuracy: 0.96875, Computation time: 0.8137731552124023\n",
      "Step: 3189, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 0.900634765625\n",
      "Step: 3190, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.7703890800476074\n",
      "Step: 3191, Loss: 0.9374717473983765, Accuracy: 0.96875, Computation time: 0.794508695602417\n",
      "Step: 3192, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 0.8446400165557861\n",
      "Step: 3193, Loss: 0.9371183514595032, Accuracy: 0.96875, Computation time: 0.8440229892730713\n",
      "Step: 3194, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 0.7730064392089844\n",
      "Step: 3195, Loss: 0.9271680116653442, Accuracy: 0.96875, Computation time: 0.8558833599090576\n",
      "Step: 3196, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 0.9011378288269043\n",
      "########################\n",
      "Test loss: 1.0707488059997559, Test Accuracy_epoch23: 0.7761485576629639\n",
      "########################\n",
      "Step: 3197, Loss: 0.9159142374992371, Accuracy: 1.0, Computation time: 1.0555081367492676\n",
      "Step: 3198, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 0.8963322639465332\n",
      "Step: 3199, Loss: 0.915932297706604, Accuracy: 1.0, Computation time: 0.8232207298278809\n",
      "Step: 3200, Loss: 0.9161607623100281, Accuracy: 1.0, Computation time: 0.9272463321685791\n",
      "Step: 3201, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.0003416538238525\n",
      "Step: 3202, Loss: 0.9159539937973022, Accuracy: 1.0, Computation time: 0.7784130573272705\n",
      "Step: 3203, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 0.7307016849517822\n",
      "Step: 3204, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 0.7453670501708984\n",
      "Step: 3205, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 0.8174290657043457\n",
      "Step: 3206, Loss: 0.9160501956939697, Accuracy: 1.0, Computation time: 0.9069130420684814\n",
      "Step: 3207, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 0.7932758331298828\n",
      "Step: 3208, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 0.8061046600341797\n",
      "Step: 3209, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 0.8581254482269287\n",
      "Step: 3210, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 0.7098324298858643\n",
      "Step: 3211, Loss: 0.9166234731674194, Accuracy: 1.0, Computation time: 0.8074848651885986\n",
      "Step: 3212, Loss: 0.9312165379524231, Accuracy: 0.96875, Computation time: 1.1325840950012207\n",
      "Step: 3213, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 0.8362221717834473\n",
      "Step: 3214, Loss: 0.9159309267997742, Accuracy: 1.0, Computation time: 0.8670246601104736\n",
      "Step: 3215, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 0.7306501865386963\n",
      "Step: 3216, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 0.8193681240081787\n",
      "Step: 3217, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 0.9606637954711914\n",
      "Step: 3218, Loss: 0.9273125529289246, Accuracy: 0.96875, Computation time: 0.8418231010437012\n",
      "Step: 3219, Loss: 0.9191746115684509, Accuracy: 1.0, Computation time: 1.6736688613891602\n",
      "Step: 3220, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 0.763333797454834\n",
      "Step: 3221, Loss: 0.9159848690032959, Accuracy: 1.0, Computation time: 0.8677771091461182\n",
      "Step: 3222, Loss: 0.9160102605819702, Accuracy: 1.0, Computation time: 1.511824607849121\n",
      "Step: 3223, Loss: 0.9160053133964539, Accuracy: 1.0, Computation time: 0.9514458179473877\n",
      "Step: 3224, Loss: 0.9362267255783081, Accuracy: 0.96875, Computation time: 0.9411225318908691\n",
      "Step: 3225, Loss: 0.9159616827964783, Accuracy: 1.0, Computation time: 1.142723798751831\n",
      "Step: 3226, Loss: 0.9159101247787476, Accuracy: 1.0, Computation time: 0.9026916027069092\n",
      "Step: 3227, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 0.7245292663574219\n",
      "Step: 3228, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 0.9593000411987305\n",
      "Step: 3229, Loss: 0.9162024259567261, Accuracy: 1.0, Computation time: 1.2196941375732422\n",
      "Step: 3230, Loss: 0.9320818781852722, Accuracy: 0.96875, Computation time: 1.0253102779388428\n",
      "Step: 3231, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 0.9898769855499268\n",
      "Step: 3232, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 0.9403283596038818\n",
      "Step: 3233, Loss: 0.9159694910049438, Accuracy: 1.0, Computation time: 1.4461562633514404\n",
      "Step: 3234, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.8886380195617676\n",
      "Step: 3235, Loss: 0.9372127056121826, Accuracy: 0.96875, Computation time: 1.82405686378479\n",
      "Step: 3236, Loss: 0.9371733665466309, Accuracy: 0.96875, Computation time: 0.8618197441101074\n",
      "Step: 3237, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 0.9797232151031494\n",
      "Step: 3238, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 0.8295383453369141\n",
      "Step: 3239, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 0.8417863845825195\n",
      "Step: 3240, Loss: 0.937545657157898, Accuracy: 0.96875, Computation time: 0.8498387336730957\n",
      "Step: 3241, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 0.8954949378967285\n",
      "Step: 3242, Loss: 0.9159838557243347, Accuracy: 1.0, Computation time: 1.037630319595337\n",
      "Step: 3243, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 0.9404025077819824\n",
      "Step: 3244, Loss: 0.9374496340751648, Accuracy: 0.96875, Computation time: 1.1215214729309082\n",
      "Step: 3245, Loss: 0.921217143535614, Accuracy: 1.0, Computation time: 1.0785784721374512\n",
      "Step: 3246, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 0.9571437835693359\n",
      "Step: 3247, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.0365581512451172\n",
      "Step: 3248, Loss: 0.9288650155067444, Accuracy: 0.96875, Computation time: 0.9272191524505615\n",
      "Step: 3249, Loss: 0.9266698360443115, Accuracy: 0.96875, Computation time: 0.9018301963806152\n",
      "Step: 3250, Loss: 0.9163275361061096, Accuracy: 1.0, Computation time: 1.1149835586547852\n",
      "Step: 3251, Loss: 0.916009247303009, Accuracy: 1.0, Computation time: 1.0614681243896484\n",
      "Step: 3252, Loss: 0.916128396987915, Accuracy: 1.0, Computation time: 0.9357514381408691\n",
      "Step: 3253, Loss: 0.916847288608551, Accuracy: 1.0, Computation time: 0.8084506988525391\n",
      "Step: 3254, Loss: 0.9160435795783997, Accuracy: 1.0, Computation time: 0.8175663948059082\n",
      "Step: 3255, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 1.0283992290496826\n",
      "Step: 3256, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 0.8747732639312744\n",
      "Step: 3257, Loss: 0.9375575184822083, Accuracy: 0.96875, Computation time: 0.9392764568328857\n",
      "Step: 3258, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 0.8159160614013672\n",
      "Step: 3259, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 0.9849655628204346\n",
      "Step: 3260, Loss: 0.9159886240959167, Accuracy: 1.0, Computation time: 0.8900196552276611\n",
      "Step: 3261, Loss: 0.9159559607505798, Accuracy: 1.0, Computation time: 0.8962264060974121\n",
      "Step: 3262, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 1.0214319229125977\n",
      "Step: 3263, Loss: 0.9159820079803467, Accuracy: 1.0, Computation time: 1.2249603271484375\n",
      "Step: 3264, Loss: 0.9159116744995117, Accuracy: 1.0, Computation time: 0.9625847339630127\n",
      "Step: 3265, Loss: 0.9163010120391846, Accuracy: 1.0, Computation time: 1.269094467163086\n",
      "Step: 3266, Loss: 0.9159616231918335, Accuracy: 1.0, Computation time: 1.0647265911102295\n",
      "Step: 3267, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 1.0081744194030762\n",
      "Step: 3268, Loss: 0.9159579873085022, Accuracy: 1.0, Computation time: 0.9306676387786865\n",
      "Step: 3269, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 1.259279727935791\n",
      "Step: 3270, Loss: 0.9188550114631653, Accuracy: 1.0, Computation time: 0.9646565914154053\n",
      "Step: 3271, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 1.001175880432129\n",
      "Step: 3272, Loss: 0.9160170555114746, Accuracy: 1.0, Computation time: 0.9318633079528809\n",
      "Step: 3273, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 0.8686068058013916\n",
      "Step: 3274, Loss: 0.9235386252403259, Accuracy: 1.0, Computation time: 1.039478063583374\n",
      "Step: 3275, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 0.812431812286377\n",
      "Step: 3276, Loss: 0.9195668697357178, Accuracy: 1.0, Computation time: 0.872908353805542\n",
      "Step: 3277, Loss: 0.9159630537033081, Accuracy: 1.0, Computation time: 0.9728689193725586\n",
      "Step: 3278, Loss: 0.9159542322158813, Accuracy: 1.0, Computation time: 0.881458044052124\n",
      "Step: 3279, Loss: 0.9189131259918213, Accuracy: 1.0, Computation time: 0.91782546043396\n",
      "Step: 3280, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 1.3073313236236572\n",
      "Step: 3281, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 1.280820369720459\n",
      "Step: 3282, Loss: 0.9174301624298096, Accuracy: 1.0, Computation time: 0.9713907241821289\n",
      "Step: 3283, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.1192004680633545\n",
      "Step: 3284, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 0.9375908374786377\n",
      "Step: 3285, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 0.9024715423583984\n",
      "Step: 3286, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 0.9521827697753906\n",
      "Step: 3287, Loss: 0.9162713885307312, Accuracy: 1.0, Computation time: 0.9550535678863525\n",
      "Step: 3288, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.1430563926696777\n",
      "Step: 3289, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 0.9216034412384033\n",
      "Step: 3290, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 0.848834753036499\n",
      "Step: 3291, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 1.2790217399597168\n",
      "Step: 3292, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.0112018585205078\n",
      "Step: 3293, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 0.9396381378173828\n",
      "Step: 3294, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.0495967864990234\n",
      "Step: 3295, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 1.0773594379425049\n",
      "Step: 3296, Loss: 0.9811466932296753, Accuracy: 0.90625, Computation time: 1.0623939037322998\n",
      "Step: 3297, Loss: 0.9382448196411133, Accuracy: 0.96875, Computation time: 0.8746294975280762\n",
      "Step: 3298, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 0.8522303104400635\n",
      "Step: 3299, Loss: 0.9374317526817322, Accuracy: 0.96875, Computation time: 1.1067957878112793\n",
      "Step: 3300, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 1.0298752784729004\n",
      "Step: 3301, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 0.895327091217041\n",
      "Step: 3302, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.040562391281128\n",
      "Step: 3303, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 0.9464247226715088\n",
      "Step: 3304, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 0.9328453540802002\n",
      "Step: 3305, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.031275987625122\n",
      "Step: 3306, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 0.9209442138671875\n",
      "Step: 3307, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 0.7962274551391602\n",
      "Step: 3308, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 0.8759286403656006\n",
      "Step: 3309, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.070143461227417\n",
      "Step: 3310, Loss: 0.9377251863479614, Accuracy: 0.96875, Computation time: 0.8473870754241943\n",
      "Step: 3311, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.2808256149291992\n",
      "Step: 3312, Loss: 0.9159660935401917, Accuracy: 1.0, Computation time: 1.1028075218200684\n",
      "Step: 3313, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 0.9253435134887695\n",
      "Step: 3314, Loss: 0.9185295701026917, Accuracy: 1.0, Computation time: 1.1301498413085938\n",
      "Step: 3315, Loss: 0.9196542501449585, Accuracy: 1.0, Computation time: 1.0559892654418945\n",
      "Step: 3316, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.0992209911346436\n",
      "Step: 3317, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 0.9024248123168945\n",
      "Step: 3318, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 0.9838335514068604\n",
      "Step: 3319, Loss: 0.9159090518951416, Accuracy: 1.0, Computation time: 1.0221645832061768\n",
      "Step: 3320, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 0.8063597679138184\n",
      "Step: 3321, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 0.9534294605255127\n",
      "Step: 3322, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 0.858043909072876\n",
      "Step: 3323, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 0.8506417274475098\n",
      "Step: 3324, Loss: 0.9163200855255127, Accuracy: 1.0, Computation time: 0.9468798637390137\n",
      "Step: 3325, Loss: 0.9158380627632141, Accuracy: 1.0, Computation time: 1.0063817501068115\n",
      "Step: 3326, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 0.8659994602203369\n",
      "Step: 3327, Loss: 0.9253731966018677, Accuracy: 0.96875, Computation time: 1.1048533916473389\n",
      "Step: 3328, Loss: 0.9159178137779236, Accuracy: 1.0, Computation time: 0.9525237083435059\n",
      "Step: 3329, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.09987211227417\n",
      "Step: 3330, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 0.931190013885498\n",
      "Step: 3331, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 0.9343264102935791\n",
      "Step: 3332, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 0.9275836944580078\n",
      "Step: 3333, Loss: 0.9570385217666626, Accuracy: 0.9375, Computation time: 1.2119557857513428\n",
      "Step: 3334, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.235422134399414\n",
      "Step: 3335, Loss: 0.9384242296218872, Accuracy: 0.96875, Computation time: 1.0726475715637207\n",
      "########################\n",
      "Test loss: 1.0722614526748657, Test Accuracy_epoch24: 0.7722384929656982\n",
      "########################\n",
      "Step: 3336, Loss: 0.9160274267196655, Accuracy: 1.0, Computation time: 1.1285018920898438\n",
      "Step: 3337, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 0.9679217338562012\n",
      "Step: 3338, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 1.1955208778381348\n",
      "Step: 3339, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.06085205078125\n",
      "Step: 3340, Loss: 0.9265899062156677, Accuracy: 0.96875, Computation time: 0.9468905925750732\n",
      "Step: 3341, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 1.318770170211792\n",
      "Step: 3342, Loss: 0.918330729007721, Accuracy: 1.0, Computation time: 1.1437506675720215\n",
      "Step: 3343, Loss: 0.9160668849945068, Accuracy: 1.0, Computation time: 1.0513050556182861\n",
      "Step: 3344, Loss: 0.9160363674163818, Accuracy: 1.0, Computation time: 0.9840824604034424\n",
      "Step: 3345, Loss: 0.9160775542259216, Accuracy: 1.0, Computation time: 1.1250755786895752\n",
      "Step: 3346, Loss: 0.9161443710327148, Accuracy: 1.0, Computation time: 1.0674645900726318\n",
      "Step: 3347, Loss: 0.915928065776825, Accuracy: 1.0, Computation time: 0.8835089206695557\n",
      "Step: 3348, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 0.9367620944976807\n",
      "Step: 3349, Loss: 0.9375254511833191, Accuracy: 0.96875, Computation time: 1.095881462097168\n",
      "Step: 3350, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.239710807800293\n",
      "Step: 3351, Loss: 0.9159793853759766, Accuracy: 1.0, Computation time: 1.0042319297790527\n",
      "Step: 3352, Loss: 0.9333938360214233, Accuracy: 0.96875, Computation time: 1.3486082553863525\n",
      "Step: 3353, Loss: 0.9161686301231384, Accuracy: 1.0, Computation time: 0.8381500244140625\n",
      "Step: 3354, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 0.9314191341400146\n",
      "Step: 3355, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 0.9818191528320312\n",
      "Step: 3356, Loss: 0.916044294834137, Accuracy: 1.0, Computation time: 1.1467106342315674\n",
      "Step: 3357, Loss: 0.9320582747459412, Accuracy: 0.96875, Computation time: 1.208686351776123\n",
      "Step: 3358, Loss: 0.9170578718185425, Accuracy: 1.0, Computation time: 1.0682976245880127\n",
      "Step: 3359, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 0.921445369720459\n",
      "Step: 3360, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 0.912696123123169\n",
      "Step: 3361, Loss: 0.91591477394104, Accuracy: 1.0, Computation time: 0.8550217151641846\n",
      "Step: 3362, Loss: 0.9343967437744141, Accuracy: 0.96875, Computation time: 1.1581153869628906\n",
      "Step: 3363, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 0.8517403602600098\n",
      "Step: 3364, Loss: 0.9159385561943054, Accuracy: 1.0, Computation time: 0.9764487743377686\n",
      "Step: 3365, Loss: 0.9159787893295288, Accuracy: 1.0, Computation time: 0.8222196102142334\n",
      "Step: 3366, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.0208885669708252\n",
      "Step: 3367, Loss: 0.9160564541816711, Accuracy: 1.0, Computation time: 1.0820441246032715\n",
      "Step: 3368, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 0.9826109409332275\n",
      "Step: 3369, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 0.9922494888305664\n",
      "Step: 3370, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.1577413082122803\n",
      "Step: 3371, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.015183925628662\n",
      "Step: 3372, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 0.8742034435272217\n",
      "Step: 3373, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 0.8446338176727295\n",
      "Step: 3374, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 0.9403870105743408\n",
      "Step: 3375, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 0.9174196720123291\n",
      "Step: 3376, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.9936463832855225\n",
      "Step: 3377, Loss: 0.9353017210960388, Accuracy: 0.96875, Computation time: 0.9448928833007812\n",
      "Step: 3378, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.0298197269439697\n",
      "Step: 3379, Loss: 0.9512753486633301, Accuracy: 0.9375, Computation time: 1.4920237064361572\n",
      "Step: 3380, Loss: 0.9160120487213135, Accuracy: 1.0, Computation time: 1.080998182296753\n",
      "Step: 3381, Loss: 0.916172206401825, Accuracy: 1.0, Computation time: 1.0795059204101562\n",
      "Step: 3382, Loss: 0.9161210060119629, Accuracy: 1.0, Computation time: 0.8261501789093018\n",
      "Step: 3383, Loss: 0.9161722660064697, Accuracy: 1.0, Computation time: 0.871187686920166\n",
      "Step: 3384, Loss: 0.9159992933273315, Accuracy: 1.0, Computation time: 0.9568850994110107\n",
      "Step: 3385, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 0.8781459331512451\n",
      "Step: 3386, Loss: 0.9159122109413147, Accuracy: 1.0, Computation time: 1.098109245300293\n",
      "Step: 3387, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 0.7945077419281006\n",
      "Step: 3388, Loss: 0.9159612655639648, Accuracy: 1.0, Computation time: 0.9492125511169434\n",
      "Step: 3389, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 0.9075524806976318\n",
      "Step: 3390, Loss: 0.9159601926803589, Accuracy: 1.0, Computation time: 0.8723959922790527\n",
      "Step: 3391, Loss: 0.9165821075439453, Accuracy: 1.0, Computation time: 1.0412871837615967\n",
      "Step: 3392, Loss: 0.9403908252716064, Accuracy: 0.96875, Computation time: 1.5993940830230713\n",
      "Step: 3393, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 0.990936279296875\n",
      "Step: 3394, Loss: 0.9301286935806274, Accuracy: 0.96875, Computation time: 0.9801082611083984\n",
      "Step: 3395, Loss: 0.9159798622131348, Accuracy: 1.0, Computation time: 1.0980546474456787\n",
      "Step: 3396, Loss: 0.9164134860038757, Accuracy: 1.0, Computation time: 0.8934183120727539\n",
      "Step: 3397, Loss: 0.9160265922546387, Accuracy: 1.0, Computation time: 0.9212982654571533\n",
      "Step: 3398, Loss: 0.9214214086532593, Accuracy: 1.0, Computation time: 1.0818848609924316\n",
      "Step: 3399, Loss: 0.9333754777908325, Accuracy: 0.96875, Computation time: 1.2016072273254395\n",
      "Step: 3400, Loss: 0.9160577058792114, Accuracy: 1.0, Computation time: 1.0537176132202148\n",
      "Step: 3401, Loss: 0.9160564541816711, Accuracy: 1.0, Computation time: 0.8763313293457031\n",
      "Step: 3402, Loss: 0.9161039590835571, Accuracy: 1.0, Computation time: 1.2135963439941406\n",
      "Step: 3403, Loss: 0.9161176681518555, Accuracy: 1.0, Computation time: 1.1272764205932617\n",
      "Step: 3404, Loss: 0.9163284301757812, Accuracy: 1.0, Computation time: 1.7252323627471924\n",
      "Step: 3405, Loss: 0.954678475856781, Accuracy: 0.9375, Computation time: 1.391214370727539\n",
      "Step: 3406, Loss: 0.9161631464958191, Accuracy: 1.0, Computation time: 0.8109495639801025\n",
      "Step: 3407, Loss: 0.9317215085029602, Accuracy: 0.96875, Computation time: 1.0404515266418457\n",
      "Step: 3408, Loss: 0.9160171151161194, Accuracy: 1.0, Computation time: 1.1553676128387451\n",
      "Step: 3409, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 0.8844356536865234\n",
      "Step: 3410, Loss: 0.9187309145927429, Accuracy: 1.0, Computation time: 0.9319972991943359\n",
      "Step: 3411, Loss: 0.9162636995315552, Accuracy: 1.0, Computation time: 0.9539797306060791\n",
      "Step: 3412, Loss: 0.9164270758628845, Accuracy: 1.0, Computation time: 1.113461971282959\n",
      "Step: 3413, Loss: 0.916190505027771, Accuracy: 1.0, Computation time: 1.0520076751708984\n",
      "Step: 3414, Loss: 0.9162825345993042, Accuracy: 1.0, Computation time: 0.8949124813079834\n",
      "Step: 3415, Loss: 0.9161516427993774, Accuracy: 1.0, Computation time: 0.9191403388977051\n",
      "Step: 3416, Loss: 0.9311559200286865, Accuracy: 0.96875, Computation time: 1.1000266075134277\n",
      "Step: 3417, Loss: 0.9161008596420288, Accuracy: 1.0, Computation time: 1.168140172958374\n",
      "Step: 3418, Loss: 0.9160552024841309, Accuracy: 1.0, Computation time: 1.0159363746643066\n",
      "Step: 3419, Loss: 0.9168487787246704, Accuracy: 1.0, Computation time: 1.1967837810516357\n",
      "Step: 3420, Loss: 0.9161214828491211, Accuracy: 1.0, Computation time: 0.9995467662811279\n",
      "Step: 3421, Loss: 0.9160382151603699, Accuracy: 1.0, Computation time: 1.038818597793579\n",
      "Step: 3422, Loss: 0.9160011410713196, Accuracy: 1.0, Computation time: 0.846111536026001\n",
      "Step: 3423, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.0520401000976562\n",
      "Step: 3424, Loss: 0.937522292137146, Accuracy: 0.96875, Computation time: 1.0189692974090576\n",
      "Step: 3425, Loss: 0.9159231781959534, Accuracy: 1.0, Computation time: 1.0428624153137207\n",
      "Step: 3426, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 0.9793956279754639\n",
      "Step: 3427, Loss: 0.9375832080841064, Accuracy: 0.96875, Computation time: 0.8452537059783936\n",
      "Step: 3428, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 0.8775851726531982\n",
      "Step: 3429, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 0.8679351806640625\n",
      "Step: 3430, Loss: 0.9159248471260071, Accuracy: 1.0, Computation time: 0.9937419891357422\n",
      "Step: 3431, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 0.9167771339416504\n",
      "Step: 3432, Loss: 0.9162875413894653, Accuracy: 1.0, Computation time: 0.9994344711303711\n",
      "Step: 3433, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 0.8074131011962891\n",
      "Step: 3434, Loss: 0.916434645652771, Accuracy: 1.0, Computation time: 0.8778805732727051\n",
      "Step: 3435, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 0.7778069972991943\n",
      "Step: 3436, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 0.8653616905212402\n",
      "Step: 3437, Loss: 0.9268538951873779, Accuracy: 0.96875, Computation time: 0.898989200592041\n",
      "Step: 3438, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 0.8687138557434082\n",
      "Step: 3439, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 0.791304349899292\n",
      "Step: 3440, Loss: 0.9159809947013855, Accuracy: 1.0, Computation time: 1.0047383308410645\n",
      "Step: 3441, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 1.2726645469665527\n",
      "Step: 3442, Loss: 0.9375976920127869, Accuracy: 0.96875, Computation time: 1.0200278759002686\n",
      "Step: 3443, Loss: 0.9234283566474915, Accuracy: 1.0, Computation time: 0.7676830291748047\n",
      "Step: 3444, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 0.8680858612060547\n",
      "Step: 3445, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 0.9055349826812744\n",
      "Step: 3446, Loss: 0.9377750158309937, Accuracy: 0.96875, Computation time: 0.8767096996307373\n",
      "Step: 3447, Loss: 0.9160359501838684, Accuracy: 1.0, Computation time: 0.8508310317993164\n",
      "Step: 3448, Loss: 0.9206565618515015, Accuracy: 1.0, Computation time: 1.0990700721740723\n",
      "Step: 3449, Loss: 0.915920078754425, Accuracy: 1.0, Computation time: 1.0074336528778076\n",
      "Step: 3450, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 0.9060616493225098\n",
      "Step: 3451, Loss: 0.9397823810577393, Accuracy: 0.96875, Computation time: 0.7891957759857178\n",
      "Step: 3452, Loss: 0.9160287380218506, Accuracy: 1.0, Computation time: 0.8668792247772217\n",
      "Step: 3453, Loss: 0.9160485863685608, Accuracy: 1.0, Computation time: 0.920177698135376\n",
      "Step: 3454, Loss: 0.9160420894622803, Accuracy: 1.0, Computation time: 1.1786458492279053\n",
      "Step: 3455, Loss: 0.9200652837753296, Accuracy: 1.0, Computation time: 0.8281664848327637\n",
      "Step: 3456, Loss: 0.9159504175186157, Accuracy: 1.0, Computation time: 0.7359058856964111\n",
      "Step: 3457, Loss: 0.9272147417068481, Accuracy: 0.96875, Computation time: 0.928581953048706\n",
      "Step: 3458, Loss: 0.9160343408584595, Accuracy: 1.0, Computation time: 0.8075153827667236\n",
      "Step: 3459, Loss: 0.9159687161445618, Accuracy: 1.0, Computation time: 0.876988410949707\n",
      "Step: 3460, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 0.8493444919586182\n",
      "Step: 3461, Loss: 0.9159407615661621, Accuracy: 1.0, Computation time: 0.8782312870025635\n",
      "Step: 3462, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 0.8464202880859375\n",
      "Step: 3463, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.0812084674835205\n",
      "Step: 3464, Loss: 0.9375019669532776, Accuracy: 0.96875, Computation time: 0.8286788463592529\n",
      "Step: 3465, Loss: 0.9378748536109924, Accuracy: 0.96875, Computation time: 0.9424374103546143\n",
      "Step: 3466, Loss: 0.9159228205680847, Accuracy: 1.0, Computation time: 0.8938498497009277\n",
      "Step: 3467, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 0.7790777683258057\n",
      "Step: 3468, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 1.0238430500030518\n",
      "Step: 3469, Loss: 0.9199824333190918, Accuracy: 1.0, Computation time: 0.8349642753601074\n",
      "Step: 3470, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.0590529441833496\n",
      "Step: 3471, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.2831754684448242\n",
      "Step: 3472, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.030989170074463\n",
      "Step: 3473, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 1.0374236106872559\n",
      "Step: 3474, Loss: 0.9159851670265198, Accuracy: 1.0, Computation time: 0.9962327480316162\n",
      "########################\n",
      "Test loss: 1.0731170177459717, Test Accuracy_epoch25: 0.7702834606170654\n",
      "########################\n",
      "Step: 3475, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 0.9496750831604004\n",
      "Step: 3476, Loss: 0.9159284830093384, Accuracy: 1.0, Computation time: 1.163921594619751\n",
      "Step: 3477, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 0.9786252975463867\n",
      "Step: 3478, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 0.9091095924377441\n",
      "Step: 3479, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 0.8229117393493652\n",
      "Step: 3480, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 0.8299050331115723\n",
      "Step: 3481, Loss: 0.9382739663124084, Accuracy: 0.96875, Computation time: 0.9601080417633057\n",
      "Step: 3482, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 0.8626720905303955\n",
      "Step: 3483, Loss: 0.9164252281188965, Accuracy: 1.0, Computation time: 1.0095627307891846\n",
      "Step: 3484, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 0.8104438781738281\n",
      "Step: 3485, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.053457260131836\n",
      "Step: 3486, Loss: 0.9180161952972412, Accuracy: 1.0, Computation time: 0.9829668998718262\n",
      "Step: 3487, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 0.8278076648712158\n",
      "Step: 3488, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 0.7599139213562012\n",
      "Step: 3489, Loss: 0.935895562171936, Accuracy: 0.96875, Computation time: 1.0098960399627686\n",
      "Step: 3490, Loss: 0.9329583644866943, Accuracy: 0.96875, Computation time: 1.1386737823486328\n",
      "Step: 3491, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 0.927248477935791\n",
      "Step: 3492, Loss: 0.9163723587989807, Accuracy: 1.0, Computation time: 0.8739831447601318\n",
      "Step: 3493, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 0.9806811809539795\n",
      "Step: 3494, Loss: 0.9159801006317139, Accuracy: 1.0, Computation time: 0.8809466361999512\n",
      "Step: 3495, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 0.8631935119628906\n",
      "Step: 3496, Loss: 0.9386485815048218, Accuracy: 0.96875, Computation time: 1.1040608882904053\n",
      "Step: 3497, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 0.8462817668914795\n",
      "Step: 3498, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.0684330463409424\n",
      "Step: 3499, Loss: 0.9161480665206909, Accuracy: 1.0, Computation time: 0.9158096313476562\n",
      "Step: 3500, Loss: 0.9165142178535461, Accuracy: 1.0, Computation time: 1.0030949115753174\n",
      "Step: 3501, Loss: 0.9159620404243469, Accuracy: 1.0, Computation time: 0.821664571762085\n",
      "Step: 3502, Loss: 0.9335577487945557, Accuracy: 0.96875, Computation time: 0.885307788848877\n",
      "Step: 3503, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 0.7734613418579102\n",
      "Step: 3504, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 0.8270134925842285\n",
      "Step: 3505, Loss: 0.9159988760948181, Accuracy: 1.0, Computation time: 0.8214399814605713\n",
      "Step: 3506, Loss: 0.9160153269767761, Accuracy: 1.0, Computation time: 1.0246317386627197\n",
      "Step: 3507, Loss: 0.9162915945053101, Accuracy: 1.0, Computation time: 0.932363748550415\n",
      "Step: 3508, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 0.907379150390625\n",
      "Step: 3509, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 0.8783235549926758\n",
      "Step: 3510, Loss: 0.9238444566726685, Accuracy: 1.0, Computation time: 1.0035574436187744\n",
      "Step: 3511, Loss: 0.9164935946464539, Accuracy: 1.0, Computation time: 0.9055924415588379\n",
      "Step: 3512, Loss: 0.934908390045166, Accuracy: 0.96875, Computation time: 0.8941628932952881\n",
      "Step: 3513, Loss: 0.9378988742828369, Accuracy: 0.96875, Computation time: 0.9389774799346924\n",
      "Step: 3514, Loss: 0.9163691401481628, Accuracy: 1.0, Computation time: 0.8084237575531006\n",
      "Step: 3515, Loss: 0.9160051941871643, Accuracy: 1.0, Computation time: 0.8284845352172852\n",
      "Step: 3516, Loss: 0.9169806838035583, Accuracy: 1.0, Computation time: 0.9164056777954102\n",
      "Step: 3517, Loss: 0.9188193082809448, Accuracy: 1.0, Computation time: 0.9328935146331787\n",
      "Step: 3518, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 0.8693592548370361\n",
      "Step: 3519, Loss: 0.9159612059593201, Accuracy: 1.0, Computation time: 0.7628424167633057\n",
      "Step: 3520, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 0.8681728839874268\n",
      "Step: 3521, Loss: 0.9160726070404053, Accuracy: 1.0, Computation time: 0.7963657379150391\n",
      "Step: 3522, Loss: 0.9165628552436829, Accuracy: 1.0, Computation time: 0.9641296863555908\n",
      "Step: 3523, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 0.8510839939117432\n",
      "Step: 3524, Loss: 0.9159581065177917, Accuracy: 1.0, Computation time: 0.7609648704528809\n",
      "Step: 3525, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.0250024795532227\n",
      "Step: 3526, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.0664708614349365\n",
      "Step: 3527, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.1732163429260254\n",
      "Step: 3528, Loss: 0.9161531329154968, Accuracy: 1.0, Computation time: 0.794938325881958\n",
      "Step: 3529, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 1.0268981456756592\n",
      "Step: 3530, Loss: 0.9168034791946411, Accuracy: 1.0, Computation time: 1.13948392868042\n",
      "Step: 3531, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 1.081542730331421\n",
      "Step: 3532, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.8400774002075195\n",
      "Step: 3533, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 0.8730168342590332\n",
      "Step: 3534, Loss: 0.9219347238540649, Accuracy: 1.0, Computation time: 0.9695634841918945\n",
      "Step: 3535, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 0.6908962726593018\n",
      "Step: 3536, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 0.7394626140594482\n",
      "Step: 3537, Loss: 0.9160880446434021, Accuracy: 1.0, Computation time: 0.8892467021942139\n",
      "Step: 3538, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 0.8142874240875244\n",
      "Step: 3539, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 0.9115738868713379\n",
      "Step: 3540, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 0.7839534282684326\n",
      "Step: 3541, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 0.9202001094818115\n",
      "Step: 3542, Loss: 0.9159917235374451, Accuracy: 1.0, Computation time: 0.8973381519317627\n",
      "Step: 3543, Loss: 0.9360370635986328, Accuracy: 0.96875, Computation time: 0.7788677215576172\n",
      "Step: 3544, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 0.7655789852142334\n",
      "Step: 3545, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 0.8253490924835205\n",
      "Step: 3546, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 0.81807541847229\n",
      "Step: 3547, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 0.7686240673065186\n",
      "Step: 3548, Loss: 0.9160465598106384, Accuracy: 1.0, Computation time: 0.7116522789001465\n",
      "Step: 3549, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 0.8333053588867188\n",
      "Step: 3550, Loss: 0.918087363243103, Accuracy: 1.0, Computation time: 0.8740968704223633\n",
      "Step: 3551, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 0.8172862529754639\n",
      "Step: 3552, Loss: 0.9159424901008606, Accuracy: 1.0, Computation time: 1.243218183517456\n",
      "Step: 3553, Loss: 0.9158453941345215, Accuracy: 1.0, Computation time: 0.756267786026001\n",
      "Step: 3554, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 0.9379873275756836\n",
      "Step: 3555, Loss: 0.9178001284599304, Accuracy: 1.0, Computation time: 0.9947679042816162\n",
      "Step: 3556, Loss: 0.9160575866699219, Accuracy: 1.0, Computation time: 0.8956365585327148\n",
      "Step: 3557, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 0.8004381656646729\n",
      "Step: 3558, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 0.7992303371429443\n",
      "Step: 3559, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.049124002456665\n",
      "Step: 3560, Loss: 0.9272273182868958, Accuracy: 0.96875, Computation time: 0.9313950538635254\n",
      "Step: 3561, Loss: 0.9159665703773499, Accuracy: 1.0, Computation time: 0.964874267578125\n",
      "Step: 3562, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 0.8481628894805908\n",
      "Step: 3563, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 0.8305878639221191\n",
      "Step: 3564, Loss: 0.9159070253372192, Accuracy: 1.0, Computation time: 0.9860978126525879\n",
      "Step: 3565, Loss: 0.9161887168884277, Accuracy: 1.0, Computation time: 1.0360326766967773\n",
      "Step: 3566, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 0.9810431003570557\n",
      "Step: 3567, Loss: 0.9374385476112366, Accuracy: 0.96875, Computation time: 1.9733712673187256\n",
      "Step: 3568, Loss: 0.9376081824302673, Accuracy: 0.96875, Computation time: 0.9714491367340088\n",
      "Step: 3569, Loss: 0.916540801525116, Accuracy: 1.0, Computation time: 0.9320905208587646\n",
      "Step: 3570, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.074125051498413\n",
      "Step: 3571, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 0.9254944324493408\n",
      "Step: 3572, Loss: 0.9159018993377686, Accuracy: 1.0, Computation time: 1.072648525238037\n",
      "Step: 3573, Loss: 0.9374117255210876, Accuracy: 0.96875, Computation time: 0.9368898868560791\n",
      "Step: 3574, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 0.9398744106292725\n",
      "Step: 3575, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 0.9989607334136963\n",
      "Step: 3576, Loss: 0.9159329533576965, Accuracy: 1.0, Computation time: 1.007235050201416\n",
      "Step: 3577, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.517310619354248\n",
      "Step: 3578, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 0.9644565582275391\n",
      "Step: 3579, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.111664056777954\n",
      "Step: 3580, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 1.5221672058105469\n",
      "Step: 3581, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.0442583560943604\n",
      "Step: 3582, Loss: 0.937636137008667, Accuracy: 0.96875, Computation time: 1.235912799835205\n",
      "Step: 3583, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 0.9667510986328125\n",
      "Step: 3584, Loss: 0.9375694990158081, Accuracy: 0.96875, Computation time: 1.2606043815612793\n",
      "Step: 3585, Loss: 0.9374964237213135, Accuracy: 0.96875, Computation time: 1.076629877090454\n",
      "Step: 3586, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 0.9767513275146484\n",
      "Step: 3587, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 1.0753698348999023\n",
      "Step: 3588, Loss: 0.9374823570251465, Accuracy: 0.96875, Computation time: 1.0637969970703125\n",
      "Step: 3589, Loss: 0.9162580966949463, Accuracy: 1.0, Computation time: 0.9322202205657959\n",
      "Step: 3590, Loss: 0.9167850017547607, Accuracy: 1.0, Computation time: 0.991774320602417\n",
      "Step: 3591, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.0219597816467285\n",
      "Step: 3592, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.1271111965179443\n",
      "Step: 3593, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 1.1131844520568848\n",
      "Step: 3594, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.2104699611663818\n",
      "Step: 3595, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.2158057689666748\n",
      "Step: 3596, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.0769031047821045\n",
      "Step: 3597, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.0293731689453125\n",
      "Step: 3598, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.1395461559295654\n",
      "Step: 3599, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.0023488998413086\n",
      "Step: 3600, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.7245683670043945\n",
      "Step: 3601, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.478926420211792\n",
      "Step: 3602, Loss: 0.9333865642547607, Accuracy: 0.96875, Computation time: 1.3198113441467285\n",
      "Step: 3603, Loss: 0.9177384972572327, Accuracy: 1.0, Computation time: 1.0118763446807861\n",
      "Step: 3604, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.0233185291290283\n",
      "Step: 3605, Loss: 0.9376711845397949, Accuracy: 0.96875, Computation time: 1.1246261596679688\n",
      "Step: 3606, Loss: 0.9166369438171387, Accuracy: 1.0, Computation time: 1.1073403358459473\n",
      "Step: 3607, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 0.9885330200195312\n",
      "Step: 3608, Loss: 0.9159470796585083, Accuracy: 1.0, Computation time: 0.9537079334259033\n",
      "Step: 3609, Loss: 0.9196303486824036, Accuracy: 1.0, Computation time: 0.9260637760162354\n",
      "Step: 3610, Loss: 0.9195562601089478, Accuracy: 1.0, Computation time: 0.9758930206298828\n",
      "Step: 3611, Loss: 0.9159355759620667, Accuracy: 1.0, Computation time: 0.9051389694213867\n",
      "Step: 3612, Loss: 0.9159832000732422, Accuracy: 1.0, Computation time: 1.1164186000823975\n",
      "Step: 3613, Loss: 0.9160534739494324, Accuracy: 1.0, Computation time: 0.9284884929656982\n",
      "########################\n",
      "Test loss: 1.0700899362564087, Test Accuracy_epoch26: 0.7781035900115967\n",
      "########################\n",
      "Step: 3614, Loss: 0.916276216506958, Accuracy: 1.0, Computation time: 0.8573336601257324\n",
      "Step: 3615, Loss: 0.9160169959068298, Accuracy: 1.0, Computation time: 1.1319563388824463\n",
      "Step: 3616, Loss: 0.9159172177314758, Accuracy: 1.0, Computation time: 0.84194016456604\n",
      "Step: 3617, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 0.7320466041564941\n",
      "Step: 3618, Loss: 0.9375503659248352, Accuracy: 0.96875, Computation time: 0.9472305774688721\n",
      "Step: 3619, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 1.049797773361206\n",
      "Step: 3620, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 0.7795875072479248\n",
      "Step: 3621, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 0.7801163196563721\n",
      "Step: 3622, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 0.9077203273773193\n",
      "Step: 3623, Loss: 0.9159297347068787, Accuracy: 1.0, Computation time: 0.7033543586730957\n",
      "Step: 3624, Loss: 0.9159340858459473, Accuracy: 1.0, Computation time: 0.8533470630645752\n",
      "Step: 3625, Loss: 0.9375286102294922, Accuracy: 0.96875, Computation time: 0.8182618618011475\n",
      "Step: 3626, Loss: 0.9339690208435059, Accuracy: 0.96875, Computation time: 0.9959492683410645\n",
      "Step: 3627, Loss: 0.937615692615509, Accuracy: 0.96875, Computation time: 0.794896125793457\n",
      "Step: 3628, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 0.802375078201294\n",
      "Step: 3629, Loss: 0.9160836338996887, Accuracy: 1.0, Computation time: 0.7472505569458008\n",
      "Step: 3630, Loss: 0.9161739349365234, Accuracy: 1.0, Computation time: 1.3465466499328613\n",
      "Step: 3631, Loss: 0.9160169959068298, Accuracy: 1.0, Computation time: 0.8210601806640625\n",
      "Step: 3632, Loss: 0.9159655570983887, Accuracy: 1.0, Computation time: 0.7548620700836182\n",
      "Step: 3633, Loss: 0.9160051345825195, Accuracy: 1.0, Computation time: 0.8478450775146484\n",
      "Step: 3634, Loss: 0.9374505877494812, Accuracy: 0.96875, Computation time: 0.8803224563598633\n",
      "Step: 3635, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 0.7639153003692627\n",
      "Step: 3636, Loss: 0.9170896410942078, Accuracy: 1.0, Computation time: 0.9052963256835938\n",
      "Step: 3637, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 0.9674901962280273\n",
      "Step: 3638, Loss: 0.9158948063850403, Accuracy: 1.0, Computation time: 0.8044586181640625\n",
      "Step: 3639, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 0.813037633895874\n",
      "Step: 3640, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 0.7435801029205322\n",
      "Step: 3641, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 0.789125919342041\n",
      "Step: 3642, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 0.8155057430267334\n",
      "Step: 3643, Loss: 0.9379003643989563, Accuracy: 0.96875, Computation time: 1.4870331287384033\n",
      "Step: 3644, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.8662383556365967\n",
      "Step: 3645, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 0.8087127208709717\n",
      "Step: 3646, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.8784439563751221\n",
      "Step: 3647, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 0.846350908279419\n",
      "Step: 3648, Loss: 0.9160293340682983, Accuracy: 1.0, Computation time: 0.8434009552001953\n",
      "Step: 3649, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 0.714501142501831\n",
      "Step: 3650, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 0.8099973201751709\n",
      "Step: 3651, Loss: 0.9171683192253113, Accuracy: 1.0, Computation time: 0.8738021850585938\n",
      "Step: 3652, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 0.7525687217712402\n",
      "Step: 3653, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 0.8482203483581543\n",
      "Step: 3654, Loss: 0.958631157875061, Accuracy: 0.9375, Computation time: 0.8152389526367188\n",
      "Step: 3655, Loss: 0.9375893473625183, Accuracy: 0.96875, Computation time: 0.8799786567687988\n",
      "Step: 3656, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.460280418395996\n",
      "Step: 3657, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 0.7766509056091309\n",
      "Step: 3658, Loss: 0.9319258332252502, Accuracy: 0.96875, Computation time: 1.0738255977630615\n",
      "Step: 3659, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 0.7304110527038574\n",
      "Step: 3660, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 0.9872806072235107\n",
      "Step: 3661, Loss: 0.9159876108169556, Accuracy: 1.0, Computation time: 0.8027327060699463\n",
      "Step: 3662, Loss: 0.9161339998245239, Accuracy: 1.0, Computation time: 1.0697546005249023\n",
      "Step: 3663, Loss: 0.9159339666366577, Accuracy: 1.0, Computation time: 0.8189082145690918\n",
      "Step: 3664, Loss: 0.9160244464874268, Accuracy: 1.0, Computation time: 1.1160366535186768\n",
      "Step: 3665, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 0.9494333267211914\n",
      "Step: 3666, Loss: 0.9164241552352905, Accuracy: 1.0, Computation time: 0.907977819442749\n",
      "Step: 3667, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 0.7369987964630127\n",
      "Step: 3668, Loss: 0.9158357977867126, Accuracy: 1.0, Computation time: 0.6960482597351074\n",
      "Step: 3669, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 0.8849117755889893\n",
      "Step: 3670, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 0.7440371513366699\n",
      "Step: 3671, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 0.7640402317047119\n",
      "Step: 3672, Loss: 0.9169788956642151, Accuracy: 1.0, Computation time: 0.8159785270690918\n",
      "Step: 3673, Loss: 0.9163210391998291, Accuracy: 1.0, Computation time: 0.8808784484863281\n",
      "Step: 3674, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 0.8321104049682617\n",
      "Step: 3675, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 0.87896728515625\n",
      "Step: 3676, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.9304056167602539\n",
      "Step: 3677, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 0.8789806365966797\n",
      "Step: 3678, Loss: 0.9278725981712341, Accuracy: 0.96875, Computation time: 0.9242422580718994\n",
      "Step: 3679, Loss: 0.9162477850914001, Accuracy: 1.0, Computation time: 1.686241865158081\n",
      "Step: 3680, Loss: 0.9159354567527771, Accuracy: 1.0, Computation time: 0.8546314239501953\n",
      "Step: 3681, Loss: 0.9159950017929077, Accuracy: 1.0, Computation time: 0.934826135635376\n",
      "Step: 3682, Loss: 0.9160345196723938, Accuracy: 1.0, Computation time: 0.8470299243927002\n",
      "Step: 3683, Loss: 0.9188030362129211, Accuracy: 1.0, Computation time: 1.0221779346466064\n",
      "Step: 3684, Loss: 0.9159930944442749, Accuracy: 1.0, Computation time: 0.9667339324951172\n",
      "Step: 3685, Loss: 0.9160225987434387, Accuracy: 1.0, Computation time: 0.834465742111206\n",
      "Step: 3686, Loss: 0.9159513711929321, Accuracy: 1.0, Computation time: 1.0751795768737793\n",
      "Step: 3687, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 1.1482462882995605\n",
      "Step: 3688, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 0.9737627506256104\n",
      "Step: 3689, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.0386054515838623\n",
      "Step: 3690, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 1.1297132968902588\n",
      "Step: 3691, Loss: 0.9166233539581299, Accuracy: 1.0, Computation time: 1.2903532981872559\n",
      "Step: 3692, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.0240278244018555\n",
      "Step: 3693, Loss: 0.9160143733024597, Accuracy: 1.0, Computation time: 0.9628651142120361\n",
      "Step: 3694, Loss: 0.9160255789756775, Accuracy: 1.0, Computation time: 1.2241625785827637\n",
      "Step: 3695, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 0.9780271053314209\n",
      "Step: 3696, Loss: 0.9160233736038208, Accuracy: 1.0, Computation time: 1.034461498260498\n",
      "Step: 3697, Loss: 0.9159501194953918, Accuracy: 1.0, Computation time: 1.1909840106964111\n",
      "Step: 3698, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.031506061553955\n",
      "Step: 3699, Loss: 0.9159892797470093, Accuracy: 1.0, Computation time: 0.8727729320526123\n",
      "Step: 3700, Loss: 0.9178972840309143, Accuracy: 1.0, Computation time: 0.9138739109039307\n",
      "Step: 3701, Loss: 0.93649822473526, Accuracy: 0.96875, Computation time: 1.1442570686340332\n",
      "Step: 3702, Loss: 0.9375808835029602, Accuracy: 0.96875, Computation time: 0.963604211807251\n",
      "Step: 3703, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 0.965667724609375\n",
      "Step: 3704, Loss: 0.9374764561653137, Accuracy: 0.96875, Computation time: 0.8121435642242432\n",
      "Step: 3705, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 0.9500045776367188\n",
      "Step: 3706, Loss: 0.9159774780273438, Accuracy: 1.0, Computation time: 0.9670686721801758\n",
      "Step: 3707, Loss: 0.915890634059906, Accuracy: 1.0, Computation time: 0.9180295467376709\n",
      "Step: 3708, Loss: 0.9377864599227905, Accuracy: 0.96875, Computation time: 1.040107250213623\n",
      "Step: 3709, Loss: 0.9160395860671997, Accuracy: 1.0, Computation time: 1.074761152267456\n",
      "Step: 3710, Loss: 0.916081964969635, Accuracy: 1.0, Computation time: 1.2503857612609863\n",
      "Step: 3711, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.0317132472991943\n",
      "Step: 3712, Loss: 0.9158430695533752, Accuracy: 1.0, Computation time: 1.2023169994354248\n",
      "Step: 3713, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 0.7639989852905273\n",
      "Step: 3714, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 0.8375673294067383\n",
      "Step: 3715, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.1164729595184326\n",
      "Step: 3716, Loss: 0.9158833026885986, Accuracy: 1.0, Computation time: 1.1459076404571533\n",
      "Step: 3717, Loss: 0.9375767111778259, Accuracy: 0.96875, Computation time: 1.2205333709716797\n",
      "Step: 3718, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.3118276596069336\n",
      "Step: 3719, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 1.0333473682403564\n",
      "Step: 3720, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.8321917057037354\n",
      "Step: 3721, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.2357239723205566\n",
      "Step: 3722, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 0.9474489688873291\n",
      "Step: 3723, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.2038931846618652\n",
      "Step: 3724, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 0.8744065761566162\n",
      "Step: 3725, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.071707010269165\n",
      "Step: 3726, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 1.0627732276916504\n",
      "Step: 3727, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 0.9915554523468018\n",
      "Step: 3728, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.0150935649871826\n",
      "Step: 3729, Loss: 0.9159941077232361, Accuracy: 1.0, Computation time: 1.2011973857879639\n",
      "Step: 3730, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 0.869408369064331\n",
      "Step: 3731, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 0.9229249954223633\n",
      "Step: 3732, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.1541063785552979\n",
      "Step: 3733, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 0.9381284713745117\n",
      "Step: 3734, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 0.9102933406829834\n",
      "Step: 3735, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.0624730587005615\n",
      "Step: 3736, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.060088872909546\n",
      "Step: 3737, Loss: 0.9158390760421753, Accuracy: 1.0, Computation time: 0.8417654037475586\n",
      "Step: 3738, Loss: 0.9335246682167053, Accuracy: 0.96875, Computation time: 0.7925002574920654\n",
      "Step: 3739, Loss: 0.9374392032623291, Accuracy: 0.96875, Computation time: 1.0328736305236816\n",
      "Step: 3740, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 1.2345199584960938\n",
      "Step: 3741, Loss: 0.9330710172653198, Accuracy: 0.96875, Computation time: 1.0005674362182617\n",
      "Step: 3742, Loss: 0.9376518726348877, Accuracy: 0.96875, Computation time: 1.0301294326782227\n",
      "Step: 3743, Loss: 0.915968120098114, Accuracy: 1.0, Computation time: 0.9517040252685547\n",
      "Step: 3744, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.2451720237731934\n",
      "Step: 3745, Loss: 0.9375076293945312, Accuracy: 0.96875, Computation time: 1.0062689781188965\n",
      "Step: 3746, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 0.9031269550323486\n",
      "Step: 3747, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 1.0496768951416016\n",
      "Step: 3748, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 0.9392964839935303\n",
      "Step: 3749, Loss: 0.9159270524978638, Accuracy: 1.0, Computation time: 1.2537741661071777\n",
      "Step: 3750, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 0.9738500118255615\n",
      "Step: 3751, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 0.9282996654510498\n",
      "Step: 3752, Loss: 0.9206681251525879, Accuracy: 1.0, Computation time: 0.9856438636779785\n",
      "########################\n",
      "Test loss: 1.0721220970153809, Test Accuracy_epoch27: 0.774193525314331\n",
      "########################\n",
      "Step: 3753, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 0.8720259666442871\n",
      "Step: 3754, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 0.9441680908203125\n",
      "Step: 3755, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 0.9500553607940674\n",
      "Step: 3756, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.122762680053711\n",
      "Step: 3757, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.0148799419403076\n",
      "Step: 3758, Loss: 0.9372251033782959, Accuracy: 0.96875, Computation time: 0.9272110462188721\n",
      "Step: 3759, Loss: 0.9375179409980774, Accuracy: 0.96875, Computation time: 1.112586498260498\n",
      "Step: 3760, Loss: 0.9190000295639038, Accuracy: 1.0, Computation time: 1.155822515487671\n",
      "Step: 3761, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 0.915147066116333\n",
      "Step: 3762, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 0.858795166015625\n",
      "Step: 3763, Loss: 0.925397515296936, Accuracy: 0.96875, Computation time: 1.0247085094451904\n",
      "Step: 3764, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 0.9008784294128418\n",
      "Step: 3765, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 1.0578827857971191\n",
      "Step: 3766, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 0.8428878784179688\n",
      "Step: 3767, Loss: 0.9159817695617676, Accuracy: 1.0, Computation time: 1.0784788131713867\n",
      "Step: 3768, Loss: 0.9159524440765381, Accuracy: 1.0, Computation time: 0.8921215534210205\n",
      "Step: 3769, Loss: 0.9159290194511414, Accuracy: 1.0, Computation time: 1.1395652294158936\n",
      "Step: 3770, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.9999980926513672\n",
      "Step: 3771, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 0.9337060451507568\n",
      "Step: 3772, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 0.8879227638244629\n",
      "Step: 3773, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.2055027484893799\n",
      "Step: 3774, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 0.8313350677490234\n",
      "Step: 3775, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.007293939590454\n",
      "Step: 3776, Loss: 0.9308156371116638, Accuracy: 0.96875, Computation time: 1.116403579711914\n",
      "Step: 3777, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 0.9272117614746094\n",
      "Step: 3778, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 2.2464380264282227\n",
      "Step: 3779, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.9984889030456543\n",
      "Step: 3780, Loss: 0.9375907778739929, Accuracy: 0.96875, Computation time: 1.0023667812347412\n",
      "Step: 3781, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 0.9599032402038574\n",
      "Step: 3782, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 0.8945331573486328\n",
      "Step: 3783, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 0.8406522274017334\n",
      "Step: 3784, Loss: 0.9160030484199524, Accuracy: 1.0, Computation time: 1.0137913227081299\n",
      "Step: 3785, Loss: 0.9211758375167847, Accuracy: 1.0, Computation time: 0.9821102619171143\n",
      "Step: 3786, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 0.9820098876953125\n",
      "Step: 3787, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 0.9725587368011475\n",
      "Step: 3788, Loss: 0.9160664677619934, Accuracy: 1.0, Computation time: 1.0796482563018799\n",
      "Step: 3789, Loss: 0.9159892201423645, Accuracy: 1.0, Computation time: 0.9463272094726562\n",
      "Step: 3790, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.4278099536895752\n",
      "Step: 3791, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.427621603012085\n",
      "Step: 3792, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 0.8352541923522949\n",
      "Step: 3793, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 0.9938101768493652\n",
      "Step: 3794, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 1.10321044921875\n",
      "Step: 3795, Loss: 0.9374648928642273, Accuracy: 0.96875, Computation time: 1.2102043628692627\n",
      "Step: 3796, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.3789010047912598\n",
      "Step: 3797, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.0490925312042236\n",
      "Step: 3798, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 0.8884761333465576\n",
      "Step: 3799, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 1.4653940200805664\n",
      "Step: 3800, Loss: 0.9158758521080017, Accuracy: 1.0, Computation time: 0.8301904201507568\n",
      "Step: 3801, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 1.0268974304199219\n",
      "Step: 3802, Loss: 0.915922999382019, Accuracy: 1.0, Computation time: 1.1896989345550537\n",
      "Step: 3803, Loss: 0.9159594774246216, Accuracy: 1.0, Computation time: 0.9626576900482178\n",
      "Step: 3804, Loss: 0.9379759430885315, Accuracy: 0.96875, Computation time: 1.6742608547210693\n",
      "Step: 3805, Loss: 0.9391078352928162, Accuracy: 0.96875, Computation time: 0.9614341259002686\n",
      "Step: 3806, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.2136552333831787\n",
      "Step: 3807, Loss: 0.9171301126480103, Accuracy: 1.0, Computation time: 0.8626420497894287\n",
      "Step: 3808, Loss: 0.9167075753211975, Accuracy: 1.0, Computation time: 0.9301393032073975\n",
      "Step: 3809, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 0.926579475402832\n",
      "Step: 3810, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 0.9313664436340332\n",
      "Step: 3811, Loss: 0.91615229845047, Accuracy: 1.0, Computation time: 1.0697894096374512\n",
      "Step: 3812, Loss: 0.9376890659332275, Accuracy: 0.96875, Computation time: 1.0308148860931396\n",
      "Step: 3813, Loss: 0.9372248649597168, Accuracy: 0.96875, Computation time: 1.371168851852417\n",
      "Step: 3814, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 0.9423873424530029\n",
      "Step: 3815, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 1.3197226524353027\n",
      "Step: 3816, Loss: 0.9161421656608582, Accuracy: 1.0, Computation time: 0.8669579029083252\n",
      "Step: 3817, Loss: 0.9167172312736511, Accuracy: 1.0, Computation time: 1.243818998336792\n",
      "Step: 3818, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.4357819557189941\n",
      "Step: 3819, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 1.2191288471221924\n",
      "Step: 3820, Loss: 0.9159122109413147, Accuracy: 1.0, Computation time: 0.9889883995056152\n",
      "Step: 3821, Loss: 0.9158821105957031, Accuracy: 1.0, Computation time: 1.0303382873535156\n",
      "Step: 3822, Loss: 0.9161701202392578, Accuracy: 1.0, Computation time: 0.9445860385894775\n",
      "Step: 3823, Loss: 0.9205859899520874, Accuracy: 1.0, Computation time: 0.9443187713623047\n",
      "Step: 3824, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 0.9448249340057373\n",
      "Step: 3825, Loss: 0.9190523028373718, Accuracy: 1.0, Computation time: 0.8959987163543701\n",
      "Step: 3826, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 0.8535959720611572\n",
      "Step: 3827, Loss: 0.9376360177993774, Accuracy: 0.96875, Computation time: 1.0600636005401611\n",
      "Step: 3828, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.2138280868530273\n",
      "Step: 3829, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.1084115505218506\n",
      "Step: 3830, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.1875319480895996\n",
      "Step: 3831, Loss: 0.917878270149231, Accuracy: 1.0, Computation time: 2.3869192600250244\n",
      "Step: 3832, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.1138384342193604\n",
      "Step: 3833, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.178736686706543\n",
      "Step: 3834, Loss: 0.9162949323654175, Accuracy: 1.0, Computation time: 1.1685841083526611\n",
      "Step: 3835, Loss: 0.9315169453620911, Accuracy: 0.96875, Computation time: 1.145763635635376\n",
      "Step: 3836, Loss: 0.9237300157546997, Accuracy: 1.0, Computation time: 0.9011485576629639\n",
      "Step: 3837, Loss: 0.9160484075546265, Accuracy: 1.0, Computation time: 0.836350679397583\n",
      "Step: 3838, Loss: 0.9159600734710693, Accuracy: 1.0, Computation time: 1.002657413482666\n",
      "Step: 3839, Loss: 0.9160317778587341, Accuracy: 1.0, Computation time: 0.7922217845916748\n",
      "Step: 3840, Loss: 0.9377607107162476, Accuracy: 0.96875, Computation time: 0.8079783916473389\n",
      "Step: 3841, Loss: 0.9159688353538513, Accuracy: 1.0, Computation time: 0.9489920139312744\n",
      "Step: 3842, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 0.8079023361206055\n",
      "Step: 3843, Loss: 0.9268637299537659, Accuracy: 0.96875, Computation time: 0.8117108345031738\n",
      "Step: 3844, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 0.8120687007904053\n",
      "Step: 3845, Loss: 0.9176645278930664, Accuracy: 1.0, Computation time: 1.272233247756958\n",
      "Step: 3846, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 0.7850890159606934\n",
      "Step: 3847, Loss: 0.9162362217903137, Accuracy: 1.0, Computation time: 0.8847219944000244\n",
      "Step: 3848, Loss: 0.9160110950469971, Accuracy: 1.0, Computation time: 0.8695375919342041\n",
      "Step: 3849, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 0.8202745914459229\n",
      "Step: 3850, Loss: 0.9165494441986084, Accuracy: 1.0, Computation time: 0.871474027633667\n",
      "Step: 3851, Loss: 0.9376393556594849, Accuracy: 0.96875, Computation time: 0.9102764129638672\n",
      "Step: 3852, Loss: 0.9162387251853943, Accuracy: 1.0, Computation time: 1.047560214996338\n",
      "Step: 3853, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 0.758852481842041\n",
      "Step: 3854, Loss: 0.9158954620361328, Accuracy: 1.0, Computation time: 1.060964822769165\n",
      "Step: 3855, Loss: 0.9167649149894714, Accuracy: 1.0, Computation time: 1.5400590896606445\n",
      "Step: 3856, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 1.118166208267212\n",
      "Step: 3857, Loss: 0.915935218334198, Accuracy: 1.0, Computation time: 0.9923760890960693\n",
      "Step: 3858, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.332982063293457\n",
      "Step: 3859, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 1.1411638259887695\n",
      "Step: 3860, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 0.8589503765106201\n",
      "Step: 3861, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.0895013809204102\n",
      "Step: 3862, Loss: 0.9158521294593811, Accuracy: 1.0, Computation time: 1.331890344619751\n",
      "Step: 3863, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 0.8685100078582764\n",
      "Step: 3864, Loss: 0.9593570828437805, Accuracy: 0.9375, Computation time: 0.8731050491333008\n",
      "Step: 3865, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 0.9156560897827148\n",
      "Step: 3866, Loss: 0.9375225901603699, Accuracy: 0.96875, Computation time: 0.7912061214447021\n",
      "Step: 3867, Loss: 0.9162667989730835, Accuracy: 1.0, Computation time: 1.0203514099121094\n",
      "Step: 3868, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 0.7832694053649902\n",
      "Step: 3869, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 1.198887586593628\n",
      "Step: 3870, Loss: 0.9162425398826599, Accuracy: 1.0, Computation time: 1.014383316040039\n",
      "Step: 3871, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 0.8068363666534424\n",
      "Step: 3872, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 0.9281506538391113\n",
      "Step: 3873, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.296377420425415\n",
      "Step: 3874, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 0.828559398651123\n",
      "Step: 3875, Loss: 0.9599722623825073, Accuracy: 0.9375, Computation time: 1.3164434432983398\n",
      "Step: 3876, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 0.9215197563171387\n",
      "Step: 3877, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.8621604442596436\n",
      "Step: 3878, Loss: 0.9380100965499878, Accuracy: 0.96875, Computation time: 1.0321450233459473\n",
      "Step: 3879, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 0.8474171161651611\n",
      "Step: 3880, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 0.9407379627227783\n",
      "Step: 3881, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 0.9537127017974854\n",
      "Step: 3882, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 0.8319308757781982\n",
      "Step: 3883, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 0.895484209060669\n",
      "Step: 3884, Loss: 0.9159131646156311, Accuracy: 1.0, Computation time: 1.0043025016784668\n",
      "Step: 3885, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.0430271625518799\n",
      "Step: 3886, Loss: 0.9183554649353027, Accuracy: 1.0, Computation time: 1.0415444374084473\n",
      "Step: 3887, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 0.8797450065612793\n",
      "Step: 3888, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 0.8288788795471191\n",
      "Step: 3889, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 0.878525972366333\n",
      "Step: 3890, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 0.8252842426300049\n",
      "Step: 3891, Loss: 0.9189326167106628, Accuracy: 1.0, Computation time: 0.9123845100402832\n",
      "########################\n",
      "Test loss: 1.0714073181152344, Test Accuracy_epoch28: 0.7712609767913818\n",
      "########################\n",
      "Step: 3892, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.5199320316314697\n",
      "Step: 3893, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 0.9520561695098877\n",
      "Step: 3894, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 1.453540325164795\n",
      "Step: 3895, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.0008594989776611\n",
      "Step: 3896, Loss: 0.9163042306900024, Accuracy: 1.0, Computation time: 1.0332486629486084\n",
      "Step: 3897, Loss: 0.9168207049369812, Accuracy: 1.0, Computation time: 1.1725058555603027\n",
      "Step: 3898, Loss: 0.9159613251686096, Accuracy: 1.0, Computation time: 0.9113128185272217\n",
      "Step: 3899, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.2388992309570312\n",
      "Step: 3900, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.9294838905334473\n",
      "Step: 3901, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 1.0642077922821045\n",
      "Step: 3902, Loss: 0.9214826822280884, Accuracy: 1.0, Computation time: 1.161794900894165\n",
      "Step: 3903, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.1078851222991943\n",
      "Step: 3904, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 0.9471912384033203\n",
      "Step: 3905, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 0.989335298538208\n",
      "Step: 3906, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.041947841644287\n",
      "Step: 3907, Loss: 0.9375652074813843, Accuracy: 0.96875, Computation time: 1.2265729904174805\n",
      "Step: 3908, Loss: 0.9159925580024719, Accuracy: 1.0, Computation time: 0.9879474639892578\n",
      "Step: 3909, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.0096104145050049\n",
      "Step: 3910, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 0.8483574390411377\n",
      "Step: 3911, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 0.8958740234375\n",
      "Step: 3912, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.0746433734893799\n",
      "Step: 3913, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 0.918022632598877\n",
      "Step: 3914, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 0.9476163387298584\n",
      "Step: 3915, Loss: 0.9160701036453247, Accuracy: 1.0, Computation time: 0.8095264434814453\n",
      "Step: 3916, Loss: 0.9251163005828857, Accuracy: 1.0, Computation time: 0.9320483207702637\n",
      "Step: 3917, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 0.8505425453186035\n",
      "Step: 3918, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 0.815326452255249\n",
      "Step: 3919, Loss: 0.9159336686134338, Accuracy: 1.0, Computation time: 1.1516046524047852\n",
      "Step: 3920, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 0.9589481353759766\n",
      "Step: 3921, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.1497843265533447\n",
      "Step: 3922, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 0.9972381591796875\n",
      "Step: 3923, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.906158685684204\n",
      "Step: 3924, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 0.8013525009155273\n",
      "Step: 3925, Loss: 0.937243640422821, Accuracy: 0.96875, Computation time: 0.8027746677398682\n",
      "Step: 3926, Loss: 0.9159228801727295, Accuracy: 1.0, Computation time: 0.9548847675323486\n",
      "Step: 3927, Loss: 0.915949285030365, Accuracy: 1.0, Computation time: 1.0558981895446777\n",
      "Step: 3928, Loss: 0.9376220703125, Accuracy: 0.96875, Computation time: 1.6023075580596924\n",
      "Step: 3929, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 0.7842633724212646\n",
      "Step: 3930, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 1.1864957809448242\n",
      "Step: 3931, Loss: 0.9163956046104431, Accuracy: 1.0, Computation time: 0.8474559783935547\n",
      "Step: 3932, Loss: 0.9161921739578247, Accuracy: 1.0, Computation time: 0.899782657623291\n",
      "Step: 3933, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.0008525848388672\n",
      "Step: 3934, Loss: 0.9163257479667664, Accuracy: 1.0, Computation time: 1.0481469631195068\n",
      "Step: 3935, Loss: 0.9158852696418762, Accuracy: 1.0, Computation time: 0.8306152820587158\n",
      "Step: 3936, Loss: 0.9158755540847778, Accuracy: 1.0, Computation time: 0.7615134716033936\n",
      "Step: 3937, Loss: 0.93621426820755, Accuracy: 0.96875, Computation time: 0.8697037696838379\n",
      "Step: 3938, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 0.8670907020568848\n",
      "Step: 3939, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 0.9382419586181641\n",
      "Step: 3940, Loss: 0.9160707592964172, Accuracy: 1.0, Computation time: 0.9983241558074951\n",
      "Step: 3941, Loss: 0.9159218668937683, Accuracy: 1.0, Computation time: 0.8411519527435303\n",
      "Step: 3942, Loss: 0.9159318208694458, Accuracy: 1.0, Computation time: 0.9501938819885254\n",
      "Step: 3943, Loss: 0.962920606136322, Accuracy: 0.9375, Computation time: 1.2008748054504395\n",
      "Step: 3944, Loss: 0.9161378741264343, Accuracy: 1.0, Computation time: 0.8477780818939209\n",
      "Step: 3945, Loss: 0.9161189198493958, Accuracy: 1.0, Computation time: 0.9395833015441895\n",
      "Step: 3946, Loss: 0.9160636067390442, Accuracy: 1.0, Computation time: 0.9775006771087646\n",
      "Step: 3947, Loss: 0.9167652726173401, Accuracy: 1.0, Computation time: 1.146507740020752\n",
      "Step: 3948, Loss: 0.9171026349067688, Accuracy: 1.0, Computation time: 1.0693306922912598\n",
      "Step: 3949, Loss: 0.9376425743103027, Accuracy: 0.96875, Computation time: 1.0748975276947021\n",
      "Step: 3950, Loss: 0.916085958480835, Accuracy: 1.0, Computation time: 0.9634735584259033\n",
      "Step: 3951, Loss: 0.9163891077041626, Accuracy: 1.0, Computation time: 1.2968776226043701\n",
      "Step: 3952, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 0.9118106365203857\n",
      "Step: 3953, Loss: 0.9196254014968872, Accuracy: 1.0, Computation time: 1.0773305892944336\n",
      "Step: 3954, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 0.9424042701721191\n",
      "Step: 3955, Loss: 0.9159795641899109, Accuracy: 1.0, Computation time: 0.9533288478851318\n",
      "Step: 3956, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.0686759948730469\n",
      "Step: 3957, Loss: 0.9595298767089844, Accuracy: 0.9375, Computation time: 0.9729475975036621\n",
      "Step: 3958, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 0.8923945426940918\n",
      "Step: 3959, Loss: 0.9161204099655151, Accuracy: 1.0, Computation time: 0.8082280158996582\n",
      "Step: 3960, Loss: 0.9160543084144592, Accuracy: 1.0, Computation time: 1.1258726119995117\n",
      "Step: 3961, Loss: 0.9161588549613953, Accuracy: 1.0, Computation time: 0.9671170711517334\n",
      "Step: 3962, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 0.9264492988586426\n",
      "Step: 3963, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 0.9568579196929932\n",
      "Step: 3964, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 0.9484655857086182\n",
      "Step: 3965, Loss: 0.9160553812980652, Accuracy: 1.0, Computation time: 0.8566131591796875\n",
      "Step: 3966, Loss: 0.9169323444366455, Accuracy: 1.0, Computation time: 0.9781739711761475\n",
      "Step: 3967, Loss: 0.9361473917961121, Accuracy: 0.96875, Computation time: 1.2829861640930176\n",
      "Step: 3968, Loss: 0.9160029888153076, Accuracy: 1.0, Computation time: 0.9255673885345459\n",
      "Step: 3969, Loss: 0.9159781336784363, Accuracy: 1.0, Computation time: 1.0548477172851562\n",
      "Step: 3970, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 1.1111540794372559\n",
      "Step: 3971, Loss: 0.9160209894180298, Accuracy: 1.0, Computation time: 0.8997795581817627\n",
      "Step: 3972, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 0.9055428504943848\n",
      "Step: 3973, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.0608677864074707\n",
      "Step: 3974, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 1.5046045780181885\n",
      "Step: 3975, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 0.8088183403015137\n",
      "Step: 3976, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 0.9584400653839111\n",
      "Step: 3977, Loss: 0.9160517454147339, Accuracy: 1.0, Computation time: 1.2555441856384277\n",
      "Step: 3978, Loss: 0.916588306427002, Accuracy: 1.0, Computation time: 1.1098923683166504\n",
      "Step: 3979, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 0.9085192680358887\n",
      "Step: 3980, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 1.0329768657684326\n",
      "Step: 3981, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 0.8354523181915283\n",
      "Step: 3982, Loss: 0.9159094095230103, Accuracy: 1.0, Computation time: 0.9447388648986816\n",
      "Step: 3983, Loss: 0.9375510811805725, Accuracy: 0.96875, Computation time: 0.7683665752410889\n",
      "Step: 3984, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 0.8159506320953369\n",
      "Step: 3985, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 0.9181098937988281\n",
      "Step: 3986, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.0740745067596436\n",
      "Step: 3987, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 0.9551222324371338\n",
      "Step: 3988, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 0.8487217426300049\n",
      "Step: 3989, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 0.9386074542999268\n",
      "Step: 3990, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.1013519763946533\n",
      "Step: 3991, Loss: 0.9387193918228149, Accuracy: 0.96875, Computation time: 1.233673095703125\n",
      "Step: 3992, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 0.8434171676635742\n",
      "Step: 3993, Loss: 0.9159315228462219, Accuracy: 1.0, Computation time: 1.3436076641082764\n",
      "Step: 3994, Loss: 0.9376334547996521, Accuracy: 0.96875, Computation time: 0.9167399406433105\n",
      "Step: 3995, Loss: 0.9159484505653381, Accuracy: 1.0, Computation time: 0.8525409698486328\n",
      "Step: 3996, Loss: 0.9161375164985657, Accuracy: 1.0, Computation time: 1.1809539794921875\n",
      "Step: 3997, Loss: 0.9159116744995117, Accuracy: 1.0, Computation time: 0.9574880599975586\n",
      "Step: 3998, Loss: 0.9375400543212891, Accuracy: 0.96875, Computation time: 0.8614826202392578\n",
      "Step: 3999, Loss: 0.9295660853385925, Accuracy: 0.96875, Computation time: 1.5761258602142334\n",
      "Step: 4000, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 0.8378322124481201\n",
      "Step: 4001, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 0.7652182579040527\n",
      "Step: 4002, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 0.7994728088378906\n",
      "Step: 4003, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 0.6891608238220215\n",
      "Step: 4004, Loss: 0.9375050067901611, Accuracy: 0.96875, Computation time: 0.7710020542144775\n",
      "Step: 4005, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 0.7205424308776855\n",
      "Step: 4006, Loss: 0.9374123811721802, Accuracy: 0.96875, Computation time: 0.7110822200775146\n",
      "Step: 4007, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 0.7927849292755127\n",
      "Step: 4008, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 0.7568163871765137\n",
      "Step: 4009, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 0.7748956680297852\n",
      "Step: 4010, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 0.7394073009490967\n",
      "Step: 4011, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 0.9470260143280029\n",
      "Step: 4012, Loss: 0.9158914685249329, Accuracy: 1.0, Computation time: 0.71181321144104\n",
      "Step: 4013, Loss: 0.9223433136940002, Accuracy: 1.0, Computation time: 0.7417137622833252\n",
      "Step: 4014, Loss: 0.9160889983177185, Accuracy: 1.0, Computation time: 0.7023162841796875\n",
      "Step: 4015, Loss: 0.915950357913971, Accuracy: 1.0, Computation time: 0.737520694732666\n",
      "Step: 4016, Loss: 0.9190223217010498, Accuracy: 1.0, Computation time: 0.842951774597168\n",
      "Step: 4017, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 0.8464295864105225\n",
      "Step: 4018, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 0.7823238372802734\n",
      "Step: 4019, Loss: 0.9161801934242249, Accuracy: 1.0, Computation time: 1.1194148063659668\n",
      "Step: 4020, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 0.7674815654754639\n",
      "Step: 4021, Loss: 0.916109561920166, Accuracy: 1.0, Computation time: 0.8307480812072754\n",
      "Step: 4022, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 0.9699952602386475\n",
      "Step: 4023, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 0.7695105075836182\n",
      "Step: 4024, Loss: 0.9162973165512085, Accuracy: 1.0, Computation time: 0.851201057434082\n",
      "Step: 4025, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 0.9169423580169678\n",
      "Step: 4026, Loss: 0.9169760346412659, Accuracy: 1.0, Computation time: 0.871776819229126\n",
      "Step: 4027, Loss: 0.9159146547317505, Accuracy: 1.0, Computation time: 0.8686237335205078\n",
      "Step: 4028, Loss: 0.9163667559623718, Accuracy: 1.0, Computation time: 1.2199594974517822\n",
      "Step: 4029, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 0.7595677375793457\n",
      "Step: 4030, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 0.9537408351898193\n",
      "########################\n",
      "Test loss: 1.0694801807403564, Test Accuracy_epoch29: 0.7771260738372803\n",
      "########################\n",
      "Step: 4031, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 0.8764207363128662\n",
      "Step: 4032, Loss: 0.915990948677063, Accuracy: 1.0, Computation time: 0.7705175876617432\n",
      "Step: 4033, Loss: 0.9376727938652039, Accuracy: 0.96875, Computation time: 0.8559300899505615\n",
      "Step: 4034, Loss: 0.9159634113311768, Accuracy: 1.0, Computation time: 1.228011131286621\n",
      "Step: 4035, Loss: 0.9160466194152832, Accuracy: 1.0, Computation time: 0.8437888622283936\n",
      "Step: 4036, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 0.9381988048553467\n",
      "Step: 4037, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 0.8352365493774414\n",
      "Step: 4038, Loss: 0.9160012006759644, Accuracy: 1.0, Computation time: 0.9238550662994385\n",
      "Step: 4039, Loss: 0.9376075863838196, Accuracy: 0.96875, Computation time: 0.975761890411377\n",
      "Step: 4040, Loss: 0.9160803556442261, Accuracy: 1.0, Computation time: 0.7768864631652832\n",
      "Step: 4041, Loss: 0.9160860180854797, Accuracy: 1.0, Computation time: 0.8838579654693604\n",
      "Step: 4042, Loss: 0.9260584115982056, Accuracy: 0.96875, Computation time: 0.9112277030944824\n",
      "Step: 4043, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 0.7319986820220947\n",
      "Step: 4044, Loss: 0.9166876673698425, Accuracy: 1.0, Computation time: 0.770277738571167\n",
      "Step: 4045, Loss: 0.9161685705184937, Accuracy: 1.0, Computation time: 0.7601242065429688\n",
      "Step: 4046, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 0.9661598205566406\n",
      "Step: 4047, Loss: 0.9160827398300171, Accuracy: 1.0, Computation time: 0.9165112972259521\n",
      "Step: 4048, Loss: 0.9376044273376465, Accuracy: 0.96875, Computation time: 0.7954254150390625\n",
      "Step: 4049, Loss: 0.9171125292778015, Accuracy: 1.0, Computation time: 0.7635715007781982\n",
      "Step: 4050, Loss: 0.9159141778945923, Accuracy: 1.0, Computation time: 0.8404541015625\n",
      "Step: 4051, Loss: 0.9159997701644897, Accuracy: 1.0, Computation time: 0.8552570343017578\n",
      "Step: 4052, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 0.8258893489837646\n",
      "Step: 4053, Loss: 0.9159959554672241, Accuracy: 1.0, Computation time: 1.4002254009246826\n",
      "Step: 4054, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 0.7525067329406738\n",
      "Step: 4055, Loss: 0.9169589877128601, Accuracy: 1.0, Computation time: 1.0545015335083008\n",
      "Step: 4056, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 0.7383401393890381\n",
      "Step: 4057, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 0.8945305347442627\n",
      "Step: 4058, Loss: 0.9159507751464844, Accuracy: 1.0, Computation time: 1.0200538635253906\n",
      "Step: 4059, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 0.773578405380249\n",
      "Step: 4060, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 0.8376023769378662\n",
      "Step: 4061, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.005068063735962\n",
      "Step: 4062, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 0.8353631496429443\n",
      "Step: 4063, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 0.8709564208984375\n",
      "Step: 4064, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 0.8364686965942383\n",
      "Step: 4065, Loss: 0.9376283884048462, Accuracy: 0.96875, Computation time: 0.8897769451141357\n",
      "Step: 4066, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 0.9011523723602295\n",
      "Step: 4067, Loss: 0.9294323921203613, Accuracy: 0.96875, Computation time: 0.834608793258667\n",
      "Step: 4068, Loss: 0.9374877214431763, Accuracy: 0.96875, Computation time: 0.9172344207763672\n",
      "Step: 4069, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.0055854320526123\n",
      "Step: 4070, Loss: 0.9204695224761963, Accuracy: 1.0, Computation time: 0.9663317203521729\n",
      "Step: 4071, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.0485751628875732\n",
      "Step: 4072, Loss: 0.9159566164016724, Accuracy: 1.0, Computation time: 0.858424186706543\n",
      "Step: 4073, Loss: 0.9160704612731934, Accuracy: 1.0, Computation time: 0.8731653690338135\n",
      "Step: 4074, Loss: 0.9159917831420898, Accuracy: 1.0, Computation time: 0.855398416519165\n",
      "Step: 4075, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 0.8085155487060547\n",
      "Step: 4076, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 0.8023867607116699\n",
      "Step: 4077, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 0.8336024284362793\n",
      "Step: 4078, Loss: 0.9255146384239197, Accuracy: 0.96875, Computation time: 0.9334611892700195\n",
      "Step: 4079, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 0.9714856147766113\n",
      "Step: 4080, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 1.0576164722442627\n",
      "Step: 4081, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 1.004058837890625\n",
      "Step: 4082, Loss: 0.9160816073417664, Accuracy: 1.0, Computation time: 1.12614107131958\n",
      "Step: 4083, Loss: 0.9159322381019592, Accuracy: 1.0, Computation time: 0.9043915271759033\n",
      "Step: 4084, Loss: 0.9159330725669861, Accuracy: 1.0, Computation time: 1.1063520908355713\n",
      "Step: 4085, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.2125298976898193\n",
      "Step: 4086, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 0.9157297611236572\n",
      "Step: 4087, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 2.3307390213012695\n",
      "Step: 4088, Loss: 0.9219462275505066, Accuracy: 1.0, Computation time: 1.1905543804168701\n",
      "Step: 4089, Loss: 0.9375632405281067, Accuracy: 0.96875, Computation time: 0.8295223712921143\n",
      "Step: 4090, Loss: 0.9159860610961914, Accuracy: 1.0, Computation time: 1.0540614128112793\n",
      "Step: 4091, Loss: 0.9160105586051941, Accuracy: 1.0, Computation time: 0.947972297668457\n",
      "Step: 4092, Loss: 0.9160934090614319, Accuracy: 1.0, Computation time: 1.012336015701294\n",
      "Step: 4093, Loss: 0.916146993637085, Accuracy: 1.0, Computation time: 1.2079105377197266\n",
      "Step: 4094, Loss: 0.9159497618675232, Accuracy: 1.0, Computation time: 1.2114126682281494\n",
      "Step: 4095, Loss: 0.9376808404922485, Accuracy: 0.96875, Computation time: 0.9878497123718262\n",
      "Step: 4096, Loss: 0.9164882898330688, Accuracy: 1.0, Computation time: 1.1687679290771484\n",
      "Step: 4097, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 0.8854465484619141\n",
      "Step: 4098, Loss: 0.9159335494041443, Accuracy: 1.0, Computation time: 1.0431783199310303\n",
      "Step: 4099, Loss: 0.9371101260185242, Accuracy: 0.96875, Computation time: 0.8631713390350342\n",
      "Step: 4100, Loss: 0.9160022139549255, Accuracy: 1.0, Computation time: 1.0036616325378418\n",
      "Step: 4101, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 0.9494569301605225\n",
      "Step: 4102, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.024343729019165\n",
      "Step: 4103, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.2393701076507568\n",
      "Step: 4104, Loss: 0.9167907238006592, Accuracy: 1.0, Computation time: 0.9153716564178467\n",
      "Step: 4105, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 0.9931435585021973\n",
      "Step: 4106, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 0.9390592575073242\n",
      "Step: 4107, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 0.9982008934020996\n",
      "Step: 4108, Loss: 0.9158888459205627, Accuracy: 1.0, Computation time: 0.8408362865447998\n",
      "Step: 4109, Loss: 0.9160704016685486, Accuracy: 1.0, Computation time: 0.9879467487335205\n",
      "Step: 4110, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.0224506855010986\n",
      "Step: 4111, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 0.9342522621154785\n",
      "Step: 4112, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 0.8403289318084717\n",
      "Step: 4113, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 0.9772417545318604\n",
      "Step: 4114, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.1063520908355713\n",
      "Step: 4115, Loss: 0.9592463374137878, Accuracy: 0.9375, Computation time: 0.9177775382995605\n",
      "Step: 4116, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 0.8724503517150879\n",
      "Step: 4117, Loss: 0.9376500248908997, Accuracy: 0.96875, Computation time: 1.0760812759399414\n",
      "Step: 4118, Loss: 0.9374948740005493, Accuracy: 0.96875, Computation time: 1.163325309753418\n",
      "Step: 4119, Loss: 0.9160056114196777, Accuracy: 1.0, Computation time: 0.9270603656768799\n",
      "Step: 4120, Loss: 0.9159860014915466, Accuracy: 1.0, Computation time: 0.9281842708587646\n",
      "Step: 4121, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 0.8607845306396484\n",
      "Step: 4122, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 1.1707208156585693\n",
      "Step: 4123, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.2679407596588135\n",
      "Step: 4124, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.0042331218719482\n",
      "Step: 4125, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 0.9525477886199951\n",
      "Step: 4126, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 0.8761768341064453\n",
      "Step: 4127, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.0590412616729736\n",
      "Step: 4128, Loss: 0.9334161281585693, Accuracy: 0.96875, Computation time: 1.1358964443206787\n",
      "Step: 4129, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.141061544418335\n",
      "Step: 4130, Loss: 0.9159371852874756, Accuracy: 1.0, Computation time: 1.1017041206359863\n",
      "Step: 4131, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 1.1994953155517578\n",
      "Step: 4132, Loss: 0.9375886917114258, Accuracy: 0.96875, Computation time: 1.0715217590332031\n",
      "Step: 4133, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 0.9373462200164795\n",
      "Step: 4134, Loss: 0.9174992442131042, Accuracy: 1.0, Computation time: 0.9442851543426514\n",
      "Step: 4135, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 0.8836889266967773\n",
      "Step: 4136, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.0649805068969727\n",
      "Step: 4137, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 0.9334921836853027\n",
      "Step: 4138, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 1.1365294456481934\n",
      "Step: 4139, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 0.9511566162109375\n",
      "Step: 4140, Loss: 0.9166739583015442, Accuracy: 1.0, Computation time: 1.1542837619781494\n",
      "Step: 4141, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 0.8883209228515625\n",
      "Step: 4142, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.0327212810516357\n",
      "Step: 4143, Loss: 0.9159579873085022, Accuracy: 1.0, Computation time: 1.3700861930847168\n",
      "Step: 4144, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 0.9390549659729004\n",
      "Step: 4145, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.1361701488494873\n",
      "Step: 4146, Loss: 0.9237409830093384, Accuracy: 1.0, Computation time: 1.235978364944458\n",
      "Step: 4147, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 1.0504181385040283\n",
      "Step: 4148, Loss: 0.9160416126251221, Accuracy: 1.0, Computation time: 1.2875421047210693\n",
      "Step: 4149, Loss: 0.9159160852432251, Accuracy: 1.0, Computation time: 0.8279426097869873\n",
      "Step: 4150, Loss: 0.9160316586494446, Accuracy: 1.0, Computation time: 0.8191268444061279\n",
      "Step: 4151, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 0.8356449604034424\n",
      "Step: 4152, Loss: 0.9159852862358093, Accuracy: 1.0, Computation time: 0.9334568977355957\n",
      "Step: 4153, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 0.7372605800628662\n",
      "Step: 4154, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 0.884519100189209\n",
      "Step: 4155, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.1303026676177979\n",
      "Step: 4156, Loss: 0.9374719262123108, Accuracy: 0.96875, Computation time: 0.6992559432983398\n",
      "Step: 4157, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.1089715957641602\n",
      "Step: 4158, Loss: 0.9161084294319153, Accuracy: 1.0, Computation time: 0.9129142761230469\n",
      "Step: 4159, Loss: 0.9159830212593079, Accuracy: 1.0, Computation time: 1.502619981765747\n",
      "Step: 4160, Loss: 0.9159033894538879, Accuracy: 1.0, Computation time: 0.7402694225311279\n",
      "Step: 4161, Loss: 0.916008710861206, Accuracy: 1.0, Computation time: 0.8188502788543701\n",
      "Step: 4162, Loss: 0.916472852230072, Accuracy: 1.0, Computation time: 0.8028819561004639\n",
      "Step: 4163, Loss: 0.9396553635597229, Accuracy: 0.96875, Computation time: 1.2523233890533447\n",
      "Step: 4164, Loss: 0.9383307695388794, Accuracy: 0.96875, Computation time: 0.768118143081665\n",
      "Step: 4165, Loss: 0.9361168146133423, Accuracy: 0.96875, Computation time: 0.8845913410186768\n",
      "Step: 4166, Loss: 0.9343751072883606, Accuracy: 0.96875, Computation time: 0.8627409934997559\n",
      "Step: 4167, Loss: 0.915941596031189, Accuracy: 1.0, Computation time: 0.7905097007751465\n",
      "Step: 4168, Loss: 0.9228353500366211, Accuracy: 1.0, Computation time: 0.8197979927062988\n",
      "Step: 4169, Loss: 0.9162878394126892, Accuracy: 1.0, Computation time: 0.7738227844238281\n",
      "########################\n",
      "Test loss: 1.0699896812438965, Test Accuracy_epoch30: 0.7761485576629639\n",
      "########################\n",
      "Step: 4170, Loss: 0.9160756468772888, Accuracy: 1.0, Computation time: 0.8016669750213623\n",
      "Step: 4171, Loss: 0.9376717209815979, Accuracy: 0.96875, Computation time: 0.7545945644378662\n",
      "Step: 4172, Loss: 0.9161087870597839, Accuracy: 1.0, Computation time: 0.975574254989624\n",
      "Step: 4173, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 0.7642257213592529\n",
      "Step: 4174, Loss: 0.9379908442497253, Accuracy: 0.96875, Computation time: 0.9022941589355469\n",
      "Step: 4175, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 0.8007259368896484\n",
      "Step: 4176, Loss: 0.9161016941070557, Accuracy: 1.0, Computation time: 0.7256712913513184\n",
      "Step: 4177, Loss: 0.916120171546936, Accuracy: 1.0, Computation time: 0.7472517490386963\n",
      "Step: 4178, Loss: 0.91620934009552, Accuracy: 1.0, Computation time: 0.7619318962097168\n",
      "Step: 4179, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 0.7187473773956299\n",
      "Step: 4180, Loss: 0.9161600470542908, Accuracy: 1.0, Computation time: 0.8044838905334473\n",
      "Step: 4181, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 0.8403809070587158\n",
      "Step: 4182, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 0.7756407260894775\n",
      "Step: 4183, Loss: 0.9176183938980103, Accuracy: 1.0, Computation time: 1.0128097534179688\n",
      "Step: 4184, Loss: 0.9159388542175293, Accuracy: 1.0, Computation time: 0.8606441020965576\n",
      "Step: 4185, Loss: 0.9160565137863159, Accuracy: 1.0, Computation time: 1.0222268104553223\n",
      "Step: 4186, Loss: 0.916045606136322, Accuracy: 1.0, Computation time: 0.6804111003875732\n",
      "Step: 4187, Loss: 0.9159964323043823, Accuracy: 1.0, Computation time: 0.7792906761169434\n",
      "Step: 4188, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 0.7978293895721436\n",
      "Step: 4189, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 0.8202991485595703\n",
      "Step: 4190, Loss: 0.9161777496337891, Accuracy: 1.0, Computation time: 0.9760847091674805\n",
      "Step: 4191, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 0.7908709049224854\n",
      "Step: 4192, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.0475711822509766\n",
      "Step: 4193, Loss: 0.9159385561943054, Accuracy: 1.0, Computation time: 0.7110216617584229\n",
      "Step: 4194, Loss: 0.9161368012428284, Accuracy: 1.0, Computation time: 0.9527068138122559\n",
      "Step: 4195, Loss: 0.9159480333328247, Accuracy: 1.0, Computation time: 0.8368241786956787\n",
      "Step: 4196, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 0.7547123432159424\n",
      "Step: 4197, Loss: 0.9256719946861267, Accuracy: 0.96875, Computation time: 0.7319896221160889\n",
      "Step: 4198, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 0.8171653747558594\n",
      "Step: 4199, Loss: 0.9160683155059814, Accuracy: 1.0, Computation time: 1.037992000579834\n",
      "Step: 4200, Loss: 0.9160223603248596, Accuracy: 1.0, Computation time: 0.9402604103088379\n",
      "Step: 4201, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 0.8694164752960205\n",
      "Step: 4202, Loss: 0.9379975199699402, Accuracy: 0.96875, Computation time: 0.9499456882476807\n",
      "Step: 4203, Loss: 0.916022539138794, Accuracy: 1.0, Computation time: 1.1471748352050781\n",
      "Step: 4204, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 0.6896078586578369\n",
      "Step: 4205, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 0.7096354961395264\n",
      "Step: 4206, Loss: 0.9159018993377686, Accuracy: 1.0, Computation time: 0.8362181186676025\n",
      "Step: 4207, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 1.0148975849151611\n",
      "Step: 4208, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 0.7898399829864502\n",
      "Step: 4209, Loss: 0.933661162853241, Accuracy: 0.96875, Computation time: 0.9432480335235596\n",
      "Step: 4210, Loss: 0.9376398324966431, Accuracy: 0.96875, Computation time: 0.8087193965911865\n",
      "Step: 4211, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 0.8796055316925049\n",
      "Step: 4212, Loss: 0.9319131374359131, Accuracy: 0.96875, Computation time: 1.0221436023712158\n",
      "Step: 4213, Loss: 0.9170776605606079, Accuracy: 1.0, Computation time: 1.068882942199707\n",
      "Step: 4214, Loss: 0.9159307479858398, Accuracy: 1.0, Computation time: 0.8178143501281738\n",
      "Step: 4215, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 0.8848128318786621\n",
      "Step: 4216, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 0.8015353679656982\n",
      "Step: 4217, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 0.8432881832122803\n",
      "Step: 4218, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 0.7973675727844238\n",
      "Step: 4219, Loss: 0.9161772727966309, Accuracy: 1.0, Computation time: 1.1803691387176514\n",
      "Step: 4220, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 0.8941569328308105\n",
      "Step: 4221, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 0.9019405841827393\n",
      "Step: 4222, Loss: 0.9209572076797485, Accuracy: 1.0, Computation time: 0.8570892810821533\n",
      "Step: 4223, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 0.8087482452392578\n",
      "Step: 4224, Loss: 0.9180328249931335, Accuracy: 1.0, Computation time: 1.0521771907806396\n",
      "Step: 4225, Loss: 0.9326427578926086, Accuracy: 0.96875, Computation time: 0.7991640567779541\n",
      "Step: 4226, Loss: 0.9166194796562195, Accuracy: 1.0, Computation time: 0.9996151924133301\n",
      "Step: 4227, Loss: 0.9161467552185059, Accuracy: 1.0, Computation time: 0.9747402667999268\n",
      "Step: 4228, Loss: 0.9160521626472473, Accuracy: 1.0, Computation time: 0.9210481643676758\n",
      "Step: 4229, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 0.8740200996398926\n",
      "Step: 4230, Loss: 0.9159376621246338, Accuracy: 1.0, Computation time: 0.9581749439239502\n",
      "Step: 4231, Loss: 0.9376786351203918, Accuracy: 0.96875, Computation time: 0.7850015163421631\n",
      "Step: 4232, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 0.8014645576477051\n",
      "Step: 4233, Loss: 0.9226324558258057, Accuracy: 1.0, Computation time: 0.8531599044799805\n",
      "Step: 4234, Loss: 0.9601900577545166, Accuracy: 0.9375, Computation time: 0.8931534290313721\n",
      "Step: 4235, Loss: 0.9159567356109619, Accuracy: 1.0, Computation time: 0.7817461490631104\n",
      "Step: 4236, Loss: 0.9159929156303406, Accuracy: 1.0, Computation time: 1.0381824970245361\n",
      "Step: 4237, Loss: 0.9161081910133362, Accuracy: 1.0, Computation time: 0.8299989700317383\n",
      "Step: 4238, Loss: 0.9162064790725708, Accuracy: 1.0, Computation time: 0.7315878868103027\n",
      "Step: 4239, Loss: 0.9161573052406311, Accuracy: 1.0, Computation time: 0.8575987815856934\n",
      "Step: 4240, Loss: 0.916061282157898, Accuracy: 1.0, Computation time: 0.8225457668304443\n",
      "Step: 4241, Loss: 0.9159450531005859, Accuracy: 1.0, Computation time: 0.9906814098358154\n",
      "Step: 4242, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 0.8375985622406006\n",
      "Step: 4243, Loss: 0.9376487731933594, Accuracy: 0.96875, Computation time: 1.1078081130981445\n",
      "Step: 4244, Loss: 0.9165461659431458, Accuracy: 1.0, Computation time: 1.325674057006836\n",
      "Step: 4245, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 0.8462412357330322\n",
      "Step: 4246, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 0.8270771503448486\n",
      "Step: 4247, Loss: 0.9159727096557617, Accuracy: 1.0, Computation time: 1.0205130577087402\n",
      "Step: 4248, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.0457813739776611\n",
      "Step: 4249, Loss: 0.9159557223320007, Accuracy: 1.0, Computation time: 0.9396929740905762\n",
      "Step: 4250, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 0.8664793968200684\n",
      "Step: 4251, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 0.878587007522583\n",
      "Step: 4252, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 0.8987207412719727\n",
      "Step: 4253, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 0.9273829460144043\n",
      "Step: 4254, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 0.8632211685180664\n",
      "Step: 4255, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 0.8945732116699219\n",
      "Step: 4256, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 1.3704791069030762\n",
      "Step: 4257, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 0.8320093154907227\n",
      "Step: 4258, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 0.8723278045654297\n",
      "Step: 4259, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 0.8305659294128418\n",
      "Step: 4260, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.0728938579559326\n",
      "Step: 4261, Loss: 0.9163581132888794, Accuracy: 1.0, Computation time: 1.1684057712554932\n",
      "Step: 4262, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 0.8289036750793457\n",
      "Step: 4263, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 0.7319700717926025\n",
      "Step: 4264, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 0.9202117919921875\n",
      "Step: 4265, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 0.872643232345581\n",
      "Step: 4266, Loss: 0.9375984072685242, Accuracy: 0.96875, Computation time: 0.9517028331756592\n",
      "Step: 4267, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 0.9074499607086182\n",
      "Step: 4268, Loss: 0.9158361554145813, Accuracy: 1.0, Computation time: 0.9117391109466553\n",
      "Step: 4269, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 0.8140037059783936\n",
      "Step: 4270, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.0219521522521973\n",
      "Step: 4271, Loss: 0.9375526309013367, Accuracy: 0.96875, Computation time: 0.9518325328826904\n",
      "Step: 4272, Loss: 0.9293392300605774, Accuracy: 0.96875, Computation time: 0.9039087295532227\n",
      "Step: 4273, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.0588736534118652\n",
      "Step: 4274, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 0.9442980289459229\n",
      "Step: 4275, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 0.8014769554138184\n",
      "Step: 4276, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 0.7958095073699951\n",
      "Step: 4277, Loss: 0.9159323573112488, Accuracy: 1.0, Computation time: 0.9674668312072754\n",
      "Step: 4278, Loss: 0.9301134943962097, Accuracy: 0.96875, Computation time: 1.1187212467193604\n",
      "Step: 4279, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 0.8578228950500488\n",
      "Step: 4280, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 0.939011812210083\n",
      "Step: 4281, Loss: 0.9160462617874146, Accuracy: 1.0, Computation time: 1.0548481941223145\n",
      "Step: 4282, Loss: 0.9161232113838196, Accuracy: 1.0, Computation time: 1.1903958320617676\n",
      "Step: 4283, Loss: 0.9160205125808716, Accuracy: 1.0, Computation time: 1.0015084743499756\n",
      "Step: 4284, Loss: 0.9375690817832947, Accuracy: 0.96875, Computation time: 0.9414176940917969\n",
      "Step: 4285, Loss: 0.917546272277832, Accuracy: 1.0, Computation time: 0.9569957256317139\n",
      "Step: 4286, Loss: 0.9375163316726685, Accuracy: 0.96875, Computation time: 0.8914675712585449\n",
      "Step: 4287, Loss: 0.9159794449806213, Accuracy: 1.0, Computation time: 0.8649303913116455\n",
      "Step: 4288, Loss: 0.9160292148590088, Accuracy: 1.0, Computation time: 0.9268829822540283\n",
      "Step: 4289, Loss: 0.9159882664680481, Accuracy: 1.0, Computation time: 0.7657320499420166\n",
      "Step: 4290, Loss: 0.9161241054534912, Accuracy: 1.0, Computation time: 1.014561414718628\n",
      "Step: 4291, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 0.8697679042816162\n",
      "Step: 4292, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.1803841590881348\n",
      "Step: 4293, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 0.821936845779419\n",
      "Step: 4294, Loss: 0.9375877976417542, Accuracy: 0.96875, Computation time: 0.993354082107544\n",
      "Step: 4295, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 0.7711212635040283\n",
      "Step: 4296, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 0.864290714263916\n",
      "Step: 4297, Loss: 0.9376004338264465, Accuracy: 0.96875, Computation time: 0.859168529510498\n",
      "Step: 4298, Loss: 0.9376159310340881, Accuracy: 0.96875, Computation time: 1.187978744506836\n",
      "Step: 4299, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 0.845954179763794\n",
      "Step: 4300, Loss: 0.9375525712966919, Accuracy: 0.96875, Computation time: 0.7387552261352539\n",
      "Step: 4301, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 0.8198199272155762\n",
      "Step: 4302, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.008821725845337\n",
      "Step: 4303, Loss: 0.9175110459327698, Accuracy: 1.0, Computation time: 0.911874532699585\n",
      "Step: 4304, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 0.8850102424621582\n",
      "Step: 4305, Loss: 0.9171758890151978, Accuracy: 1.0, Computation time: 0.9340758323669434\n",
      "Step: 4306, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 0.8603861331939697\n",
      "Step: 4307, Loss: 0.9168094396591187, Accuracy: 1.0, Computation time: 1.7376949787139893\n",
      "Step: 4308, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 0.779426097869873\n",
      "########################\n",
      "Test loss: 1.0689748525619507, Test Accuracy_epoch31: 0.7781035900115967\n",
      "########################\n",
      "Step: 4309, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.0915350914001465\n",
      "Step: 4310, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 0.7789490222930908\n",
      "Step: 4311, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 0.8626043796539307\n",
      "Step: 4312, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 0.8361525535583496\n",
      "Step: 4313, Loss: 0.9166266322135925, Accuracy: 1.0, Computation time: 0.9046216011047363\n",
      "Step: 4314, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 0.7206816673278809\n",
      "Step: 4315, Loss: 0.9337916374206543, Accuracy: 0.96875, Computation time: 0.9700131416320801\n",
      "Step: 4316, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 1.4624578952789307\n",
      "Step: 4317, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 0.8442485332489014\n",
      "Step: 4318, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 0.8579175472259521\n",
      "Step: 4319, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 1.0794215202331543\n",
      "Step: 4320, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.0841655731201172\n",
      "Step: 4321, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 0.860905647277832\n",
      "Step: 4322, Loss: 0.9163215160369873, Accuracy: 1.0, Computation time: 1.3430180549621582\n",
      "Step: 4323, Loss: 0.9181703329086304, Accuracy: 1.0, Computation time: 1.106031894683838\n",
      "Step: 4324, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 0.9778485298156738\n",
      "Step: 4325, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 0.9150540828704834\n",
      "Step: 4326, Loss: 0.9159699082374573, Accuracy: 1.0, Computation time: 0.999060869216919\n",
      "Step: 4327, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.0744943618774414\n",
      "Step: 4328, Loss: 0.9159123301506042, Accuracy: 1.0, Computation time: 0.8865795135498047\n",
      "Step: 4329, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 1.0844435691833496\n",
      "Step: 4330, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.866870641708374\n",
      "Step: 4331, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 0.9680919647216797\n",
      "Step: 4332, Loss: 0.9193244576454163, Accuracy: 1.0, Computation time: 0.9937038421630859\n",
      "Step: 4333, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.0446891784667969\n",
      "Step: 4334, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 0.8199911117553711\n",
      "Step: 4335, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 0.9057455062866211\n",
      "Step: 4336, Loss: 0.9161294102668762, Accuracy: 1.0, Computation time: 1.5108959674835205\n",
      "Step: 4337, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 0.8377482891082764\n",
      "Step: 4338, Loss: 0.9160555005073547, Accuracy: 1.0, Computation time: 0.8914763927459717\n",
      "Step: 4339, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 0.7986011505126953\n",
      "Step: 4340, Loss: 0.915988564491272, Accuracy: 1.0, Computation time: 0.982184886932373\n",
      "Step: 4341, Loss: 0.9158470034599304, Accuracy: 1.0, Computation time: 0.851283073425293\n",
      "Step: 4342, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 0.7914097309112549\n",
      "Step: 4343, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 0.892608642578125\n",
      "Step: 4344, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 0.8355660438537598\n",
      "Step: 4345, Loss: 0.9159508347511292, Accuracy: 1.0, Computation time: 0.9689178466796875\n",
      "Step: 4346, Loss: 0.9269618988037109, Accuracy: 0.96875, Computation time: 0.8231782913208008\n",
      "Step: 4347, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 0.8641974925994873\n",
      "Step: 4348, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 0.8596479892730713\n",
      "Step: 4349, Loss: 0.9160995483398438, Accuracy: 1.0, Computation time: 0.9097235202789307\n",
      "Step: 4350, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 0.7924504280090332\n",
      "Step: 4351, Loss: 0.9159305691719055, Accuracy: 1.0, Computation time: 1.226606845855713\n",
      "Step: 4352, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 0.8811097145080566\n",
      "Step: 4353, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 0.7552065849304199\n",
      "Step: 4354, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 0.7210631370544434\n",
      "Step: 4355, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 0.7825188636779785\n",
      "Step: 4356, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 0.8196260929107666\n",
      "Step: 4357, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 0.9159667491912842\n",
      "Step: 4358, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 0.8407540321350098\n",
      "Step: 4359, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 0.8494570255279541\n",
      "Step: 4360, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 0.9973013401031494\n",
      "Step: 4361, Loss: 0.9381758570671082, Accuracy: 0.96875, Computation time: 1.3369269371032715\n",
      "Step: 4362, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 0.8064136505126953\n",
      "Step: 4363, Loss: 0.9375651478767395, Accuracy: 0.96875, Computation time: 1.0126254558563232\n",
      "Step: 4364, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.0428621768951416\n",
      "Step: 4365, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 0.8469488620758057\n",
      "Step: 4366, Loss: 0.9164873957633972, Accuracy: 1.0, Computation time: 0.9555885791778564\n",
      "Step: 4367, Loss: 0.9376170039176941, Accuracy: 0.96875, Computation time: 0.7821331024169922\n",
      "Step: 4368, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 0.8468408584594727\n",
      "Step: 4369, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 0.7716472148895264\n",
      "Step: 4370, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 0.9777154922485352\n",
      "Step: 4371, Loss: 0.9158603549003601, Accuracy: 1.0, Computation time: 0.7822589874267578\n",
      "Step: 4372, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 0.8951983451843262\n",
      "Step: 4373, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 0.8434553146362305\n",
      "Step: 4374, Loss: 0.9161457419395447, Accuracy: 1.0, Computation time: 0.7526609897613525\n",
      "Step: 4375, Loss: 0.9166815876960754, Accuracy: 1.0, Computation time: 0.9183340072631836\n",
      "Step: 4376, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 0.7861959934234619\n",
      "Step: 4377, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 0.8502435684204102\n",
      "Step: 4378, Loss: 0.9378863573074341, Accuracy: 0.96875, Computation time: 0.8965778350830078\n",
      "Step: 4379, Loss: 0.9165343046188354, Accuracy: 1.0, Computation time: 0.9010405540466309\n",
      "Step: 4380, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 1.4574368000030518\n",
      "Step: 4381, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 0.7761707305908203\n",
      "Step: 4382, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 0.8902997970581055\n",
      "Step: 4383, Loss: 0.9162221550941467, Accuracy: 1.0, Computation time: 1.0071501731872559\n",
      "Step: 4384, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 0.8660829067230225\n",
      "Step: 4385, Loss: 0.9375936388969421, Accuracy: 0.96875, Computation time: 1.036198377609253\n",
      "Step: 4386, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 0.7922103404998779\n",
      "Step: 4387, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 0.8525960445404053\n",
      "Step: 4388, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 0.9819989204406738\n",
      "Step: 4389, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 0.8634548187255859\n",
      "Step: 4390, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 0.7749943733215332\n",
      "Step: 4391, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 0.8051402568817139\n",
      "Step: 4392, Loss: 0.9158343076705933, Accuracy: 1.0, Computation time: 0.8405702114105225\n",
      "Step: 4393, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 1.151846170425415\n",
      "Step: 4394, Loss: 0.9158408045768738, Accuracy: 1.0, Computation time: 0.9290831089019775\n",
      "Step: 4395, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 0.8426220417022705\n",
      "Step: 4396, Loss: 0.9451487064361572, Accuracy: 0.96875, Computation time: 1.0842247009277344\n",
      "Step: 4397, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 0.987515926361084\n",
      "Step: 4398, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.7526693344116211\n",
      "Step: 4399, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 0.7502992153167725\n",
      "Step: 4400, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 0.7636754512786865\n",
      "Step: 4401, Loss: 0.9159955382347107, Accuracy: 1.0, Computation time: 0.8821876049041748\n",
      "Step: 4402, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 0.8927133083343506\n",
      "Step: 4403, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.1713852882385254\n",
      "Step: 4404, Loss: 0.9375230073928833, Accuracy: 0.96875, Computation time: 0.8626687526702881\n",
      "Step: 4405, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 0.8754935264587402\n",
      "Step: 4406, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 0.8582618236541748\n",
      "Step: 4407, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 0.8694674968719482\n",
      "Step: 4408, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 0.7963802814483643\n",
      "Step: 4409, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 0.9290368556976318\n",
      "Step: 4410, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 0.8846197128295898\n",
      "Step: 4411, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 0.9550378322601318\n",
      "Step: 4412, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 0.8573207855224609\n",
      "Step: 4413, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 0.9082889556884766\n",
      "Step: 4414, Loss: 0.9212595224380493, Accuracy: 1.0, Computation time: 0.9229915142059326\n",
      "Step: 4415, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 0.7599637508392334\n",
      "Step: 4416, Loss: 0.9160525798797607, Accuracy: 1.0, Computation time: 0.8860609531402588\n",
      "Step: 4417, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 0.8707492351531982\n",
      "Step: 4418, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 0.7697184085845947\n",
      "Step: 4419, Loss: 0.9376501441001892, Accuracy: 0.96875, Computation time: 0.8493645191192627\n",
      "Step: 4420, Loss: 0.9159510135650635, Accuracy: 1.0, Computation time: 1.0239155292510986\n",
      "Step: 4421, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.7988598346710205\n",
      "Step: 4422, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.1639132499694824\n",
      "Step: 4423, Loss: 0.959082305431366, Accuracy: 0.9375, Computation time: 0.9985074996948242\n",
      "Step: 4424, Loss: 0.9161188006401062, Accuracy: 1.0, Computation time: 0.9228839874267578\n",
      "Step: 4425, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 0.9007792472839355\n",
      "Step: 4426, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 0.7533743381500244\n",
      "Step: 4427, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.889711856842041\n",
      "Step: 4428, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 0.8782439231872559\n",
      "Step: 4429, Loss: 0.9375460147857666, Accuracy: 0.96875, Computation time: 1.0657086372375488\n",
      "Step: 4430, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 0.7953946590423584\n",
      "Step: 4431, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.0034408569335938\n",
      "Step: 4432, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 0.9303152561187744\n",
      "Step: 4433, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.0956978797912598\n",
      "Step: 4434, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 0.8095822334289551\n",
      "Step: 4435, Loss: 0.9159688353538513, Accuracy: 1.0, Computation time: 1.1522655487060547\n",
      "Step: 4436, Loss: 0.9375201463699341, Accuracy: 0.96875, Computation time: 0.7898681163787842\n",
      "Step: 4437, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 0.7767548561096191\n",
      "Step: 4438, Loss: 0.9165353775024414, Accuracy: 1.0, Computation time: 1.440359115600586\n",
      "Step: 4439, Loss: 0.9162623286247253, Accuracy: 1.0, Computation time: 1.007946491241455\n",
      "Step: 4440, Loss: 0.9209117293357849, Accuracy: 1.0, Computation time: 0.9180059432983398\n",
      "Step: 4441, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 0.9318802356719971\n",
      "Step: 4442, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 0.8406350612640381\n",
      "Step: 4443, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 0.8205204010009766\n",
      "Step: 4444, Loss: 0.9161270260810852, Accuracy: 1.0, Computation time: 0.9499857425689697\n",
      "Step: 4445, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.7859482765197754\n",
      "Step: 4446, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 0.856086015701294\n",
      "########################\n",
      "Test loss: 1.0710314512252808, Test Accuracy_epoch32: 0.774193525314331\n",
      "########################\n",
      "Step: 4447, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 0.8024866580963135\n",
      "Step: 4448, Loss: 0.9161697030067444, Accuracy: 1.0, Computation time: 1.1477553844451904\n",
      "Step: 4449, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 0.836188554763794\n",
      "Step: 4450, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 0.8510646820068359\n",
      "Step: 4451, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 0.8341968059539795\n",
      "Step: 4452, Loss: 0.9375516772270203, Accuracy: 0.96875, Computation time: 0.7985715866088867\n",
      "Step: 4453, Loss: 0.9167817831039429, Accuracy: 1.0, Computation time: 1.0086181163787842\n",
      "Step: 4454, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 0.7387468814849854\n",
      "Step: 4455, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 0.7521238327026367\n",
      "Step: 4456, Loss: 0.9358818531036377, Accuracy: 0.96875, Computation time: 0.8017926216125488\n",
      "Step: 4457, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.002945899963379\n",
      "Step: 4458, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 0.8136897087097168\n",
      "Step: 4459, Loss: 0.9160019159317017, Accuracy: 1.0, Computation time: 0.9400308132171631\n",
      "Step: 4460, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 0.8244338035583496\n",
      "Step: 4461, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 0.910599946975708\n",
      "Step: 4462, Loss: 0.9158894419670105, Accuracy: 1.0, Computation time: 0.8623244762420654\n",
      "Step: 4463, Loss: 0.9638845324516296, Accuracy: 0.9375, Computation time: 0.9410161972045898\n",
      "Step: 4464, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 0.9094362258911133\n",
      "Step: 4465, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 1.2575161457061768\n",
      "Step: 4466, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 0.8055629730224609\n",
      "Step: 4467, Loss: 0.9159222841262817, Accuracy: 1.0, Computation time: 0.8272542953491211\n",
      "Step: 4468, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 0.8807733058929443\n",
      "Step: 4469, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 0.8120877742767334\n",
      "Step: 4470, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 0.9080078601837158\n",
      "Step: 4471, Loss: 0.9304085373878479, Accuracy: 0.96875, Computation time: 0.9052937030792236\n",
      "Step: 4472, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 0.7988059520721436\n",
      "Step: 4473, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 0.9149706363677979\n",
      "Step: 4474, Loss: 0.915931761264801, Accuracy: 1.0, Computation time: 0.891167402267456\n",
      "Step: 4475, Loss: 0.9159607291221619, Accuracy: 1.0, Computation time: 1.077514410018921\n",
      "Step: 4476, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.0862550735473633\n",
      "Step: 4477, Loss: 0.9159249663352966, Accuracy: 1.0, Computation time: 0.9014103412628174\n",
      "Step: 4478, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.9666874408721924\n",
      "Step: 4479, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.083967685699463\n",
      "Step: 4480, Loss: 0.9158377647399902, Accuracy: 1.0, Computation time: 0.8774106502532959\n",
      "Step: 4481, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 0.8784418106079102\n",
      "Step: 4482, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 0.8201651573181152\n",
      "Step: 4483, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 0.8390283584594727\n",
      "Step: 4484, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 1.0366933345794678\n",
      "Step: 4485, Loss: 0.9160528779029846, Accuracy: 1.0, Computation time: 0.899329662322998\n",
      "Step: 4486, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.334975004196167\n",
      "Step: 4487, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 0.7924802303314209\n",
      "Step: 4488, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 0.8314676284790039\n",
      "Step: 4489, Loss: 0.9375205636024475, Accuracy: 0.96875, Computation time: 0.7827491760253906\n",
      "Step: 4490, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 1.1081678867340088\n",
      "Step: 4491, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.055708646774292\n",
      "Step: 4492, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 0.830979585647583\n",
      "Step: 4493, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 0.8292896747589111\n",
      "Step: 4494, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.8138926029205322\n",
      "Step: 4495, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 0.8634829521179199\n",
      "Step: 4496, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 0.8636338710784912\n",
      "Step: 4497, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 0.8795516490936279\n",
      "Step: 4498, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 0.8740420341491699\n",
      "Step: 4499, Loss: 0.915838360786438, Accuracy: 1.0, Computation time: 0.9489412307739258\n",
      "Step: 4500, Loss: 0.9205847382545471, Accuracy: 1.0, Computation time: 1.0010857582092285\n",
      "Step: 4501, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 1.0635268688201904\n",
      "Step: 4502, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 0.9379069805145264\n",
      "Step: 4503, Loss: 0.9161157608032227, Accuracy: 1.0, Computation time: 0.9395136833190918\n",
      "Step: 4504, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.0724380016326904\n",
      "Step: 4505, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.122779130935669\n",
      "Step: 4506, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.1053826808929443\n",
      "Step: 4507, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 0.9479708671569824\n",
      "Step: 4508, Loss: 0.9167988300323486, Accuracy: 1.0, Computation time: 0.9129774570465088\n",
      "Step: 4509, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.7202370166778564\n",
      "Step: 4510, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 0.8431801795959473\n",
      "Step: 4511, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.1066794395446777\n",
      "Step: 4512, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 0.8507916927337646\n",
      "Step: 4513, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 0.8461852073669434\n",
      "Step: 4514, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.3542015552520752\n",
      "Step: 4515, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 0.8677988052368164\n",
      "Step: 4516, Loss: 0.9163468480110168, Accuracy: 1.0, Computation time: 1.0902981758117676\n",
      "Step: 4517, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 0.8472652435302734\n",
      "Step: 4518, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 0.7662804126739502\n",
      "Step: 4519, Loss: 0.9158398509025574, Accuracy: 1.0, Computation time: 0.7809808254241943\n",
      "Step: 4520, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 0.8552427291870117\n",
      "Step: 4521, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.0121564865112305\n",
      "Step: 4522, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 0.8311095237731934\n",
      "Step: 4523, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.3401532173156738\n",
      "Step: 4524, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 0.9283030033111572\n",
      "Step: 4525, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 0.8552262783050537\n",
      "Step: 4526, Loss: 0.9375852346420288, Accuracy: 0.96875, Computation time: 0.9335813522338867\n",
      "Step: 4527, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 0.8142502307891846\n",
      "Step: 4528, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.1300816535949707\n",
      "Step: 4529, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 0.9367043972015381\n",
      "Step: 4530, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.0404739379882812\n",
      "Step: 4531, Loss: 0.9164148569107056, Accuracy: 1.0, Computation time: 0.9074416160583496\n",
      "Step: 4532, Loss: 0.9375567436218262, Accuracy: 0.96875, Computation time: 1.011841058731079\n",
      "Step: 4533, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 0.8988194465637207\n",
      "Step: 4534, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 0.9357633590698242\n",
      "Step: 4535, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.2198033332824707\n",
      "Step: 4536, Loss: 0.9330352544784546, Accuracy: 0.96875, Computation time: 0.9814493656158447\n",
      "Step: 4537, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 0.8156530857086182\n",
      "Step: 4538, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.016287088394165\n",
      "Step: 4539, Loss: 0.9190788269042969, Accuracy: 1.0, Computation time: 0.8734679222106934\n",
      "Step: 4540, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 0.9029862880706787\n",
      "Step: 4541, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 0.8309941291809082\n",
      "Step: 4542, Loss: 0.9160215258598328, Accuracy: 1.0, Computation time: 0.967498779296875\n",
      "Step: 4543, Loss: 0.9375907778739929, Accuracy: 0.96875, Computation time: 0.927454948425293\n",
      "Step: 4544, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 0.8603610992431641\n",
      "Step: 4545, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 1.8102803230285645\n",
      "Step: 4546, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 1.0609500408172607\n",
      "Step: 4547, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.9534971714019775\n",
      "Step: 4548, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 0.8409373760223389\n",
      "Step: 4549, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.056563138961792\n",
      "Step: 4550, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 0.7583940029144287\n",
      "Step: 4551, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 0.8703455924987793\n",
      "Step: 4552, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 0.7902631759643555\n",
      "Step: 4553, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.4895474910736084\n",
      "Step: 4554, Loss: 0.9160488843917847, Accuracy: 1.0, Computation time: 1.042881727218628\n",
      "Step: 4555, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 0.8121562004089355\n",
      "Step: 4556, Loss: 0.9160159230232239, Accuracy: 1.0, Computation time: 1.4479331970214844\n",
      "Step: 4557, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 0.8141534328460693\n",
      "Step: 4558, Loss: 0.9375601410865784, Accuracy: 0.96875, Computation time: 0.9910683631896973\n",
      "Step: 4559, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 0.9340853691101074\n",
      "Step: 4560, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.1365275382995605\n",
      "Step: 4561, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 0.8598489761352539\n",
      "Step: 4562, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 0.9195034503936768\n",
      "Step: 4563, Loss: 0.9158428907394409, Accuracy: 1.0, Computation time: 1.0284366607666016\n",
      "Step: 4564, Loss: 0.9159719944000244, Accuracy: 1.0, Computation time: 0.8969917297363281\n",
      "Step: 4565, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.057607889175415\n",
      "Step: 4566, Loss: 0.9375364780426025, Accuracy: 0.96875, Computation time: 0.9979543685913086\n",
      "Step: 4567, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.0272836685180664\n",
      "Step: 4568, Loss: 0.9163259267807007, Accuracy: 1.0, Computation time: 0.9746198654174805\n",
      "Step: 4569, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.009765863418579\n",
      "Step: 4570, Loss: 0.9375813007354736, Accuracy: 0.96875, Computation time: 0.7732124328613281\n",
      "Step: 4571, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 0.9749002456665039\n",
      "Step: 4572, Loss: 0.9372641444206238, Accuracy: 0.96875, Computation time: 0.9628350734710693\n",
      "Step: 4573, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 0.9537849426269531\n",
      "Step: 4574, Loss: 0.9161601662635803, Accuracy: 1.0, Computation time: 0.9948537349700928\n",
      "Step: 4575, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 0.7684187889099121\n",
      "Step: 4576, Loss: 0.9371822476387024, Accuracy: 0.96875, Computation time: 1.0567700862884521\n",
      "Step: 4577, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 0.8228740692138672\n",
      "Step: 4578, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.0504589080810547\n",
      "Step: 4579, Loss: 0.9457626938819885, Accuracy: 0.96875, Computation time: 0.9420082569122314\n",
      "Step: 4580, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 0.7731380462646484\n",
      "Step: 4581, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.0477194786071777\n",
      "Step: 4582, Loss: 0.9159324765205383, Accuracy: 1.0, Computation time: 0.8309025764465332\n",
      "Step: 4583, Loss: 0.915932834148407, Accuracy: 1.0, Computation time: 0.9427835941314697\n",
      "Step: 4584, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.0784711837768555\n",
      "Step: 4585, Loss: 0.917258083820343, Accuracy: 1.0, Computation time: 0.9303379058837891\n",
      "########################\n",
      "Test loss: 1.0712558031082153, Test Accuracy_epoch33: 0.774193525314331\n",
      "########################\n",
      "Step: 4586, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 0.9281656742095947\n",
      "Step: 4587, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.1049771308898926\n",
      "Step: 4588, Loss: 0.9159616827964783, Accuracy: 1.0, Computation time: 0.809967041015625\n",
      "Step: 4589, Loss: 0.9375522136688232, Accuracy: 0.96875, Computation time: 0.8330926895141602\n",
      "Step: 4590, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 0.806713342666626\n",
      "Step: 4591, Loss: 0.9376905560493469, Accuracy: 0.96875, Computation time: 0.9715251922607422\n",
      "Step: 4592, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 0.8502705097198486\n",
      "Step: 4593, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 0.9290530681610107\n",
      "Step: 4594, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 0.9233675003051758\n",
      "Step: 4595, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 0.7840518951416016\n",
      "Step: 4596, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 0.8371837139129639\n",
      "Step: 4597, Loss: 0.937595784664154, Accuracy: 0.96875, Computation time: 0.8794975280761719\n",
      "Step: 4598, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 2.116255283355713\n",
      "Step: 4599, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 0.7087311744689941\n",
      "Step: 4600, Loss: 0.9359893202781677, Accuracy: 0.96875, Computation time: 0.9399847984313965\n",
      "Step: 4601, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 0.7905678749084473\n",
      "Step: 4602, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 0.9754955768585205\n",
      "Step: 4603, Loss: 0.9160012006759644, Accuracy: 1.0, Computation time: 0.9333689212799072\n",
      "Step: 4604, Loss: 0.9180410504341125, Accuracy: 1.0, Computation time: 0.860173225402832\n",
      "Step: 4605, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 0.8346066474914551\n",
      "Step: 4606, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 0.8000497817993164\n",
      "Step: 4607, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 0.8610882759094238\n",
      "Step: 4608, Loss: 0.9280612468719482, Accuracy: 0.96875, Computation time: 0.9545307159423828\n",
      "Step: 4609, Loss: 0.9159754514694214, Accuracy: 1.0, Computation time: 0.9752488136291504\n",
      "Step: 4610, Loss: 0.9159649014472961, Accuracy: 1.0, Computation time: 0.8457021713256836\n",
      "Step: 4611, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 0.803779125213623\n",
      "Step: 4612, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 0.8930206298828125\n",
      "Step: 4613, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 0.9060420989990234\n",
      "Step: 4614, Loss: 0.9374576210975647, Accuracy: 0.96875, Computation time: 0.8983218669891357\n",
      "Step: 4615, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 1.1491003036499023\n",
      "Step: 4616, Loss: 0.9159098863601685, Accuracy: 1.0, Computation time: 1.2258539199829102\n",
      "Step: 4617, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 0.767939567565918\n",
      "Step: 4618, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.7979085445404053\n",
      "Step: 4619, Loss: 0.9226776361465454, Accuracy: 1.0, Computation time: 1.18654465675354\n",
      "Step: 4620, Loss: 0.9159995317459106, Accuracy: 1.0, Computation time: 0.8468306064605713\n",
      "Step: 4621, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 0.8566427230834961\n",
      "Step: 4622, Loss: 0.9166841506958008, Accuracy: 1.0, Computation time: 1.042419195175171\n",
      "Step: 4623, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 0.8146510124206543\n",
      "Step: 4624, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 0.8432893753051758\n",
      "Step: 4625, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 0.9270567893981934\n",
      "Step: 4626, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.1673951148986816\n",
      "Step: 4627, Loss: 0.9159414768218994, Accuracy: 1.0, Computation time: 0.9574451446533203\n",
      "Step: 4628, Loss: 0.9159392714500427, Accuracy: 1.0, Computation time: 0.9108724594116211\n",
      "Step: 4629, Loss: 0.9159592986106873, Accuracy: 1.0, Computation time: 0.9270534515380859\n",
      "Step: 4630, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 0.8533902168273926\n",
      "Step: 4631, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 0.9742112159729004\n",
      "Step: 4632, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.2171063423156738\n",
      "Step: 4633, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 2.4837448596954346\n",
      "Step: 4634, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 0.8630645275115967\n",
      "Step: 4635, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 0.8619101047515869\n",
      "Step: 4636, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 0.9962747097015381\n",
      "Step: 4637, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 0.9510700702667236\n",
      "Step: 4638, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 0.833765983581543\n",
      "Step: 4639, Loss: 0.9589747190475464, Accuracy: 0.9375, Computation time: 0.8406062126159668\n",
      "Step: 4640, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 1.1434032917022705\n",
      "Step: 4641, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 0.8523502349853516\n",
      "Step: 4642, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 1.1402175426483154\n",
      "Step: 4643, Loss: 0.9192567467689514, Accuracy: 1.0, Computation time: 1.3017525672912598\n",
      "Step: 4644, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 0.8702218532562256\n",
      "Step: 4645, Loss: 0.9160526990890503, Accuracy: 1.0, Computation time: 0.8835568428039551\n",
      "Step: 4646, Loss: 0.9194327592849731, Accuracy: 1.0, Computation time: 1.244922161102295\n",
      "Step: 4647, Loss: 0.9162141680717468, Accuracy: 1.0, Computation time: 1.0101501941680908\n",
      "Step: 4648, Loss: 0.9379660487174988, Accuracy: 0.96875, Computation time: 1.027496099472046\n",
      "Step: 4649, Loss: 0.9159978628158569, Accuracy: 1.0, Computation time: 0.9064655303955078\n",
      "Step: 4650, Loss: 0.9164065718650818, Accuracy: 1.0, Computation time: 0.9422206878662109\n",
      "Step: 4651, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.3521828651428223\n",
      "Step: 4652, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 0.9742605686187744\n",
      "Step: 4653, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 0.8478410243988037\n",
      "Step: 4654, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 0.8855321407318115\n",
      "Step: 4655, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 0.9529290199279785\n",
      "Step: 4656, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 0.772446870803833\n",
      "Step: 4657, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 0.8072166442871094\n",
      "Step: 4658, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 0.8326499462127686\n",
      "Step: 4659, Loss: 0.9159602522850037, Accuracy: 1.0, Computation time: 0.8558602333068848\n",
      "Step: 4660, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 0.7877531051635742\n",
      "Step: 4661, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 0.7638237476348877\n",
      "Step: 4662, Loss: 0.9235324859619141, Accuracy: 1.0, Computation time: 0.8516407012939453\n",
      "Step: 4663, Loss: 0.9159465432167053, Accuracy: 1.0, Computation time: 1.0906944274902344\n",
      "Step: 4664, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 0.7882397174835205\n",
      "Step: 4665, Loss: 0.9159716963768005, Accuracy: 1.0, Computation time: 0.9032185077667236\n",
      "Step: 4666, Loss: 0.9181639552116394, Accuracy: 1.0, Computation time: 0.9381439685821533\n",
      "Step: 4667, Loss: 0.9161494374275208, Accuracy: 1.0, Computation time: 0.8946912288665771\n",
      "Step: 4668, Loss: 0.916000247001648, Accuracy: 1.0, Computation time: 0.968195915222168\n",
      "Step: 4669, Loss: 0.9378277063369751, Accuracy: 0.96875, Computation time: 1.0654582977294922\n",
      "Step: 4670, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 0.8016493320465088\n",
      "Step: 4671, Loss: 0.915920615196228, Accuracy: 1.0, Computation time: 0.9231913089752197\n",
      "Step: 4672, Loss: 0.9159709811210632, Accuracy: 1.0, Computation time: 0.939619779586792\n",
      "Step: 4673, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 0.8584351539611816\n",
      "Step: 4674, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 0.8875641822814941\n",
      "Step: 4675, Loss: 0.9159204363822937, Accuracy: 1.0, Computation time: 0.8346517086029053\n",
      "Step: 4676, Loss: 0.93789142370224, Accuracy: 0.96875, Computation time: 0.8508377075195312\n",
      "Step: 4677, Loss: 0.9160081148147583, Accuracy: 1.0, Computation time: 0.8398253917694092\n",
      "Step: 4678, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 1.016559362411499\n",
      "Step: 4679, Loss: 0.9159338474273682, Accuracy: 1.0, Computation time: 1.1111400127410889\n",
      "Step: 4680, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 0.8391952514648438\n",
      "Step: 4681, Loss: 0.915974497795105, Accuracy: 1.0, Computation time: 1.191110372543335\n",
      "Step: 4682, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 0.8456676006317139\n",
      "Step: 4683, Loss: 0.9160424470901489, Accuracy: 1.0, Computation time: 1.0762507915496826\n",
      "Step: 4684, Loss: 0.91617751121521, Accuracy: 1.0, Computation time: 1.202986717224121\n",
      "Step: 4685, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.983576774597168\n",
      "Step: 4686, Loss: 0.9163227081298828, Accuracy: 1.0, Computation time: 1.1652164459228516\n",
      "Step: 4687, Loss: 0.937536358833313, Accuracy: 0.96875, Computation time: 0.9336073398590088\n",
      "Step: 4688, Loss: 0.9346557259559631, Accuracy: 0.96875, Computation time: 0.8419575691223145\n",
      "Step: 4689, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.0620875358581543\n",
      "Step: 4690, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 1.0311825275421143\n",
      "Step: 4691, Loss: 0.9159820675849915, Accuracy: 1.0, Computation time: 1.2201242446899414\n",
      "Step: 4692, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 0.8064873218536377\n",
      "Step: 4693, Loss: 0.9380787014961243, Accuracy: 0.96875, Computation time: 1.1190989017486572\n",
      "Step: 4694, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 0.9832329750061035\n",
      "Step: 4695, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 0.8833222389221191\n",
      "Step: 4696, Loss: 0.9159107804298401, Accuracy: 1.0, Computation time: 0.8250906467437744\n",
      "Step: 4697, Loss: 0.9162001609802246, Accuracy: 1.0, Computation time: 0.8819882869720459\n",
      "Step: 4698, Loss: 0.9160615801811218, Accuracy: 1.0, Computation time: 1.5778906345367432\n",
      "Step: 4699, Loss: 0.9159514307975769, Accuracy: 1.0, Computation time: 0.8960003852844238\n",
      "Step: 4700, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.0431747436523438\n",
      "Step: 4701, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.0409955978393555\n",
      "Step: 4702, Loss: 0.918165922164917, Accuracy: 1.0, Computation time: 0.9445245265960693\n",
      "Step: 4703, Loss: 0.9378159642219543, Accuracy: 0.96875, Computation time: 1.0010759830474854\n",
      "Step: 4704, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 0.8287913799285889\n",
      "Step: 4705, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 0.8886692523956299\n",
      "Step: 4706, Loss: 0.916607141494751, Accuracy: 1.0, Computation time: 1.1084136962890625\n",
      "Step: 4707, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 0.7417278289794922\n",
      "Step: 4708, Loss: 0.9204487800598145, Accuracy: 1.0, Computation time: 1.1665194034576416\n",
      "Step: 4709, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 0.8563787937164307\n",
      "Step: 4710, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 0.8151776790618896\n",
      "Step: 4711, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 0.7534699440002441\n",
      "Step: 4712, Loss: 0.9159609079360962, Accuracy: 1.0, Computation time: 0.885552167892456\n",
      "Step: 4713, Loss: 0.9376600980758667, Accuracy: 0.96875, Computation time: 1.2075858116149902\n",
      "Step: 4714, Loss: 0.9160861968994141, Accuracy: 1.0, Computation time: 1.5924060344696045\n",
      "Step: 4715, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 0.9284346103668213\n",
      "Step: 4716, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 0.872063398361206\n",
      "Step: 4717, Loss: 0.9180107712745667, Accuracy: 1.0, Computation time: 0.8337640762329102\n",
      "Step: 4718, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 0.791536808013916\n",
      "Step: 4719, Loss: 0.9160294532775879, Accuracy: 1.0, Computation time: 0.8852646350860596\n",
      "Step: 4720, Loss: 0.9162006974220276, Accuracy: 1.0, Computation time: 0.844649076461792\n",
      "Step: 4721, Loss: 0.9161011576652527, Accuracy: 1.0, Computation time: 0.7719388008117676\n",
      "Step: 4722, Loss: 0.9168037176132202, Accuracy: 1.0, Computation time: 0.9197866916656494\n",
      "Step: 4723, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 1.264587640762329\n",
      "Step: 4724, Loss: 0.927657961845398, Accuracy: 0.96875, Computation time: 1.0498149394989014\n",
      "########################\n",
      "Test loss: 1.0721359252929688, Test Accuracy_epoch34: 0.774193525314331\n",
      "########################\n",
      "Step: 4725, Loss: 0.9159667491912842, Accuracy: 1.0, Computation time: 1.197537899017334\n",
      "Step: 4726, Loss: 0.9209045767784119, Accuracy: 1.0, Computation time: 1.0231783390045166\n",
      "Step: 4727, Loss: 0.9378873109817505, Accuracy: 0.96875, Computation time: 1.5425999164581299\n",
      "Step: 4728, Loss: 0.915982186794281, Accuracy: 1.0, Computation time: 1.0037288665771484\n",
      "Step: 4729, Loss: 0.9165949821472168, Accuracy: 1.0, Computation time: 1.2172815799713135\n",
      "Step: 4730, Loss: 0.9159753322601318, Accuracy: 1.0, Computation time: 0.8629586696624756\n",
      "Step: 4731, Loss: 0.9161370992660522, Accuracy: 1.0, Computation time: 1.0532548427581787\n",
      "Step: 4732, Loss: 0.9159896373748779, Accuracy: 1.0, Computation time: 0.9006955623626709\n",
      "Step: 4733, Loss: 0.9376410245895386, Accuracy: 0.96875, Computation time: 0.8335099220275879\n",
      "Step: 4734, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 0.8621268272399902\n",
      "Step: 4735, Loss: 0.915936291217804, Accuracy: 1.0, Computation time: 1.1720495223999023\n",
      "Step: 4736, Loss: 0.9159589409828186, Accuracy: 1.0, Computation time: 0.9664063453674316\n",
      "Step: 4737, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 0.990551233291626\n",
      "Step: 4738, Loss: 0.9208968877792358, Accuracy: 1.0, Computation time: 1.004967212677002\n",
      "Step: 4739, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 0.921515941619873\n",
      "Step: 4740, Loss: 0.915936291217804, Accuracy: 1.0, Computation time: 0.8891904354095459\n",
      "Step: 4741, Loss: 0.9160363078117371, Accuracy: 1.0, Computation time: 0.8701736927032471\n",
      "Step: 4742, Loss: 0.9159510731697083, Accuracy: 1.0, Computation time: 0.77618408203125\n",
      "Step: 4743, Loss: 0.9160959124565125, Accuracy: 1.0, Computation time: 0.9031789302825928\n",
      "Step: 4744, Loss: 0.91600102186203, Accuracy: 1.0, Computation time: 1.0273303985595703\n",
      "Step: 4745, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 0.992872953414917\n",
      "Step: 4746, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.0453033447265625\n",
      "Step: 4747, Loss: 0.9160988926887512, Accuracy: 1.0, Computation time: 1.2393856048583984\n",
      "Step: 4748, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 0.8406531810760498\n",
      "Step: 4749, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 0.7808468341827393\n",
      "Step: 4750, Loss: 0.9377760887145996, Accuracy: 0.96875, Computation time: 1.1120784282684326\n",
      "Step: 4751, Loss: 0.9170054793357849, Accuracy: 1.0, Computation time: 1.1307392120361328\n",
      "Step: 4752, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 0.8257968425750732\n",
      "Step: 4753, Loss: 0.9160746335983276, Accuracy: 1.0, Computation time: 1.123683214187622\n",
      "Step: 4754, Loss: 0.9159799218177795, Accuracy: 1.0, Computation time: 0.9195642471313477\n",
      "Step: 4755, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 0.8457703590393066\n",
      "Step: 4756, Loss: 0.9159754514694214, Accuracy: 1.0, Computation time: 0.7546546459197998\n",
      "Step: 4757, Loss: 0.9375254511833191, Accuracy: 0.96875, Computation time: 0.8019938468933105\n",
      "Step: 4758, Loss: 0.9374979138374329, Accuracy: 0.96875, Computation time: 0.8939368724822998\n",
      "Step: 4759, Loss: 0.9369514584541321, Accuracy: 0.96875, Computation time: 0.9908828735351562\n",
      "Step: 4760, Loss: 0.937628448009491, Accuracy: 0.96875, Computation time: 0.7271559238433838\n",
      "Step: 4761, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 1.1846163272857666\n",
      "Step: 4762, Loss: 0.9160552024841309, Accuracy: 1.0, Computation time: 0.7877998352050781\n",
      "Step: 4763, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 0.9122111797332764\n",
      "Step: 4764, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 0.9709610939025879\n",
      "Step: 4765, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 0.7962884902954102\n",
      "Step: 4766, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 0.9035508632659912\n",
      "Step: 4767, Loss: 0.9376817345619202, Accuracy: 0.96875, Computation time: 0.8362030982971191\n",
      "Step: 4768, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 0.7752902507781982\n",
      "Step: 4769, Loss: 0.9466305375099182, Accuracy: 0.9375, Computation time: 1.2444086074829102\n",
      "Step: 4770, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 0.718170166015625\n",
      "Step: 4771, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 0.7491328716278076\n",
      "Step: 4772, Loss: 0.9178041219711304, Accuracy: 1.0, Computation time: 1.02461838722229\n",
      "Step: 4773, Loss: 0.9160194396972656, Accuracy: 1.0, Computation time: 1.0187160968780518\n",
      "Step: 4774, Loss: 0.9162098169326782, Accuracy: 1.0, Computation time: 1.3999407291412354\n",
      "Step: 4775, Loss: 0.9161146283149719, Accuracy: 1.0, Computation time: 1.1070449352264404\n",
      "Step: 4776, Loss: 0.9159751534461975, Accuracy: 1.0, Computation time: 0.8949940204620361\n",
      "Step: 4777, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 0.7894721031188965\n",
      "Step: 4778, Loss: 0.9158425331115723, Accuracy: 1.0, Computation time: 0.820817232131958\n",
      "Step: 4779, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 0.9200766086578369\n",
      "Step: 4780, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 0.918975830078125\n",
      "Step: 4781, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 0.8384194374084473\n",
      "Step: 4782, Loss: 0.916381299495697, Accuracy: 1.0, Computation time: 1.048940658569336\n",
      "Step: 4783, Loss: 0.9159087538719177, Accuracy: 1.0, Computation time: 0.9207637310028076\n",
      "Step: 4784, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 0.7726924419403076\n",
      "Step: 4785, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 1.080439805984497\n",
      "Step: 4786, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 1.069063425064087\n",
      "Step: 4787, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 0.9648008346557617\n",
      "Step: 4788, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 0.9081916809082031\n",
      "Step: 4789, Loss: 0.9377175569534302, Accuracy: 0.96875, Computation time: 1.6498005390167236\n",
      "Step: 4790, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 0.952946662902832\n",
      "Step: 4791, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 0.8769245147705078\n",
      "Step: 4792, Loss: 0.9170612096786499, Accuracy: 1.0, Computation time: 1.0208499431610107\n",
      "Step: 4793, Loss: 0.915993332862854, Accuracy: 1.0, Computation time: 0.88974928855896\n",
      "Step: 4794, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 0.8607299327850342\n",
      "Step: 4795, Loss: 0.9166778326034546, Accuracy: 1.0, Computation time: 1.2582604885101318\n",
      "Step: 4796, Loss: 0.9166477918624878, Accuracy: 1.0, Computation time: 1.4514245986938477\n",
      "Step: 4797, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 0.9285445213317871\n",
      "Step: 4798, Loss: 0.9159539937973022, Accuracy: 1.0, Computation time: 1.2581121921539307\n",
      "Step: 4799, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 1.0325751304626465\n",
      "Step: 4800, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 0.8488912582397461\n",
      "Step: 4801, Loss: 0.9376926422119141, Accuracy: 0.96875, Computation time: 0.793015718460083\n",
      "Step: 4802, Loss: 0.9160317182540894, Accuracy: 1.0, Computation time: 0.9956865310668945\n",
      "Step: 4803, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 1.0428240299224854\n",
      "Step: 4804, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 0.9025223255157471\n",
      "Step: 4805, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.2245986461639404\n",
      "Step: 4806, Loss: 0.9161159992218018, Accuracy: 1.0, Computation time: 1.2324244976043701\n",
      "Step: 4807, Loss: 0.9159099459648132, Accuracy: 1.0, Computation time: 0.8898954391479492\n",
      "Step: 4808, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 0.886012077331543\n",
      "Step: 4809, Loss: 0.9162532687187195, Accuracy: 1.0, Computation time: 0.8619849681854248\n",
      "Step: 4810, Loss: 0.9429069757461548, Accuracy: 0.96875, Computation time: 0.9764063358306885\n",
      "Step: 4811, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 0.8998169898986816\n",
      "Step: 4812, Loss: 0.9375433921813965, Accuracy: 0.96875, Computation time: 1.0129072666168213\n",
      "Step: 4813, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.163935899734497\n",
      "Step: 4814, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 0.7326796054840088\n",
      "Step: 4815, Loss: 0.9374518990516663, Accuracy: 0.96875, Computation time: 0.8190643787384033\n",
      "Step: 4816, Loss: 0.9169007539749146, Accuracy: 1.0, Computation time: 1.0930256843566895\n",
      "Step: 4817, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 0.9865438938140869\n",
      "Step: 4818, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 0.9954562187194824\n",
      "Step: 4819, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 0.7725238800048828\n",
      "Step: 4820, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 0.8697617053985596\n",
      "Step: 4821, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 1.081693410873413\n",
      "Step: 4822, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 0.871267557144165\n",
      "Step: 4823, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 0.7866322994232178\n",
      "Step: 4824, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 0.9241204261779785\n",
      "Step: 4825, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 0.8936831951141357\n",
      "Step: 4826, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 0.8921852111816406\n",
      "Step: 4827, Loss: 0.9374856352806091, Accuracy: 0.96875, Computation time: 0.9080665111541748\n",
      "Step: 4828, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 0.8814871311187744\n",
      "Step: 4829, Loss: 0.9375618696212769, Accuracy: 0.96875, Computation time: 1.1190214157104492\n",
      "Step: 4830, Loss: 0.9160215854644775, Accuracy: 1.0, Computation time: 0.8299829959869385\n",
      "Step: 4831, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 0.8218815326690674\n",
      "Step: 4832, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 0.9741332530975342\n",
      "Step: 4833, Loss: 0.9160971641540527, Accuracy: 1.0, Computation time: 1.1379187107086182\n",
      "Step: 4834, Loss: 0.9594411253929138, Accuracy: 0.9375, Computation time: 0.9789438247680664\n",
      "Step: 4835, Loss: 0.9159277081489563, Accuracy: 1.0, Computation time: 0.8492441177368164\n",
      "Step: 4836, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 0.7654688358306885\n",
      "Step: 4837, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 2.0059144496917725\n",
      "Step: 4838, Loss: 0.9165587425231934, Accuracy: 1.0, Computation time: 0.8010902404785156\n",
      "Step: 4839, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.1836495399475098\n",
      "Step: 4840, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 0.9353959560394287\n",
      "Step: 4841, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.301372766494751\n",
      "Step: 4842, Loss: 0.9379079937934875, Accuracy: 0.96875, Computation time: 1.0957729816436768\n",
      "Step: 4843, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 0.8872101306915283\n",
      "Step: 4844, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 0.8421413898468018\n",
      "Step: 4845, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 0.779778242111206\n",
      "Step: 4846, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 0.7285637855529785\n",
      "Step: 4847, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 0.8661665916442871\n",
      "Step: 4848, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 0.8758649826049805\n",
      "Step: 4849, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.013150691986084\n",
      "Step: 4850, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 1.1506638526916504\n",
      "Step: 4851, Loss: 0.9159094095230103, Accuracy: 1.0, Computation time: 1.0900702476501465\n",
      "Step: 4852, Loss: 0.936115026473999, Accuracy: 0.96875, Computation time: 0.8479416370391846\n",
      "Step: 4853, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 0.7346229553222656\n",
      "Step: 4854, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 0.7854030132293701\n",
      "Step: 4855, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 0.7894158363342285\n",
      "Step: 4856, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.775883674621582\n",
      "Step: 4857, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 0.9556636810302734\n",
      "Step: 4858, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 0.7656805515289307\n",
      "Step: 4859, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 0.8124570846557617\n",
      "Step: 4860, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 0.9510700702667236\n",
      "Step: 4861, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 0.7555301189422607\n",
      "Step: 4862, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 0.7806317806243896\n",
      "Step: 4863, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 0.9298553466796875\n",
      "########################\n",
      "Test loss: 1.0722235441207886, Test Accuracy_epoch35: 0.7702834606170654\n",
      "########################\n",
      "Step: 4864, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 0.7609107494354248\n",
      "Step: 4865, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.9052755832672119\n",
      "Step: 4866, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 0.8955128192901611\n",
      "Step: 4867, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 0.962327241897583\n",
      "Step: 4868, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 0.7737240791320801\n",
      "Step: 4869, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 0.6858868598937988\n",
      "Step: 4870, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.0241272449493408\n",
      "Step: 4871, Loss: 0.9376339316368103, Accuracy: 0.96875, Computation time: 0.7745699882507324\n",
      "Step: 4872, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 0.9103167057037354\n",
      "Step: 4873, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 0.8818092346191406\n",
      "Step: 4874, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 0.7488620281219482\n",
      "Step: 4875, Loss: 0.9375534653663635, Accuracy: 0.96875, Computation time: 0.8341233730316162\n",
      "Step: 4876, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 0.8530924320220947\n",
      "Step: 4877, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 0.8720622062683105\n",
      "Step: 4878, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.169384241104126\n",
      "Step: 4879, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 0.9700274467468262\n",
      "Step: 4880, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 0.9643080234527588\n",
      "Step: 4881, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 0.7475829124450684\n",
      "Step: 4882, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 0.8705723285675049\n",
      "Step: 4883, Loss: 0.9160071611404419, Accuracy: 1.0, Computation time: 0.8086614608764648\n",
      "Step: 4884, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 0.8383467197418213\n",
      "Step: 4885, Loss: 0.9192793369293213, Accuracy: 1.0, Computation time: 1.0637037754058838\n",
      "Step: 4886, Loss: 0.9158371090888977, Accuracy: 1.0, Computation time: 0.8459751605987549\n",
      "Step: 4887, Loss: 0.9375330805778503, Accuracy: 0.96875, Computation time: 0.8684818744659424\n",
      "Step: 4888, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 0.7564513683319092\n",
      "Step: 4889, Loss: 0.9160089492797852, Accuracy: 1.0, Computation time: 0.8788783550262451\n",
      "Step: 4890, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 0.932610034942627\n",
      "Step: 4891, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 0.8200063705444336\n",
      "Step: 4892, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 0.7732093334197998\n",
      "Step: 4893, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 0.8149442672729492\n",
      "Step: 4894, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 0.7743823528289795\n",
      "Step: 4895, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 0.7269296646118164\n",
      "Step: 4896, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.1473331451416016\n",
      "Step: 4897, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 0.6939771175384521\n",
      "Step: 4898, Loss: 0.9183050990104675, Accuracy: 1.0, Computation time: 1.1508636474609375\n",
      "Step: 4899, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 0.7975325584411621\n",
      "Step: 4900, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 0.7769391536712646\n",
      "Step: 4901, Loss: 0.9160828590393066, Accuracy: 1.0, Computation time: 0.8396952152252197\n",
      "Step: 4902, Loss: 0.915963888168335, Accuracy: 1.0, Computation time: 0.8713855743408203\n",
      "Step: 4903, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 0.7107851505279541\n",
      "Step: 4904, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 0.6994047164916992\n",
      "Step: 4905, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 0.9132523536682129\n",
      "Step: 4906, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 0.8593649864196777\n",
      "Step: 4907, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 0.7146942615509033\n",
      "Step: 4908, Loss: 0.9162426590919495, Accuracy: 1.0, Computation time: 0.9364383220672607\n",
      "Step: 4909, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 0.7940256595611572\n",
      "Step: 4910, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 0.749502420425415\n",
      "Step: 4911, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 0.7732455730438232\n",
      "Step: 4912, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 0.9147765636444092\n",
      "Step: 4913, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 0.7487821578979492\n",
      "Step: 4914, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 0.7713558673858643\n",
      "Step: 4915, Loss: 0.9351121783256531, Accuracy: 0.96875, Computation time: 0.9948413372039795\n",
      "Step: 4916, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 0.7438733577728271\n",
      "Step: 4917, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 0.8099625110626221\n",
      "Step: 4918, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.8579607009887695\n",
      "Step: 4919, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 0.8744714260101318\n",
      "Step: 4920, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 0.8005127906799316\n",
      "Step: 4921, Loss: 0.9166520237922668, Accuracy: 1.0, Computation time: 0.733288049697876\n",
      "Step: 4922, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 0.9420042037963867\n",
      "Step: 4923, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 0.7590906620025635\n",
      "Step: 4924, Loss: 0.9167556166648865, Accuracy: 1.0, Computation time: 0.8520219326019287\n",
      "Step: 4925, Loss: 0.921514093875885, Accuracy: 1.0, Computation time: 1.1075360774993896\n",
      "Step: 4926, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 0.8320257663726807\n",
      "Step: 4927, Loss: 0.9160107970237732, Accuracy: 1.0, Computation time: 0.9413778781890869\n",
      "Step: 4928, Loss: 0.9339021444320679, Accuracy: 0.96875, Computation time: 1.7754523754119873\n",
      "Step: 4929, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 0.9540500640869141\n",
      "Step: 4930, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.1720330715179443\n",
      "Step: 4931, Loss: 0.9160630106925964, Accuracy: 1.0, Computation time: 1.150264024734497\n",
      "Step: 4932, Loss: 0.9161417484283447, Accuracy: 1.0, Computation time: 1.0848870277404785\n",
      "Step: 4933, Loss: 0.9161871671676636, Accuracy: 1.0, Computation time: 0.8973116874694824\n",
      "Step: 4934, Loss: 0.9161629676818848, Accuracy: 1.0, Computation time: 0.9861536026000977\n",
      "Step: 4935, Loss: 0.9376998543739319, Accuracy: 0.96875, Computation time: 0.8658881187438965\n",
      "Step: 4936, Loss: 0.9160590767860413, Accuracy: 1.0, Computation time: 0.8874080181121826\n",
      "Step: 4937, Loss: 0.915999710559845, Accuracy: 1.0, Computation time: 1.2360990047454834\n",
      "Step: 4938, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.1065614223480225\n",
      "Step: 4939, Loss: 0.9230757355690002, Accuracy: 1.0, Computation time: 1.7982978820800781\n",
      "Step: 4940, Loss: 0.916063129901886, Accuracy: 1.0, Computation time: 1.0739212036132812\n",
      "Step: 4941, Loss: 0.9160598516464233, Accuracy: 1.0, Computation time: 0.8842318058013916\n",
      "Step: 4942, Loss: 0.9160740971565247, Accuracy: 1.0, Computation time: 1.0851538181304932\n",
      "Step: 4943, Loss: 0.916000485420227, Accuracy: 1.0, Computation time: 1.0186691284179688\n",
      "Step: 4944, Loss: 0.958716094493866, Accuracy: 0.9375, Computation time: 1.1564855575561523\n",
      "Step: 4945, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 0.9679248332977295\n",
      "Step: 4946, Loss: 0.9374493956565857, Accuracy: 0.96875, Computation time: 1.332261562347412\n",
      "Step: 4947, Loss: 0.9159346222877502, Accuracy: 1.0, Computation time: 1.0596938133239746\n",
      "Step: 4948, Loss: 0.9160702228546143, Accuracy: 1.0, Computation time: 1.126772165298462\n",
      "Step: 4949, Loss: 0.9159769415855408, Accuracy: 1.0, Computation time: 1.012568712234497\n",
      "Step: 4950, Loss: 0.91595858335495, Accuracy: 1.0, Computation time: 0.9603860378265381\n",
      "Step: 4951, Loss: 0.9159930348396301, Accuracy: 1.0, Computation time: 1.127274990081787\n",
      "Step: 4952, Loss: 0.9159212708473206, Accuracy: 1.0, Computation time: 1.6392066478729248\n",
      "Step: 4953, Loss: 0.9375696182250977, Accuracy: 0.96875, Computation time: 1.0078198909759521\n",
      "Step: 4954, Loss: 0.9312484264373779, Accuracy: 0.96875, Computation time: 1.462963342666626\n",
      "Step: 4955, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 1.0735528469085693\n",
      "Step: 4956, Loss: 0.9160747528076172, Accuracy: 1.0, Computation time: 0.8994152545928955\n",
      "Step: 4957, Loss: 0.9161614775657654, Accuracy: 1.0, Computation time: 0.9531965255737305\n",
      "Step: 4958, Loss: 0.9162245988845825, Accuracy: 1.0, Computation time: 1.0195107460021973\n",
      "Step: 4959, Loss: 0.916114330291748, Accuracy: 1.0, Computation time: 1.047055721282959\n",
      "Step: 4960, Loss: 0.9159694314002991, Accuracy: 1.0, Computation time: 1.0227153301239014\n",
      "Step: 4961, Loss: 0.9161654114723206, Accuracy: 1.0, Computation time: 0.9353673458099365\n",
      "Step: 4962, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 0.9456272125244141\n",
      "Step: 4963, Loss: 0.9173097014427185, Accuracy: 1.0, Computation time: 1.0560345649719238\n",
      "Step: 4964, Loss: 0.915963888168335, Accuracy: 1.0, Computation time: 1.034954309463501\n",
      "Step: 4965, Loss: 0.9164766073226929, Accuracy: 1.0, Computation time: 0.947354793548584\n",
      "Step: 4966, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 0.906273603439331\n",
      "Step: 4967, Loss: 0.9159746170043945, Accuracy: 1.0, Computation time: 0.9126396179199219\n",
      "Step: 4968, Loss: 0.9160318374633789, Accuracy: 1.0, Computation time: 1.4213130474090576\n",
      "Step: 4969, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 0.9241955280303955\n",
      "Step: 4970, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 0.9214041233062744\n",
      "Step: 4971, Loss: 0.9380087852478027, Accuracy: 0.96875, Computation time: 1.0240697860717773\n",
      "Step: 4972, Loss: 0.9159504175186157, Accuracy: 1.0, Computation time: 0.9620828628540039\n",
      "Step: 4973, Loss: 0.9383381605148315, Accuracy: 0.96875, Computation time: 1.495356798171997\n",
      "Step: 4974, Loss: 0.9159591794013977, Accuracy: 1.0, Computation time: 0.9421975612640381\n",
      "Step: 4975, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 0.8624916076660156\n",
      "Step: 4976, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 0.930896520614624\n",
      "Step: 4977, Loss: 0.9171525239944458, Accuracy: 1.0, Computation time: 1.4623138904571533\n",
      "Step: 4978, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 0.9842960834503174\n",
      "Step: 4979, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.012406349182129\n",
      "Step: 4980, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 0.9698200225830078\n",
      "Step: 4981, Loss: 0.9159172773361206, Accuracy: 1.0, Computation time: 0.8914225101470947\n",
      "Step: 4982, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 0.8381900787353516\n",
      "Step: 4983, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 0.9826428890228271\n",
      "Step: 4984, Loss: 0.9159585237503052, Accuracy: 1.0, Computation time: 0.995110034942627\n",
      "Step: 4985, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.0152196884155273\n",
      "Step: 4986, Loss: 0.9374387264251709, Accuracy: 0.96875, Computation time: 0.9852492809295654\n",
      "Step: 4987, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 0.908372163772583\n",
      "Step: 4988, Loss: 0.9374712705612183, Accuracy: 0.96875, Computation time: 0.953244686126709\n",
      "Step: 4989, Loss: 0.9376523494720459, Accuracy: 0.96875, Computation time: 1.0010647773742676\n",
      "Step: 4990, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.2747204303741455\n",
      "Step: 4991, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.176032543182373\n",
      "Step: 4992, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 0.9419894218444824\n",
      "Step: 4993, Loss: 0.9161032438278198, Accuracy: 1.0, Computation time: 0.9411802291870117\n",
      "Step: 4994, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.1163759231567383\n",
      "Step: 4995, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 0.8035640716552734\n",
      "Step: 4996, Loss: 0.9159541726112366, Accuracy: 1.0, Computation time: 0.9680602550506592\n",
      "Step: 4997, Loss: 0.9374814629554749, Accuracy: 0.96875, Computation time: 0.8302700519561768\n",
      "Step: 4998, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 1.3557753562927246\n",
      "Step: 4999, Loss: 0.9174638390541077, Accuracy: 1.0, Computation time: 0.9961347579956055\n",
      "Step: 5000, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.0412635803222656\n",
      "Step: 5001, Loss: 0.9236695766448975, Accuracy: 1.0, Computation time: 1.1853814125061035\n",
      "Step: 5002, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 0.9028446674346924\n",
      "########################\n",
      "Test loss: 1.0694211721420288, Test Accuracy_epoch36: 0.7761485576629639\n",
      "########################\n",
      "Step: 5003, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 0.8532233238220215\n",
      "Step: 5004, Loss: 0.9159560799598694, Accuracy: 1.0, Computation time: 0.9487595558166504\n",
      "Step: 5005, Loss: 0.9169186353683472, Accuracy: 1.0, Computation time: 0.8777170181274414\n",
      "Step: 5006, Loss: 0.9159731864929199, Accuracy: 1.0, Computation time: 0.8436727523803711\n",
      "Step: 5007, Loss: 0.9159957766532898, Accuracy: 1.0, Computation time: 0.8796064853668213\n",
      "Step: 5008, Loss: 0.938904345035553, Accuracy: 0.96875, Computation time: 2.0846478939056396\n",
      "Step: 5009, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 0.9119069576263428\n",
      "Step: 5010, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 0.8689010143280029\n",
      "Step: 5011, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 0.9356930255889893\n",
      "Step: 5012, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 0.8506584167480469\n",
      "Step: 5013, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 0.8034865856170654\n",
      "Step: 5014, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 0.9597887992858887\n",
      "Step: 5015, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 0.8575363159179688\n",
      "Step: 5016, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 0.9340612888336182\n",
      "Step: 5017, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 1.0273337364196777\n",
      "Step: 5018, Loss: 0.9375714659690857, Accuracy: 0.96875, Computation time: 1.039687156677246\n",
      "Step: 5019, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 0.8741810321807861\n",
      "Step: 5020, Loss: 0.9169228076934814, Accuracy: 1.0, Computation time: 1.0469982624053955\n",
      "Step: 5021, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 0.7527849674224854\n",
      "Step: 5022, Loss: 0.9161452651023865, Accuracy: 1.0, Computation time: 0.8455891609191895\n",
      "Step: 5023, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 0.9242265224456787\n",
      "Step: 5024, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 1.000636339187622\n",
      "Step: 5025, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 0.8801946640014648\n",
      "Step: 5026, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 0.8222148418426514\n",
      "Step: 5027, Loss: 0.9375497102737427, Accuracy: 0.96875, Computation time: 1.1703436374664307\n",
      "Step: 5028, Loss: 0.9160031080245972, Accuracy: 1.0, Computation time: 1.5247347354888916\n",
      "Step: 5029, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 0.9828801155090332\n",
      "Step: 5030, Loss: 0.9158995151519775, Accuracy: 1.0, Computation time: 0.9500455856323242\n",
      "Step: 5031, Loss: 0.9158541560173035, Accuracy: 1.0, Computation time: 0.8303663730621338\n",
      "Step: 5032, Loss: 0.9376537799835205, Accuracy: 0.96875, Computation time: 0.8730292320251465\n",
      "Step: 5033, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.0767853260040283\n",
      "Step: 5034, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.066511869430542\n",
      "Step: 5035, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 0.9314203262329102\n",
      "Step: 5036, Loss: 0.9166080355644226, Accuracy: 1.0, Computation time: 1.274583101272583\n",
      "Step: 5037, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 0.9119462966918945\n",
      "Step: 5038, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.9147377014160156\n",
      "Step: 5039, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 0.907891035079956\n",
      "Step: 5040, Loss: 0.9164729714393616, Accuracy: 1.0, Computation time: 0.9432094097137451\n",
      "Step: 5041, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.167766809463501\n",
      "Step: 5042, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 0.9680323600769043\n",
      "Step: 5043, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.1735880374908447\n",
      "Step: 5044, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 0.9660434722900391\n",
      "Step: 5045, Loss: 0.9171527028083801, Accuracy: 1.0, Computation time: 0.9607491493225098\n",
      "Step: 5046, Loss: 0.9159401655197144, Accuracy: 1.0, Computation time: 1.2461271286010742\n",
      "Step: 5047, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 0.9471142292022705\n",
      "Step: 5048, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 0.8205814361572266\n",
      "Step: 5049, Loss: 0.9374434351921082, Accuracy: 0.96875, Computation time: 1.1380987167358398\n",
      "Step: 5050, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.1361327171325684\n",
      "Step: 5051, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.1110718250274658\n",
      "Step: 5052, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 1.6596693992614746\n",
      "Step: 5053, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 0.8562719821929932\n",
      "Step: 5054, Loss: 0.9161752462387085, Accuracy: 1.0, Computation time: 1.3875503540039062\n",
      "Step: 5055, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 0.8581483364105225\n",
      "Step: 5056, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 0.9156589508056641\n",
      "Step: 5057, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 0.8852927684783936\n",
      "Step: 5058, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 0.9239575862884521\n",
      "Step: 5059, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 0.9675183296203613\n",
      "Step: 5060, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.1395297050476074\n",
      "Step: 5061, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 0.8147287368774414\n",
      "Step: 5062, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 0.9325814247131348\n",
      "Step: 5063, Loss: 0.9165775775909424, Accuracy: 1.0, Computation time: 0.9530856609344482\n",
      "Step: 5064, Loss: 0.937537670135498, Accuracy: 0.96875, Computation time: 0.8944282531738281\n",
      "Step: 5065, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 1.0116491317749023\n",
      "Step: 5066, Loss: 0.9375057220458984, Accuracy: 0.96875, Computation time: 1.2341666221618652\n",
      "Step: 5067, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 0.994718074798584\n",
      "Step: 5068, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 0.9421794414520264\n",
      "Step: 5069, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 0.9012773036956787\n",
      "Step: 5070, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 0.9296994209289551\n",
      "Step: 5071, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 0.9609403610229492\n",
      "Step: 5072, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.0658743381500244\n",
      "Step: 5073, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 0.8526351451873779\n",
      "Step: 5074, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 0.9214067459106445\n",
      "Step: 5075, Loss: 0.9264521598815918, Accuracy: 0.96875, Computation time: 1.0851130485534668\n",
      "Step: 5076, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 0.9697632789611816\n",
      "Step: 5077, Loss: 0.9160537719726562, Accuracy: 1.0, Computation time: 0.9010558128356934\n",
      "Step: 5078, Loss: 0.9160352945327759, Accuracy: 1.0, Computation time: 1.3964383602142334\n",
      "Step: 5079, Loss: 0.916943371295929, Accuracy: 1.0, Computation time: 0.9619741439819336\n",
      "Step: 5080, Loss: 0.9160299897193909, Accuracy: 1.0, Computation time: 0.8264870643615723\n",
      "Step: 5081, Loss: 0.9159606099128723, Accuracy: 1.0, Computation time: 0.8675861358642578\n",
      "Step: 5082, Loss: 0.9159573912620544, Accuracy: 1.0, Computation time: 0.9939546585083008\n",
      "Step: 5083, Loss: 0.9381133913993835, Accuracy: 0.96875, Computation time: 0.8735840320587158\n",
      "Step: 5084, Loss: 0.9163179993629456, Accuracy: 1.0, Computation time: 1.216843843460083\n",
      "Step: 5085, Loss: 0.9190570712089539, Accuracy: 1.0, Computation time: 0.9901037216186523\n",
      "Step: 5086, Loss: 0.9159950017929077, Accuracy: 1.0, Computation time: 1.3663380146026611\n",
      "Step: 5087, Loss: 0.9159939289093018, Accuracy: 1.0, Computation time: 0.9502418041229248\n",
      "Step: 5088, Loss: 0.9334163665771484, Accuracy: 0.96875, Computation time: 1.1243562698364258\n",
      "Step: 5089, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 0.928250789642334\n",
      "Step: 5090, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 0.8918116092681885\n",
      "Step: 5091, Loss: 0.9376638531684875, Accuracy: 0.96875, Computation time: 1.1636760234832764\n",
      "Step: 5092, Loss: 0.9159919023513794, Accuracy: 1.0, Computation time: 0.9312608242034912\n",
      "Step: 5093, Loss: 0.9169950485229492, Accuracy: 1.0, Computation time: 0.9760713577270508\n",
      "Step: 5094, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.0058114528656006\n",
      "Step: 5095, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 0.9008803367614746\n",
      "Step: 5096, Loss: 0.9159090518951416, Accuracy: 1.0, Computation time: 2.0049703121185303\n",
      "Step: 5097, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 0.9911634922027588\n",
      "Step: 5098, Loss: 0.9591347575187683, Accuracy: 0.9375, Computation time: 1.0431480407714844\n",
      "Step: 5099, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.0261716842651367\n",
      "Step: 5100, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 0.9880561828613281\n",
      "Step: 5101, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 0.9396007061004639\n",
      "Step: 5102, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 0.8696169853210449\n",
      "Step: 5103, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 0.8842039108276367\n",
      "Step: 5104, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 0.8556199073791504\n",
      "Step: 5105, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 1.028090476989746\n",
      "Step: 5106, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 0.900503396987915\n",
      "Step: 5107, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 0.9500594139099121\n",
      "Step: 5108, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.1760005950927734\n",
      "Step: 5109, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.9342448711395264\n",
      "Step: 5110, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 0.976355791091919\n",
      "Step: 5111, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 0.8529479503631592\n",
      "Step: 5112, Loss: 0.9374586343765259, Accuracy: 0.96875, Computation time: 0.8393652439117432\n",
      "Step: 5113, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.3092787265777588\n",
      "Step: 5114, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.0556809902191162\n",
      "Step: 5115, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 0.9251484870910645\n",
      "Step: 5116, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.1088647842407227\n",
      "Step: 5117, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 1.9099304676055908\n",
      "Step: 5118, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 0.8777799606323242\n",
      "Step: 5119, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.9609525203704834\n",
      "Step: 5120, Loss: 0.9158352613449097, Accuracy: 1.0, Computation time: 1.0731620788574219\n",
      "Step: 5121, Loss: 0.9158370494842529, Accuracy: 1.0, Computation time: 0.9568009376525879\n",
      "Step: 5122, Loss: 0.9158727526664734, Accuracy: 1.0, Computation time: 0.88863205909729\n",
      "Step: 5123, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 0.8743939399719238\n",
      "Step: 5124, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 0.8684744834899902\n",
      "Step: 5125, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.0214285850524902\n",
      "Step: 5126, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 0.8636534214019775\n",
      "Step: 5127, Loss: 0.9158322215080261, Accuracy: 1.0, Computation time: 0.8574340343475342\n",
      "Step: 5128, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 1.0902371406555176\n",
      "Step: 5129, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 0.9154901504516602\n",
      "Step: 5130, Loss: 0.9158415794372559, Accuracy: 1.0, Computation time: 0.9886207580566406\n",
      "Step: 5131, Loss: 0.9209601283073425, Accuracy: 1.0, Computation time: 1.129981279373169\n",
      "Step: 5132, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 0.913909912109375\n",
      "Step: 5133, Loss: 0.9375423789024353, Accuracy: 0.96875, Computation time: 1.0360746383666992\n",
      "Step: 5134, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.2603557109832764\n",
      "Step: 5135, Loss: 0.9159470200538635, Accuracy: 1.0, Computation time: 0.9350230693817139\n",
      "Step: 5136, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.0366895198822021\n",
      "Step: 5137, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 0.9793636798858643\n",
      "Step: 5138, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.0337855815887451\n",
      "Step: 5139, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 0.9098780155181885\n",
      "Step: 5140, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 0.9588332176208496\n",
      "Step: 5141, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 0.9174299240112305\n",
      "########################\n",
      "Test loss: 1.0695637464523315, Test Accuracy_epoch37: 0.7751710414886475\n",
      "########################\n",
      "Step: 5142, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 0.9354314804077148\n",
      "Step: 5143, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.0128908157348633\n",
      "Step: 5144, Loss: 0.9158390164375305, Accuracy: 1.0, Computation time: 1.0375146865844727\n",
      "Step: 5145, Loss: 0.9158384799957275, Accuracy: 1.0, Computation time: 1.009455919265747\n",
      "Step: 5146, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.085432767868042\n",
      "Step: 5147, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.1875174045562744\n",
      "Step: 5148, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 1.0629551410675049\n",
      "Step: 5149, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.1053261756896973\n",
      "Step: 5150, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.0027577877044678\n",
      "Step: 5151, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 1.0613470077514648\n",
      "Step: 5152, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 0.9933679103851318\n",
      "Step: 5153, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 0.9457886219024658\n",
      "Step: 5154, Loss: 0.9375311136245728, Accuracy: 0.96875, Computation time: 0.9838216304779053\n",
      "Step: 5155, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.0098538398742676\n",
      "Step: 5156, Loss: 0.9187036156654358, Accuracy: 1.0, Computation time: 1.1461129188537598\n",
      "Step: 5157, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 1.078690528869629\n",
      "Step: 5158, Loss: 0.9161913990974426, Accuracy: 1.0, Computation time: 1.0903692245483398\n",
      "Step: 5159, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.1297454833984375\n",
      "Step: 5160, Loss: 0.9374948143959045, Accuracy: 0.96875, Computation time: 1.049424409866333\n",
      "Step: 5161, Loss: 0.9181689620018005, Accuracy: 1.0, Computation time: 1.1671745777130127\n",
      "Step: 5162, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.0514237880706787\n",
      "Step: 5163, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 0.9545660018920898\n",
      "Step: 5164, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 0.943580150604248\n",
      "Step: 5165, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 0.8873591423034668\n",
      "Step: 5166, Loss: 0.9375227093696594, Accuracy: 0.96875, Computation time: 1.106961965560913\n",
      "Step: 5167, Loss: 0.9166908264160156, Accuracy: 1.0, Computation time: 1.2117998600006104\n",
      "Step: 5168, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 0.8894081115722656\n",
      "Step: 5169, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 0.9470412731170654\n",
      "Step: 5170, Loss: 0.937688946723938, Accuracy: 0.96875, Computation time: 0.9404628276824951\n",
      "Step: 5171, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.0795655250549316\n",
      "Step: 5172, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.435892105102539\n",
      "Step: 5173, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.070392370223999\n",
      "Step: 5174, Loss: 0.9159141778945923, Accuracy: 1.0, Computation time: 0.9209449291229248\n",
      "Step: 5175, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 0.9660699367523193\n",
      "Step: 5176, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 0.8286821842193604\n",
      "Step: 5177, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 0.9885513782501221\n",
      "Step: 5178, Loss: 0.9375375509262085, Accuracy: 0.96875, Computation time: 1.0205633640289307\n",
      "Step: 5179, Loss: 0.9167861938476562, Accuracy: 1.0, Computation time: 1.437894344329834\n",
      "Step: 5180, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 0.7695703506469727\n",
      "Step: 5181, Loss: 0.9375555515289307, Accuracy: 0.96875, Computation time: 0.9480383396148682\n",
      "Step: 5182, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 0.7831766605377197\n",
      "Step: 5183, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.0402579307556152\n",
      "Step: 5184, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.034775733947754\n",
      "Step: 5185, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 0.8823451995849609\n",
      "Step: 5186, Loss: 0.9162446856498718, Accuracy: 1.0, Computation time: 1.122460126876831\n",
      "Step: 5187, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.0727593898773193\n",
      "Step: 5188, Loss: 0.9231052994728088, Accuracy: 1.0, Computation time: 1.2142648696899414\n",
      "Step: 5189, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.2104735374450684\n",
      "Step: 5190, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 0.9723865985870361\n",
      "Step: 5191, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.0136454105377197\n",
      "Step: 5192, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 1.0429537296295166\n",
      "Step: 5193, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 0.914330244064331\n",
      "Step: 5194, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.0786488056182861\n",
      "Step: 5195, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 0.9975135326385498\n",
      "Step: 5196, Loss: 0.9163921475410461, Accuracy: 1.0, Computation time: 0.9874594211578369\n",
      "Step: 5197, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.0477895736694336\n",
      "Step: 5198, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.0094120502471924\n",
      "Step: 5199, Loss: 0.9159358739852905, Accuracy: 1.0, Computation time: 0.9419596195220947\n",
      "Step: 5200, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 0.8496901988983154\n",
      "Step: 5201, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 0.9197812080383301\n",
      "Step: 5202, Loss: 0.9168370366096497, Accuracy: 1.0, Computation time: 0.9052505493164062\n",
      "Step: 5203, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 0.8555517196655273\n",
      "Step: 5204, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 0.8941516876220703\n",
      "Step: 5205, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.1183431148529053\n",
      "Step: 5206, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 0.8542547225952148\n",
      "Step: 5207, Loss: 0.9377700090408325, Accuracy: 0.96875, Computation time: 1.0257244110107422\n",
      "Step: 5208, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 0.8957328796386719\n",
      "Step: 5209, Loss: 0.9377069473266602, Accuracy: 0.96875, Computation time: 0.9141778945922852\n",
      "Step: 5210, Loss: 0.9167965650558472, Accuracy: 1.0, Computation time: 1.1409423351287842\n",
      "Step: 5211, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.2335901260375977\n",
      "Step: 5212, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 0.849492073059082\n",
      "Step: 5213, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 0.782503604888916\n",
      "Step: 5214, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 0.855884313583374\n",
      "Step: 5215, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 0.9158399105072021\n",
      "Step: 5216, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.0529043674468994\n",
      "Step: 5217, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 0.9690265655517578\n",
      "Step: 5218, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 0.8312983512878418\n",
      "Step: 5219, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 0.8211278915405273\n",
      "Step: 5220, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.8853917121887207\n",
      "Step: 5221, Loss: 0.9172268509864807, Accuracy: 1.0, Computation time: 1.0576889514923096\n",
      "Step: 5222, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 0.910865306854248\n",
      "Step: 5223, Loss: 0.9158521294593811, Accuracy: 1.0, Computation time: 0.8161170482635498\n",
      "Step: 5224, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 0.901564359664917\n",
      "Step: 5225, Loss: 0.9375742077827454, Accuracy: 0.96875, Computation time: 0.9633364677429199\n",
      "Step: 5226, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 0.8346168994903564\n",
      "Step: 5227, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.03067946434021\n",
      "Step: 5228, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 0.9050390720367432\n",
      "Step: 5229, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 0.9683628082275391\n",
      "Step: 5230, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 0.8420939445495605\n",
      "Step: 5231, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 1.072394609451294\n",
      "Step: 5232, Loss: 0.9292356967926025, Accuracy: 0.96875, Computation time: 1.000701904296875\n",
      "Step: 5233, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.0066189765930176\n",
      "Step: 5234, Loss: 0.9161586761474609, Accuracy: 1.0, Computation time: 0.9869468212127686\n",
      "Step: 5235, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 0.8527810573577881\n",
      "Step: 5236, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 0.9203262329101562\n",
      "Step: 5237, Loss: 0.915908694267273, Accuracy: 1.0, Computation time: 0.9129841327667236\n",
      "Step: 5238, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 0.8901088237762451\n",
      "Step: 5239, Loss: 0.9377006888389587, Accuracy: 0.96875, Computation time: 0.9630310535430908\n",
      "Step: 5240, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 0.9161596298217773\n",
      "Step: 5241, Loss: 0.937570333480835, Accuracy: 0.96875, Computation time: 0.9198019504547119\n",
      "Step: 5242, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.0569477081298828\n",
      "Step: 5243, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 0.8274102210998535\n",
      "Step: 5244, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 0.8130381107330322\n",
      "Step: 5245, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 0.7826848030090332\n",
      "Step: 5246, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 0.7964534759521484\n",
      "Step: 5247, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.0149757862091064\n",
      "Step: 5248, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 0.8323144912719727\n",
      "Step: 5249, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 0.9508993625640869\n",
      "Step: 5250, Loss: 0.9167909026145935, Accuracy: 1.0, Computation time: 1.5037741661071777\n",
      "Step: 5251, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.0649762153625488\n",
      "Step: 5252, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 0.8558058738708496\n",
      "Step: 5253, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 0.9136533737182617\n",
      "Step: 5254, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.1927378177642822\n",
      "Step: 5255, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.9082860946655273\n",
      "Step: 5256, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.015974760055542\n",
      "Step: 5257, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 0.9155669212341309\n",
      "Step: 5258, Loss: 0.9181133508682251, Accuracy: 1.0, Computation time: 0.8679742813110352\n",
      "Step: 5259, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.9871068000793457\n",
      "Step: 5260, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 0.8481926918029785\n",
      "Step: 5261, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 0.9780356884002686\n",
      "Step: 5262, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.8353173732757568\n",
      "Step: 5263, Loss: 0.916445791721344, Accuracy: 1.0, Computation time: 0.8276472091674805\n",
      "Step: 5264, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.3377718925476074\n",
      "Step: 5265, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.0627892017364502\n",
      "Step: 5266, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 0.8267395496368408\n",
      "Step: 5267, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 0.810077428817749\n",
      "Step: 5268, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.3492345809936523\n",
      "Step: 5269, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 0.7915058135986328\n",
      "Step: 5270, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 0.935814619064331\n",
      "Step: 5271, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 0.9970283508300781\n",
      "Step: 5272, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 0.843332052230835\n",
      "Step: 5273, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 0.8590257167816162\n",
      "Step: 5274, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 0.873204231262207\n",
      "Step: 5275, Loss: 0.9374701976776123, Accuracy: 0.96875, Computation time: 0.9215500354766846\n",
      "Step: 5276, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 0.9406144618988037\n",
      "Step: 5277, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 0.8589155673980713\n",
      "Step: 5278, Loss: 0.9172565937042236, Accuracy: 1.0, Computation time: 0.8705534934997559\n",
      "Step: 5279, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 0.9416646957397461\n",
      "Step: 5280, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 0.8675498962402344\n",
      "########################\n",
      "Test loss: 1.0695053339004517, Test Accuracy_epoch38: 0.774193525314331\n",
      "########################\n",
      "Step: 5281, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 0.9202768802642822\n",
      "Step: 5282, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 0.9694771766662598\n",
      "Step: 5283, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 0.9078848361968994\n",
      "Step: 5284, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 0.8996932506561279\n",
      "Step: 5285, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.8388502597808838\n",
      "Step: 5286, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 0.9357070922851562\n",
      "Step: 5287, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 0.9728357791900635\n",
      "Step: 5288, Loss: 0.937560498714447, Accuracy: 0.96875, Computation time: 0.7922253608703613\n",
      "Step: 5289, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.9685342311859131\n",
      "Step: 5290, Loss: 0.937518298625946, Accuracy: 0.96875, Computation time: 0.911754846572876\n",
      "Step: 5291, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 0.8877260684967041\n",
      "Step: 5292, Loss: 0.9158385396003723, Accuracy: 1.0, Computation time: 0.824951171875\n",
      "Step: 5293, Loss: 0.9307685494422913, Accuracy: 0.96875, Computation time: 0.8999826908111572\n",
      "Step: 5294, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 0.8193047046661377\n",
      "Step: 5295, Loss: 0.9377071261405945, Accuracy: 0.96875, Computation time: 0.8841989040374756\n",
      "Step: 5296, Loss: 0.9160712361335754, Accuracy: 1.0, Computation time: 0.8980071544647217\n",
      "Step: 5297, Loss: 0.9161675572395325, Accuracy: 1.0, Computation time: 0.8916223049163818\n",
      "Step: 5298, Loss: 0.9162554740905762, Accuracy: 1.0, Computation time: 0.812281608581543\n",
      "Step: 5299, Loss: 0.9159485101699829, Accuracy: 1.0, Computation time: 0.944232702255249\n",
      "Step: 5300, Loss: 0.9172158241271973, Accuracy: 1.0, Computation time: 1.2366583347320557\n",
      "Step: 5301, Loss: 0.9280672073364258, Accuracy: 0.96875, Computation time: 0.7637643814086914\n",
      "Step: 5302, Loss: 0.9169694185256958, Accuracy: 1.0, Computation time: 0.8067760467529297\n",
      "Step: 5303, Loss: 0.9168130159378052, Accuracy: 1.0, Computation time: 0.8304247856140137\n",
      "Step: 5304, Loss: 0.9162333607673645, Accuracy: 1.0, Computation time: 0.7527182102203369\n",
      "Step: 5305, Loss: 0.92197585105896, Accuracy: 1.0, Computation time: 0.9872660636901855\n",
      "Step: 5306, Loss: 0.9159442782402039, Accuracy: 1.0, Computation time: 0.9276478290557861\n",
      "Step: 5307, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 0.8699636459350586\n",
      "Step: 5308, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 0.7816083431243896\n",
      "Step: 5309, Loss: 0.9378896951675415, Accuracy: 0.96875, Computation time: 2.228086233139038\n",
      "Step: 5310, Loss: 0.9331914186477661, Accuracy: 0.96875, Computation time: 0.8458566665649414\n",
      "Step: 5311, Loss: 0.9376903772354126, Accuracy: 0.96875, Computation time: 0.7980327606201172\n",
      "Step: 5312, Loss: 0.9290624856948853, Accuracy: 0.96875, Computation time: 1.0807101726531982\n",
      "Step: 5313, Loss: 0.9162265062332153, Accuracy: 1.0, Computation time: 1.1196613311767578\n",
      "Step: 5314, Loss: 0.9377655386924744, Accuracy: 0.96875, Computation time: 0.8586869239807129\n",
      "Step: 5315, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 0.8966646194458008\n",
      "Step: 5316, Loss: 0.915937066078186, Accuracy: 1.0, Computation time: 0.8628664016723633\n",
      "Step: 5317, Loss: 0.9376053214073181, Accuracy: 0.96875, Computation time: 0.8339097499847412\n",
      "Step: 5318, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 0.8864443302154541\n",
      "Step: 5319, Loss: 0.9159278869628906, Accuracy: 1.0, Computation time: 0.9999024868011475\n",
      "Step: 5320, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 0.9437477588653564\n",
      "Step: 5321, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 0.8470659255981445\n",
      "Step: 5322, Loss: 0.9375509023666382, Accuracy: 0.96875, Computation time: 1.012397289276123\n",
      "Step: 5323, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 0.8741199970245361\n",
      "Step: 5324, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.016343116760254\n",
      "Step: 5325, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.020932912826538\n",
      "Step: 5326, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.0538136959075928\n",
      "Step: 5327, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 0.8921711444854736\n",
      "Step: 5328, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 0.9809327125549316\n",
      "Step: 5329, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.000260591506958\n",
      "Step: 5330, Loss: 0.9376096725463867, Accuracy: 0.96875, Computation time: 0.8609025478363037\n",
      "Step: 5331, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 0.8982253074645996\n",
      "Step: 5332, Loss: 0.9375130534172058, Accuracy: 0.96875, Computation time: 1.0460870265960693\n",
      "Step: 5333, Loss: 0.917926549911499, Accuracy: 1.0, Computation time: 0.8464183807373047\n",
      "Step: 5334, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 0.9971568584442139\n",
      "Step: 5335, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 0.7903409004211426\n",
      "Step: 5336, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 0.99027419090271\n",
      "Step: 5337, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.1727685928344727\n",
      "Step: 5338, Loss: 0.9228934049606323, Accuracy: 1.0, Computation time: 0.9779300689697266\n",
      "Step: 5339, Loss: 0.9159146547317505, Accuracy: 1.0, Computation time: 1.482940912246704\n",
      "Step: 5340, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 0.9907689094543457\n",
      "Step: 5341, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.0561320781707764\n",
      "Step: 5342, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 0.9313712120056152\n",
      "Step: 5343, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 0.80086350440979\n",
      "Step: 5344, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.5432252883911133\n",
      "Step: 5345, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 0.9722518920898438\n",
      "Step: 5346, Loss: 0.9164758920669556, Accuracy: 1.0, Computation time: 1.0564355850219727\n",
      "Step: 5347, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 0.8593578338623047\n",
      "Step: 5348, Loss: 0.9417686462402344, Accuracy: 0.96875, Computation time: 1.0692229270935059\n",
      "Step: 5349, Loss: 0.915968120098114, Accuracy: 1.0, Computation time: 0.9262135028839111\n",
      "Step: 5350, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 0.7377159595489502\n",
      "Step: 5351, Loss: 0.9159720540046692, Accuracy: 1.0, Computation time: 0.8087868690490723\n",
      "Step: 5352, Loss: 0.9376659393310547, Accuracy: 0.96875, Computation time: 1.011552095413208\n",
      "Step: 5353, Loss: 0.9159980416297913, Accuracy: 1.0, Computation time: 0.8862388134002686\n",
      "Step: 5354, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 0.8952248096466064\n",
      "Step: 5355, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 0.930912971496582\n",
      "Step: 5356, Loss: 0.9171014428138733, Accuracy: 1.0, Computation time: 0.876396894454956\n",
      "Step: 5357, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 0.8748400211334229\n",
      "Step: 5358, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 0.9412033557891846\n",
      "Step: 5359, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 0.810183048248291\n",
      "Step: 5360, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 0.9662597179412842\n",
      "Step: 5361, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.8319075107574463\n",
      "Step: 5362, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 0.898535966873169\n",
      "Step: 5363, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 0.80184006690979\n",
      "Step: 5364, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 0.8362858295440674\n",
      "Step: 5365, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.0277855396270752\n",
      "Step: 5366, Loss: 0.9339105486869812, Accuracy: 0.96875, Computation time: 0.919363260269165\n",
      "Step: 5367, Loss: 0.9169459342956543, Accuracy: 1.0, Computation time: 0.7984621524810791\n",
      "Step: 5368, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 0.854832649230957\n",
      "Step: 5369, Loss: 0.9159650802612305, Accuracy: 1.0, Computation time: 1.1702704429626465\n",
      "Step: 5370, Loss: 0.9160007834434509, Accuracy: 1.0, Computation time: 0.8742160797119141\n",
      "Step: 5371, Loss: 0.9159883260726929, Accuracy: 1.0, Computation time: 0.8657217025756836\n",
      "Step: 5372, Loss: 0.9159439206123352, Accuracy: 1.0, Computation time: 0.7829186916351318\n",
      "Step: 5373, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.869978666305542\n",
      "Step: 5374, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.8591799736022949\n",
      "Step: 5375, Loss: 0.9390436410903931, Accuracy: 0.96875, Computation time: 0.912797212600708\n",
      "Step: 5376, Loss: 0.9375545382499695, Accuracy: 0.96875, Computation time: 0.8445985317230225\n",
      "Step: 5377, Loss: 0.9163568019866943, Accuracy: 1.0, Computation time: 1.0359737873077393\n",
      "Step: 5378, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 0.9196910858154297\n",
      "Step: 5379, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 0.7823293209075928\n",
      "Step: 5380, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 0.762871265411377\n",
      "Step: 5381, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 0.8154234886169434\n",
      "Step: 5382, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 0.8707773685455322\n",
      "Step: 5383, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.0402164459228516\n",
      "Step: 5384, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.0144696235656738\n",
      "Step: 5385, Loss: 0.9159550070762634, Accuracy: 1.0, Computation time: 0.9452264308929443\n",
      "Step: 5386, Loss: 0.9287823438644409, Accuracy: 0.96875, Computation time: 1.0388000011444092\n",
      "Step: 5387, Loss: 0.9164423942565918, Accuracy: 1.0, Computation time: 0.8941521644592285\n",
      "Step: 5388, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 0.8708219528198242\n",
      "Step: 5389, Loss: 0.9159623980522156, Accuracy: 1.0, Computation time: 1.0981898307800293\n",
      "Step: 5390, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 0.858004093170166\n",
      "Step: 5391, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 0.8525636196136475\n",
      "Step: 5392, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 0.9993405342102051\n",
      "Step: 5393, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 0.9614477157592773\n",
      "Step: 5394, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 0.9652385711669922\n",
      "Step: 5395, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 0.7822902202606201\n",
      "Step: 5396, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.3262841701507568\n",
      "Step: 5397, Loss: 0.9376400113105774, Accuracy: 0.96875, Computation time: 0.752631664276123\n",
      "Step: 5398, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 1.1256325244903564\n",
      "Step: 5399, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 0.9067716598510742\n",
      "Step: 5400, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 0.8312304019927979\n",
      "Step: 5401, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 1.0167553424835205\n",
      "Step: 5402, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 0.7930395603179932\n",
      "Step: 5403, Loss: 0.916641354560852, Accuracy: 1.0, Computation time: 1.7037928104400635\n",
      "Step: 5404, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 0.8233513832092285\n",
      "Step: 5405, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 0.9572458267211914\n",
      "Step: 5406, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 0.8156464099884033\n",
      "Step: 5407, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 0.9148728847503662\n",
      "Step: 5408, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 1.0833747386932373\n",
      "Step: 5409, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 0.8374676704406738\n",
      "Step: 5410, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.5304338932037354\n",
      "Step: 5411, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 0.9608972072601318\n",
      "Step: 5412, Loss: 0.9160206913948059, Accuracy: 1.0, Computation time: 0.8549013137817383\n",
      "Step: 5413, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 0.8498620986938477\n",
      "Step: 5414, Loss: 0.9183036088943481, Accuracy: 1.0, Computation time: 1.0565109252929688\n",
      "Step: 5415, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 0.7392294406890869\n",
      "Step: 5416, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 0.9181368350982666\n",
      "Step: 5417, Loss: 0.9185305237770081, Accuracy: 1.0, Computation time: 1.1853291988372803\n",
      "Step: 5418, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 0.8540229797363281\n",
      "Step: 5419, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.0286586284637451\n",
      "########################\n",
      "Test loss: 1.0714750289916992, Test Accuracy_epoch39: 0.7761485576629639\n",
      "########################\n",
      "Step: 5420, Loss: 0.9177242517471313, Accuracy: 1.0, Computation time: 1.336913824081421\n",
      "Step: 5421, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.6012349128723145\n",
      "Step: 5422, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 0.8619956970214844\n",
      "Step: 5423, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 0.9555375576019287\n",
      "Step: 5424, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 0.825310468673706\n",
      "Step: 5425, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 0.9800922870635986\n",
      "Step: 5426, Loss: 0.9159197211265564, Accuracy: 1.0, Computation time: 0.930680513381958\n",
      "Step: 5427, Loss: 0.9178945422172546, Accuracy: 1.0, Computation time: 1.5423429012298584\n",
      "Step: 5428, Loss: 0.9159247279167175, Accuracy: 1.0, Computation time: 0.8703043460845947\n",
      "Step: 5429, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 0.912121057510376\n",
      "Step: 5430, Loss: 0.9372938871383667, Accuracy: 0.96875, Computation time: 0.8669037818908691\n",
      "Step: 5431, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 0.8723070621490479\n",
      "Step: 5432, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 0.9320261478424072\n",
      "Step: 5433, Loss: 0.9376688003540039, Accuracy: 0.96875, Computation time: 0.8603675365447998\n",
      "Step: 5434, Loss: 0.9158408045768738, Accuracy: 1.0, Computation time: 0.8959941864013672\n",
      "Step: 5435, Loss: 0.9499162435531616, Accuracy: 0.9375, Computation time: 1.3627572059631348\n",
      "Step: 5436, Loss: 0.9178532958030701, Accuracy: 1.0, Computation time: 1.1134750843048096\n",
      "Step: 5437, Loss: 0.9159356355667114, Accuracy: 1.0, Computation time: 0.8788390159606934\n",
      "Step: 5438, Loss: 0.9159614443778992, Accuracy: 1.0, Computation time: 0.9335076808929443\n",
      "Step: 5439, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 0.8890953063964844\n",
      "Step: 5440, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 0.8306827545166016\n",
      "Step: 5441, Loss: 0.9374450445175171, Accuracy: 0.96875, Computation time: 0.8240060806274414\n",
      "Step: 5442, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 0.8720605373382568\n",
      "Step: 5443, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 0.9568524360656738\n",
      "Step: 5444, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 0.8406648635864258\n",
      "Step: 5445, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 0.8237049579620361\n",
      "Step: 5446, Loss: 0.9165323972702026, Accuracy: 1.0, Computation time: 0.8548657894134521\n",
      "Step: 5447, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.1308643817901611\n",
      "Step: 5448, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.081449031829834\n",
      "Step: 5449, Loss: 0.9159477353096008, Accuracy: 1.0, Computation time: 0.9272491931915283\n",
      "Step: 5450, Loss: 0.9159253239631653, Accuracy: 1.0, Computation time: 0.8529002666473389\n",
      "Step: 5451, Loss: 0.9377751350402832, Accuracy: 0.96875, Computation time: 0.8397781848907471\n",
      "Step: 5452, Loss: 0.9376052021980286, Accuracy: 0.96875, Computation time: 0.9301249980926514\n",
      "Step: 5453, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 0.9751706123352051\n",
      "Step: 5454, Loss: 0.9376130104064941, Accuracy: 0.96875, Computation time: 0.9994602203369141\n",
      "Step: 5455, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 0.8135342597961426\n",
      "Step: 5456, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 0.8267760276794434\n",
      "Step: 5457, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 0.8989765644073486\n",
      "Step: 5458, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 0.9820544719696045\n",
      "Step: 5459, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 0.8219161033630371\n",
      "Step: 5460, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 0.99863600730896\n",
      "Step: 5461, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.1940889358520508\n",
      "Step: 5462, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 0.8391482830047607\n",
      "Step: 5463, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 0.8495407104492188\n",
      "Step: 5464, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 0.8998827934265137\n",
      "Step: 5465, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 0.7978699207305908\n",
      "Step: 5466, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 0.8038477897644043\n",
      "Step: 5467, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 0.8880441188812256\n",
      "Step: 5468, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 0.8612489700317383\n",
      "Step: 5469, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 0.8503572940826416\n",
      "Step: 5470, Loss: 0.9158387184143066, Accuracy: 1.0, Computation time: 0.8361532688140869\n",
      "Step: 5471, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 0.9174826145172119\n",
      "Step: 5472, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.0323855876922607\n",
      "Step: 5473, Loss: 0.9374814033508301, Accuracy: 0.96875, Computation time: 1.2898197174072266\n",
      "Step: 5474, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 0.7329857349395752\n",
      "Step: 5475, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 0.8999648094177246\n",
      "Step: 5476, Loss: 0.9390388131141663, Accuracy: 0.96875, Computation time: 1.1195502281188965\n",
      "Step: 5477, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 0.8315722942352295\n",
      "Step: 5478, Loss: 0.9159269332885742, Accuracy: 1.0, Computation time: 0.8534433841705322\n",
      "Step: 5479, Loss: 0.9376185536384583, Accuracy: 0.96875, Computation time: 0.7880556583404541\n",
      "Step: 5480, Loss: 0.9376916885375977, Accuracy: 0.96875, Computation time: 0.9407894611358643\n",
      "Step: 5481, Loss: 0.9159141778945923, Accuracy: 1.0, Computation time: 0.879502534866333\n",
      "Step: 5482, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 0.8733382225036621\n",
      "Step: 5483, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 0.8867835998535156\n",
      "Step: 5484, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 0.9343178272247314\n",
      "Step: 5485, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 0.9761853218078613\n",
      "Step: 5486, Loss: 0.9374538064002991, Accuracy: 0.96875, Computation time: 1.0096824169158936\n",
      "Step: 5487, Loss: 0.9171401858329773, Accuracy: 1.0, Computation time: 1.2255074977874756\n",
      "Step: 5488, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.085726022720337\n",
      "Step: 5489, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.068542718887329\n",
      "Step: 5490, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.0902760028839111\n",
      "Step: 5491, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 1.0388882160186768\n",
      "Step: 5492, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.1093058586120605\n",
      "Step: 5493, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 0.9697563648223877\n",
      "Step: 5494, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.8667471408843994\n",
      "Step: 5495, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.0181183815002441\n",
      "Step: 5496, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 0.8312618732452393\n",
      "Step: 5497, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 1.0741591453552246\n",
      "Step: 5498, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 0.8927364349365234\n",
      "Step: 5499, Loss: 0.9160170555114746, Accuracy: 1.0, Computation time: 1.285792350769043\n",
      "Step: 5500, Loss: 0.9199374914169312, Accuracy: 1.0, Computation time: 1.0711102485656738\n",
      "Step: 5501, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 0.9803853034973145\n",
      "Step: 5502, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 0.858816385269165\n",
      "Step: 5503, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 0.9026260375976562\n",
      "Step: 5504, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 0.9320399761199951\n",
      "Step: 5505, Loss: 0.9374843239784241, Accuracy: 0.96875, Computation time: 0.8378782272338867\n",
      "Step: 5506, Loss: 0.9160110950469971, Accuracy: 1.0, Computation time: 0.8344876766204834\n",
      "Step: 5507, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 0.8162178993225098\n",
      "Step: 5508, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 0.9750208854675293\n",
      "Step: 5509, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 0.7992970943450928\n",
      "Step: 5510, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 0.9124569892883301\n",
      "Step: 5511, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 0.8526020050048828\n",
      "Step: 5512, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 0.8824522495269775\n",
      "Step: 5513, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 0.8662698268890381\n",
      "Step: 5514, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.7978713512420654\n",
      "Step: 5515, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 0.8424239158630371\n",
      "Step: 5516, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.02349853515625\n",
      "Step: 5517, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 0.8229928016662598\n",
      "Step: 5518, Loss: 0.9375389814376831, Accuracy: 0.96875, Computation time: 0.9502549171447754\n",
      "Step: 5519, Loss: 0.9174599647521973, Accuracy: 1.0, Computation time: 1.184739351272583\n",
      "Step: 5520, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 0.8644566535949707\n",
      "Step: 5521, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 0.8002991676330566\n",
      "Step: 5522, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.697066068649292\n",
      "Step: 5523, Loss: 0.9189329147338867, Accuracy: 1.0, Computation time: 0.9673349857330322\n",
      "Step: 5524, Loss: 0.9376871585845947, Accuracy: 0.96875, Computation time: 0.939321756362915\n",
      "Step: 5525, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 1.020188808441162\n",
      "Step: 5526, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 0.923954963684082\n",
      "Step: 5527, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 0.8815724849700928\n",
      "Step: 5528, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 0.8923721313476562\n",
      "Step: 5529, Loss: 0.9160943627357483, Accuracy: 1.0, Computation time: 0.8198111057281494\n",
      "Step: 5530, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 0.8949484825134277\n",
      "Step: 5531, Loss: 0.9159594774246216, Accuracy: 1.0, Computation time: 1.033656358718872\n",
      "Step: 5532, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 0.9792757034301758\n",
      "Step: 5533, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 1.333543300628662\n",
      "Step: 5534, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 0.9636545181274414\n",
      "Step: 5535, Loss: 0.9375344514846802, Accuracy: 0.96875, Computation time: 1.6555888652801514\n",
      "Step: 5536, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 0.9945693016052246\n",
      "Step: 5537, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 0.8361902236938477\n",
      "Step: 5538, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 0.912830114364624\n",
      "Step: 5539, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 0.9531495571136475\n",
      "Step: 5540, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 0.7928524017333984\n",
      "Step: 5541, Loss: 0.9158387780189514, Accuracy: 1.0, Computation time: 0.8211545944213867\n",
      "Step: 5542, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 0.8683993816375732\n",
      "Step: 5543, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 0.8097178936004639\n",
      "Step: 5544, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 0.7918045520782471\n",
      "Step: 5545, Loss: 0.9210272431373596, Accuracy: 1.0, Computation time: 1.0971322059631348\n",
      "Step: 5546, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 0.8087637424468994\n",
      "Step: 5547, Loss: 0.937693178653717, Accuracy: 0.96875, Computation time: 1.0283823013305664\n",
      "Step: 5548, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 0.7918510437011719\n",
      "Step: 5549, Loss: 0.9230636358261108, Accuracy: 1.0, Computation time: 1.1630184650421143\n",
      "Step: 5550, Loss: 0.9373841285705566, Accuracy: 0.96875, Computation time: 0.897442102432251\n",
      "Step: 5551, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 1.0027410984039307\n",
      "Step: 5552, Loss: 0.9160345792770386, Accuracy: 1.0, Computation time: 0.8375451564788818\n",
      "Step: 5553, Loss: 0.9160271286964417, Accuracy: 1.0, Computation time: 1.100475549697876\n",
      "Step: 5554, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 0.9705288410186768\n",
      "Step: 5555, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 0.8933923244476318\n",
      "Step: 5556, Loss: 0.9168053269386292, Accuracy: 1.0, Computation time: 0.8201346397399902\n",
      "Step: 5557, Loss: 0.9323044419288635, Accuracy: 0.96875, Computation time: 1.1554617881774902\n",
      "Step: 5558, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 0.8582699298858643\n",
      "########################\n",
      "Test loss: 1.0767521858215332, Test Accuracy_epoch40: 0.7653958797454834\n",
      "########################\n",
      "Step: 5559, Loss: 0.9160017371177673, Accuracy: 1.0, Computation time: 0.8211264610290527\n",
      "Step: 5560, Loss: 0.9160924553871155, Accuracy: 1.0, Computation time: 0.8533086776733398\n",
      "Step: 5561, Loss: 0.9161054491996765, Accuracy: 1.0, Computation time: 1.0549848079681396\n",
      "Step: 5562, Loss: 0.9190725088119507, Accuracy: 1.0, Computation time: 1.1939141750335693\n",
      "Step: 5563, Loss: 0.9160459637641907, Accuracy: 1.0, Computation time: 0.8896372318267822\n",
      "Step: 5564, Loss: 0.9164348244667053, Accuracy: 1.0, Computation time: 0.9329676628112793\n",
      "Step: 5565, Loss: 0.9379330277442932, Accuracy: 0.96875, Computation time: 1.0321769714355469\n",
      "Step: 5566, Loss: 0.9359287619590759, Accuracy: 0.96875, Computation time: 1.008408546447754\n",
      "Step: 5567, Loss: 0.9178804755210876, Accuracy: 1.0, Computation time: 0.9433660507202148\n",
      "Step: 5568, Loss: 0.9161158204078674, Accuracy: 1.0, Computation time: 0.8259520530700684\n",
      "Step: 5569, Loss: 0.9252232909202576, Accuracy: 1.0, Computation time: 2.384058713912964\n",
      "Step: 5570, Loss: 0.9160889387130737, Accuracy: 1.0, Computation time: 1.3640878200531006\n",
      "Step: 5571, Loss: 0.9163202047348022, Accuracy: 1.0, Computation time: 1.157280445098877\n",
      "Step: 5572, Loss: 0.9192360043525696, Accuracy: 1.0, Computation time: 1.2224230766296387\n",
      "Step: 5573, Loss: 0.9163511395454407, Accuracy: 1.0, Computation time: 0.9552547931671143\n",
      "Step: 5574, Loss: 0.9379571080207825, Accuracy: 0.96875, Computation time: 0.8419153690338135\n",
      "Step: 5575, Loss: 0.9162248373031616, Accuracy: 1.0, Computation time: 0.7831716537475586\n",
      "Step: 5576, Loss: 0.9160858392715454, Accuracy: 1.0, Computation time: 0.8825111389160156\n",
      "Step: 5577, Loss: 0.9162054657936096, Accuracy: 1.0, Computation time: 0.7377064228057861\n",
      "Step: 5578, Loss: 0.9160460829734802, Accuracy: 1.0, Computation time: 0.7562012672424316\n",
      "Step: 5579, Loss: 0.9160800576210022, Accuracy: 1.0, Computation time: 0.8173387050628662\n",
      "Step: 5580, Loss: 0.9159821271896362, Accuracy: 1.0, Computation time: 0.747530460357666\n",
      "Step: 5581, Loss: 0.9274812340736389, Accuracy: 0.96875, Computation time: 0.8665449619293213\n",
      "Step: 5582, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 0.7113895416259766\n",
      "Step: 5583, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 0.8166511058807373\n",
      "Step: 5584, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 0.7615499496459961\n",
      "Step: 5585, Loss: 0.9159753322601318, Accuracy: 1.0, Computation time: 0.9023089408874512\n",
      "Step: 5586, Loss: 0.9159572124481201, Accuracy: 1.0, Computation time: 0.889916181564331\n",
      "Step: 5587, Loss: 0.9161602258682251, Accuracy: 1.0, Computation time: 1.0628702640533447\n",
      "Step: 5588, Loss: 0.9375700950622559, Accuracy: 0.96875, Computation time: 0.7527306079864502\n",
      "Step: 5589, Loss: 0.9382321238517761, Accuracy: 0.96875, Computation time: 0.7981905937194824\n",
      "Step: 5590, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 0.8641867637634277\n",
      "Step: 5591, Loss: 0.9159902930259705, Accuracy: 1.0, Computation time: 0.949148416519165\n",
      "Step: 5592, Loss: 0.9160068035125732, Accuracy: 1.0, Computation time: 0.869321346282959\n",
      "Step: 5593, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 0.7622020244598389\n",
      "Step: 5594, Loss: 0.9160578846931458, Accuracy: 1.0, Computation time: 0.7968504428863525\n",
      "Step: 5595, Loss: 0.9161196947097778, Accuracy: 1.0, Computation time: 0.7322590351104736\n",
      "Step: 5596, Loss: 0.9160199165344238, Accuracy: 1.0, Computation time: 1.0539638996124268\n",
      "Step: 5597, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 0.7476654052734375\n",
      "Step: 5598, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 0.8122444152832031\n",
      "Step: 5599, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 0.9978101253509521\n",
      "Step: 5600, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 0.7444071769714355\n",
      "Step: 5601, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 0.8102223873138428\n",
      "Step: 5602, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 0.8407211303710938\n",
      "Step: 5603, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 0.7811729907989502\n",
      "Step: 5604, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 0.8699817657470703\n",
      "Step: 5605, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 0.8220870494842529\n",
      "Step: 5606, Loss: 0.915971577167511, Accuracy: 1.0, Computation time: 0.7793498039245605\n",
      "Step: 5607, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 0.8954875469207764\n",
      "Step: 5608, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 0.9126837253570557\n",
      "Step: 5609, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 0.8905937671661377\n",
      "Step: 5610, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 0.7658321857452393\n",
      "Step: 5611, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 0.9037470817565918\n",
      "Step: 5612, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 0.9388916492462158\n",
      "Step: 5613, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 0.8544883728027344\n",
      "Step: 5614, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 0.9287645816802979\n",
      "Step: 5615, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 0.8997976779937744\n",
      "Step: 5616, Loss: 0.9159380197525024, Accuracy: 1.0, Computation time: 0.8178143501281738\n",
      "Step: 5617, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 1.4649851322174072\n",
      "Step: 5618, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 0.8844461441040039\n",
      "Step: 5619, Loss: 0.9179457426071167, Accuracy: 1.0, Computation time: 0.9511609077453613\n",
      "Step: 5620, Loss: 0.938434362411499, Accuracy: 0.96875, Computation time: 0.9158720970153809\n",
      "Step: 5621, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 0.8693175315856934\n",
      "Step: 5622, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 0.7748560905456543\n",
      "Step: 5623, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 0.7476909160614014\n",
      "Step: 5624, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 0.773557186126709\n",
      "Step: 5625, Loss: 0.9163259863853455, Accuracy: 1.0, Computation time: 0.8640580177307129\n",
      "Step: 5626, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 0.8369064331054688\n",
      "Step: 5627, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 0.7963309288024902\n",
      "Step: 5628, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 0.9935784339904785\n",
      "Step: 5629, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 0.8257708549499512\n",
      "Step: 5630, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 0.8500771522521973\n",
      "Step: 5631, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 0.8852875232696533\n",
      "Step: 5632, Loss: 0.9158543944358826, Accuracy: 1.0, Computation time: 0.775390625\n",
      "Step: 5633, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 0.7362213134765625\n",
      "Step: 5634, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.1048266887664795\n",
      "Step: 5635, Loss: 0.9378774762153625, Accuracy: 0.96875, Computation time: 0.8320209980010986\n",
      "Step: 5636, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 0.8125460147857666\n",
      "Step: 5637, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.7391307353973389\n",
      "Step: 5638, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 0.8031373023986816\n",
      "Step: 5639, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 0.8147697448730469\n",
      "Step: 5640, Loss: 0.9374849796295166, Accuracy: 0.96875, Computation time: 0.7475869655609131\n",
      "Step: 5641, Loss: 0.9375621676445007, Accuracy: 0.96875, Computation time: 0.8096115589141846\n",
      "Step: 5642, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 0.8066976070404053\n",
      "Step: 5643, Loss: 0.9159566760063171, Accuracy: 1.0, Computation time: 0.8544213771820068\n",
      "Step: 5644, Loss: 0.9374793171882629, Accuracy: 0.96875, Computation time: 0.8296985626220703\n",
      "Step: 5645, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 0.7807474136352539\n",
      "Step: 5646, Loss: 0.9160289168357849, Accuracy: 1.0, Computation time: 0.752223014831543\n",
      "Step: 5647, Loss: 0.9227199554443359, Accuracy: 1.0, Computation time: 0.864354133605957\n",
      "Step: 5648, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 0.8645641803741455\n",
      "Step: 5649, Loss: 0.9159718155860901, Accuracy: 1.0, Computation time: 0.7657997608184814\n",
      "Step: 5650, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 0.7675967216491699\n",
      "Step: 5651, Loss: 0.9161378145217896, Accuracy: 1.0, Computation time: 0.9299957752227783\n",
      "Step: 5652, Loss: 0.9301368594169617, Accuracy: 0.96875, Computation time: 0.9577839374542236\n",
      "Step: 5653, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 0.8278865814208984\n",
      "Step: 5654, Loss: 0.9364189505577087, Accuracy: 0.96875, Computation time: 0.7202990055084229\n",
      "Step: 5655, Loss: 0.916060209274292, Accuracy: 1.0, Computation time: 0.7772786617279053\n",
      "Step: 5656, Loss: 0.9159753918647766, Accuracy: 1.0, Computation time: 0.7699425220489502\n",
      "Step: 5657, Loss: 0.9159719347953796, Accuracy: 1.0, Computation time: 0.797417402267456\n",
      "Step: 5658, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 0.8237230777740479\n",
      "Step: 5659, Loss: 0.9376072287559509, Accuracy: 0.96875, Computation time: 0.7156686782836914\n",
      "Step: 5660, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 0.8559913635253906\n",
      "Step: 5661, Loss: 0.9158779978752136, Accuracy: 1.0, Computation time: 0.7823963165283203\n",
      "Step: 5662, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 0.769390344619751\n",
      "Step: 5663, Loss: 0.9178575277328491, Accuracy: 1.0, Computation time: 1.2473442554473877\n",
      "Step: 5664, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 0.8332493305206299\n",
      "Step: 5665, Loss: 0.9160434007644653, Accuracy: 1.0, Computation time: 0.8031325340270996\n",
      "Step: 5666, Loss: 0.916164219379425, Accuracy: 1.0, Computation time: 1.4990522861480713\n",
      "Step: 5667, Loss: 0.9164603352546692, Accuracy: 1.0, Computation time: 0.8196010589599609\n",
      "Step: 5668, Loss: 0.9159005284309387, Accuracy: 1.0, Computation time: 0.7617805004119873\n",
      "Step: 5669, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 0.7109882831573486\n",
      "Step: 5670, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 0.7630598545074463\n",
      "Step: 5671, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 0.7168219089508057\n",
      "Step: 5672, Loss: 0.9159306287765503, Accuracy: 1.0, Computation time: 0.7422993183135986\n",
      "Step: 5673, Loss: 0.9159649610519409, Accuracy: 1.0, Computation time: 0.8718621730804443\n",
      "Step: 5674, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 0.7196266651153564\n",
      "Step: 5675, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 0.7685797214508057\n",
      "Step: 5676, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 0.7269439697265625\n",
      "Step: 5677, Loss: 0.9159516096115112, Accuracy: 1.0, Computation time: 0.9738755226135254\n",
      "Step: 5678, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 0.7802042961120605\n",
      "Step: 5679, Loss: 0.9434006810188293, Accuracy: 0.96875, Computation time: 1.0248193740844727\n",
      "Step: 5680, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 0.7964608669281006\n",
      "Step: 5681, Loss: 0.9160618782043457, Accuracy: 1.0, Computation time: 0.9505043029785156\n",
      "Step: 5682, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 0.7511372566223145\n",
      "Step: 5683, Loss: 0.925023078918457, Accuracy: 1.0, Computation time: 0.9926221370697021\n",
      "Step: 5684, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 0.7647988796234131\n",
      "Step: 5685, Loss: 0.9399870038032532, Accuracy: 0.96875, Computation time: 0.85843825340271\n",
      "Step: 5686, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 0.8616948127746582\n",
      "Step: 5687, Loss: 0.9161590337753296, Accuracy: 1.0, Computation time: 1.0152835845947266\n",
      "Step: 5688, Loss: 0.916459321975708, Accuracy: 1.0, Computation time: 0.7976243495941162\n",
      "Step: 5689, Loss: 0.9378060698509216, Accuracy: 0.96875, Computation time: 0.7879600524902344\n",
      "Step: 5690, Loss: 0.937524139881134, Accuracy: 0.96875, Computation time: 0.7850584983825684\n",
      "Step: 5691, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 1.04691481590271\n",
      "Step: 5692, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 0.8679485321044922\n",
      "Step: 5693, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 1.049149751663208\n",
      "Step: 5694, Loss: 0.9159568548202515, Accuracy: 1.0, Computation time: 0.9444446563720703\n",
      "Step: 5695, Loss: 0.9159833788871765, Accuracy: 1.0, Computation time: 1.1121418476104736\n",
      "Step: 5696, Loss: 0.915947437286377, Accuracy: 1.0, Computation time: 0.9426555633544922\n",
      "Step: 5697, Loss: 0.9159404039382935, Accuracy: 1.0, Computation time: 0.8095309734344482\n",
      "########################\n",
      "Test loss: 1.0706990957260132, Test Accuracy_epoch41: 0.7761485576629639\n",
      "########################\n",
      "Step: 5698, Loss: 0.9159969091415405, Accuracy: 1.0, Computation time: 1.2530434131622314\n",
      "Step: 5699, Loss: 0.9376294612884521, Accuracy: 0.96875, Computation time: 1.13385009765625\n",
      "Step: 5700, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 0.8252711296081543\n",
      "Step: 5701, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.0822358131408691\n",
      "Step: 5702, Loss: 0.9375760555267334, Accuracy: 0.96875, Computation time: 0.8656446933746338\n",
      "Step: 5703, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.052121877670288\n",
      "Step: 5704, Loss: 0.9267693758010864, Accuracy: 0.96875, Computation time: 1.4004862308502197\n",
      "Step: 5705, Loss: 0.937616765499115, Accuracy: 0.96875, Computation time: 0.789278507232666\n",
      "Step: 5706, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 0.9083185195922852\n",
      "Step: 5707, Loss: 0.9509633779525757, Accuracy: 0.9375, Computation time: 0.9069132804870605\n",
      "Step: 5708, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 0.9462010860443115\n",
      "Step: 5709, Loss: 0.922796368598938, Accuracy: 1.0, Computation time: 0.8156719207763672\n",
      "Step: 5710, Loss: 0.9184319376945496, Accuracy: 1.0, Computation time: 0.9475774765014648\n",
      "Step: 5711, Loss: 0.9159476161003113, Accuracy: 1.0, Computation time: 0.829383134841919\n",
      "Step: 5712, Loss: 0.9408840537071228, Accuracy: 0.96875, Computation time: 0.9214675426483154\n",
      "Step: 5713, Loss: 0.9161067008972168, Accuracy: 1.0, Computation time: 0.8684267997741699\n",
      "Step: 5714, Loss: 0.9169466495513916, Accuracy: 1.0, Computation time: 1.1562249660491943\n",
      "Step: 5715, Loss: 0.9162341952323914, Accuracy: 1.0, Computation time: 0.8369262218475342\n",
      "Step: 5716, Loss: 0.9168270230293274, Accuracy: 1.0, Computation time: 1.1339786052703857\n",
      "Step: 5717, Loss: 0.9165142774581909, Accuracy: 1.0, Computation time: 0.9757916927337646\n",
      "Step: 5718, Loss: 0.9325311779975891, Accuracy: 0.96875, Computation time: 0.8397274017333984\n",
      "Step: 5719, Loss: 0.9168871641159058, Accuracy: 1.0, Computation time: 1.0299084186553955\n",
      "Step: 5720, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 0.8247179985046387\n",
      "Step: 5721, Loss: 0.9159368872642517, Accuracy: 1.0, Computation time: 0.8132069110870361\n",
      "Step: 5722, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 0.808704137802124\n",
      "Step: 5723, Loss: 0.9159929156303406, Accuracy: 1.0, Computation time: 0.9175581932067871\n",
      "Step: 5724, Loss: 0.9201935529708862, Accuracy: 1.0, Computation time: 0.9728643894195557\n",
      "Step: 5725, Loss: 0.9160780310630798, Accuracy: 1.0, Computation time: 0.883634090423584\n",
      "Step: 5726, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 0.8079981803894043\n",
      "Step: 5727, Loss: 0.9349541068077087, Accuracy: 0.96875, Computation time: 1.3511371612548828\n",
      "Step: 5728, Loss: 0.9160656929016113, Accuracy: 1.0, Computation time: 0.9313592910766602\n",
      "Step: 5729, Loss: 0.915950357913971, Accuracy: 1.0, Computation time: 0.7953023910522461\n",
      "Step: 5730, Loss: 0.9160001873970032, Accuracy: 1.0, Computation time: 0.9503517150878906\n",
      "Step: 5731, Loss: 0.9159356355667114, Accuracy: 1.0, Computation time: 0.7935707569122314\n",
      "Step: 5732, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 0.7783381938934326\n",
      "Step: 5733, Loss: 0.9375355839729309, Accuracy: 0.96875, Computation time: 0.8473126888275146\n",
      "Step: 5734, Loss: 0.918187141418457, Accuracy: 1.0, Computation time: 0.9014315605163574\n",
      "Step: 5735, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 0.8189835548400879\n",
      "Step: 5736, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.1498734951019287\n",
      "Step: 5737, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 0.827218770980835\n",
      "Step: 5738, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 0.8795108795166016\n",
      "Step: 5739, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.0354561805725098\n",
      "Step: 5740, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 0.8054444789886475\n",
      "Step: 5741, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 0.8979175090789795\n",
      "Step: 5742, Loss: 0.9159241318702698, Accuracy: 1.0, Computation time: 1.0938441753387451\n",
      "Step: 5743, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.9912192821502686\n",
      "Step: 5744, Loss: 0.9374307990074158, Accuracy: 0.96875, Computation time: 1.020507574081421\n",
      "Step: 5745, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 0.7632026672363281\n",
      "Step: 5746, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 0.8143148422241211\n",
      "Step: 5747, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 1.0345711708068848\n",
      "Step: 5748, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.0332200527191162\n",
      "Step: 5749, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 0.8056666851043701\n",
      "Step: 5750, Loss: 0.9160007238388062, Accuracy: 1.0, Computation time: 1.124030590057373\n",
      "Step: 5751, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 0.7422330379486084\n",
      "Step: 5752, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 0.764092206954956\n",
      "Step: 5753, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.1039361953735352\n",
      "Step: 5754, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 0.7379677295684814\n",
      "Step: 5755, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 0.9041211605072021\n",
      "Step: 5756, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 0.965033769607544\n",
      "Step: 5757, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 0.823871374130249\n",
      "Step: 5758, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 0.8811297416687012\n",
      "Step: 5759, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.0660619735717773\n",
      "Step: 5760, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 0.8814351558685303\n",
      "Step: 5761, Loss: 0.9159502387046814, Accuracy: 1.0, Computation time: 0.8011186122894287\n",
      "Step: 5762, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 0.7925248146057129\n",
      "Step: 5763, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 0.8120887279510498\n",
      "Step: 5764, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.0557410717010498\n",
      "Step: 5765, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.1022875308990479\n",
      "Step: 5766, Loss: 0.917444109916687, Accuracy: 1.0, Computation time: 1.1484570503234863\n",
      "Step: 5767, Loss: 0.9593302607536316, Accuracy: 0.9375, Computation time: 1.0677330493927002\n",
      "Step: 5768, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.444342851638794\n",
      "Step: 5769, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 0.8020443916320801\n",
      "Step: 5770, Loss: 0.9375081658363342, Accuracy: 0.96875, Computation time: 0.9738061428070068\n",
      "Step: 5771, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 0.7755844593048096\n",
      "Step: 5772, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 0.8782436847686768\n",
      "Step: 5773, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.0753800868988037\n",
      "Step: 5774, Loss: 0.9374281167984009, Accuracy: 0.96875, Computation time: 1.0077438354492188\n",
      "Step: 5775, Loss: 0.9160147309303284, Accuracy: 1.0, Computation time: 0.9734697341918945\n",
      "Step: 5776, Loss: 0.9161878824234009, Accuracy: 1.0, Computation time: 1.0091772079467773\n",
      "Step: 5777, Loss: 0.9160000681877136, Accuracy: 1.0, Computation time: 1.4283974170684814\n",
      "Step: 5778, Loss: 0.9159724116325378, Accuracy: 1.0, Computation time: 1.4210519790649414\n",
      "Step: 5779, Loss: 0.9161749482154846, Accuracy: 1.0, Computation time: 0.8971447944641113\n",
      "Step: 5780, Loss: 0.9373363852500916, Accuracy: 0.96875, Computation time: 0.8251776695251465\n",
      "Step: 5781, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 0.9522736072540283\n",
      "Step: 5782, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.2619130611419678\n",
      "Step: 5783, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 0.8666660785675049\n",
      "Step: 5784, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 0.8953309059143066\n",
      "Step: 5785, Loss: 0.9374449849128723, Accuracy: 0.96875, Computation time: 0.8137955665588379\n",
      "Step: 5786, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 0.9731323719024658\n",
      "Step: 5787, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 0.9502599239349365\n",
      "Step: 5788, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 0.9410936832427979\n",
      "Step: 5789, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 0.7854630947113037\n",
      "Step: 5790, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 0.8940558433532715\n",
      "Step: 5791, Loss: 0.9214580059051514, Accuracy: 1.0, Computation time: 0.9829866886138916\n",
      "Step: 5792, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.0555999279022217\n",
      "Step: 5793, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 0.7306456565856934\n",
      "Step: 5794, Loss: 0.9158928394317627, Accuracy: 1.0, Computation time: 0.9014673233032227\n",
      "Step: 5795, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 0.7966208457946777\n",
      "Step: 5796, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 0.750694751739502\n",
      "Step: 5797, Loss: 0.9159441590309143, Accuracy: 1.0, Computation time: 0.7610220909118652\n",
      "Step: 5798, Loss: 0.9159358143806458, Accuracy: 1.0, Computation time: 0.7605984210968018\n",
      "Step: 5799, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 0.7914860248565674\n",
      "Step: 5800, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 0.8483374118804932\n",
      "Step: 5801, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 0.7910094261169434\n",
      "Step: 5802, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 0.8257038593292236\n",
      "Step: 5803, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 0.8406541347503662\n",
      "Step: 5804, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 0.7865140438079834\n",
      "Step: 5805, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 2.2416739463806152\n",
      "Step: 5806, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 0.7472817897796631\n",
      "Step: 5807, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 0.915499210357666\n",
      "Step: 5808, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 0.7049703598022461\n",
      "Step: 5809, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 0.9435584545135498\n",
      "Step: 5810, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 0.7754952907562256\n",
      "Step: 5811, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 0.8320868015289307\n",
      "Step: 5812, Loss: 0.9168866872787476, Accuracy: 1.0, Computation time: 1.2494761943817139\n",
      "Step: 5813, Loss: 0.9158875346183777, Accuracy: 1.0, Computation time: 0.9303228855133057\n",
      "Step: 5814, Loss: 0.915909469127655, Accuracy: 1.0, Computation time: 0.819455623626709\n",
      "Step: 5815, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.0155720710754395\n",
      "Step: 5816, Loss: 0.9252737164497375, Accuracy: 0.96875, Computation time: 0.9590210914611816\n",
      "Step: 5817, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 0.9140398502349854\n",
      "Step: 5818, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 0.8803777694702148\n",
      "Step: 5819, Loss: 0.9159614443778992, Accuracy: 1.0, Computation time: 0.7341976165771484\n",
      "Step: 5820, Loss: 0.9159604907035828, Accuracy: 1.0, Computation time: 0.7642550468444824\n",
      "Step: 5821, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 0.7197442054748535\n",
      "Step: 5822, Loss: 0.9159549474716187, Accuracy: 1.0, Computation time: 0.8456332683563232\n",
      "Step: 5823, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 0.9920856952667236\n",
      "Step: 5824, Loss: 0.937578558921814, Accuracy: 0.96875, Computation time: 0.8033852577209473\n",
      "Step: 5825, Loss: 0.9162947535514832, Accuracy: 1.0, Computation time: 0.8697638511657715\n",
      "Step: 5826, Loss: 0.918720006942749, Accuracy: 1.0, Computation time: 1.0053281784057617\n",
      "Step: 5827, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 0.8345599174499512\n",
      "Step: 5828, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 0.7748606204986572\n",
      "Step: 5829, Loss: 0.9375742673873901, Accuracy: 0.96875, Computation time: 0.9432063102722168\n",
      "Step: 5830, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 0.8357162475585938\n",
      "Step: 5831, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.777320146560669\n",
      "Step: 5832, Loss: 0.9174492955207825, Accuracy: 1.0, Computation time: 0.8708829879760742\n",
      "Step: 5833, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 0.9553811550140381\n",
      "Step: 5834, Loss: 0.9183352589607239, Accuracy: 1.0, Computation time: 1.4639437198638916\n",
      "Step: 5835, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.0289621353149414\n",
      "Step: 5836, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 0.9702503681182861\n",
      "########################\n",
      "Test loss: 1.071916937828064, Test Accuracy_epoch42: 0.7722384929656982\n",
      "########################\n",
      "Step: 5837, Loss: 0.9159863591194153, Accuracy: 1.0, Computation time: 0.873187780380249\n",
      "Step: 5838, Loss: 0.9160614013671875, Accuracy: 1.0, Computation time: 0.9970479011535645\n",
      "Step: 5839, Loss: 0.9377729296684265, Accuracy: 0.96875, Computation time: 1.1408884525299072\n",
      "Step: 5840, Loss: 0.9159027934074402, Accuracy: 1.0, Computation time: 0.7775468826293945\n",
      "Step: 5841, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 0.9310545921325684\n",
      "Step: 5842, Loss: 0.9160282015800476, Accuracy: 1.0, Computation time: 0.8266937732696533\n",
      "Step: 5843, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 0.7475612163543701\n",
      "Step: 5844, Loss: 0.9159750938415527, Accuracy: 1.0, Computation time: 0.891559362411499\n",
      "Step: 5845, Loss: 0.9159675240516663, Accuracy: 1.0, Computation time: 0.9657361507415771\n",
      "Step: 5846, Loss: 0.9159548878669739, Accuracy: 1.0, Computation time: 0.9154980182647705\n",
      "Step: 5847, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 0.7820601463317871\n",
      "Step: 5848, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 1.314619779586792\n",
      "Step: 5849, Loss: 0.9364817142486572, Accuracy: 0.96875, Computation time: 0.9854238033294678\n",
      "Step: 5850, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 0.9902026653289795\n",
      "Step: 5851, Loss: 0.9177744388580322, Accuracy: 1.0, Computation time: 0.8438448905944824\n",
      "Step: 5852, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 0.9505062103271484\n",
      "Step: 5853, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 0.8496048450469971\n",
      "Step: 5854, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 0.7984247207641602\n",
      "Step: 5855, Loss: 0.9376417398452759, Accuracy: 0.96875, Computation time: 0.8483004570007324\n",
      "Step: 5856, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 0.8256325721740723\n",
      "Step: 5857, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 0.953481912612915\n",
      "Step: 5858, Loss: 0.9167835116386414, Accuracy: 1.0, Computation time: 0.9052066802978516\n",
      "Step: 5859, Loss: 0.9159348011016846, Accuracy: 1.0, Computation time: 0.9070608615875244\n",
      "Step: 5860, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.2620229721069336\n",
      "Step: 5861, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 0.9207077026367188\n",
      "Step: 5862, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 0.9722168445587158\n",
      "Step: 5863, Loss: 0.9158763289451599, Accuracy: 1.0, Computation time: 0.8063511848449707\n",
      "Step: 5864, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 0.924605131149292\n",
      "Step: 5865, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.2066988945007324\n",
      "Step: 5866, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 0.8245267868041992\n",
      "Step: 5867, Loss: 0.9375247359275818, Accuracy: 0.96875, Computation time: 0.7983005046844482\n",
      "Step: 5868, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 0.9821546077728271\n",
      "Step: 5869, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 0.8286547660827637\n",
      "Step: 5870, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.0965659618377686\n",
      "Step: 5871, Loss: 0.9375683069229126, Accuracy: 0.96875, Computation time: 0.8364081382751465\n",
      "Step: 5872, Loss: 0.932582676410675, Accuracy: 0.96875, Computation time: 1.0878992080688477\n",
      "Step: 5873, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.7469508647918701\n",
      "Step: 5874, Loss: 0.9377093315124512, Accuracy: 0.96875, Computation time: 0.910703182220459\n",
      "Step: 5875, Loss: 0.9246259331703186, Accuracy: 1.0, Computation time: 1.2330162525177002\n",
      "Step: 5876, Loss: 0.9159771800041199, Accuracy: 1.0, Computation time: 0.9200623035430908\n",
      "Step: 5877, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 0.8225610256195068\n",
      "Step: 5878, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 0.7818922996520996\n",
      "Step: 5879, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 0.8816418647766113\n",
      "Step: 5880, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.0196506977081299\n",
      "Step: 5881, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 0.9695980548858643\n",
      "Step: 5882, Loss: 0.9374868869781494, Accuracy: 0.96875, Computation time: 0.8930456638336182\n",
      "Step: 5883, Loss: 0.9160014390945435, Accuracy: 1.0, Computation time: 0.8563003540039062\n",
      "Step: 5884, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 0.9059708118438721\n",
      "Step: 5885, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 0.9972286224365234\n",
      "Step: 5886, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.1360976696014404\n",
      "Step: 5887, Loss: 0.9161869883537292, Accuracy: 1.0, Computation time: 1.016775131225586\n",
      "Step: 5888, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 0.991326093673706\n",
      "Step: 5889, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 0.9024958610534668\n",
      "Step: 5890, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 0.9497172832489014\n",
      "Step: 5891, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.0385932922363281\n",
      "Step: 5892, Loss: 0.9160767197608948, Accuracy: 1.0, Computation time: 0.8470399379730225\n",
      "Step: 5893, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.0036330223083496\n",
      "Step: 5894, Loss: 0.9375824928283691, Accuracy: 0.96875, Computation time: 1.0679340362548828\n",
      "Step: 5895, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 0.9230039119720459\n",
      "Step: 5896, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 0.7957015037536621\n",
      "Step: 5897, Loss: 0.9158357381820679, Accuracy: 1.0, Computation time: 0.8825526237487793\n",
      "Step: 5898, Loss: 0.9158387184143066, Accuracy: 1.0, Computation time: 1.2388405799865723\n",
      "Step: 5899, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.785508394241333\n",
      "Step: 5900, Loss: 0.9375425577163696, Accuracy: 0.96875, Computation time: 0.9440724849700928\n",
      "Step: 5901, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 0.9576723575592041\n",
      "Step: 5902, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.005735158920288\n",
      "Step: 5903, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.0546627044677734\n",
      "Step: 5904, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 0.8918311595916748\n",
      "Step: 5905, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 0.9342460632324219\n",
      "Step: 5906, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 0.9980325698852539\n",
      "Step: 5907, Loss: 0.9161115288734436, Accuracy: 1.0, Computation time: 1.1383874416351318\n",
      "Step: 5908, Loss: 0.9211183190345764, Accuracy: 1.0, Computation time: 0.8468248844146729\n",
      "Step: 5909, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.3336851596832275\n",
      "Step: 5910, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.1265065670013428\n",
      "Step: 5911, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.1024696826934814\n",
      "Step: 5912, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.0629158020019531\n",
      "Step: 5913, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 0.9345920085906982\n",
      "Step: 5914, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 0.9294443130493164\n",
      "Step: 5915, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 0.7911806106567383\n",
      "Step: 5916, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 0.8527188301086426\n",
      "Step: 5917, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 0.7918715476989746\n",
      "Step: 5918, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 0.8315000534057617\n",
      "Step: 5919, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 0.7804145812988281\n",
      "Step: 5920, Loss: 0.9170200824737549, Accuracy: 1.0, Computation time: 0.9560303688049316\n",
      "Step: 5921, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 0.7578139305114746\n",
      "Step: 5922, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 0.7661397457122803\n",
      "Step: 5923, Loss: 0.9498422145843506, Accuracy: 0.9375, Computation time: 0.8184342384338379\n",
      "Step: 5924, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 0.8044347763061523\n",
      "Step: 5925, Loss: 0.9161787629127502, Accuracy: 1.0, Computation time: 0.8259439468383789\n",
      "Step: 5926, Loss: 0.9183396100997925, Accuracy: 1.0, Computation time: 0.8441073894500732\n",
      "Step: 5927, Loss: 0.9160191416740417, Accuracy: 1.0, Computation time: 0.6457984447479248\n",
      "Step: 5928, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 0.6826846599578857\n",
      "Step: 5929, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 0.7301003932952881\n",
      "Step: 5930, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 0.6826634407043457\n",
      "Step: 5931, Loss: 0.9218631982803345, Accuracy: 1.0, Computation time: 0.9722497463226318\n",
      "Step: 5932, Loss: 0.9162728190422058, Accuracy: 1.0, Computation time: 0.6765227317810059\n",
      "Step: 5933, Loss: 0.9160993099212646, Accuracy: 1.0, Computation time: 0.783090353012085\n",
      "Step: 5934, Loss: 0.9163007140159607, Accuracy: 1.0, Computation time: 0.725212574005127\n",
      "Step: 5935, Loss: 0.9160871505737305, Accuracy: 1.0, Computation time: 0.784276008605957\n",
      "Step: 5936, Loss: 0.9365457892417908, Accuracy: 0.96875, Computation time: 0.9049887657165527\n",
      "Step: 5937, Loss: 0.9159930348396301, Accuracy: 1.0, Computation time: 0.7028372287750244\n",
      "Step: 5938, Loss: 0.9161336421966553, Accuracy: 1.0, Computation time: 0.7084221839904785\n",
      "Step: 5939, Loss: 0.9182522296905518, Accuracy: 1.0, Computation time: 0.8786909580230713\n",
      "Step: 5940, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 0.9022552967071533\n",
      "Step: 5941, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 0.7170639038085938\n",
      "Step: 5942, Loss: 0.9592570662498474, Accuracy: 0.9375, Computation time: 0.8450474739074707\n",
      "Step: 5943, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 0.6975793838500977\n",
      "Step: 5944, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 0.7958517074584961\n",
      "Step: 5945, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 0.9068770408630371\n",
      "Step: 5946, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 0.8207452297210693\n",
      "Step: 5947, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 0.7868914604187012\n",
      "Step: 5948, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 0.8755993843078613\n",
      "Step: 5949, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.8838269710540771\n",
      "Step: 5950, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 0.9080286026000977\n",
      "Step: 5951, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.020594835281372\n",
      "Step: 5952, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.7855679988861084\n",
      "Step: 5953, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 0.8219983577728271\n",
      "Step: 5954, Loss: 0.9375659823417664, Accuracy: 0.96875, Computation time: 0.7775259017944336\n",
      "Step: 5955, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 0.9217069149017334\n",
      "Step: 5956, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 0.9100713729858398\n",
      "Step: 5957, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 0.974571943283081\n",
      "Step: 5958, Loss: 0.9160593152046204, Accuracy: 1.0, Computation time: 0.8838338851928711\n",
      "Step: 5959, Loss: 0.9179335236549377, Accuracy: 1.0, Computation time: 0.9026224613189697\n",
      "Step: 5960, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 0.8274235725402832\n",
      "Step: 5961, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.1352453231811523\n",
      "Step: 5962, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.7330608367919922\n",
      "Step: 5963, Loss: 0.9375206232070923, Accuracy: 0.96875, Computation time: 0.7388768196105957\n",
      "Step: 5964, Loss: 0.9160562753677368, Accuracy: 1.0, Computation time: 1.4680421352386475\n",
      "Step: 5965, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 0.8704888820648193\n",
      "Step: 5966, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 0.8586390018463135\n",
      "Step: 5967, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 0.8385968208312988\n",
      "Step: 5968, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 0.7482035160064697\n",
      "Step: 5969, Loss: 0.93753582239151, Accuracy: 0.96875, Computation time: 0.8936464786529541\n",
      "Step: 5970, Loss: 0.937459409236908, Accuracy: 0.96875, Computation time: 0.7696371078491211\n",
      "Step: 5971, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 0.8483049869537354\n",
      "Step: 5972, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.0730037689208984\n",
      "Step: 5973, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 0.7837200164794922\n",
      "Step: 5974, Loss: 0.9376724362373352, Accuracy: 0.96875, Computation time: 0.7618975639343262\n",
      "Step: 5975, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 0.7723782062530518\n",
      "########################\n",
      "Test loss: 1.0702216625213623, Test Accuracy_epoch43: 0.7761485576629639\n",
      "########################\n",
      "Step: 5976, Loss: 0.9374628067016602, Accuracy: 0.96875, Computation time: 1.9095783233642578\n",
      "Step: 5977, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 0.8805406093597412\n",
      "Step: 5978, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.0784416198730469\n",
      "Step: 5979, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 0.8245298862457275\n",
      "Step: 5980, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 0.9282364845275879\n",
      "Step: 5981, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 0.8794679641723633\n",
      "Step: 5982, Loss: 0.9158393740653992, Accuracy: 1.0, Computation time: 0.7903144359588623\n",
      "Step: 5983, Loss: 0.9159567356109619, Accuracy: 1.0, Computation time: 0.8456771373748779\n",
      "Step: 5984, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.1096158027648926\n",
      "Step: 5985, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.071976661682129\n",
      "Step: 5986, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 0.7946527004241943\n",
      "Step: 5987, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 0.928790807723999\n",
      "Step: 5988, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 0.7788891792297363\n",
      "Step: 5989, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 0.8851439952850342\n",
      "Step: 5990, Loss: 0.9164026975631714, Accuracy: 1.0, Computation time: 1.0191822052001953\n",
      "Step: 5991, Loss: 0.9338003396987915, Accuracy: 0.96875, Computation time: 0.9950475692749023\n",
      "Step: 5992, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 0.8475401401519775\n",
      "Step: 5993, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 0.8306682109832764\n",
      "Step: 5994, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 0.9699883460998535\n",
      "Step: 5995, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 0.9421207904815674\n",
      "Step: 5996, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 0.868981122970581\n",
      "Step: 5997, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 1.197669267654419\n",
      "Step: 5998, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.1840503215789795\n",
      "Step: 5999, Loss: 0.937406599521637, Accuracy: 0.96875, Computation time: 0.9172749519348145\n",
      "Step: 6000, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 1.0737807750701904\n",
      "Step: 6001, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.2037570476531982\n",
      "Step: 6002, Loss: 0.915834903717041, Accuracy: 1.0, Computation time: 0.7558331489562988\n",
      "Step: 6003, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.074380874633789\n",
      "Step: 6004, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 1.2457835674285889\n",
      "Step: 6005, Loss: 0.9158369898796082, Accuracy: 1.0, Computation time: 1.001295566558838\n",
      "Step: 6006, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.3245823383331299\n",
      "Step: 6007, Loss: 0.9158350229263306, Accuracy: 1.0, Computation time: 1.2101361751556396\n",
      "Step: 6008, Loss: 0.9158377647399902, Accuracy: 1.0, Computation time: 1.0523619651794434\n",
      "Step: 6009, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 0.9778347015380859\n",
      "Step: 6010, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 1.2380118370056152\n",
      "Step: 6011, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.1007084846496582\n",
      "Step: 6012, Loss: 0.9161444902420044, Accuracy: 1.0, Computation time: 1.11873459815979\n",
      "Step: 6013, Loss: 0.9375178813934326, Accuracy: 0.96875, Computation time: 1.2400658130645752\n",
      "Step: 6014, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 0.882854700088501\n",
      "Step: 6015, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.3667242527008057\n",
      "Step: 6016, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 0.967663049697876\n",
      "Step: 6017, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 0.9340336322784424\n",
      "Step: 6018, Loss: 0.9159061312675476, Accuracy: 1.0, Computation time: 0.9194841384887695\n",
      "Step: 6019, Loss: 0.9374562501907349, Accuracy: 0.96875, Computation time: 0.9941201210021973\n",
      "Step: 6020, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 1.050994873046875\n",
      "Step: 6021, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.1011559963226318\n",
      "Step: 6022, Loss: 0.9158357977867126, Accuracy: 1.0, Computation time: 1.0365691184997559\n",
      "Step: 6023, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.1195285320281982\n",
      "Step: 6024, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.1343019008636475\n",
      "Step: 6025, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 0.8896093368530273\n",
      "Step: 6026, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 1.2757210731506348\n",
      "Step: 6027, Loss: 0.9158397316932678, Accuracy: 1.0, Computation time: 1.1216731071472168\n",
      "Step: 6028, Loss: 0.9160240888595581, Accuracy: 1.0, Computation time: 0.7953684329986572\n",
      "Step: 6029, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.0883152484893799\n",
      "Step: 6030, Loss: 0.9158384799957275, Accuracy: 1.0, Computation time: 1.1671361923217773\n",
      "Step: 6031, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.1249749660491943\n",
      "Step: 6032, Loss: 0.9158352613449097, Accuracy: 1.0, Computation time: 0.8391005992889404\n",
      "Step: 6033, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.1149265766143799\n",
      "Step: 6034, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 0.9771246910095215\n",
      "Step: 6035, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 0.9779605865478516\n",
      "Step: 6036, Loss: 0.9374955892562866, Accuracy: 0.96875, Computation time: 1.0092053413391113\n",
      "Step: 6037, Loss: 0.9158324003219604, Accuracy: 1.0, Computation time: 1.10374116897583\n",
      "Step: 6038, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.3278594017028809\n",
      "Step: 6039, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.3686633110046387\n",
      "Step: 6040, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 1.106999158859253\n",
      "Step: 6041, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 0.9474189281463623\n",
      "Step: 6042, Loss: 0.9158337116241455, Accuracy: 1.0, Computation time: 1.0340466499328613\n",
      "Step: 6043, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 1.0340914726257324\n",
      "Step: 6044, Loss: 0.916111409664154, Accuracy: 1.0, Computation time: 1.1323227882385254\n",
      "Step: 6045, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.1963574886322021\n",
      "Step: 6046, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.1701734066009521\n",
      "Step: 6047, Loss: 0.9160804152488708, Accuracy: 1.0, Computation time: 1.0222244262695312\n",
      "Step: 6048, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.0330026149749756\n",
      "Step: 6049, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.05747652053833\n",
      "Step: 6050, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 1.3797955513000488\n",
      "Step: 6051, Loss: 0.9374603033065796, Accuracy: 0.96875, Computation time: 0.9731879234313965\n",
      "Step: 6052, Loss: 0.9374938011169434, Accuracy: 0.96875, Computation time: 0.9426963329315186\n",
      "Step: 6053, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.209151268005371\n",
      "Step: 6054, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.1295502185821533\n",
      "Step: 6055, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 0.9779002666473389\n",
      "Step: 6056, Loss: 0.9158381819725037, Accuracy: 1.0, Computation time: 0.9819416999816895\n",
      "Step: 6057, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 0.9902117252349854\n",
      "Step: 6058, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 1.176072120666504\n",
      "Step: 6059, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.2729949951171875\n",
      "Step: 6060, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 1.0961604118347168\n",
      "Step: 6061, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 0.965994119644165\n",
      "Step: 6062, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.060486078262329\n",
      "Step: 6063, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 1.2808842658996582\n",
      "Step: 6064, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 0.9023826122283936\n",
      "Step: 6065, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.0611286163330078\n",
      "Step: 6066, Loss: 0.9375208020210266, Accuracy: 0.96875, Computation time: 1.0831758975982666\n",
      "Step: 6067, Loss: 0.915835976600647, Accuracy: 1.0, Computation time: 1.063122272491455\n",
      "Step: 6068, Loss: 0.9158377051353455, Accuracy: 1.0, Computation time: 1.0019986629486084\n",
      "Step: 6069, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.072479009628296\n",
      "Step: 6070, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.0878705978393555\n",
      "Step: 6071, Loss: 0.9158370494842529, Accuracy: 1.0, Computation time: 0.9737529754638672\n",
      "Step: 6072, Loss: 0.9158400297164917, Accuracy: 1.0, Computation time: 0.9869434833526611\n",
      "Step: 6073, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.0652778148651123\n",
      "Step: 6074, Loss: 0.9158397912979126, Accuracy: 1.0, Computation time: 1.3262262344360352\n",
      "Step: 6075, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.2424368858337402\n",
      "Step: 6076, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.0750856399536133\n",
      "Step: 6077, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.8613197803497314\n",
      "Step: 6078, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.2211356163024902\n",
      "Step: 6079, Loss: 0.9377555847167969, Accuracy: 0.96875, Computation time: 0.9899544715881348\n",
      "Step: 6080, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.0660498142242432\n",
      "Step: 6081, Loss: 0.9158337116241455, Accuracy: 1.0, Computation time: 1.0206756591796875\n",
      "Step: 6082, Loss: 0.9187409281730652, Accuracy: 1.0, Computation time: 1.0913960933685303\n",
      "Step: 6083, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.1116034984588623\n",
      "Step: 6084, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.2606234550476074\n",
      "Step: 6085, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 0.9447131156921387\n",
      "Step: 6086, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.1087963581085205\n",
      "Step: 6087, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.152979850769043\n",
      "Step: 6088, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.1367111206054688\n",
      "Step: 6089, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 0.98893141746521\n",
      "Step: 6090, Loss: 0.9158340692520142, Accuracy: 1.0, Computation time: 1.2138831615447998\n",
      "Step: 6091, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.0777697563171387\n",
      "Step: 6092, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.029264211654663\n",
      "Step: 6093, Loss: 0.9374920725822449, Accuracy: 0.96875, Computation time: 1.0823500156402588\n",
      "Step: 6094, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.2567074298858643\n",
      "Step: 6095, Loss: 0.9161757826805115, Accuracy: 1.0, Computation time: 1.0615286827087402\n",
      "Step: 6096, Loss: 0.9158331155776978, Accuracy: 1.0, Computation time: 0.9225428104400635\n",
      "Step: 6097, Loss: 0.9158346056938171, Accuracy: 1.0, Computation time: 1.2453067302703857\n",
      "Step: 6098, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 1.0333678722381592\n",
      "Step: 6099, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 0.9282822608947754\n",
      "Step: 6100, Loss: 0.9158521294593811, Accuracy: 1.0, Computation time: 0.8834381103515625\n",
      "Step: 6101, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.709282636642456\n",
      "Step: 6102, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.1064989566802979\n",
      "Step: 6103, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.027134656906128\n",
      "Step: 6104, Loss: 0.9158336520195007, Accuracy: 1.0, Computation time: 0.8079366683959961\n",
      "Step: 6105, Loss: 0.9375963807106018, Accuracy: 0.96875, Computation time: 1.094869613647461\n",
      "Step: 6106, Loss: 0.9158372282981873, Accuracy: 1.0, Computation time: 0.8898015022277832\n",
      "Step: 6107, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 0.9226007461547852\n",
      "Step: 6108, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 0.9531171321868896\n",
      "Step: 6109, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 0.8999695777893066\n",
      "Step: 6110, Loss: 0.9158401489257812, Accuracy: 1.0, Computation time: 0.9177536964416504\n",
      "Step: 6111, Loss: 0.91583651304245, Accuracy: 1.0, Computation time: 0.8619959354400635\n",
      "Step: 6112, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.0256099700927734\n",
      "Step: 6113, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 0.8694946765899658\n",
      "Step: 6114, Loss: 0.9375302791595459, Accuracy: 0.96875, Computation time: 0.8930187225341797\n",
      "########################\n",
      "Test loss: 1.069491982460022, Test Accuracy_epoch44: 0.7751710414886475\n",
      "########################\n",
      "Step: 6115, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.4616055488586426\n",
      "Step: 6116, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.0536062717437744\n",
      "Step: 6117, Loss: 0.9375, Accuracy: 0.96875, Computation time: 0.9363613128662109\n",
      "Step: 6118, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 0.9289922714233398\n",
      "Step: 6119, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 0.7535059452056885\n",
      "Step: 6120, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 0.9827616214752197\n",
      "Step: 6121, Loss: 0.9168083667755127, Accuracy: 1.0, Computation time: 0.9061019420623779\n",
      "Step: 6122, Loss: 0.9366562366485596, Accuracy: 0.96875, Computation time: 0.9690985679626465\n",
      "Step: 6123, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.4852213859558105\n",
      "Step: 6124, Loss: 0.9309736490249634, Accuracy: 0.96875, Computation time: 0.8803918361663818\n",
      "Step: 6125, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 0.9885854721069336\n",
      "Step: 6126, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.2527968883514404\n",
      "Step: 6127, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.2254397869110107\n",
      "Step: 6128, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 0.9377105236053467\n",
      "Step: 6129, Loss: 0.9361187219619751, Accuracy: 0.96875, Computation time: 1.0975546836853027\n",
      "Step: 6130, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 1.0424268245697021\n",
      "Step: 6131, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.4316298961639404\n",
      "Step: 6132, Loss: 0.916153073310852, Accuracy: 1.0, Computation time: 0.9789829254150391\n",
      "Step: 6133, Loss: 0.9159632325172424, Accuracy: 1.0, Computation time: 1.166541337966919\n",
      "Step: 6134, Loss: 0.9173987507820129, Accuracy: 1.0, Computation time: 1.2547903060913086\n",
      "Step: 6135, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.9685628414154053\n",
      "Step: 6136, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 1.0718002319335938\n",
      "Step: 6137, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 1.0749783515930176\n",
      "Step: 6138, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 0.9784479141235352\n",
      "Step: 6139, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 0.8953592777252197\n",
      "Step: 6140, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 0.9635236263275146\n",
      "Step: 6141, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 0.9714255332946777\n",
      "Step: 6142, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 0.8847479820251465\n",
      "Step: 6143, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 1.155041217803955\n",
      "Step: 6144, Loss: 0.9375449419021606, Accuracy: 0.96875, Computation time: 1.0713534355163574\n",
      "Step: 6145, Loss: 0.937254786491394, Accuracy: 0.96875, Computation time: 1.3823497295379639\n",
      "Step: 6146, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.0568091869354248\n",
      "Step: 6147, Loss: 0.9375306963920593, Accuracy: 0.96875, Computation time: 0.8348913192749023\n",
      "Step: 6148, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 0.8795421123504639\n",
      "Step: 6149, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 0.9082958698272705\n",
      "Step: 6150, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.2727885246276855\n",
      "Step: 6151, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 0.868295431137085\n",
      "Step: 6152, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 0.8226006031036377\n",
      "Step: 6153, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 0.9574425220489502\n",
      "Step: 6154, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 0.8576943874359131\n",
      "Step: 6155, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.054445505142212\n",
      "Step: 6156, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 0.824505090713501\n",
      "Step: 6157, Loss: 0.9374310374259949, Accuracy: 0.96875, Computation time: 0.8173296451568604\n",
      "Step: 6158, Loss: 0.9375811219215393, Accuracy: 0.96875, Computation time: 0.9236688613891602\n",
      "Step: 6159, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 0.7890245914459229\n",
      "Step: 6160, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 2.3384876251220703\n",
      "Step: 6161, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 0.8443470001220703\n",
      "Step: 6162, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 0.9911766052246094\n",
      "Step: 6163, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.113121747970581\n",
      "Step: 6164, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 0.9235947132110596\n",
      "Step: 6165, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.0374820232391357\n",
      "Step: 6166, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 0.8467707633972168\n",
      "Step: 6167, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 0.8770885467529297\n",
      "Step: 6168, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 1.0599114894866943\n",
      "Step: 6169, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 0.8923873901367188\n",
      "Step: 6170, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.219736099243164\n",
      "Step: 6171, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 0.9777188301086426\n",
      "Step: 6172, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 0.9339807033538818\n",
      "Step: 6173, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 0.9264817237854004\n",
      "Step: 6174, Loss: 0.9192617535591125, Accuracy: 1.0, Computation time: 1.7949767112731934\n",
      "Step: 6175, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 0.9002809524536133\n",
      "Step: 6176, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.162780523300171\n",
      "Step: 6177, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.0143721103668213\n",
      "Step: 6178, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.7584071159362793\n",
      "Step: 6179, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.0308070182800293\n",
      "Step: 6180, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 0.9490697383880615\n",
      "Step: 6181, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 0.9420812129974365\n",
      "Step: 6182, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.5906529426574707\n",
      "Step: 6183, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.3367109298706055\n",
      "Step: 6184, Loss: 0.9388501644134521, Accuracy: 0.96875, Computation time: 1.0931789875030518\n",
      "Step: 6185, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.0960509777069092\n",
      "Step: 6186, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 0.9480080604553223\n",
      "Step: 6187, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.009993314743042\n",
      "Step: 6188, Loss: 0.937571108341217, Accuracy: 0.96875, Computation time: 1.0125131607055664\n",
      "Step: 6189, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 0.912571907043457\n",
      "Step: 6190, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.1052823066711426\n",
      "Step: 6191, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 0.8379232883453369\n",
      "Step: 6192, Loss: 0.9479936361312866, Accuracy: 0.9375, Computation time: 1.1481876373291016\n",
      "Step: 6193, Loss: 0.9159358739852905, Accuracy: 1.0, Computation time: 0.9396908283233643\n",
      "Step: 6194, Loss: 0.9162043333053589, Accuracy: 1.0, Computation time: 1.2273991107940674\n",
      "Step: 6195, Loss: 0.9162659645080566, Accuracy: 1.0, Computation time: 1.0010976791381836\n",
      "Step: 6196, Loss: 0.934414267539978, Accuracy: 0.96875, Computation time: 1.039358139038086\n",
      "Step: 6197, Loss: 0.9159423112869263, Accuracy: 1.0, Computation time: 0.938014030456543\n",
      "Step: 6198, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 0.8726072311401367\n",
      "Step: 6199, Loss: 0.9375180602073669, Accuracy: 0.96875, Computation time: 0.8690743446350098\n",
      "Step: 6200, Loss: 0.9188932776451111, Accuracy: 1.0, Computation time: 0.8575930595397949\n",
      "Step: 6201, Loss: 0.9159470200538635, Accuracy: 1.0, Computation time: 0.9492025375366211\n",
      "Step: 6202, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 0.8245851993560791\n",
      "Step: 6203, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 1.1021463871002197\n",
      "Step: 6204, Loss: 0.9376343488693237, Accuracy: 0.96875, Computation time: 0.940025806427002\n",
      "Step: 6205, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 0.9102637767791748\n",
      "Step: 6206, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 0.963475227355957\n",
      "Step: 6207, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 0.9819962978363037\n",
      "Step: 6208, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 0.9567403793334961\n",
      "Step: 6209, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 0.8198251724243164\n",
      "Step: 6210, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.0630779266357422\n",
      "Step: 6211, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 0.8901083469390869\n",
      "Step: 6212, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.1482579708099365\n",
      "Step: 6213, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 0.8129091262817383\n",
      "Step: 6214, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.0611507892608643\n",
      "Step: 6215, Loss: 0.915835976600647, Accuracy: 1.0, Computation time: 1.2056350708007812\n",
      "Step: 6216, Loss: 0.9158387184143066, Accuracy: 1.0, Computation time: 0.8650023937225342\n",
      "Step: 6217, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.0887811183929443\n",
      "Step: 6218, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 0.9052064418792725\n",
      "Step: 6219, Loss: 0.9192181825637817, Accuracy: 1.0, Computation time: 1.1492030620574951\n",
      "Step: 6220, Loss: 0.9161245226860046, Accuracy: 1.0, Computation time: 0.835118293762207\n",
      "Step: 6221, Loss: 0.9158409833908081, Accuracy: 1.0, Computation time: 1.0472526550292969\n",
      "Step: 6222, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 0.804408073425293\n",
      "Step: 6223, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.9882991313934326\n",
      "Step: 6224, Loss: 0.9158763289451599, Accuracy: 1.0, Computation time: 0.9476809501647949\n",
      "Step: 6225, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 0.7579023838043213\n",
      "Step: 6226, Loss: 0.9160628318786621, Accuracy: 1.0, Computation time: 0.8581609725952148\n",
      "Step: 6227, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 0.9079415798187256\n",
      "Step: 6228, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.076735019683838\n",
      "Step: 6229, Loss: 0.9158366322517395, Accuracy: 1.0, Computation time: 0.8670971393585205\n",
      "Step: 6230, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.0230076313018799\n",
      "Step: 6231, Loss: 0.9172915816307068, Accuracy: 1.0, Computation time: 1.068204641342163\n",
      "Step: 6232, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 1.0413603782653809\n",
      "Step: 6233, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 0.7810947895050049\n",
      "Step: 6234, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 0.7671253681182861\n",
      "Step: 6235, Loss: 0.9160090088844299, Accuracy: 1.0, Computation time: 0.7363660335540771\n",
      "Step: 6236, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 0.7524409294128418\n",
      "Step: 6237, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 0.9511051177978516\n",
      "Step: 6238, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 0.9279036521911621\n",
      "Step: 6239, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 1.107245922088623\n",
      "Step: 6240, Loss: 0.9375286102294922, Accuracy: 0.96875, Computation time: 0.82356858253479\n",
      "Step: 6241, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 0.9906392097473145\n",
      "Step: 6242, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 0.855410099029541\n",
      "Step: 6243, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.2151367664337158\n",
      "Step: 6244, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 0.9991340637207031\n",
      "Step: 6245, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 0.8703138828277588\n",
      "Step: 6246, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 0.9329824447631836\n",
      "Step: 6247, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.1400797367095947\n",
      "Step: 6248, Loss: 0.9253177642822266, Accuracy: 0.96875, Computation time: 1.05552077293396\n",
      "Step: 6249, Loss: 0.9161641001701355, Accuracy: 1.0, Computation time: 0.8506166934967041\n",
      "Step: 6250, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 0.7999317646026611\n",
      "Step: 6251, Loss: 0.9375713467597961, Accuracy: 0.96875, Computation time: 0.8867309093475342\n",
      "Step: 6252, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 0.8383114337921143\n",
      "Step: 6253, Loss: 0.9358630776405334, Accuracy: 0.96875, Computation time: 0.7732734680175781\n",
      "########################\n",
      "Test loss: 1.0664076805114746, Test Accuracy_epoch45: 0.7820136547088623\n",
      "########################\n",
      "Step: 6254, Loss: 0.9159598350524902, Accuracy: 1.0, Computation time: 0.7819862365722656\n",
      "Step: 6255, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 0.8777565956115723\n",
      "Step: 6256, Loss: 0.9344803094863892, Accuracy: 0.96875, Computation time: 1.380507230758667\n",
      "Step: 6257, Loss: 0.9176783561706543, Accuracy: 1.0, Computation time: 1.10835599899292\n",
      "Step: 6258, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.7307431697845459\n",
      "Step: 6259, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 0.7128100395202637\n",
      "Step: 6260, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 1.172609567642212\n",
      "Step: 6261, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 0.9602329730987549\n",
      "Step: 6262, Loss: 0.9159471988677979, Accuracy: 1.0, Computation time: 0.9679880142211914\n",
      "Step: 6263, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 0.8785741329193115\n",
      "Step: 6264, Loss: 0.9392054080963135, Accuracy: 0.96875, Computation time: 0.7691161632537842\n",
      "Step: 6265, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 0.8131732940673828\n",
      "Step: 6266, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 0.8057482242584229\n",
      "Step: 6267, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 0.7788784503936768\n",
      "Step: 6268, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 0.7932748794555664\n",
      "Step: 6269, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 0.768049955368042\n",
      "Step: 6270, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 0.8126876354217529\n",
      "Step: 6271, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.1907005310058594\n",
      "Step: 6272, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 0.8555052280426025\n",
      "Step: 6273, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 0.7757408618927002\n",
      "Step: 6274, Loss: 0.9376839995384216, Accuracy: 0.96875, Computation time: 0.8683786392211914\n",
      "Step: 6275, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 0.7527120113372803\n",
      "Step: 6276, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 1.2141687870025635\n",
      "Step: 6277, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 0.7304866313934326\n",
      "Step: 6278, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 0.7605857849121094\n",
      "Step: 6279, Loss: 0.9160557985305786, Accuracy: 1.0, Computation time: 1.1138925552368164\n",
      "Step: 6280, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 0.7931995391845703\n",
      "Step: 6281, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 0.9510037899017334\n",
      "Step: 6282, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 0.8241016864776611\n",
      "Step: 6283, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 0.7910017967224121\n",
      "Step: 6284, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 0.8801321983337402\n",
      "Step: 6285, Loss: 0.9375653266906738, Accuracy: 0.96875, Computation time: 0.7850627899169922\n",
      "Step: 6286, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 0.9451613426208496\n",
      "Step: 6287, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 1.0738353729248047\n",
      "Step: 6288, Loss: 0.9175773859024048, Accuracy: 1.0, Computation time: 0.9676170349121094\n",
      "Step: 6289, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 0.7703800201416016\n",
      "Step: 6290, Loss: 0.9376344680786133, Accuracy: 0.96875, Computation time: 0.786677360534668\n",
      "Step: 6291, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 0.8277733325958252\n",
      "Step: 6292, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 0.8159434795379639\n",
      "Step: 6293, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.8126513957977295\n",
      "Step: 6294, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 0.9338188171386719\n",
      "Step: 6295, Loss: 0.9379485845565796, Accuracy: 0.96875, Computation time: 0.9429478645324707\n",
      "Step: 6296, Loss: 0.9375426769256592, Accuracy: 0.96875, Computation time: 0.951223611831665\n",
      "Step: 6297, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 0.7872793674468994\n",
      "Step: 6298, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 1.1229374408721924\n",
      "Step: 6299, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.0101044178009033\n",
      "Step: 6300, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 0.7254953384399414\n",
      "Step: 6301, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 0.8641424179077148\n",
      "Step: 6302, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 0.929332971572876\n",
      "Step: 6303, Loss: 0.9202619194984436, Accuracy: 1.0, Computation time: 1.0230567455291748\n",
      "Step: 6304, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 0.8378827571868896\n",
      "Step: 6305, Loss: 0.9359422922134399, Accuracy: 0.96875, Computation time: 1.0557208061218262\n",
      "Step: 6306, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 0.8182535171508789\n",
      "Step: 6307, Loss: 0.9159923195838928, Accuracy: 1.0, Computation time: 0.899336576461792\n",
      "Step: 6308, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.863673210144043\n",
      "Step: 6309, Loss: 0.9375830888748169, Accuracy: 0.96875, Computation time: 0.8455729484558105\n",
      "Step: 6310, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 0.7211523056030273\n",
      "Step: 6311, Loss: 0.9377384781837463, Accuracy: 0.96875, Computation time: 0.9611473083496094\n",
      "Step: 6312, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 0.9892358779907227\n",
      "Step: 6313, Loss: 0.9298692941665649, Accuracy: 0.96875, Computation time: 1.0866599082946777\n",
      "Step: 6314, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 0.8649032115936279\n",
      "Step: 6315, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 0.8402924537658691\n",
      "Step: 6316, Loss: 0.9159368872642517, Accuracy: 1.0, Computation time: 1.088808536529541\n",
      "Step: 6317, Loss: 0.9162039756774902, Accuracy: 1.0, Computation time: 1.0417633056640625\n",
      "Step: 6318, Loss: 0.9160146713256836, Accuracy: 1.0, Computation time: 1.1345858573913574\n",
      "Step: 6319, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 0.9280405044555664\n",
      "Step: 6320, Loss: 0.9166693091392517, Accuracy: 1.0, Computation time: 1.0636718273162842\n",
      "Step: 6321, Loss: 0.9375365972518921, Accuracy: 0.96875, Computation time: 0.9110898971557617\n",
      "Step: 6322, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.0459678173065186\n",
      "Step: 6323, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 0.8741416931152344\n",
      "Step: 6324, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 0.8896424770355225\n",
      "Step: 6325, Loss: 0.9160966873168945, Accuracy: 1.0, Computation time: 0.970789909362793\n",
      "Step: 6326, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.0616893768310547\n",
      "Step: 6327, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 0.879131555557251\n",
      "Step: 6328, Loss: 0.9376188516616821, Accuracy: 0.96875, Computation time: 1.2989943027496338\n",
      "Step: 6329, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.0024213790893555\n",
      "Step: 6330, Loss: 0.9160214066505432, Accuracy: 1.0, Computation time: 1.3526058197021484\n",
      "Step: 6331, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 0.9016177654266357\n",
      "Step: 6332, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 0.9994800090789795\n",
      "Step: 6333, Loss: 0.915945291519165, Accuracy: 1.0, Computation time: 1.0215699672698975\n",
      "Step: 6334, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.008453607559204\n",
      "Step: 6335, Loss: 0.916164219379425, Accuracy: 1.0, Computation time: 1.0641605854034424\n",
      "Step: 6336, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 0.97867751121521\n",
      "Step: 6337, Loss: 0.93351149559021, Accuracy: 0.96875, Computation time: 1.332784652709961\n",
      "Step: 6338, Loss: 0.9159700274467468, Accuracy: 1.0, Computation time: 0.8523309230804443\n",
      "Step: 6339, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 0.9838099479675293\n",
      "Step: 6340, Loss: 0.9375971555709839, Accuracy: 0.96875, Computation time: 0.9892590045928955\n",
      "Step: 6341, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 0.9233007431030273\n",
      "Step: 6342, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.1923260688781738\n",
      "Step: 6343, Loss: 0.916412353515625, Accuracy: 1.0, Computation time: 0.9866926670074463\n",
      "Step: 6344, Loss: 0.9338139295578003, Accuracy: 0.96875, Computation time: 0.9199984073638916\n",
      "Step: 6345, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 0.9488089084625244\n",
      "Step: 6346, Loss: 0.9375457167625427, Accuracy: 0.96875, Computation time: 0.8475837707519531\n",
      "Step: 6347, Loss: 0.9166524410247803, Accuracy: 1.0, Computation time: 1.078117847442627\n",
      "Step: 6348, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 0.8255312442779541\n",
      "Step: 6349, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.0431876182556152\n",
      "Step: 6350, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 0.9836809635162354\n",
      "Step: 6351, Loss: 0.9171551465988159, Accuracy: 1.0, Computation time: 1.0865070819854736\n",
      "Step: 6352, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 0.8743441104888916\n",
      "Step: 6353, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.1961455345153809\n",
      "Step: 6354, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 0.8413238525390625\n",
      "Step: 6355, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 0.8460111618041992\n",
      "Step: 6356, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.029090166091919\n",
      "Step: 6357, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 0.9946706295013428\n",
      "Step: 6358, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 0.8299193382263184\n",
      "Step: 6359, Loss: 0.9202610850334167, Accuracy: 1.0, Computation time: 0.8530123233795166\n",
      "Step: 6360, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 0.91383957862854\n",
      "Step: 6361, Loss: 0.959179699420929, Accuracy: 0.9375, Computation time: 0.8574142456054688\n",
      "Step: 6362, Loss: 0.915958046913147, Accuracy: 1.0, Computation time: 0.9884107112884521\n",
      "Step: 6363, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.070859432220459\n",
      "Step: 6364, Loss: 0.9159671068191528, Accuracy: 1.0, Computation time: 0.8916404247283936\n",
      "Step: 6365, Loss: 0.937663197517395, Accuracy: 0.96875, Computation time: 0.885725736618042\n",
      "Step: 6366, Loss: 0.9159094095230103, Accuracy: 1.0, Computation time: 1.1748673915863037\n",
      "Step: 6367, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 0.884974479675293\n",
      "Step: 6368, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.065591812133789\n",
      "Step: 6369, Loss: 0.9337174296379089, Accuracy: 0.96875, Computation time: 0.8737859725952148\n",
      "Step: 6370, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 0.8188323974609375\n",
      "Step: 6371, Loss: 0.9159119129180908, Accuracy: 1.0, Computation time: 0.8299119472503662\n",
      "Step: 6372, Loss: 0.9166946411132812, Accuracy: 1.0, Computation time: 1.148380994796753\n",
      "Step: 6373, Loss: 0.9159526228904724, Accuracy: 1.0, Computation time: 0.955561637878418\n",
      "Step: 6374, Loss: 0.9190483689308167, Accuracy: 1.0, Computation time: 1.1164710521697998\n",
      "Step: 6375, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 0.9631893634796143\n",
      "Step: 6376, Loss: 0.9160297513008118, Accuracy: 1.0, Computation time: 0.8589324951171875\n",
      "Step: 6377, Loss: 0.9162059426307678, Accuracy: 1.0, Computation time: 1.714836597442627\n",
      "Step: 6378, Loss: 0.9162408709526062, Accuracy: 1.0, Computation time: 0.9841461181640625\n",
      "Step: 6379, Loss: 0.9160111546516418, Accuracy: 1.0, Computation time: 1.0873441696166992\n",
      "Step: 6380, Loss: 0.9160106778144836, Accuracy: 1.0, Computation time: 0.8560762405395508\n",
      "Step: 6381, Loss: 0.9159311056137085, Accuracy: 1.0, Computation time: 0.8294670581817627\n",
      "Step: 6382, Loss: 0.9376204013824463, Accuracy: 0.96875, Computation time: 0.8319015502929688\n",
      "Step: 6383, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 0.8631882667541504\n",
      "Step: 6384, Loss: 0.9158950448036194, Accuracy: 1.0, Computation time: 1.0314502716064453\n",
      "Step: 6385, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 0.8891627788543701\n",
      "Step: 6386, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 0.8461377620697021\n",
      "Step: 6387, Loss: 0.9160177111625671, Accuracy: 1.0, Computation time: 0.8794310092926025\n",
      "Step: 6388, Loss: 0.9229314923286438, Accuracy: 1.0, Computation time: 0.9653477668762207\n",
      "Step: 6389, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 0.8299689292907715\n",
      "Step: 6390, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 0.7817568778991699\n",
      "Step: 6391, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 0.9622402191162109\n",
      "Step: 6392, Loss: 0.9160453677177429, Accuracy: 1.0, Computation time: 0.9180035591125488\n",
      "########################\n",
      "Test loss: 1.0680803060531616, Test Accuracy_epoch46: 0.7800586223602295\n",
      "########################\n",
      "Step: 6393, Loss: 0.9160792827606201, Accuracy: 1.0, Computation time: 1.066314458847046\n",
      "Step: 6394, Loss: 0.9159448742866516, Accuracy: 1.0, Computation time: 0.8495738506317139\n",
      "Step: 6395, Loss: 0.9160351157188416, Accuracy: 1.0, Computation time: 1.021143913269043\n",
      "Step: 6396, Loss: 0.9159506559371948, Accuracy: 1.0, Computation time: 0.9759657382965088\n",
      "Step: 6397, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 0.9824128150939941\n",
      "Step: 6398, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 0.8791825771331787\n",
      "Step: 6399, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 0.8397459983825684\n",
      "Step: 6400, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.0186879634857178\n",
      "Step: 6401, Loss: 0.9161109328269958, Accuracy: 1.0, Computation time: 1.429908275604248\n",
      "Step: 6402, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 0.8645660877227783\n",
      "Step: 6403, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 0.9048514366149902\n",
      "Step: 6404, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.1273272037506104\n",
      "Step: 6405, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 0.9239952564239502\n",
      "Step: 6406, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 0.8706784248352051\n",
      "Step: 6407, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 0.8645567893981934\n",
      "Step: 6408, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.0436277389526367\n",
      "Step: 6409, Loss: 0.937410831451416, Accuracy: 0.96875, Computation time: 0.8872435092926025\n",
      "Step: 6410, Loss: 0.9159293174743652, Accuracy: 1.0, Computation time: 0.917487621307373\n",
      "Step: 6411, Loss: 0.9170358777046204, Accuracy: 1.0, Computation time: 1.0193672180175781\n",
      "Step: 6412, Loss: 0.9158843159675598, Accuracy: 1.0, Computation time: 1.6835930347442627\n",
      "Step: 6413, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.0011224746704102\n",
      "Step: 6414, Loss: 0.9161218404769897, Accuracy: 1.0, Computation time: 1.329233169555664\n",
      "Step: 6415, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 0.9946527481079102\n",
      "Step: 6416, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.0386180877685547\n",
      "Step: 6417, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.1930079460144043\n",
      "Step: 6418, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.0678553581237793\n",
      "Step: 6419, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.0122034549713135\n",
      "Step: 6420, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 0.7663655281066895\n",
      "Step: 6421, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.242262840270996\n",
      "Step: 6422, Loss: 0.9158458113670349, Accuracy: 1.0, Computation time: 1.012927770614624\n",
      "Step: 6423, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 0.8942713737487793\n",
      "Step: 6424, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 0.8049955368041992\n",
      "Step: 6425, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 0.9055843353271484\n",
      "Step: 6426, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 0.8573005199432373\n",
      "Step: 6427, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 0.9790971279144287\n",
      "Step: 6428, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 0.9799385070800781\n",
      "Step: 6429, Loss: 0.9374897480010986, Accuracy: 0.96875, Computation time: 0.9357554912567139\n",
      "Step: 6430, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 1.0299875736236572\n",
      "Step: 6431, Loss: 0.9375156760215759, Accuracy: 0.96875, Computation time: 1.0554661750793457\n",
      "Step: 6432, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.0501506328582764\n",
      "Step: 6433, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.1968562602996826\n",
      "Step: 6434, Loss: 0.9271847009658813, Accuracy: 0.96875, Computation time: 1.2842364311218262\n",
      "Step: 6435, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 0.8982937335968018\n",
      "Step: 6436, Loss: 0.9159369468688965, Accuracy: 1.0, Computation time: 0.9273171424865723\n",
      "Step: 6437, Loss: 0.9160511493682861, Accuracy: 1.0, Computation time: 0.9214026927947998\n",
      "Step: 6438, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 1.151559829711914\n",
      "Step: 6439, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 0.8520834445953369\n",
      "Step: 6440, Loss: 0.9375473856925964, Accuracy: 0.96875, Computation time: 0.938147783279419\n",
      "Step: 6441, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.9589343070983887\n",
      "Step: 6442, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 1.062138319015503\n",
      "Step: 6443, Loss: 0.9160345196723938, Accuracy: 1.0, Computation time: 0.8199772834777832\n",
      "Step: 6444, Loss: 0.9587951302528381, Accuracy: 0.9375, Computation time: 1.094653844833374\n",
      "Step: 6445, Loss: 0.9163673520088196, Accuracy: 1.0, Computation time: 0.9720766544342041\n",
      "Step: 6446, Loss: 0.9160863757133484, Accuracy: 1.0, Computation time: 0.9973833560943604\n",
      "Step: 6447, Loss: 0.9160209894180298, Accuracy: 1.0, Computation time: 0.8232057094573975\n",
      "Step: 6448, Loss: 0.9169367551803589, Accuracy: 1.0, Computation time: 0.8220217227935791\n",
      "Step: 6449, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 0.8787093162536621\n",
      "Step: 6450, Loss: 0.9195919632911682, Accuracy: 1.0, Computation time: 1.0279099941253662\n",
      "Step: 6451, Loss: 0.9364709854125977, Accuracy: 0.96875, Computation time: 1.0830812454223633\n",
      "Step: 6452, Loss: 0.9374980926513672, Accuracy: 0.96875, Computation time: 1.164107084274292\n",
      "Step: 6453, Loss: 0.9160009622573853, Accuracy: 1.0, Computation time: 0.8295388221740723\n",
      "Step: 6454, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 0.7966587543487549\n",
      "Step: 6455, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 0.8841276168823242\n",
      "Step: 6456, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 0.8168823719024658\n",
      "Step: 6457, Loss: 0.9375492334365845, Accuracy: 0.96875, Computation time: 0.8444728851318359\n",
      "Step: 6458, Loss: 0.9159918427467346, Accuracy: 1.0, Computation time: 1.3924345970153809\n",
      "Step: 6459, Loss: 0.9159584045410156, Accuracy: 1.0, Computation time: 1.0870602130889893\n",
      "Step: 6460, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 0.9756755828857422\n",
      "Step: 6461, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 0.9823613166809082\n",
      "Step: 6462, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 0.9203343391418457\n",
      "Step: 6463, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 1.0299246311187744\n",
      "Step: 6464, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.1113710403442383\n",
      "Step: 6465, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.1935548782348633\n",
      "Step: 6466, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 0.8603007793426514\n",
      "Step: 6467, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.0754313468933105\n",
      "Step: 6468, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 1.0571331977844238\n",
      "Step: 6469, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 0.9281761646270752\n",
      "Step: 6470, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 0.9083960056304932\n",
      "Step: 6471, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.0114758014678955\n",
      "Step: 6472, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.2000560760498047\n",
      "Step: 6473, Loss: 0.9160799384117126, Accuracy: 1.0, Computation time: 1.0731556415557861\n",
      "Step: 6474, Loss: 0.9165877103805542, Accuracy: 1.0, Computation time: 1.0288660526275635\n",
      "Step: 6475, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 0.9579758644104004\n",
      "Step: 6476, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 0.9363241195678711\n",
      "Step: 6477, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 0.9829235076904297\n",
      "Step: 6478, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 0.9792559146881104\n",
      "Step: 6479, Loss: 0.9161884188652039, Accuracy: 1.0, Computation time: 0.8662946224212646\n",
      "Step: 6480, Loss: 0.9161410927772522, Accuracy: 1.0, Computation time: 0.9857182502746582\n",
      "Step: 6481, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.9449722766876221\n",
      "Step: 6482, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.044142484664917\n",
      "Step: 6483, Loss: 0.9377741813659668, Accuracy: 0.96875, Computation time: 0.9659874439239502\n",
      "Step: 6484, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 0.9062330722808838\n",
      "Step: 6485, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 0.9073226451873779\n",
      "Step: 6486, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.123699426651001\n",
      "Step: 6487, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 0.899498701095581\n",
      "Step: 6488, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 0.9237098693847656\n",
      "Step: 6489, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.0663321018218994\n",
      "Step: 6490, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.212914228439331\n",
      "Step: 6491, Loss: 0.9166715145111084, Accuracy: 1.0, Computation time: 1.159430980682373\n",
      "Step: 6492, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 0.914447546005249\n",
      "Step: 6493, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 0.7549374103546143\n",
      "Step: 6494, Loss: 0.916141927242279, Accuracy: 1.0, Computation time: 1.2617361545562744\n",
      "Step: 6495, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.1968181133270264\n",
      "Step: 6496, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 0.8483884334564209\n",
      "Step: 6497, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.0812592506408691\n",
      "Step: 6498, Loss: 0.9161854386329651, Accuracy: 1.0, Computation time: 0.8943119049072266\n",
      "Step: 6499, Loss: 0.9195337891578674, Accuracy: 1.0, Computation time: 0.9524223804473877\n",
      "Step: 6500, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 0.8220839500427246\n",
      "Step: 6501, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 0.9688847064971924\n",
      "Step: 6502, Loss: 0.9159743189811707, Accuracy: 1.0, Computation time: 0.8351011276245117\n",
      "Step: 6503, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 0.9235203266143799\n",
      "Step: 6504, Loss: 0.9159603714942932, Accuracy: 1.0, Computation time: 0.8739109039306641\n",
      "Step: 6505, Loss: 0.9167725443840027, Accuracy: 1.0, Computation time: 1.025700569152832\n",
      "Step: 6506, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.1519560813903809\n",
      "Step: 6507, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 0.8517496585845947\n",
      "Step: 6508, Loss: 0.9277237057685852, Accuracy: 0.96875, Computation time: 0.8840854167938232\n",
      "Step: 6509, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 0.7732877731323242\n",
      "Step: 6510, Loss: 0.9274479746818542, Accuracy: 0.96875, Computation time: 1.039557933807373\n",
      "Step: 6511, Loss: 0.9161444306373596, Accuracy: 1.0, Computation time: 0.7949388027191162\n",
      "Step: 6512, Loss: 0.9163329601287842, Accuracy: 1.0, Computation time: 1.3639578819274902\n",
      "Step: 6513, Loss: 0.9160639643669128, Accuracy: 1.0, Computation time: 0.7996339797973633\n",
      "Step: 6514, Loss: 0.9164737462997437, Accuracy: 1.0, Computation time: 0.8679671287536621\n",
      "Step: 6515, Loss: 0.9375852346420288, Accuracy: 0.96875, Computation time: 0.9183533191680908\n",
      "Step: 6516, Loss: 0.9159038662910461, Accuracy: 1.0, Computation time: 0.9931182861328125\n",
      "Step: 6517, Loss: 0.9160823225975037, Accuracy: 1.0, Computation time: 0.8145027160644531\n",
      "Step: 6518, Loss: 0.9160733819007874, Accuracy: 1.0, Computation time: 1.1666362285614014\n",
      "Step: 6519, Loss: 0.9583351016044617, Accuracy: 0.9375, Computation time: 0.8075895309448242\n",
      "Step: 6520, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 0.839414119720459\n",
      "Step: 6521, Loss: 0.9203823208808899, Accuracy: 1.0, Computation time: 0.9900531768798828\n",
      "Step: 6522, Loss: 0.9159833788871765, Accuracy: 1.0, Computation time: 0.7418949604034424\n",
      "Step: 6523, Loss: 0.9159852862358093, Accuracy: 1.0, Computation time: 0.9211649894714355\n",
      "Step: 6524, Loss: 0.9159099459648132, Accuracy: 1.0, Computation time: 0.8631205558776855\n",
      "Step: 6525, Loss: 0.9374810457229614, Accuracy: 0.96875, Computation time: 0.7356290817260742\n",
      "Step: 6526, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 0.7231519222259521\n",
      "Step: 6527, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 0.8679275512695312\n",
      "Step: 6528, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 0.7580850124359131\n",
      "Step: 6529, Loss: 0.9159172177314758, Accuracy: 1.0, Computation time: 1.015420913696289\n",
      "Step: 6530, Loss: 0.9161674976348877, Accuracy: 1.0, Computation time: 0.7898948192596436\n",
      "Step: 6531, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.163707971572876\n",
      "########################\n",
      "Test loss: 1.0724412202835083, Test Accuracy_epoch47: 0.7722384929656982\n",
      "########################\n",
      "Step: 6532, Loss: 0.9196099042892456, Accuracy: 1.0, Computation time: 1.0301365852355957\n",
      "Step: 6533, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.0535361766815186\n",
      "Step: 6534, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.0334031581878662\n",
      "Step: 6535, Loss: 0.9159762263298035, Accuracy: 1.0, Computation time: 1.0736160278320312\n",
      "Step: 6536, Loss: 0.9339033365249634, Accuracy: 0.96875, Computation time: 1.0892894268035889\n",
      "Step: 6537, Loss: 0.9209629893302917, Accuracy: 1.0, Computation time: 1.0415701866149902\n",
      "Step: 6538, Loss: 0.9345958828926086, Accuracy: 0.96875, Computation time: 0.9075789451599121\n",
      "Step: 6539, Loss: 0.9159505367279053, Accuracy: 1.0, Computation time: 1.0623927116394043\n",
      "Step: 6540, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 0.8192546367645264\n",
      "Step: 6541, Loss: 0.9159656167030334, Accuracy: 1.0, Computation time: 1.3761138916015625\n",
      "Step: 6542, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 0.8843793869018555\n",
      "Step: 6543, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 0.879347562789917\n",
      "Step: 6544, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 0.8513269424438477\n",
      "Step: 6545, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 0.9150922298431396\n",
      "Step: 6546, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 0.8205773830413818\n",
      "Step: 6547, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 0.8985717296600342\n",
      "Step: 6548, Loss: 0.9241508841514587, Accuracy: 1.0, Computation time: 0.9037966728210449\n",
      "Step: 6549, Loss: 0.9375651478767395, Accuracy: 0.96875, Computation time: 1.260709285736084\n",
      "Step: 6550, Loss: 0.915928065776825, Accuracy: 1.0, Computation time: 0.9750113487243652\n",
      "Step: 6551, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 0.9387969970703125\n",
      "Step: 6552, Loss: 0.9160000681877136, Accuracy: 1.0, Computation time: 0.8477904796600342\n",
      "Step: 6553, Loss: 0.9160336852073669, Accuracy: 1.0, Computation time: 0.8678774833679199\n",
      "Step: 6554, Loss: 0.9160856008529663, Accuracy: 1.0, Computation time: 0.8652422428131104\n",
      "Step: 6555, Loss: 0.9160090684890747, Accuracy: 1.0, Computation time: 0.868945837020874\n",
      "Step: 6556, Loss: 0.9159852266311646, Accuracy: 1.0, Computation time: 0.885277271270752\n",
      "Step: 6557, Loss: 0.9158833026885986, Accuracy: 1.0, Computation time: 0.8413538932800293\n",
      "Step: 6558, Loss: 0.9159178137779236, Accuracy: 1.0, Computation time: 0.9790983200073242\n",
      "Step: 6559, Loss: 0.9281215071678162, Accuracy: 0.96875, Computation time: 0.963616132736206\n",
      "Step: 6560, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 1.007014274597168\n",
      "Step: 6561, Loss: 0.9159988164901733, Accuracy: 1.0, Computation time: 0.8846116065979004\n",
      "Step: 6562, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 0.8855984210968018\n",
      "Step: 6563, Loss: 0.9159536957740784, Accuracy: 1.0, Computation time: 1.059041976928711\n",
      "Step: 6564, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 1.0146009922027588\n",
      "Step: 6565, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 0.8802881240844727\n",
      "Step: 6566, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 0.9672651290893555\n",
      "Step: 6567, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 0.8451592922210693\n",
      "Step: 6568, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 0.9662055969238281\n",
      "Step: 6569, Loss: 0.9374908804893494, Accuracy: 0.96875, Computation time: 0.9672396183013916\n",
      "Step: 6570, Loss: 0.9158967137336731, Accuracy: 1.0, Computation time: 0.9647085666656494\n",
      "Step: 6571, Loss: 0.9174137711524963, Accuracy: 1.0, Computation time: 0.9446842670440674\n",
      "Step: 6572, Loss: 0.9163625240325928, Accuracy: 1.0, Computation time: 0.9533534049987793\n",
      "Step: 6573, Loss: 0.91606205701828, Accuracy: 1.0, Computation time: 0.838320255279541\n",
      "Step: 6574, Loss: 0.9160040616989136, Accuracy: 1.0, Computation time: 0.8260648250579834\n",
      "Step: 6575, Loss: 0.9160009622573853, Accuracy: 1.0, Computation time: 1.080824851989746\n",
      "Step: 6576, Loss: 0.9450171589851379, Accuracy: 0.96875, Computation time: 0.995678186416626\n",
      "Step: 6577, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 0.8802268505096436\n",
      "Step: 6578, Loss: 0.9159967303276062, Accuracy: 1.0, Computation time: 0.8403582572937012\n",
      "Step: 6579, Loss: 0.9377394914627075, Accuracy: 0.96875, Computation time: 0.8033366203308105\n",
      "Step: 6580, Loss: 0.9162496328353882, Accuracy: 1.0, Computation time: 1.0826337337493896\n",
      "Step: 6581, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.2364096641540527\n",
      "Step: 6582, Loss: 0.915932297706604, Accuracy: 1.0, Computation time: 0.841414213180542\n",
      "Step: 6583, Loss: 0.915937066078186, Accuracy: 1.0, Computation time: 0.86673903465271\n",
      "Step: 6584, Loss: 0.9237298369407654, Accuracy: 1.0, Computation time: 0.9777796268463135\n",
      "Step: 6585, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 0.9107120037078857\n",
      "Step: 6586, Loss: 0.9164406061172485, Accuracy: 1.0, Computation time: 0.8810875415802002\n",
      "Step: 6587, Loss: 0.916357159614563, Accuracy: 1.0, Computation time: 0.8995282649993896\n",
      "Step: 6588, Loss: 0.916163444519043, Accuracy: 1.0, Computation time: 0.8695809841156006\n",
      "Step: 6589, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 0.7599353790283203\n",
      "Step: 6590, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 0.9005367755889893\n",
      "Step: 6591, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 0.9219415187835693\n",
      "Step: 6592, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 0.8342952728271484\n",
      "Step: 6593, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 0.8859703540802002\n",
      "Step: 6594, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.1913423538208008\n",
      "Step: 6595, Loss: 0.9198046326637268, Accuracy: 1.0, Computation time: 0.9048447608947754\n",
      "Step: 6596, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 0.8279297351837158\n",
      "Step: 6597, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 0.9660375118255615\n",
      "Step: 6598, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 0.7137696743011475\n",
      "Step: 6599, Loss: 0.9376272559165955, Accuracy: 0.96875, Computation time: 0.7359645366668701\n",
      "Step: 6600, Loss: 0.9301704168319702, Accuracy: 0.96875, Computation time: 0.807504415512085\n",
      "Step: 6601, Loss: 0.9160785675048828, Accuracy: 1.0, Computation time: 0.9445533752441406\n",
      "Step: 6602, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 0.7553269863128662\n",
      "Step: 6603, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 1.1182103157043457\n",
      "Step: 6604, Loss: 0.916964590549469, Accuracy: 1.0, Computation time: 0.8628976345062256\n",
      "Step: 6605, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 0.7181832790374756\n",
      "Step: 6606, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 0.8960378170013428\n",
      "Step: 6607, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 0.8593645095825195\n",
      "Step: 6608, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 0.6997811794281006\n",
      "Step: 6609, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 0.7620956897735596\n",
      "Step: 6610, Loss: 0.9159567356109619, Accuracy: 1.0, Computation time: 0.7659189701080322\n",
      "Step: 6611, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.049917459487915\n",
      "Step: 6612, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 0.7045953273773193\n",
      "Step: 6613, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 0.7249472141265869\n",
      "Step: 6614, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 0.8185420036315918\n",
      "Step: 6615, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 0.8072242736816406\n",
      "Step: 6616, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 0.7304365634918213\n",
      "Step: 6617, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 0.6950457096099854\n",
      "Step: 6618, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 0.7991123199462891\n",
      "Step: 6619, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 0.6770591735839844\n",
      "Step: 6620, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 0.8578650951385498\n",
      "Step: 6621, Loss: 0.9165816307067871, Accuracy: 1.0, Computation time: 1.997809648513794\n",
      "Step: 6622, Loss: 0.9350179433822632, Accuracy: 0.96875, Computation time: 0.7842001914978027\n",
      "Step: 6623, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 0.742168664932251\n",
      "Step: 6624, Loss: 0.9352513551712036, Accuracy: 0.96875, Computation time: 0.88629150390625\n",
      "Step: 6625, Loss: 0.9160453677177429, Accuracy: 1.0, Computation time: 0.9150435924530029\n",
      "Step: 6626, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 0.9547431468963623\n",
      "Step: 6627, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 0.8185479640960693\n",
      "Step: 6628, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 0.6754605770111084\n",
      "Step: 6629, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 0.7875063419342041\n",
      "Step: 6630, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 0.8907928466796875\n",
      "Step: 6631, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 0.7572109699249268\n",
      "Step: 6632, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 0.6827325820922852\n",
      "Step: 6633, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.4415397644042969\n",
      "Step: 6634, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 0.8372898101806641\n",
      "Step: 6635, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 0.6996369361877441\n",
      "Step: 6636, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 0.7148094177246094\n",
      "Step: 6637, Loss: 0.9168717861175537, Accuracy: 1.0, Computation time: 1.275723934173584\n",
      "Step: 6638, Loss: 0.9374887943267822, Accuracy: 0.96875, Computation time: 0.8043687343597412\n",
      "Step: 6639, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 0.7878680229187012\n",
      "Step: 6640, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 0.713097333908081\n",
      "Step: 6641, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 0.8538670539855957\n",
      "Step: 6642, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 0.6929314136505127\n",
      "Step: 6643, Loss: 0.9375195503234863, Accuracy: 0.96875, Computation time: 0.7775371074676514\n",
      "Step: 6644, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 0.7914271354675293\n",
      "Step: 6645, Loss: 0.9158346056938171, Accuracy: 1.0, Computation time: 0.912971019744873\n",
      "Step: 6646, Loss: 0.9171016216278076, Accuracy: 1.0, Computation time: 0.8303074836730957\n",
      "Step: 6647, Loss: 0.9175841808319092, Accuracy: 1.0, Computation time: 0.7244153022766113\n",
      "Step: 6648, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 0.6966419219970703\n",
      "Step: 6649, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 0.8463144302368164\n",
      "Step: 6650, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.6698884963989258\n",
      "Step: 6651, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 0.7985169887542725\n",
      "Step: 6652, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 0.7149209976196289\n",
      "Step: 6653, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 0.7152142524719238\n",
      "Step: 6654, Loss: 0.9159497618675232, Accuracy: 1.0, Computation time: 0.7705607414245605\n",
      "Step: 6655, Loss: 0.9158340096473694, Accuracy: 1.0, Computation time: 0.770531415939331\n",
      "Step: 6656, Loss: 0.9363318681716919, Accuracy: 0.96875, Computation time: 0.947432279586792\n",
      "Step: 6657, Loss: 0.9164655208587646, Accuracy: 1.0, Computation time: 0.765671968460083\n",
      "Step: 6658, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 1.0264084339141846\n",
      "Step: 6659, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 0.6921563148498535\n",
      "Step: 6660, Loss: 0.9375409483909607, Accuracy: 0.96875, Computation time: 0.7161681652069092\n",
      "Step: 6661, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.7337243556976318\n",
      "Step: 6662, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 0.9353020191192627\n",
      "Step: 6663, Loss: 0.915836751461029, Accuracy: 1.0, Computation time: 0.727006196975708\n",
      "Step: 6664, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 1.1018445491790771\n",
      "Step: 6665, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 0.8068442344665527\n",
      "Step: 6666, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 0.8165702819824219\n",
      "Step: 6667, Loss: 0.9161393642425537, Accuracy: 1.0, Computation time: 0.8398044109344482\n",
      "Step: 6668, Loss: 0.9375986456871033, Accuracy: 0.96875, Computation time: 0.727043867111206\n",
      "Step: 6669, Loss: 0.9590672254562378, Accuracy: 0.9375, Computation time: 0.8688867092132568\n",
      "########################\n",
      "Test loss: 1.069618582725525, Test Accuracy_epoch48: 0.7771260738372803\n",
      "########################\n",
      "Step: 6670, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 0.6806304454803467\n",
      "Step: 6671, Loss: 0.9174597859382629, Accuracy: 1.0, Computation time: 0.6953010559082031\n",
      "Step: 6672, Loss: 0.9376031160354614, Accuracy: 0.96875, Computation time: 0.819732666015625\n",
      "Step: 6673, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 0.7320652008056641\n",
      "Step: 6674, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 1.0817370414733887\n",
      "Step: 6675, Loss: 0.915937066078186, Accuracy: 1.0, Computation time: 0.7409286499023438\n",
      "Step: 6676, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 0.8362646102905273\n",
      "Step: 6677, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 0.7105286121368408\n",
      "Step: 6678, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.7460808753967285\n",
      "Step: 6679, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 0.8229880332946777\n",
      "Step: 6680, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.0065813064575195\n",
      "Step: 6681, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 0.796917200088501\n",
      "Step: 6682, Loss: 0.9163138270378113, Accuracy: 1.0, Computation time: 0.7385759353637695\n",
      "Step: 6683, Loss: 0.9592000246047974, Accuracy: 0.9375, Computation time: 0.7587440013885498\n",
      "Step: 6684, Loss: 0.9158315658569336, Accuracy: 1.0, Computation time: 0.8138480186462402\n",
      "Step: 6685, Loss: 0.91583651304245, Accuracy: 1.0, Computation time: 0.7857587337493896\n",
      "Step: 6686, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 0.7520601749420166\n",
      "Step: 6687, Loss: 0.9374997615814209, Accuracy: 0.96875, Computation time: 0.7973506450653076\n",
      "Step: 6688, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 0.7971107959747314\n",
      "Step: 6689, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 0.7055637836456299\n",
      "Step: 6690, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 0.6820623874664307\n",
      "Step: 6691, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.0005064010620117\n",
      "Step: 6692, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 0.8149957656860352\n",
      "Step: 6693, Loss: 0.9158354997634888, Accuracy: 1.0, Computation time: 0.8637242317199707\n",
      "Step: 6694, Loss: 0.9375290870666504, Accuracy: 0.96875, Computation time: 0.7422432899475098\n",
      "Step: 6695, Loss: 0.9158345460891724, Accuracy: 1.0, Computation time: 0.727294921875\n",
      "Step: 6696, Loss: 0.915840208530426, Accuracy: 1.0, Computation time: 0.676685094833374\n",
      "Step: 6697, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 0.7767016887664795\n",
      "Step: 6698, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 0.7020461559295654\n",
      "Step: 6699, Loss: 0.9158337116241455, Accuracy: 1.0, Computation time: 0.6627511978149414\n",
      "Step: 6700, Loss: 0.9158321022987366, Accuracy: 1.0, Computation time: 0.6766018867492676\n",
      "Step: 6701, Loss: 0.9158338308334351, Accuracy: 1.0, Computation time: 0.6944499015808105\n",
      "Step: 6702, Loss: 0.915830671787262, Accuracy: 1.0, Computation time: 0.7719409465789795\n",
      "Step: 6703, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 0.7747178077697754\n",
      "Step: 6704, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 0.7643499374389648\n",
      "Step: 6705, Loss: 0.9158321619033813, Accuracy: 1.0, Computation time: 0.7265658378601074\n",
      "Step: 6706, Loss: 0.9158362746238708, Accuracy: 1.0, Computation time: 0.7300198078155518\n",
      "Step: 6707, Loss: 0.9158322215080261, Accuracy: 1.0, Computation time: 0.9090912342071533\n",
      "Step: 6708, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 0.872856855392456\n",
      "Step: 6709, Loss: 0.9161515235900879, Accuracy: 1.0, Computation time: 0.7918872833251953\n",
      "Step: 6710, Loss: 0.9158316254615784, Accuracy: 1.0, Computation time: 0.6883759498596191\n",
      "Step: 6711, Loss: 0.9158428907394409, Accuracy: 1.0, Computation time: 0.7506766319274902\n",
      "Step: 6712, Loss: 0.915833592414856, Accuracy: 1.0, Computation time: 0.7422568798065186\n",
      "Step: 6713, Loss: 0.915840208530426, Accuracy: 1.0, Computation time: 0.9585862159729004\n",
      "Step: 6714, Loss: 0.9374723434448242, Accuracy: 0.96875, Computation time: 0.710618257522583\n",
      "Step: 6715, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 0.8821694850921631\n",
      "Step: 6716, Loss: 0.9159205555915833, Accuracy: 1.0, Computation time: 1.4957072734832764\n",
      "Step: 6717, Loss: 0.9158333539962769, Accuracy: 1.0, Computation time: 0.6908590793609619\n",
      "Step: 6718, Loss: 0.9158355593681335, Accuracy: 1.0, Computation time: 0.70686936378479\n",
      "Step: 6719, Loss: 0.9158366322517395, Accuracy: 1.0, Computation time: 0.7702417373657227\n",
      "Step: 6720, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 0.8688168525695801\n",
      "Step: 6721, Loss: 0.9307664036750793, Accuracy: 0.96875, Computation time: 0.7196321487426758\n",
      "Step: 6722, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 0.7662382125854492\n",
      "Step: 6723, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 0.6944446563720703\n",
      "Step: 6724, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 0.8459868431091309\n",
      "Step: 6725, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 0.8678431510925293\n",
      "Step: 6726, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 0.7872846126556396\n",
      "Step: 6727, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 0.8972575664520264\n",
      "Step: 6728, Loss: 0.915838897228241, Accuracy: 1.0, Computation time: 0.721621036529541\n",
      "Step: 6729, Loss: 0.9158425331115723, Accuracy: 1.0, Computation time: 1.0335686206817627\n",
      "Step: 6730, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 0.8094584941864014\n",
      "Step: 6731, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.8661918640136719\n",
      "Step: 6732, Loss: 0.9169298410415649, Accuracy: 1.0, Computation time: 1.0432145595550537\n",
      "Step: 6733, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 0.7920486927032471\n",
      "Step: 6734, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 0.9413654804229736\n",
      "Step: 6735, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 0.7415080070495605\n",
      "Step: 6736, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 0.8784480094909668\n",
      "Step: 6737, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 1.008037805557251\n",
      "Step: 6738, Loss: 0.9165093898773193, Accuracy: 1.0, Computation time: 0.8169145584106445\n",
      "Step: 6739, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 0.7707855701446533\n",
      "Step: 6740, Loss: 0.9158338308334351, Accuracy: 1.0, Computation time: 1.0551378726959229\n",
      "Step: 6741, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 0.7513372898101807\n",
      "Step: 6742, Loss: 0.9538332223892212, Accuracy: 0.9375, Computation time: 1.8244507312774658\n",
      "Step: 6743, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 0.7098627090454102\n",
      "Step: 6744, Loss: 0.9375943541526794, Accuracy: 0.96875, Computation time: 0.9617445468902588\n",
      "Step: 6745, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 0.6784183979034424\n",
      "Step: 6746, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 0.9744737148284912\n",
      "Step: 6747, Loss: 0.93658447265625, Accuracy: 0.96875, Computation time: 1.0761637687683105\n",
      "Step: 6748, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 0.8650517463684082\n",
      "Step: 6749, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 0.8566193580627441\n",
      "Step: 6750, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 0.732750654220581\n",
      "Step: 6751, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 0.7840735912322998\n",
      "Step: 6752, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 0.8934664726257324\n",
      "Step: 6753, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 0.9843881130218506\n",
      "Step: 6754, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 0.7549741268157959\n",
      "Step: 6755, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 0.7678768634796143\n",
      "Step: 6756, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 0.7662906646728516\n",
      "Step: 6757, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 0.7822732925415039\n",
      "Step: 6758, Loss: 0.9171611070632935, Accuracy: 1.0, Computation time: 0.8300719261169434\n",
      "Step: 6759, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 1.0910346508026123\n",
      "Step: 6760, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 0.8808643817901611\n",
      "Step: 6761, Loss: 0.9165728092193604, Accuracy: 1.0, Computation time: 0.9629273414611816\n",
      "Step: 6762, Loss: 0.9174877405166626, Accuracy: 1.0, Computation time: 0.7592868804931641\n",
      "Step: 6763, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 0.8669764995574951\n",
      "Step: 6764, Loss: 0.9159461259841919, Accuracy: 1.0, Computation time: 0.8714697360992432\n",
      "Step: 6765, Loss: 0.9158900380134583, Accuracy: 1.0, Computation time: 0.7367737293243408\n",
      "Step: 6766, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 0.8955056667327881\n",
      "Step: 6767, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 0.8686666488647461\n",
      "Step: 6768, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.8733861446380615\n",
      "Step: 6769, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 0.8305721282958984\n",
      "Step: 6770, Loss: 0.9159491658210754, Accuracy: 1.0, Computation time: 0.7831065654754639\n",
      "Step: 6771, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 0.7999773025512695\n",
      "Step: 6772, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 0.7398183345794678\n",
      "Step: 6773, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 0.8412909507751465\n",
      "Step: 6774, Loss: 0.9375171065330505, Accuracy: 0.96875, Computation time: 0.9020977020263672\n",
      "Step: 6775, Loss: 0.9162587523460388, Accuracy: 1.0, Computation time: 0.7589783668518066\n",
      "Step: 6776, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 0.862863302230835\n",
      "Step: 6777, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 0.9633140563964844\n",
      "Step: 6778, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 0.7793104648590088\n",
      "Step: 6779, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 0.8250288963317871\n",
      "Step: 6780, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.8961248397827148\n",
      "Step: 6781, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 0.780872106552124\n",
      "Step: 6782, Loss: 0.9158352017402649, Accuracy: 1.0, Computation time: 1.1144211292266846\n",
      "Step: 6783, Loss: 0.9158434271812439, Accuracy: 1.0, Computation time: 0.8136496543884277\n",
      "Step: 6784, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 0.7877182960510254\n",
      "Step: 6785, Loss: 0.9270833730697632, Accuracy: 0.96875, Computation time: 0.8903746604919434\n",
      "Step: 6786, Loss: 0.937491238117218, Accuracy: 0.96875, Computation time: 0.8367831707000732\n",
      "Step: 6787, Loss: 0.937544584274292, Accuracy: 0.96875, Computation time: 0.7462480068206787\n",
      "Step: 6788, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 0.7354915142059326\n",
      "Step: 6789, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 0.7888553142547607\n",
      "Step: 6790, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 0.8766705989837646\n",
      "Step: 6791, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 0.746070146560669\n",
      "Step: 6792, Loss: 0.9375289082527161, Accuracy: 0.96875, Computation time: 1.1564228534698486\n",
      "Step: 6793, Loss: 0.9162607192993164, Accuracy: 1.0, Computation time: 0.7320144176483154\n",
      "Step: 6794, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 0.8723263740539551\n",
      "Step: 6795, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 0.7715137004852295\n",
      "Step: 6796, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 0.752507209777832\n",
      "Step: 6797, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 0.8045732975006104\n",
      "Step: 6798, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.10772705078125\n",
      "Step: 6799, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 0.8858094215393066\n",
      "Step: 6800, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.144580602645874\n",
      "Step: 6801, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 0.7842016220092773\n",
      "Step: 6802, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 0.8359889984130859\n",
      "Step: 6803, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 0.9048786163330078\n",
      "Step: 6804, Loss: 0.9162647128105164, Accuracy: 1.0, Computation time: 0.8247270584106445\n",
      "Step: 6805, Loss: 0.937477707862854, Accuracy: 0.96875, Computation time: 0.8903260231018066\n",
      "Step: 6806, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 0.840256929397583\n",
      "Step: 6807, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 0.8354983329772949\n",
      "Step: 6808, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 0.821967601776123\n",
      "########################\n",
      "Test loss: 1.0698243379592896, Test Accuracy_epoch49: 0.7761485576629639\n",
      "########################\n",
      "Step: 6809, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 0.7832562923431396\n",
      "Step: 6810, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 0.7267358303070068\n",
      "Step: 6811, Loss: 0.9375616908073425, Accuracy: 0.96875, Computation time: 1.2064142227172852\n",
      "Step: 6812, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 0.8190860748291016\n",
      "Step: 6813, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 0.748835563659668\n",
      "Step: 6814, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 0.7581746578216553\n",
      "Step: 6815, Loss: 0.9591757655143738, Accuracy: 0.9375, Computation time: 0.7376019954681396\n",
      "Step: 6816, Loss: 0.9173986315727234, Accuracy: 1.0, Computation time: 1.2974584102630615\n",
      "Step: 6817, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 0.8299458026885986\n",
      "Step: 6818, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 0.8017780780792236\n",
      "Step: 6819, Loss: 0.9375581741333008, Accuracy: 0.96875, Computation time: 1.7087347507476807\n",
      "Step: 6820, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 0.9227726459503174\n",
      "Step: 6821, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 0.8908309936523438\n",
      "Step: 6822, Loss: 0.9388427138328552, Accuracy: 0.96875, Computation time: 0.9424481391906738\n",
      "Step: 6823, Loss: 0.9168751835823059, Accuracy: 1.0, Computation time: 0.8908305168151855\n",
      "Step: 6824, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 0.8365755081176758\n",
      "Step: 6825, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 0.7481026649475098\n",
      "Step: 6826, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 0.9842793941497803\n",
      "Step: 6827, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 0.7638978958129883\n",
      "Step: 6828, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 0.7851846218109131\n",
      "Step: 6829, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 0.9914717674255371\n",
      "Step: 6830, Loss: 0.9158357381820679, Accuracy: 1.0, Computation time: 0.764441728591919\n",
      "Step: 6831, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 0.8306727409362793\n",
      "Step: 6832, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 0.8229241371154785\n",
      "Step: 6833, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 0.7854571342468262\n",
      "Step: 6834, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 0.8267855644226074\n",
      "Step: 6835, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 0.6931488513946533\n",
      "Step: 6836, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 0.7708024978637695\n",
      "Step: 6837, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 0.7500326633453369\n",
      "Step: 6838, Loss: 0.9295697212219238, Accuracy: 0.96875, Computation time: 1.041304588317871\n",
      "Step: 6839, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 0.7596492767333984\n",
      "Step: 6840, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 0.7231225967407227\n",
      "Step: 6841, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 0.7184948921203613\n",
      "Step: 6842, Loss: 0.9159579277038574, Accuracy: 1.0, Computation time: 0.9118225574493408\n",
      "Step: 6843, Loss: 0.9160518646240234, Accuracy: 1.0, Computation time: 1.5155386924743652\n",
      "Step: 6844, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 0.7656493186950684\n",
      "Step: 6845, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.09415864944458\n",
      "Step: 6846, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 0.7864017486572266\n",
      "Step: 6847, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 0.8329935073852539\n",
      "Step: 6848, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 0.8906114101409912\n",
      "Step: 6849, Loss: 0.9210641384124756, Accuracy: 1.0, Computation time: 1.0892879962921143\n",
      "Step: 6850, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 0.7330121994018555\n",
      "Step: 6851, Loss: 0.9377169609069824, Accuracy: 0.96875, Computation time: 0.8584144115447998\n",
      "Step: 6852, Loss: 0.9159460067749023, Accuracy: 1.0, Computation time: 0.854027509689331\n",
      "Step: 6853, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 0.7951641082763672\n",
      "Step: 6854, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.0661101341247559\n",
      "Step: 6855, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 0.8902888298034668\n",
      "Step: 6856, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 0.8503642082214355\n",
      "Step: 6857, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.7870986461639404\n",
      "Step: 6858, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.9835450649261475\n",
      "Step: 6859, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 0.8019425868988037\n",
      "Step: 6860, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 0.7747950553894043\n",
      "Step: 6861, Loss: 0.9373789429664612, Accuracy: 0.96875, Computation time: 1.1223397254943848\n",
      "Step: 6862, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 0.7807536125183105\n",
      "Step: 6863, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.1555194854736328\n",
      "Step: 6864, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 0.8651340007781982\n",
      "Step: 6865, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 0.877910852432251\n",
      "Step: 6866, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 1.0363774299621582\n",
      "Step: 6867, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 1.4503068923950195\n",
      "Step: 6868, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 0.7847204208374023\n",
      "Step: 6869, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.0408220291137695\n",
      "Step: 6870, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 0.7457640171051025\n",
      "Step: 6871, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 0.8265788555145264\n",
      "Step: 6872, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 0.798715353012085\n",
      "Step: 6873, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 0.8068828582763672\n",
      "Step: 6874, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 0.7380557060241699\n",
      "Step: 6875, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 0.8013763427734375\n",
      "Step: 6876, Loss: 0.9158329963684082, Accuracy: 1.0, Computation time: 0.7594549655914307\n",
      "Step: 6877, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 0.7850356101989746\n",
      "Step: 6878, Loss: 0.9375303387641907, Accuracy: 0.96875, Computation time: 0.8997237682342529\n",
      "Step: 6879, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 0.8408246040344238\n",
      "Step: 6880, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.010906457901001\n",
      "Step: 6881, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 0.7959637641906738\n",
      "Step: 6882, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.257185697555542\n",
      "Step: 6883, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 0.7668797969818115\n",
      "Step: 6884, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 0.9267325401306152\n",
      "Step: 6885, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 0.8220322132110596\n",
      "Step: 6886, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.0204811096191406\n",
      "Step: 6887, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 0.8692004680633545\n",
      "Step: 6888, Loss: 0.916000485420227, Accuracy: 1.0, Computation time: 1.8002684116363525\n",
      "Step: 6889, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 0.7989420890808105\n",
      "Step: 6890, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 0.7927889823913574\n",
      "Step: 6891, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 0.893796443939209\n",
      "Step: 6892, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 0.8712809085845947\n",
      "Step: 6893, Loss: 0.9158341288566589, Accuracy: 1.0, Computation time: 0.8574426174163818\n",
      "Step: 6894, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 0.862123966217041\n",
      "Step: 6895, Loss: 0.9158430695533752, Accuracy: 1.0, Computation time: 0.7937321662902832\n",
      "Step: 6896, Loss: 0.9158320426940918, Accuracy: 1.0, Computation time: 0.8126637935638428\n",
      "Step: 6897, Loss: 0.937485933303833, Accuracy: 0.96875, Computation time: 0.764031171798706\n",
      "Step: 6898, Loss: 0.9158352613449097, Accuracy: 1.0, Computation time: 0.9466304779052734\n",
      "Step: 6899, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 0.8130104541778564\n",
      "Step: 6900, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 0.853858232498169\n",
      "Step: 6901, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.0178370475769043\n",
      "Step: 6902, Loss: 0.9375545978546143, Accuracy: 0.96875, Computation time: 1.0973174571990967\n",
      "Step: 6903, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 0.7348198890686035\n",
      "Step: 6904, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 0.7568159103393555\n",
      "Step: 6905, Loss: 0.9342983365058899, Accuracy: 0.96875, Computation time: 1.142993450164795\n",
      "Step: 6906, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 0.7850394248962402\n",
      "Step: 6907, Loss: 0.9375573992729187, Accuracy: 0.96875, Computation time: 0.7934794425964355\n",
      "Step: 6908, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 0.7260396480560303\n",
      "Step: 6909, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 0.8553421497344971\n",
      "Step: 6910, Loss: 0.9375454187393188, Accuracy: 0.96875, Computation time: 0.8490538597106934\n",
      "Step: 6911, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 0.841646671295166\n",
      "Step: 6912, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 0.7412028312683105\n",
      "Step: 6913, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 0.8486220836639404\n",
      "Step: 6914, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 0.709453821182251\n",
      "Step: 6915, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.3405845165252686\n",
      "Step: 6916, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 0.857595682144165\n",
      "Step: 6917, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 0.7989745140075684\n",
      "Step: 6918, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 0.9400572776794434\n",
      "Step: 6919, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 1.0100128650665283\n",
      "Step: 6920, Loss: 0.9158394932746887, Accuracy: 1.0, Computation time: 0.7688522338867188\n",
      "Step: 6921, Loss: 0.9374246597290039, Accuracy: 0.96875, Computation time: 0.8648984432220459\n",
      "Step: 6922, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 0.9105372428894043\n",
      "Step: 6923, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.0675756931304932\n",
      "Step: 6924, Loss: 0.9365267157554626, Accuracy: 0.96875, Computation time: 0.855074405670166\n",
      "Step: 6925, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 0.737001895904541\n",
      "Step: 6926, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 0.8150782585144043\n",
      "Step: 6927, Loss: 0.9374917149543762, Accuracy: 0.96875, Computation time: 0.7629616260528564\n",
      "Step: 6928, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 0.7892124652862549\n",
      "Step: 6929, Loss: 0.9375395178794861, Accuracy: 0.96875, Computation time: 0.8826138973236084\n",
      "Step: 6930, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.0743961334228516\n",
      "Step: 6931, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 0.8535730838775635\n",
      "Step: 6932, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 0.8448348045349121\n",
      "Step: 6933, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 0.8572745323181152\n",
      "Step: 6934, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 0.9126465320587158\n",
      "Step: 6935, Loss: 0.9287158250808716, Accuracy: 0.96875, Computation time: 1.0253779888153076\n",
      "Step: 6936, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 0.9091014862060547\n",
      "Step: 6937, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 0.7607021331787109\n",
      "Step: 6938, Loss: 0.9159802794456482, Accuracy: 1.0, Computation time: 0.910158634185791\n",
      "Step: 6939, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.0468406677246094\n",
      "Step: 6940, Loss: 0.9374894499778748, Accuracy: 0.96875, Computation time: 0.7669951915740967\n",
      "Step: 6941, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 0.8089826107025146\n",
      "Step: 6942, Loss: 0.9375342130661011, Accuracy: 0.96875, Computation time: 0.7301998138427734\n",
      "Step: 6943, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 0.8806116580963135\n",
      "Step: 6944, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 0.8285300731658936\n",
      "Step: 6945, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.7807531356811523\n",
      "Step: 6946, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.4023356437683105\n",
      "Step: 6947, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.0034372806549072\n",
      "########################\n",
      "Test loss: 1.0715974569320679, Test Accuracy_epoch50: 0.774193525314331\n",
      "########################\n",
      "Step: 6948, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 0.7887496948242188\n",
      "Step: 6949, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 0.9302644729614258\n",
      "Step: 6950, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 0.8029677867889404\n",
      "Step: 6951, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.9574964046478271\n",
      "Step: 6952, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 1.057011604309082\n",
      "Step: 6953, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 0.8848493099212646\n",
      "Step: 6954, Loss: 0.9373652935028076, Accuracy: 0.96875, Computation time: 0.8548576831817627\n",
      "Step: 6955, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 0.9964969158172607\n",
      "Step: 6956, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 0.8098728656768799\n",
      "Step: 6957, Loss: 0.91584312915802, Accuracy: 1.0, Computation time: 0.8173990249633789\n",
      "Step: 6958, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 0.8525207042694092\n",
      "Step: 6959, Loss: 0.9165602326393127, Accuracy: 1.0, Computation time: 1.0112204551696777\n",
      "Step: 6960, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 0.8374271392822266\n",
      "Step: 6961, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 0.764359712600708\n",
      "Step: 6962, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 0.8761699199676514\n",
      "Step: 6963, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 0.7793710231781006\n",
      "Step: 6964, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 0.751227617263794\n",
      "Step: 6965, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 0.9353973865509033\n",
      "Step: 6966, Loss: 0.9160881638526917, Accuracy: 1.0, Computation time: 0.9039900302886963\n",
      "Step: 6967, Loss: 0.9158351421356201, Accuracy: 1.0, Computation time: 0.9656250476837158\n",
      "Step: 6968, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 0.774864673614502\n",
      "Step: 6969, Loss: 0.9158961176872253, Accuracy: 1.0, Computation time: 0.8019707202911377\n",
      "Step: 6970, Loss: 0.9375334978103638, Accuracy: 0.96875, Computation time: 0.8939831256866455\n",
      "Step: 6971, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 0.8945457935333252\n",
      "Step: 6972, Loss: 0.9164626598358154, Accuracy: 1.0, Computation time: 0.799750804901123\n",
      "Step: 6973, Loss: 0.9246150851249695, Accuracy: 1.0, Computation time: 1.1537718772888184\n",
      "Step: 6974, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 0.8735177516937256\n",
      "Step: 6975, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 0.9891905784606934\n",
      "Step: 6976, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 0.9963879585266113\n",
      "Step: 6977, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 0.8377652168273926\n",
      "Step: 6978, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 0.8655221462249756\n",
      "Step: 6979, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 0.7557473182678223\n",
      "Step: 6980, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 0.932293176651001\n",
      "Step: 6981, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 0.9051954746246338\n",
      "Step: 6982, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 0.8870775699615479\n",
      "Step: 6983, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 0.9622271060943604\n",
      "Step: 6984, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.047776699066162\n",
      "Step: 6985, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.0599370002746582\n",
      "Step: 6986, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 0.8602089881896973\n",
      "Step: 6987, Loss: 0.9188624024391174, Accuracy: 1.0, Computation time: 1.235231876373291\n",
      "Step: 6988, Loss: 0.9159919023513794, Accuracy: 1.0, Computation time: 0.9967279434204102\n",
      "Step: 6989, Loss: 0.9161808490753174, Accuracy: 1.0, Computation time: 0.8110182285308838\n",
      "Step: 6990, Loss: 0.9373289346694946, Accuracy: 0.96875, Computation time: 0.9885013103485107\n",
      "Step: 6991, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 0.8772580623626709\n",
      "Step: 6992, Loss: 0.916257381439209, Accuracy: 1.0, Computation time: 0.9675905704498291\n",
      "Step: 6993, Loss: 0.9202496409416199, Accuracy: 1.0, Computation time: 0.7563672065734863\n",
      "Step: 6994, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 0.9681012630462646\n",
      "Step: 6995, Loss: 0.9159551858901978, Accuracy: 1.0, Computation time: 0.9197769165039062\n",
      "Step: 6996, Loss: 0.9159601926803589, Accuracy: 1.0, Computation time: 0.933255672454834\n",
      "Step: 6997, Loss: 0.9160327315330505, Accuracy: 1.0, Computation time: 0.8612358570098877\n",
      "Step: 6998, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 0.745891809463501\n",
      "Step: 6999, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 0.930131196975708\n",
      "Step: 7000, Loss: 0.9374877214431763, Accuracy: 0.96875, Computation time: 0.8549859523773193\n",
      "Step: 7001, Loss: 0.9161372184753418, Accuracy: 1.0, Computation time: 1.0834953784942627\n",
      "Step: 7002, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.0792765617370605\n",
      "Step: 7003, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 0.8193778991699219\n",
      "Step: 7004, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.0253207683563232\n",
      "Step: 7005, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 0.9429633617401123\n",
      "Step: 7006, Loss: 0.9374411106109619, Accuracy: 0.96875, Computation time: 0.9423313140869141\n",
      "Step: 7007, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.0397474765777588\n",
      "Step: 7008, Loss: 0.9346857666969299, Accuracy: 0.96875, Computation time: 1.9747116565704346\n",
      "Step: 7009, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 0.7725672721862793\n",
      "Step: 7010, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 0.7556002140045166\n",
      "Step: 7011, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 0.7254858016967773\n",
      "Step: 7012, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 0.8436741828918457\n",
      "Step: 7013, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 0.8200545310974121\n",
      "Step: 7014, Loss: 0.9162403345108032, Accuracy: 1.0, Computation time: 0.7768080234527588\n",
      "Step: 7015, Loss: 0.9375295042991638, Accuracy: 0.96875, Computation time: 0.7308135032653809\n",
      "Step: 7016, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.8101439476013184\n",
      "Step: 7017, Loss: 0.9160179495811462, Accuracy: 1.0, Computation time: 0.7309768199920654\n",
      "Step: 7018, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.0158569812774658\n",
      "Step: 7019, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 0.8394992351531982\n",
      "Step: 7020, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 0.7653293609619141\n",
      "Step: 7021, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 0.7305448055267334\n",
      "Step: 7022, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 0.8281667232513428\n",
      "Step: 7023, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 0.7739717960357666\n",
      "Step: 7024, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 0.7121784687042236\n",
      "Step: 7025, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 0.7250311374664307\n",
      "Step: 7026, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 0.8533008098602295\n",
      "Step: 7027, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 0.94842529296875\n",
      "Step: 7028, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 0.7289378643035889\n",
      "Step: 7029, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 0.927844762802124\n",
      "Step: 7030, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 0.686410665512085\n",
      "Step: 7031, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 0.7453646659851074\n",
      "Step: 7032, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 0.8459975719451904\n",
      "Step: 7033, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 0.868811845779419\n",
      "Step: 7034, Loss: 0.9160094857215881, Accuracy: 1.0, Computation time: 0.8108768463134766\n",
      "Step: 7035, Loss: 0.9158366918563843, Accuracy: 1.0, Computation time: 1.003748893737793\n",
      "Step: 7036, Loss: 0.9374493956565857, Accuracy: 0.96875, Computation time: 0.7738685607910156\n",
      "Step: 7037, Loss: 0.9158341884613037, Accuracy: 1.0, Computation time: 0.7687039375305176\n",
      "Step: 7038, Loss: 0.918384313583374, Accuracy: 1.0, Computation time: 0.7285089492797852\n",
      "Step: 7039, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 0.7839107513427734\n",
      "Step: 7040, Loss: 0.9161896109580994, Accuracy: 1.0, Computation time: 0.7509613037109375\n",
      "Step: 7041, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.7464585304260254\n",
      "Step: 7042, Loss: 0.916167676448822, Accuracy: 1.0, Computation time: 0.7627713680267334\n",
      "Step: 7043, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 0.7361750602722168\n",
      "Step: 7044, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 0.7743542194366455\n",
      "Step: 7045, Loss: 0.9388806819915771, Accuracy: 0.96875, Computation time: 0.8433611392974854\n",
      "Step: 7046, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 0.7901430130004883\n",
      "Step: 7047, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 0.8004109859466553\n",
      "Step: 7048, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 0.9389867782592773\n",
      "Step: 7049, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 0.7423462867736816\n",
      "Step: 7050, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.7767574787139893\n",
      "Step: 7051, Loss: 0.9367609620094299, Accuracy: 0.96875, Computation time: 1.0805532932281494\n",
      "Step: 7052, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 0.8024332523345947\n",
      "Step: 7053, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 1.042464256286621\n",
      "Step: 7054, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 0.7344961166381836\n",
      "Step: 7055, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 0.9074568748474121\n",
      "Step: 7056, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 0.7777304649353027\n",
      "Step: 7057, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 0.7823741436004639\n",
      "Step: 7058, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 0.8608529567718506\n",
      "Step: 7059, Loss: 0.9345265626907349, Accuracy: 0.96875, Computation time: 0.8881804943084717\n",
      "Step: 7060, Loss: 0.9169677495956421, Accuracy: 1.0, Computation time: 1.009014368057251\n",
      "Step: 7061, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 0.8908543586730957\n",
      "Step: 7062, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.8370108604431152\n",
      "Step: 7063, Loss: 0.9159557223320007, Accuracy: 1.0, Computation time: 0.9534006118774414\n",
      "Step: 7064, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.7456562519073486\n",
      "Step: 7065, Loss: 0.9375054240226746, Accuracy: 0.96875, Computation time: 1.0866281986236572\n",
      "Step: 7066, Loss: 0.9158361554145813, Accuracy: 1.0, Computation time: 0.7518503665924072\n",
      "Step: 7067, Loss: 0.9375066757202148, Accuracy: 0.96875, Computation time: 0.9669995307922363\n",
      "Step: 7068, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 0.702491044998169\n",
      "Step: 7069, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 0.7968399524688721\n",
      "Step: 7070, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 0.7341530323028564\n",
      "Step: 7071, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 0.8069198131561279\n",
      "Step: 7072, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 0.9062290191650391\n",
      "Step: 7073, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 0.7522363662719727\n",
      "Step: 7074, Loss: 0.9158332943916321, Accuracy: 1.0, Computation time: 0.6896464824676514\n",
      "Step: 7075, Loss: 0.9375315308570862, Accuracy: 0.96875, Computation time: 0.8059728145599365\n",
      "Step: 7076, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 0.7123737335205078\n",
      "Step: 7077, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 0.756432294845581\n",
      "Step: 7078, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 0.8070888519287109\n",
      "Step: 7079, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 0.7521066665649414\n",
      "Step: 7080, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 0.7820084095001221\n",
      "Step: 7081, Loss: 0.9158372282981873, Accuracy: 1.0, Computation time: 0.6971597671508789\n",
      "Step: 7082, Loss: 0.9158384799957275, Accuracy: 1.0, Computation time: 0.9054214954376221\n",
      "Step: 7083, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 0.9049427509307861\n",
      "Step: 7084, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 0.9165916442871094\n",
      "Step: 7085, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 1.0388810634613037\n",
      "Test loss: 1.0733989477157593, Test Accuracy: 0.7732160091400146\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff465f88-a503-4e26-a794-81b8a0580665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
