{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d58ae9-42a2-443c-945f-caca612b5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7865807-cea5-4fe5-a25b-9af26443938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session4_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947941ab-eb22-49dd-90ad-59aafa95c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 256, 768) (4500,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last2, wav2vec_last3, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last2,label_last3,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820fd969-7946-43dd-9804-eaafbfc1bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fda037ab-cd5d-4227-b7b8-9f17c90964da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9118a689-a8c2-4289-afb6-9e8d1d1c29d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            #zn = jnp.concatenate((coeffs_i, yn_array), axis=1)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0)  \n",
    "            pred_var=prediction.var(axis=0)   \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)    \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0742aecb-51ae-4f63-833a-a01dd62ad593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a5d4cd-b8ee-4d97-ae29-0044badee82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last4], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last4\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2464ee4f-7f3e-4464-a34f-c34e923a20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6051d49-d1f0-4613-a1f3-6043daa119f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a63538bf-ae33-434c-af04-97a772a87b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4500,\n",
    "    dataset_test_size=1031,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=8765,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    import jax.numpy as jnp\n",
    "    from jax import jit\n",
    "    def calculate_confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "        true_labels = true_labels.astype(jnp.int32)\n",
    "        pred_labels = pred_labels.astype(jnp.int32)\n",
    "        conf_matrix = jnp.zeros((num_classes, num_classes), dtype=jnp.int32)\n",
    "        for t, p in zip(true_labels, pred_labels):\n",
    "            conf_matrix = conf_matrix.at[t, p].add(1)\n",
    "        return conf_matrix\n",
    "\n",
    "    @jit\n",
    "    def calculate_ua(conf_matrix):\n",
    "        class_accuracy = jnp.diag(conf_matrix) / jnp.sum(conf_matrix, axis=1)\n",
    "        UA = jnp.mean(class_accuracy)\n",
    "        return UA\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        conf_matrix = calculate_confusion_matrix(label_i, pred, num_classes=4)\n",
    "        UA = calculate_ua(conf_matrix)\n",
    "        return UA\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aafdcb7-8bfa-4512-9c4a-c1bb246eeabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.415205955505371, Accuracy: 0.25, Computation time: 18.54773211479187\n",
      "Step: 1, Loss: 1.417286992073059, Accuracy: 0.25, Computation time: 4.280116558074951\n",
      "Step: 2, Loss: 1.323161005973816, Accuracy: 0.25, Computation time: 4.064533710479736\n",
      "Step: 3, Loss: 1.3732821941375732, Accuracy: 0.25, Computation time: 3.830676794052124\n",
      "Step: 4, Loss: 1.3153953552246094, Accuracy: 0.25, Computation time: 3.446611166000366\n",
      "Step: 5, Loss: 1.3780683279037476, Accuracy: 0.25, Computation time: 3.478804588317871\n",
      "Step: 6, Loss: 1.3701716661453247, Accuracy: 0.25, Computation time: 3.2954723834991455\n",
      "Step: 7, Loss: 1.372124433517456, Accuracy: 0.25, Computation time: 3.6222846508026123\n",
      "Step: 8, Loss: 1.357979416847229, Accuracy: 0.25, Computation time: 3.432652235031128\n",
      "Step: 9, Loss: 1.3450162410736084, Accuracy: 0.25, Computation time: 3.132253408432007\n",
      "Step: 10, Loss: 1.3247977495193481, Accuracy: 0.25, Computation time: 2.6294002532958984\n",
      "Step: 11, Loss: 1.3457130193710327, Accuracy: 0.25, Computation time: 3.378873586654663\n",
      "Step: 12, Loss: 1.3241292238235474, Accuracy: 0.25, Computation time: 3.251986026763916\n",
      "Step: 13, Loss: 1.34629487991333, Accuracy: 0.25, Computation time: 3.4600510597229004\n",
      "Step: 14, Loss: 1.3833385705947876, Accuracy: 0.5, Computation time: 3.2037885189056396\n",
      "Step: 15, Loss: 1.387067437171936, Accuracy: 0.25, Computation time: 3.5608232021331787\n",
      "Step: 16, Loss: 1.3875681161880493, Accuracy: 0.25, Computation time: 3.308542013168335\n",
      "Step: 17, Loss: 1.3148348331451416, Accuracy: 0.25, Computation time: 3.2897703647613525\n",
      "Step: 18, Loss: 1.242400884628296, Accuracy: 0.5, Computation time: 3.0923984050750732\n",
      "Step: 19, Loss: 1.3283528089523315, Accuracy: 0.5, Computation time: 4.107804775238037\n",
      "Step: 20, Loss: 1.220767617225647, Accuracy: 0.5, Computation time: 2.495842456817627\n",
      "Step: 21, Loss: 1.2833243608474731, Accuracy: 0.47727274894714355, Computation time: 2.137389898300171\n",
      "Step: 22, Loss: 1.2309373617172241, Accuracy: 0.5, Computation time: 1.8927109241485596\n",
      "Step: 23, Loss: 1.2146052122116089, Accuracy: 0.5, Computation time: 2.1472294330596924\n",
      "Step: 24, Loss: 1.2091150283813477, Accuracy: 0.4722222089767456, Computation time: 2.1144661903381348\n",
      "Step: 25, Loss: 1.2680176496505737, Accuracy: 0.5, Computation time: 2.224407196044922\n",
      "Step: 26, Loss: 1.2494677305221558, Accuracy: 0.4166666865348816, Computation time: 2.220393657684326\n",
      "Step: 27, Loss: 1.268938422203064, Accuracy: 0.5, Computation time: 3.7429308891296387\n",
      "Step: 28, Loss: 1.1730570793151855, Accuracy: 0.5, Computation time: 4.057380199432373\n",
      "Step: 29, Loss: 1.242548942565918, Accuracy: 0.5, Computation time: 3.455659866333008\n",
      "Step: 30, Loss: 1.132325530052185, Accuracy: 0.5, Computation time: 3.1100480556488037\n",
      "Step: 31, Loss: 1.1017699241638184, Accuracy: 0.5, Computation time: 3.5845460891723633\n",
      "Step: 32, Loss: 1.2222810983657837, Accuracy: 0.5, Computation time: 3.2853400707244873\n",
      "Step: 33, Loss: 1.2101095914840698, Accuracy: 0.5, Computation time: 3.4430155754089355\n",
      "Step: 34, Loss: 1.2068769931793213, Accuracy: 0.5, Computation time: 3.158184051513672\n",
      "Step: 35, Loss: 1.245107889175415, Accuracy: 0.5, Computation time: 3.056699275970459\n",
      "Step: 36, Loss: 1.196339726448059, Accuracy: 0.48076924681663513, Computation time: 3.08872652053833\n",
      "Step: 37, Loss: 1.1717805862426758, Accuracy: 0.5, Computation time: 3.5876755714416504\n",
      "Step: 38, Loss: 1.2452058792114258, Accuracy: 0.5, Computation time: 3.5347633361816406\n",
      "Step: 39, Loss: 1.1634809970855713, Accuracy: 0.5, Computation time: 2.996642589569092\n",
      "Step: 40, Loss: 1.2376779317855835, Accuracy: 0.5, Computation time: 2.7943243980407715\n",
      "Step: 41, Loss: 1.203258991241455, Accuracy: 0.5, Computation time: 3.1733920574188232\n",
      "Step: 42, Loss: 1.0905717611312866, Accuracy: 0.5, Computation time: 3.2858686447143555\n",
      "Step: 43, Loss: 1.2145230770111084, Accuracy: 0.46875, Computation time: 3.307607650756836\n",
      "Step: 44, Loss: 1.1328206062316895, Accuracy: 0.5, Computation time: 2.9800233840942383\n",
      "Step: 45, Loss: 1.2489677667617798, Accuracy: 0.5, Computation time: 3.4607207775115967\n",
      "Step: 46, Loss: 1.2384223937988281, Accuracy: 0.5, Computation time: 2.7710633277893066\n",
      "Step: 47, Loss: 1.1929861307144165, Accuracy: 0.5, Computation time: 2.6106956005096436\n",
      "Step: 48, Loss: 1.205274224281311, Accuracy: 0.5, Computation time: 2.905336856842041\n",
      "Step: 49, Loss: 1.2376283407211304, Accuracy: 0.5, Computation time: 2.322481632232666\n",
      "Step: 50, Loss: 1.2402948141098022, Accuracy: 0.4750000238418579, Computation time: 2.404528856277466\n",
      "Step: 51, Loss: 1.167262315750122, Accuracy: 0.5, Computation time: 2.1195027828216553\n",
      "Step: 52, Loss: 1.1917223930358887, Accuracy: 0.5, Computation time: 1.8882231712341309\n",
      "Step: 53, Loss: 1.168988823890686, Accuracy: 0.5, Computation time: 1.8722882270812988\n",
      "Step: 54, Loss: 1.1895800828933716, Accuracy: 0.5, Computation time: 2.1403448581695557\n",
      "Step: 55, Loss: 1.1372325420379639, Accuracy: 0.5, Computation time: 2.007812976837158\n",
      "Step: 56, Loss: 1.0933454036712646, Accuracy: 0.5, Computation time: 1.8675448894500732\n",
      "Step: 57, Loss: 1.160266399383545, Accuracy: 0.5, Computation time: 2.4288058280944824\n",
      "Step: 58, Loss: 1.1913392543792725, Accuracy: 0.5, Computation time: 1.6534085273742676\n",
      "Step: 59, Loss: 1.1044946908950806, Accuracy: 0.5, Computation time: 1.7900390625\n",
      "Step: 60, Loss: 1.1687421798706055, Accuracy: 0.6875, Computation time: 1.8880424499511719\n",
      "Step: 61, Loss: 1.1052125692367554, Accuracy: 0.7115384340286255, Computation time: 1.6797685623168945\n",
      "Step: 62, Loss: 1.164200782775879, Accuracy: 0.75, Computation time: 1.8406832218170166\n",
      "Step: 63, Loss: 1.0638757944107056, Accuracy: 0.75, Computation time: 1.8688721656799316\n",
      "Step: 64, Loss: 1.065545916557312, Accuracy: 0.71875, Computation time: 1.621107578277588\n",
      "Step: 65, Loss: 1.0973632335662842, Accuracy: 0.75, Computation time: 1.6113708019256592\n",
      "Step: 66, Loss: 1.021813988685608, Accuracy: 0.75, Computation time: 1.758887529373169\n",
      "Step: 67, Loss: 0.9844741821289062, Accuracy: 0.9375, Computation time: 1.801481008529663\n",
      "Step: 68, Loss: 0.9774700403213501, Accuracy: 1.0, Computation time: 1.6962361335754395\n",
      "Step: 69, Loss: 0.9756496548652649, Accuracy: 1.0, Computation time: 1.575930118560791\n",
      "Step: 70, Loss: 0.9871894717216492, Accuracy: 1.0, Computation time: 1.9590940475463867\n",
      "Step: 71, Loss: 0.9668252468109131, Accuracy: 1.0, Computation time: 1.8553972244262695\n",
      "Step: 72, Loss: 0.9620682597160339, Accuracy: 1.0, Computation time: 1.4873032569885254\n",
      "Step: 73, Loss: 0.9654867053031921, Accuracy: 1.0, Computation time: 1.79168701171875\n",
      "Step: 74, Loss: 0.9634087085723877, Accuracy: 1.0, Computation time: 1.6582231521606445\n",
      "Step: 75, Loss: 0.9408011436462402, Accuracy: 1.0, Computation time: 1.3648154735565186\n",
      "Step: 76, Loss: 0.9451119303703308, Accuracy: 1.0, Computation time: 1.4991579055786133\n",
      "Step: 77, Loss: 0.9429996609687805, Accuracy: 1.0, Computation time: 1.3585550785064697\n",
      "Step: 78, Loss: 0.9462358355522156, Accuracy: 0.9807692766189575, Computation time: 1.5798084735870361\n",
      "Step: 79, Loss: 0.9308367967605591, Accuracy: 1.0, Computation time: 1.342179775238037\n",
      "Step: 80, Loss: 0.927289605140686, Accuracy: 1.0, Computation time: 1.291322946548462\n",
      "Step: 81, Loss: 0.9265804886817932, Accuracy: 1.0, Computation time: 1.3300251960754395\n",
      "Step: 82, Loss: 0.9448082447052002, Accuracy: 0.9642857313156128, Computation time: 1.3930275440216064\n",
      "Step: 83, Loss: 0.9234859943389893, Accuracy: 1.0, Computation time: 1.880159616470337\n",
      "Step: 84, Loss: 0.9202892184257507, Accuracy: 1.0, Computation time: 1.4689428806304932\n",
      "Step: 85, Loss: 0.9369978308677673, Accuracy: 0.9772727489471436, Computation time: 1.7126762866973877\n",
      "Step: 86, Loss: 0.9206092357635498, Accuracy: 1.0, Computation time: 1.7202942371368408\n",
      "Step: 87, Loss: 0.9201381802558899, Accuracy: 1.0, Computation time: 1.551306962966919\n",
      "Step: 88, Loss: 0.9233030080795288, Accuracy: 1.0, Computation time: 1.5854496955871582\n",
      "Step: 89, Loss: 0.9298165440559387, Accuracy: 1.0, Computation time: 1.5181384086608887\n",
      "Step: 90, Loss: 0.9209036231040955, Accuracy: 1.0, Computation time: 1.3634629249572754\n",
      "Step: 91, Loss: 0.9202672839164734, Accuracy: 1.0, Computation time: 1.7912158966064453\n",
      "Step: 92, Loss: 0.9271697998046875, Accuracy: 1.0, Computation time: 1.5105838775634766\n",
      "Step: 93, Loss: 0.9284715056419373, Accuracy: 1.0, Computation time: 1.6978650093078613\n",
      "Step: 94, Loss: 0.9186521172523499, Accuracy: 1.0, Computation time: 1.2227356433868408\n",
      "Step: 95, Loss: 0.9185888767242432, Accuracy: 1.0, Computation time: 1.2600557804107666\n",
      "Step: 96, Loss: 0.9173670411109924, Accuracy: 1.0, Computation time: 1.6177761554718018\n",
      "Step: 97, Loss: 0.9190467000007629, Accuracy: 1.0, Computation time: 1.4022505283355713\n",
      "Step: 98, Loss: 0.9182088375091553, Accuracy: 1.0, Computation time: 1.6364223957061768\n",
      "Step: 99, Loss: 0.9244866967201233, Accuracy: 1.0, Computation time: 1.6072235107421875\n",
      "Step: 100, Loss: 0.9175117015838623, Accuracy: 1.0, Computation time: 1.255138874053955\n",
      "Step: 101, Loss: 0.939324140548706, Accuracy: 0.9791666865348816, Computation time: 1.1614794731140137\n",
      "Step: 102, Loss: 0.9174816608428955, Accuracy: 1.0, Computation time: 1.2688994407653809\n",
      "Step: 103, Loss: 0.9178986549377441, Accuracy: 1.0, Computation time: 1.4891371726989746\n",
      "Step: 104, Loss: 0.9174956679344177, Accuracy: 1.0, Computation time: 1.177670955657959\n",
      "Step: 105, Loss: 0.920944094657898, Accuracy: 1.0, Computation time: 1.2961416244506836\n",
      "Step: 106, Loss: 0.9182156920433044, Accuracy: 1.0, Computation time: 1.234752893447876\n",
      "Step: 107, Loss: 0.917054295539856, Accuracy: 1.0, Computation time: 1.5249593257904053\n",
      "Step: 108, Loss: 0.9571044445037842, Accuracy: 0.9392857551574707, Computation time: 1.4325952529907227\n",
      "Step: 109, Loss: 0.9168121814727783, Accuracy: 1.0, Computation time: 1.6263494491577148\n",
      "Step: 110, Loss: 0.9166609644889832, Accuracy: 1.0, Computation time: 1.261641502380371\n",
      "Step: 111, Loss: 0.9170066118240356, Accuracy: 1.0, Computation time: 1.3340537548065186\n",
      "Step: 112, Loss: 0.9191166758537292, Accuracy: 1.0, Computation time: 1.3822815418243408\n",
      "Step: 113, Loss: 0.9255509376525879, Accuracy: 1.0, Computation time: 1.2176742553710938\n",
      "Step: 114, Loss: 0.9180643558502197, Accuracy: 1.0, Computation time: 1.3509316444396973\n",
      "Step: 115, Loss: 0.9381527900695801, Accuracy: 0.9750000238418579, Computation time: 1.2139737606048584\n",
      "Step: 116, Loss: 0.938206672668457, Accuracy: 0.9807692766189575, Computation time: 1.2249889373779297\n",
      "Step: 117, Loss: 0.9165603518486023, Accuracy: 1.0, Computation time: 1.429628610610962\n",
      "Step: 118, Loss: 0.9167818427085876, Accuracy: 1.0, Computation time: 1.4285407066345215\n",
      "Step: 119, Loss: 0.9243711233139038, Accuracy: 1.0, Computation time: 1.508265733718872\n",
      "Step: 120, Loss: 0.917610764503479, Accuracy: 1.0, Computation time: 1.700408935546875\n",
      "Step: 121, Loss: 0.9229326248168945, Accuracy: 1.0, Computation time: 1.3025884628295898\n",
      "Step: 122, Loss: 0.9323611259460449, Accuracy: 0.9750000238418579, Computation time: 1.5642058849334717\n",
      "Step: 123, Loss: 0.926956832408905, Accuracy: 1.0, Computation time: 1.8451929092407227\n",
      "Step: 124, Loss: 0.9255102872848511, Accuracy: 1.0, Computation time: 1.5050485134124756\n",
      "Step: 125, Loss: 0.9499601125717163, Accuracy: 0.970588207244873, Computation time: 1.8280606269836426\n",
      "Step: 126, Loss: 0.9215812087059021, Accuracy: 1.0, Computation time: 1.2388734817504883\n",
      "Step: 127, Loss: 0.9223030209541321, Accuracy: 1.0, Computation time: 1.3886985778808594\n",
      "Step: 128, Loss: 0.9190493822097778, Accuracy: 1.0, Computation time: 1.2157042026519775\n",
      "Step: 129, Loss: 0.9188087582588196, Accuracy: 1.0, Computation time: 1.6342780590057373\n",
      "Step: 130, Loss: 0.9169795513153076, Accuracy: 1.0, Computation time: 1.1491796970367432\n",
      "Step: 131, Loss: 0.9180795550346375, Accuracy: 1.0, Computation time: 1.2718636989593506\n",
      "Step: 132, Loss: 0.9194904565811157, Accuracy: 1.0, Computation time: 1.2437169551849365\n",
      "Step: 133, Loss: 0.9190257787704468, Accuracy: 1.0, Computation time: 1.3011043071746826\n",
      "Step: 134, Loss: 0.9194678068161011, Accuracy: 1.0, Computation time: 1.1363506317138672\n",
      "Step: 135, Loss: 0.9188684225082397, Accuracy: 1.0, Computation time: 1.3907577991485596\n",
      "Step: 136, Loss: 0.916703999042511, Accuracy: 1.0, Computation time: 1.5946271419525146\n",
      "Step: 137, Loss: 0.9176583886146545, Accuracy: 1.0, Computation time: 1.3456594944000244\n",
      "Step: 138, Loss: 0.9185981750488281, Accuracy: 1.0, Computation time: 1.1664226055145264\n",
      "Step: 139, Loss: 0.9174178838729858, Accuracy: 1.0, Computation time: 1.485384464263916\n",
      "########################\n",
      "Test loss: 1.0792049169540405, Test Accuracy_epoch1: 0.7543707489967346\n",
      "########################\n",
      "Step: 140, Loss: 0.9167932271957397, Accuracy: 1.0, Computation time: 1.6574883460998535\n",
      "Step: 141, Loss: 0.9164592027664185, Accuracy: 1.0, Computation time: 1.2423019409179688\n",
      "Step: 142, Loss: 0.9175598621368408, Accuracy: 1.0, Computation time: 1.2008733749389648\n",
      "Step: 143, Loss: 0.9379398822784424, Accuracy: 0.9583333730697632, Computation time: 1.5032086372375488\n",
      "Step: 144, Loss: 0.9165972471237183, Accuracy: 1.0, Computation time: 1.2685322761535645\n",
      "Step: 145, Loss: 0.9389800429344177, Accuracy: 0.9791666865348816, Computation time: 1.397261142730713\n",
      "Step: 146, Loss: 0.9255498051643372, Accuracy: 1.0, Computation time: 1.4452693462371826\n",
      "Step: 147, Loss: 0.9183686375617981, Accuracy: 1.0, Computation time: 1.6474456787109375\n",
      "Step: 148, Loss: 0.9167165756225586, Accuracy: 1.0, Computation time: 1.8670024871826172\n",
      "Step: 149, Loss: 0.9174439907073975, Accuracy: 1.0, Computation time: 1.1694183349609375\n",
      "Step: 150, Loss: 0.929962158203125, Accuracy: 0.9750000238418579, Computation time: 1.735194444656372\n",
      "Step: 151, Loss: 0.9163045883178711, Accuracy: 1.0, Computation time: 1.4405090808868408\n",
      "Step: 152, Loss: 0.9178875684738159, Accuracy: 1.0, Computation time: 1.346064567565918\n",
      "Step: 153, Loss: 0.9228569865226746, Accuracy: 1.0, Computation time: 2.3073675632476807\n",
      "Step: 154, Loss: 0.9209344983100891, Accuracy: 1.0, Computation time: 1.3914666175842285\n",
      "Step: 155, Loss: 0.9168316721916199, Accuracy: 1.0, Computation time: 1.307478427886963\n",
      "Step: 156, Loss: 0.917112410068512, Accuracy: 1.0, Computation time: 1.5608303546905518\n",
      "Step: 157, Loss: 0.916724443435669, Accuracy: 1.0, Computation time: 1.2425084114074707\n",
      "Step: 158, Loss: 0.9377135038375854, Accuracy: 0.96875, Computation time: 2.292872905731201\n",
      "Step: 159, Loss: 0.9165675640106201, Accuracy: 1.0, Computation time: 2.0608842372894287\n",
      "Step: 160, Loss: 0.9346306920051575, Accuracy: 0.9750000238418579, Computation time: 1.8777437210083008\n",
      "Step: 161, Loss: 0.9187391400337219, Accuracy: 1.0, Computation time: 3.3640732765197754\n",
      "Step: 162, Loss: 0.9164305925369263, Accuracy: 1.0, Computation time: 1.9713797569274902\n",
      "Step: 163, Loss: 0.9164150953292847, Accuracy: 1.0, Computation time: 2.2251882553100586\n",
      "Step: 164, Loss: 0.918491005897522, Accuracy: nan, Computation time: 1.981083631515503\n",
      "Step: 165, Loss: 0.9161643385887146, Accuracy: 1.0, Computation time: 2.2160427570343018\n",
      "Step: 166, Loss: 0.9165440201759338, Accuracy: 1.0, Computation time: 1.7723286151885986\n",
      "Step: 167, Loss: 0.9163442254066467, Accuracy: 1.0, Computation time: 2.306957483291626\n",
      "Step: 168, Loss: 0.9168258905410767, Accuracy: 1.0, Computation time: 2.6102776527404785\n",
      "Step: 169, Loss: 0.9194310903549194, Accuracy: 1.0, Computation time: 2.394212484359741\n",
      "Step: 170, Loss: 0.922446072101593, Accuracy: 1.0, Computation time: 2.446373701095581\n",
      "Step: 171, Loss: 0.9171907305717468, Accuracy: 1.0, Computation time: 1.9568977355957031\n",
      "Step: 172, Loss: 0.9169180989265442, Accuracy: 1.0, Computation time: 1.8506200313568115\n",
      "Step: 173, Loss: 0.9179034233093262, Accuracy: 1.0, Computation time: 2.1230077743530273\n",
      "Step: 174, Loss: 0.9163314700126648, Accuracy: 1.0, Computation time: 2.424625873565674\n",
      "Step: 175, Loss: 0.9210756421089172, Accuracy: 1.0, Computation time: 2.7241532802581787\n",
      "Step: 176, Loss: 0.9164838194847107, Accuracy: 1.0, Computation time: 1.9495124816894531\n",
      "Step: 177, Loss: 0.9188627600669861, Accuracy: 1.0, Computation time: 2.3563685417175293\n",
      "Step: 178, Loss: 0.9206694960594177, Accuracy: 1.0, Computation time: 1.9560868740081787\n",
      "Step: 179, Loss: 0.9374286532402039, Accuracy: 0.984375, Computation time: 2.460007905960083\n",
      "Step: 180, Loss: 0.9379690885543823, Accuracy: 0.9791666865348816, Computation time: 1.4550917148590088\n",
      "Step: 181, Loss: 0.9164998531341553, Accuracy: 1.0, Computation time: 1.3193960189819336\n",
      "Step: 182, Loss: 0.9601786732673645, Accuracy: 0.9583333730697632, Computation time: 1.3253731727600098\n",
      "Step: 183, Loss: 0.9172074794769287, Accuracy: 1.0, Computation time: 1.3499822616577148\n",
      "Step: 184, Loss: 0.9174785614013672, Accuracy: 1.0, Computation time: 1.623828411102295\n",
      "Step: 185, Loss: 0.9162279963493347, Accuracy: 1.0, Computation time: 1.247823715209961\n",
      "Step: 186, Loss: 0.9162776470184326, Accuracy: 1.0, Computation time: 1.835364818572998\n",
      "Step: 187, Loss: 0.9215816259384155, Accuracy: 1.0, Computation time: 1.6571381092071533\n",
      "Step: 188, Loss: 0.9173364639282227, Accuracy: 1.0, Computation time: 1.3945379257202148\n",
      "Step: 189, Loss: 0.9165742993354797, Accuracy: 1.0, Computation time: 1.4879326820373535\n",
      "Step: 190, Loss: 0.9163392186164856, Accuracy: 1.0, Computation time: 1.3985629081726074\n",
      "Step: 191, Loss: 0.9183472990989685, Accuracy: 1.0, Computation time: 1.3122270107269287\n",
      "Step: 192, Loss: 0.9607597589492798, Accuracy: 0.9356061220169067, Computation time: 1.5715036392211914\n",
      "Step: 193, Loss: 0.9213244915008545, Accuracy: 1.0, Computation time: 1.1686909198760986\n",
      "Step: 194, Loss: 0.9162945747375488, Accuracy: 1.0, Computation time: 1.6063830852508545\n",
      "Step: 195, Loss: 0.9164929986000061, Accuracy: 1.0, Computation time: 1.381753921508789\n",
      "Step: 196, Loss: 0.9346746802330017, Accuracy: 0.96875, Computation time: 1.2404558658599854\n",
      "Step: 197, Loss: 0.9170572757720947, Accuracy: 1.0, Computation time: 1.3462109565734863\n",
      "Step: 198, Loss: 0.9163856506347656, Accuracy: 1.0, Computation time: 1.364722490310669\n",
      "Step: 199, Loss: 0.9413358569145203, Accuracy: 0.96875, Computation time: 1.34794020652771\n",
      "Step: 200, Loss: 0.9165282845497131, Accuracy: 1.0, Computation time: 1.2977173328399658\n",
      "Step: 201, Loss: 0.916521430015564, Accuracy: 1.0, Computation time: 1.2084581851959229\n",
      "Step: 202, Loss: 0.9163728356361389, Accuracy: 1.0, Computation time: 1.1428680419921875\n",
      "Step: 203, Loss: 0.9493904709815979, Accuracy: 0.9722222089767456, Computation time: 1.6571941375732422\n",
      "Step: 204, Loss: 0.920073390007019, Accuracy: 1.0, Computation time: 1.62306809425354\n",
      "Step: 205, Loss: 0.9227690100669861, Accuracy: 1.0, Computation time: 1.9500806331634521\n",
      "Step: 206, Loss: 0.9162262082099915, Accuracy: 1.0, Computation time: 1.1135437488555908\n",
      "Step: 207, Loss: 0.9163829684257507, Accuracy: 1.0, Computation time: 1.3210318088531494\n",
      "Step: 208, Loss: 0.9248413443565369, Accuracy: 1.0, Computation time: 1.1629269123077393\n",
      "Step: 209, Loss: 0.9165693521499634, Accuracy: 1.0, Computation time: 1.407926321029663\n",
      "Step: 210, Loss: 0.9311091899871826, Accuracy: 0.9772727489471436, Computation time: 1.6311721801757812\n",
      "Step: 211, Loss: 0.928341805934906, Accuracy: 1.0, Computation time: 1.696594476699829\n",
      "Step: 212, Loss: 0.9189111590385437, Accuracy: 1.0, Computation time: 1.5331478118896484\n",
      "Step: 213, Loss: 0.9167572259902954, Accuracy: 1.0, Computation time: 1.5888562202453613\n",
      "Step: 214, Loss: 0.9403285980224609, Accuracy: 0.9821428656578064, Computation time: 1.4478435516357422\n",
      "Step: 215, Loss: 0.9167217016220093, Accuracy: 1.0, Computation time: 1.8973054885864258\n",
      "Step: 216, Loss: 0.9170041084289551, Accuracy: 1.0, Computation time: 1.5855045318603516\n",
      "Step: 217, Loss: 0.9170548319816589, Accuracy: 1.0, Computation time: 1.7666544914245605\n",
      "Step: 218, Loss: 0.9306252598762512, Accuracy: 0.9642857313156128, Computation time: 1.386784315109253\n",
      "Step: 219, Loss: 0.9164402484893799, Accuracy: 1.0, Computation time: 1.2328779697418213\n",
      "Step: 220, Loss: 0.9161111116409302, Accuracy: 1.0, Computation time: 1.092789888381958\n",
      "Step: 221, Loss: 0.9164033532142639, Accuracy: 1.0, Computation time: 1.197812557220459\n",
      "Step: 222, Loss: 0.9379028677940369, Accuracy: 0.9772727489471436, Computation time: 1.6068167686462402\n",
      "Step: 223, Loss: 0.9196533560752869, Accuracy: 1.0, Computation time: 1.4939055442810059\n",
      "Step: 224, Loss: 0.9208459258079529, Accuracy: 1.0, Computation time: 1.5095672607421875\n",
      "Step: 225, Loss: 0.9168710112571716, Accuracy: 1.0, Computation time: 1.337996006011963\n",
      "Step: 226, Loss: 0.9163541197776794, Accuracy: 1.0, Computation time: 1.645108938217163\n",
      "Step: 227, Loss: 0.9163857698440552, Accuracy: 1.0, Computation time: 1.1485443115234375\n",
      "Step: 228, Loss: 0.9374503493309021, Accuracy: 0.9772727489471436, Computation time: 1.134397029876709\n",
      "Step: 229, Loss: 0.9278092384338379, Accuracy: 1.0, Computation time: 1.4360604286193848\n",
      "Step: 230, Loss: 0.9163112640380859, Accuracy: 1.0, Computation time: 1.1206047534942627\n",
      "Step: 231, Loss: 0.916347086429596, Accuracy: 1.0, Computation time: 1.7338464260101318\n",
      "Step: 232, Loss: 0.9351292252540588, Accuracy: 0.984375, Computation time: 1.4593915939331055\n",
      "Step: 233, Loss: 0.9394235610961914, Accuracy: 0.9772727489471436, Computation time: 1.9798822402954102\n",
      "Step: 234, Loss: 0.9161362051963806, Accuracy: 1.0, Computation time: 1.2046630382537842\n",
      "Step: 235, Loss: 0.9242208003997803, Accuracy: 1.0, Computation time: 1.8003473281860352\n",
      "Step: 236, Loss: 0.9166975021362305, Accuracy: 1.0, Computation time: 1.3246662616729736\n",
      "Step: 237, Loss: 0.9161707758903503, Accuracy: 1.0, Computation time: 1.1413772106170654\n",
      "Step: 238, Loss: 0.9164072275161743, Accuracy: 1.0, Computation time: 1.4270493984222412\n",
      "Step: 239, Loss: 0.9337660074234009, Accuracy: 0.9791666865348816, Computation time: 1.5435471534729004\n",
      "Step: 240, Loss: 0.9162123799324036, Accuracy: 1.0, Computation time: 1.2037842273712158\n",
      "Step: 241, Loss: 0.9575751423835754, Accuracy: 0.949999988079071, Computation time: 1.4494929313659668\n",
      "Step: 242, Loss: 0.9182946085929871, Accuracy: 1.0, Computation time: 1.258678913116455\n",
      "Step: 243, Loss: 0.9181784391403198, Accuracy: 1.0, Computation time: 1.514503002166748\n",
      "Step: 244, Loss: 0.9176236987113953, Accuracy: 1.0, Computation time: 1.268371343612671\n",
      "Step: 245, Loss: 0.9167852401733398, Accuracy: 1.0, Computation time: 1.4685029983520508\n",
      "Step: 246, Loss: 0.9164929986000061, Accuracy: 1.0, Computation time: 1.1465137004852295\n",
      "Step: 247, Loss: 0.919904887676239, Accuracy: 1.0, Computation time: 2.023650646209717\n",
      "Step: 248, Loss: 0.9320144653320312, Accuracy: 0.9833333492279053, Computation time: 1.6233012676239014\n",
      "Step: 249, Loss: 0.9171149730682373, Accuracy: 1.0, Computation time: 1.474339485168457\n",
      "Step: 250, Loss: 0.9170758128166199, Accuracy: 1.0, Computation time: 1.4173059463500977\n",
      "Step: 251, Loss: 0.9160497784614563, Accuracy: 1.0, Computation time: 1.314255714416504\n",
      "Step: 252, Loss: 0.9294439554214478, Accuracy: 0.96875, Computation time: 1.176196575164795\n",
      "Step: 253, Loss: 0.9573560953140259, Accuracy: 0.9391025900840759, Computation time: 1.6394679546356201\n",
      "Step: 254, Loss: 0.916111409664154, Accuracy: 1.0, Computation time: 1.2384798526763916\n",
      "Step: 255, Loss: 0.9162604808807373, Accuracy: 1.0, Computation time: 1.1879143714904785\n",
      "Step: 256, Loss: 0.9209541082382202, Accuracy: 1.0, Computation time: 1.511286735534668\n",
      "Step: 257, Loss: 0.9163225889205933, Accuracy: 1.0, Computation time: 1.6203534603118896\n",
      "Step: 258, Loss: 0.9171673059463501, Accuracy: 1.0, Computation time: 1.3992135524749756\n",
      "Step: 259, Loss: 0.9174560308456421, Accuracy: 1.0, Computation time: 1.3419971466064453\n",
      "Step: 260, Loss: 0.9163611531257629, Accuracy: 1.0, Computation time: 1.3833987712860107\n",
      "Step: 261, Loss: 0.9160893559455872, Accuracy: 1.0, Computation time: 1.2865753173828125\n",
      "Step: 262, Loss: 0.9341810345649719, Accuracy: 0.9807692766189575, Computation time: 1.1571080684661865\n",
      "Step: 263, Loss: 0.9161115884780884, Accuracy: 1.0, Computation time: 1.164135217666626\n",
      "Step: 264, Loss: 0.9161226153373718, Accuracy: 1.0, Computation time: 1.1582155227661133\n",
      "Step: 265, Loss: 0.9415929317474365, Accuracy: 0.9821428656578064, Computation time: 1.1543223857879639\n",
      "Step: 266, Loss: 0.9163967967033386, Accuracy: 1.0, Computation time: 1.3261160850524902\n",
      "Step: 267, Loss: 0.9163837432861328, Accuracy: 1.0, Computation time: 1.572972059249878\n",
      "Step: 268, Loss: 0.9160066246986389, Accuracy: 1.0, Computation time: 1.2101478576660156\n",
      "Step: 269, Loss: 0.9303653836250305, Accuracy: 0.9807692766189575, Computation time: 1.3974764347076416\n",
      "Step: 270, Loss: 0.9161839485168457, Accuracy: 1.0, Computation time: 1.185791015625\n",
      "Step: 271, Loss: 0.9162015318870544, Accuracy: 1.0, Computation time: 2.4673306941986084\n",
      "Step: 272, Loss: 0.916222333908081, Accuracy: 1.0, Computation time: 2.4634482860565186\n",
      "Step: 273, Loss: 0.9414048790931702, Accuracy: 0.9722222089767456, Computation time: 2.15274715423584\n",
      "Step: 274, Loss: 0.9162883162498474, Accuracy: 1.0, Computation time: 2.168887138366699\n",
      "Step: 275, Loss: 0.9166098833084106, Accuracy: 1.0, Computation time: 2.1913514137268066\n",
      "Step: 276, Loss: 0.9233955144882202, Accuracy: 1.0, Computation time: 2.602504014968872\n",
      "Step: 277, Loss: 0.9188705682754517, Accuracy: 1.0, Computation time: 2.4394826889038086\n",
      "Step: 278, Loss: 0.9168910384178162, Accuracy: 1.0, Computation time: 2.685882806777954\n",
      "########################\n",
      "Test loss: 1.076758861541748, Test Accuracy_epoch2: 0.7575951814651489\n",
      "########################\n",
      "Step: 279, Loss: 0.9167275428771973, Accuracy: 1.0, Computation time: 1.907073736190796\n",
      "Step: 280, Loss: 0.9160850048065186, Accuracy: 1.0, Computation time: 1.8546316623687744\n",
      "Step: 281, Loss: 0.9166375994682312, Accuracy: 1.0, Computation time: 2.673830509185791\n",
      "Step: 282, Loss: 0.9162455201148987, Accuracy: 1.0, Computation time: 2.061980724334717\n",
      "Step: 283, Loss: 0.9243969321250916, Accuracy: 1.0, Computation time: 2.015921115875244\n",
      "Step: 284, Loss: 0.9170568585395813, Accuracy: 1.0, Computation time: 2.2654268741607666\n",
      "Step: 285, Loss: 0.9218273162841797, Accuracy: 1.0, Computation time: 1.9783754348754883\n",
      "Step: 286, Loss: 0.9172762036323547, Accuracy: 1.0, Computation time: 2.286966562271118\n",
      "Step: 287, Loss: 0.9161527156829834, Accuracy: 1.0, Computation time: 1.7885384559631348\n",
      "Step: 288, Loss: 0.9219436049461365, Accuracy: 1.0, Computation time: 2.6306610107421875\n",
      "Step: 289, Loss: 0.9412137866020203, Accuracy: 0.9772727489471436, Computation time: 2.3957772254943848\n",
      "Step: 290, Loss: 0.9389864206314087, Accuracy: 0.9722222089767456, Computation time: 2.411729574203491\n",
      "Step: 291, Loss: 0.9160592555999756, Accuracy: 1.0, Computation time: 1.8612656593322754\n",
      "Step: 292, Loss: 0.9164981842041016, Accuracy: 1.0, Computation time: 1.985769271850586\n",
      "Step: 293, Loss: 0.9218626618385315, Accuracy: 1.0, Computation time: 2.2137527465820312\n",
      "Step: 294, Loss: 0.9246007204055786, Accuracy: 1.0, Computation time: 2.6316745281219482\n",
      "Step: 295, Loss: 0.9380117654800415, Accuracy: 0.9583333730697632, Computation time: 1.9336249828338623\n",
      "Step: 296, Loss: 0.9162169098854065, Accuracy: 1.0, Computation time: 2.07224178314209\n",
      "Step: 297, Loss: 0.9353583455085754, Accuracy: 0.9583333730697632, Computation time: 2.4809768199920654\n",
      "Step: 298, Loss: 0.916154146194458, Accuracy: 1.0, Computation time: 2.4500930309295654\n",
      "Step: 299, Loss: 0.9362527132034302, Accuracy: 0.9750000238418579, Computation time: 3.0066330432891846\n",
      "Step: 300, Loss: 0.9159850478172302, Accuracy: 1.0, Computation time: 1.8433647155761719\n",
      "Step: 301, Loss: 0.9164442420005798, Accuracy: 1.0, Computation time: 1.783405065536499\n",
      "Step: 302, Loss: 0.9399373531341553, Accuracy: 0.9642857313156128, Computation time: 2.1808481216430664\n",
      "Step: 303, Loss: 0.916077196598053, Accuracy: 1.0, Computation time: 3.140817403793335\n",
      "Step: 304, Loss: 0.939489483833313, Accuracy: 0.9772727489471436, Computation time: 2.3848724365234375\n",
      "Step: 305, Loss: 0.9164734482765198, Accuracy: 1.0, Computation time: 3.724626302719116\n",
      "Step: 306, Loss: 0.9164714813232422, Accuracy: 1.0, Computation time: 2.247056722640991\n",
      "Step: 307, Loss: 0.9184243083000183, Accuracy: 1.0, Computation time: 2.856149911880493\n",
      "Step: 308, Loss: 0.9163413047790527, Accuracy: 1.0, Computation time: 2.384291648864746\n",
      "Step: 309, Loss: 0.9182256460189819, Accuracy: 1.0, Computation time: 2.2300238609313965\n",
      "Step: 310, Loss: 0.9166700839996338, Accuracy: 1.0, Computation time: 1.8649849891662598\n",
      "Step: 311, Loss: 0.9450771808624268, Accuracy: 0.9807692766189575, Computation time: 2.82958722114563\n",
      "Step: 312, Loss: 0.9160009622573853, Accuracy: 1.0, Computation time: 2.118655204772949\n",
      "Step: 313, Loss: 0.9388129115104675, Accuracy: 0.96875, Computation time: 2.1732044219970703\n",
      "Step: 314, Loss: 0.9164769053459167, Accuracy: 1.0, Computation time: 2.276996612548828\n",
      "Step: 315, Loss: 0.916283369064331, Accuracy: 1.0, Computation time: 1.9548754692077637\n",
      "Step: 316, Loss: 0.9365637898445129, Accuracy: 0.949999988079071, Computation time: 2.0614330768585205\n",
      "Step: 317, Loss: 0.9170430302619934, Accuracy: 1.0, Computation time: 2.5507900714874268\n",
      "Step: 318, Loss: 0.9160336256027222, Accuracy: 1.0, Computation time: 1.9552316665649414\n",
      "Step: 319, Loss: 0.9191496968269348, Accuracy: 1.0, Computation time: 2.5228826999664307\n",
      "Step: 320, Loss: 0.9218358397483826, Accuracy: 1.0, Computation time: 2.1708381175994873\n",
      "Step: 321, Loss: 0.9371207356452942, Accuracy: 0.9583333730697632, Computation time: 2.585310935974121\n",
      "Step: 322, Loss: 0.9366657137870789, Accuracy: 0.9833333492279053, Computation time: 2.3561885356903076\n",
      "Step: 323, Loss: 0.916234016418457, Accuracy: 1.0, Computation time: 2.3106281757354736\n",
      "Step: 324, Loss: 0.9161376357078552, Accuracy: 1.0, Computation time: 2.1286942958831787\n",
      "Step: 325, Loss: 0.9160885214805603, Accuracy: 1.0, Computation time: 2.6498358249664307\n",
      "Step: 326, Loss: 0.9163489937782288, Accuracy: 1.0, Computation time: 2.8193070888519287\n",
      "Step: 327, Loss: 0.9361506104469299, Accuracy: 0.9807692766189575, Computation time: 3.0033514499664307\n",
      "Step: 328, Loss: 0.9164817929267883, Accuracy: 1.0, Computation time: 2.7386345863342285\n",
      "Step: 329, Loss: 0.9160411953926086, Accuracy: 1.0, Computation time: 2.2288029193878174\n",
      "Step: 330, Loss: 0.9379579424858093, Accuracy: 0.96875, Computation time: 1.909057378768921\n",
      "Step: 331, Loss: 0.9217934608459473, Accuracy: 1.0, Computation time: 2.7610416412353516\n",
      "Step: 332, Loss: 0.9380944967269897, Accuracy: 0.9821428656578064, Computation time: 2.4734230041503906\n",
      "Step: 333, Loss: 0.9317824840545654, Accuracy: 0.9821428656578064, Computation time: 2.3855135440826416\n",
      "Step: 334, Loss: 0.9322245121002197, Accuracy: 0.9772727489471436, Computation time: 2.322718381881714\n",
      "Step: 335, Loss: 0.9162518978118896, Accuracy: 1.0, Computation time: 2.0603504180908203\n",
      "Step: 336, Loss: 0.9163639545440674, Accuracy: 1.0, Computation time: 2.4962124824523926\n",
      "Step: 337, Loss: 0.9239631295204163, Accuracy: 1.0, Computation time: 2.338033437728882\n",
      "Step: 338, Loss: 0.9476149082183838, Accuracy: 0.9583333730697632, Computation time: 2.0440714359283447\n",
      "Step: 339, Loss: 0.9162020683288574, Accuracy: 1.0, Computation time: 2.087329626083374\n",
      "Step: 340, Loss: 0.917037844657898, Accuracy: 1.0, Computation time: 2.0541388988494873\n",
      "Step: 341, Loss: 0.916228711605072, Accuracy: 1.0, Computation time: 2.576615571975708\n",
      "Step: 342, Loss: 0.9161964058876038, Accuracy: 1.0, Computation time: 1.7302663326263428\n",
      "Step: 343, Loss: 0.9178938865661621, Accuracy: 1.0, Computation time: 2.233185052871704\n",
      "Step: 344, Loss: 0.9275804162025452, Accuracy: 0.9750000238418579, Computation time: 2.231996536254883\n",
      "Step: 345, Loss: 0.9355440735816956, Accuracy: 0.9583333730697632, Computation time: 1.915632724761963\n",
      "Step: 346, Loss: 0.9162874817848206, Accuracy: 1.0, Computation time: 2.2833070755004883\n",
      "Step: 347, Loss: 0.9160908460617065, Accuracy: 1.0, Computation time: 2.031583309173584\n",
      "Step: 348, Loss: 0.9162130951881409, Accuracy: 1.0, Computation time: 1.981065273284912\n",
      "Step: 349, Loss: 0.9159685373306274, Accuracy: 1.0, Computation time: 1.8750519752502441\n",
      "Step: 350, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 1.898477554321289\n",
      "Step: 351, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 2.1946489810943604\n",
      "Step: 352, Loss: 0.9335821866989136, Accuracy: 0.9166666865348816, Computation time: 2.115739583969116\n",
      "Step: 353, Loss: 0.9176006317138672, Accuracy: 1.0, Computation time: 1.7289783954620361\n",
      "Step: 354, Loss: 0.9159626364707947, Accuracy: 1.0, Computation time: 2.223529100418091\n",
      "Step: 355, Loss: 0.9160411953926086, Accuracy: 1.0, Computation time: 2.194153070449829\n",
      "Step: 356, Loss: 0.9160224795341492, Accuracy: 1.0, Computation time: 1.8077561855316162\n",
      "Step: 357, Loss: 0.9160811901092529, Accuracy: 1.0, Computation time: 1.9963245391845703\n",
      "Step: 358, Loss: 0.9162341356277466, Accuracy: 1.0, Computation time: 2.14811110496521\n",
      "Step: 359, Loss: 0.9160559177398682, Accuracy: 1.0, Computation time: 1.982131004333496\n",
      "Step: 360, Loss: 0.9575961232185364, Accuracy: 0.9305555820465088, Computation time: 2.590823173522949\n",
      "Step: 361, Loss: 0.916189432144165, Accuracy: 1.0, Computation time: 2.676201105117798\n",
      "Step: 362, Loss: 0.9163433909416199, Accuracy: 1.0, Computation time: 2.1160805225372314\n",
      "Step: 363, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 2.3167340755462646\n",
      "Step: 364, Loss: 0.9271839261054993, Accuracy: 1.0, Computation time: 2.253199815750122\n",
      "Step: 365, Loss: 0.9160903692245483, Accuracy: 1.0, Computation time: 1.9968757629394531\n",
      "Step: 366, Loss: 0.9160820245742798, Accuracy: 1.0, Computation time: 2.04284405708313\n",
      "Step: 367, Loss: 0.9159600138664246, Accuracy: 1.0, Computation time: 2.551499128341675\n",
      "Step: 368, Loss: 0.9311361312866211, Accuracy: 0.9722222089767456, Computation time: 1.8995914459228516\n",
      "Step: 369, Loss: 0.9175585508346558, Accuracy: 1.0, Computation time: 2.189544200897217\n",
      "Step: 370, Loss: 0.9160975813865662, Accuracy: 1.0, Computation time: 2.2016565799713135\n",
      "Step: 371, Loss: 0.9191431403160095, Accuracy: 1.0, Computation time: 2.0821049213409424\n",
      "Step: 372, Loss: 0.9389269948005676, Accuracy: 0.9642857313156128, Computation time: 2.154186964035034\n",
      "Step: 373, Loss: 0.916150689125061, Accuracy: 1.0, Computation time: 1.7685673236846924\n",
      "Step: 374, Loss: 0.9172703623771667, Accuracy: 1.0, Computation time: 2.8734710216522217\n",
      "Step: 375, Loss: 0.9161201119422913, Accuracy: 1.0, Computation time: 2.004528760910034\n",
      "Step: 376, Loss: 0.9163450598716736, Accuracy: 1.0, Computation time: 2.144465684890747\n",
      "Step: 377, Loss: 0.9242291450500488, Accuracy: 1.0, Computation time: 2.096153736114502\n",
      "Step: 378, Loss: 0.9161601662635803, Accuracy: 1.0, Computation time: 2.2965333461761475\n",
      "Step: 379, Loss: 0.923641562461853, Accuracy: 1.0, Computation time: 2.6658143997192383\n",
      "Step: 380, Loss: 0.9162961840629578, Accuracy: 1.0, Computation time: 2.7641122341156006\n",
      "Step: 381, Loss: 0.9161635041236877, Accuracy: 1.0, Computation time: 2.015406847000122\n",
      "Step: 382, Loss: 0.916022777557373, Accuracy: 1.0, Computation time: 2.658996105194092\n",
      "Step: 383, Loss: 0.929314136505127, Accuracy: 0.9750000238418579, Computation time: 2.2804160118103027\n",
      "Step: 384, Loss: 0.9366965889930725, Accuracy: 0.9807692766189575, Computation time: 2.4868569374084473\n",
      "Step: 385, Loss: 0.9160997271537781, Accuracy: 1.0, Computation time: 2.5940544605255127\n",
      "Step: 386, Loss: 0.9161272048950195, Accuracy: 1.0, Computation time: 1.9406945705413818\n",
      "Step: 387, Loss: 0.9163375496864319, Accuracy: 1.0, Computation time: 2.3709123134613037\n",
      "Step: 388, Loss: 0.9389914274215698, Accuracy: 0.9722222089767456, Computation time: 2.1501760482788086\n",
      "Step: 389, Loss: 0.9165082573890686, Accuracy: 1.0, Computation time: 1.8074688911437988\n",
      "Step: 390, Loss: 0.9186775088310242, Accuracy: 1.0, Computation time: 2.0087225437164307\n",
      "Step: 391, Loss: 0.9167127013206482, Accuracy: 1.0, Computation time: 2.433234930038452\n",
      "Step: 392, Loss: 0.9164911508560181, Accuracy: 1.0, Computation time: 1.9542486667633057\n",
      "Step: 393, Loss: 0.9160012602806091, Accuracy: 1.0, Computation time: 2.011530637741089\n",
      "Step: 394, Loss: 0.916094958782196, Accuracy: 1.0, Computation time: 2.0210306644439697\n",
      "Step: 395, Loss: 0.9162012338638306, Accuracy: 1.0, Computation time: 1.9951848983764648\n",
      "Step: 396, Loss: 0.9174681305885315, Accuracy: 1.0, Computation time: 3.095064163208008\n",
      "Step: 397, Loss: 0.9163444638252258, Accuracy: 1.0, Computation time: 1.9961464405059814\n",
      "Step: 398, Loss: 0.9161531329154968, Accuracy: 1.0, Computation time: 1.968916893005371\n",
      "Step: 399, Loss: 0.9360527396202087, Accuracy: 0.9750000238418579, Computation time: 2.096306085586548\n",
      "Step: 400, Loss: 0.9160839319229126, Accuracy: 1.0, Computation time: 3.1121299266815186\n",
      "Step: 401, Loss: 0.9161683917045593, Accuracy: 1.0, Computation time: 2.5702719688415527\n",
      "Step: 402, Loss: 0.9160207509994507, Accuracy: 1.0, Computation time: 1.9066669940948486\n",
      "Step: 403, Loss: 0.9376739859580994, Accuracy: 0.9821428656578064, Computation time: 2.9869256019592285\n",
      "Step: 404, Loss: 0.9168359637260437, Accuracy: 1.0, Computation time: 1.894840955734253\n",
      "Step: 405, Loss: 0.9160612225532532, Accuracy: 1.0, Computation time: 2.2156877517700195\n",
      "Step: 406, Loss: 0.9185225963592529, Accuracy: 1.0, Computation time: 1.9695212841033936\n",
      "Step: 407, Loss: 0.9161604046821594, Accuracy: 1.0, Computation time: 2.231605291366577\n",
      "Step: 408, Loss: 0.9163227081298828, Accuracy: 1.0, Computation time: 2.2825849056243896\n",
      "Step: 409, Loss: 0.9183570146560669, Accuracy: 1.0, Computation time: 3.2301268577575684\n",
      "Step: 410, Loss: 0.9165164232254028, Accuracy: 1.0, Computation time: 1.9656124114990234\n",
      "Step: 411, Loss: 0.916064977645874, Accuracy: 1.0, Computation time: 2.057588815689087\n",
      "Step: 412, Loss: 0.9190855026245117, Accuracy: 1.0, Computation time: 2.3607444763183594\n",
      "Step: 413, Loss: 0.9226474761962891, Accuracy: 1.0, Computation time: 2.023282289505005\n",
      "Step: 414, Loss: 0.9372712969779968, Accuracy: 0.9642857313156128, Computation time: 2.0997118949890137\n",
      "Step: 415, Loss: 0.9166204333305359, Accuracy: 1.0, Computation time: 2.0705904960632324\n",
      "Step: 416, Loss: 0.916404664516449, Accuracy: 1.0, Computation time: 2.019068717956543\n",
      "Step: 417, Loss: 0.9163853526115417, Accuracy: 1.0, Computation time: 2.1342365741729736\n",
      "########################\n",
      "Test loss: 1.0726274251937866, Test Accuracy_epoch3: 0.7684266567230225\n",
      "########################\n",
      "Step: 418, Loss: 0.9160614609718323, Accuracy: 1.0, Computation time: 2.09867787361145\n",
      "Step: 419, Loss: 0.9160270690917969, Accuracy: 1.0, Computation time: 2.030803680419922\n",
      "Step: 420, Loss: 0.9161051511764526, Accuracy: 1.0, Computation time: 2.089294195175171\n",
      "Step: 421, Loss: 0.9162026047706604, Accuracy: 1.0, Computation time: 2.097714424133301\n",
      "Step: 422, Loss: 0.9160206913948059, Accuracy: 1.0, Computation time: 2.1170473098754883\n",
      "Step: 423, Loss: 0.9159675240516663, Accuracy: 1.0, Computation time: 2.0987884998321533\n",
      "Step: 424, Loss: 0.9164265394210815, Accuracy: 1.0, Computation time: 2.0455822944641113\n",
      "Step: 425, Loss: 0.9160323143005371, Accuracy: 1.0, Computation time: 1.7990734577178955\n",
      "Step: 426, Loss: 0.9168077111244202, Accuracy: 1.0, Computation time: 1.8245186805725098\n",
      "Step: 427, Loss: 0.9160463809967041, Accuracy: 1.0, Computation time: 2.7945263385772705\n",
      "Step: 428, Loss: 0.9161816835403442, Accuracy: 1.0, Computation time: 2.3248181343078613\n",
      "Step: 429, Loss: 0.9159269332885742, Accuracy: 1.0, Computation time: 2.3444201946258545\n",
      "Step: 430, Loss: 0.9377618432044983, Accuracy: 0.9807692766189575, Computation time: 2.2470176219940186\n",
      "Step: 431, Loss: 0.9160016179084778, Accuracy: 1.0, Computation time: 3.3287839889526367\n",
      "Step: 432, Loss: 0.9183675050735474, Accuracy: 1.0, Computation time: 1.9356489181518555\n",
      "Step: 433, Loss: 0.9184361100196838, Accuracy: 1.0, Computation time: 2.7176120281219482\n",
      "Step: 434, Loss: 0.9160812497138977, Accuracy: 1.0, Computation time: 2.3094074726104736\n",
      "Step: 435, Loss: 0.9573596119880676, Accuracy: 0.9330357313156128, Computation time: 2.5794405937194824\n",
      "Step: 436, Loss: 0.9162691831588745, Accuracy: 1.0, Computation time: 2.427403688430786\n",
      "Step: 437, Loss: 0.9177240133285522, Accuracy: 1.0, Computation time: 2.1181490421295166\n",
      "Step: 438, Loss: 0.9161109924316406, Accuracy: 1.0, Computation time: 2.465224504470825\n",
      "Step: 439, Loss: 0.9160329699516296, Accuracy: 1.0, Computation time: 2.291809558868408\n",
      "Step: 440, Loss: 0.9189852476119995, Accuracy: 1.0, Computation time: 2.2897937297821045\n",
      "Step: 441, Loss: 0.9160001873970032, Accuracy: 1.0, Computation time: 1.961890459060669\n",
      "Step: 442, Loss: 0.9177517890930176, Accuracy: 1.0, Computation time: 1.7543127536773682\n",
      "Step: 443, Loss: 0.9160720109939575, Accuracy: 1.0, Computation time: 2.7675719261169434\n",
      "Step: 444, Loss: 0.9160640835762024, Accuracy: 1.0, Computation time: 1.9422612190246582\n",
      "Step: 445, Loss: 0.9178872108459473, Accuracy: 1.0, Computation time: 2.2842159271240234\n",
      "Step: 446, Loss: 0.9161515831947327, Accuracy: 1.0, Computation time: 2.586698055267334\n",
      "Step: 447, Loss: 0.9173827767372131, Accuracy: 1.0, Computation time: 2.3247275352478027\n",
      "Step: 448, Loss: 0.9165801405906677, Accuracy: 1.0, Computation time: 2.364236354827881\n",
      "Step: 449, Loss: 0.9160193204879761, Accuracy: 1.0, Computation time: 2.012845993041992\n",
      "Step: 450, Loss: 0.9163899421691895, Accuracy: 1.0, Computation time: 2.707566499710083\n",
      "Step: 451, Loss: 0.9165664911270142, Accuracy: 1.0, Computation time: 2.9152908325195312\n",
      "Step: 452, Loss: 0.9161548614501953, Accuracy: 1.0, Computation time: 3.0262742042541504\n",
      "Step: 453, Loss: 0.9366609454154968, Accuracy: 0.9772727489471436, Computation time: 1.7635321617126465\n",
      "Step: 454, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 1.92838454246521\n",
      "Step: 455, Loss: 0.9162951707839966, Accuracy: 1.0, Computation time: 2.189321279525757\n",
      "Step: 456, Loss: 0.9161998629570007, Accuracy: 1.0, Computation time: 2.4644527435302734\n",
      "Step: 457, Loss: 0.9176477789878845, Accuracy: 1.0, Computation time: 1.9673452377319336\n",
      "Step: 458, Loss: 0.9179214239120483, Accuracy: 1.0, Computation time: 2.069650173187256\n",
      "Step: 459, Loss: 0.9161276817321777, Accuracy: 1.0, Computation time: 2.376558303833008\n",
      "Step: 460, Loss: 0.9159734845161438, Accuracy: 1.0, Computation time: 2.215224027633667\n",
      "Step: 461, Loss: 0.9160348773002625, Accuracy: 1.0, Computation time: 2.551496982574463\n",
      "Step: 462, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 2.1330602169036865\n",
      "Step: 463, Loss: 0.9160535931587219, Accuracy: 1.0, Computation time: 1.8467869758605957\n",
      "Step: 464, Loss: 0.9163524508476257, Accuracy: 1.0, Computation time: 2.1121041774749756\n",
      "Step: 465, Loss: 0.9370272755622864, Accuracy: 0.949999988079071, Computation time: 2.247201681137085\n",
      "Step: 466, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 2.1199986934661865\n",
      "Step: 467, Loss: 0.9175092577934265, Accuracy: 1.0, Computation time: 2.3088412284851074\n",
      "Step: 468, Loss: 0.9168531894683838, Accuracy: 1.0, Computation time: 2.1793301105499268\n",
      "Step: 469, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 1.975374460220337\n",
      "Step: 470, Loss: 0.9160662293434143, Accuracy: 1.0, Computation time: 2.0257010459899902\n",
      "Step: 471, Loss: 0.9160189628601074, Accuracy: 1.0, Computation time: 1.96885347366333\n",
      "Step: 472, Loss: 0.9163776636123657, Accuracy: 1.0, Computation time: 2.6669716835021973\n",
      "Step: 473, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 2.1921138763427734\n",
      "Step: 474, Loss: 0.9159873723983765, Accuracy: 1.0, Computation time: 2.63179087638855\n",
      "Step: 475, Loss: 0.9161542654037476, Accuracy: 1.0, Computation time: 2.281313419342041\n",
      "Step: 476, Loss: 0.9192019701004028, Accuracy: 1.0, Computation time: 2.1061525344848633\n",
      "Step: 477, Loss: 0.9313510060310364, Accuracy: 0.96875, Computation time: 2.194448471069336\n",
      "Step: 478, Loss: 0.9167990684509277, Accuracy: 1.0, Computation time: 2.2442514896392822\n",
      "Step: 479, Loss: 0.9167035818099976, Accuracy: 1.0, Computation time: 2.8961329460144043\n",
      "Step: 480, Loss: 0.9210987687110901, Accuracy: 1.0, Computation time: 1.9468443393707275\n",
      "Step: 481, Loss: 0.9161337018013, Accuracy: 1.0, Computation time: 2.229982376098633\n",
      "Step: 482, Loss: 0.9169387221336365, Accuracy: 1.0, Computation time: 2.0958735942840576\n",
      "Step: 483, Loss: 0.9162000417709351, Accuracy: 1.0, Computation time: 2.1167850494384766\n",
      "Step: 484, Loss: 0.9323167204856873, Accuracy: 0.9750000238418579, Computation time: 2.510916233062744\n",
      "Step: 485, Loss: 0.9379592537879944, Accuracy: 0.96875, Computation time: 2.113862991333008\n",
      "Step: 486, Loss: 0.9377546906471252, Accuracy: 0.9750000238418579, Computation time: 1.9594407081604004\n",
      "Step: 487, Loss: 0.9170569777488708, Accuracy: 1.0, Computation time: 1.739546537399292\n",
      "Step: 488, Loss: 0.9162766933441162, Accuracy: 1.0, Computation time: 2.4794301986694336\n",
      "Step: 489, Loss: 0.9374127388000488, Accuracy: 0.9833333492279053, Computation time: 2.1029551029205322\n",
      "Step: 490, Loss: 0.9159776568412781, Accuracy: 1.0, Computation time: 2.0902421474456787\n",
      "Step: 491, Loss: 0.916062593460083, Accuracy: 1.0, Computation time: 1.4921765327453613\n",
      "Step: 492, Loss: 0.9163885116577148, Accuracy: 1.0, Computation time: 1.5839836597442627\n",
      "Step: 493, Loss: 0.9181310534477234, Accuracy: 1.0, Computation time: 1.7808136940002441\n",
      "Step: 494, Loss: 0.916499674320221, Accuracy: 1.0, Computation time: 1.4470233917236328\n",
      "Step: 495, Loss: 0.9168805480003357, Accuracy: 1.0, Computation time: 1.4801945686340332\n",
      "Step: 496, Loss: 0.916037917137146, Accuracy: 1.0, Computation time: 1.5587682723999023\n",
      "Step: 497, Loss: 0.9160135388374329, Accuracy: 1.0, Computation time: 1.1422955989837646\n",
      "Step: 498, Loss: 0.9163079857826233, Accuracy: 1.0, Computation time: 1.3082096576690674\n",
      "Step: 499, Loss: 0.9161586761474609, Accuracy: 1.0, Computation time: 1.9474143981933594\n",
      "Step: 500, Loss: 0.9540681838989258, Accuracy: 0.9583333730697632, Computation time: 1.704958438873291\n",
      "Step: 501, Loss: 0.9173809885978699, Accuracy: 1.0, Computation time: 1.5786669254302979\n",
      "Step: 502, Loss: 0.9174180030822754, Accuracy: 1.0, Computation time: 1.4584639072418213\n",
      "Step: 503, Loss: 0.9160513877868652, Accuracy: 1.0, Computation time: 1.9318320751190186\n",
      "Step: 504, Loss: 0.9374570846557617, Accuracy: 0.9750000238418579, Computation time: 1.409898281097412\n",
      "Step: 505, Loss: 0.9209516644477844, Accuracy: 1.0, Computation time: 1.6841742992401123\n",
      "Step: 506, Loss: 0.9166685342788696, Accuracy: 1.0, Computation time: 1.433591365814209\n",
      "Step: 507, Loss: 0.916293740272522, Accuracy: 1.0, Computation time: 1.2424805164337158\n",
      "Step: 508, Loss: 0.9171251654624939, Accuracy: 1.0, Computation time: 2.2656662464141846\n",
      "Step: 509, Loss: 0.9321520328521729, Accuracy: 0.96875, Computation time: 1.4713213443756104\n",
      "Step: 510, Loss: 0.9417201280593872, Accuracy: 0.9791666865348816, Computation time: 1.9902911186218262\n",
      "Step: 511, Loss: 0.9162609577178955, Accuracy: 1.0, Computation time: 1.452406883239746\n",
      "Step: 512, Loss: 0.9372608065605164, Accuracy: 0.96875, Computation time: 1.4111347198486328\n",
      "Step: 513, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 1.486114740371704\n",
      "Step: 514, Loss: 0.9165919423103333, Accuracy: 1.0, Computation time: 1.2751362323760986\n",
      "Step: 515, Loss: 0.9383620023727417, Accuracy: 0.9791666865348816, Computation time: 1.6605372428894043\n",
      "Step: 516, Loss: 0.9183335304260254, Accuracy: 1.0, Computation time: 1.968902349472046\n",
      "Step: 517, Loss: 0.9160951972007751, Accuracy: 1.0, Computation time: 1.317042350769043\n",
      "Step: 518, Loss: 0.9168514013290405, Accuracy: 1.0, Computation time: 1.301732063293457\n",
      "Step: 519, Loss: 0.9168474674224854, Accuracy: 1.0, Computation time: 2.0027153491973877\n",
      "Step: 520, Loss: 0.9172660112380981, Accuracy: 1.0, Computation time: 1.325596570968628\n",
      "Step: 521, Loss: 0.9162366986274719, Accuracy: 1.0, Computation time: 1.5364584922790527\n",
      "Step: 522, Loss: 0.9160774350166321, Accuracy: 1.0, Computation time: 1.4398977756500244\n",
      "Step: 523, Loss: 0.9165522456169128, Accuracy: 1.0, Computation time: 1.3417303562164307\n",
      "Step: 524, Loss: 0.9161810874938965, Accuracy: 1.0, Computation time: 1.3826875686645508\n",
      "Step: 525, Loss: 0.916239857673645, Accuracy: 1.0, Computation time: 1.8124191761016846\n",
      "Step: 526, Loss: 0.9159406423568726, Accuracy: 1.0, Computation time: 1.7869622707366943\n",
      "Step: 527, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.4206161499023438\n",
      "Step: 528, Loss: 0.9160020351409912, Accuracy: 1.0, Computation time: 1.2247462272644043\n",
      "Step: 529, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 1.7149786949157715\n",
      "Step: 530, Loss: 0.9162948727607727, Accuracy: 1.0, Computation time: 1.7333984375\n",
      "Step: 531, Loss: 0.9175673127174377, Accuracy: 1.0, Computation time: 1.7077765464782715\n",
      "Step: 532, Loss: 0.9269056916236877, Accuracy: 0.9821428656578064, Computation time: 1.8370616436004639\n",
      "Step: 533, Loss: 0.9737288951873779, Accuracy: 0.9232954978942871, Computation time: 2.1314399242401123\n",
      "Step: 534, Loss: 0.9436069130897522, Accuracy: 0.9642857313156128, Computation time: 1.4857697486877441\n",
      "Step: 535, Loss: 0.9164618253707886, Accuracy: 1.0, Computation time: 1.3791851997375488\n",
      "Step: 536, Loss: 0.916231095790863, Accuracy: 1.0, Computation time: 2.024049997329712\n",
      "Step: 537, Loss: 0.9316524267196655, Accuracy: 0.9722222089767456, Computation time: 1.656430959701538\n",
      "Step: 538, Loss: 0.9381099939346313, Accuracy: 0.9791666865348816, Computation time: 1.483614444732666\n",
      "Step: 539, Loss: 0.930903971195221, Accuracy: 0.9583333730697632, Computation time: 1.462951421737671\n",
      "Step: 540, Loss: 0.919346272945404, Accuracy: 1.0, Computation time: 1.7044484615325928\n",
      "Step: 541, Loss: 0.9165798425674438, Accuracy: 1.0, Computation time: 1.6638624668121338\n",
      "Step: 542, Loss: 0.9169196486473083, Accuracy: 1.0, Computation time: 1.1287522315979004\n",
      "Step: 543, Loss: 0.9170870780944824, Accuracy: 1.0, Computation time: 1.3518919944763184\n",
      "Step: 544, Loss: 0.9172006249427795, Accuracy: 1.0, Computation time: 1.3190460205078125\n",
      "Step: 545, Loss: 0.9172239899635315, Accuracy: 1.0, Computation time: 1.47483491897583\n",
      "Step: 546, Loss: 0.9382922053337097, Accuracy: 0.9722222089767456, Computation time: 1.3222413063049316\n",
      "Step: 547, Loss: 0.9384233951568604, Accuracy: 0.9772727489471436, Computation time: 1.6597397327423096\n",
      "Step: 548, Loss: 0.9163095951080322, Accuracy: 1.0, Computation time: 1.1196246147155762\n",
      "Step: 549, Loss: 0.9169602990150452, Accuracy: 1.0, Computation time: 1.2510936260223389\n",
      "Step: 550, Loss: 0.9161611795425415, Accuracy: 1.0, Computation time: 1.5643134117126465\n",
      "Step: 551, Loss: 0.9706893563270569, Accuracy: 0.942307710647583, Computation time: 1.8252527713775635\n",
      "Step: 552, Loss: 0.9162259101867676, Accuracy: 1.0, Computation time: 1.7418372631072998\n",
      "Step: 553, Loss: 0.9427357912063599, Accuracy: 0.9807692766189575, Computation time: 2.048236846923828\n",
      "Step: 554, Loss: 0.916290283203125, Accuracy: 1.0, Computation time: 1.2360749244689941\n",
      "Step: 555, Loss: 0.9387737512588501, Accuracy: 0.9772727489471436, Computation time: 1.1699602603912354\n",
      "Step: 556, Loss: 0.9237748384475708, Accuracy: 1.0, Computation time: 1.537426233291626\n",
      "########################\n",
      "Test loss: 1.0796738862991333, Test Accuracy_epoch4: 0.756026029586792\n",
      "########################\n",
      "Step: 557, Loss: 0.9169402122497559, Accuracy: 1.0, Computation time: 1.5006937980651855\n",
      "Step: 558, Loss: 0.9378440976142883, Accuracy: 0.9642857313156128, Computation time: 1.3958032131195068\n",
      "Step: 559, Loss: 0.9194724559783936, Accuracy: 1.0, Computation time: 1.4896628856658936\n",
      "Step: 560, Loss: 0.938473641872406, Accuracy: 0.9772727489471436, Computation time: 1.4566256999969482\n",
      "Step: 561, Loss: 0.9367741942405701, Accuracy: 0.9750000238418579, Computation time: 1.3923301696777344\n",
      "Step: 562, Loss: 0.9162211418151855, Accuracy: 1.0, Computation time: 1.2184572219848633\n",
      "Step: 563, Loss: 0.9162216186523438, Accuracy: 1.0, Computation time: 1.742370843887329\n",
      "Step: 564, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 1.3948047161102295\n",
      "Step: 565, Loss: 0.917400598526001, Accuracy: 1.0, Computation time: 1.4103002548217773\n",
      "Step: 566, Loss: 0.9161099791526794, Accuracy: 1.0, Computation time: 1.5018362998962402\n",
      "Step: 567, Loss: 0.9171914458274841, Accuracy: 1.0, Computation time: 1.199470043182373\n",
      "Step: 568, Loss: 0.9160448908805847, Accuracy: 1.0, Computation time: 1.73219895362854\n",
      "Step: 569, Loss: 0.916126549243927, Accuracy: 1.0, Computation time: 2.143460988998413\n",
      "Step: 570, Loss: 0.9361019730567932, Accuracy: 0.9642857313156128, Computation time: 2.4481685161590576\n",
      "Step: 571, Loss: 0.9161492586135864, Accuracy: 1.0, Computation time: 2.2802085876464844\n",
      "Step: 572, Loss: 0.940250039100647, Accuracy: 0.9722222089767456, Computation time: 2.308952569961548\n",
      "Step: 573, Loss: 0.9159824848175049, Accuracy: 1.0, Computation time: 1.9309887886047363\n",
      "Step: 574, Loss: 0.9160920977592468, Accuracy: 1.0, Computation time: 1.8770997524261475\n",
      "Step: 575, Loss: 0.9369718432426453, Accuracy: 0.949999988079071, Computation time: 2.452336072921753\n",
      "Step: 576, Loss: 0.9160590767860413, Accuracy: 1.0, Computation time: 1.9272065162658691\n",
      "Step: 577, Loss: 0.916216254234314, Accuracy: 1.0, Computation time: 2.8116743564605713\n",
      "Step: 578, Loss: 0.9160351157188416, Accuracy: 1.0, Computation time: 2.540523052215576\n",
      "Step: 579, Loss: 0.9160425662994385, Accuracy: 1.0, Computation time: 2.2819886207580566\n",
      "Step: 580, Loss: 0.9160063862800598, Accuracy: 1.0, Computation time: 2.167274236679077\n",
      "Step: 581, Loss: 0.9161590337753296, Accuracy: 1.0, Computation time: 1.8996303081512451\n",
      "Step: 582, Loss: 0.9160318374633789, Accuracy: 1.0, Computation time: 1.815596580505371\n",
      "Step: 583, Loss: 0.9172830581665039, Accuracy: 1.0, Computation time: 2.311859130859375\n",
      "Step: 584, Loss: 0.9159559607505798, Accuracy: 1.0, Computation time: 1.9432082176208496\n",
      "Step: 585, Loss: 0.9170125126838684, Accuracy: 1.0, Computation time: 2.0891058444976807\n",
      "Step: 586, Loss: 0.9372605085372925, Accuracy: 0.96875, Computation time: 2.8122589588165283\n",
      "Step: 587, Loss: 0.9159553050994873, Accuracy: 1.0, Computation time: 2.0963516235351562\n",
      "Step: 588, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 2.6300771236419678\n",
      "Step: 589, Loss: 0.9163830876350403, Accuracy: 1.0, Computation time: 2.3291232585906982\n",
      "Step: 590, Loss: 0.924113392829895, Accuracy: 1.0, Computation time: 2.7508385181427\n",
      "Step: 591, Loss: 0.9160340428352356, Accuracy: 1.0, Computation time: 2.110779285430908\n",
      "Step: 592, Loss: 0.9373200535774231, Accuracy: 0.9821428656578064, Computation time: 2.2986607551574707\n",
      "Step: 593, Loss: 0.9159970283508301, Accuracy: 1.0, Computation time: 2.0798122882843018\n",
      "Step: 594, Loss: 0.9160057306289673, Accuracy: 1.0, Computation time: 2.263843536376953\n",
      "Step: 595, Loss: 0.9231342077255249, Accuracy: 1.0, Computation time: 2.101067304611206\n",
      "Step: 596, Loss: 0.9160052537918091, Accuracy: 1.0, Computation time: 2.7271385192871094\n",
      "Step: 597, Loss: 0.916250467300415, Accuracy: 1.0, Computation time: 2.194941520690918\n",
      "Step: 598, Loss: 0.9180004596710205, Accuracy: 1.0, Computation time: 2.3780016899108887\n",
      "Step: 599, Loss: 0.9352313876152039, Accuracy: 0.9642857313156128, Computation time: 2.0446715354919434\n",
      "Step: 600, Loss: 0.9160409569740295, Accuracy: 1.0, Computation time: 2.050638198852539\n",
      "Step: 601, Loss: 0.9159986972808838, Accuracy: 1.0, Computation time: 1.9568843841552734\n",
      "Step: 602, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 2.634687900543213\n",
      "Step: 603, Loss: 0.9161276817321777, Accuracy: 1.0, Computation time: 1.7224833965301514\n",
      "Step: 604, Loss: 0.9177836179733276, Accuracy: 1.0, Computation time: 1.5980620384216309\n",
      "Step: 605, Loss: 0.9163548946380615, Accuracy: 1.0, Computation time: 1.7577812671661377\n",
      "Step: 606, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.5054421424865723\n",
      "Step: 607, Loss: 0.9159459471702576, Accuracy: 1.0, Computation time: 1.2839748859405518\n",
      "Step: 608, Loss: 0.9159244298934937, Accuracy: 1.0, Computation time: 1.282331943511963\n",
      "Step: 609, Loss: 0.9265552759170532, Accuracy: 0.9750000238418579, Computation time: 1.474064588546753\n",
      "Step: 610, Loss: 0.9159848093986511, Accuracy: 1.0, Computation time: 1.869044542312622\n",
      "Step: 611, Loss: 0.9160459637641907, Accuracy: 1.0, Computation time: 1.7902836799621582\n",
      "Step: 612, Loss: 0.9161742329597473, Accuracy: 1.0, Computation time: 1.4540050029754639\n",
      "Step: 613, Loss: 0.9415513277053833, Accuracy: 0.9722222089767456, Computation time: 1.6188995838165283\n",
      "Step: 614, Loss: 0.9160779118537903, Accuracy: 1.0, Computation time: 1.3996312618255615\n",
      "Step: 615, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 1.4005753993988037\n",
      "Step: 616, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.309427261352539\n",
      "Step: 617, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.5632681846618652\n",
      "Step: 618, Loss: 0.9383127689361572, Accuracy: 0.9750000238418579, Computation time: 1.2735397815704346\n",
      "Step: 619, Loss: 0.9159244894981384, Accuracy: 1.0, Computation time: 1.1119282245635986\n",
      "Step: 620, Loss: 0.9161456227302551, Accuracy: 1.0, Computation time: 1.5339319705963135\n",
      "Step: 621, Loss: 0.915954053401947, Accuracy: 1.0, Computation time: 1.500361680984497\n",
      "Step: 622, Loss: 0.916033923625946, Accuracy: 1.0, Computation time: 1.3403823375701904\n",
      "Step: 623, Loss: 0.9456320405006409, Accuracy: 0.9583333730697632, Computation time: 1.7316770553588867\n",
      "Step: 624, Loss: 0.9382533431053162, Accuracy: 0.9750000238418579, Computation time: 1.723249912261963\n",
      "Step: 625, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 1.3388042449951172\n",
      "Step: 626, Loss: 0.9161446690559387, Accuracy: 1.0, Computation time: 1.3441839218139648\n",
      "Step: 627, Loss: 0.9369422793388367, Accuracy: 0.96875, Computation time: 1.7807753086090088\n",
      "Step: 628, Loss: 0.9162082672119141, Accuracy: 1.0, Computation time: 1.5688385963439941\n",
      "Step: 629, Loss: 0.9160839915275574, Accuracy: 1.0, Computation time: 1.3623359203338623\n",
      "Step: 630, Loss: 0.9264429211616516, Accuracy: 0.9833333492279053, Computation time: 1.8492202758789062\n",
      "Step: 631, Loss: 0.9175776243209839, Accuracy: 1.0, Computation time: 1.58406662940979\n",
      "Step: 632, Loss: 0.9164300560951233, Accuracy: 1.0, Computation time: 1.5613937377929688\n",
      "Step: 633, Loss: 0.9375290870666504, Accuracy: 0.9772727489471436, Computation time: 1.8417527675628662\n",
      "Step: 634, Loss: 0.9799792170524597, Accuracy: 0.9166666865348816, Computation time: 1.503448724746704\n",
      "Step: 635, Loss: 0.9559307098388672, Accuracy: 0.954365074634552, Computation time: 1.7116122245788574\n",
      "Step: 636, Loss: 0.9387094378471375, Accuracy: 0.9750000238418579, Computation time: 1.6885457038879395\n",
      "Step: 637, Loss: 0.9169458746910095, Accuracy: 1.0, Computation time: 1.5460543632507324\n",
      "Step: 638, Loss: 0.9176269173622131, Accuracy: 1.0, Computation time: 1.678758144378662\n",
      "Step: 639, Loss: 0.916976809501648, Accuracy: 1.0, Computation time: 1.3983893394470215\n",
      "Step: 640, Loss: 0.9388247132301331, Accuracy: 0.96875, Computation time: 1.3561229705810547\n",
      "Step: 641, Loss: 0.9166023135185242, Accuracy: 1.0, Computation time: 1.4166920185089111\n",
      "Step: 642, Loss: 0.9230656623840332, Accuracy: 1.0, Computation time: 1.6347036361694336\n",
      "Step: 643, Loss: 0.916576623916626, Accuracy: 1.0, Computation time: 1.6564357280731201\n",
      "Step: 644, Loss: 0.9172035455703735, Accuracy: 1.0, Computation time: 1.3130466938018799\n",
      "Step: 645, Loss: 0.916467010974884, Accuracy: 1.0, Computation time: 1.3716654777526855\n",
      "Step: 646, Loss: 0.917887806892395, Accuracy: 1.0, Computation time: 1.297393560409546\n",
      "Step: 647, Loss: 0.9179670810699463, Accuracy: 1.0, Computation time: 1.7768845558166504\n",
      "Step: 648, Loss: 0.9217950105667114, Accuracy: 1.0, Computation time: 1.4377641677856445\n",
      "Step: 649, Loss: 0.9168606996536255, Accuracy: 1.0, Computation time: 1.4631578922271729\n",
      "Step: 650, Loss: 0.9165120720863342, Accuracy: 1.0, Computation time: 1.7001068592071533\n",
      "Step: 651, Loss: 0.9166867733001709, Accuracy: 1.0, Computation time: 1.6396453380584717\n",
      "Step: 652, Loss: 0.9171304702758789, Accuracy: 1.0, Computation time: 1.8502583503723145\n",
      "Step: 653, Loss: 0.9162194132804871, Accuracy: 1.0, Computation time: 1.7656421661376953\n",
      "Step: 654, Loss: 0.916106641292572, Accuracy: 1.0, Computation time: 1.4618914127349854\n",
      "Step: 655, Loss: 0.9202974438667297, Accuracy: 1.0, Computation time: 1.5865819454193115\n",
      "Step: 656, Loss: 0.916440486907959, Accuracy: 1.0, Computation time: 1.460240125656128\n",
      "Step: 657, Loss: 0.9198131561279297, Accuracy: 1.0, Computation time: 1.533127784729004\n",
      "Step: 658, Loss: 0.9376906752586365, Accuracy: 0.9583333730697632, Computation time: 1.5066001415252686\n",
      "Step: 659, Loss: 0.9163178205490112, Accuracy: 1.0, Computation time: 1.5859355926513672\n",
      "Step: 660, Loss: 0.9161778688430786, Accuracy: 1.0, Computation time: 1.466064214706421\n",
      "Step: 661, Loss: 0.9260522127151489, Accuracy: 1.0, Computation time: 1.5845506191253662\n",
      "Step: 662, Loss: 0.9183787107467651, Accuracy: 1.0, Computation time: 1.2387545108795166\n",
      "Step: 663, Loss: 0.9160795211791992, Accuracy: 1.0, Computation time: 1.617548942565918\n",
      "Step: 664, Loss: 0.916011393070221, Accuracy: 1.0, Computation time: 1.3717927932739258\n",
      "Step: 665, Loss: 0.9160718321800232, Accuracy: 1.0, Computation time: 1.4356951713562012\n",
      "Step: 666, Loss: 0.9161732196807861, Accuracy: 1.0, Computation time: 1.6072025299072266\n",
      "Step: 667, Loss: 0.916312575340271, Accuracy: 1.0, Computation time: 1.5581645965576172\n",
      "Step: 668, Loss: 0.9199591279029846, Accuracy: 1.0, Computation time: 1.8068981170654297\n",
      "Step: 669, Loss: 0.9272971153259277, Accuracy: 0.96875, Computation time: 1.6505098342895508\n",
      "Step: 670, Loss: 0.9168672561645508, Accuracy: 1.0, Computation time: 1.630913257598877\n",
      "Step: 671, Loss: 0.9201743006706238, Accuracy: 1.0, Computation time: 1.4174795150756836\n",
      "Step: 672, Loss: 0.9371479153633118, Accuracy: 0.9750000238418579, Computation time: 1.564197063446045\n",
      "Step: 673, Loss: 0.9167385697364807, Accuracy: 1.0, Computation time: 1.446157693862915\n",
      "Step: 674, Loss: 0.9179689288139343, Accuracy: 1.0, Computation time: 1.603954553604126\n",
      "Step: 675, Loss: 0.9397880434989929, Accuracy: 0.9750000238418579, Computation time: 1.6997637748718262\n",
      "Step: 676, Loss: 0.9163828492164612, Accuracy: 1.0, Computation time: 1.333829402923584\n",
      "Step: 677, Loss: 0.9161684513092041, Accuracy: 1.0, Computation time: 1.766279697418213\n",
      "Step: 678, Loss: 0.9161269664764404, Accuracy: 1.0, Computation time: 1.3109519481658936\n",
      "Step: 679, Loss: 0.9195398688316345, Accuracy: 1.0, Computation time: 1.5193185806274414\n",
      "Step: 680, Loss: 0.9159811735153198, Accuracy: 1.0, Computation time: 1.2294809818267822\n",
      "Step: 681, Loss: 0.9171523451805115, Accuracy: 1.0, Computation time: 2.045952081680298\n",
      "Step: 682, Loss: 0.9379289150238037, Accuracy: 0.9807692766189575, Computation time: 1.686600923538208\n",
      "Step: 683, Loss: 0.9161098599433899, Accuracy: 1.0, Computation time: 1.4317781925201416\n",
      "Step: 684, Loss: 0.9162247180938721, Accuracy: 1.0, Computation time: 1.5123090744018555\n",
      "Step: 685, Loss: 0.9263818264007568, Accuracy: 0.9821428656578064, Computation time: 1.769653558731079\n",
      "Step: 686, Loss: 0.9163232445716858, Accuracy: 1.0, Computation time: 1.4864115715026855\n",
      "Step: 687, Loss: 0.9162634611129761, Accuracy: 1.0, Computation time: 1.3378651142120361\n",
      "Step: 688, Loss: 0.9236526489257812, Accuracy: 1.0, Computation time: 1.9480681419372559\n",
      "Step: 689, Loss: 0.9161496162414551, Accuracy: 1.0, Computation time: 1.4552266597747803\n",
      "Step: 690, Loss: 0.9160120487213135, Accuracy: 1.0, Computation time: 1.5593559741973877\n",
      "Step: 691, Loss: 0.916032075881958, Accuracy: 1.0, Computation time: 1.4998385906219482\n",
      "Step: 692, Loss: 0.9368467926979065, Accuracy: 0.9750000238418579, Computation time: 1.2060136795043945\n",
      "Step: 693, Loss: 0.9160012602806091, Accuracy: 1.0, Computation time: 1.7550430297851562\n",
      "Step: 694, Loss: 0.9169231057167053, Accuracy: 1.0, Computation time: 1.4709389209747314\n",
      "Step: 695, Loss: 0.916100263595581, Accuracy: 1.0, Computation time: 1.3516044616699219\n",
      "########################\n",
      "Test loss: 1.0774333477020264, Test Accuracy_epoch5: 0.7597677707672119\n",
      "########################\n",
      "Step: 696, Loss: 0.9581214189529419, Accuracy: 0.9583333730697632, Computation time: 1.5938644409179688\n",
      "Step: 697, Loss: 0.9160367250442505, Accuracy: 1.0, Computation time: 1.3672616481781006\n",
      "Step: 698, Loss: 0.916018545627594, Accuracy: 1.0, Computation time: 1.5666828155517578\n",
      "Step: 699, Loss: 0.916678249835968, Accuracy: 1.0, Computation time: 1.403656005859375\n",
      "Step: 700, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.3570456504821777\n",
      "Step: 701, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.8082070350646973\n",
      "Step: 702, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.345874547958374\n",
      "Step: 703, Loss: 0.9165376424789429, Accuracy: 1.0, Computation time: 1.5138721466064453\n",
      "Step: 704, Loss: 0.9160710573196411, Accuracy: 1.0, Computation time: 1.335291862487793\n",
      "Step: 705, Loss: 0.9160006046295166, Accuracy: 1.0, Computation time: 1.3564660549163818\n",
      "Step: 706, Loss: 0.9159881472587585, Accuracy: 1.0, Computation time: 1.6782042980194092\n",
      "Step: 707, Loss: 0.9571260213851929, Accuracy: 0.9479166865348816, Computation time: 1.928828239440918\n",
      "Step: 708, Loss: 0.9195464253425598, Accuracy: 1.0, Computation time: 1.203071117401123\n",
      "Step: 709, Loss: 0.9550734758377075, Accuracy: 0.9333333969116211, Computation time: 1.2685420513153076\n",
      "Step: 710, Loss: 0.9159585237503052, Accuracy: 1.0, Computation time: 1.5469977855682373\n",
      "Step: 711, Loss: 0.9169356226921082, Accuracy: 1.0, Computation time: 1.3933770656585693\n",
      "Step: 712, Loss: 0.9384891390800476, Accuracy: 0.9750000238418579, Computation time: 1.2848167419433594\n",
      "Step: 713, Loss: 0.9348226189613342, Accuracy: 0.9791666865348816, Computation time: 1.45112943649292\n",
      "Step: 714, Loss: 0.9202592372894287, Accuracy: 1.0, Computation time: 1.7708654403686523\n",
      "Step: 715, Loss: 0.9165226221084595, Accuracy: 1.0, Computation time: 1.490220308303833\n",
      "Step: 716, Loss: 0.9162691235542297, Accuracy: 1.0, Computation time: 1.3882389068603516\n",
      "Step: 717, Loss: 0.9162322878837585, Accuracy: 1.0, Computation time: 2.280305862426758\n",
      "Step: 718, Loss: 0.9161057472229004, Accuracy: 1.0, Computation time: 1.4537522792816162\n",
      "Step: 719, Loss: 0.9161567091941833, Accuracy: 1.0, Computation time: 1.6931557655334473\n",
      "Step: 720, Loss: 0.9161562919616699, Accuracy: 1.0, Computation time: 1.6295528411865234\n",
      "Step: 721, Loss: 0.916236400604248, Accuracy: 1.0, Computation time: 1.6386699676513672\n",
      "Step: 722, Loss: 0.915982723236084, Accuracy: 1.0, Computation time: 2.032639741897583\n",
      "Step: 723, Loss: 0.9159993529319763, Accuracy: 1.0, Computation time: 1.8526275157928467\n",
      "Step: 724, Loss: 0.9159070253372192, Accuracy: 1.0, Computation time: 1.6551127433776855\n",
      "Step: 725, Loss: 0.927499532699585, Accuracy: 0.9772727489471436, Computation time: 1.4836571216583252\n",
      "Step: 726, Loss: 0.9159626960754395, Accuracy: 1.0, Computation time: 1.5585558414459229\n",
      "Step: 727, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 1.0977427959442139\n",
      "Step: 728, Loss: 0.9161261320114136, Accuracy: 1.0, Computation time: 2.229808807373047\n",
      "Step: 729, Loss: 0.9160851836204529, Accuracy: 1.0, Computation time: 1.389223337173462\n",
      "Step: 730, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 1.4266078472137451\n",
      "Step: 731, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.6325714588165283\n",
      "Step: 732, Loss: 0.9159723520278931, Accuracy: 1.0, Computation time: 1.712242603302002\n",
      "Step: 733, Loss: 0.9210140109062195, Accuracy: 1.0, Computation time: 1.5475389957427979\n",
      "Step: 734, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.7819361686706543\n",
      "Step: 735, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 1.3009631633758545\n",
      "Step: 736, Loss: 0.9361169934272766, Accuracy: 0.9852941036224365, Computation time: 1.5161962509155273\n",
      "Step: 737, Loss: 0.9159942865371704, Accuracy: 1.0, Computation time: 1.8319854736328125\n",
      "Step: 738, Loss: 0.9377146363258362, Accuracy: 0.9750000238418579, Computation time: 1.5071845054626465\n",
      "Step: 739, Loss: 0.918242335319519, Accuracy: 1.0, Computation time: 1.7201805114746094\n",
      "Step: 740, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 2.2797679901123047\n",
      "Step: 741, Loss: 0.9159530401229858, Accuracy: 1.0, Computation time: 1.8638591766357422\n",
      "Step: 742, Loss: 0.9177232980728149, Accuracy: 1.0, Computation time: 1.6073436737060547\n",
      "Step: 743, Loss: 0.9166433215141296, Accuracy: 1.0, Computation time: 1.9554932117462158\n",
      "Step: 744, Loss: 0.9159810543060303, Accuracy: 1.0, Computation time: 1.3222379684448242\n",
      "Step: 745, Loss: 0.9159861207008362, Accuracy: 1.0, Computation time: 1.6784873008728027\n",
      "Step: 746, Loss: 0.9159391522407532, Accuracy: 1.0, Computation time: 1.5029957294464111\n",
      "Step: 747, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.456089735031128\n",
      "Step: 748, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.890808343887329\n",
      "Step: 749, Loss: 0.9163939952850342, Accuracy: 1.0, Computation time: 1.3882591724395752\n",
      "Step: 750, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.569087266921997\n",
      "Step: 751, Loss: 0.9592684507369995, Accuracy: 0.9564393758773804, Computation time: 1.7762980461120605\n",
      "Step: 752, Loss: 0.9159809350967407, Accuracy: 1.0, Computation time: 1.8684751987457275\n",
      "Step: 753, Loss: 0.9378206133842468, Accuracy: 0.96875, Computation time: 1.3475749492645264\n",
      "Step: 754, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 1.7441661357879639\n",
      "Step: 755, Loss: 0.91599440574646, Accuracy: 1.0, Computation time: 1.3519532680511475\n",
      "Step: 756, Loss: 0.91634202003479, Accuracy: 1.0, Computation time: 1.3680193424224854\n",
      "Step: 757, Loss: 0.9160705804824829, Accuracy: 1.0, Computation time: 1.6282775402069092\n",
      "Step: 758, Loss: 0.9179520010948181, Accuracy: 1.0, Computation time: 2.1861655712127686\n",
      "Step: 759, Loss: 0.9377314448356628, Accuracy: 0.96875, Computation time: 1.4301512241363525\n",
      "Step: 760, Loss: 0.9161961078643799, Accuracy: 1.0, Computation time: 1.6393764019012451\n",
      "Step: 761, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.2486143112182617\n",
      "Step: 762, Loss: 0.9166873693466187, Accuracy: 1.0, Computation time: 1.63128662109375\n",
      "Step: 763, Loss: 0.9161624908447266, Accuracy: 1.0, Computation time: 1.6179866790771484\n",
      "Step: 764, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.403299331665039\n",
      "Step: 765, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.567192792892456\n",
      "Step: 766, Loss: 0.9159774780273438, Accuracy: 1.0, Computation time: 1.259824514389038\n",
      "Step: 767, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.1808192729949951\n",
      "Step: 768, Loss: 0.9159594774246216, Accuracy: 1.0, Computation time: 1.762242317199707\n",
      "Step: 769, Loss: 0.9375316500663757, Accuracy: 0.9772727489471436, Computation time: 1.5802972316741943\n",
      "Step: 770, Loss: 0.933631181716919, Accuracy: 0.9642857313156128, Computation time: 1.9162886142730713\n",
      "Step: 771, Loss: 0.915992021560669, Accuracy: 1.0, Computation time: 1.373427391052246\n",
      "Step: 772, Loss: 0.9405736327171326, Accuracy: 0.9722222089767456, Computation time: 1.3987114429473877\n",
      "Step: 773, Loss: 0.9163147211074829, Accuracy: 1.0, Computation time: 1.2753117084503174\n",
      "Step: 774, Loss: 0.9344169497489929, Accuracy: 0.9375, Computation time: 2.054145336151123\n",
      "Step: 775, Loss: 0.9160773754119873, Accuracy: 1.0, Computation time: 1.2383232116699219\n",
      "Step: 776, Loss: 0.9165026545524597, Accuracy: 1.0, Computation time: 1.4806280136108398\n",
      "Step: 777, Loss: 0.9395192265510559, Accuracy: 0.9750000238418579, Computation time: 1.8771677017211914\n",
      "Step: 778, Loss: 0.9373074173927307, Accuracy: 0.9772727489471436, Computation time: 1.4686262607574463\n",
      "Step: 779, Loss: 0.916566014289856, Accuracy: 1.0, Computation time: 1.5251755714416504\n",
      "Step: 780, Loss: 0.9219456911087036, Accuracy: 1.0, Computation time: 1.5182702541351318\n",
      "Step: 781, Loss: 0.9160545468330383, Accuracy: 1.0, Computation time: 1.2519805431365967\n",
      "Step: 782, Loss: 0.9166394472122192, Accuracy: 1.0, Computation time: 1.4934008121490479\n",
      "Step: 783, Loss: 0.9173345565795898, Accuracy: 1.0, Computation time: 1.421886682510376\n",
      "Step: 784, Loss: 0.9159559607505798, Accuracy: 1.0, Computation time: 1.1786508560180664\n",
      "Step: 785, Loss: 0.916225254535675, Accuracy: 1.0, Computation time: 1.3299386501312256\n",
      "Step: 786, Loss: 0.9379764795303345, Accuracy: 0.9750000238418579, Computation time: 1.145946979522705\n",
      "Step: 787, Loss: 0.9160435199737549, Accuracy: 1.0, Computation time: 1.3612570762634277\n",
      "Step: 788, Loss: 0.9486775398254395, Accuracy: 0.9330357313156128, Computation time: 1.3740770816802979\n",
      "Step: 789, Loss: 0.9376415014266968, Accuracy: 0.9821428656578064, Computation time: 1.2899386882781982\n",
      "Step: 790, Loss: 0.9159939885139465, Accuracy: 1.0, Computation time: 1.43477201461792\n",
      "Step: 791, Loss: 0.9160270690917969, Accuracy: 1.0, Computation time: 1.7308237552642822\n",
      "Step: 792, Loss: 0.9217959642410278, Accuracy: 1.0, Computation time: 1.7127645015716553\n",
      "Step: 793, Loss: 0.9182980060577393, Accuracy: 1.0, Computation time: 1.2911598682403564\n",
      "Step: 794, Loss: 0.9161598086357117, Accuracy: 1.0, Computation time: 1.2344250679016113\n",
      "Step: 795, Loss: 0.9181180000305176, Accuracy: 1.0, Computation time: 1.5570721626281738\n",
      "Step: 796, Loss: 0.9162206649780273, Accuracy: 1.0, Computation time: 1.2614667415618896\n",
      "Step: 797, Loss: 0.937147319316864, Accuracy: 0.9772727489471436, Computation time: 2.031923532485962\n",
      "Step: 798, Loss: 0.9347907304763794, Accuracy: 0.96875, Computation time: 1.6952214241027832\n",
      "Step: 799, Loss: 0.9163048267364502, Accuracy: 1.0, Computation time: 1.3661236763000488\n",
      "Step: 800, Loss: 0.919256865978241, Accuracy: 1.0, Computation time: 1.466756820678711\n",
      "Step: 801, Loss: 0.9161255359649658, Accuracy: 1.0, Computation time: 1.3811585903167725\n",
      "Step: 802, Loss: 0.937807559967041, Accuracy: 0.9791666865348816, Computation time: 1.3238325119018555\n",
      "Step: 803, Loss: 0.9160267114639282, Accuracy: 1.0, Computation time: 1.2592384815216064\n",
      "Step: 804, Loss: 0.9160309433937073, Accuracy: 1.0, Computation time: 1.3627188205718994\n",
      "Step: 805, Loss: 0.9376811981201172, Accuracy: 0.9722222089767456, Computation time: 1.3322060108184814\n",
      "Step: 806, Loss: 0.9174134731292725, Accuracy: 1.0, Computation time: 1.4788551330566406\n",
      "Step: 807, Loss: 0.916056752204895, Accuracy: 1.0, Computation time: 1.1840887069702148\n",
      "Step: 808, Loss: 0.917540431022644, Accuracy: 1.0, Computation time: 1.8175249099731445\n",
      "Step: 809, Loss: 0.9167965054512024, Accuracy: 1.0, Computation time: 1.5307648181915283\n",
      "Step: 810, Loss: 0.9165191650390625, Accuracy: 1.0, Computation time: 1.264758586883545\n",
      "Step: 811, Loss: 0.9160233736038208, Accuracy: 1.0, Computation time: 1.5027213096618652\n",
      "Step: 812, Loss: 0.9159734845161438, Accuracy: 1.0, Computation time: 1.585148811340332\n",
      "Step: 813, Loss: 0.915996789932251, Accuracy: 1.0, Computation time: 1.6951332092285156\n",
      "Step: 814, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.4673290252685547\n",
      "Step: 815, Loss: 0.9159654378890991, Accuracy: 1.0, Computation time: 1.327096700668335\n",
      "Step: 816, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 1.2724289894104004\n",
      "Step: 817, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.473726749420166\n",
      "Step: 818, Loss: 0.916094183921814, Accuracy: 1.0, Computation time: 1.1393394470214844\n",
      "Step: 819, Loss: 0.9160255789756775, Accuracy: 1.0, Computation time: 1.238595724105835\n",
      "Step: 820, Loss: 0.9375244975090027, Accuracy: 0.9852941036224365, Computation time: 1.9262549877166748\n",
      "Step: 821, Loss: 0.9159294366836548, Accuracy: 1.0, Computation time: 1.5411291122436523\n",
      "Step: 822, Loss: 0.9163786172866821, Accuracy: 1.0, Computation time: 1.1370062828063965\n",
      "Step: 823, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.554940938949585\n",
      "Step: 824, Loss: 0.9348403215408325, Accuracy: 0.9750000238418579, Computation time: 1.658454179763794\n",
      "Step: 825, Loss: 0.9159280061721802, Accuracy: 1.0, Computation time: 1.226642370223999\n",
      "Step: 826, Loss: 0.9373428821563721, Accuracy: 0.9750000238418579, Computation time: 1.4773166179656982\n",
      "Step: 827, Loss: 0.9160194993019104, Accuracy: 1.0, Computation time: 1.2772736549377441\n",
      "Step: 828, Loss: 0.9289644360542297, Accuracy: 0.96875, Computation time: 1.155428409576416\n",
      "Step: 829, Loss: 0.9165813326835632, Accuracy: 1.0, Computation time: 1.4617314338684082\n",
      "Step: 830, Loss: 0.9160245060920715, Accuracy: 1.0, Computation time: 1.1312499046325684\n",
      "Step: 831, Loss: 0.9159592390060425, Accuracy: 1.0, Computation time: 1.258324384689331\n",
      "Step: 832, Loss: 0.9188068509101868, Accuracy: 1.0, Computation time: 1.616790533065796\n",
      "Step: 833, Loss: 0.9349597096443176, Accuracy: 0.9722222089767456, Computation time: 1.4547791481018066\n",
      "Step: 834, Loss: 0.9159509539604187, Accuracy: 1.0, Computation time: 1.3479008674621582\n",
      "########################\n",
      "Test loss: 1.0725678205490112, Test Accuracy_epoch6: 0.7674192190170288\n",
      "########################\n",
      "Step: 835, Loss: 0.9199570417404175, Accuracy: 1.0, Computation time: 1.5725908279418945\n",
      "Step: 836, Loss: 0.9163588285446167, Accuracy: 1.0, Computation time: 1.3748645782470703\n",
      "Step: 837, Loss: 0.9159514307975769, Accuracy: 1.0, Computation time: 1.3473596572875977\n",
      "Step: 838, Loss: 0.9159601330757141, Accuracy: 1.0, Computation time: 1.2365386486053467\n",
      "Step: 839, Loss: 0.9373969435691833, Accuracy: 0.9791666865348816, Computation time: 1.7850041389465332\n",
      "Step: 840, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.3560264110565186\n",
      "Step: 841, Loss: 0.9411776065826416, Accuracy: 0.9807692766189575, Computation time: 1.4254417419433594\n",
      "Step: 842, Loss: 0.9338535666465759, Accuracy: 0.9772727489471436, Computation time: 1.832991361618042\n",
      "Step: 843, Loss: 0.9590111970901489, Accuracy: 0.9500000476837158, Computation time: 1.3406896591186523\n",
      "Step: 844, Loss: 0.9160253405570984, Accuracy: 1.0, Computation time: 1.3100495338439941\n",
      "Step: 845, Loss: 0.9375079870223999, Accuracy: 0.9791666865348816, Computation time: 1.6841356754302979\n",
      "Step: 846, Loss: 0.9160675406455994, Accuracy: 1.0, Computation time: 1.1029934883117676\n",
      "Step: 847, Loss: 0.9160544276237488, Accuracy: 1.0, Computation time: 1.3749260902404785\n",
      "Step: 848, Loss: 0.9161010384559631, Accuracy: 1.0, Computation time: 1.0470263957977295\n",
      "Step: 849, Loss: 0.9160887598991394, Accuracy: 1.0, Computation time: 1.3203415870666504\n",
      "Step: 850, Loss: 0.9161714315414429, Accuracy: 1.0, Computation time: 1.2135875225067139\n",
      "Step: 851, Loss: 0.9175335168838501, Accuracy: 1.0, Computation time: 1.3428306579589844\n",
      "Step: 852, Loss: 0.9373121857643127, Accuracy: 0.9861111044883728, Computation time: 1.2359211444854736\n",
      "Step: 853, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.0900304317474365\n",
      "Step: 854, Loss: 0.9159713983535767, Accuracy: 1.0, Computation time: 1.31550931930542\n",
      "Step: 855, Loss: 0.9167805314064026, Accuracy: 1.0, Computation time: 1.9671721458435059\n",
      "Step: 856, Loss: 0.9160224199295044, Accuracy: 1.0, Computation time: 1.3823819160461426\n",
      "Step: 857, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 1.6236541271209717\n",
      "Step: 858, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.493527889251709\n",
      "Step: 859, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 1.557037115097046\n",
      "Step: 860, Loss: 0.9377607703208923, Accuracy: 0.9807692766189575, Computation time: 1.4839370250701904\n",
      "Step: 861, Loss: 0.9160968065261841, Accuracy: 1.0, Computation time: 1.431544303894043\n",
      "Step: 862, Loss: 0.9159598350524902, Accuracy: 1.0, Computation time: 1.3354084491729736\n",
      "Step: 863, Loss: 0.9160569310188293, Accuracy: 1.0, Computation time: 1.4737391471862793\n",
      "Step: 864, Loss: 0.9171984791755676, Accuracy: 1.0, Computation time: 1.347442388534546\n",
      "Step: 865, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 1.1446325778961182\n",
      "Step: 866, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 1.266563892364502\n",
      "Step: 867, Loss: 0.9171014428138733, Accuracy: 1.0, Computation time: 1.7573907375335693\n",
      "Step: 868, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.6212184429168701\n",
      "Step: 869, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 2.0302236080169678\n",
      "Step: 870, Loss: 0.9160268306732178, Accuracy: 1.0, Computation time: 1.421278953552246\n",
      "Step: 871, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.6985013484954834\n",
      "Step: 872, Loss: 0.9159661531448364, Accuracy: 1.0, Computation time: 1.2096836566925049\n",
      "Step: 873, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 1.3807392120361328\n",
      "Step: 874, Loss: 0.9158975481987, Accuracy: 1.0, Computation time: 1.5375518798828125\n",
      "Step: 875, Loss: 0.9360412359237671, Accuracy: 0.9821428656578064, Computation time: 1.5871355533599854\n",
      "Step: 876, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.5919005870819092\n",
      "Step: 877, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 1.2100553512573242\n",
      "Step: 878, Loss: 0.9381292462348938, Accuracy: 0.9791666865348816, Computation time: 1.6387555599212646\n",
      "Step: 879, Loss: 0.9166853427886963, Accuracy: 1.0, Computation time: 1.4716856479644775\n",
      "Step: 880, Loss: 0.9159579277038574, Accuracy: 1.0, Computation time: 1.8349249362945557\n",
      "Step: 881, Loss: 0.9362910985946655, Accuracy: 0.9821428656578064, Computation time: 1.3784759044647217\n",
      "Step: 882, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 1.6979894638061523\n",
      "Step: 883, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.1635997295379639\n",
      "Step: 884, Loss: 0.916144609451294, Accuracy: 1.0, Computation time: 1.3088693618774414\n",
      "Step: 885, Loss: 0.915956974029541, Accuracy: 1.0, Computation time: 1.270979404449463\n",
      "Step: 886, Loss: 0.9289407134056091, Accuracy: 0.9722222089767456, Computation time: 1.7574398517608643\n",
      "Step: 887, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.2995412349700928\n",
      "Step: 888, Loss: 0.9159922003746033, Accuracy: 1.0, Computation time: 1.6018097400665283\n",
      "Step: 889, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.3388700485229492\n",
      "Step: 890, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.7475056648254395\n",
      "Step: 891, Loss: 0.916007399559021, Accuracy: 1.0, Computation time: 1.6133465766906738\n",
      "Step: 892, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 1.3282432556152344\n",
      "Step: 893, Loss: 0.9165724515914917, Accuracy: 1.0, Computation time: 2.152434825897217\n",
      "Step: 894, Loss: 0.9376950263977051, Accuracy: 0.96875, Computation time: 1.312807559967041\n",
      "Step: 895, Loss: 0.9277350306510925, Accuracy: 0.9722222089767456, Computation time: 1.5476949214935303\n",
      "Step: 896, Loss: 0.9159110188484192, Accuracy: 1.0, Computation time: 1.211700439453125\n",
      "Step: 897, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 1.5527021884918213\n",
      "Step: 898, Loss: 0.9223525524139404, Accuracy: 1.0, Computation time: 1.6459760665893555\n",
      "Step: 899, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.509948968887329\n",
      "Step: 900, Loss: 0.9167674779891968, Accuracy: 1.0, Computation time: 1.6818995475769043\n",
      "Step: 901, Loss: 0.9167678356170654, Accuracy: 1.0, Computation time: 1.4668567180633545\n",
      "Step: 902, Loss: 0.9222679734230042, Accuracy: 1.0, Computation time: 1.5331504344940186\n",
      "Step: 903, Loss: 0.9160768985748291, Accuracy: 1.0, Computation time: 1.4910683631896973\n",
      "Step: 904, Loss: 0.9161658883094788, Accuracy: 1.0, Computation time: 1.4264421463012695\n",
      "Step: 905, Loss: 0.9367220401763916, Accuracy: 0.9833333492279053, Computation time: 1.2250230312347412\n",
      "Step: 906, Loss: 0.9164278507232666, Accuracy: 1.0, Computation time: 1.278444766998291\n",
      "Step: 907, Loss: 0.9159966707229614, Accuracy: 1.0, Computation time: 1.3203275203704834\n",
      "Step: 908, Loss: 0.9163750410079956, Accuracy: 1.0, Computation time: 1.79569673538208\n",
      "Step: 909, Loss: 0.922820508480072, Accuracy: 1.0, Computation time: 1.3880066871643066\n",
      "Step: 910, Loss: 0.9352596402168274, Accuracy: 0.9833333492279053, Computation time: 1.4146728515625\n",
      "Step: 911, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.2602450847625732\n",
      "Step: 912, Loss: 0.9159485697746277, Accuracy: 1.0, Computation time: 1.6179132461547852\n",
      "Step: 913, Loss: 0.9375929832458496, Accuracy: 0.9642857313156128, Computation time: 1.3593263626098633\n",
      "Step: 914, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.2573130130767822\n",
      "Step: 915, Loss: 0.9362310767173767, Accuracy: 0.9807692766189575, Computation time: 1.3578546047210693\n",
      "Step: 916, Loss: 0.9159545302391052, Accuracy: 1.0, Computation time: 1.4565417766571045\n",
      "Step: 917, Loss: 0.9592099189758301, Accuracy: 0.9545454978942871, Computation time: 1.5120432376861572\n",
      "Step: 918, Loss: 0.916398286819458, Accuracy: 1.0, Computation time: 1.4557886123657227\n",
      "Step: 919, Loss: 0.9175235629081726, Accuracy: 1.0, Computation time: 1.680647373199463\n",
      "Step: 920, Loss: 0.9375360012054443, Accuracy: 0.9642857313156128, Computation time: 1.6774849891662598\n",
      "Step: 921, Loss: 0.936893105506897, Accuracy: 0.9750000238418579, Computation time: 1.3721811771392822\n",
      "Step: 922, Loss: 0.9162772297859192, Accuracy: 1.0, Computation time: 1.3967053890228271\n",
      "Step: 923, Loss: 0.9161359667778015, Accuracy: 1.0, Computation time: 1.2786304950714111\n",
      "Step: 924, Loss: 0.9381720423698425, Accuracy: 0.9807692766189575, Computation time: 1.771176815032959\n",
      "Step: 925, Loss: 0.9160824418067932, Accuracy: 1.0, Computation time: 1.3410890102386475\n",
      "Step: 926, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 1.5866727828979492\n",
      "Step: 927, Loss: 0.9172531366348267, Accuracy: 1.0, Computation time: 1.4668223857879639\n",
      "Step: 928, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.4889013767242432\n",
      "Step: 929, Loss: 0.919306755065918, Accuracy: 1.0, Computation time: 1.658078908920288\n",
      "Step: 930, Loss: 0.9376009702682495, Accuracy: 0.9791666865348816, Computation time: 1.5348668098449707\n",
      "Step: 931, Loss: 0.9252904653549194, Accuracy: 1.0, Computation time: 1.43477463722229\n",
      "Step: 932, Loss: 0.9388806819915771, Accuracy: 0.96875, Computation time: 1.5897717475891113\n",
      "Step: 933, Loss: 0.9169032573699951, Accuracy: 1.0, Computation time: 1.2940311431884766\n",
      "Step: 934, Loss: 0.9159916043281555, Accuracy: 1.0, Computation time: 1.8665580749511719\n",
      "Step: 935, Loss: 0.9159725904464722, Accuracy: 1.0, Computation time: 1.6025605201721191\n",
      "Step: 936, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 1.6099863052368164\n",
      "Step: 937, Loss: 0.9161920547485352, Accuracy: 1.0, Computation time: 1.4532561302185059\n",
      "Step: 938, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.3685600757598877\n",
      "Step: 939, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.4108355045318604\n",
      "Step: 940, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.3938617706298828\n",
      "Step: 941, Loss: 0.9166927933692932, Accuracy: 1.0, Computation time: 1.7021856307983398\n",
      "Step: 942, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.6271350383758545\n",
      "Step: 943, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.6866674423217773\n",
      "Step: 944, Loss: 0.9169244766235352, Accuracy: 1.0, Computation time: 1.7309865951538086\n",
      "Step: 945, Loss: 0.9354032874107361, Accuracy: 0.9772727489471436, Computation time: 2.1506917476654053\n",
      "Step: 946, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.3004462718963623\n",
      "Step: 947, Loss: 0.9159078001976013, Accuracy: 1.0, Computation time: 1.6028075218200684\n",
      "Step: 948, Loss: 0.915947437286377, Accuracy: 1.0, Computation time: 1.5448949337005615\n",
      "Step: 949, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.5504002571105957\n",
      "Step: 950, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 1.4263744354248047\n",
      "Step: 951, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 1.315495491027832\n",
      "Step: 952, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.5704631805419922\n",
      "Step: 953, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.3946151733398438\n",
      "Step: 954, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.8772010803222656\n",
      "Step: 955, Loss: 0.9264246821403503, Accuracy: 0.9791666865348816, Computation time: 1.2970836162567139\n",
      "Step: 956, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.451401948928833\n",
      "Step: 957, Loss: 0.9274344444274902, Accuracy: 0.9772727489471436, Computation time: 1.4430897235870361\n",
      "Step: 958, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.125\n",
      "Step: 959, Loss: 0.915997326374054, Accuracy: 1.0, Computation time: 1.7703723907470703\n",
      "Step: 960, Loss: 0.9162545204162598, Accuracy: 1.0, Computation time: 1.513685941696167\n",
      "Step: 961, Loss: 0.9161226749420166, Accuracy: 1.0, Computation time: 1.3484771251678467\n",
      "Step: 962, Loss: 0.9365162253379822, Accuracy: 0.9772727489471436, Computation time: 1.6971704959869385\n",
      "Step: 963, Loss: 0.9160845279693604, Accuracy: 1.0, Computation time: 1.5040879249572754\n",
      "Step: 964, Loss: 0.9159772992134094, Accuracy: 1.0, Computation time: 1.5318021774291992\n",
      "Step: 965, Loss: 0.9186199307441711, Accuracy: 1.0, Computation time: 2.111405611038208\n",
      "Step: 966, Loss: 0.9236271977424622, Accuracy: 1.0, Computation time: 1.6876182556152344\n",
      "Step: 967, Loss: 0.9161020517349243, Accuracy: 1.0, Computation time: 1.6785197257995605\n",
      "Step: 968, Loss: 0.9321820139884949, Accuracy: 0.9833333492279053, Computation time: 1.9433650970458984\n",
      "Step: 969, Loss: 0.9252071976661682, Accuracy: 1.0, Computation time: 1.4535818099975586\n",
      "Step: 970, Loss: 0.9159964919090271, Accuracy: 1.0, Computation time: 1.35728120803833\n",
      "Step: 971, Loss: 0.9160361886024475, Accuracy: 1.0, Computation time: 1.3959848880767822\n",
      "Step: 972, Loss: 0.9160905480384827, Accuracy: 1.0, Computation time: 1.3592009544372559\n",
      "Step: 973, Loss: 0.917578935623169, Accuracy: 1.0, Computation time: 1.7155530452728271\n",
      "########################\n",
      "Test loss: 1.0723623037338257, Test Accuracy_epoch7: 0.7640986442565918\n",
      "########################\n",
      "Step: 974, Loss: 0.9437237977981567, Accuracy: 0.9791666865348816, Computation time: 1.544447422027588\n",
      "Step: 975, Loss: 0.9161152839660645, Accuracy: 1.0, Computation time: 1.1556105613708496\n",
      "Step: 976, Loss: 0.9386284947395325, Accuracy: 0.9583333730697632, Computation time: 1.4802618026733398\n",
      "Step: 977, Loss: 0.915977954864502, Accuracy: 1.0, Computation time: 1.57350492477417\n",
      "Step: 978, Loss: 0.9163382053375244, Accuracy: 1.0, Computation time: 1.9104979038238525\n",
      "Step: 979, Loss: 0.9159935116767883, Accuracy: 1.0, Computation time: 1.4658973217010498\n",
      "Step: 980, Loss: 0.9160019159317017, Accuracy: 1.0, Computation time: 1.5374729633331299\n",
      "Step: 981, Loss: 0.9375202655792236, Accuracy: 0.9791666865348816, Computation time: 1.6404104232788086\n",
      "Step: 982, Loss: 0.9163455367088318, Accuracy: 1.0, Computation time: 1.7354912757873535\n",
      "Step: 983, Loss: 0.9162449240684509, Accuracy: 1.0, Computation time: 1.2643578052520752\n",
      "Step: 984, Loss: 0.9162857532501221, Accuracy: 1.0, Computation time: 1.4398558139801025\n",
      "Step: 985, Loss: 0.9176839590072632, Accuracy: 1.0, Computation time: 1.4391546249389648\n",
      "Step: 986, Loss: 0.9162560701370239, Accuracy: 1.0, Computation time: 1.2846646308898926\n",
      "Step: 987, Loss: 0.9177945852279663, Accuracy: 1.0, Computation time: 1.2622594833374023\n",
      "Step: 988, Loss: 0.9160029292106628, Accuracy: 1.0, Computation time: 1.4089758396148682\n",
      "Step: 989, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.5598235130310059\n",
      "Step: 990, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 1.5229930877685547\n",
      "Step: 991, Loss: 0.9159644246101379, Accuracy: 1.0, Computation time: 1.2880232334136963\n",
      "Step: 992, Loss: 0.9159761667251587, Accuracy: 1.0, Computation time: 1.1972270011901855\n",
      "Step: 993, Loss: 0.9160404801368713, Accuracy: 1.0, Computation time: 1.2517995834350586\n",
      "Step: 994, Loss: 0.9160537719726562, Accuracy: 1.0, Computation time: 1.3346397876739502\n",
      "Step: 995, Loss: 0.9161969423294067, Accuracy: 1.0, Computation time: 1.5077977180480957\n",
      "Step: 996, Loss: 0.9376039505004883, Accuracy: 0.984375, Computation time: 1.420020580291748\n",
      "Step: 997, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.2095832824707031\n",
      "Step: 998, Loss: 0.9159586429595947, Accuracy: 1.0, Computation time: 1.1639950275421143\n",
      "Step: 999, Loss: 0.9166516661643982, Accuracy: 1.0, Computation time: 2.1002891063690186\n",
      "Step: 1000, Loss: 0.9160761833190918, Accuracy: 1.0, Computation time: 1.3473467826843262\n",
      "Step: 1001, Loss: 0.9279167652130127, Accuracy: 0.9772727489471436, Computation time: 2.110217809677124\n",
      "Step: 1002, Loss: 0.9160626530647278, Accuracy: 1.0, Computation time: 1.703382968902588\n",
      "Step: 1003, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.415376901626587\n",
      "Step: 1004, Loss: 0.9160441160202026, Accuracy: 1.0, Computation time: 1.2522997856140137\n",
      "Step: 1005, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 1.4353277683258057\n",
      "Step: 1006, Loss: 0.9358770847320557, Accuracy: 0.9750000238418579, Computation time: 1.2379529476165771\n",
      "Step: 1007, Loss: 0.9377305507659912, Accuracy: 0.9750000238418579, Computation time: 1.3118150234222412\n",
      "Step: 1008, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 1.213042974472046\n",
      "Step: 1009, Loss: 0.91886967420578, Accuracy: 1.0, Computation time: 1.099560022354126\n",
      "Step: 1010, Loss: 0.9378699660301208, Accuracy: 0.9807692766189575, Computation time: 1.2711358070373535\n",
      "Step: 1011, Loss: 0.9159393906593323, Accuracy: 1.0, Computation time: 1.4309465885162354\n",
      "Step: 1012, Loss: 0.9159222841262817, Accuracy: 1.0, Computation time: 1.339843988418579\n",
      "Step: 1013, Loss: 0.9170053601264954, Accuracy: 1.0, Computation time: 1.5076618194580078\n",
      "Step: 1014, Loss: 0.9171772599220276, Accuracy: 1.0, Computation time: 1.2922194004058838\n",
      "Step: 1015, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.2555136680603027\n",
      "Step: 1016, Loss: 0.9228244423866272, Accuracy: 1.0, Computation time: 1.892627239227295\n",
      "Step: 1017, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.5105247497558594\n",
      "Step: 1018, Loss: 0.9161654114723206, Accuracy: 1.0, Computation time: 1.266066551208496\n",
      "Step: 1019, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 1.4427525997161865\n",
      "Step: 1020, Loss: 0.9159106612205505, Accuracy: 1.0, Computation time: 1.1686773300170898\n",
      "Step: 1021, Loss: 0.915920615196228, Accuracy: 1.0, Computation time: 1.1193981170654297\n",
      "Step: 1022, Loss: 0.9159470796585083, Accuracy: 1.0, Computation time: 1.4694013595581055\n",
      "Step: 1023, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.1345431804656982\n",
      "Step: 1024, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.622610330581665\n",
      "Step: 1025, Loss: 0.9375157356262207, Accuracy: 0.9772727489471436, Computation time: 1.452385425567627\n",
      "Step: 1026, Loss: 0.9160804152488708, Accuracy: 1.0, Computation time: 1.397843599319458\n",
      "Step: 1027, Loss: 0.9383205771446228, Accuracy: 0.9722222089767456, Computation time: 1.577017068862915\n",
      "Step: 1028, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.2590177059173584\n",
      "Step: 1029, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.580472469329834\n",
      "Step: 1030, Loss: 0.9362519383430481, Accuracy: 0.9807692766189575, Computation time: 1.859483003616333\n",
      "Step: 1031, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 1.5092060565948486\n",
      "Step: 1032, Loss: 0.9342799186706543, Accuracy: 0.96875, Computation time: 1.3014748096466064\n",
      "Step: 1033, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.5642316341400146\n",
      "Step: 1034, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.2877113819122314\n",
      "Step: 1035, Loss: 0.9378017783164978, Accuracy: 0.96875, Computation time: 1.3385803699493408\n",
      "Step: 1036, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.4017090797424316\n",
      "Step: 1037, Loss: 0.9374945163726807, Accuracy: 0.9722222089767456, Computation time: 1.5137324333190918\n",
      "Step: 1038, Loss: 0.9161091446876526, Accuracy: 1.0, Computation time: 1.5794775485992432\n",
      "Step: 1039, Loss: 0.9300581812858582, Accuracy: 0.9791666865348816, Computation time: 1.509427785873413\n",
      "Step: 1040, Loss: 0.916096031665802, Accuracy: 1.0, Computation time: 1.518878698348999\n",
      "Step: 1041, Loss: 0.9343491792678833, Accuracy: 0.96875, Computation time: 1.5930142402648926\n",
      "Step: 1042, Loss: 0.9273437261581421, Accuracy: 0.9750000238418579, Computation time: 1.5289883613586426\n",
      "Step: 1043, Loss: 0.9162179827690125, Accuracy: 1.0, Computation time: 1.4768013954162598\n",
      "Step: 1044, Loss: 0.9164488911628723, Accuracy: 1.0, Computation time: 1.3541326522827148\n",
      "Step: 1045, Loss: 0.9161338806152344, Accuracy: 1.0, Computation time: 1.3998360633850098\n",
      "Step: 1046, Loss: 0.9163753390312195, Accuracy: 1.0, Computation time: 1.7718050479888916\n",
      "Step: 1047, Loss: 0.9163217544555664, Accuracy: 1.0, Computation time: 1.493276834487915\n",
      "Step: 1048, Loss: 0.916527509689331, Accuracy: 1.0, Computation time: 1.301919937133789\n",
      "Step: 1049, Loss: 0.9166718125343323, Accuracy: 1.0, Computation time: 1.5781424045562744\n",
      "Step: 1050, Loss: 0.9161650538444519, Accuracy: 1.0, Computation time: 1.4656181335449219\n",
      "Step: 1051, Loss: 0.9160492420196533, Accuracy: 1.0, Computation time: 1.4166922569274902\n",
      "Step: 1052, Loss: 0.9160614609718323, Accuracy: 1.0, Computation time: 1.5253210067749023\n",
      "Step: 1053, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.5050301551818848\n",
      "Step: 1054, Loss: 0.9159369468688965, Accuracy: 1.0, Computation time: 1.1579864025115967\n",
      "Step: 1055, Loss: 0.9181225299835205, Accuracy: 1.0, Computation time: 1.2143354415893555\n",
      "Step: 1056, Loss: 0.9480246305465698, Accuracy: 0.8666666746139526, Computation time: 1.399653434753418\n",
      "Step: 1057, Loss: 0.9171124696731567, Accuracy: 1.0, Computation time: 1.2923328876495361\n",
      "Step: 1058, Loss: 0.9161568880081177, Accuracy: 1.0, Computation time: 1.066953182220459\n",
      "Step: 1059, Loss: 0.9382644295692444, Accuracy: 0.9833333492279053, Computation time: 1.3659918308258057\n",
      "Step: 1060, Loss: 0.9178721308708191, Accuracy: 1.0, Computation time: 1.1810498237609863\n",
      "Step: 1061, Loss: 0.9169363379478455, Accuracy: 1.0, Computation time: 1.1208157539367676\n",
      "Step: 1062, Loss: 0.9163344502449036, Accuracy: 1.0, Computation time: 1.2648475170135498\n",
      "Step: 1063, Loss: 0.9167494773864746, Accuracy: 1.0, Computation time: 1.3769772052764893\n",
      "Step: 1064, Loss: 0.9161677360534668, Accuracy: 1.0, Computation time: 1.2093803882598877\n",
      "Step: 1065, Loss: 0.916160523891449, Accuracy: 1.0, Computation time: 1.166414499282837\n",
      "Step: 1066, Loss: 0.9376717805862427, Accuracy: 0.9722222089767456, Computation time: 1.2309744358062744\n",
      "Step: 1067, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.8831572532653809\n",
      "Step: 1068, Loss: 0.9169562458992004, Accuracy: 1.0, Computation time: 1.1816248893737793\n",
      "Step: 1069, Loss: 0.9159725308418274, Accuracy: 1.0, Computation time: 1.271047592163086\n",
      "Step: 1070, Loss: 0.9318739175796509, Accuracy: 0.96875, Computation time: 1.2537157535552979\n",
      "Step: 1071, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.0508499145507812\n",
      "Step: 1072, Loss: 0.9164094924926758, Accuracy: 1.0, Computation time: 1.1276240348815918\n",
      "Step: 1073, Loss: 0.9175357222557068, Accuracy: 1.0, Computation time: 1.1624417304992676\n",
      "Step: 1074, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 1.3401620388031006\n",
      "Step: 1075, Loss: 0.9160629510879517, Accuracy: 1.0, Computation time: 1.2556359767913818\n",
      "Step: 1076, Loss: 0.9386776089668274, Accuracy: 0.9642857313156128, Computation time: 1.1617388725280762\n",
      "Step: 1077, Loss: 0.9160130620002747, Accuracy: 1.0, Computation time: 1.5292549133300781\n",
      "Step: 1078, Loss: 0.9198295474052429, Accuracy: 1.0, Computation time: 1.6550190448760986\n",
      "Step: 1079, Loss: 0.9481017589569092, Accuracy: 0.908730149269104, Computation time: 1.457021951675415\n",
      "Step: 1080, Loss: 0.9192447066307068, Accuracy: 1.0, Computation time: 1.9050707817077637\n",
      "Step: 1081, Loss: 0.9383748769760132, Accuracy: 0.9583333730697632, Computation time: 1.3723399639129639\n",
      "Step: 1082, Loss: 0.9170539379119873, Accuracy: 1.0, Computation time: 1.2674391269683838\n",
      "Step: 1083, Loss: 0.9173083901405334, Accuracy: 1.0, Computation time: 1.4871087074279785\n",
      "Step: 1084, Loss: 0.9224046468734741, Accuracy: 1.0, Computation time: 1.6354293823242188\n",
      "Step: 1085, Loss: 0.9176222681999207, Accuracy: 1.0, Computation time: 1.1625416278839111\n",
      "Step: 1086, Loss: 0.9175934791564941, Accuracy: 1.0, Computation time: 1.2230355739593506\n",
      "Step: 1087, Loss: 0.9184277057647705, Accuracy: 1.0, Computation time: 1.2101037502288818\n",
      "Step: 1088, Loss: 0.9170587062835693, Accuracy: 1.0, Computation time: 2.006789445877075\n",
      "Step: 1089, Loss: 0.9161389470100403, Accuracy: 1.0, Computation time: 1.4566619396209717\n",
      "Step: 1090, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 1.3646187782287598\n",
      "Step: 1091, Loss: 0.9159676432609558, Accuracy: 1.0, Computation time: 1.4157612323760986\n",
      "Step: 1092, Loss: 0.9246652126312256, Accuracy: 1.0, Computation time: 1.6214838027954102\n",
      "Step: 1093, Loss: 0.9163621068000793, Accuracy: 1.0, Computation time: 1.5468816757202148\n",
      "Step: 1094, Loss: 0.9379972815513611, Accuracy: 0.9750000238418579, Computation time: 1.4848551750183105\n",
      "Step: 1095, Loss: 0.9165531396865845, Accuracy: 1.0, Computation time: 1.206552505493164\n",
      "Step: 1096, Loss: 0.959862470626831, Accuracy: 0.9583333730697632, Computation time: 1.5844340324401855\n",
      "Step: 1097, Loss: 0.9164209961891174, Accuracy: 1.0, Computation time: 1.134939432144165\n",
      "Step: 1098, Loss: 0.9196836948394775, Accuracy: 1.0, Computation time: 1.3471190929412842\n",
      "Step: 1099, Loss: 0.9354029297828674, Accuracy: 0.9821428656578064, Computation time: 1.0687458515167236\n",
      "Step: 1100, Loss: 0.9162439703941345, Accuracy: 1.0, Computation time: 1.1511797904968262\n",
      "Step: 1101, Loss: 0.9375309944152832, Accuracy: 0.9722222089767456, Computation time: 1.3568308353424072\n",
      "Step: 1102, Loss: 0.9160158038139343, Accuracy: 1.0, Computation time: 1.4229788780212402\n",
      "Step: 1103, Loss: 0.9353581666946411, Accuracy: 0.9821428656578064, Computation time: 1.186161994934082\n",
      "Step: 1104, Loss: 0.9159334301948547, Accuracy: 1.0, Computation time: 1.2669804096221924\n",
      "Step: 1105, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 1.1264948844909668\n",
      "Step: 1106, Loss: 0.9204221367835999, Accuracy: 1.0, Computation time: 1.4261929988861084\n",
      "Step: 1107, Loss: 0.9162458777427673, Accuracy: 1.0, Computation time: 1.338597059249878\n",
      "Step: 1108, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 1.147437334060669\n",
      "Step: 1109, Loss: 0.9160088300704956, Accuracy: 1.0, Computation time: 1.385298728942871\n",
      "Step: 1110, Loss: 0.9168670773506165, Accuracy: 1.0, Computation time: 1.6966114044189453\n",
      "Step: 1111, Loss: 0.9171093702316284, Accuracy: 1.0, Computation time: 1.6939060688018799\n",
      "Step: 1112, Loss: 0.9160314202308655, Accuracy: 1.0, Computation time: 1.1907975673675537\n",
      "########################\n",
      "Test loss: 1.0747824907302856, Test Accuracy_epoch8: 0.764655351638794\n",
      "########################\n",
      "Step: 1113, Loss: 0.9159826040267944, Accuracy: 1.0, Computation time: 1.3045411109924316\n",
      "Step: 1114, Loss: 0.9378114938735962, Accuracy: 0.9642857313156128, Computation time: 1.3007376194000244\n",
      "Step: 1115, Loss: 0.9168537855148315, Accuracy: 1.0, Computation time: 1.1954090595245361\n",
      "Step: 1116, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 1.3285095691680908\n",
      "Step: 1117, Loss: 0.916193425655365, Accuracy: 1.0, Computation time: 1.035531997680664\n",
      "Step: 1118, Loss: 0.9159186482429504, Accuracy: 1.0, Computation time: 1.256629228591919\n",
      "Step: 1119, Loss: 0.9159278273582458, Accuracy: 1.0, Computation time: 1.3971600532531738\n",
      "Step: 1120, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.2972900867462158\n",
      "Step: 1121, Loss: 0.9376338720321655, Accuracy: 0.9722222089767456, Computation time: 1.7775394916534424\n",
      "Step: 1122, Loss: 0.9161363244056702, Accuracy: 1.0, Computation time: 1.2343125343322754\n",
      "Step: 1123, Loss: 0.934940755367279, Accuracy: 0.9750000238418579, Computation time: 1.7142255306243896\n",
      "Step: 1124, Loss: 0.9250442385673523, Accuracy: 1.0, Computation time: 1.5372600555419922\n",
      "Step: 1125, Loss: 0.9159586429595947, Accuracy: 1.0, Computation time: 1.6772618293762207\n",
      "Step: 1126, Loss: 0.9159896969795227, Accuracy: 1.0, Computation time: 1.3786585330963135\n",
      "Step: 1127, Loss: 0.9169489741325378, Accuracy: 1.0, Computation time: 1.2459888458251953\n",
      "Step: 1128, Loss: 0.9159824252128601, Accuracy: 1.0, Computation time: 1.014350414276123\n",
      "Step: 1129, Loss: 0.9378272891044617, Accuracy: 0.9722222089767456, Computation time: 1.3934729099273682\n",
      "Step: 1130, Loss: 0.9159771800041199, Accuracy: 1.0, Computation time: 1.3379395008087158\n",
      "Step: 1131, Loss: 0.9376981854438782, Accuracy: 0.9750000238418579, Computation time: 1.2090020179748535\n",
      "Step: 1132, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.419511079788208\n",
      "Step: 1133, Loss: 0.9231595396995544, Accuracy: 1.0, Computation time: 1.6429619789123535\n",
      "Step: 1134, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.322314739227295\n",
      "Step: 1135, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.5771055221557617\n",
      "Step: 1136, Loss: 0.9248033761978149, Accuracy: 1.0, Computation time: 1.733738899230957\n",
      "Step: 1137, Loss: 0.9163992404937744, Accuracy: 1.0, Computation time: 1.461465835571289\n",
      "Step: 1138, Loss: 0.9368435740470886, Accuracy: 0.9642857313156128, Computation time: 1.4860377311706543\n",
      "Step: 1139, Loss: 0.9159374833106995, Accuracy: 1.0, Computation time: 1.264589548110962\n",
      "Step: 1140, Loss: 0.9160779118537903, Accuracy: 1.0, Computation time: 1.6670961380004883\n",
      "Step: 1141, Loss: 0.915998101234436, Accuracy: 1.0, Computation time: 1.363053560256958\n",
      "Step: 1142, Loss: 0.9158980250358582, Accuracy: 1.0, Computation time: 1.2221910953521729\n",
      "Step: 1143, Loss: 0.9166389107704163, Accuracy: 1.0, Computation time: 1.2129709720611572\n",
      "Step: 1144, Loss: 0.9159374237060547, Accuracy: 1.0, Computation time: 1.3693664073944092\n",
      "Step: 1145, Loss: 0.9338090419769287, Accuracy: 0.96875, Computation time: 1.5086963176727295\n",
      "Step: 1146, Loss: 0.9376314878463745, Accuracy: 0.9722222089767456, Computation time: 1.187699794769287\n",
      "Step: 1147, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.2429955005645752\n",
      "Step: 1148, Loss: 0.9159736633300781, Accuracy: 1.0, Computation time: 1.433115005493164\n",
      "Step: 1149, Loss: 0.9160758852958679, Accuracy: 1.0, Computation time: 1.363095760345459\n",
      "Step: 1150, Loss: 0.9186803698539734, Accuracy: 1.0, Computation time: 1.6582098007202148\n",
      "Step: 1151, Loss: 0.9159347414970398, Accuracy: 1.0, Computation time: 2.2652957439422607\n",
      "Step: 1152, Loss: 0.924699068069458, Accuracy: 1.0, Computation time: 1.39328932762146\n",
      "Step: 1153, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.3763878345489502\n",
      "Step: 1154, Loss: 0.9159290194511414, Accuracy: 1.0, Computation time: 1.7443890571594238\n",
      "Step: 1155, Loss: 0.9286444783210754, Accuracy: 0.9791666865348816, Computation time: 1.5084359645843506\n",
      "Step: 1156, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.258260726928711\n",
      "Step: 1157, Loss: 0.9368106126785278, Accuracy: 0.9642857313156128, Computation time: 1.5214908123016357\n",
      "Step: 1158, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.6859302520751953\n",
      "Step: 1159, Loss: 0.9175867438316345, Accuracy: 1.0, Computation time: 1.6012108325958252\n",
      "Step: 1160, Loss: 0.9160815477371216, Accuracy: 1.0, Computation time: 1.6269629001617432\n",
      "Step: 1161, Loss: 0.9340734481811523, Accuracy: 0.9833333492279053, Computation time: 1.1974208354949951\n",
      "Step: 1162, Loss: 0.9160150289535522, Accuracy: 1.0, Computation time: 1.1384999752044678\n",
      "Step: 1163, Loss: 0.9159372448921204, Accuracy: 1.0, Computation time: 1.6180341243743896\n",
      "Step: 1164, Loss: 0.937338650226593, Accuracy: 0.9791666865348816, Computation time: 1.745373249053955\n",
      "Step: 1165, Loss: 0.9159441590309143, Accuracy: 1.0, Computation time: 1.606011152267456\n",
      "Step: 1166, Loss: 0.915970504283905, Accuracy: 1.0, Computation time: 1.4380710124969482\n",
      "Step: 1167, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.115447759628296\n",
      "Step: 1168, Loss: 0.9159336686134338, Accuracy: 1.0, Computation time: 1.2746386528015137\n",
      "Step: 1169, Loss: 0.9351644515991211, Accuracy: 0.9791666865348816, Computation time: 1.3783116340637207\n",
      "Step: 1170, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.1738121509552002\n",
      "Step: 1171, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.2985098361968994\n",
      "Step: 1172, Loss: 0.9159994721412659, Accuracy: 1.0, Computation time: 1.160780668258667\n",
      "Step: 1173, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 1.3100903034210205\n",
      "Step: 1174, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.5009551048278809\n",
      "Step: 1175, Loss: 0.9162195324897766, Accuracy: 1.0, Computation time: 1.4922528266906738\n",
      "Step: 1176, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 1.3619046211242676\n",
      "Step: 1177, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.35994291305542\n",
      "Step: 1178, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 1.5045666694641113\n",
      "Step: 1179, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.1893196105957031\n",
      "Step: 1180, Loss: 0.9168305397033691, Accuracy: 1.0, Computation time: 1.4647877216339111\n",
      "Step: 1181, Loss: 0.9162784218788147, Accuracy: 1.0, Computation time: 1.3275508880615234\n",
      "Step: 1182, Loss: 0.9373126029968262, Accuracy: 0.9722222089767456, Computation time: 1.1782162189483643\n",
      "Step: 1183, Loss: 0.9159749150276184, Accuracy: 1.0, Computation time: 1.6558253765106201\n",
      "Step: 1184, Loss: 0.9363973140716553, Accuracy: 0.9583333730697632, Computation time: 1.2796299457550049\n",
      "Step: 1185, Loss: 0.9161025285720825, Accuracy: 1.0, Computation time: 1.37428617477417\n",
      "Step: 1186, Loss: 0.9208773970603943, Accuracy: 1.0, Computation time: 1.9861187934875488\n",
      "Step: 1187, Loss: 0.9159078001976013, Accuracy: 1.0, Computation time: 1.7523748874664307\n",
      "Step: 1188, Loss: 0.9161036610603333, Accuracy: 1.0, Computation time: 1.3155879974365234\n",
      "Step: 1189, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.3727378845214844\n",
      "Step: 1190, Loss: 0.9375118017196655, Accuracy: 0.9791666865348816, Computation time: 1.475980281829834\n",
      "Step: 1191, Loss: 0.9376869201660156, Accuracy: 0.9750000238418579, Computation time: 1.469468355178833\n",
      "Step: 1192, Loss: 0.916542112827301, Accuracy: 1.0, Computation time: 1.6772856712341309\n",
      "Step: 1193, Loss: 0.9159559607505798, Accuracy: 1.0, Computation time: 1.2126350402832031\n",
      "Step: 1194, Loss: 0.9374489188194275, Accuracy: 0.9750000238418579, Computation time: 1.5625803470611572\n",
      "Step: 1195, Loss: 0.9159291386604309, Accuracy: 1.0, Computation time: 1.2236087322235107\n",
      "Step: 1196, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 1.6485466957092285\n",
      "Step: 1197, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.1916067600250244\n",
      "Step: 1198, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.4560258388519287\n",
      "Step: 1199, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.395232915878296\n",
      "Step: 1200, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.1447334289550781\n",
      "Step: 1201, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.1789946556091309\n",
      "Step: 1202, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 1.083604335784912\n",
      "Step: 1203, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.233154296875\n",
      "Step: 1204, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.1133713722229004\n",
      "Step: 1205, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.1144394874572754\n",
      "Step: 1206, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.5437557697296143\n",
      "Step: 1207, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.0821337699890137\n",
      "Step: 1208, Loss: 0.9165464639663696, Accuracy: 1.0, Computation time: 1.6147644519805908\n",
      "Step: 1209, Loss: 0.9162595868110657, Accuracy: 1.0, Computation time: 1.2790813446044922\n",
      "Step: 1210, Loss: 0.9387677311897278, Accuracy: 0.9642857313156128, Computation time: 1.5921990871429443\n",
      "Step: 1211, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.3345940113067627\n",
      "Step: 1212, Loss: 0.9367120862007141, Accuracy: 0.9791666865348816, Computation time: 1.520592451095581\n",
      "Step: 1213, Loss: 0.9398671984672546, Accuracy: 0.9583333730697632, Computation time: 1.9653303623199463\n",
      "Step: 1214, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 1.3399927616119385\n",
      "Step: 1215, Loss: 0.915978193283081, Accuracy: 1.0, Computation time: 1.2921738624572754\n",
      "Step: 1216, Loss: 0.9215165376663208, Accuracy: nan, Computation time: 1.4553940296173096\n",
      "Step: 1217, Loss: 0.9160193204879761, Accuracy: 1.0, Computation time: 1.325028419494629\n",
      "Step: 1218, Loss: 0.9551264047622681, Accuracy: 0.9375, Computation time: 1.4522724151611328\n",
      "Step: 1219, Loss: 0.9161426424980164, Accuracy: 1.0, Computation time: 1.6036653518676758\n",
      "Step: 1220, Loss: 0.9358627200126648, Accuracy: 0.9821428656578064, Computation time: 1.2655484676361084\n",
      "Step: 1221, Loss: 0.9169315099716187, Accuracy: 1.0, Computation time: 1.4346389770507812\n",
      "Step: 1222, Loss: 0.9553221464157104, Accuracy: 0.9305555820465088, Computation time: 1.7059805393218994\n",
      "Step: 1223, Loss: 0.9159930348396301, Accuracy: 1.0, Computation time: 1.3061401844024658\n",
      "Step: 1224, Loss: 0.933623194694519, Accuracy: 0.9772727489471436, Computation time: 1.9710392951965332\n",
      "Step: 1225, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 1.3001728057861328\n",
      "Step: 1226, Loss: 0.9159892201423645, Accuracy: 1.0, Computation time: 1.8060522079467773\n",
      "Step: 1227, Loss: 0.9162368178367615, Accuracy: 1.0, Computation time: 1.3924119472503662\n",
      "Step: 1228, Loss: 0.916546642780304, Accuracy: 1.0, Computation time: 2.090376377105713\n",
      "Step: 1229, Loss: 0.9162163734436035, Accuracy: 1.0, Computation time: 1.6853289604187012\n",
      "Step: 1230, Loss: 0.9161822199821472, Accuracy: 1.0, Computation time: 1.6698822975158691\n",
      "Step: 1231, Loss: 0.9161981344223022, Accuracy: 1.0, Computation time: 1.342085361480713\n",
      "Step: 1232, Loss: 0.9162960052490234, Accuracy: 1.0, Computation time: 1.707444429397583\n",
      "Step: 1233, Loss: 0.9161869287490845, Accuracy: 1.0, Computation time: 1.3574481010437012\n",
      "Step: 1234, Loss: 0.9160639047622681, Accuracy: 1.0, Computation time: 1.3085551261901855\n",
      "Step: 1235, Loss: 0.9159989356994629, Accuracy: 1.0, Computation time: 1.2557921409606934\n",
      "Step: 1236, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 1.0138874053955078\n",
      "Step: 1237, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 1.3440558910369873\n",
      "Step: 1238, Loss: 0.9185846447944641, Accuracy: 1.0, Computation time: 1.6816952228546143\n",
      "Step: 1239, Loss: 0.9166980981826782, Accuracy: 1.0, Computation time: 1.6474730968475342\n",
      "Step: 1240, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 1.8410179615020752\n",
      "Step: 1241, Loss: 0.937305748462677, Accuracy: 0.9852941036224365, Computation time: 1.1975479125976562\n",
      "Step: 1242, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.3865222930908203\n",
      "Step: 1243, Loss: 0.9381149411201477, Accuracy: 0.9821428656578064, Computation time: 1.55120849609375\n",
      "Step: 1244, Loss: 0.916774570941925, Accuracy: 1.0, Computation time: 1.6809351444244385\n",
      "Step: 1245, Loss: 0.9161397218704224, Accuracy: 1.0, Computation time: 1.2190122604370117\n",
      "Step: 1246, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.1790406703948975\n",
      "Step: 1247, Loss: 0.9375905990600586, Accuracy: 0.9642857313156128, Computation time: 1.0404002666473389\n",
      "Step: 1248, Loss: 0.9161664247512817, Accuracy: 1.0, Computation time: 1.2775204181671143\n",
      "Step: 1249, Loss: 0.9160130620002747, Accuracy: 1.0, Computation time: 1.4649455547332764\n",
      "Step: 1250, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 1.0996973514556885\n",
      "Step: 1251, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 0.8934428691864014\n",
      "########################\n",
      "Test loss: 1.0708413124084473, Test Accuracy_epoch9: 0.7697973251342773\n",
      "########################\n",
      "Step: 1252, Loss: 0.9162817597389221, Accuracy: 1.0, Computation time: 1.6381340026855469\n",
      "Step: 1253, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.1560218334197998\n",
      "Step: 1254, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.118687391281128\n",
      "Step: 1255, Loss: 0.9389638304710388, Accuracy: 0.9722222089767456, Computation time: 1.4195373058319092\n",
      "Step: 1256, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 1.0544798374176025\n",
      "Step: 1257, Loss: 0.9161074161529541, Accuracy: 1.0, Computation time: 1.478724718093872\n",
      "Step: 1258, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.4698433876037598\n",
      "Step: 1259, Loss: 0.9160515666007996, Accuracy: 1.0, Computation time: 1.2627756595611572\n",
      "Step: 1260, Loss: 0.9159291982650757, Accuracy: 1.0, Computation time: 0.9734499454498291\n",
      "Step: 1261, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.3064143657684326\n",
      "Step: 1262, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 0.9942440986633301\n",
      "Step: 1263, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.2828588485717773\n",
      "Step: 1264, Loss: 0.9160940051078796, Accuracy: 1.0, Computation time: 1.3754608631134033\n",
      "Step: 1265, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.2256083488464355\n",
      "Step: 1266, Loss: 0.9365140199661255, Accuracy: 0.9722222089767456, Computation time: 1.5201878547668457\n",
      "Step: 1267, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.2683579921722412\n",
      "Step: 1268, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.1065595149993896\n",
      "Step: 1269, Loss: 0.9159179329872131, Accuracy: 1.0, Computation time: 1.0940430164337158\n",
      "Step: 1270, Loss: 0.9159482717514038, Accuracy: 1.0, Computation time: 1.1899604797363281\n",
      "Step: 1271, Loss: 0.9366542100906372, Accuracy: 0.9821428656578064, Computation time: 1.32228684425354\n",
      "Step: 1272, Loss: 0.9158712029457092, Accuracy: 1.0, Computation time: 1.1652271747589111\n",
      "Step: 1273, Loss: 0.9163265824317932, Accuracy: 1.0, Computation time: 1.7070202827453613\n",
      "Step: 1274, Loss: 0.9363813996315002, Accuracy: 0.9750000238418579, Computation time: 1.2395949363708496\n",
      "Step: 1275, Loss: 0.9160038828849792, Accuracy: 1.0, Computation time: 1.447455883026123\n",
      "Step: 1276, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.3283863067626953\n",
      "Step: 1277, Loss: 0.9374383687973022, Accuracy: 0.9791666865348816, Computation time: 1.4328088760375977\n",
      "Step: 1278, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.6465296745300293\n",
      "Step: 1279, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.37752103805542\n",
      "Step: 1280, Loss: 0.9161171317100525, Accuracy: 1.0, Computation time: 1.9833204746246338\n",
      "Step: 1281, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.3573076725006104\n",
      "Step: 1282, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.2135629653930664\n",
      "Step: 1283, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.2114827632904053\n",
      "Step: 1284, Loss: 0.9376540184020996, Accuracy: 0.9750000238418579, Computation time: 1.1101651191711426\n",
      "Step: 1285, Loss: 0.9375634789466858, Accuracy: 0.9583333730697632, Computation time: 1.4727370738983154\n",
      "Step: 1286, Loss: 0.935331404209137, Accuracy: 0.9722222089767456, Computation time: 1.2636964321136475\n",
      "Step: 1287, Loss: 0.9297770261764526, Accuracy: 0.9772727489471436, Computation time: 2.0342373847961426\n",
      "Step: 1288, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.2120981216430664\n",
      "Step: 1289, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.1246535778045654\n",
      "Step: 1290, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 1.289881706237793\n",
      "Step: 1291, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.3422231674194336\n",
      "Step: 1292, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.2617154121398926\n",
      "Step: 1293, Loss: 0.9192726016044617, Accuracy: 1.0, Computation time: 1.4431157112121582\n",
      "Step: 1294, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.4350857734680176\n",
      "Step: 1295, Loss: 0.9170287847518921, Accuracy: 1.0, Computation time: 1.478515386581421\n",
      "Step: 1296, Loss: 0.9348516464233398, Accuracy: 0.9642857313156128, Computation time: 1.610525131225586\n",
      "Step: 1297, Loss: 0.9163352847099304, Accuracy: 1.0, Computation time: 1.2677085399627686\n",
      "Step: 1298, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 1.3190829753875732\n",
      "Step: 1299, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.2962944507598877\n",
      "Step: 1300, Loss: 0.9159751534461975, Accuracy: 1.0, Computation time: 1.6544866561889648\n",
      "Step: 1301, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 1.5075526237487793\n",
      "Step: 1302, Loss: 0.9165047407150269, Accuracy: 1.0, Computation time: 1.8177320957183838\n",
      "Step: 1303, Loss: 0.9359144568443298, Accuracy: 0.9642857313156128, Computation time: 1.7501373291015625\n",
      "Step: 1304, Loss: 0.9159342646598816, Accuracy: 1.0, Computation time: 1.247948408126831\n",
      "Step: 1305, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.1354398727416992\n",
      "Step: 1306, Loss: 0.94954913854599, Accuracy: 0.9147727489471436, Computation time: 1.442136526107788\n",
      "Step: 1307, Loss: 0.9159730076789856, Accuracy: 1.0, Computation time: 1.1326770782470703\n",
      "Step: 1308, Loss: 0.9161278605461121, Accuracy: 1.0, Computation time: 1.525123119354248\n",
      "Step: 1309, Loss: 0.9378624558448792, Accuracy: 0.9583333730697632, Computation time: 1.2463552951812744\n",
      "Step: 1310, Loss: 0.9167903661727905, Accuracy: 1.0, Computation time: 1.3833317756652832\n",
      "Step: 1311, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.2749557495117188\n",
      "Step: 1312, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 1.1792387962341309\n",
      "Step: 1313, Loss: 0.9311937689781189, Accuracy: 0.9821428656578064, Computation time: 1.698969841003418\n",
      "Step: 1314, Loss: 0.9159356355667114, Accuracy: 1.0, Computation time: 1.2550809383392334\n",
      "Step: 1315, Loss: 0.9321972727775574, Accuracy: 0.9722222089767456, Computation time: 1.1804883480072021\n",
      "Step: 1316, Loss: 0.9160727858543396, Accuracy: 1.0, Computation time: 1.2943766117095947\n",
      "Step: 1317, Loss: 0.9159831404685974, Accuracy: 1.0, Computation time: 1.202815055847168\n",
      "Step: 1318, Loss: 0.916057825088501, Accuracy: 1.0, Computation time: 1.1217753887176514\n",
      "Step: 1319, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 1.331521987915039\n",
      "Step: 1320, Loss: 0.9166238903999329, Accuracy: 1.0, Computation time: 1.303638219833374\n",
      "Step: 1321, Loss: 0.9164500832557678, Accuracy: 1.0, Computation time: 1.5134620666503906\n",
      "Step: 1322, Loss: 0.9164326786994934, Accuracy: 1.0, Computation time: 1.5024592876434326\n",
      "Step: 1323, Loss: 0.9159513115882874, Accuracy: 1.0, Computation time: 1.0818824768066406\n",
      "Step: 1324, Loss: 0.9594423770904541, Accuracy: 0.9500000476837158, Computation time: 1.3573591709136963\n",
      "Step: 1325, Loss: 0.9191476702690125, Accuracy: 1.0, Computation time: 2.209223985671997\n",
      "Step: 1326, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.3218297958374023\n",
      "Step: 1327, Loss: 0.9185935258865356, Accuracy: 1.0, Computation time: 1.409515380859375\n",
      "Step: 1328, Loss: 0.9181441068649292, Accuracy: 1.0, Computation time: 1.1623477935791016\n",
      "Step: 1329, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.3185968399047852\n",
      "Step: 1330, Loss: 0.9377401471138, Accuracy: 0.9722222089767456, Computation time: 1.3560335636138916\n",
      "Step: 1331, Loss: 0.9159500002861023, Accuracy: 1.0, Computation time: 1.1034040451049805\n",
      "Step: 1332, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.2375376224517822\n",
      "Step: 1333, Loss: 0.9373306632041931, Accuracy: 0.9772727489471436, Computation time: 1.2384235858917236\n",
      "Step: 1334, Loss: 0.9376037120819092, Accuracy: 0.9750000238418579, Computation time: 0.8639407157897949\n",
      "Step: 1335, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 1.4039921760559082\n",
      "Step: 1336, Loss: 0.9159406423568726, Accuracy: 1.0, Computation time: 1.309455394744873\n",
      "Step: 1337, Loss: 0.9159500598907471, Accuracy: 1.0, Computation time: 1.1596477031707764\n",
      "Step: 1338, Loss: 0.9160661101341248, Accuracy: 1.0, Computation time: 1.2712948322296143\n",
      "Step: 1339, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.3751676082611084\n",
      "Step: 1340, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.1037251949310303\n",
      "Step: 1341, Loss: 0.9160124659538269, Accuracy: 1.0, Computation time: 1.5627245903015137\n",
      "Step: 1342, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.4265615940093994\n",
      "Step: 1343, Loss: 0.937507152557373, Accuracy: 0.9772727489471436, Computation time: 1.0658626556396484\n",
      "Step: 1344, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.2946491241455078\n",
      "Step: 1345, Loss: 0.9175230264663696, Accuracy: 1.0, Computation time: 1.2794830799102783\n",
      "Step: 1346, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.1206376552581787\n",
      "Step: 1347, Loss: 0.9163554906845093, Accuracy: 1.0, Computation time: 1.3256735801696777\n",
      "Step: 1348, Loss: 0.9503430128097534, Accuracy: 0.949999988079071, Computation time: 1.181847333908081\n",
      "Step: 1349, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 1.0996782779693604\n",
      "Step: 1350, Loss: 0.9159788489341736, Accuracy: 1.0, Computation time: 1.6119534969329834\n",
      "Step: 1351, Loss: 0.9286065101623535, Accuracy: 0.9750000238418579, Computation time: 1.6064660549163818\n",
      "Step: 1352, Loss: 0.9375971555709839, Accuracy: 0.96875, Computation time: 1.4558377265930176\n",
      "Step: 1353, Loss: 0.921074390411377, Accuracy: 1.0, Computation time: 1.1194822788238525\n",
      "Step: 1354, Loss: 0.9160133600234985, Accuracy: 1.0, Computation time: 1.1433231830596924\n",
      "Step: 1355, Loss: 0.9160557389259338, Accuracy: 1.0, Computation time: 1.3230469226837158\n",
      "Step: 1356, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 1.1833202838897705\n",
      "Step: 1357, Loss: 0.9160309433937073, Accuracy: 1.0, Computation time: 1.2802751064300537\n",
      "Step: 1358, Loss: 0.9167497754096985, Accuracy: 1.0, Computation time: 1.4858465194702148\n",
      "Step: 1359, Loss: 0.9161194562911987, Accuracy: 1.0, Computation time: 1.3484587669372559\n",
      "Step: 1360, Loss: 0.9160803556442261, Accuracy: 1.0, Computation time: 1.1310529708862305\n",
      "Step: 1361, Loss: 0.9160340428352356, Accuracy: 1.0, Computation time: 1.33310866355896\n",
      "Step: 1362, Loss: 0.9322001338005066, Accuracy: 0.9722222089767456, Computation time: 1.5733191967010498\n",
      "Step: 1363, Loss: 0.9169630408287048, Accuracy: 1.0, Computation time: 1.0548830032348633\n",
      "Step: 1364, Loss: 0.9159343838691711, Accuracy: 1.0, Computation time: 1.4505257606506348\n",
      "Step: 1365, Loss: 0.916181743144989, Accuracy: 1.0, Computation time: 1.6187632083892822\n",
      "Step: 1366, Loss: 0.9160271883010864, Accuracy: 1.0, Computation time: 1.3010637760162354\n",
      "Step: 1367, Loss: 0.9162052273750305, Accuracy: 1.0, Computation time: 1.1442162990570068\n",
      "Step: 1368, Loss: 0.9159504175186157, Accuracy: 1.0, Computation time: 1.2525856494903564\n",
      "Step: 1369, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 1.5896310806274414\n",
      "Step: 1370, Loss: 0.9160171151161194, Accuracy: 1.0, Computation time: 1.2907993793487549\n",
      "Step: 1371, Loss: 0.9159668684005737, Accuracy: 1.0, Computation time: 1.474691390991211\n",
      "Step: 1372, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 1.1669647693634033\n",
      "Step: 1373, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 1.099419116973877\n",
      "Step: 1374, Loss: 0.9371799230575562, Accuracy: 0.9791666865348816, Computation time: 1.2534470558166504\n",
      "Step: 1375, Loss: 0.9366409182548523, Accuracy: 0.9807692766189575, Computation time: 1.0864391326904297\n",
      "Step: 1376, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.248932123184204\n",
      "Step: 1377, Loss: 0.9195610284805298, Accuracy: 1.0, Computation time: 1.5298078060150146\n",
      "Step: 1378, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 1.42452073097229\n",
      "Step: 1379, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 1.1110138893127441\n",
      "Step: 1380, Loss: 0.9186012744903564, Accuracy: 1.0, Computation time: 1.4668550491333008\n",
      "Step: 1381, Loss: 0.916342556476593, Accuracy: 1.0, Computation time: 1.2784578800201416\n",
      "Step: 1382, Loss: 0.9195897579193115, Accuracy: 1.0, Computation time: 1.1370091438293457\n",
      "Step: 1383, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.4267454147338867\n",
      "Step: 1384, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.2250292301177979\n",
      "Step: 1385, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.4623103141784668\n",
      "Step: 1386, Loss: 0.9159802794456482, Accuracy: 1.0, Computation time: 1.6335439682006836\n",
      "Step: 1387, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 1.0651257038116455\n",
      "Step: 1388, Loss: 0.9159786701202393, Accuracy: 1.0, Computation time: 1.1331052780151367\n",
      "Step: 1389, Loss: 0.9159326553344727, Accuracy: 1.0, Computation time: 1.6281511783599854\n",
      "Step: 1390, Loss: 0.9376211762428284, Accuracy: 0.9722222089767456, Computation time: 1.1684484481811523\n",
      "########################\n",
      "Test loss: 1.0715892314910889, Test Accuracy_epoch10: 0.7694031000137329\n",
      "########################\n",
      "Step: 1391, Loss: 0.9261289834976196, Accuracy: 0.9791666865348816, Computation time: 1.1230640411376953\n",
      "Step: 1392, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.4423305988311768\n",
      "Step: 1393, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.2313435077667236\n",
      "Step: 1394, Loss: 0.9161069393157959, Accuracy: 1.0, Computation time: 1.325735330581665\n",
      "Step: 1395, Loss: 0.9165143370628357, Accuracy: 1.0, Computation time: 1.2402312755584717\n",
      "Step: 1396, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 1.1540286540985107\n",
      "Step: 1397, Loss: 0.9166852831840515, Accuracy: 1.0, Computation time: 1.436856985092163\n",
      "Step: 1398, Loss: 0.9163826704025269, Accuracy: 1.0, Computation time: 1.287825584411621\n",
      "Step: 1399, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 1.4773049354553223\n",
      "Step: 1400, Loss: 0.9376291632652283, Accuracy: 0.96875, Computation time: 1.372692346572876\n",
      "Step: 1401, Loss: 0.9366119503974915, Accuracy: 0.9750000238418579, Computation time: 1.644869089126587\n",
      "Step: 1402, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 1.0284171104431152\n",
      "Step: 1403, Loss: 0.9159650802612305, Accuracy: 1.0, Computation time: 1.1965136528015137\n",
      "Step: 1404, Loss: 0.9160488247871399, Accuracy: 1.0, Computation time: 1.1575868129730225\n",
      "Step: 1405, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.076263189315796\n",
      "Step: 1406, Loss: 0.9240037202835083, Accuracy: 1.0, Computation time: 1.6335852146148682\n",
      "Step: 1407, Loss: 0.9159306287765503, Accuracy: 1.0, Computation time: 1.4118351936340332\n",
      "Step: 1408, Loss: 0.9159632325172424, Accuracy: 1.0, Computation time: 1.0989537239074707\n",
      "Step: 1409, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 1.2062220573425293\n",
      "Step: 1410, Loss: 0.9221739172935486, Accuracy: 1.0, Computation time: 1.2456719875335693\n",
      "Step: 1411, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.1563642024993896\n",
      "Step: 1412, Loss: 0.9161940813064575, Accuracy: 1.0, Computation time: 1.2959613800048828\n",
      "Step: 1413, Loss: 0.9161977171897888, Accuracy: 1.0, Computation time: 1.4672470092773438\n",
      "Step: 1414, Loss: 0.9159368872642517, Accuracy: 1.0, Computation time: 1.4933900833129883\n",
      "Step: 1415, Loss: 0.9167807698249817, Accuracy: 1.0, Computation time: 1.0993096828460693\n",
      "Step: 1416, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 0.967017650604248\n",
      "Step: 1417, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.0993056297302246\n",
      "Step: 1418, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.3309004306793213\n",
      "Step: 1419, Loss: 0.9376179575920105, Accuracy: 0.9791666865348816, Computation time: 1.1610212326049805\n",
      "Step: 1420, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.3194026947021484\n",
      "Step: 1421, Loss: 0.9160638451576233, Accuracy: 1.0, Computation time: 1.0930330753326416\n",
      "Step: 1422, Loss: 0.9242331981658936, Accuracy: 1.0, Computation time: 1.346236228942871\n",
      "Step: 1423, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 1.3027656078338623\n",
      "Step: 1424, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.4823732376098633\n",
      "Step: 1425, Loss: 0.9161257743835449, Accuracy: 1.0, Computation time: 1.3299527168273926\n",
      "Step: 1426, Loss: 0.9376057386398315, Accuracy: 0.9750000238418579, Computation time: 1.3585443496704102\n",
      "Step: 1427, Loss: 0.9235741496086121, Accuracy: 1.0, Computation time: 1.7272517681121826\n",
      "Step: 1428, Loss: 0.916218101978302, Accuracy: 1.0, Computation time: 1.4917097091674805\n",
      "Step: 1429, Loss: 0.9163840413093567, Accuracy: 1.0, Computation time: 1.7573518753051758\n",
      "Step: 1430, Loss: 0.9159116744995117, Accuracy: 1.0, Computation time: 0.9791181087493896\n",
      "Step: 1431, Loss: 0.9159231185913086, Accuracy: 1.0, Computation time: 1.18442702293396\n",
      "Step: 1432, Loss: 0.9159428477287292, Accuracy: nan, Computation time: 1.4003653526306152\n",
      "Step: 1433, Loss: 0.9164358973503113, Accuracy: 1.0, Computation time: 1.677206039428711\n",
      "Step: 1434, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 1.1730029582977295\n",
      "Step: 1435, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.2053635120391846\n",
      "Step: 1436, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.2957499027252197\n",
      "Step: 1437, Loss: 0.9166491031646729, Accuracy: 1.0, Computation time: 1.4613137245178223\n",
      "Step: 1438, Loss: 0.9159782528877258, Accuracy: 1.0, Computation time: 1.4407660961151123\n",
      "Step: 1439, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.2684071063995361\n",
      "Step: 1440, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.5536959171295166\n",
      "Step: 1441, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.2568728923797607\n",
      "Step: 1442, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.6104776859283447\n",
      "Step: 1443, Loss: 0.9168979525566101, Accuracy: 1.0, Computation time: 1.5416984558105469\n",
      "Step: 1444, Loss: 0.9160258769989014, Accuracy: 1.0, Computation time: 1.104654312133789\n",
      "Step: 1445, Loss: 0.91592937707901, Accuracy: 1.0, Computation time: 1.4487559795379639\n",
      "Step: 1446, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.023665189743042\n",
      "Step: 1447, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.3358900547027588\n",
      "Step: 1448, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 1.3356757164001465\n",
      "Step: 1449, Loss: 0.9172554612159729, Accuracy: 1.0, Computation time: 1.704599142074585\n",
      "Step: 1450, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.5003464221954346\n",
      "Step: 1451, Loss: 0.9366351366043091, Accuracy: 0.9722222089767456, Computation time: 1.3357608318328857\n",
      "Step: 1452, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.473567247390747\n",
      "Step: 1453, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 1.4621622562408447\n",
      "Step: 1454, Loss: 0.9374555349349976, Accuracy: 0.9583333730697632, Computation time: 1.4291505813598633\n",
      "Step: 1455, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.1098508834838867\n",
      "Step: 1456, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.09682035446167\n",
      "Step: 1457, Loss: 0.9160707592964172, Accuracy: 1.0, Computation time: 1.3278453350067139\n",
      "Step: 1458, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.5219347476959229\n",
      "Step: 1459, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.751199722290039\n",
      "Step: 1460, Loss: 0.9367779493331909, Accuracy: 0.9772727489471436, Computation time: 1.3150341510772705\n",
      "Step: 1461, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.2908599376678467\n",
      "Step: 1462, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.5703513622283936\n",
      "Step: 1463, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 1.3189797401428223\n",
      "Step: 1464, Loss: 0.9166878461837769, Accuracy: 1.0, Computation time: 1.2997074127197266\n",
      "Step: 1465, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.2808899879455566\n",
      "Step: 1466, Loss: 0.9159447550773621, Accuracy: 1.0, Computation time: 1.9776520729064941\n",
      "Step: 1467, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 0.9754211902618408\n",
      "Step: 1468, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 1.417405366897583\n",
      "Step: 1469, Loss: 0.9159104228019714, Accuracy: 1.0, Computation time: 1.2053782939910889\n",
      "Step: 1470, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 1.348371982574463\n",
      "Step: 1471, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.691765308380127\n",
      "Step: 1472, Loss: 0.9378228187561035, Accuracy: 0.9722222089767456, Computation time: 1.4750709533691406\n",
      "Step: 1473, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.3324322700500488\n",
      "Step: 1474, Loss: 0.9160300493240356, Accuracy: 1.0, Computation time: 1.3508315086364746\n",
      "Step: 1475, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.4008233547210693\n",
      "Step: 1476, Loss: 0.9159867763519287, Accuracy: 1.0, Computation time: 1.3012022972106934\n",
      "Step: 1477, Loss: 0.9673537015914917, Accuracy: 0.9375, Computation time: 1.5299265384674072\n",
      "Step: 1478, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.1910991668701172\n",
      "Step: 1479, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.424485206604004\n",
      "Step: 1480, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.146885633468628\n",
      "Step: 1481, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.2637906074523926\n",
      "Step: 1482, Loss: 0.9173848628997803, Accuracy: 1.0, Computation time: 1.787273645401001\n",
      "Step: 1483, Loss: 0.9375686645507812, Accuracy: 0.9791666865348816, Computation time: 1.4385349750518799\n",
      "Step: 1484, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.7558977603912354\n",
      "Step: 1485, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.4011383056640625\n",
      "Step: 1486, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 1.224097490310669\n",
      "Step: 1487, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.7272725105285645\n",
      "Step: 1488, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.2855198383331299\n",
      "Step: 1489, Loss: 0.9364375472068787, Accuracy: 0.9772727489471436, Computation time: 1.3037314414978027\n",
      "Step: 1490, Loss: 0.9185928702354431, Accuracy: 1.0, Computation time: 1.6223533153533936\n",
      "Step: 1491, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.297163963317871\n",
      "Step: 1492, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.2578401565551758\n",
      "Step: 1493, Loss: 0.9306786060333252, Accuracy: 0.96875, Computation time: 2.0557374954223633\n",
      "Step: 1494, Loss: 0.9160287976264954, Accuracy: 1.0, Computation time: 1.4492228031158447\n",
      "Step: 1495, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.5095009803771973\n",
      "Step: 1496, Loss: 0.915956974029541, Accuracy: 1.0, Computation time: 1.1576910018920898\n",
      "Step: 1497, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.1661977767944336\n",
      "Step: 1498, Loss: 0.937732458114624, Accuracy: 0.9772727489471436, Computation time: 1.3632001876831055\n",
      "Step: 1499, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 1.37363600730896\n",
      "Step: 1500, Loss: 0.9159553050994873, Accuracy: 1.0, Computation time: 1.166130781173706\n",
      "Step: 1501, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 1.3268146514892578\n",
      "Step: 1502, Loss: 0.9161620140075684, Accuracy: 1.0, Computation time: 1.1880671977996826\n",
      "Step: 1503, Loss: 0.916008472442627, Accuracy: 1.0, Computation time: 1.3479561805725098\n",
      "Step: 1504, Loss: 0.9168224930763245, Accuracy: 1.0, Computation time: 1.2262599468231201\n",
      "Step: 1505, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 1.0715956687927246\n",
      "Step: 1506, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.0537736415863037\n",
      "Step: 1507, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.138448715209961\n",
      "Step: 1508, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 1.1084527969360352\n",
      "Step: 1509, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 1.4881348609924316\n",
      "Step: 1510, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.2312939167022705\n",
      "Step: 1511, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.1193957328796387\n",
      "Step: 1512, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.2393405437469482\n",
      "Step: 1513, Loss: 0.9159244894981384, Accuracy: 1.0, Computation time: 1.1826660633087158\n",
      "Step: 1514, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.3109924793243408\n",
      "Step: 1515, Loss: 0.9374868869781494, Accuracy: 0.9791666865348816, Computation time: 1.207430124282837\n",
      "Step: 1516, Loss: 0.9286994338035583, Accuracy: 0.96875, Computation time: 1.5070834159851074\n",
      "Step: 1517, Loss: 0.9550442695617676, Accuracy: 0.9285714626312256, Computation time: 1.5611815452575684\n",
      "Step: 1518, Loss: 0.9366249442100525, Accuracy: 0.9807692766189575, Computation time: 1.5838241577148438\n",
      "Step: 1519, Loss: 0.935624361038208, Accuracy: 0.9722222089767456, Computation time: 1.0083632469177246\n",
      "Step: 1520, Loss: 0.9368146657943726, Accuracy: 0.9722222089767456, Computation time: 1.2104616165161133\n",
      "Step: 1521, Loss: 0.9377647042274475, Accuracy: 0.9791666865348816, Computation time: 1.1218948364257812\n",
      "Step: 1522, Loss: 0.916670560836792, Accuracy: 1.0, Computation time: 1.6020293235778809\n",
      "Step: 1523, Loss: 0.9160104990005493, Accuracy: 1.0, Computation time: 1.1494109630584717\n",
      "Step: 1524, Loss: 0.9160762429237366, Accuracy: 1.0, Computation time: 1.1271884441375732\n",
      "Step: 1525, Loss: 0.9159499406814575, Accuracy: 1.0, Computation time: 1.1988706588745117\n",
      "Step: 1526, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.1300318241119385\n",
      "Step: 1527, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 1.0844972133636475\n",
      "Step: 1528, Loss: 0.9376001954078674, Accuracy: 0.9833333492279053, Computation time: 1.1076717376708984\n",
      "Step: 1529, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.203167200088501\n",
      "########################\n",
      "Test loss: 1.0718852281570435, Test Accuracy_epoch11: 0.7685779929161072\n",
      "########################\n",
      "Step: 1530, Loss: 0.9159873723983765, Accuracy: 1.0, Computation time: 1.530379056930542\n",
      "Step: 1531, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 1.2412707805633545\n",
      "Step: 1532, Loss: 0.9159082770347595, Accuracy: 1.0, Computation time: 1.210909366607666\n",
      "Step: 1533, Loss: 0.9373344779014587, Accuracy: 0.9807692766189575, Computation time: 1.1315019130706787\n",
      "Step: 1534, Loss: 0.9161669611930847, Accuracy: 1.0, Computation time: 1.377479076385498\n",
      "Step: 1535, Loss: 0.9160463809967041, Accuracy: 1.0, Computation time: 1.1652700901031494\n",
      "Step: 1536, Loss: 0.9376406073570251, Accuracy: 0.9722222089767456, Computation time: 1.0794987678527832\n",
      "Step: 1537, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.255687952041626\n",
      "Step: 1538, Loss: 0.9368215203285217, Accuracy: 0.9642857313156128, Computation time: 1.4047396183013916\n",
      "Step: 1539, Loss: 0.9160204529762268, Accuracy: 1.0, Computation time: 1.0644910335540771\n",
      "Step: 1540, Loss: 0.9159823656082153, Accuracy: 1.0, Computation time: 0.9811286926269531\n",
      "Step: 1541, Loss: 0.9336298108100891, Accuracy: 0.9750000238418579, Computation time: 1.805649757385254\n",
      "Step: 1542, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.2722549438476562\n",
      "Step: 1543, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.30479097366333\n",
      "Step: 1544, Loss: 0.935723602771759, Accuracy: 0.9750000238418579, Computation time: 1.3730180263519287\n",
      "Step: 1545, Loss: 0.9376164078712463, Accuracy: 0.949999988079071, Computation time: 1.1806294918060303\n",
      "Step: 1546, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.395073413848877\n",
      "Step: 1547, Loss: 0.9160000085830688, Accuracy: 1.0, Computation time: 2.0909976959228516\n",
      "Step: 1548, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 1.0804381370544434\n",
      "Step: 1549, Loss: 0.9373796582221985, Accuracy: 0.9583333730697632, Computation time: 1.1946690082550049\n",
      "Step: 1550, Loss: 0.9193178415298462, Accuracy: 1.0, Computation time: 1.0977766513824463\n",
      "Step: 1551, Loss: 0.9160982370376587, Accuracy: 1.0, Computation time: 1.2533915042877197\n",
      "Step: 1552, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 1.1316077709197998\n",
      "Step: 1553, Loss: 0.9367486834526062, Accuracy: 0.9821428656578064, Computation time: 1.240736961364746\n",
      "Step: 1554, Loss: 0.9162505865097046, Accuracy: 1.0, Computation time: 1.2076189517974854\n",
      "Step: 1555, Loss: 0.9159901142120361, Accuracy: 1.0, Computation time: 1.2313323020935059\n",
      "Step: 1556, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.262730360031128\n",
      "Step: 1557, Loss: 0.9162561297416687, Accuracy: 1.0, Computation time: 1.3819358348846436\n",
      "Step: 1558, Loss: 0.9374501705169678, Accuracy: 0.9583333730697632, Computation time: 1.527371883392334\n",
      "Step: 1559, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.2236206531524658\n",
      "Step: 1560, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 1.1657814979553223\n",
      "Step: 1561, Loss: 0.9248815774917603, Accuracy: 1.0, Computation time: 1.2491965293884277\n",
      "Step: 1562, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.1695764064788818\n",
      "Step: 1563, Loss: 0.9168223738670349, Accuracy: 1.0, Computation time: 2.0618462562561035\n",
      "Step: 1564, Loss: 0.918084442615509, Accuracy: 1.0, Computation time: 1.560689926147461\n",
      "Step: 1565, Loss: 0.9375627040863037, Accuracy: 0.9807692766189575, Computation time: 1.1938393115997314\n",
      "Step: 1566, Loss: 0.9172865152359009, Accuracy: 1.0, Computation time: 1.3186924457550049\n",
      "Step: 1567, Loss: 0.9172369837760925, Accuracy: 1.0, Computation time: 1.5627720355987549\n",
      "Step: 1568, Loss: 0.938400387763977, Accuracy: 0.9791666865348816, Computation time: 1.2954962253570557\n",
      "Step: 1569, Loss: 0.9160059690475464, Accuracy: 1.0, Computation time: 1.316586971282959\n",
      "Step: 1570, Loss: 0.9374712705612183, Accuracy: 0.9583333730697632, Computation time: 1.700425386428833\n",
      "Step: 1571, Loss: 0.9160200953483582, Accuracy: 1.0, Computation time: 1.3949899673461914\n",
      "Step: 1572, Loss: 0.9160587787628174, Accuracy: 1.0, Computation time: 1.3245761394500732\n",
      "Step: 1573, Loss: 0.9159843325614929, Accuracy: 1.0, Computation time: 1.2162933349609375\n",
      "Step: 1574, Loss: 0.9159393906593323, Accuracy: 1.0, Computation time: 1.3440310955047607\n",
      "Step: 1575, Loss: 0.9161440134048462, Accuracy: 1.0, Computation time: 1.4403576850891113\n",
      "Step: 1576, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 1.9013729095458984\n",
      "Step: 1577, Loss: 0.9376732110977173, Accuracy: 0.9722222089767456, Computation time: 1.2634460926055908\n",
      "Step: 1578, Loss: 0.9161799550056458, Accuracy: 1.0, Computation time: 1.2052710056304932\n",
      "Step: 1579, Loss: 0.9374886155128479, Accuracy: 0.96875, Computation time: 1.3096988201141357\n",
      "Step: 1580, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.2197284698486328\n",
      "Step: 1581, Loss: 0.9360352754592896, Accuracy: 0.9722222089767456, Computation time: 1.3254435062408447\n",
      "Step: 1582, Loss: 0.9310719966888428, Accuracy: 0.9166666865348816, Computation time: 1.7242367267608643\n",
      "Step: 1583, Loss: 0.9160060882568359, Accuracy: 1.0, Computation time: 1.3757131099700928\n",
      "Step: 1584, Loss: 0.934877336025238, Accuracy: 0.96875, Computation time: 1.3645904064178467\n",
      "Step: 1585, Loss: 0.9571061730384827, Accuracy: 0.9495192766189575, Computation time: 1.3952021598815918\n",
      "Step: 1586, Loss: 0.9159652590751648, Accuracy: 1.0, Computation time: 1.2042820453643799\n",
      "Step: 1587, Loss: 0.9160993099212646, Accuracy: 1.0, Computation time: 1.5735726356506348\n",
      "Step: 1588, Loss: 0.9160897135734558, Accuracy: 1.0, Computation time: 1.35555100440979\n",
      "Step: 1589, Loss: 0.916107177734375, Accuracy: 1.0, Computation time: 1.0405550003051758\n",
      "Step: 1590, Loss: 0.9161030650138855, Accuracy: 1.0, Computation time: 1.13262939453125\n",
      "Step: 1591, Loss: 0.915938138961792, Accuracy: 1.0, Computation time: 1.1551642417907715\n",
      "Step: 1592, Loss: 0.9160442352294922, Accuracy: 1.0, Computation time: 1.4939889907836914\n",
      "Step: 1593, Loss: 0.9159902334213257, Accuracy: 1.0, Computation time: 1.4761097431182861\n",
      "Step: 1594, Loss: 0.9159644246101379, Accuracy: 1.0, Computation time: 1.367398977279663\n",
      "Step: 1595, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.3154122829437256\n",
      "Step: 1596, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.2930943965911865\n",
      "Step: 1597, Loss: 0.9378750324249268, Accuracy: 0.9807692766189575, Computation time: 1.2548761367797852\n",
      "Step: 1598, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 1.2402632236480713\n",
      "Step: 1599, Loss: 0.9160928130149841, Accuracy: 1.0, Computation time: 2.8841540813446045\n",
      "Step: 1600, Loss: 0.9166171550750732, Accuracy: 1.0, Computation time: 2.226179838180542\n",
      "Step: 1601, Loss: 0.9159630537033081, Accuracy: 1.0, Computation time: 2.2471847534179688\n",
      "Step: 1602, Loss: 0.9159699082374573, Accuracy: 1.0, Computation time: 1.8138175010681152\n",
      "Step: 1603, Loss: 0.916732907295227, Accuracy: 1.0, Computation time: 2.197793960571289\n",
      "Step: 1604, Loss: 0.9159852266311646, Accuracy: 1.0, Computation time: 1.7597692012786865\n",
      "Step: 1605, Loss: 0.9159555435180664, Accuracy: 1.0, Computation time: 2.2249066829681396\n",
      "Step: 1606, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.6821224689483643\n",
      "Step: 1607, Loss: 0.9159101247787476, Accuracy: 1.0, Computation time: 1.8688428401947021\n",
      "Step: 1608, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 2.1755807399749756\n",
      "Step: 1609, Loss: 0.9196497797966003, Accuracy: 1.0, Computation time: 2.1525044441223145\n",
      "Step: 1610, Loss: 0.917340099811554, Accuracy: 1.0, Computation time: 2.7277965545654297\n",
      "Step: 1611, Loss: 0.9160803556442261, Accuracy: 1.0, Computation time: 2.162646532058716\n",
      "Step: 1612, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 2.073315143585205\n",
      "Step: 1613, Loss: 0.9160428643226624, Accuracy: 1.0, Computation time: 2.58205509185791\n",
      "Step: 1614, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 1.983048439025879\n",
      "Step: 1615, Loss: 0.9206529259681702, Accuracy: 1.0, Computation time: 2.31895112991333\n",
      "Step: 1616, Loss: 0.9317011833190918, Accuracy: 0.96875, Computation time: 2.0630884170532227\n",
      "Step: 1617, Loss: 0.9287519454956055, Accuracy: 0.9166666865348816, Computation time: 2.32016921043396\n",
      "Step: 1618, Loss: 0.9159618020057678, Accuracy: 1.0, Computation time: 1.833770990371704\n",
      "Step: 1619, Loss: 0.9161434769630432, Accuracy: 1.0, Computation time: 1.7266318798065186\n",
      "Step: 1620, Loss: 0.9161898493766785, Accuracy: 1.0, Computation time: 1.8095238208770752\n",
      "Step: 1621, Loss: 0.925316572189331, Accuracy: 1.0, Computation time: 2.4266719818115234\n",
      "Step: 1622, Loss: 0.938048243522644, Accuracy: 0.984375, Computation time: 1.711108684539795\n",
      "Step: 1623, Loss: 0.916791558265686, Accuracy: 1.0, Computation time: 2.5318031311035156\n",
      "Step: 1624, Loss: 0.9161058068275452, Accuracy: 1.0, Computation time: 1.6441571712493896\n",
      "Step: 1625, Loss: 0.9160805940628052, Accuracy: 1.0, Computation time: 2.7652766704559326\n",
      "Step: 1626, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 2.039292097091675\n",
      "Step: 1627, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 1.7254576683044434\n",
      "Step: 1628, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.6551387310028076\n",
      "Step: 1629, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.8085646629333496\n",
      "Step: 1630, Loss: 0.9379528164863586, Accuracy: 0.9642857313156128, Computation time: 1.6396770477294922\n",
      "Step: 1631, Loss: 0.9169539213180542, Accuracy: 1.0, Computation time: 1.372121810913086\n",
      "Step: 1632, Loss: 0.9160450100898743, Accuracy: 1.0, Computation time: 1.1901254653930664\n",
      "Step: 1633, Loss: 0.9161485433578491, Accuracy: 1.0, Computation time: 1.2697157859802246\n",
      "Step: 1634, Loss: 0.916065514087677, Accuracy: 1.0, Computation time: 1.1303024291992188\n",
      "Step: 1635, Loss: 0.9160332083702087, Accuracy: 1.0, Computation time: 1.2031559944152832\n",
      "Step: 1636, Loss: 0.9160287976264954, Accuracy: 1.0, Computation time: 1.3297538757324219\n",
      "Step: 1637, Loss: 0.9160646796226501, Accuracy: 1.0, Computation time: 1.1218397617340088\n",
      "Step: 1638, Loss: 0.9159558415412903, Accuracy: 1.0, Computation time: 1.5164616107940674\n",
      "Step: 1639, Loss: 0.934203028678894, Accuracy: 0.9722222089767456, Computation time: 1.2832162380218506\n",
      "Step: 1640, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 1.3166944980621338\n",
      "Step: 1641, Loss: 0.9159278273582458, Accuracy: 1.0, Computation time: 0.9993538856506348\n",
      "Step: 1642, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.2898223400115967\n",
      "Step: 1643, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.382333517074585\n",
      "Step: 1644, Loss: 0.9175028800964355, Accuracy: 1.0, Computation time: 1.4098141193389893\n",
      "Step: 1645, Loss: 0.9162916541099548, Accuracy: 1.0, Computation time: 1.2559070587158203\n",
      "Step: 1646, Loss: 0.9313126802444458, Accuracy: 0.9750000238418579, Computation time: 1.5371754169464111\n",
      "Step: 1647, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.1697256565093994\n",
      "Step: 1648, Loss: 0.9221439957618713, Accuracy: 1.0, Computation time: 1.7787609100341797\n",
      "Step: 1649, Loss: 0.9166007041931152, Accuracy: 1.0, Computation time: 1.2945661544799805\n",
      "Step: 1650, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 1.0030975341796875\n",
      "Step: 1651, Loss: 0.9167239665985107, Accuracy: 1.0, Computation time: 1.3406047821044922\n",
      "Step: 1652, Loss: 0.9160293340682983, Accuracy: 1.0, Computation time: 1.1630263328552246\n",
      "Step: 1653, Loss: 0.936485767364502, Accuracy: 0.9583333730697632, Computation time: 1.8731434345245361\n",
      "Step: 1654, Loss: 0.9375525116920471, Accuracy: 0.9772727489471436, Computation time: 1.3105006217956543\n",
      "Step: 1655, Loss: 0.915997326374054, Accuracy: 1.0, Computation time: 1.2088053226470947\n",
      "Step: 1656, Loss: 0.9159460067749023, Accuracy: 1.0, Computation time: 1.1725409030914307\n",
      "Step: 1657, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 1.3976898193359375\n",
      "Step: 1658, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.4275546073913574\n",
      "Step: 1659, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.259784460067749\n",
      "Step: 1660, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 1.2368271350860596\n",
      "Step: 1661, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.5125758647918701\n",
      "Step: 1662, Loss: 0.9159707427024841, Accuracy: 1.0, Computation time: 1.1372382640838623\n",
      "Step: 1663, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.1877477169036865\n",
      "Step: 1664, Loss: 0.9346785545349121, Accuracy: 0.9791666865348816, Computation time: 1.1280007362365723\n",
      "Step: 1665, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.317662239074707\n",
      "Step: 1666, Loss: 0.9165154695510864, Accuracy: 1.0, Computation time: 1.1541030406951904\n",
      "Step: 1667, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 1.1982226371765137\n",
      "Step: 1668, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.0462651252746582\n",
      "########################\n",
      "Test loss: 1.0735512971878052, Test Accuracy_epoch12: 0.763250470161438\n",
      "########################\n",
      "Step: 1669, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.2965316772460938\n",
      "Step: 1670, Loss: 0.9162582755088806, Accuracy: 1.0, Computation time: 1.1742668151855469\n",
      "Step: 1671, Loss: 0.9159446358680725, Accuracy: 1.0, Computation time: 1.5926413536071777\n",
      "Step: 1672, Loss: 0.9371742010116577, Accuracy: 0.9750000238418579, Computation time: 1.4113471508026123\n",
      "Step: 1673, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.2373483180999756\n",
      "Step: 1674, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 1.2615222930908203\n",
      "Step: 1675, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.1065986156463623\n",
      "Step: 1676, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.1912522315979004\n",
      "Step: 1677, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 1.3711192607879639\n",
      "Step: 1678, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.3243942260742188\n",
      "Step: 1679, Loss: 0.9160597324371338, Accuracy: 1.0, Computation time: 1.293379783630371\n",
      "Step: 1680, Loss: 0.9232855439186096, Accuracy: 1.0, Computation time: 1.6498558521270752\n",
      "Step: 1681, Loss: 0.9162915945053101, Accuracy: 1.0, Computation time: 1.226210355758667\n",
      "Step: 1682, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.0716240406036377\n",
      "Step: 1683, Loss: 0.9161059856414795, Accuracy: 1.0, Computation time: 1.2255144119262695\n",
      "Step: 1684, Loss: 0.9160236120223999, Accuracy: 1.0, Computation time: 1.1895194053649902\n",
      "Step: 1685, Loss: 0.9159045815467834, Accuracy: 1.0, Computation time: 1.0617454051971436\n",
      "Step: 1686, Loss: 0.9159769415855408, Accuracy: 1.0, Computation time: 1.005913496017456\n",
      "Step: 1687, Loss: 0.9159612059593201, Accuracy: 1.0, Computation time: 1.175999641418457\n",
      "Step: 1688, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.1560051441192627\n",
      "Step: 1689, Loss: 0.9159152507781982, Accuracy: 1.0, Computation time: 1.275526762008667\n",
      "Step: 1690, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.1591012477874756\n",
      "Step: 1691, Loss: 0.9159348607063293, Accuracy: 1.0, Computation time: 1.2801833152770996\n",
      "Step: 1692, Loss: 0.9164919853210449, Accuracy: 1.0, Computation time: 1.2654521465301514\n",
      "Step: 1693, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.212930679321289\n",
      "Step: 1694, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.213719367980957\n",
      "Step: 1695, Loss: 0.9375545382499695, Accuracy: 0.9722222089767456, Computation time: 1.178473949432373\n",
      "Step: 1696, Loss: 0.937246561050415, Accuracy: 0.96875, Computation time: 1.355034589767456\n",
      "Step: 1697, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 1.098607063293457\n",
      "Step: 1698, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.286271333694458\n",
      "Step: 1699, Loss: 0.9168969988822937, Accuracy: 1.0, Computation time: 1.4334995746612549\n",
      "Step: 1700, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.3948233127593994\n",
      "Step: 1701, Loss: 0.9374604225158691, Accuracy: 0.9833333492279053, Computation time: 1.1721796989440918\n",
      "Step: 1702, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 1.4846558570861816\n",
      "Step: 1703, Loss: 0.9382480382919312, Accuracy: 0.9642857313156128, Computation time: 1.0586962699890137\n",
      "Step: 1704, Loss: 0.9374014735221863, Accuracy: 0.9772727489471436, Computation time: 1.5555121898651123\n",
      "Step: 1705, Loss: 0.9175665974617004, Accuracy: 1.0, Computation time: 1.6064729690551758\n",
      "Step: 1706, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 1.0731821060180664\n",
      "Step: 1707, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 0.9699273109436035\n",
      "Step: 1708, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.043717622756958\n",
      "Step: 1709, Loss: 0.9378535151481628, Accuracy: 0.9807692766189575, Computation time: 1.1766924858093262\n",
      "Step: 1710, Loss: 0.9375015497207642, Accuracy: 0.949999988079071, Computation time: 1.4975926876068115\n",
      "Step: 1711, Loss: 0.918619692325592, Accuracy: 1.0, Computation time: 1.2231988906860352\n",
      "Step: 1712, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 1.3240795135498047\n",
      "Step: 1713, Loss: 0.9375475645065308, Accuracy: 0.9750000238418579, Computation time: 1.2805819511413574\n",
      "Step: 1714, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 1.4306297302246094\n",
      "Step: 1715, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.030221700668335\n",
      "Step: 1716, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.5947551727294922\n",
      "Step: 1717, Loss: 0.9378736615180969, Accuracy: 0.9807692766189575, Computation time: 1.3009727001190186\n",
      "Step: 1718, Loss: 0.9159724116325378, Accuracy: 1.0, Computation time: 1.3066229820251465\n",
      "Step: 1719, Loss: 0.9375151991844177, Accuracy: 0.9722222089767456, Computation time: 1.2940175533294678\n",
      "Step: 1720, Loss: 0.9176653623580933, Accuracy: 1.0, Computation time: 1.2660131454467773\n",
      "Step: 1721, Loss: 0.9161427021026611, Accuracy: 1.0, Computation time: 1.1864659786224365\n",
      "Step: 1722, Loss: 0.9362637996673584, Accuracy: 0.984375, Computation time: 1.193593978881836\n",
      "Step: 1723, Loss: 0.9160008430480957, Accuracy: 1.0, Computation time: 1.1615421772003174\n",
      "Step: 1724, Loss: 0.915973424911499, Accuracy: 1.0, Computation time: 1.5336010456085205\n",
      "Step: 1725, Loss: 0.9159827828407288, Accuracy: 1.0, Computation time: 1.5733752250671387\n",
      "Step: 1726, Loss: 0.9159717559814453, Accuracy: 1.0, Computation time: 1.8261916637420654\n",
      "Step: 1727, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.1522412300109863\n",
      "Step: 1728, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.3414428234100342\n",
      "Step: 1729, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 1.8253517150878906\n",
      "Step: 1730, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.667980432510376\n",
      "Step: 1731, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.3255760669708252\n",
      "Step: 1732, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.5676398277282715\n",
      "Step: 1733, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.5773561000823975\n",
      "Step: 1734, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 1.8019800186157227\n",
      "Step: 1735, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.747528314590454\n",
      "Step: 1736, Loss: 0.9164333939552307, Accuracy: 1.0, Computation time: 1.8991549015045166\n",
      "Step: 1737, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.975883960723877\n",
      "Step: 1738, Loss: 0.9322553277015686, Accuracy: 0.9722222089767456, Computation time: 1.9217276573181152\n",
      "Step: 1739, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.5640382766723633\n",
      "Step: 1740, Loss: 0.917248547077179, Accuracy: 1.0, Computation time: 2.1244618892669678\n",
      "Step: 1741, Loss: 0.9375916123390198, Accuracy: 0.9642857313156128, Computation time: 1.725433349609375\n",
      "Step: 1742, Loss: 0.932330310344696, Accuracy: 0.9791666865348816, Computation time: 1.8223133087158203\n",
      "Step: 1743, Loss: 0.9185395836830139, Accuracy: 1.0, Computation time: 2.4775712490081787\n",
      "Step: 1744, Loss: 0.9159265756607056, Accuracy: 1.0, Computation time: 1.810858964920044\n",
      "Step: 1745, Loss: 0.9372122287750244, Accuracy: 0.9791666865348816, Computation time: 1.78493070602417\n",
      "Step: 1746, Loss: 0.9161974191665649, Accuracy: 1.0, Computation time: 2.1793932914733887\n",
      "Step: 1747, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 1.9025425910949707\n",
      "Step: 1748, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 2.0262227058410645\n",
      "Step: 1749, Loss: 0.9166443347930908, Accuracy: 1.0, Computation time: 2.1589372158050537\n",
      "Step: 1750, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 2.1798465251922607\n",
      "Step: 1751, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 1.8847465515136719\n",
      "Step: 1752, Loss: 0.9166733026504517, Accuracy: 1.0, Computation time: 2.57505202293396\n",
      "Step: 1753, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 2.150935173034668\n",
      "Step: 1754, Loss: 0.9162817597389221, Accuracy: 1.0, Computation time: 2.1052541732788086\n",
      "Step: 1755, Loss: 0.927649736404419, Accuracy: 0.9750000238418579, Computation time: 2.3196990489959717\n",
      "Step: 1756, Loss: 0.9173405766487122, Accuracy: 1.0, Computation time: 2.4404001235961914\n",
      "Step: 1757, Loss: 0.9252960085868835, Accuracy: 1.0, Computation time: 2.15084171295166\n",
      "Step: 1758, Loss: 0.9159762263298035, Accuracy: 1.0, Computation time: 1.9857606887817383\n",
      "Step: 1759, Loss: 0.9161307215690613, Accuracy: 1.0, Computation time: 1.994154453277588\n",
      "Step: 1760, Loss: 0.9161762595176697, Accuracy: 1.0, Computation time: 1.8210985660552979\n",
      "Step: 1761, Loss: 0.9164089560508728, Accuracy: 1.0, Computation time: 2.2103631496429443\n",
      "Step: 1762, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.8986468315124512\n",
      "Step: 1763, Loss: 0.9173226952552795, Accuracy: 1.0, Computation time: 2.833712339401245\n",
      "Step: 1764, Loss: 0.9159799218177795, Accuracy: 1.0, Computation time: 1.9928030967712402\n",
      "Step: 1765, Loss: 0.9160367846488953, Accuracy: 1.0, Computation time: 2.055746078491211\n",
      "Step: 1766, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.9531323909759521\n",
      "Step: 1767, Loss: 0.9160130023956299, Accuracy: 1.0, Computation time: 2.212620973587036\n",
      "Step: 1768, Loss: 0.9160165190696716, Accuracy: 1.0, Computation time: 1.8315987586975098\n",
      "Step: 1769, Loss: 0.9188025593757629, Accuracy: 1.0, Computation time: 2.4532110691070557\n",
      "Step: 1770, Loss: 0.9160082340240479, Accuracy: 1.0, Computation time: 2.1372501850128174\n",
      "Step: 1771, Loss: 0.9159756302833557, Accuracy: 1.0, Computation time: 2.4508421421051025\n",
      "Step: 1772, Loss: 0.9161930084228516, Accuracy: 1.0, Computation time: 2.510396957397461\n",
      "Step: 1773, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 2.151550054550171\n",
      "Step: 1774, Loss: 0.9182565808296204, Accuracy: 1.0, Computation time: 2.918111562728882\n",
      "Step: 1775, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 2.6398556232452393\n",
      "Step: 1776, Loss: 0.9375898838043213, Accuracy: 0.9821428656578064, Computation time: 2.405514717102051\n",
      "Step: 1777, Loss: 0.9161558747291565, Accuracy: 1.0, Computation time: 2.4100019931793213\n",
      "Step: 1778, Loss: 0.9377650022506714, Accuracy: 0.9772727489471436, Computation time: 2.154812812805176\n",
      "Step: 1779, Loss: 0.91642165184021, Accuracy: 1.0, Computation time: 2.138455867767334\n",
      "Step: 1780, Loss: 0.9166393280029297, Accuracy: 1.0, Computation time: 2.3173940181732178\n",
      "Step: 1781, Loss: 0.9185741543769836, Accuracy: 1.0, Computation time: 2.7407777309417725\n",
      "Step: 1782, Loss: 0.9160138368606567, Accuracy: 1.0, Computation time: 2.2083287239074707\n",
      "Step: 1783, Loss: 0.916019856929779, Accuracy: 1.0, Computation time: 1.974531650543213\n",
      "Step: 1784, Loss: 0.9160424470901489, Accuracy: 1.0, Computation time: 2.168057680130005\n",
      "Step: 1785, Loss: 0.9166405200958252, Accuracy: 1.0, Computation time: 1.302300214767456\n",
      "Step: 1786, Loss: 0.9160014986991882, Accuracy: 1.0, Computation time: 1.4649767875671387\n",
      "Step: 1787, Loss: 0.9160414934158325, Accuracy: 1.0, Computation time: 1.2831952571868896\n",
      "Step: 1788, Loss: 0.922075629234314, Accuracy: 1.0, Computation time: 1.1648898124694824\n",
      "Step: 1789, Loss: 0.9162030816078186, Accuracy: 1.0, Computation time: 1.4028561115264893\n",
      "Step: 1790, Loss: 0.9159103631973267, Accuracy: 1.0, Computation time: 1.3821580410003662\n",
      "Step: 1791, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.1292707920074463\n",
      "Step: 1792, Loss: 0.9171576499938965, Accuracy: 1.0, Computation time: 1.3496904373168945\n",
      "Step: 1793, Loss: 0.9159786105155945, Accuracy: 1.0, Computation time: 1.305190086364746\n",
      "Step: 1794, Loss: 0.9353343844413757, Accuracy: 0.9772727489471436, Computation time: 1.7552986145019531\n",
      "Step: 1795, Loss: 0.9163066148757935, Accuracy: 1.0, Computation time: 1.5118682384490967\n",
      "Step: 1796, Loss: 0.9160827398300171, Accuracy: 1.0, Computation time: 1.7670862674713135\n",
      "Step: 1797, Loss: 0.9160813689231873, Accuracy: 1.0, Computation time: 1.7823512554168701\n",
      "Step: 1798, Loss: 0.916000485420227, Accuracy: 1.0, Computation time: 1.1279890537261963\n",
      "Step: 1799, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 1.6303200721740723\n",
      "Step: 1800, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.3070573806762695\n",
      "Step: 1801, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.7817294597625732\n",
      "Step: 1802, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.1472299098968506\n",
      "Step: 1803, Loss: 0.9376001358032227, Accuracy: 0.9821428656578064, Computation time: 1.832930564880371\n",
      "Step: 1804, Loss: 0.9302271604537964, Accuracy: 0.949999988079071, Computation time: 1.448117733001709\n",
      "Step: 1805, Loss: 0.9159422516822815, Accuracy: 1.0, Computation time: 1.1206223964691162\n",
      "Step: 1806, Loss: 0.9198766946792603, Accuracy: 1.0, Computation time: 1.4692683219909668\n",
      "Step: 1807, Loss: 0.916109025478363, Accuracy: 1.0, Computation time: 1.425171136856079\n",
      "########################\n",
      "Test loss: 1.0709720849990845, Test Accuracy_epoch13: 0.7658918499946594\n",
      "########################\n",
      "Step: 1808, Loss: 0.916111171245575, Accuracy: 1.0, Computation time: 1.3862333297729492\n",
      "Step: 1809, Loss: 0.9161735773086548, Accuracy: 1.0, Computation time: 1.463413953781128\n",
      "Step: 1810, Loss: 0.9162392616271973, Accuracy: 1.0, Computation time: 1.6111295223236084\n",
      "Step: 1811, Loss: 0.947780966758728, Accuracy: 0.9615384340286255, Computation time: 1.4224092960357666\n",
      "Step: 1812, Loss: 0.9247097373008728, Accuracy: 1.0, Computation time: 1.3585186004638672\n",
      "Step: 1813, Loss: 0.9161578416824341, Accuracy: 1.0, Computation time: 1.3283367156982422\n",
      "Step: 1814, Loss: 0.9161472320556641, Accuracy: 1.0, Computation time: 1.588463306427002\n",
      "Step: 1815, Loss: 0.9160120487213135, Accuracy: 1.0, Computation time: 1.1490676403045654\n",
      "Step: 1816, Loss: 0.9162482023239136, Accuracy: 1.0, Computation time: 1.1294841766357422\n",
      "Step: 1817, Loss: 0.9332273006439209, Accuracy: 0.9750000238418579, Computation time: 1.1129441261291504\n",
      "Step: 1818, Loss: 0.9160950183868408, Accuracy: 1.0, Computation time: 1.7394356727600098\n",
      "Step: 1819, Loss: 0.916064441204071, Accuracy: 1.0, Computation time: 1.30104660987854\n",
      "Step: 1820, Loss: 0.915977418422699, Accuracy: 1.0, Computation time: 1.160813331604004\n",
      "Step: 1821, Loss: 0.9594129323959351, Accuracy: 0.9583333730697632, Computation time: 1.3034107685089111\n",
      "Step: 1822, Loss: 0.9177185893058777, Accuracy: 1.0, Computation time: 1.5259623527526855\n",
      "Step: 1823, Loss: 0.9159979820251465, Accuracy: 1.0, Computation time: 1.3521244525909424\n",
      "Step: 1824, Loss: 0.9161702394485474, Accuracy: 1.0, Computation time: 1.0803165435791016\n",
      "Step: 1825, Loss: 0.9163199067115784, Accuracy: 1.0, Computation time: 1.6268515586853027\n",
      "Step: 1826, Loss: 0.9161319732666016, Accuracy: 1.0, Computation time: 1.2250556945800781\n",
      "Step: 1827, Loss: 0.938389241695404, Accuracy: 0.9722222089767456, Computation time: 1.7618136405944824\n",
      "Step: 1828, Loss: 0.9161982536315918, Accuracy: 1.0, Computation time: 1.1248197555541992\n",
      "Step: 1829, Loss: 0.91600102186203, Accuracy: 1.0, Computation time: 1.42746901512146\n",
      "Step: 1830, Loss: 0.9162315130233765, Accuracy: 1.0, Computation time: 1.4994423389434814\n",
      "Step: 1831, Loss: 0.9159727692604065, Accuracy: 1.0, Computation time: 1.3118605613708496\n",
      "Step: 1832, Loss: 0.9159186482429504, Accuracy: 1.0, Computation time: 1.1958794593811035\n",
      "Step: 1833, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.1849699020385742\n",
      "Step: 1834, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.8732852935791016\n",
      "Step: 1835, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.1246724128723145\n",
      "Step: 1836, Loss: 0.9159989356994629, Accuracy: 1.0, Computation time: 1.400237798690796\n",
      "Step: 1837, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.1417477130889893\n",
      "Step: 1838, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.4057433605194092\n",
      "Step: 1839, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.2025582790374756\n",
      "Step: 1840, Loss: 0.9374303817749023, Accuracy: 0.9642857313156128, Computation time: 1.34144926071167\n",
      "Step: 1841, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.4310121536254883\n",
      "Step: 1842, Loss: 0.9166049957275391, Accuracy: 1.0, Computation time: 1.4112777709960938\n",
      "Step: 1843, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.4167931079864502\n",
      "Step: 1844, Loss: 0.9177884459495544, Accuracy: 1.0, Computation time: 1.2831356525421143\n",
      "Step: 1845, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.4695439338684082\n",
      "Step: 1846, Loss: 0.9354821443557739, Accuracy: 0.9791666865348816, Computation time: 1.3386662006378174\n",
      "Step: 1847, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.4358479976654053\n",
      "Step: 1848, Loss: 0.9159714579582214, Accuracy: 1.0, Computation time: 1.3910541534423828\n",
      "Step: 1849, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.3161952495574951\n",
      "Step: 1850, Loss: 0.915976345539093, Accuracy: 1.0, Computation time: 1.2348089218139648\n",
      "Step: 1851, Loss: 0.9160385727882385, Accuracy: 1.0, Computation time: 1.2026896476745605\n",
      "Step: 1852, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.3283860683441162\n",
      "Step: 1853, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.3534448146820068\n",
      "Step: 1854, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.1505465507507324\n",
      "Step: 1855, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.3742170333862305\n",
      "Step: 1856, Loss: 0.9181772470474243, Accuracy: 1.0, Computation time: 1.1440086364746094\n",
      "Step: 1857, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.1336288452148438\n",
      "Step: 1858, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.5373430252075195\n",
      "Step: 1859, Loss: 0.9263741374015808, Accuracy: 0.9642857313156128, Computation time: 1.2693712711334229\n",
      "Step: 1860, Loss: 0.9160174131393433, Accuracy: 1.0, Computation time: 1.3234195709228516\n",
      "Step: 1861, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.2176830768585205\n",
      "Step: 1862, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.1651320457458496\n",
      "Step: 1863, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.1931087970733643\n",
      "Step: 1864, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.1757512092590332\n",
      "Step: 1865, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.0394294261932373\n",
      "Step: 1866, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.4570472240447998\n",
      "Step: 1867, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 1.1533780097961426\n",
      "Step: 1868, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.3401529788970947\n",
      "Step: 1869, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 1.3042547702789307\n",
      "Step: 1870, Loss: 0.9376013875007629, Accuracy: 0.9821428656578064, Computation time: 1.0901052951812744\n",
      "Step: 1871, Loss: 0.9359758496284485, Accuracy: 0.9722222089767456, Computation time: 1.7867872714996338\n",
      "Step: 1872, Loss: 0.9375683665275574, Accuracy: 0.9791666865348816, Computation time: 1.236868143081665\n",
      "Step: 1873, Loss: 0.9368925094604492, Accuracy: 0.984375, Computation time: 1.3519647121429443\n",
      "Step: 1874, Loss: 0.9159070253372192, Accuracy: 1.0, Computation time: 1.086151361465454\n",
      "Step: 1875, Loss: 0.9378308653831482, Accuracy: 0.96875, Computation time: 1.406925916671753\n",
      "Step: 1876, Loss: 0.9389819502830505, Accuracy: 0.9833333492279053, Computation time: 1.296962022781372\n",
      "Step: 1877, Loss: 0.9159571528434753, Accuracy: 1.0, Computation time: 1.271547794342041\n",
      "Step: 1878, Loss: 0.9162594079971313, Accuracy: 1.0, Computation time: 1.4813389778137207\n",
      "Step: 1879, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 1.273228406906128\n",
      "Step: 1880, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 1.2181041240692139\n",
      "Step: 1881, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 1.3558743000030518\n",
      "Step: 1882, Loss: 0.9374784827232361, Accuracy: 0.9807692766189575, Computation time: 1.0941343307495117\n",
      "Step: 1883, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.2969028949737549\n",
      "Step: 1884, Loss: 0.937714159488678, Accuracy: 0.9642857313156128, Computation time: 2.011054277420044\n",
      "Step: 1885, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 1.35359787940979\n",
      "Step: 1886, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.2292380332946777\n",
      "Step: 1887, Loss: 0.9190373420715332, Accuracy: 1.0, Computation time: 1.485097885131836\n",
      "Step: 1888, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 1.2906920909881592\n",
      "Step: 1889, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 0.9470770359039307\n",
      "Step: 1890, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 0.9703466892242432\n",
      "Step: 1891, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 1.3265252113342285\n",
      "Step: 1892, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 1.3012974262237549\n",
      "Step: 1893, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.1630871295928955\n",
      "Step: 1894, Loss: 0.9376044869422913, Accuracy: 0.96875, Computation time: 1.1428866386413574\n",
      "Step: 1895, Loss: 0.9280475974082947, Accuracy: 0.9807692766189575, Computation time: 1.6664514541625977\n",
      "Step: 1896, Loss: 0.9399800896644592, Accuracy: 0.9375, Computation time: 1.4534761905670166\n",
      "Step: 1897, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 1.1918940544128418\n",
      "Step: 1898, Loss: 0.9162067770957947, Accuracy: 1.0, Computation time: 1.5583562850952148\n",
      "Step: 1899, Loss: 0.9187703132629395, Accuracy: 1.0, Computation time: 1.6227271556854248\n",
      "Step: 1900, Loss: 0.916033148765564, Accuracy: 1.0, Computation time: 1.3449082374572754\n",
      "Step: 1901, Loss: 0.9162029027938843, Accuracy: 1.0, Computation time: 1.2664806842803955\n",
      "Step: 1902, Loss: 0.9212274551391602, Accuracy: 1.0, Computation time: 1.473266363143921\n",
      "Step: 1903, Loss: 0.9329636693000793, Accuracy: 0.9722222089767456, Computation time: 1.5647034645080566\n",
      "Step: 1904, Loss: 0.9161046743392944, Accuracy: 1.0, Computation time: 1.3088397979736328\n",
      "Step: 1905, Loss: 0.9182170033454895, Accuracy: 1.0, Computation time: 1.4072120189666748\n",
      "Step: 1906, Loss: 0.9163100123405457, Accuracy: 1.0, Computation time: 1.2486300468444824\n",
      "Step: 1907, Loss: 0.916914165019989, Accuracy: 1.0, Computation time: 1.1438887119293213\n",
      "Step: 1908, Loss: 0.9163087010383606, Accuracy: 1.0, Computation time: 1.4937713146209717\n",
      "Step: 1909, Loss: 0.9364160895347595, Accuracy: 0.9821428656578064, Computation time: 1.4882526397705078\n",
      "Step: 1910, Loss: 0.9162155389785767, Accuracy: 1.0, Computation time: 1.300445795059204\n",
      "Step: 1911, Loss: 0.9163485765457153, Accuracy: 1.0, Computation time: 1.2456908226013184\n",
      "Step: 1912, Loss: 0.9161360263824463, Accuracy: 1.0, Computation time: 1.4770147800445557\n",
      "Step: 1913, Loss: 0.9164376854896545, Accuracy: 1.0, Computation time: 1.322693109512329\n",
      "Step: 1914, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 1.2708308696746826\n",
      "Step: 1915, Loss: 0.9165748953819275, Accuracy: 1.0, Computation time: 1.7609977722167969\n",
      "Step: 1916, Loss: 0.9359320402145386, Accuracy: 0.9642857313156128, Computation time: 1.2761263847351074\n",
      "Step: 1917, Loss: 0.9161205887794495, Accuracy: 1.0, Computation time: 1.4353785514831543\n",
      "Step: 1918, Loss: 0.9164203405380249, Accuracy: 1.0, Computation time: 2.042412042617798\n",
      "Step: 1919, Loss: 0.9232143759727478, Accuracy: 1.0, Computation time: 1.4634735584259033\n",
      "Step: 1920, Loss: 0.9160199165344238, Accuracy: 1.0, Computation time: 1.2810347080230713\n",
      "Step: 1921, Loss: 0.9162148237228394, Accuracy: 1.0, Computation time: 1.249307632446289\n",
      "Step: 1922, Loss: 0.9163665771484375, Accuracy: 1.0, Computation time: 1.3403184413909912\n",
      "Step: 1923, Loss: 0.9323351383209229, Accuracy: 0.9821428656578064, Computation time: 1.3854241371154785\n",
      "Step: 1924, Loss: 0.925214946269989, Accuracy: 1.0, Computation time: 1.202759027481079\n",
      "Step: 1925, Loss: 0.9159982204437256, Accuracy: 1.0, Computation time: 1.078279733657837\n",
      "Step: 1926, Loss: 0.9159850478172302, Accuracy: 1.0, Computation time: 1.1605579853057861\n",
      "Step: 1927, Loss: 0.9159567952156067, Accuracy: 1.0, Computation time: 1.1317365169525146\n",
      "Step: 1928, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.4121723175048828\n",
      "Step: 1929, Loss: 0.9159451127052307, Accuracy: 1.0, Computation time: 1.179189920425415\n",
      "Step: 1930, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.1528186798095703\n",
      "Step: 1931, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 1.1811916828155518\n",
      "Step: 1932, Loss: 0.9305155277252197, Accuracy: 0.9642857313156128, Computation time: 1.4307060241699219\n",
      "Step: 1933, Loss: 0.9166053533554077, Accuracy: 1.0, Computation time: 1.402482032775879\n",
      "Step: 1934, Loss: 0.9364158511161804, Accuracy: 0.9791666865348816, Computation time: 1.356734275817871\n",
      "Step: 1935, Loss: 0.9160099029541016, Accuracy: 1.0, Computation time: 1.2504792213439941\n",
      "Step: 1936, Loss: 0.9160445928573608, Accuracy: 1.0, Computation time: 1.1219477653503418\n",
      "Step: 1937, Loss: 0.9376025199890137, Accuracy: 0.949999988079071, Computation time: 1.380542278289795\n",
      "Step: 1938, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.5798382759094238\n",
      "Step: 1939, Loss: 0.9160219430923462, Accuracy: 1.0, Computation time: 1.1643948554992676\n",
      "Step: 1940, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.4193546772003174\n",
      "Step: 1941, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 1.09871244430542\n",
      "Step: 1942, Loss: 0.9170074462890625, Accuracy: 1.0, Computation time: 0.9557600021362305\n",
      "Step: 1943, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 1.2820191383361816\n",
      "Step: 1944, Loss: 0.9160175919532776, Accuracy: 1.0, Computation time: 1.5145213603973389\n",
      "Step: 1945, Loss: 0.9321998953819275, Accuracy: 0.9772727489471436, Computation time: 1.3323187828063965\n",
      "Step: 1946, Loss: 0.9254025220870972, Accuracy: 0.9722222089767456, Computation time: 1.1717536449432373\n",
      "########################\n",
      "Test loss: 1.0681312084197998, Test Accuracy_epoch14: 0.7647414207458496\n",
      "########################\n",
      "Step: 1947, Loss: 0.9160377383232117, Accuracy: 1.0, Computation time: 1.22269868850708\n",
      "Step: 1948, Loss: 0.9160359501838684, Accuracy: 1.0, Computation time: 1.1321310997009277\n",
      "Step: 1949, Loss: 0.9161087274551392, Accuracy: 1.0, Computation time: 1.1720478534698486\n",
      "Step: 1950, Loss: 0.9440005421638489, Accuracy: 0.9833333492279053, Computation time: 1.4199519157409668\n",
      "Step: 1951, Loss: 0.916076123714447, Accuracy: 1.0, Computation time: 1.191924810409546\n",
      "Step: 1952, Loss: 0.9378384351730347, Accuracy: 0.9583333730697632, Computation time: 1.182243824005127\n",
      "Step: 1953, Loss: 0.9160664677619934, Accuracy: 1.0, Computation time: 1.2223584651947021\n",
      "Step: 1954, Loss: 0.9164851903915405, Accuracy: 1.0, Computation time: 1.9305191040039062\n",
      "Step: 1955, Loss: 0.9159747958183289, Accuracy: 1.0, Computation time: 1.4563655853271484\n",
      "Step: 1956, Loss: 0.9159404635429382, Accuracy: 1.0, Computation time: 1.4241728782653809\n",
      "Step: 1957, Loss: 0.916313648223877, Accuracy: 1.0, Computation time: 1.9367716312408447\n",
      "Step: 1958, Loss: 0.9160739779472351, Accuracy: 1.0, Computation time: 1.033435583114624\n",
      "Step: 1959, Loss: 0.9162052869796753, Accuracy: 1.0, Computation time: 1.2954707145690918\n",
      "Step: 1960, Loss: 0.9160081744194031, Accuracy: 1.0, Computation time: 1.3255083560943604\n",
      "Step: 1961, Loss: 0.9159673452377319, Accuracy: 1.0, Computation time: 1.0874030590057373\n",
      "Step: 1962, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 1.423086404800415\n",
      "Step: 1963, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.3024983406066895\n",
      "Step: 1964, Loss: 0.9372161626815796, Accuracy: 0.9852941036224365, Computation time: 1.50132417678833\n",
      "Step: 1965, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.3336198329925537\n",
      "Step: 1966, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.4616060256958008\n",
      "Step: 1967, Loss: 0.9321587085723877, Accuracy: 0.9807692766189575, Computation time: 1.5405540466308594\n",
      "Step: 1968, Loss: 0.9161822199821472, Accuracy: 1.0, Computation time: 1.2243702411651611\n",
      "Step: 1969, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 1.5577857494354248\n",
      "Step: 1970, Loss: 0.9378728270530701, Accuracy: 0.9722222089767456, Computation time: 1.559236764907837\n",
      "Step: 1971, Loss: 0.9159983992576599, Accuracy: 1.0, Computation time: 1.3311138153076172\n",
      "Step: 1972, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.6130757331848145\n",
      "Step: 1973, Loss: 0.9159116744995117, Accuracy: 1.0, Computation time: 1.5026588439941406\n",
      "Step: 1974, Loss: 0.9247147440910339, Accuracy: 1.0, Computation time: 1.2857887744903564\n",
      "Step: 1975, Loss: 0.9159586429595947, Accuracy: 1.0, Computation time: 1.3328614234924316\n",
      "Step: 1976, Loss: 0.9172869324684143, Accuracy: 1.0, Computation time: 1.604078769683838\n",
      "Step: 1977, Loss: 0.9385729432106018, Accuracy: 0.9722222089767456, Computation time: 1.6081972122192383\n",
      "Step: 1978, Loss: 0.9163303971290588, Accuracy: 1.0, Computation time: 1.468940258026123\n",
      "Step: 1979, Loss: 0.9159581661224365, Accuracy: 1.0, Computation time: 1.26096510887146\n",
      "Step: 1980, Loss: 0.9160358309745789, Accuracy: 1.0, Computation time: 1.2802560329437256\n",
      "Step: 1981, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.442157506942749\n",
      "Step: 1982, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.1256086826324463\n",
      "Step: 1983, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 1.4823639392852783\n",
      "Step: 1984, Loss: 0.9160208702087402, Accuracy: 1.0, Computation time: 1.4789049625396729\n",
      "Step: 1985, Loss: 0.935806393623352, Accuracy: 0.96875, Computation time: 1.4770209789276123\n",
      "Step: 1986, Loss: 0.9166325926780701, Accuracy: 1.0, Computation time: 1.521803617477417\n",
      "Step: 1987, Loss: 0.9161263704299927, Accuracy: 1.0, Computation time: 1.8298678398132324\n",
      "Step: 1988, Loss: 0.9272271394729614, Accuracy: 0.9722222089767456, Computation time: 1.4832184314727783\n",
      "Step: 1989, Loss: 0.9160528182983398, Accuracy: 1.0, Computation time: 2.088399648666382\n",
      "Step: 1990, Loss: 0.9163781404495239, Accuracy: 1.0, Computation time: 1.3216445446014404\n",
      "Step: 1991, Loss: 0.9196069240570068, Accuracy: 1.0, Computation time: 1.8839690685272217\n",
      "Step: 1992, Loss: 0.9161942601203918, Accuracy: 1.0, Computation time: 1.3837897777557373\n",
      "Step: 1993, Loss: 0.9164432883262634, Accuracy: 1.0, Computation time: 1.2957870960235596\n",
      "Step: 1994, Loss: 0.9161670207977295, Accuracy: 1.0, Computation time: 1.6212024688720703\n",
      "Step: 1995, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 1.2370107173919678\n",
      "Step: 1996, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.6057028770446777\n",
      "Step: 1997, Loss: 0.9160816669464111, Accuracy: 1.0, Computation time: 1.4682636260986328\n",
      "Step: 1998, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.2365858554840088\n",
      "Step: 1999, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.3520174026489258\n",
      "Step: 2000, Loss: 0.9207655191421509, Accuracy: 1.0, Computation time: 2.4655990600585938\n",
      "Step: 2001, Loss: 0.9286364316940308, Accuracy: 0.9791666865348816, Computation time: 1.4904496669769287\n",
      "Step: 2002, Loss: 0.9167046546936035, Accuracy: 1.0, Computation time: 1.5281896591186523\n",
      "Step: 2003, Loss: 0.9375839233398438, Accuracy: 0.9791666865348816, Computation time: 1.2523245811462402\n",
      "Step: 2004, Loss: 0.9208140969276428, Accuracy: 1.0, Computation time: 1.6651363372802734\n",
      "Step: 2005, Loss: 0.9159815907478333, Accuracy: 1.0, Computation time: 1.1176445484161377\n",
      "Step: 2006, Loss: 0.9160739183425903, Accuracy: 1.0, Computation time: 1.7859158515930176\n",
      "Step: 2007, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.5273303985595703\n",
      "Step: 2008, Loss: 0.9381653070449829, Accuracy: 0.9583333730697632, Computation time: 1.505685567855835\n",
      "Step: 2009, Loss: 0.9159923195838928, Accuracy: 1.0, Computation time: 1.2846221923828125\n",
      "Step: 2010, Loss: 0.9188803434371948, Accuracy: 1.0, Computation time: 1.2802371978759766\n",
      "Step: 2011, Loss: 0.9161361455917358, Accuracy: 1.0, Computation time: 1.541511058807373\n",
      "Step: 2012, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 1.0032403469085693\n",
      "Step: 2013, Loss: 0.9159904718399048, Accuracy: 1.0, Computation time: 1.5645751953125\n",
      "Step: 2014, Loss: 0.9160166382789612, Accuracy: 1.0, Computation time: 1.4003112316131592\n",
      "Step: 2015, Loss: 0.9378354549407959, Accuracy: 0.9750000238418579, Computation time: 1.3320324420928955\n",
      "Step: 2016, Loss: 0.9159428477287292, Accuracy: 1.0, Computation time: 1.726728916168213\n",
      "Step: 2017, Loss: 0.926121175289154, Accuracy: 0.9791666865348816, Computation time: 1.3948967456817627\n",
      "Step: 2018, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.4296443462371826\n",
      "Step: 2019, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.390442132949829\n",
      "Step: 2020, Loss: 0.9159197211265564, Accuracy: 1.0, Computation time: 1.1981697082519531\n",
      "Step: 2021, Loss: 0.9160093665122986, Accuracy: 1.0, Computation time: 1.2428834438323975\n",
      "Step: 2022, Loss: 0.9168382287025452, Accuracy: 1.0, Computation time: 1.3089594841003418\n",
      "Step: 2023, Loss: 0.9178604483604431, Accuracy: 1.0, Computation time: 1.4206678867340088\n",
      "Step: 2024, Loss: 0.9160670042037964, Accuracy: 1.0, Computation time: 1.2507903575897217\n",
      "Step: 2025, Loss: 0.9159514904022217, Accuracy: 1.0, Computation time: 1.3537309169769287\n",
      "Step: 2026, Loss: 0.9167306423187256, Accuracy: 1.0, Computation time: 1.3534893989562988\n",
      "Step: 2027, Loss: 0.9159760475158691, Accuracy: 1.0, Computation time: 1.2430040836334229\n",
      "Step: 2028, Loss: 0.9160211086273193, Accuracy: 1.0, Computation time: 1.4141809940338135\n",
      "Step: 2029, Loss: 0.9543492794036865, Accuracy: 0.9436274766921997, Computation time: 1.6298608779907227\n",
      "Step: 2030, Loss: 0.9375402331352234, Accuracy: 0.9722222089767456, Computation time: 1.4439337253570557\n",
      "Step: 2031, Loss: 0.9162322878837585, Accuracy: 1.0, Computation time: 1.5317590236663818\n",
      "Step: 2032, Loss: 0.9164195656776428, Accuracy: 1.0, Computation time: 1.4468464851379395\n",
      "Step: 2033, Loss: 0.9162831902503967, Accuracy: 1.0, Computation time: 1.2905189990997314\n",
      "Step: 2034, Loss: 0.9162368178367615, Accuracy: 1.0, Computation time: 1.0379536151885986\n",
      "Step: 2035, Loss: 0.9352149963378906, Accuracy: 0.9791666865348816, Computation time: 1.4098296165466309\n",
      "Step: 2036, Loss: 0.916698694229126, Accuracy: 1.0, Computation time: 1.2238469123840332\n",
      "Step: 2037, Loss: 0.9347245097160339, Accuracy: 0.9791666865348816, Computation time: 1.5451323986053467\n",
      "Step: 2038, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 1.3254296779632568\n",
      "Step: 2039, Loss: 0.9372540712356567, Accuracy: 0.9772727489471436, Computation time: 1.4500679969787598\n",
      "Step: 2040, Loss: 0.9159913659095764, Accuracy: 1.0, Computation time: 1.2173314094543457\n",
      "Step: 2041, Loss: 0.9163989424705505, Accuracy: 1.0, Computation time: 1.3173725605010986\n",
      "Step: 2042, Loss: 0.9160334467887878, Accuracy: 1.0, Computation time: 1.3996071815490723\n",
      "Step: 2043, Loss: 0.9162352681159973, Accuracy: 1.0, Computation time: 1.1717431545257568\n",
      "Step: 2044, Loss: 0.9184427261352539, Accuracy: 1.0, Computation time: 1.3638560771942139\n",
      "Step: 2045, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 1.2905774116516113\n",
      "Step: 2046, Loss: 0.9274121522903442, Accuracy: 0.9583333730697632, Computation time: 1.8138244152069092\n",
      "Step: 2047, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 1.2812211513519287\n",
      "Step: 2048, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 1.3289847373962402\n",
      "Step: 2049, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.198251485824585\n",
      "Step: 2050, Loss: 0.9160361886024475, Accuracy: 1.0, Computation time: 1.4147651195526123\n",
      "Step: 2051, Loss: 0.9161287546157837, Accuracy: 1.0, Computation time: 1.6043181419372559\n",
      "Step: 2052, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 1.0780699253082275\n",
      "Step: 2053, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.2358286380767822\n",
      "Step: 2054, Loss: 0.9160149097442627, Accuracy: 1.0, Computation time: 1.3424205780029297\n",
      "Step: 2055, Loss: 0.9160742163658142, Accuracy: 1.0, Computation time: 1.2457492351531982\n",
      "Step: 2056, Loss: 0.9159784913063049, Accuracy: 1.0, Computation time: 1.3610329627990723\n",
      "Step: 2057, Loss: 0.9305159449577332, Accuracy: 0.96875, Computation time: 1.1248176097869873\n",
      "Step: 2058, Loss: 0.9159455895423889, Accuracy: 1.0, Computation time: 1.0947365760803223\n",
      "Step: 2059, Loss: 0.9159392714500427, Accuracy: 1.0, Computation time: 1.7168307304382324\n",
      "Step: 2060, Loss: 0.9160169959068298, Accuracy: 1.0, Computation time: 1.3176817893981934\n",
      "Step: 2061, Loss: 0.9160059094429016, Accuracy: 1.0, Computation time: 1.6017329692840576\n",
      "Step: 2062, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.1270091533660889\n",
      "Step: 2063, Loss: 0.935738742351532, Accuracy: 0.984375, Computation time: 1.6953840255737305\n",
      "Step: 2064, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.153942346572876\n",
      "Step: 2065, Loss: 0.9159860610961914, Accuracy: 1.0, Computation time: 1.1204311847686768\n",
      "Step: 2066, Loss: 0.9159887433052063, Accuracy: 1.0, Computation time: 1.0174930095672607\n",
      "Step: 2067, Loss: 0.9160483479499817, Accuracy: 1.0, Computation time: 1.1678991317749023\n",
      "Step: 2068, Loss: 0.9357690215110779, Accuracy: 0.9772727489471436, Computation time: 1.2791111469268799\n",
      "Step: 2069, Loss: 0.9162284135818481, Accuracy: 1.0, Computation time: 1.3884460926055908\n",
      "Step: 2070, Loss: 0.9160162210464478, Accuracy: 1.0, Computation time: 1.1982474327087402\n",
      "Step: 2071, Loss: 0.915935754776001, Accuracy: 1.0, Computation time: 1.317209243774414\n",
      "Step: 2072, Loss: 0.9213829040527344, Accuracy: 1.0, Computation time: 1.2858483791351318\n",
      "Step: 2073, Loss: 0.9347152709960938, Accuracy: 0.9583333730697632, Computation time: 1.3807661533355713\n",
      "Step: 2074, Loss: 0.9160962104797363, Accuracy: 1.0, Computation time: 1.845043659210205\n",
      "Step: 2075, Loss: 0.9159882664680481, Accuracy: 1.0, Computation time: 1.2167506217956543\n",
      "Step: 2076, Loss: 0.9160310626029968, Accuracy: 1.0, Computation time: 1.1167945861816406\n",
      "Step: 2077, Loss: 0.9376395344734192, Accuracy: 0.9722222089767456, Computation time: 1.1736650466918945\n",
      "Step: 2078, Loss: 0.9368144273757935, Accuracy: 0.9750000238418579, Computation time: 1.4792258739471436\n",
      "Step: 2079, Loss: 0.9159677624702454, Accuracy: 1.0, Computation time: 1.3870480060577393\n",
      "Step: 2080, Loss: 0.9159274697303772, Accuracy: 1.0, Computation time: 1.1104543209075928\n",
      "Step: 2081, Loss: 0.9159736037254333, Accuracy: 1.0, Computation time: 1.155336618423462\n",
      "Step: 2082, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 1.3696889877319336\n",
      "Step: 2083, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.5114166736602783\n",
      "Step: 2084, Loss: 0.91595059633255, Accuracy: 1.0, Computation time: 1.2328665256500244\n",
      "Step: 2085, Loss: 0.9376205205917358, Accuracy: 0.96875, Computation time: 1.2791390419006348\n",
      "########################\n",
      "Test loss: 1.0695563554763794, Test Accuracy_epoch15: 0.769397497177124\n",
      "########################\n",
      "Step: 2086, Loss: 0.9372445344924927, Accuracy: 0.9791666865348816, Computation time: 1.315049648284912\n",
      "Step: 2087, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.2371578216552734\n",
      "Step: 2088, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.1284620761871338\n",
      "Step: 2089, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.0434458255767822\n",
      "Step: 2090, Loss: 0.9159485101699829, Accuracy: 1.0, Computation time: 1.0914344787597656\n",
      "Step: 2091, Loss: 0.9159813523292542, Accuracy: 1.0, Computation time: 1.140364408493042\n",
      "Step: 2092, Loss: 0.9161314964294434, Accuracy: 1.0, Computation time: 1.2008895874023438\n",
      "Step: 2093, Loss: 0.9159970879554749, Accuracy: 1.0, Computation time: 1.1135730743408203\n",
      "Step: 2094, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.47377347946167\n",
      "Step: 2095, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.0467321872711182\n",
      "Step: 2096, Loss: 0.9375826120376587, Accuracy: 0.9750000238418579, Computation time: 1.1468186378479004\n",
      "Step: 2097, Loss: 0.9158888459205627, Accuracy: 1.0, Computation time: 1.2848179340362549\n",
      "Step: 2098, Loss: 0.9375616908073425, Accuracy: 0.9861111044883728, Computation time: 1.0748209953308105\n",
      "Step: 2099, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.1257333755493164\n",
      "Step: 2100, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 1.1730737686157227\n",
      "Step: 2101, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.2682559490203857\n",
      "Step: 2102, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.152986764907837\n",
      "Step: 2103, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.218580961227417\n",
      "Step: 2104, Loss: 0.916496753692627, Accuracy: 1.0, Computation time: 1.1915109157562256\n",
      "Step: 2105, Loss: 0.9159395694732666, Accuracy: 1.0, Computation time: 1.4586563110351562\n",
      "Step: 2106, Loss: 0.9166871905326843, Accuracy: 1.0, Computation time: 1.3321433067321777\n",
      "Step: 2107, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.095313549041748\n",
      "Step: 2108, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.324770212173462\n",
      "Step: 2109, Loss: 0.91667640209198, Accuracy: 1.0, Computation time: 1.1636266708374023\n",
      "Step: 2110, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 1.2755095958709717\n",
      "Step: 2111, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.2358734607696533\n",
      "Step: 2112, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.3116381168365479\n",
      "Step: 2113, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.2480907440185547\n",
      "Step: 2114, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.1693007946014404\n",
      "Step: 2115, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.1577250957489014\n",
      "Step: 2116, Loss: 0.9162653088569641, Accuracy: 1.0, Computation time: 1.1346204280853271\n",
      "Step: 2117, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.2378206253051758\n",
      "Step: 2118, Loss: 0.9159212708473206, Accuracy: 1.0, Computation time: 1.3006739616394043\n",
      "Step: 2119, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.2754223346710205\n",
      "Step: 2120, Loss: 0.9451483488082886, Accuracy: 0.9750000238418579, Computation time: 1.2265162467956543\n",
      "Step: 2121, Loss: 0.9171512126922607, Accuracy: 1.0, Computation time: 1.3757514953613281\n",
      "Step: 2122, Loss: 0.9378035068511963, Accuracy: 0.9642857313156128, Computation time: 0.934913158416748\n",
      "Step: 2123, Loss: 0.9160412549972534, Accuracy: 1.0, Computation time: 1.2553892135620117\n",
      "Step: 2124, Loss: 0.9160304665565491, Accuracy: 1.0, Computation time: 1.214967966079712\n",
      "Step: 2125, Loss: 0.9343590140342712, Accuracy: 0.9642857313156128, Computation time: 1.9049770832061768\n",
      "Step: 2126, Loss: 0.937685489654541, Accuracy: 0.949999988079071, Computation time: 1.1317241191864014\n",
      "Step: 2127, Loss: 0.915928065776825, Accuracy: 1.0, Computation time: 1.0787627696990967\n",
      "Step: 2128, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 1.3102304935455322\n",
      "Step: 2129, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.0494163036346436\n",
      "Step: 2130, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.323035478591919\n",
      "Step: 2131, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.022596836090088\n",
      "Step: 2132, Loss: 0.9159716367721558, Accuracy: 1.0, Computation time: 1.3331596851348877\n",
      "Step: 2133, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.225358247756958\n",
      "Step: 2134, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 1.0475585460662842\n",
      "Step: 2135, Loss: 0.9339016079902649, Accuracy: 0.9807692766189575, Computation time: 1.2521777153015137\n",
      "Step: 2136, Loss: 0.9160427451133728, Accuracy: 1.0, Computation time: 1.3035001754760742\n",
      "Step: 2137, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 0.9603815078735352\n",
      "Step: 2138, Loss: 0.937449038028717, Accuracy: 0.9807692766189575, Computation time: 1.125009298324585\n",
      "Step: 2139, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 1.131481647491455\n",
      "Step: 2140, Loss: 0.9159380197525024, Accuracy: 1.0, Computation time: 1.0948002338409424\n",
      "Step: 2141, Loss: 0.9167970418930054, Accuracy: 1.0, Computation time: 1.5092222690582275\n",
      "Step: 2142, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 1.0756416320800781\n",
      "Step: 2143, Loss: 0.9170588850975037, Accuracy: 1.0, Computation time: 1.4783704280853271\n",
      "Step: 2144, Loss: 0.9159589409828186, Accuracy: 1.0, Computation time: 1.5844042301177979\n",
      "Step: 2145, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.708533525466919\n",
      "Step: 2146, Loss: 0.915938138961792, Accuracy: 1.0, Computation time: 1.4809966087341309\n",
      "Step: 2147, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.06494140625\n",
      "Step: 2148, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.4664342403411865\n",
      "Step: 2149, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.3194873332977295\n",
      "Step: 2150, Loss: 0.9185580611228943, Accuracy: 1.0, Computation time: 1.3823578357696533\n",
      "Step: 2151, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.3632822036743164\n",
      "Step: 2152, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.4745326042175293\n",
      "Step: 2153, Loss: 0.937546968460083, Accuracy: 0.9821428656578064, Computation time: 1.449958324432373\n",
      "Step: 2154, Loss: 0.9337011575698853, Accuracy: 0.9722222089767456, Computation time: 1.2464535236358643\n",
      "Step: 2155, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 1.530519723892212\n",
      "Step: 2156, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.2877812385559082\n",
      "Step: 2157, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.4600753784179688\n",
      "Step: 2158, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.3012149333953857\n",
      "Step: 2159, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 1.0601861476898193\n",
      "Step: 2160, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.1417515277862549\n",
      "Step: 2161, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.4532966613769531\n",
      "Step: 2162, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 1.0850119590759277\n",
      "Step: 2163, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.6026830673217773\n",
      "Step: 2164, Loss: 0.9374694228172302, Accuracy: 0.9772727489471436, Computation time: 1.1679320335388184\n",
      "Step: 2165, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.0873446464538574\n",
      "Step: 2166, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.6637868881225586\n",
      "Step: 2167, Loss: 0.9173144102096558, Accuracy: 1.0, Computation time: 1.4205267429351807\n",
      "Step: 2168, Loss: 0.9159409999847412, Accuracy: 1.0, Computation time: 1.1149559020996094\n",
      "Step: 2169, Loss: 0.9280781745910645, Accuracy: 0.9807692766189575, Computation time: 1.8554022312164307\n",
      "Step: 2170, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.13572359085083\n",
      "Step: 2171, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.4110329151153564\n",
      "Step: 2172, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.095217227935791\n",
      "Step: 2173, Loss: 0.9373913407325745, Accuracy: 0.9821428656578064, Computation time: 1.6869499683380127\n",
      "Step: 2174, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.5046279430389404\n",
      "Step: 2175, Loss: 0.9374430775642395, Accuracy: 0.9791666865348816, Computation time: 1.3362231254577637\n",
      "Step: 2176, Loss: 0.9170071482658386, Accuracy: 1.0, Computation time: 1.1270415782928467\n",
      "Step: 2177, Loss: 0.9158984422683716, Accuracy: 1.0, Computation time: 1.2191801071166992\n",
      "Step: 2178, Loss: 0.9160718321800232, Accuracy: 1.0, Computation time: 1.237201452255249\n",
      "Step: 2179, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 1.4651331901550293\n",
      "Step: 2180, Loss: 0.9184529781341553, Accuracy: 1.0, Computation time: 1.2856111526489258\n",
      "Step: 2181, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.2611143589019775\n",
      "Step: 2182, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 1.4706740379333496\n",
      "Step: 2183, Loss: 0.9166762828826904, Accuracy: 1.0, Computation time: 1.7226874828338623\n",
      "Step: 2184, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.0947670936584473\n",
      "Step: 2185, Loss: 0.9376479387283325, Accuracy: 0.9833333492279053, Computation time: 1.391045093536377\n",
      "Step: 2186, Loss: 0.9374380111694336, Accuracy: 0.96875, Computation time: 1.6782498359680176\n",
      "Step: 2187, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.3365082740783691\n",
      "Step: 2188, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.2737412452697754\n",
      "Step: 2189, Loss: 0.9161151051521301, Accuracy: 1.0, Computation time: 1.083146333694458\n",
      "Step: 2190, Loss: 0.9376195073127747, Accuracy: 0.9642857313156128, Computation time: 1.4273691177368164\n",
      "Step: 2191, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 2.1158382892608643\n",
      "Step: 2192, Loss: 0.9181141257286072, Accuracy: 1.0, Computation time: 2.388234853744507\n",
      "Step: 2193, Loss: 0.9386596083641052, Accuracy: 0.9642857313156128, Computation time: 2.8901360034942627\n",
      "Step: 2194, Loss: 0.9392156600952148, Accuracy: 0.9375, Computation time: 1.880479335784912\n",
      "Step: 2195, Loss: 0.9357828497886658, Accuracy: 0.949999988079071, Computation time: 2.2333223819732666\n",
      "Step: 2196, Loss: 0.9159513115882874, Accuracy: 1.0, Computation time: 2.1811752319335938\n",
      "Step: 2197, Loss: 0.9159944653511047, Accuracy: 1.0, Computation time: 2.2759275436401367\n",
      "Step: 2198, Loss: 0.9192827343940735, Accuracy: 1.0, Computation time: 2.3081204891204834\n",
      "Step: 2199, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 2.675403118133545\n",
      "Step: 2200, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.7923192977905273\n",
      "Step: 2201, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.9210329055786133\n",
      "Step: 2202, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 1.9389147758483887\n",
      "Step: 2203, Loss: 0.9159445762634277, Accuracy: 1.0, Computation time: 2.8109867572784424\n",
      "Step: 2204, Loss: 0.9379940629005432, Accuracy: 0.9642857313156128, Computation time: 2.1286466121673584\n",
      "Step: 2205, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.6185531616210938\n",
      "Step: 2206, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.9614675045013428\n",
      "Step: 2207, Loss: 0.9451257586479187, Accuracy: 0.9166666865348816, Computation time: 2.2831523418426514\n",
      "Step: 2208, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 2.5805768966674805\n",
      "Step: 2209, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 1.79781174659729\n",
      "Step: 2210, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.5082385540008545\n",
      "Step: 2211, Loss: 0.9159315228462219, Accuracy: 1.0, Computation time: 1.1501789093017578\n",
      "Step: 2212, Loss: 0.9159610271453857, Accuracy: 1.0, Computation time: 1.2640697956085205\n",
      "Step: 2213, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.2787096500396729\n",
      "Step: 2214, Loss: 0.9160394072532654, Accuracy: 1.0, Computation time: 1.343172550201416\n",
      "Step: 2215, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.5207905769348145\n",
      "Step: 2216, Loss: 0.9173711538314819, Accuracy: 1.0, Computation time: 1.335622787475586\n",
      "Step: 2217, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.1936686038970947\n",
      "Step: 2218, Loss: 0.9375187754631042, Accuracy: 0.9772727489471436, Computation time: 1.0813519954681396\n",
      "Step: 2219, Loss: 0.9159707427024841, Accuracy: 1.0, Computation time: 1.438079833984375\n",
      "Step: 2220, Loss: 0.9187619090080261, Accuracy: 1.0, Computation time: 1.2443106174468994\n",
      "Step: 2221, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.1929569244384766\n",
      "Step: 2222, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.1270816326141357\n",
      "Step: 2223, Loss: 0.9374752044677734, Accuracy: 0.9722222089767456, Computation time: 1.1470835208892822\n",
      "########################\n",
      "Test loss: 1.0731885433197021, Test Accuracy_epoch16: 0.7601842880249023\n",
      "########################\n",
      "Step: 2224, Loss: 0.9159544110298157, Accuracy: 1.0, Computation time: 1.0935919284820557\n",
      "Step: 2225, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.249464511871338\n",
      "Step: 2226, Loss: 0.9159073233604431, Accuracy: 1.0, Computation time: 1.1526343822479248\n",
      "Step: 2227, Loss: 0.9161677360534668, Accuracy: 1.0, Computation time: 1.9549124240875244\n",
      "Step: 2228, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 1.5127246379852295\n",
      "Step: 2229, Loss: 0.9159564971923828, Accuracy: 1.0, Computation time: 1.1580450534820557\n",
      "Step: 2230, Loss: 0.9320827722549438, Accuracy: 0.9722222089767456, Computation time: 2.1552324295043945\n",
      "Step: 2231, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 1.7108256816864014\n",
      "Step: 2232, Loss: 0.9170509576797485, Accuracy: 1.0, Computation time: 1.1054432392120361\n",
      "Step: 2233, Loss: 0.9162998199462891, Accuracy: 1.0, Computation time: 1.2609832286834717\n",
      "Step: 2234, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 1.246718168258667\n",
      "Step: 2235, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 1.094630241394043\n",
      "Step: 2236, Loss: 0.9373989105224609, Accuracy: 0.9642857313156128, Computation time: 1.216503620147705\n",
      "Step: 2237, Loss: 0.9159694910049438, Accuracy: 1.0, Computation time: 1.570521593093872\n",
      "Step: 2238, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.388084888458252\n",
      "Step: 2239, Loss: 0.9354525208473206, Accuracy: 0.9791666865348816, Computation time: 1.1052193641662598\n",
      "Step: 2240, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.2919411659240723\n",
      "Step: 2241, Loss: 0.9158992767333984, Accuracy: 1.0, Computation time: 1.1700804233551025\n",
      "Step: 2242, Loss: 0.9375157356262207, Accuracy: 0.9642857313156128, Computation time: 1.265810489654541\n",
      "Step: 2243, Loss: 0.915982723236084, Accuracy: 1.0, Computation time: 1.4012224674224854\n",
      "Step: 2244, Loss: 0.9376976490020752, Accuracy: 0.96875, Computation time: 1.3371517658233643\n",
      "Step: 2245, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 0.9956421852111816\n",
      "Step: 2246, Loss: 0.9162158370018005, Accuracy: 1.0, Computation time: 1.3115360736846924\n",
      "Step: 2247, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.1933226585388184\n",
      "Step: 2248, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.2116456031799316\n",
      "Step: 2249, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.1109693050384521\n",
      "Step: 2250, Loss: 0.9159378409385681, Accuracy: 1.0, Computation time: 1.028996467590332\n",
      "Step: 2251, Loss: 0.9174360632896423, Accuracy: 1.0, Computation time: 1.4774386882781982\n",
      "Step: 2252, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.1132047176361084\n",
      "Step: 2253, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.3954088687896729\n",
      "Step: 2254, Loss: 0.9158833026885986, Accuracy: 1.0, Computation time: 1.1211178302764893\n",
      "Step: 2255, Loss: 0.9160941243171692, Accuracy: 1.0, Computation time: 1.218745231628418\n",
      "Step: 2256, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.2981996536254883\n",
      "Step: 2257, Loss: 0.9209192991256714, Accuracy: 1.0, Computation time: 1.5648784637451172\n",
      "Step: 2258, Loss: 0.9159627556800842, Accuracy: 1.0, Computation time: 1.2911481857299805\n",
      "Step: 2259, Loss: 0.9160409569740295, Accuracy: 1.0, Computation time: 1.2275667190551758\n",
      "Step: 2260, Loss: 0.9159471392631531, Accuracy: 1.0, Computation time: 1.2074158191680908\n",
      "Step: 2261, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 1.2630374431610107\n",
      "Step: 2262, Loss: 0.9159263968467712, Accuracy: 1.0, Computation time: 1.1860506534576416\n",
      "Step: 2263, Loss: 0.9159337282180786, Accuracy: 1.0, Computation time: 1.04225492477417\n",
      "Step: 2264, Loss: 0.9366109371185303, Accuracy: 0.9791666865348816, Computation time: 1.185469627380371\n",
      "Step: 2265, Loss: 0.9159334301948547, Accuracy: 1.0, Computation time: 0.9759399890899658\n",
      "Step: 2266, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.1970081329345703\n",
      "Step: 2267, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 1.3967561721801758\n",
      "Step: 2268, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.171875238418579\n",
      "Step: 2269, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.0370867252349854\n",
      "Step: 2270, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.2092487812042236\n",
      "Step: 2271, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.6480724811553955\n",
      "Step: 2272, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.573533296585083\n",
      "Step: 2273, Loss: 0.916473925113678, Accuracy: 1.0, Computation time: 1.4817731380462646\n",
      "Step: 2274, Loss: 0.935429573059082, Accuracy: 0.949999988079071, Computation time: 1.5268645286560059\n",
      "Step: 2275, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.162804126739502\n",
      "Step: 2276, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 1.1359741687774658\n",
      "Step: 2277, Loss: 0.9166162610054016, Accuracy: 1.0, Computation time: 1.0761146545410156\n",
      "Step: 2278, Loss: 0.9258583188056946, Accuracy: 0.9791666865348816, Computation time: 1.3241345882415771\n",
      "Step: 2279, Loss: 0.9159798622131348, Accuracy: 1.0, Computation time: 1.3073995113372803\n",
      "Step: 2280, Loss: 0.9159152507781982, Accuracy: 1.0, Computation time: 1.449944257736206\n",
      "Step: 2281, Loss: 0.9267757534980774, Accuracy: 0.9791666865348816, Computation time: 1.8438117504119873\n",
      "Step: 2282, Loss: 0.9161338210105896, Accuracy: 1.0, Computation time: 1.225266456604004\n",
      "Step: 2283, Loss: 0.9183354377746582, Accuracy: 1.0, Computation time: 2.48458194732666\n",
      "Step: 2284, Loss: 0.9159950017929077, Accuracy: 1.0, Computation time: 1.3050305843353271\n",
      "Step: 2285, Loss: 0.9160823225975037, Accuracy: 1.0, Computation time: 2.057445764541626\n",
      "Step: 2286, Loss: 0.9159924387931824, Accuracy: 1.0, Computation time: 1.542985200881958\n",
      "Step: 2287, Loss: 0.9159722328186035, Accuracy: 1.0, Computation time: 1.7591168880462646\n",
      "Step: 2288, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.679234504699707\n",
      "Step: 2289, Loss: 0.9166188836097717, Accuracy: 1.0, Computation time: 1.970644235610962\n",
      "Step: 2290, Loss: 0.9183827638626099, Accuracy: 1.0, Computation time: 1.9241082668304443\n",
      "Step: 2291, Loss: 0.9295527338981628, Accuracy: 0.9750000238418579, Computation time: 2.115576982498169\n",
      "Step: 2292, Loss: 0.9169121980667114, Accuracy: 1.0, Computation time: 1.4360778331756592\n",
      "Step: 2293, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.7154796123504639\n",
      "Step: 2294, Loss: 0.9376868605613708, Accuracy: 0.9772727489471436, Computation time: 1.9724643230438232\n",
      "Step: 2295, Loss: 0.9160096645355225, Accuracy: 1.0, Computation time: 1.8615925312042236\n",
      "Step: 2296, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.8017399311065674\n",
      "Step: 2297, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.468759298324585\n",
      "Step: 2298, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 2.877782106399536\n",
      "Step: 2299, Loss: 0.9162377715110779, Accuracy: 1.0, Computation time: 2.445695161819458\n",
      "Step: 2300, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.5905187129974365\n",
      "Step: 2301, Loss: 0.9161255359649658, Accuracy: 1.0, Computation time: 1.43392014503479\n",
      "Step: 2302, Loss: 0.9376022219657898, Accuracy: 0.949999988079071, Computation time: 1.327974557876587\n",
      "Step: 2303, Loss: 0.9162718057632446, Accuracy: 1.0, Computation time: 1.345151424407959\n",
      "Step: 2304, Loss: 0.9172353148460388, Accuracy: 1.0, Computation time: 1.5957484245300293\n",
      "Step: 2305, Loss: 0.9159398674964905, Accuracy: 1.0, Computation time: 2.1970531940460205\n",
      "Step: 2306, Loss: 0.9159578084945679, Accuracy: 1.0, Computation time: 1.6334407329559326\n",
      "Step: 2307, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.3959808349609375\n",
      "Step: 2308, Loss: 0.9163817167282104, Accuracy: 1.0, Computation time: 1.798351526260376\n",
      "Step: 2309, Loss: 0.9380393624305725, Accuracy: 0.9722222089767456, Computation time: 1.329568862915039\n",
      "Step: 2310, Loss: 0.9160683751106262, Accuracy: 1.0, Computation time: 1.3932838439941406\n",
      "Step: 2311, Loss: 0.9159753918647766, Accuracy: 1.0, Computation time: 1.6094274520874023\n",
      "Step: 2312, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 1.3216888904571533\n",
      "Step: 2313, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.2646832466125488\n",
      "Step: 2314, Loss: 0.9374973773956299, Accuracy: 0.9375, Computation time: 1.3513219356536865\n",
      "Step: 2315, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.8525018692016602\n",
      "Step: 2316, Loss: 0.9160085916519165, Accuracy: 1.0, Computation time: 2.1004817485809326\n",
      "Step: 2317, Loss: 0.9159692525863647, Accuracy: 1.0, Computation time: 1.5953121185302734\n",
      "Step: 2318, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 1.5867443084716797\n",
      "Step: 2319, Loss: 0.9375967383384705, Accuracy: 0.9642857313156128, Computation time: 1.3520457744598389\n",
      "Step: 2320, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 1.2773523330688477\n",
      "Step: 2321, Loss: 0.9346336722373962, Accuracy: 0.9791666865348816, Computation time: 1.2776975631713867\n",
      "Step: 2322, Loss: 0.9159868955612183, Accuracy: 1.0, Computation time: 1.693237543106079\n",
      "Step: 2323, Loss: 0.9159013628959656, Accuracy: 1.0, Computation time: 1.4686570167541504\n",
      "Step: 2324, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 1.2795729637145996\n",
      "Step: 2325, Loss: 0.9159389734268188, Accuracy: 1.0, Computation time: 1.198258638381958\n",
      "Step: 2326, Loss: 0.9376586079597473, Accuracy: 0.9722222089767456, Computation time: 1.4151899814605713\n",
      "Step: 2327, Loss: 0.9413022398948669, Accuracy: 0.9722222089767456, Computation time: 1.4898076057434082\n",
      "Step: 2328, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.1753709316253662\n",
      "Step: 2329, Loss: 0.9159460067749023, Accuracy: 1.0, Computation time: 1.7647578716278076\n",
      "Step: 2330, Loss: 0.9178335070610046, Accuracy: 1.0, Computation time: 1.4905450344085693\n",
      "Step: 2331, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.6694567203521729\n",
      "Step: 2332, Loss: 0.9160598516464233, Accuracy: 1.0, Computation time: 1.783738613128662\n",
      "Step: 2333, Loss: 0.9377917051315308, Accuracy: 0.96875, Computation time: 1.458078145980835\n",
      "Step: 2334, Loss: 0.9166377186775208, Accuracy: 1.0, Computation time: 1.3347890377044678\n",
      "Step: 2335, Loss: 0.9374895095825195, Accuracy: 0.9722222089767456, Computation time: 1.618605375289917\n",
      "Step: 2336, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.717792272567749\n",
      "Step: 2337, Loss: 0.9437189698219299, Accuracy: 0.9557693004608154, Computation time: 1.615032434463501\n",
      "Step: 2338, Loss: 0.9159793257713318, Accuracy: 1.0, Computation time: 1.2850127220153809\n",
      "Step: 2339, Loss: 0.9159863591194153, Accuracy: 1.0, Computation time: 1.346459150314331\n",
      "Step: 2340, Loss: 0.9165182113647461, Accuracy: 1.0, Computation time: 1.7314603328704834\n",
      "Step: 2341, Loss: 0.9160056114196777, Accuracy: 1.0, Computation time: 1.1822659969329834\n",
      "Step: 2342, Loss: 0.91676265001297, Accuracy: 1.0, Computation time: 1.3742425441741943\n",
      "Step: 2343, Loss: 0.9159753918647766, Accuracy: 1.0, Computation time: 1.3643133640289307\n",
      "Step: 2344, Loss: 0.9162572026252747, Accuracy: 1.0, Computation time: 1.298509120941162\n",
      "Step: 2345, Loss: 0.9376134276390076, Accuracy: 0.9642857313156128, Computation time: 0.995335578918457\n",
      "Step: 2346, Loss: 0.9220136404037476, Accuracy: 1.0, Computation time: 2.0966029167175293\n",
      "Step: 2347, Loss: 0.9160531759262085, Accuracy: 1.0, Computation time: 1.7693252563476562\n",
      "Step: 2348, Loss: 0.9376839399337769, Accuracy: 0.9583333730697632, Computation time: 1.4629714488983154\n",
      "Step: 2349, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.3754308223724365\n",
      "Step: 2350, Loss: 0.9159332513809204, Accuracy: 1.0, Computation time: 1.6042137145996094\n",
      "Step: 2351, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 1.2634475231170654\n",
      "Step: 2352, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 1.3587400913238525\n",
      "Step: 2353, Loss: 0.9266723990440369, Accuracy: 0.9642857313156128, Computation time: 1.4447267055511475\n",
      "Step: 2354, Loss: 0.9160477519035339, Accuracy: 1.0, Computation time: 1.5407259464263916\n",
      "Step: 2355, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.1225225925445557\n",
      "Step: 2356, Loss: 0.916347086429596, Accuracy: 1.0, Computation time: 1.3064994812011719\n",
      "Step: 2357, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 1.39345121383667\n",
      "Step: 2358, Loss: 0.9159688353538513, Accuracy: 1.0, Computation time: 1.2058947086334229\n",
      "Step: 2359, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 1.171147346496582\n",
      "Step: 2360, Loss: 0.9159330725669861, Accuracy: 1.0, Computation time: 1.2328741550445557\n",
      "Step: 2361, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 1.1021008491516113\n",
      "Step: 2362, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.1831798553466797\n",
      "########################\n",
      "Test loss: 1.0703322887420654, Test Accuracy_epoch17: 0.7665502429008484\n",
      "########################\n",
      "Step: 2363, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.4380743503570557\n",
      "Step: 2364, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 1.314544916152954\n",
      "Step: 2365, Loss: 0.916066586971283, Accuracy: 1.0, Computation time: 1.492138147354126\n",
      "Step: 2366, Loss: 0.937498152256012, Accuracy: 0.9642857313156128, Computation time: 1.361285924911499\n",
      "Step: 2367, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.3808624744415283\n",
      "Step: 2368, Loss: 0.9173689484596252, Accuracy: 1.0, Computation time: 1.4910171031951904\n",
      "Step: 2369, Loss: 0.9175361394882202, Accuracy: 1.0, Computation time: 1.3451573848724365\n",
      "Step: 2370, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.3975334167480469\n",
      "Step: 2371, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 1.1977779865264893\n",
      "Step: 2372, Loss: 0.937562108039856, Accuracy: 0.9750000238418579, Computation time: 1.2427139282226562\n",
      "Step: 2373, Loss: 0.9165491461753845, Accuracy: 1.0, Computation time: 1.6506221294403076\n",
      "Step: 2374, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.1713359355926514\n",
      "Step: 2375, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.3324451446533203\n",
      "Step: 2376, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.15641450881958\n",
      "Step: 2377, Loss: 0.9373769164085388, Accuracy: 0.9833333492279053, Computation time: 1.3106849193572998\n",
      "Step: 2378, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 1.175407886505127\n",
      "Step: 2379, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 1.4264726638793945\n",
      "Step: 2380, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 1.398827075958252\n",
      "Step: 2381, Loss: 0.9158753156661987, Accuracy: 1.0, Computation time: 1.1918714046478271\n",
      "Step: 2382, Loss: 0.9367213845252991, Accuracy: 0.9807692766189575, Computation time: 1.8178834915161133\n",
      "Step: 2383, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 1.4224047660827637\n",
      "Step: 2384, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.4012432098388672\n",
      "Step: 2385, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.512143611907959\n",
      "Step: 2386, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.5283534526824951\n",
      "Step: 2387, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.587116003036499\n",
      "Step: 2388, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 1.4001703262329102\n",
      "Step: 2389, Loss: 0.937531590461731, Accuracy: 0.9722222089767456, Computation time: 1.765704870223999\n",
      "Step: 2390, Loss: 0.9160841703414917, Accuracy: 1.0, Computation time: 1.3540904521942139\n",
      "Step: 2391, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 1.4349391460418701\n",
      "Step: 2392, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.31040620803833\n",
      "Step: 2393, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.314931869506836\n",
      "Step: 2394, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 1.600003957748413\n",
      "Step: 2395, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.3293442726135254\n",
      "Step: 2396, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.284428596496582\n",
      "Step: 2397, Loss: 0.9162157773971558, Accuracy: 1.0, Computation time: 1.289820909500122\n",
      "Step: 2398, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.3422141075134277\n",
      "Step: 2399, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.7210612297058105\n",
      "Step: 2400, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.4165370464324951\n",
      "Step: 2401, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.2224383354187012\n",
      "Step: 2402, Loss: 0.9226932525634766, Accuracy: 1.0, Computation time: 1.689814567565918\n",
      "Step: 2403, Loss: 0.9378367066383362, Accuracy: 0.96875, Computation time: 1.2899940013885498\n",
      "Step: 2404, Loss: 0.9210776090621948, Accuracy: 1.0, Computation time: 1.4206700325012207\n",
      "Step: 2405, Loss: 0.9159347414970398, Accuracy: 1.0, Computation time: 1.3993115425109863\n",
      "Step: 2406, Loss: 0.9159397482872009, Accuracy: 1.0, Computation time: 1.3555641174316406\n",
      "Step: 2407, Loss: 0.9160265326499939, Accuracy: 1.0, Computation time: 1.251612901687622\n",
      "Step: 2408, Loss: 0.9160997867584229, Accuracy: 1.0, Computation time: 1.7068688869476318\n",
      "Step: 2409, Loss: 0.9376487135887146, Accuracy: 0.9772727489471436, Computation time: 1.3262543678283691\n",
      "Step: 2410, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.387042760848999\n",
      "Step: 2411, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.3201398849487305\n",
      "Step: 2412, Loss: 0.9159773588180542, Accuracy: 1.0, Computation time: 1.3310267925262451\n",
      "Step: 2413, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.7066903114318848\n",
      "Step: 2414, Loss: 0.915959894657135, Accuracy: 1.0, Computation time: 1.687398910522461\n",
      "Step: 2415, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.424790859222412\n",
      "Step: 2416, Loss: 0.9190278649330139, Accuracy: 1.0, Computation time: 1.9349772930145264\n",
      "Step: 2417, Loss: 0.9378637075424194, Accuracy: 0.9821428656578064, Computation time: 2.323755979537964\n",
      "Step: 2418, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 1.4295930862426758\n",
      "Step: 2419, Loss: 0.9159627556800842, Accuracy: 1.0, Computation time: 1.2083580493927002\n",
      "Step: 2420, Loss: 0.9159400463104248, Accuracy: 1.0, Computation time: 1.3331167697906494\n",
      "Step: 2421, Loss: 0.9573572874069214, Accuracy: 0.9564393758773804, Computation time: 1.7854366302490234\n",
      "Step: 2422, Loss: 0.9161823391914368, Accuracy: 1.0, Computation time: 1.3472003936767578\n",
      "Step: 2423, Loss: 0.9162344336509705, Accuracy: 1.0, Computation time: 2.115110397338867\n",
      "Step: 2424, Loss: 0.9159934520721436, Accuracy: 1.0, Computation time: 1.6166846752166748\n",
      "Step: 2425, Loss: 0.9159616231918335, Accuracy: 1.0, Computation time: 1.3096656799316406\n",
      "Step: 2426, Loss: 0.9257690906524658, Accuracy: 0.9166666865348816, Computation time: 1.6425750255584717\n",
      "Step: 2427, Loss: 0.9159497618675232, Accuracy: 1.0, Computation time: 1.6679673194885254\n",
      "Step: 2428, Loss: 0.9165143966674805, Accuracy: 1.0, Computation time: 1.8489983081817627\n",
      "Step: 2429, Loss: 0.9350177049636841, Accuracy: 0.9807692766189575, Computation time: 1.7013859748840332\n",
      "Step: 2430, Loss: 0.9160116314888, Accuracy: 1.0, Computation time: 1.6962864398956299\n",
      "Step: 2431, Loss: 0.9160213470458984, Accuracy: 1.0, Computation time: 1.692413568496704\n",
      "Step: 2432, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 1.3542256355285645\n",
      "Step: 2433, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 1.5032095909118652\n",
      "Step: 2434, Loss: 0.9184831380844116, Accuracy: nan, Computation time: 1.5208544731140137\n",
      "Step: 2435, Loss: 0.9161604046821594, Accuracy: 1.0, Computation time: 1.5285677909851074\n",
      "Step: 2436, Loss: 0.9161520004272461, Accuracy: 1.0, Computation time: 1.5041389465332031\n",
      "Step: 2437, Loss: 0.916037380695343, Accuracy: 1.0, Computation time: 1.7835211753845215\n",
      "Step: 2438, Loss: 0.9375513792037964, Accuracy: 0.9772727489471436, Computation time: 1.4918320178985596\n",
      "Step: 2439, Loss: 0.9159300327301025, Accuracy: 1.0, Computation time: 1.4603478908538818\n",
      "Step: 2440, Loss: 0.9440964460372925, Accuracy: 0.9642857313156128, Computation time: 1.683009386062622\n",
      "Step: 2441, Loss: 0.9160540699958801, Accuracy: 1.0, Computation time: 1.6675450801849365\n",
      "Step: 2442, Loss: 0.9554002285003662, Accuracy: 0.9285714626312256, Computation time: 1.575871467590332\n",
      "Step: 2443, Loss: 0.9165562391281128, Accuracy: 1.0, Computation time: 1.2908275127410889\n",
      "Step: 2444, Loss: 0.916537344455719, Accuracy: 1.0, Computation time: 1.5098822116851807\n",
      "Step: 2445, Loss: 0.9162690043449402, Accuracy: 1.0, Computation time: 1.4756646156311035\n",
      "Step: 2446, Loss: 0.9161707162857056, Accuracy: 1.0, Computation time: 1.2332344055175781\n",
      "Step: 2447, Loss: 0.9160552620887756, Accuracy: 1.0, Computation time: 1.7625823020935059\n",
      "Step: 2448, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.6207091808319092\n",
      "Step: 2449, Loss: 0.9378224015235901, Accuracy: 0.9772727489471436, Computation time: 2.4929120540618896\n",
      "Step: 2450, Loss: 0.920190691947937, Accuracy: 1.0, Computation time: 1.7889001369476318\n",
      "Step: 2451, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 1.4653010368347168\n",
      "Step: 2452, Loss: 0.9160708785057068, Accuracy: 1.0, Computation time: 1.675013542175293\n",
      "Step: 2453, Loss: 0.9169523119926453, Accuracy: 1.0, Computation time: 1.550827980041504\n",
      "Step: 2454, Loss: 0.9160793423652649, Accuracy: 1.0, Computation time: 1.801818609237671\n",
      "Step: 2455, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 1.4630277156829834\n",
      "Step: 2456, Loss: 0.9159709215164185, Accuracy: 1.0, Computation time: 1.2995173931121826\n",
      "Step: 2457, Loss: 0.9163057208061218, Accuracy: 1.0, Computation time: 1.8312022686004639\n",
      "Step: 2458, Loss: 0.9335601329803467, Accuracy: 0.96875, Computation time: 1.628819465637207\n",
      "Step: 2459, Loss: 0.9413653016090393, Accuracy: 0.9791666865348816, Computation time: 2.1311588287353516\n",
      "Step: 2460, Loss: 0.9172033071517944, Accuracy: 1.0, Computation time: 1.5774390697479248\n",
      "Step: 2461, Loss: 0.9161390662193298, Accuracy: 1.0, Computation time: 2.002772092819214\n",
      "Step: 2462, Loss: 0.9161739945411682, Accuracy: 1.0, Computation time: 1.647221326828003\n",
      "Step: 2463, Loss: 0.9162726998329163, Accuracy: 1.0, Computation time: 1.8424787521362305\n",
      "Step: 2464, Loss: 0.9161185026168823, Accuracy: 1.0, Computation time: 1.711559772491455\n",
      "Step: 2465, Loss: 0.9380741715431213, Accuracy: 0.9750000238418579, Computation time: 1.4752318859100342\n",
      "Step: 2466, Loss: 0.9162595272064209, Accuracy: 1.0, Computation time: 1.4319062232971191\n",
      "Step: 2467, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 2.3424270153045654\n",
      "Step: 2468, Loss: 0.9160878658294678, Accuracy: 1.0, Computation time: 1.5779850482940674\n",
      "Step: 2469, Loss: 0.9159478545188904, Accuracy: 1.0, Computation time: 1.5027556419372559\n",
      "Step: 2470, Loss: 0.9162400364875793, Accuracy: 1.0, Computation time: 1.8676908016204834\n",
      "Step: 2471, Loss: 0.9160212278366089, Accuracy: 1.0, Computation time: 1.690999984741211\n",
      "Step: 2472, Loss: 0.9160215854644775, Accuracy: 1.0, Computation time: 1.3576533794403076\n",
      "Step: 2473, Loss: 0.9160952568054199, Accuracy: 1.0, Computation time: 1.5318024158477783\n",
      "Step: 2474, Loss: 0.9379332065582275, Accuracy: 0.9807692766189575, Computation time: 1.3956680297851562\n",
      "Step: 2475, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 1.409675121307373\n",
      "Step: 2476, Loss: 0.9161056876182556, Accuracy: 1.0, Computation time: 1.1048951148986816\n",
      "Step: 2477, Loss: 0.9378204345703125, Accuracy: 0.9772727489471436, Computation time: 1.5397474765777588\n",
      "Step: 2478, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 1.56339693069458\n",
      "Step: 2479, Loss: 0.9166290760040283, Accuracy: 1.0, Computation time: 1.379281997680664\n",
      "Step: 2480, Loss: 0.9161349534988403, Accuracy: 1.0, Computation time: 1.597989797592163\n",
      "Step: 2481, Loss: 0.9160729646682739, Accuracy: 1.0, Computation time: 1.6099040508270264\n",
      "Step: 2482, Loss: 0.9159854054450989, Accuracy: 1.0, Computation time: 1.3600614070892334\n",
      "Step: 2483, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.3390734195709229\n",
      "Step: 2484, Loss: 0.9375931620597839, Accuracy: 0.9750000238418579, Computation time: 1.4701824188232422\n",
      "Step: 2485, Loss: 0.9162948727607727, Accuracy: 1.0, Computation time: 1.6159346103668213\n",
      "Step: 2486, Loss: 0.9375686049461365, Accuracy: 0.9722222089767456, Computation time: 1.623211145401001\n",
      "Step: 2487, Loss: 0.9174565672874451, Accuracy: 1.0, Computation time: 2.4608917236328125\n",
      "Step: 2488, Loss: 0.9159788489341736, Accuracy: 1.0, Computation time: 1.2689745426177979\n",
      "Step: 2489, Loss: 0.9160203337669373, Accuracy: 1.0, Computation time: 1.2014472484588623\n",
      "Step: 2490, Loss: 0.9162654876708984, Accuracy: 1.0, Computation time: 1.2816357612609863\n",
      "Step: 2491, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 1.3137867450714111\n",
      "Step: 2492, Loss: 0.9159359335899353, Accuracy: 1.0, Computation time: 1.3690180778503418\n",
      "Step: 2493, Loss: 0.9255751371383667, Accuracy: 0.984375, Computation time: 1.6682772636413574\n",
      "Step: 2494, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 1.3794543743133545\n",
      "Step: 2495, Loss: 0.9159626960754395, Accuracy: 1.0, Computation time: 1.3231236934661865\n",
      "Step: 2496, Loss: 0.9160463213920593, Accuracy: 1.0, Computation time: 1.3446543216705322\n",
      "Step: 2497, Loss: 0.916039228439331, Accuracy: 1.0, Computation time: 1.4491546154022217\n",
      "Step: 2498, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.294844150543213\n",
      "Step: 2499, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.6488003730773926\n",
      "Step: 2500, Loss: 0.91591876745224, Accuracy: 1.0, Computation time: 1.9887025356292725\n",
      "Step: 2501, Loss: 0.9160446524620056, Accuracy: 1.0, Computation time: 1.4911816120147705\n",
      "########################\n",
      "Test loss: 1.0708924531936646, Test Accuracy_epoch18: 0.7653161883354187\n",
      "########################\n",
      "Step: 2502, Loss: 0.9159307479858398, Accuracy: 1.0, Computation time: 2.0227041244506836\n",
      "Step: 2503, Loss: 0.9513792395591736, Accuracy: 0.9272727370262146, Computation time: 1.4273300170898438\n",
      "Step: 2504, Loss: 0.9160499572753906, Accuracy: 1.0, Computation time: 1.4317052364349365\n",
      "Step: 2505, Loss: 0.9253931641578674, Accuracy: 0.9791666865348816, Computation time: 1.3136096000671387\n",
      "Step: 2506, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.7968997955322266\n",
      "Step: 2507, Loss: 0.9161455631256104, Accuracy: 1.0, Computation time: 1.453524112701416\n",
      "Step: 2508, Loss: 0.9162770509719849, Accuracy: 1.0, Computation time: 1.3110804557800293\n",
      "Step: 2509, Loss: 0.9435936808586121, Accuracy: 0.984375, Computation time: 1.647836446762085\n",
      "Step: 2510, Loss: 0.9161524772644043, Accuracy: 1.0, Computation time: 1.657688856124878\n",
      "Step: 2511, Loss: 0.9382526278495789, Accuracy: 0.9750000238418579, Computation time: 1.543459177017212\n",
      "Step: 2512, Loss: 0.9199194312095642, Accuracy: 1.0, Computation time: 1.5193605422973633\n",
      "Step: 2513, Loss: 0.9166139960289001, Accuracy: 1.0, Computation time: 1.9122507572174072\n",
      "Step: 2514, Loss: 0.9161185026168823, Accuracy: 1.0, Computation time: 1.3886077404022217\n",
      "Step: 2515, Loss: 0.9160693287849426, Accuracy: 1.0, Computation time: 1.657602071762085\n",
      "Step: 2516, Loss: 0.9160940051078796, Accuracy: 1.0, Computation time: 1.2795662879943848\n",
      "Step: 2517, Loss: 0.9159325361251831, Accuracy: 1.0, Computation time: 1.6062068939208984\n",
      "Step: 2518, Loss: 0.9164544343948364, Accuracy: 1.0, Computation time: 1.8748650550842285\n",
      "Step: 2519, Loss: 0.9162615537643433, Accuracy: 1.0, Computation time: 1.509671688079834\n",
      "Step: 2520, Loss: 0.916091799736023, Accuracy: 1.0, Computation time: 1.7247536182403564\n",
      "Step: 2521, Loss: 0.9161333441734314, Accuracy: 1.0, Computation time: 1.1210222244262695\n",
      "Step: 2522, Loss: 0.9175307154655457, Accuracy: 1.0, Computation time: 1.3743155002593994\n",
      "Step: 2523, Loss: 0.916690468788147, Accuracy: 1.0, Computation time: 1.5999231338500977\n",
      "Step: 2524, Loss: 0.9160890579223633, Accuracy: 1.0, Computation time: 1.3396968841552734\n",
      "Step: 2525, Loss: 0.9159578084945679, Accuracy: 1.0, Computation time: 1.305006980895996\n",
      "Step: 2526, Loss: 0.9364989995956421, Accuracy: 0.9772727489471436, Computation time: 1.2215306758880615\n",
      "Step: 2527, Loss: 0.9162364602088928, Accuracy: 1.0, Computation time: 1.0953023433685303\n",
      "Step: 2528, Loss: 0.9161480665206909, Accuracy: 1.0, Computation time: 1.612379789352417\n",
      "Step: 2529, Loss: 0.9161369800567627, Accuracy: 1.0, Computation time: 1.6544575691223145\n",
      "Step: 2530, Loss: 0.9159498810768127, Accuracy: 1.0, Computation time: 1.4186482429504395\n",
      "Step: 2531, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 1.3494653701782227\n",
      "Step: 2532, Loss: 0.9160857200622559, Accuracy: 1.0, Computation time: 1.7721707820892334\n",
      "Step: 2533, Loss: 0.9160135984420776, Accuracy: 1.0, Computation time: 1.7873742580413818\n",
      "Step: 2534, Loss: 0.9163863062858582, Accuracy: 1.0, Computation time: 1.5445339679718018\n",
      "Step: 2535, Loss: 0.9160963296890259, Accuracy: 1.0, Computation time: 1.9083302021026611\n",
      "Step: 2536, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.579378366470337\n",
      "Step: 2537, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 1.3373863697052002\n",
      "Step: 2538, Loss: 0.9370255470275879, Accuracy: 0.9821428656578064, Computation time: 1.4369375705718994\n",
      "Step: 2539, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 1.586094856262207\n",
      "Step: 2540, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 1.5437183380126953\n",
      "Step: 2541, Loss: 0.9160667657852173, Accuracy: 1.0, Computation time: 1.43275785446167\n",
      "Step: 2542, Loss: 0.9376749992370605, Accuracy: 0.9807692766189575, Computation time: 1.5600287914276123\n",
      "Step: 2543, Loss: 0.9160045385360718, Accuracy: 1.0, Computation time: 1.5475468635559082\n",
      "Step: 2544, Loss: 0.9159506559371948, Accuracy: 1.0, Computation time: 1.3792476654052734\n",
      "Step: 2545, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 1.1645162105560303\n",
      "Step: 2546, Loss: 0.9206914901733398, Accuracy: 1.0, Computation time: 1.6422264575958252\n",
      "Step: 2547, Loss: 0.9159843325614929, Accuracy: 1.0, Computation time: 1.3263928890228271\n",
      "Step: 2548, Loss: 0.9163278341293335, Accuracy: 1.0, Computation time: 1.5807626247406006\n",
      "Step: 2549, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 1.4092216491699219\n",
      "Step: 2550, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.5454201698303223\n",
      "Step: 2551, Loss: 0.9159554839134216, Accuracy: 1.0, Computation time: 1.3815195560455322\n",
      "Step: 2552, Loss: 0.9159426093101501, Accuracy: 1.0, Computation time: 1.3326900005340576\n",
      "Step: 2553, Loss: 0.9162525534629822, Accuracy: 1.0, Computation time: 1.5766823291778564\n",
      "Step: 2554, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 1.1358559131622314\n",
      "Step: 2555, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 1.5724539756774902\n",
      "Step: 2556, Loss: 0.9376963376998901, Accuracy: 0.9375, Computation time: 1.570422887802124\n",
      "Step: 2557, Loss: 0.9242908954620361, Accuracy: 1.0, Computation time: 1.3585901260375977\n",
      "Step: 2558, Loss: 0.91592937707901, Accuracy: 1.0, Computation time: 1.7257626056671143\n",
      "Step: 2559, Loss: 0.9160783290863037, Accuracy: 1.0, Computation time: 1.2307214736938477\n",
      "Step: 2560, Loss: 0.9160429835319519, Accuracy: 1.0, Computation time: 1.2198834419250488\n",
      "Step: 2561, Loss: 0.9376132488250732, Accuracy: 0.9166666865348816, Computation time: 1.5276708602905273\n",
      "Step: 2562, Loss: 0.916122555732727, Accuracy: 1.0, Computation time: 1.5394752025604248\n",
      "Step: 2563, Loss: 0.9161475300788879, Accuracy: 1.0, Computation time: 1.1841588020324707\n",
      "Step: 2564, Loss: 0.9160107374191284, Accuracy: 1.0, Computation time: 1.1470715999603271\n",
      "Step: 2565, Loss: 0.9160832166671753, Accuracy: 1.0, Computation time: 1.2596328258514404\n",
      "Step: 2566, Loss: 0.916312038898468, Accuracy: 1.0, Computation time: 1.1581637859344482\n",
      "Step: 2567, Loss: 0.9159662127494812, Accuracy: 1.0, Computation time: 1.165522813796997\n",
      "Step: 2568, Loss: 0.9377003312110901, Accuracy: 0.9722222089767456, Computation time: 1.2181973457336426\n",
      "Step: 2569, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.1859378814697266\n",
      "Step: 2570, Loss: 0.9202785491943359, Accuracy: 1.0, Computation time: 1.3797779083251953\n",
      "Step: 2571, Loss: 0.915950357913971, Accuracy: 1.0, Computation time: 1.2706403732299805\n",
      "Step: 2572, Loss: 0.9159677028656006, Accuracy: 1.0, Computation time: 1.3779747486114502\n",
      "Step: 2573, Loss: 0.9160529971122742, Accuracy: 1.0, Computation time: 1.0996088981628418\n",
      "Step: 2574, Loss: 0.9377533197402954, Accuracy: 0.9750000238418579, Computation time: 1.2685050964355469\n",
      "Step: 2575, Loss: 0.9159637689590454, Accuracy: 1.0, Computation time: 1.2078831195831299\n",
      "Step: 2576, Loss: 0.9159391522407532, Accuracy: 1.0, Computation time: 1.4032890796661377\n",
      "Step: 2577, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 1.5123791694641113\n",
      "Step: 2578, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.1744396686553955\n",
      "Step: 2579, Loss: 0.915947437286377, Accuracy: 1.0, Computation time: 1.383331298828125\n",
      "Step: 2580, Loss: 0.9159913063049316, Accuracy: 1.0, Computation time: 1.516176700592041\n",
      "Step: 2581, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.2980272769927979\n",
      "Step: 2582, Loss: 0.9374833703041077, Accuracy: 0.9642857313156128, Computation time: 1.1450254917144775\n",
      "Step: 2583, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.7767856121063232\n",
      "Step: 2584, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 1.3381991386413574\n",
      "Step: 2585, Loss: 0.915958821773529, Accuracy: 1.0, Computation time: 1.1079342365264893\n",
      "Step: 2586, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 1.1871135234832764\n",
      "Step: 2587, Loss: 0.9357174634933472, Accuracy: 0.9750000238418579, Computation time: 1.6048364639282227\n",
      "Step: 2588, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.3350105285644531\n",
      "Step: 2589, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 1.7382586002349854\n",
      "Step: 2590, Loss: 0.9160768985748291, Accuracy: 1.0, Computation time: 1.5986528396606445\n",
      "Step: 2591, Loss: 0.9167017936706543, Accuracy: 1.0, Computation time: 1.2298407554626465\n",
      "Step: 2592, Loss: 0.9164335131645203, Accuracy: 1.0, Computation time: 2.0202629566192627\n",
      "Step: 2593, Loss: 0.9160751700401306, Accuracy: 1.0, Computation time: 1.2923426628112793\n",
      "Step: 2594, Loss: 0.9160059690475464, Accuracy: 1.0, Computation time: 1.3451085090637207\n",
      "Step: 2595, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 1.4354870319366455\n",
      "Step: 2596, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 1.3049981594085693\n",
      "Step: 2597, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.738952398300171\n",
      "Step: 2598, Loss: 0.9375850558280945, Accuracy: 0.949999988079071, Computation time: 1.5026452541351318\n",
      "Step: 2599, Loss: 0.9181357622146606, Accuracy: 1.0, Computation time: 1.4970529079437256\n",
      "Step: 2600, Loss: 0.9159476161003113, Accuracy: 1.0, Computation time: 1.4141781330108643\n",
      "Step: 2601, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 1.9044318199157715\n",
      "Step: 2602, Loss: 0.915960431098938, Accuracy: 1.0, Computation time: 1.5264947414398193\n",
      "Step: 2603, Loss: 0.927020788192749, Accuracy: 0.9807692766189575, Computation time: 2.1422958374023438\n",
      "Step: 2604, Loss: 0.9160557985305786, Accuracy: 1.0, Computation time: 1.4772942066192627\n",
      "Step: 2605, Loss: 0.9159590005874634, Accuracy: 1.0, Computation time: 2.0745718479156494\n",
      "Step: 2606, Loss: 0.9159788489341736, Accuracy: 1.0, Computation time: 1.5870699882507324\n",
      "Step: 2607, Loss: 0.9162113666534424, Accuracy: 1.0, Computation time: 1.4862306118011475\n",
      "Step: 2608, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 1.5720837116241455\n",
      "Step: 2609, Loss: 0.9159832000732422, Accuracy: 1.0, Computation time: 1.3758184909820557\n",
      "Step: 2610, Loss: 0.9373154044151306, Accuracy: 0.9791666865348816, Computation time: 1.469447374343872\n",
      "Step: 2611, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.1624550819396973\n",
      "Step: 2612, Loss: 0.9165582656860352, Accuracy: 1.0, Computation time: 1.7718119621276855\n",
      "Step: 2613, Loss: 0.9376054406166077, Accuracy: 0.9791666865348816, Computation time: 1.6163253784179688\n",
      "Step: 2614, Loss: 0.9176609516143799, Accuracy: 1.0, Computation time: 1.5103845596313477\n",
      "Step: 2615, Loss: 0.9159400463104248, Accuracy: 1.0, Computation time: 1.465907096862793\n",
      "Step: 2616, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.5166504383087158\n",
      "Step: 2617, Loss: 0.9159598350524902, Accuracy: 1.0, Computation time: 1.5001320838928223\n",
      "Step: 2618, Loss: 0.9162521362304688, Accuracy: 1.0, Computation time: 1.5118193626403809\n",
      "Step: 2619, Loss: 0.9161284565925598, Accuracy: 1.0, Computation time: 1.4228479862213135\n",
      "Step: 2620, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 1.6468243598937988\n",
      "Step: 2621, Loss: 0.941908061504364, Accuracy: 0.9642857313156128, Computation time: 2.070404529571533\n",
      "Step: 2622, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 1.4449808597564697\n",
      "Step: 2623, Loss: 0.9375757575035095, Accuracy: 0.9642857313156128, Computation time: 1.872196912765503\n",
      "Step: 2624, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.4369308948516846\n",
      "Step: 2625, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.1708183288574219\n",
      "Step: 2626, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.3503928184509277\n",
      "Step: 2627, Loss: 0.9159312844276428, Accuracy: 1.0, Computation time: 1.3268287181854248\n",
      "Step: 2628, Loss: 0.9173991680145264, Accuracy: 1.0, Computation time: 1.8003265857696533\n",
      "Step: 2629, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.600867509841919\n",
      "Step: 2630, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.3948757648468018\n",
      "Step: 2631, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.428229570388794\n",
      "Step: 2632, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.6006126403808594\n",
      "Step: 2633, Loss: 0.9159814119338989, Accuracy: 1.0, Computation time: 1.460193157196045\n",
      "Step: 2634, Loss: 0.9379134178161621, Accuracy: 0.9722222089767456, Computation time: 1.8118371963500977\n",
      "Step: 2635, Loss: 0.9159137606620789, Accuracy: 1.0, Computation time: 1.479072093963623\n",
      "Step: 2636, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.3170387744903564\n",
      "Step: 2637, Loss: 0.9159747958183289, Accuracy: 1.0, Computation time: 1.7807841300964355\n",
      "Step: 2638, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.7748298645019531\n",
      "Step: 2639, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 1.626354455947876\n",
      "Step: 2640, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.6836621761322021\n",
      "########################\n",
      "Test loss: 1.073207974433899, Test Accuracy_epoch19: 0.7558570504188538\n",
      "########################\n",
      "Step: 2641, Loss: 0.9296965003013611, Accuracy: 0.9642857313156128, Computation time: 1.7113757133483887\n",
      "Step: 2642, Loss: 0.915944516658783, Accuracy: 1.0, Computation time: 1.6638154983520508\n",
      "Step: 2643, Loss: 0.9375879764556885, Accuracy: 0.984375, Computation time: 1.6710598468780518\n",
      "Step: 2644, Loss: 0.9159582257270813, Accuracy: 1.0, Computation time: 1.758265495300293\n",
      "Step: 2645, Loss: 0.9161025285720825, Accuracy: 1.0, Computation time: 1.5683026313781738\n",
      "Step: 2646, Loss: 0.9160192608833313, Accuracy: 1.0, Computation time: 1.3316371440887451\n",
      "Step: 2647, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 1.4709084033966064\n",
      "Step: 2648, Loss: 0.9159765243530273, Accuracy: 1.0, Computation time: 1.5664036273956299\n",
      "Step: 2649, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 1.252382755279541\n",
      "Step: 2650, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 1.937896490097046\n",
      "Step: 2651, Loss: 0.9581825733184814, Accuracy: 0.9444444179534912, Computation time: 1.6312875747680664\n",
      "Step: 2652, Loss: 0.9335944056510925, Accuracy: 0.96875, Computation time: 1.483677864074707\n",
      "Step: 2653, Loss: 0.9161439538002014, Accuracy: 1.0, Computation time: 1.7984998226165771\n",
      "Step: 2654, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.8100917339324951\n",
      "Step: 2655, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 1.7617886066436768\n",
      "Step: 2656, Loss: 0.9160553216934204, Accuracy: 1.0, Computation time: 1.3933868408203125\n",
      "Step: 2657, Loss: 0.9160383343696594, Accuracy: 1.0, Computation time: 2.3152246475219727\n",
      "Step: 2658, Loss: 0.9168913960456848, Accuracy: 1.0, Computation time: 1.456474781036377\n",
      "Step: 2659, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.681854486465454\n",
      "Step: 2660, Loss: 0.9191103577613831, Accuracy: 1.0, Computation time: 1.8018651008605957\n",
      "Step: 2661, Loss: 0.9158928394317627, Accuracy: 1.0, Computation time: 1.7013461589813232\n",
      "Step: 2662, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.960808515548706\n",
      "Step: 2663, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 1.4383361339569092\n",
      "Step: 2664, Loss: 0.9376184344291687, Accuracy: 0.9772727489471436, Computation time: 1.5050163269042969\n",
      "Step: 2665, Loss: 0.9159640073776245, Accuracy: 1.0, Computation time: 1.7401695251464844\n",
      "Step: 2666, Loss: 0.9391309022903442, Accuracy: 0.96875, Computation time: 1.4315032958984375\n",
      "Step: 2667, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 1.4419035911560059\n",
      "Step: 2668, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.4814271926879883\n",
      "Step: 2669, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 1.5415732860565186\n",
      "Step: 2670, Loss: 0.9163084626197815, Accuracy: 1.0, Computation time: 1.6193840503692627\n",
      "Step: 2671, Loss: 0.9159377217292786, Accuracy: 1.0, Computation time: 1.3340520858764648\n",
      "Step: 2672, Loss: 0.9160065650939941, Accuracy: 1.0, Computation time: 1.4518208503723145\n",
      "Step: 2673, Loss: 0.9375202655792236, Accuracy: 0.9772727489471436, Computation time: 1.4393310546875\n",
      "Step: 2674, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.5243701934814453\n",
      "Step: 2675, Loss: 0.9160838723182678, Accuracy: 1.0, Computation time: 1.4078140258789062\n",
      "Step: 2676, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.520320177078247\n",
      "Step: 2677, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 1.4809415340423584\n",
      "Step: 2678, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.4968407154083252\n",
      "Step: 2679, Loss: 0.9172283411026001, Accuracy: 1.0, Computation time: 1.6551177501678467\n",
      "Step: 2680, Loss: 0.9159327745437622, Accuracy: 1.0, Computation time: 1.5708589553833008\n",
      "Step: 2681, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 2.2254979610443115\n",
      "Step: 2682, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.3707275390625\n",
      "Step: 2683, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.3797614574432373\n",
      "Step: 2684, Loss: 0.9159635305404663, Accuracy: 1.0, Computation time: 1.063213586807251\n",
      "Step: 2685, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.3570432662963867\n",
      "Step: 2686, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 1.3803038597106934\n",
      "Step: 2687, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.7723016738891602\n",
      "Step: 2688, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.4974150657653809\n",
      "Step: 2689, Loss: 0.9159042239189148, Accuracy: 1.0, Computation time: 1.4917173385620117\n",
      "Step: 2690, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.4178626537322998\n",
      "Step: 2691, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.6534759998321533\n",
      "Step: 2692, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 1.6771681308746338\n",
      "Step: 2693, Loss: 0.9162935018539429, Accuracy: 1.0, Computation time: 1.8318002223968506\n",
      "Step: 2694, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.384993314743042\n",
      "Step: 2695, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.6571969985961914\n",
      "Step: 2696, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.9565684795379639\n",
      "Step: 2697, Loss: 0.9163832664489746, Accuracy: 1.0, Computation time: 1.381366491317749\n",
      "Step: 2698, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.5792796611785889\n",
      "Step: 2699, Loss: 0.9376004934310913, Accuracy: 0.96875, Computation time: 1.662614107131958\n",
      "Step: 2700, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.3880774974822998\n",
      "Step: 2701, Loss: 0.9195621609687805, Accuracy: 1.0, Computation time: 1.409087896347046\n",
      "Step: 2702, Loss: 0.937400758266449, Accuracy: 0.9821428656578064, Computation time: 1.888188123703003\n",
      "Step: 2703, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.5335273742675781\n",
      "Step: 2704, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 1.8095283508300781\n",
      "Step: 2705, Loss: 0.9159477353096008, Accuracy: 1.0, Computation time: 1.7143840789794922\n",
      "Step: 2706, Loss: 0.9159898161888123, Accuracy: 1.0, Computation time: 1.4307036399841309\n",
      "Step: 2707, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 2.025148391723633\n",
      "Step: 2708, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.3646197319030762\n",
      "Step: 2709, Loss: 0.9164556860923767, Accuracy: 1.0, Computation time: 1.9235610961914062\n",
      "Step: 2710, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.7375133037567139\n",
      "Step: 2711, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.6725420951843262\n",
      "Step: 2712, Loss: 0.9201045632362366, Accuracy: 1.0, Computation time: 2.1723473072052\n",
      "Step: 2713, Loss: 0.9159286618232727, Accuracy: 1.0, Computation time: 1.9174625873565674\n",
      "Step: 2714, Loss: 0.9217053651809692, Accuracy: 1.0, Computation time: 1.4305078983306885\n",
      "Step: 2715, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 1.3862738609313965\n",
      "Step: 2716, Loss: 0.9160661101341248, Accuracy: 1.0, Computation time: 2.3899447917938232\n",
      "Step: 2717, Loss: 0.9161847233772278, Accuracy: 1.0, Computation time: 1.3187575340270996\n",
      "Step: 2718, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.4725394248962402\n",
      "Step: 2719, Loss: 0.9159488081932068, Accuracy: 1.0, Computation time: 1.9628691673278809\n",
      "Step: 2720, Loss: 0.9288923144340515, Accuracy: 0.9375, Computation time: 2.094820976257324\n",
      "Step: 2721, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 1.3672904968261719\n",
      "Step: 2722, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 1.6221351623535156\n",
      "Step: 2723, Loss: 0.9160627722740173, Accuracy: 1.0, Computation time: 1.6292662620544434\n",
      "Step: 2724, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 2.0449671745300293\n",
      "Step: 2725, Loss: 0.9160606861114502, Accuracy: 1.0, Computation time: 1.4814090728759766\n",
      "Step: 2726, Loss: 0.9161645770072937, Accuracy: 1.0, Computation time: 1.7239651679992676\n",
      "Step: 2727, Loss: 0.9159078001976013, Accuracy: 1.0, Computation time: 2.7039809226989746\n",
      "Step: 2728, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 2.5660560131073\n",
      "Step: 2729, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.4107823371887207\n",
      "Step: 2730, Loss: 0.9164853692054749, Accuracy: 1.0, Computation time: 1.586240291595459\n",
      "Step: 2731, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 1.4292819499969482\n",
      "Step: 2732, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.6573657989501953\n",
      "Step: 2733, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 1.3319237232208252\n",
      "Step: 2734, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.3245370388031006\n",
      "Step: 2735, Loss: 0.9159273505210876, Accuracy: 1.0, Computation time: 1.6018218994140625\n",
      "Step: 2736, Loss: 0.9339604377746582, Accuracy: 0.9821428656578064, Computation time: 2.2769923210144043\n",
      "Step: 2737, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.1990463733673096\n",
      "Step: 2738, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.5478637218475342\n",
      "Step: 2739, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.2255573272705078\n",
      "Step: 2740, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.1783161163330078\n",
      "Step: 2741, Loss: 0.9159854054450989, Accuracy: 1.0, Computation time: 1.173997163772583\n",
      "Step: 2742, Loss: 0.9160386323928833, Accuracy: 1.0, Computation time: 1.196763038635254\n",
      "Step: 2743, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 1.212615728378296\n",
      "Step: 2744, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 1.5835559368133545\n",
      "Step: 2745, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.2399158477783203\n",
      "Step: 2746, Loss: 0.9269176721572876, Accuracy: 0.9772727489471436, Computation time: 1.3095369338989258\n",
      "Step: 2747, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.3092999458312988\n",
      "Step: 2748, Loss: 0.9369056224822998, Accuracy: 0.984375, Computation time: 1.7017395496368408\n",
      "Step: 2749, Loss: 0.9160099625587463, Accuracy: 1.0, Computation time: 1.5412569046020508\n",
      "Step: 2750, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.4637339115142822\n",
      "Step: 2751, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.230994701385498\n",
      "Step: 2752, Loss: 0.9376764297485352, Accuracy: 0.9583333730697632, Computation time: 1.440474033355713\n",
      "Step: 2753, Loss: 0.916039764881134, Accuracy: 1.0, Computation time: 1.4018733501434326\n",
      "Step: 2754, Loss: 0.9164847731590271, Accuracy: 1.0, Computation time: 1.7096765041351318\n",
      "Step: 2755, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.7187435626983643\n",
      "Step: 2756, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.4427571296691895\n",
      "Step: 2757, Loss: 0.9483439326286316, Accuracy: 0.9434524178504944, Computation time: 1.9838840961456299\n",
      "Step: 2758, Loss: 0.915969729423523, Accuracy: 1.0, Computation time: 2.097184419631958\n",
      "Step: 2759, Loss: 0.9161364436149597, Accuracy: 1.0, Computation time: 1.712568759918213\n",
      "Step: 2760, Loss: 0.9173775911331177, Accuracy: 1.0, Computation time: 1.7512638568878174\n",
      "Step: 2761, Loss: 0.9196884632110596, Accuracy: 1.0, Computation time: 1.6162958145141602\n",
      "Step: 2762, Loss: 0.9160909056663513, Accuracy: 1.0, Computation time: 1.2401258945465088\n",
      "Step: 2763, Loss: 0.9160128235816956, Accuracy: 1.0, Computation time: 1.279475450515747\n",
      "Step: 2764, Loss: 0.9162379503250122, Accuracy: 1.0, Computation time: 1.2027735710144043\n",
      "Step: 2765, Loss: 0.9417709708213806, Accuracy: 0.9722222089767456, Computation time: 1.2527339458465576\n",
      "Step: 2766, Loss: 0.9160035848617554, Accuracy: 1.0, Computation time: 1.2232208251953125\n",
      "Step: 2767, Loss: 0.917739748954773, Accuracy: 1.0, Computation time: 1.3555817604064941\n",
      "Step: 2768, Loss: 0.916102409362793, Accuracy: 1.0, Computation time: 1.3674650192260742\n",
      "Step: 2769, Loss: 0.9160687327384949, Accuracy: 1.0, Computation time: 1.1340677738189697\n",
      "Step: 2770, Loss: 0.9376727938652039, Accuracy: 0.9772727489471436, Computation time: 1.1499013900756836\n",
      "Step: 2771, Loss: 0.9160000085830688, Accuracy: 1.0, Computation time: 1.4320604801177979\n",
      "Step: 2772, Loss: 0.9160823225975037, Accuracy: 1.0, Computation time: 1.1395182609558105\n",
      "Step: 2773, Loss: 0.9297935366630554, Accuracy: 0.9642857313156128, Computation time: 1.2473819255828857\n",
      "Step: 2774, Loss: 0.9376910328865051, Accuracy: 0.9833333492279053, Computation time: 1.4901998043060303\n",
      "Step: 2775, Loss: 0.9161339998245239, Accuracy: 1.0, Computation time: 1.3364064693450928\n",
      "Step: 2776, Loss: 0.9161694645881653, Accuracy: 1.0, Computation time: 1.5772123336791992\n",
      "Step: 2777, Loss: 0.9161686897277832, Accuracy: 1.0, Computation time: 1.3470056056976318\n",
      "Step: 2778, Loss: 0.9447779059410095, Accuracy: 0.9807692766189575, Computation time: 1.5053231716156006\n",
      "Step: 2779, Loss: 0.9575268626213074, Accuracy: 0.9545454978942871, Computation time: 1.957747459411621\n",
      "########################\n",
      "Test loss: 1.0739662647247314, Test Accuracy_epoch20: 0.7636635899543762\n",
      "########################\n",
      "Step: 2780, Loss: 0.937820315361023, Accuracy: 0.9791666865348816, Computation time: 1.2468912601470947\n",
      "Step: 2781, Loss: 0.9174520373344421, Accuracy: 1.0, Computation time: 1.3447785377502441\n",
      "Step: 2782, Loss: 0.9197604060173035, Accuracy: 1.0, Computation time: 2.12797212600708\n",
      "Step: 2783, Loss: 0.9390628933906555, Accuracy: 0.9772727489471436, Computation time: 1.794123888015747\n",
      "Step: 2784, Loss: 0.9164316058158875, Accuracy: 1.0, Computation time: 1.417931318283081\n",
      "Step: 2785, Loss: 0.9375230073928833, Accuracy: 0.9772727489471436, Computation time: 1.3334331512451172\n",
      "Step: 2786, Loss: 0.9162064790725708, Accuracy: 1.0, Computation time: 1.3175454139709473\n",
      "Step: 2787, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 1.699007272720337\n",
      "Step: 2788, Loss: 0.9164859056472778, Accuracy: 1.0, Computation time: 1.3223049640655518\n",
      "Step: 2789, Loss: 0.9165104031562805, Accuracy: 1.0, Computation time: 1.3578827381134033\n",
      "Step: 2790, Loss: 0.9161286354064941, Accuracy: 1.0, Computation time: 1.390763521194458\n",
      "Step: 2791, Loss: 0.9163901805877686, Accuracy: 1.0, Computation time: 1.513537883758545\n",
      "Step: 2792, Loss: 0.916313648223877, Accuracy: 1.0, Computation time: 1.189971923828125\n",
      "Step: 2793, Loss: 0.9369990229606628, Accuracy: 0.9642857313156128, Computation time: 1.4825232028961182\n",
      "Step: 2794, Loss: 0.9160912036895752, Accuracy: 1.0, Computation time: 1.118088960647583\n",
      "Step: 2795, Loss: 0.9159797430038452, Accuracy: 1.0, Computation time: 1.4438209533691406\n",
      "Step: 2796, Loss: 0.9164021611213684, Accuracy: 1.0, Computation time: 1.3921566009521484\n",
      "Step: 2797, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 1.5052490234375\n",
      "Step: 2798, Loss: 0.956195056438446, Accuracy: 0.9642857313156128, Computation time: 1.4449923038482666\n",
      "Step: 2799, Loss: 0.9162614941596985, Accuracy: 1.0, Computation time: 1.2931759357452393\n",
      "Step: 2800, Loss: 0.9160764813423157, Accuracy: 1.0, Computation time: 1.3712995052337646\n",
      "Step: 2801, Loss: 0.93748539686203, Accuracy: 0.9821428656578064, Computation time: 1.2929048538208008\n",
      "Step: 2802, Loss: 0.9162238836288452, Accuracy: 1.0, Computation time: 1.4765760898590088\n",
      "Step: 2803, Loss: 0.9168599843978882, Accuracy: 1.0, Computation time: 1.4426865577697754\n",
      "Step: 2804, Loss: 0.916083812713623, Accuracy: 1.0, Computation time: 1.5229222774505615\n",
      "Step: 2805, Loss: 0.9160435199737549, Accuracy: 1.0, Computation time: 1.6292157173156738\n",
      "Step: 2806, Loss: 0.9160550236701965, Accuracy: 1.0, Computation time: 1.3798136711120605\n",
      "Step: 2807, Loss: 0.9164028167724609, Accuracy: 1.0, Computation time: 1.6893467903137207\n",
      "Step: 2808, Loss: 0.9161954522132874, Accuracy: 1.0, Computation time: 1.325476884841919\n",
      "Step: 2809, Loss: 0.9164714217185974, Accuracy: 1.0, Computation time: 1.278073787689209\n",
      "Step: 2810, Loss: 0.9162106513977051, Accuracy: 1.0, Computation time: 1.508329153060913\n",
      "Step: 2811, Loss: 0.9159778356552124, Accuracy: 1.0, Computation time: 1.4161434173583984\n",
      "Step: 2812, Loss: 0.9374367594718933, Accuracy: 0.9807692766189575, Computation time: 1.3688437938690186\n",
      "Step: 2813, Loss: 0.9161980152130127, Accuracy: 1.0, Computation time: 1.2685456275939941\n",
      "Step: 2814, Loss: 0.951800525188446, Accuracy: 0.9615384340286255, Computation time: 1.5565359592437744\n",
      "Step: 2815, Loss: 0.916840672492981, Accuracy: 1.0, Computation time: 1.4344134330749512\n",
      "Step: 2816, Loss: 0.9162489175796509, Accuracy: 1.0, Computation time: 1.2058460712432861\n",
      "Step: 2817, Loss: 0.9166940450668335, Accuracy: 1.0, Computation time: 1.3725833892822266\n",
      "Step: 2818, Loss: 0.9229173064231873, Accuracy: 1.0, Computation time: 1.8060619831085205\n",
      "Step: 2819, Loss: 0.9169593453407288, Accuracy: 1.0, Computation time: 1.2819504737854004\n",
      "Step: 2820, Loss: 0.916296124458313, Accuracy: 1.0, Computation time: 1.386270523071289\n",
      "Step: 2821, Loss: 0.9173781275749207, Accuracy: 1.0, Computation time: 2.1188693046569824\n",
      "Step: 2822, Loss: 0.9163966774940491, Accuracy: 1.0, Computation time: 1.3532657623291016\n",
      "Step: 2823, Loss: 0.9164152145385742, Accuracy: 1.0, Computation time: 1.0985510349273682\n",
      "Step: 2824, Loss: 0.9169398546218872, Accuracy: 1.0, Computation time: 1.2786903381347656\n",
      "Step: 2825, Loss: 0.9256126284599304, Accuracy: 1.0, Computation time: 1.2940607070922852\n",
      "Step: 2826, Loss: 0.9166685938835144, Accuracy: 1.0, Computation time: 1.168766736984253\n",
      "Step: 2827, Loss: 0.9163298010826111, Accuracy: 1.0, Computation time: 1.2365081310272217\n",
      "Step: 2828, Loss: 0.9162488579750061, Accuracy: 1.0, Computation time: 1.1315631866455078\n",
      "Step: 2829, Loss: 0.938633143901825, Accuracy: 0.9807692766189575, Computation time: 1.1171388626098633\n",
      "Step: 2830, Loss: 0.9162449836730957, Accuracy: 1.0, Computation time: 1.1341066360473633\n",
      "Step: 2831, Loss: 0.9164801836013794, Accuracy: 1.0, Computation time: 1.1215860843658447\n",
      "Step: 2832, Loss: 0.9161942601203918, Accuracy: 1.0, Computation time: 1.2941093444824219\n",
      "Step: 2833, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 1.5751569271087646\n",
      "Step: 2834, Loss: 0.9164018630981445, Accuracy: 1.0, Computation time: 1.1629843711853027\n",
      "Step: 2835, Loss: 0.9378989338874817, Accuracy: 0.9772727489471436, Computation time: 1.1700477600097656\n",
      "Step: 2836, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.1297168731689453\n",
      "Step: 2837, Loss: 0.9375762343406677, Accuracy: 0.96875, Computation time: 1.914585828781128\n",
      "Step: 2838, Loss: 0.9372991323471069, Accuracy: 0.9807692766189575, Computation time: 1.4110136032104492\n",
      "Step: 2839, Loss: 0.9160237312316895, Accuracy: 1.0, Computation time: 1.328890085220337\n",
      "Step: 2840, Loss: 0.9563446044921875, Accuracy: 0.9545454978942871, Computation time: 1.331921100616455\n",
      "Step: 2841, Loss: 0.9159975647926331, Accuracy: 1.0, Computation time: 1.2050955295562744\n",
      "Step: 2842, Loss: 0.9161025881767273, Accuracy: 1.0, Computation time: 1.38901948928833\n",
      "Step: 2843, Loss: 0.9160206317901611, Accuracy: 1.0, Computation time: 1.8174867630004883\n",
      "Step: 2844, Loss: 0.9167095422744751, Accuracy: 1.0, Computation time: 1.3316104412078857\n",
      "Step: 2845, Loss: 0.9160436987876892, Accuracy: 1.0, Computation time: 1.7783441543579102\n",
      "Step: 2846, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.345306396484375\n",
      "Step: 2847, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.36871337890625\n",
      "Step: 2848, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.0670664310455322\n",
      "Step: 2849, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.364769697189331\n",
      "Step: 2850, Loss: 0.9159937500953674, Accuracy: 1.0, Computation time: 1.5171730518341064\n",
      "Step: 2851, Loss: 0.9373844861984253, Accuracy: 0.9791666865348816, Computation time: 1.437990665435791\n",
      "Step: 2852, Loss: 0.9210068583488464, Accuracy: 1.0, Computation time: 1.7464525699615479\n",
      "Step: 2853, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 1.4046649932861328\n",
      "Step: 2854, Loss: 0.9167510271072388, Accuracy: 1.0, Computation time: 1.2686607837677002\n",
      "Step: 2855, Loss: 0.9160423874855042, Accuracy: 1.0, Computation time: 1.7704839706420898\n",
      "Step: 2856, Loss: 0.9159556031227112, Accuracy: 1.0, Computation time: 2.08819317817688\n",
      "Step: 2857, Loss: 0.9208508133888245, Accuracy: 1.0, Computation time: 1.5183429718017578\n",
      "Step: 2858, Loss: 0.9159451723098755, Accuracy: 1.0, Computation time: 1.4654269218444824\n",
      "Step: 2859, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 1.0966382026672363\n",
      "Step: 2860, Loss: 0.9160265326499939, Accuracy: 1.0, Computation time: 1.421302318572998\n",
      "Step: 2861, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.3111813068389893\n",
      "Step: 2862, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 1.5313994884490967\n",
      "Step: 2863, Loss: 0.9161771535873413, Accuracy: 1.0, Computation time: 1.2919974327087402\n",
      "Step: 2864, Loss: 0.9159770011901855, Accuracy: 1.0, Computation time: 1.5223002433776855\n",
      "Step: 2865, Loss: 0.9160670638084412, Accuracy: 1.0, Computation time: 1.1707639694213867\n",
      "Step: 2866, Loss: 0.916102409362793, Accuracy: 1.0, Computation time: 1.2040841579437256\n",
      "Step: 2867, Loss: 0.9161081910133362, Accuracy: 1.0, Computation time: 1.7154245376586914\n",
      "Step: 2868, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.3365988731384277\n",
      "Step: 2869, Loss: 0.9168862104415894, Accuracy: 1.0, Computation time: 1.4669411182403564\n",
      "Step: 2870, Loss: 0.9273250699043274, Accuracy: 0.9772727489471436, Computation time: 1.6374139785766602\n",
      "Step: 2871, Loss: 0.9161428213119507, Accuracy: 1.0, Computation time: 1.3296024799346924\n",
      "Step: 2872, Loss: 0.9163075089454651, Accuracy: 1.0, Computation time: 1.4099559783935547\n",
      "Step: 2873, Loss: 0.9161975979804993, Accuracy: 1.0, Computation time: 1.622969627380371\n",
      "Step: 2874, Loss: 0.9160784482955933, Accuracy: 1.0, Computation time: 1.4163460731506348\n",
      "Step: 2875, Loss: 0.9161446690559387, Accuracy: 1.0, Computation time: 1.7294704914093018\n",
      "Step: 2876, Loss: 0.9377225637435913, Accuracy: 0.9750000238418579, Computation time: 1.5609891414642334\n",
      "Step: 2877, Loss: 0.9310649633407593, Accuracy: 0.96875, Computation time: 1.1816575527191162\n",
      "Step: 2878, Loss: 0.939617395401001, Accuracy: 0.9821428656578064, Computation time: 1.869032621383667\n",
      "Step: 2879, Loss: 0.9160524606704712, Accuracy: 1.0, Computation time: 1.8185291290283203\n",
      "Step: 2880, Loss: 0.9160797595977783, Accuracy: 1.0, Computation time: 1.3065767288208008\n",
      "Step: 2881, Loss: 0.9384728670120239, Accuracy: 0.9722222089767456, Computation time: 1.2431859970092773\n",
      "Step: 2882, Loss: 0.9159981608390808, Accuracy: 1.0, Computation time: 1.5986042022705078\n",
      "Step: 2883, Loss: 0.9164791107177734, Accuracy: 1.0, Computation time: 1.2880542278289795\n",
      "Step: 2884, Loss: 0.916287899017334, Accuracy: 1.0, Computation time: 1.457953929901123\n",
      "Step: 2885, Loss: 0.9161616563796997, Accuracy: 1.0, Computation time: 1.9913434982299805\n",
      "Step: 2886, Loss: 0.917041540145874, Accuracy: 1.0, Computation time: 1.4565725326538086\n",
      "Step: 2887, Loss: 0.9374178647994995, Accuracy: 0.9807692766189575, Computation time: 1.626814365386963\n",
      "Step: 2888, Loss: 0.9162675738334656, Accuracy: 1.0, Computation time: 1.2187514305114746\n",
      "Step: 2889, Loss: 0.9161097407341003, Accuracy: 1.0, Computation time: 1.338501214981079\n",
      "Step: 2890, Loss: 0.9160792231559753, Accuracy: 1.0, Computation time: 1.4028637409210205\n",
      "Step: 2891, Loss: 0.9165133833885193, Accuracy: 1.0, Computation time: 1.5874111652374268\n",
      "Step: 2892, Loss: 0.9160706400871277, Accuracy: 1.0, Computation time: 1.5980546474456787\n",
      "Step: 2893, Loss: 0.9336103796958923, Accuracy: 0.9642857313156128, Computation time: 1.315746545791626\n",
      "Step: 2894, Loss: 0.9161373376846313, Accuracy: 1.0, Computation time: 1.370192289352417\n",
      "Step: 2895, Loss: 0.9168124198913574, Accuracy: 1.0, Computation time: 1.9385604858398438\n",
      "Step: 2896, Loss: 0.9161030054092407, Accuracy: 1.0, Computation time: 1.379316806793213\n",
      "Step: 2897, Loss: 0.916387677192688, Accuracy: 1.0, Computation time: 1.6865580081939697\n",
      "Step: 2898, Loss: 0.9164263010025024, Accuracy: 1.0, Computation time: 1.9879100322723389\n",
      "Step: 2899, Loss: 0.9160411357879639, Accuracy: 1.0, Computation time: 1.611945629119873\n",
      "Step: 2900, Loss: 0.9160722494125366, Accuracy: 1.0, Computation time: 1.3978626728057861\n",
      "Step: 2901, Loss: 0.9283387660980225, Accuracy: 0.96875, Computation time: 1.2548251152038574\n",
      "Step: 2902, Loss: 0.916296124458313, Accuracy: 1.0, Computation time: 1.4137253761291504\n",
      "Step: 2903, Loss: 0.9159263968467712, Accuracy: 1.0, Computation time: 1.1314268112182617\n",
      "Step: 2904, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 1.3130581378936768\n",
      "Step: 2905, Loss: 0.9159612059593201, Accuracy: 1.0, Computation time: 1.1608245372772217\n",
      "Step: 2906, Loss: 0.9159272313117981, Accuracy: 1.0, Computation time: 1.421567440032959\n",
      "Step: 2907, Loss: 0.9159640073776245, Accuracy: 1.0, Computation time: 1.8063106536865234\n",
      "Step: 2908, Loss: 0.9378940463066101, Accuracy: 0.9583333730697632, Computation time: 1.2947497367858887\n",
      "Step: 2909, Loss: 0.9165396690368652, Accuracy: 1.0, Computation time: 1.637760877609253\n",
      "Step: 2910, Loss: 0.9161056280136108, Accuracy: 1.0, Computation time: 1.2692937850952148\n",
      "Step: 2911, Loss: 0.9176838994026184, Accuracy: 1.0, Computation time: 1.193526029586792\n",
      "Step: 2912, Loss: 0.9160370230674744, Accuracy: 1.0, Computation time: 1.8516883850097656\n",
      "Step: 2913, Loss: 0.9304794073104858, Accuracy: 0.9807692766189575, Computation time: 1.5827136039733887\n",
      "Step: 2914, Loss: 0.9160594344139099, Accuracy: 1.0, Computation time: 1.72471022605896\n",
      "Step: 2915, Loss: 0.9375823736190796, Accuracy: 0.9642857313156128, Computation time: 1.1247429847717285\n",
      "Step: 2916, Loss: 0.9232994914054871, Accuracy: 1.0, Computation time: 1.5564231872558594\n",
      "Step: 2917, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 1.2472259998321533\n",
      "Step: 2918, Loss: 0.9160084128379822, Accuracy: 1.0, Computation time: 1.601736307144165\n",
      "########################\n",
      "Test loss: 1.0697957277297974, Test Accuracy_epoch21: 0.7699183821678162\n",
      "########################\n",
      "Step: 2919, Loss: 0.9159887433052063, Accuracy: 1.0, Computation time: 2.005439281463623\n",
      "Step: 2920, Loss: 0.9160241484642029, Accuracy: 1.0, Computation time: 1.5289723873138428\n",
      "Step: 2921, Loss: 0.9159910678863525, Accuracy: 1.0, Computation time: 1.164527177810669\n",
      "Step: 2922, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 1.335033655166626\n",
      "Step: 2923, Loss: 0.9160050749778748, Accuracy: 1.0, Computation time: 2.043076753616333\n",
      "Step: 2924, Loss: 0.9373973608016968, Accuracy: 0.9166666865348816, Computation time: 1.5393867492675781\n",
      "Step: 2925, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.3728117942810059\n",
      "Step: 2926, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.6327283382415771\n",
      "Step: 2927, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.579040288925171\n",
      "Step: 2928, Loss: 0.9160484671592712, Accuracy: 1.0, Computation time: 1.5316779613494873\n",
      "Step: 2929, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 1.7156970500946045\n",
      "Step: 2930, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 1.2580139636993408\n",
      "Step: 2931, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 1.1616406440734863\n",
      "Step: 2932, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 1.1207222938537598\n",
      "Step: 2933, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.566072702407837\n",
      "Step: 2934, Loss: 0.9175832271575928, Accuracy: 1.0, Computation time: 1.6702630519866943\n",
      "Step: 2935, Loss: 0.9160380959510803, Accuracy: 1.0, Computation time: 1.3283090591430664\n",
      "Step: 2936, Loss: 0.9159099459648132, Accuracy: 1.0, Computation time: 1.3629722595214844\n",
      "Step: 2937, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 1.871546745300293\n",
      "Step: 2938, Loss: 0.9373618960380554, Accuracy: 0.96875, Computation time: 1.47420072555542\n",
      "Step: 2939, Loss: 0.9361277222633362, Accuracy: 0.9833333492279053, Computation time: 1.5012378692626953\n",
      "Step: 2940, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 2.1736538410186768\n",
      "Step: 2941, Loss: 0.9163079261779785, Accuracy: 1.0, Computation time: 1.4424262046813965\n",
      "Step: 2942, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 1.3777902126312256\n",
      "Step: 2943, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.793508768081665\n",
      "Step: 2944, Loss: 0.9172062277793884, Accuracy: 1.0, Computation time: 1.2075128555297852\n",
      "Step: 2945, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 2.309523344039917\n",
      "Step: 2946, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 1.521700143814087\n",
      "Step: 2947, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 1.2567331790924072\n",
      "Step: 2948, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.178467035293579\n",
      "Step: 2949, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.3930420875549316\n",
      "Step: 2950, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.4126372337341309\n",
      "Step: 2951, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.1192119121551514\n",
      "Step: 2952, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.1897587776184082\n",
      "Step: 2953, Loss: 0.9158729910850525, Accuracy: 1.0, Computation time: 1.227787733078003\n",
      "Step: 2954, Loss: 0.9365581274032593, Accuracy: 0.9791666865348816, Computation time: 1.2778239250183105\n",
      "Step: 2955, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.6312828063964844\n",
      "Step: 2956, Loss: 0.9317881464958191, Accuracy: 0.96875, Computation time: 1.0726194381713867\n",
      "Step: 2957, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 1.2466235160827637\n",
      "Step: 2958, Loss: 0.9375306367874146, Accuracy: 0.9583333730697632, Computation time: 1.5651121139526367\n",
      "Step: 2959, Loss: 0.9161532521247864, Accuracy: 1.0, Computation time: 1.4425671100616455\n",
      "Step: 2960, Loss: 0.9211301207542419, Accuracy: 1.0, Computation time: 1.7106750011444092\n",
      "Step: 2961, Loss: 0.9376558065414429, Accuracy: 0.9722222089767456, Computation time: 1.3817336559295654\n",
      "Step: 2962, Loss: 0.9375085830688477, Accuracy: 0.9583333730697632, Computation time: 1.8878896236419678\n",
      "Step: 2963, Loss: 0.9160245060920715, Accuracy: 1.0, Computation time: 1.5611546039581299\n",
      "Step: 2964, Loss: 0.9323753714561462, Accuracy: 0.9791666865348816, Computation time: 1.2339167594909668\n",
      "Step: 2965, Loss: 0.9380130767822266, Accuracy: 0.9791666865348816, Computation time: 1.3288428783416748\n",
      "Step: 2966, Loss: 0.9179045557975769, Accuracy: 1.0, Computation time: 1.2005038261413574\n",
      "Step: 2967, Loss: 0.9161980152130127, Accuracy: 1.0, Computation time: 1.8741061687469482\n",
      "Step: 2968, Loss: 0.9161489009857178, Accuracy: 1.0, Computation time: 1.6631743907928467\n",
      "Step: 2969, Loss: 0.9311169981956482, Accuracy: 0.96875, Computation time: 1.3491103649139404\n",
      "Step: 2970, Loss: 0.9160667061805725, Accuracy: 1.0, Computation time: 1.4029977321624756\n",
      "Step: 2971, Loss: 0.916009783744812, Accuracy: 1.0, Computation time: 1.4621915817260742\n",
      "Step: 2972, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.1635329723358154\n",
      "Step: 2973, Loss: 0.9160772562026978, Accuracy: 1.0, Computation time: 1.6752076148986816\n",
      "Step: 2974, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 1.1048619747161865\n",
      "Step: 2975, Loss: 0.9160294532775879, Accuracy: 1.0, Computation time: 1.144754409790039\n",
      "Step: 2976, Loss: 0.9160443544387817, Accuracy: 1.0, Computation time: 1.3843016624450684\n",
      "Step: 2977, Loss: 0.9161146283149719, Accuracy: 1.0, Computation time: 1.216310977935791\n",
      "Step: 2978, Loss: 0.9377384781837463, Accuracy: 0.9807692766189575, Computation time: 1.1270759105682373\n",
      "Step: 2979, Loss: 0.9381919503211975, Accuracy: 0.9772727489471436, Computation time: 1.1107394695281982\n",
      "Step: 2980, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.2249805927276611\n",
      "Step: 2981, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.1355679035186768\n",
      "Step: 2982, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 1.1517655849456787\n",
      "Step: 2983, Loss: 0.9159661531448364, Accuracy: 1.0, Computation time: 1.4301438331604004\n",
      "Step: 2984, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 1.4502549171447754\n",
      "Step: 2985, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 1.3786005973815918\n",
      "Step: 2986, Loss: 0.9158995151519775, Accuracy: 1.0, Computation time: 1.3047106266021729\n",
      "Step: 2987, Loss: 0.9159576892852783, Accuracy: 1.0, Computation time: 1.321772575378418\n",
      "Step: 2988, Loss: 0.9259355664253235, Accuracy: 0.9791666865348816, Computation time: 1.3007795810699463\n",
      "Step: 2989, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 1.084054946899414\n",
      "Step: 2990, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 1.143951654434204\n",
      "Step: 2991, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.1885759830474854\n",
      "Step: 2992, Loss: 0.9159125089645386, Accuracy: 1.0, Computation time: 1.796560287475586\n",
      "Step: 2993, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 1.2571754455566406\n",
      "Step: 2994, Loss: 0.9578582048416138, Accuracy: 0.9666666984558105, Computation time: 1.3936188220977783\n",
      "Step: 2995, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.3487308025360107\n",
      "Step: 2996, Loss: 0.9159325957298279, Accuracy: 1.0, Computation time: 1.524646520614624\n",
      "Step: 2997, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.309995412826538\n",
      "Step: 2998, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.2709100246429443\n",
      "Step: 2999, Loss: 0.9159424901008606, Accuracy: 1.0, Computation time: 1.2307195663452148\n",
      "Step: 3000, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.3479788303375244\n",
      "Step: 3001, Loss: 0.9233390688896179, Accuracy: 1.0, Computation time: 1.3747775554656982\n",
      "Step: 3002, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.4235477447509766\n",
      "Step: 3003, Loss: 0.9159325957298279, Accuracy: 1.0, Computation time: 1.265740156173706\n",
      "Step: 3004, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.105285406112671\n",
      "Step: 3005, Loss: 0.9160990118980408, Accuracy: 1.0, Computation time: 1.330185890197754\n",
      "Step: 3006, Loss: 0.9374176859855652, Accuracy: 0.96875, Computation time: 1.3000600337982178\n",
      "Step: 3007, Loss: 0.9159297347068787, Accuracy: 1.0, Computation time: 1.156930923461914\n",
      "Step: 3008, Loss: 0.9168664813041687, Accuracy: 1.0, Computation time: 1.050091028213501\n",
      "Step: 3009, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.4178247451782227\n",
      "Step: 3010, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.0953094959259033\n",
      "Step: 3011, Loss: 0.9375820755958557, Accuracy: 0.9833333492279053, Computation time: 1.2490143775939941\n",
      "Step: 3012, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.2342119216918945\n",
      "Step: 3013, Loss: 0.9159762859344482, Accuracy: 1.0, Computation time: 1.186586856842041\n",
      "Step: 3014, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.2361376285552979\n",
      "Step: 3015, Loss: 0.9323795437812805, Accuracy: 0.96875, Computation time: 1.4241652488708496\n",
      "Step: 3016, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.1622018814086914\n",
      "Step: 3017, Loss: 0.9264069199562073, Accuracy: 1.0, Computation time: 1.505262851715088\n",
      "Step: 3018, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 1.2989323139190674\n",
      "Step: 3019, Loss: 0.9373497366905212, Accuracy: 0.984375, Computation time: 1.1574461460113525\n",
      "Step: 3020, Loss: 0.9159575700759888, Accuracy: 1.0, Computation time: 1.262284278869629\n",
      "Step: 3021, Loss: 0.9159618020057678, Accuracy: 1.0, Computation time: 1.4875555038452148\n",
      "Step: 3022, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 1.1415412425994873\n",
      "Step: 3023, Loss: 0.9159098863601685, Accuracy: 1.0, Computation time: 1.353379726409912\n",
      "Step: 3024, Loss: 0.9192858338356018, Accuracy: 1.0, Computation time: 1.2126078605651855\n",
      "Step: 3025, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 1.3617303371429443\n",
      "Step: 3026, Loss: 0.9376317858695984, Accuracy: 0.9807692766189575, Computation time: 1.074556589126587\n",
      "Step: 3027, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.096717119216919\n",
      "Step: 3028, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 1.2580230236053467\n",
      "Step: 3029, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 1.0823781490325928\n",
      "Step: 3030, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 1.2341563701629639\n",
      "Step: 3031, Loss: 0.9160863757133484, Accuracy: 1.0, Computation time: 1.2667787075042725\n",
      "Step: 3032, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.6014933586120605\n",
      "Step: 3033, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.2238082885742188\n",
      "Step: 3034, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.8469979763031006\n",
      "Step: 3035, Loss: 0.9602400660514832, Accuracy: 0.9494949579238892, Computation time: 1.4474859237670898\n",
      "Step: 3036, Loss: 0.9159643054008484, Accuracy: 1.0, Computation time: 1.3870677947998047\n",
      "Step: 3037, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 2.174952507019043\n",
      "Step: 3038, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.3701868057250977\n",
      "Step: 3039, Loss: 0.9257457852363586, Accuracy: 0.9583333730697632, Computation time: 1.2764787673950195\n",
      "Step: 3040, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.8253204822540283\n",
      "Step: 3041, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 1.4092168807983398\n",
      "Step: 3042, Loss: 0.9161328077316284, Accuracy: 1.0, Computation time: 1.7563893795013428\n",
      "Step: 3043, Loss: 0.9602441191673279, Accuracy: 0.9529914855957031, Computation time: 1.8907217979431152\n",
      "Step: 3044, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.6051628589630127\n",
      "Step: 3045, Loss: 0.9159349799156189, Accuracy: 1.0, Computation time: 1.1727170944213867\n",
      "Step: 3046, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 1.1794624328613281\n",
      "Step: 3047, Loss: 0.9212722778320312, Accuracy: 1.0, Computation time: 1.4502038955688477\n",
      "Step: 3048, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.1307957172393799\n",
      "Step: 3049, Loss: 0.916094183921814, Accuracy: 1.0, Computation time: 1.4300034046173096\n",
      "Step: 3050, Loss: 0.9181661605834961, Accuracy: 1.0, Computation time: 1.6856448650360107\n",
      "Step: 3051, Loss: 0.9361201524734497, Accuracy: 0.9807692766189575, Computation time: 1.299318790435791\n",
      "Step: 3052, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.177382469177246\n",
      "Step: 3053, Loss: 0.9327652454376221, Accuracy: 0.9772727489471436, Computation time: 1.2398087978363037\n",
      "Step: 3054, Loss: 0.9159504175186157, Accuracy: 1.0, Computation time: 1.6383438110351562\n",
      "Step: 3055, Loss: 0.9159429669380188, Accuracy: 1.0, Computation time: 1.209040880203247\n",
      "Step: 3056, Loss: 0.9160417914390564, Accuracy: 1.0, Computation time: 2.4169702529907227\n",
      "Step: 3057, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.1718182563781738\n",
      "########################\n",
      "Test loss: 1.0728645324707031, Test Accuracy_epoch22: 0.7649751305580139\n",
      "########################\n",
      "Step: 3058, Loss: 0.9160757064819336, Accuracy: 1.0, Computation time: 1.3741021156311035\n",
      "Step: 3059, Loss: 0.9160177111625671, Accuracy: 1.0, Computation time: 1.4775614738464355\n",
      "Step: 3060, Loss: 0.9160059094429016, Accuracy: 1.0, Computation time: 1.6194517612457275\n",
      "Step: 3061, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 1.1110682487487793\n",
      "Step: 3062, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.4524545669555664\n",
      "Step: 3063, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 1.3174102306365967\n",
      "Step: 3064, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 1.9817523956298828\n",
      "Step: 3065, Loss: 0.9159250855445862, Accuracy: 1.0, Computation time: 1.2456419467926025\n",
      "Step: 3066, Loss: 0.9165562391281128, Accuracy: 1.0, Computation time: 1.104748010635376\n",
      "Step: 3067, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.8821289539337158\n",
      "Step: 3068, Loss: 0.9163179397583008, Accuracy: 1.0, Computation time: 1.5939509868621826\n",
      "Step: 3069, Loss: 0.9159202575683594, Accuracy: 1.0, Computation time: 1.6503822803497314\n",
      "Step: 3070, Loss: 0.9159694314002991, Accuracy: 1.0, Computation time: 1.3253424167633057\n",
      "Step: 3071, Loss: 0.9159891605377197, Accuracy: 1.0, Computation time: 1.2083370685577393\n",
      "Step: 3072, Loss: 0.916363537311554, Accuracy: 1.0, Computation time: 1.21230149269104\n",
      "Step: 3073, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.2583930492401123\n",
      "Step: 3074, Loss: 0.9344765543937683, Accuracy: 0.9722222089767456, Computation time: 1.5061233043670654\n",
      "Step: 3075, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.3402974605560303\n",
      "Step: 3076, Loss: 0.9375021457672119, Accuracy: 0.9807692766189575, Computation time: 1.1267883777618408\n",
      "Step: 3077, Loss: 0.9159860014915466, Accuracy: 1.0, Computation time: 0.9987120628356934\n",
      "Step: 3078, Loss: 0.9160204529762268, Accuracy: 1.0, Computation time: 1.2323226928710938\n",
      "Step: 3079, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.451859712600708\n",
      "Step: 3080, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.0579650402069092\n",
      "Step: 3081, Loss: 0.9375479221343994, Accuracy: 0.9750000238418579, Computation time: 1.6866123676300049\n",
      "Step: 3082, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.5930674076080322\n",
      "Step: 3083, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.336179256439209\n",
      "Step: 3084, Loss: 0.9213542342185974, Accuracy: 1.0, Computation time: 1.4393930435180664\n",
      "Step: 3085, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.2130217552185059\n",
      "Step: 3086, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.8691699504852295\n",
      "Step: 3087, Loss: 0.9591399431228638, Accuracy: 0.949999988079071, Computation time: 1.794858694076538\n",
      "Step: 3088, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.5515072345733643\n",
      "Step: 3089, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 0.9880547523498535\n",
      "Step: 3090, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 1.324275016784668\n",
      "Step: 3091, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.103348731994629\n",
      "Step: 3092, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.0478289127349854\n",
      "Step: 3093, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 1.4321682453155518\n",
      "Step: 3094, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.0616047382354736\n",
      "Step: 3095, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 0.992647647857666\n",
      "Step: 3096, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 1.4170372486114502\n",
      "Step: 3097, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.1998343467712402\n",
      "Step: 3098, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 1.4809682369232178\n",
      "Step: 3099, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 1.1797637939453125\n",
      "Step: 3100, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.3773643970489502\n",
      "Step: 3101, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.3097777366638184\n",
      "Step: 3102, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 1.1263127326965332\n",
      "Step: 3103, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 1.3002820014953613\n",
      "Step: 3104, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 1.1929428577423096\n",
      "Step: 3105, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.3956236839294434\n",
      "Step: 3106, Loss: 0.9158390164375305, Accuracy: 1.0, Computation time: 1.134446620941162\n",
      "Step: 3107, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.0195322036743164\n",
      "Step: 3108, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.282621145248413\n",
      "Step: 3109, Loss: 0.937541663646698, Accuracy: 0.9772727489471436, Computation time: 1.0391590595245361\n",
      "Step: 3110, Loss: 0.9159929752349854, Accuracy: 1.0, Computation time: 1.4357681274414062\n",
      "Step: 3111, Loss: 0.91583251953125, Accuracy: 1.0, Computation time: 1.1437909603118896\n",
      "Step: 3112, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.4053914546966553\n",
      "Step: 3113, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 1.6021814346313477\n",
      "Step: 3114, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.1846745014190674\n",
      "Step: 3115, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.0697753429412842\n",
      "Step: 3116, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 1.0483968257904053\n",
      "Step: 3117, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 0.9571523666381836\n",
      "Step: 3118, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.1209137439727783\n",
      "Step: 3119, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 1.052335500717163\n",
      "Step: 3120, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.2621519565582275\n",
      "Step: 3121, Loss: 0.9159979820251465, Accuracy: 1.0, Computation time: 1.2492396831512451\n",
      "Step: 3122, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.2607121467590332\n",
      "Step: 3123, Loss: 0.9272431135177612, Accuracy: 0.9750000238418579, Computation time: 1.3856377601623535\n",
      "Step: 3124, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.2432258129119873\n",
      "Step: 3125, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.4050695896148682\n",
      "Step: 3126, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.1720514297485352\n",
      "Step: 3127, Loss: 0.9159274101257324, Accuracy: 1.0, Computation time: 1.3238482475280762\n",
      "Step: 3128, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.0988500118255615\n",
      "Step: 3129, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.6046535968780518\n",
      "Step: 3130, Loss: 0.9160638451576233, Accuracy: 1.0, Computation time: 1.7114670276641846\n",
      "Step: 3131, Loss: 0.9161286950111389, Accuracy: 1.0, Computation time: 2.0441441535949707\n",
      "Step: 3132, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.6262240409851074\n",
      "Step: 3133, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.3873281478881836\n",
      "Step: 3134, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.6188888549804688\n",
      "Step: 3135, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 2.1116373538970947\n",
      "Step: 3136, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.4450244903564453\n",
      "Step: 3137, Loss: 0.944481611251831, Accuracy: 0.9750000238418579, Computation time: 1.6236686706542969\n",
      "Step: 3138, Loss: 0.9177181124687195, Accuracy: 1.0, Computation time: 1.4169433116912842\n",
      "Step: 3139, Loss: 0.9160687923431396, Accuracy: 1.0, Computation time: 1.5363388061523438\n",
      "Step: 3140, Loss: 0.916327178478241, Accuracy: 1.0, Computation time: 1.9718384742736816\n",
      "Step: 3141, Loss: 0.9159509539604187, Accuracy: 1.0, Computation time: 1.1996822357177734\n",
      "Step: 3142, Loss: 0.9161273241043091, Accuracy: 1.0, Computation time: 2.1941683292388916\n",
      "Step: 3143, Loss: 0.9160552620887756, Accuracy: 1.0, Computation time: 1.902508020401001\n",
      "Step: 3144, Loss: 0.9160282015800476, Accuracy: 1.0, Computation time: 1.3233940601348877\n",
      "Step: 3145, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.605309247970581\n",
      "Step: 3146, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.4766974449157715\n",
      "Step: 3147, Loss: 0.9376499652862549, Accuracy: 0.9642857313156128, Computation time: 2.0602827072143555\n",
      "Step: 3148, Loss: 0.9160841703414917, Accuracy: 1.0, Computation time: 2.857945442199707\n",
      "Step: 3149, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.8692288398742676\n",
      "Step: 3150, Loss: 0.9161481261253357, Accuracy: 1.0, Computation time: 2.1351897716522217\n",
      "Step: 3151, Loss: 0.9215761423110962, Accuracy: 1.0, Computation time: 1.6958370208740234\n",
      "Step: 3152, Loss: 0.9253330230712891, Accuracy: 1.0, Computation time: 1.6397223472595215\n",
      "Step: 3153, Loss: 0.9516944885253906, Accuracy: 0.925000011920929, Computation time: 1.7021918296813965\n",
      "Step: 3154, Loss: 0.9159106612205505, Accuracy: 1.0, Computation time: 1.2657573223114014\n",
      "Step: 3155, Loss: 0.9173871874809265, Accuracy: 1.0, Computation time: 1.2238709926605225\n",
      "Step: 3156, Loss: 0.9261422753334045, Accuracy: 1.0, Computation time: 1.696756362915039\n",
      "Step: 3157, Loss: 0.9163222908973694, Accuracy: 1.0, Computation time: 1.970165729522705\n",
      "Step: 3158, Loss: 0.9272660613059998, Accuracy: 0.9772727489471436, Computation time: 1.303135633468628\n",
      "Step: 3159, Loss: 0.9166867136955261, Accuracy: 1.0, Computation time: 1.3202531337738037\n",
      "Step: 3160, Loss: 0.9165871143341064, Accuracy: 1.0, Computation time: 1.496244192123413\n",
      "Step: 3161, Loss: 0.9173431992530823, Accuracy: 1.0, Computation time: 1.364074945449829\n",
      "Step: 3162, Loss: 0.9172826409339905, Accuracy: 1.0, Computation time: 1.2243268489837646\n",
      "Step: 3163, Loss: 0.9164623618125916, Accuracy: 1.0, Computation time: 1.3407301902770996\n",
      "Step: 3164, Loss: 0.9166971445083618, Accuracy: 1.0, Computation time: 1.2443585395812988\n",
      "Step: 3165, Loss: 0.9168346524238586, Accuracy: 1.0, Computation time: 1.6681938171386719\n",
      "Step: 3166, Loss: 0.9165141582489014, Accuracy: 1.0, Computation time: 1.604731559753418\n",
      "Step: 3167, Loss: 0.9161049723625183, Accuracy: 1.0, Computation time: 2.1381919384002686\n",
      "Step: 3168, Loss: 0.9239885807037354, Accuracy: 1.0, Computation time: 1.5703113079071045\n",
      "Step: 3169, Loss: 0.9160086512565613, Accuracy: 1.0, Computation time: 1.581371784210205\n",
      "Step: 3170, Loss: 0.9378080368041992, Accuracy: 0.9722222089767456, Computation time: 1.5646097660064697\n",
      "Step: 3171, Loss: 0.9166218042373657, Accuracy: 1.0, Computation time: 1.3491578102111816\n",
      "Step: 3172, Loss: 0.9317887425422668, Accuracy: 0.9750000238418579, Computation time: 1.285247802734375\n",
      "Step: 3173, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 1.2426934242248535\n",
      "Step: 3174, Loss: 0.9168537855148315, Accuracy: 1.0, Computation time: 2.039968967437744\n",
      "Step: 3175, Loss: 0.9162983894348145, Accuracy: 1.0, Computation time: 2.257383108139038\n",
      "Step: 3176, Loss: 0.9164831042289734, Accuracy: 1.0, Computation time: 1.3236331939697266\n",
      "Step: 3177, Loss: 0.9163835048675537, Accuracy: 1.0, Computation time: 1.2451026439666748\n",
      "Step: 3178, Loss: 0.9165049195289612, Accuracy: 1.0, Computation time: 2.182756185531616\n",
      "Step: 3179, Loss: 0.9162663221359253, Accuracy: 1.0, Computation time: 1.2699038982391357\n",
      "Step: 3180, Loss: 0.9299028515815735, Accuracy: 0.9642857313156128, Computation time: 1.972641944885254\n",
      "Step: 3181, Loss: 0.9384215474128723, Accuracy: 0.9833333492279053, Computation time: 1.743945598602295\n",
      "Step: 3182, Loss: 0.9162918329238892, Accuracy: 1.0, Computation time: 1.4373795986175537\n",
      "Step: 3183, Loss: 0.9373640418052673, Accuracy: 0.9791666865348816, Computation time: 1.574885368347168\n",
      "Step: 3184, Loss: 0.9160529375076294, Accuracy: 1.0, Computation time: 2.1386845111846924\n",
      "Step: 3185, Loss: 0.9452592730522156, Accuracy: 0.9642857313156128, Computation time: 2.2288095951080322\n",
      "Step: 3186, Loss: 0.9162270426750183, Accuracy: 1.0, Computation time: 1.1724505424499512\n",
      "Step: 3187, Loss: 0.9173046350479126, Accuracy: 1.0, Computation time: 2.3328351974487305\n",
      "Step: 3188, Loss: 0.9160408973693848, Accuracy: 1.0, Computation time: 1.5176594257354736\n",
      "Step: 3189, Loss: 0.9308441281318665, Accuracy: 0.9772727489471436, Computation time: 1.7000024318695068\n",
      "Step: 3190, Loss: 0.9159706830978394, Accuracy: 1.0, Computation time: 1.6375577449798584\n",
      "Step: 3191, Loss: 0.9371640086174011, Accuracy: 0.9722222089767456, Computation time: 1.4714984893798828\n",
      "Step: 3192, Loss: 0.9161081910133362, Accuracy: 1.0, Computation time: 1.1985089778900146\n",
      "Step: 3193, Loss: 0.9196240901947021, Accuracy: 1.0, Computation time: 1.9585185050964355\n",
      "Step: 3194, Loss: 0.9161192178726196, Accuracy: 1.0, Computation time: 1.3557915687561035\n",
      "Step: 3195, Loss: 0.9169341325759888, Accuracy: 1.0, Computation time: 1.7980742454528809\n",
      "Step: 3196, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 1.6558160781860352\n",
      "########################\n",
      "Test loss: 1.0698765516281128, Test Accuracy_epoch23: 0.7690785527229309\n",
      "########################\n",
      "Step: 3197, Loss: 0.9161617755889893, Accuracy: 1.0, Computation time: 1.4324734210968018\n",
      "Step: 3198, Loss: 0.9164209961891174, Accuracy: 1.0, Computation time: 1.437072992324829\n",
      "Step: 3199, Loss: 0.9159567952156067, Accuracy: 1.0, Computation time: 1.5487523078918457\n",
      "Step: 3200, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.4874987602233887\n",
      "Step: 3201, Loss: 0.9375982284545898, Accuracy: 0.9791666865348816, Computation time: 1.903416633605957\n",
      "Step: 3202, Loss: 0.9161350131034851, Accuracy: 1.0, Computation time: 1.183065414428711\n",
      "Step: 3203, Loss: 0.9173622131347656, Accuracy: 1.0, Computation time: 1.517869472503662\n",
      "Step: 3204, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 1.4732792377471924\n",
      "Step: 3205, Loss: 0.9228578209877014, Accuracy: 1.0, Computation time: 1.283097743988037\n",
      "Step: 3206, Loss: 0.9160563945770264, Accuracy: 1.0, Computation time: 1.646134614944458\n",
      "Step: 3207, Loss: 0.9160229563713074, Accuracy: 1.0, Computation time: 1.1819555759429932\n",
      "Step: 3208, Loss: 0.9164928197860718, Accuracy: 1.0, Computation time: 1.2701175212860107\n",
      "Step: 3209, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 1.3900632858276367\n",
      "Step: 3210, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 1.2293214797973633\n",
      "Step: 3211, Loss: 0.9159918427467346, Accuracy: 1.0, Computation time: 1.348372459411621\n",
      "Step: 3212, Loss: 0.9160104990005493, Accuracy: 1.0, Computation time: 1.41927170753479\n",
      "Step: 3213, Loss: 0.916022777557373, Accuracy: 1.0, Computation time: 1.3547008037567139\n",
      "Step: 3214, Loss: 0.9159917831420898, Accuracy: 1.0, Computation time: 1.4435226917266846\n",
      "Step: 3215, Loss: 0.9377007484436035, Accuracy: 0.9791666865348816, Computation time: 1.173276662826538\n",
      "Step: 3216, Loss: 0.9375074505805969, Accuracy: 0.96875, Computation time: 1.5218379497528076\n",
      "Step: 3217, Loss: 0.9159790277481079, Accuracy: 1.0, Computation time: 1.351916790008545\n",
      "Step: 3218, Loss: 0.9162381291389465, Accuracy: 1.0, Computation time: 1.1376922130584717\n",
      "Step: 3219, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 1.3772218227386475\n",
      "Step: 3220, Loss: 0.9253523945808411, Accuracy: 0.96875, Computation time: 1.5415370464324951\n",
      "Step: 3221, Loss: 0.9169748425483704, Accuracy: 1.0, Computation time: 1.413454294204712\n",
      "Step: 3222, Loss: 0.9160521626472473, Accuracy: 1.0, Computation time: 1.428729772567749\n",
      "Step: 3223, Loss: 0.927487313747406, Accuracy: nan, Computation time: 1.6038212776184082\n",
      "Step: 3224, Loss: 0.9159141778945923, Accuracy: 1.0, Computation time: 1.2103211879730225\n",
      "Step: 3225, Loss: 0.920835018157959, Accuracy: 1.0, Computation time: 1.3563714027404785\n",
      "Step: 3226, Loss: 0.9159439206123352, Accuracy: 1.0, Computation time: 1.2873201370239258\n",
      "Step: 3227, Loss: 0.9159698486328125, Accuracy: 1.0, Computation time: 1.3692257404327393\n",
      "Step: 3228, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 1.2190988063812256\n",
      "Step: 3229, Loss: 0.91634202003479, Accuracy: 1.0, Computation time: 1.2213380336761475\n",
      "Step: 3230, Loss: 0.9168258905410767, Accuracy: 1.0, Computation time: 1.4883131980895996\n",
      "Step: 3231, Loss: 0.9161284565925598, Accuracy: 1.0, Computation time: 1.3982102870941162\n",
      "Step: 3232, Loss: 0.9159574508666992, Accuracy: 1.0, Computation time: 1.4811110496520996\n",
      "Step: 3233, Loss: 0.9377006888389587, Accuracy: 0.96875, Computation time: 1.2156059741973877\n",
      "Step: 3234, Loss: 0.9183748960494995, Accuracy: 1.0, Computation time: 1.4365451335906982\n",
      "Step: 3235, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 1.2435998916625977\n",
      "Step: 3236, Loss: 0.9160519242286682, Accuracy: 1.0, Computation time: 1.3179452419281006\n",
      "Step: 3237, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 1.3621392250061035\n",
      "Step: 3238, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 1.2697606086730957\n",
      "Step: 3239, Loss: 0.9159599542617798, Accuracy: 1.0, Computation time: 1.4544880390167236\n",
      "Step: 3240, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 1.5358409881591797\n",
      "Step: 3241, Loss: 0.9387463331222534, Accuracy: 0.9807692766189575, Computation time: 2.178209066390991\n",
      "Step: 3242, Loss: 0.9166909456253052, Accuracy: 1.0, Computation time: 2.5917580127716064\n",
      "Step: 3243, Loss: 0.9159263968467712, Accuracy: 1.0, Computation time: 1.3568012714385986\n",
      "Step: 3244, Loss: 0.9159390330314636, Accuracy: 1.0, Computation time: 1.2445738315582275\n",
      "Step: 3245, Loss: 0.9159122109413147, Accuracy: 1.0, Computation time: 1.6543123722076416\n",
      "Step: 3246, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.6677181720733643\n",
      "Step: 3247, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 1.4648206233978271\n",
      "Step: 3248, Loss: 0.9160333275794983, Accuracy: 1.0, Computation time: 1.582596778869629\n",
      "Step: 3249, Loss: 0.9376855492591858, Accuracy: 0.9807692766189575, Computation time: 1.6334784030914307\n",
      "Step: 3250, Loss: 0.9159709215164185, Accuracy: 1.0, Computation time: 1.398376226425171\n",
      "Step: 3251, Loss: 0.9172092080116272, Accuracy: 1.0, Computation time: 1.3783154487609863\n",
      "Step: 3252, Loss: 0.9160797595977783, Accuracy: 1.0, Computation time: 1.0887744426727295\n",
      "Step: 3253, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.1992247104644775\n",
      "Step: 3254, Loss: 0.9170060753822327, Accuracy: 1.0, Computation time: 2.026916742324829\n",
      "Step: 3255, Loss: 0.9159979224205017, Accuracy: 1.0, Computation time: 1.3506834506988525\n",
      "Step: 3256, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.361687421798706\n",
      "Step: 3257, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.393218755722046\n",
      "Step: 3258, Loss: 0.916144847869873, Accuracy: 1.0, Computation time: 1.2430689334869385\n",
      "Step: 3259, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 1.301825761795044\n",
      "Step: 3260, Loss: 0.9159537553787231, Accuracy: 1.0, Computation time: 1.3210959434509277\n",
      "Step: 3261, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.2264480590820312\n",
      "Step: 3262, Loss: 0.9163778424263, Accuracy: 1.0, Computation time: 1.3882579803466797\n",
      "Step: 3263, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.1616559028625488\n",
      "Step: 3264, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.1796989440917969\n",
      "Step: 3265, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 1.382594108581543\n",
      "Step: 3266, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 2.079949378967285\n",
      "Step: 3267, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.533719539642334\n",
      "Step: 3268, Loss: 0.915961503982544, Accuracy: 1.0, Computation time: 1.4081439971923828\n",
      "Step: 3269, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.2041919231414795\n",
      "Step: 3270, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 1.2129874229431152\n",
      "Step: 3271, Loss: 0.9161805510520935, Accuracy: 1.0, Computation time: 1.6780283451080322\n",
      "Step: 3272, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.2531843185424805\n",
      "Step: 3273, Loss: 0.9375931024551392, Accuracy: 0.9583333730697632, Computation time: 1.4180378913879395\n",
      "Step: 3274, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 1.230783224105835\n",
      "Step: 3275, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.2390472888946533\n",
      "Step: 3276, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.4673452377319336\n",
      "Step: 3277, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.4274864196777344\n",
      "Step: 3278, Loss: 0.9168282747268677, Accuracy: 1.0, Computation time: 1.4094352722167969\n",
      "Step: 3279, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.4316635131835938\n",
      "Step: 3280, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.3574471473693848\n",
      "Step: 3281, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 1.3158695697784424\n",
      "Step: 3282, Loss: 0.9375386834144592, Accuracy: 0.9791666865348816, Computation time: 1.2872645854949951\n",
      "Step: 3283, Loss: 0.9162516593933105, Accuracy: 1.0, Computation time: 1.8191511631011963\n",
      "Step: 3284, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.047983169555664\n",
      "Step: 3285, Loss: 0.9375780820846558, Accuracy: 0.9772727489471436, Computation time: 1.7498455047607422\n",
      "Step: 3286, Loss: 0.9160029888153076, Accuracy: 1.0, Computation time: 1.356567144393921\n",
      "Step: 3287, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.1597447395324707\n",
      "Step: 3288, Loss: 0.9374083280563354, Accuracy: 0.949999988079071, Computation time: 1.3914523124694824\n",
      "Step: 3289, Loss: 0.9159078001976013, Accuracy: 1.0, Computation time: 1.6077525615692139\n",
      "Step: 3290, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 1.4892292022705078\n",
      "Step: 3291, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.2739958763122559\n",
      "Step: 3292, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.1981725692749023\n",
      "Step: 3293, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.202265739440918\n",
      "Step: 3294, Loss: 0.9173254370689392, Accuracy: 1.0, Computation time: 1.4611756801605225\n",
      "Step: 3295, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.2977235317230225\n",
      "Step: 3296, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.2348463535308838\n",
      "Step: 3297, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.0402750968933105\n",
      "Step: 3298, Loss: 0.9208255410194397, Accuracy: 1.0, Computation time: 1.7844774723052979\n",
      "Step: 3299, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 1.2560148239135742\n",
      "Step: 3300, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.4429750442504883\n",
      "Step: 3301, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.732520580291748\n",
      "Step: 3302, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.2426834106445312\n",
      "Step: 3303, Loss: 0.91603022813797, Accuracy: 1.0, Computation time: 1.4662353992462158\n",
      "Step: 3304, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.1125400066375732\n",
      "Step: 3305, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.2658100128173828\n",
      "Step: 3306, Loss: 0.9321844577789307, Accuracy: 0.9166666865348816, Computation time: 1.3048655986785889\n",
      "Step: 3307, Loss: 0.9376953840255737, Accuracy: 0.9722222089767456, Computation time: 1.4956743717193604\n",
      "Step: 3308, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.182558298110962\n",
      "Step: 3309, Loss: 0.9375473856925964, Accuracy: 0.9375, Computation time: 1.4859187602996826\n",
      "Step: 3310, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.4783904552459717\n",
      "Step: 3311, Loss: 0.916063129901886, Accuracy: 1.0, Computation time: 1.9930455684661865\n",
      "Step: 3312, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.5897150039672852\n",
      "Step: 3313, Loss: 0.9162322282791138, Accuracy: 1.0, Computation time: 1.5073940753936768\n",
      "Step: 3314, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 2.0076961517333984\n",
      "Step: 3315, Loss: 0.9375917911529541, Accuracy: 0.9166666865348816, Computation time: 1.4952614307403564\n",
      "Step: 3316, Loss: 0.9375422596931458, Accuracy: 0.9791666865348816, Computation time: 1.086836338043213\n",
      "Step: 3317, Loss: 0.9203230142593384, Accuracy: 1.0, Computation time: 1.5617294311523438\n",
      "Step: 3318, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.2825324535369873\n",
      "Step: 3319, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.636272668838501\n",
      "Step: 3320, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.288675308227539\n",
      "Step: 3321, Loss: 0.9263964891433716, Accuracy: 0.9750000238418579, Computation time: 1.578395128250122\n",
      "Step: 3322, Loss: 0.9161669015884399, Accuracy: 1.0, Computation time: 1.5017147064208984\n",
      "Step: 3323, Loss: 0.92122483253479, Accuracy: 1.0, Computation time: 1.5976948738098145\n",
      "Step: 3324, Loss: 0.9161210060119629, Accuracy: 1.0, Computation time: 1.5234766006469727\n",
      "Step: 3325, Loss: 0.9164477586746216, Accuracy: 1.0, Computation time: 1.3605189323425293\n",
      "Step: 3326, Loss: 0.9164271950721741, Accuracy: 1.0, Computation time: 1.3647353649139404\n",
      "Step: 3327, Loss: 0.9165059328079224, Accuracy: 1.0, Computation time: 1.3146355152130127\n",
      "Step: 3328, Loss: 0.9241985082626343, Accuracy: 1.0, Computation time: 1.7463972568511963\n",
      "Step: 3329, Loss: 0.9163303971290588, Accuracy: 1.0, Computation time: 1.9212844371795654\n",
      "Step: 3330, Loss: 0.9162347912788391, Accuracy: 1.0, Computation time: 1.5059971809387207\n",
      "Step: 3331, Loss: 0.9159880876541138, Accuracy: 1.0, Computation time: 1.5259687900543213\n",
      "Step: 3332, Loss: 0.9170059561729431, Accuracy: 1.0, Computation time: 1.8734183311462402\n",
      "Step: 3333, Loss: 0.9160242676734924, Accuracy: 1.0, Computation time: 1.4732747077941895\n",
      "Step: 3334, Loss: 0.916013777256012, Accuracy: 1.0, Computation time: 1.228511095046997\n",
      "Step: 3335, Loss: 0.916357159614563, Accuracy: 1.0, Computation time: 1.4247419834136963\n",
      "########################\n",
      "Test loss: 1.0671234130859375, Test Accuracy_epoch24: 0.7698513269424438\n",
      "########################\n",
      "Step: 3336, Loss: 0.9171105623245239, Accuracy: nan, Computation time: 1.6131312847137451\n",
      "Step: 3337, Loss: 0.9379019141197205, Accuracy: 0.9791666865348816, Computation time: 1.7494957447052002\n",
      "Step: 3338, Loss: 0.9164397716522217, Accuracy: 1.0, Computation time: 1.360494613647461\n",
      "Step: 3339, Loss: 0.9165319800376892, Accuracy: 1.0, Computation time: 2.259030818939209\n",
      "Step: 3340, Loss: 0.9160295724868774, Accuracy: 1.0, Computation time: 1.4460194110870361\n",
      "Step: 3341, Loss: 0.9337636232376099, Accuracy: 0.949999988079071, Computation time: 1.5767111778259277\n",
      "Step: 3342, Loss: 0.9160591959953308, Accuracy: 1.0, Computation time: 1.3799166679382324\n",
      "Step: 3343, Loss: 0.9161657691001892, Accuracy: 1.0, Computation time: 1.3418474197387695\n",
      "Step: 3344, Loss: 0.9164091348648071, Accuracy: 1.0, Computation time: 1.399465560913086\n",
      "Step: 3345, Loss: 0.9160611629486084, Accuracy: 1.0, Computation time: 1.5144107341766357\n",
      "Step: 3346, Loss: 0.9160013794898987, Accuracy: 1.0, Computation time: 1.4441452026367188\n",
      "Step: 3347, Loss: 0.9162777066230774, Accuracy: 1.0, Computation time: 1.3168723583221436\n",
      "Step: 3348, Loss: 0.9373798966407776, Accuracy: 0.9772727489471436, Computation time: 1.2906560897827148\n",
      "Step: 3349, Loss: 0.9162225127220154, Accuracy: 1.0, Computation time: 1.3323140144348145\n",
      "Step: 3350, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.4540424346923828\n",
      "Step: 3351, Loss: 0.9374569654464722, Accuracy: 0.9722222089767456, Computation time: 1.4530415534973145\n",
      "Step: 3352, Loss: 0.9246329069137573, Accuracy: 1.0, Computation time: 1.9720075130462646\n",
      "Step: 3353, Loss: 0.9163873791694641, Accuracy: 1.0, Computation time: 1.639531135559082\n",
      "Step: 3354, Loss: 0.9223046898841858, Accuracy: 1.0, Computation time: 1.453329086303711\n",
      "Step: 3355, Loss: 0.915995717048645, Accuracy: 1.0, Computation time: 1.5972602367401123\n",
      "Step: 3356, Loss: 0.9428620338439941, Accuracy: 0.96875, Computation time: 1.4034228324890137\n",
      "Step: 3357, Loss: 0.9160618782043457, Accuracy: 1.0, Computation time: 1.379307508468628\n",
      "Step: 3358, Loss: 0.9159687757492065, Accuracy: 1.0, Computation time: 1.2861781120300293\n",
      "Step: 3359, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.856564998626709\n",
      "Step: 3360, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 1.1189777851104736\n",
      "Step: 3361, Loss: 0.9159624576568604, Accuracy: 1.0, Computation time: 1.3541309833526611\n",
      "Step: 3362, Loss: 0.9160485863685608, Accuracy: 1.0, Computation time: 1.4787604808807373\n",
      "Step: 3363, Loss: 0.916144073009491, Accuracy: 1.0, Computation time: 1.395462989807129\n",
      "Step: 3364, Loss: 0.9159492254257202, Accuracy: 1.0, Computation time: 1.4271612167358398\n",
      "Step: 3365, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 1.8370065689086914\n",
      "Step: 3366, Loss: 0.9160528779029846, Accuracy: 1.0, Computation time: 1.6496608257293701\n",
      "Step: 3367, Loss: 0.9168424010276794, Accuracy: 1.0, Computation time: 1.34761643409729\n",
      "Step: 3368, Loss: 0.9159331321716309, Accuracy: 1.0, Computation time: 1.1829543113708496\n",
      "Step: 3369, Loss: 0.9160507917404175, Accuracy: 1.0, Computation time: 1.2575881481170654\n",
      "Step: 3370, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.4338057041168213\n",
      "Step: 3371, Loss: 0.9159258008003235, Accuracy: 1.0, Computation time: 1.491126298904419\n",
      "Step: 3372, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 1.9257023334503174\n",
      "Step: 3373, Loss: 0.9220636487007141, Accuracy: 1.0, Computation time: 1.4525020122528076\n",
      "Step: 3374, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 1.5524029731750488\n",
      "Step: 3375, Loss: 0.9166327714920044, Accuracy: 1.0, Computation time: 1.409419059753418\n",
      "Step: 3376, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.4579777717590332\n",
      "Step: 3377, Loss: 0.9376603364944458, Accuracy: 0.949999988079071, Computation time: 1.3188929557800293\n",
      "Step: 3378, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 1.313542127609253\n",
      "Step: 3379, Loss: 0.9159391522407532, Accuracy: 1.0, Computation time: 1.4967095851898193\n",
      "Step: 3380, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.5254104137420654\n",
      "Step: 3381, Loss: 0.915911078453064, Accuracy: 1.0, Computation time: 1.6812419891357422\n",
      "Step: 3382, Loss: 0.9159553647041321, Accuracy: 1.0, Computation time: 1.5317802429199219\n",
      "Step: 3383, Loss: 0.9171478152275085, Accuracy: 1.0, Computation time: 1.4760916233062744\n",
      "Step: 3384, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.3319227695465088\n",
      "Step: 3385, Loss: 0.915932834148407, Accuracy: 1.0, Computation time: 1.4801461696624756\n",
      "Step: 3386, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 1.4987678527832031\n",
      "Step: 3387, Loss: 0.937508225440979, Accuracy: 0.9750000238418579, Computation time: 1.4862234592437744\n",
      "Step: 3388, Loss: 0.9160019159317017, Accuracy: 1.0, Computation time: 1.4422385692596436\n",
      "Step: 3389, Loss: 0.9159365296363831, Accuracy: 1.0, Computation time: 1.320680856704712\n",
      "Step: 3390, Loss: 0.9395469427108765, Accuracy: 0.9722222089767456, Computation time: 1.554100513458252\n",
      "Step: 3391, Loss: 0.9179468750953674, Accuracy: 1.0, Computation time: 1.3458590507507324\n",
      "Step: 3392, Loss: 0.9196678996086121, Accuracy: 1.0, Computation time: 1.7183399200439453\n",
      "Step: 3393, Loss: 0.915947675704956, Accuracy: 1.0, Computation time: 1.4802074432373047\n",
      "Step: 3394, Loss: 0.9159508943557739, Accuracy: 1.0, Computation time: 1.4021482467651367\n",
      "Step: 3395, Loss: 0.9382817149162292, Accuracy: 0.9722222089767456, Computation time: 1.2600421905517578\n",
      "Step: 3396, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 1.3433451652526855\n",
      "Step: 3397, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.6035048961639404\n",
      "Step: 3398, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.511805772781372\n",
      "Step: 3399, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.527209758758545\n",
      "Step: 3400, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.1923837661743164\n",
      "Step: 3401, Loss: 0.9230480790138245, Accuracy: 1.0, Computation time: 1.586942434310913\n",
      "Step: 3402, Loss: 0.916091799736023, Accuracy: 1.0, Computation time: 1.63383150100708\n",
      "Step: 3403, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.550079584121704\n",
      "Step: 3404, Loss: 0.9160056710243225, Accuracy: 1.0, Computation time: 1.6131160259246826\n",
      "Step: 3405, Loss: 0.9175014495849609, Accuracy: 1.0, Computation time: 1.5904183387756348\n",
      "Step: 3406, Loss: 0.9161375164985657, Accuracy: 1.0, Computation time: 1.7490999698638916\n",
      "Step: 3407, Loss: 0.9160133600234985, Accuracy: 1.0, Computation time: 1.7656848430633545\n",
      "Step: 3408, Loss: 0.938066840171814, Accuracy: 0.9722222089767456, Computation time: 1.529503345489502\n",
      "Step: 3409, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.6097164154052734\n",
      "Step: 3410, Loss: 0.9160130620002747, Accuracy: 1.0, Computation time: 1.4237840175628662\n",
      "Step: 3411, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.3696928024291992\n",
      "Step: 3412, Loss: 0.937576174736023, Accuracy: 0.9833333492279053, Computation time: 1.5361483097076416\n",
      "Step: 3413, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 1.665391445159912\n",
      "Step: 3414, Loss: 0.9159814715385437, Accuracy: 1.0, Computation time: 1.9569058418273926\n",
      "Step: 3415, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 1.3947241306304932\n",
      "Step: 3416, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.5337355136871338\n",
      "Step: 3417, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.4607343673706055\n",
      "Step: 3418, Loss: 0.9159746170043945, Accuracy: 1.0, Computation time: 1.5475735664367676\n",
      "Step: 3419, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.5655930042266846\n",
      "Step: 3420, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 1.6146879196166992\n",
      "Step: 3421, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.3538694381713867\n",
      "Step: 3422, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 1.504051923751831\n",
      "Step: 3423, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.408262014389038\n",
      "Step: 3424, Loss: 0.9161999821662903, Accuracy: 1.0, Computation time: 1.2007620334625244\n",
      "Step: 3425, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 1.3934297561645508\n",
      "Step: 3426, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.4343655109405518\n",
      "Step: 3427, Loss: 0.9376222491264343, Accuracy: 0.9750000238418579, Computation time: 2.4754655361175537\n",
      "Step: 3428, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.3268475532531738\n",
      "Step: 3429, Loss: 0.9375452995300293, Accuracy: 0.9791666865348816, Computation time: 1.2327187061309814\n",
      "Step: 3430, Loss: 0.9159807562828064, Accuracy: 1.0, Computation time: 1.3301527500152588\n",
      "Step: 3431, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.6052665710449219\n",
      "Step: 3432, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.2878971099853516\n",
      "Step: 3433, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 1.3465054035186768\n",
      "Step: 3434, Loss: 0.9159241914749146, Accuracy: 1.0, Computation time: 1.3907220363616943\n",
      "Step: 3435, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.4764740467071533\n",
      "Step: 3436, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.2999958992004395\n",
      "Step: 3437, Loss: 0.9166750907897949, Accuracy: 1.0, Computation time: 1.634889841079712\n",
      "Step: 3438, Loss: 0.9198666214942932, Accuracy: 1.0, Computation time: 1.5588874816894531\n",
      "Step: 3439, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.4632275104522705\n",
      "Step: 3440, Loss: 0.9160690903663635, Accuracy: 1.0, Computation time: 1.7652971744537354\n",
      "Step: 3441, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 1.5826547145843506\n",
      "Step: 3442, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.309814453125\n",
      "Step: 3443, Loss: 0.9159887433052063, Accuracy: 1.0, Computation time: 1.4240539073944092\n",
      "Step: 3444, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.4917585849761963\n",
      "Step: 3445, Loss: 0.9159587025642395, Accuracy: 1.0, Computation time: 1.5377705097198486\n",
      "Step: 3446, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.4601571559906006\n",
      "Step: 3447, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 1.5364418029785156\n",
      "Step: 3448, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.4214816093444824\n",
      "Step: 3449, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.2038049697875977\n",
      "Step: 3450, Loss: 0.9164564609527588, Accuracy: 1.0, Computation time: 1.5552082061767578\n",
      "Step: 3451, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 1.4461514949798584\n",
      "Step: 3452, Loss: 0.9375256896018982, Accuracy: 0.9821428656578064, Computation time: 1.1602070331573486\n",
      "Step: 3453, Loss: 0.9165558815002441, Accuracy: 1.0, Computation time: 2.1736559867858887\n",
      "Step: 3454, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.0869603157043457\n",
      "Step: 3455, Loss: 0.9215209484100342, Accuracy: 1.0, Computation time: 1.5977306365966797\n",
      "Step: 3456, Loss: 0.915996253490448, Accuracy: 1.0, Computation time: 1.3604135513305664\n",
      "Step: 3457, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 1.1817662715911865\n",
      "Step: 3458, Loss: 0.9159631729125977, Accuracy: 1.0, Computation time: 1.4380888938903809\n",
      "Step: 3459, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.545879602432251\n",
      "Step: 3460, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.39383864402771\n",
      "Step: 3461, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.318457841873169\n",
      "Step: 3462, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 1.6704366207122803\n",
      "Step: 3463, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.6273918151855469\n",
      "Step: 3464, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.798705816268921\n",
      "Step: 3465, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 1.5294828414916992\n",
      "Step: 3466, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.386258602142334\n",
      "Step: 3467, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 1.5296778678894043\n",
      "Step: 3468, Loss: 0.9159278869628906, Accuracy: 1.0, Computation time: 1.311704158782959\n",
      "Step: 3469, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.5732815265655518\n",
      "Step: 3470, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.7118809223175049\n",
      "Step: 3471, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.6449544429779053\n",
      "Step: 3472, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 1.3816795349121094\n",
      "Step: 3473, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 1.6834232807159424\n",
      "Step: 3474, Loss: 0.9346396327018738, Accuracy: 0.9750000238418579, Computation time: 1.44508695602417\n",
      "########################\n",
      "Test loss: 1.0716259479522705, Test Accuracy_epoch25: 0.7641894221305847\n",
      "########################\n",
      "Step: 3475, Loss: 0.9169541001319885, Accuracy: 1.0, Computation time: 1.8310184478759766\n",
      "Step: 3476, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 1.6929385662078857\n",
      "Step: 3477, Loss: 0.9160455465316772, Accuracy: 1.0, Computation time: 1.4286541938781738\n",
      "Step: 3478, Loss: 0.9159628748893738, Accuracy: 1.0, Computation time: 1.560739278793335\n",
      "Step: 3479, Loss: 0.9539468884468079, Accuracy: 0.9444444179534912, Computation time: 1.5457584857940674\n",
      "Step: 3480, Loss: 0.9160066246986389, Accuracy: 1.0, Computation time: 1.5421333312988281\n",
      "Step: 3481, Loss: 0.9159722924232483, Accuracy: 1.0, Computation time: 1.1118762493133545\n",
      "Step: 3482, Loss: 0.9159677028656006, Accuracy: 1.0, Computation time: 1.4534566402435303\n",
      "Step: 3483, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.349860429763794\n",
      "Step: 3484, Loss: 0.9377198219299316, Accuracy: 0.9642857313156128, Computation time: 1.7234323024749756\n",
      "Step: 3485, Loss: 0.939655065536499, Accuracy: 0.9722222089767456, Computation time: 1.7717475891113281\n",
      "Step: 3486, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 1.3008131980895996\n",
      "Step: 3487, Loss: 0.9172436594963074, Accuracy: 1.0, Computation time: 1.7622754573822021\n",
      "Step: 3488, Loss: 0.9377397298812866, Accuracy: 0.9807692766189575, Computation time: 1.297938585281372\n",
      "Step: 3489, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.676131248474121\n",
      "Step: 3490, Loss: 0.9164063930511475, Accuracy: 1.0, Computation time: 1.7185864448547363\n",
      "Step: 3491, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 2.247962236404419\n",
      "Step: 3492, Loss: 0.9376911520957947, Accuracy: 0.9772727489471436, Computation time: 1.1927480697631836\n",
      "Step: 3493, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.4143459796905518\n",
      "Step: 3494, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.555253028869629\n",
      "Step: 3495, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 1.239722728729248\n",
      "Step: 3496, Loss: 0.9168750643730164, Accuracy: 1.0, Computation time: 1.3071036338806152\n",
      "Step: 3497, Loss: 0.92835533618927, Accuracy: 0.9772727489471436, Computation time: 1.2738125324249268\n",
      "Step: 3498, Loss: 0.9374978542327881, Accuracy: 0.9772727489471436, Computation time: 1.8030171394348145\n",
      "Step: 3499, Loss: 0.9165685176849365, Accuracy: 1.0, Computation time: 1.3257904052734375\n",
      "Step: 3500, Loss: 0.9158850312232971, Accuracy: 1.0, Computation time: 1.3591816425323486\n",
      "Step: 3501, Loss: 0.9170446991920471, Accuracy: 1.0, Computation time: 1.817948341369629\n",
      "Step: 3502, Loss: 0.9164196252822876, Accuracy: 1.0, Computation time: 1.1350231170654297\n",
      "Step: 3503, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.0481293201446533\n",
      "Step: 3504, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.5676822662353516\n",
      "Step: 3505, Loss: 0.9159972071647644, Accuracy: 1.0, Computation time: 1.516401767730713\n",
      "Step: 3506, Loss: 0.9165803790092468, Accuracy: 1.0, Computation time: 1.9432027339935303\n",
      "Step: 3507, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.5063049793243408\n",
      "Step: 3508, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 1.6567857265472412\n",
      "Step: 3509, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.5687856674194336\n",
      "Step: 3510, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.654543161392212\n",
      "Step: 3511, Loss: 0.9189113974571228, Accuracy: 1.0, Computation time: 2.119427442550659\n",
      "Step: 3512, Loss: 0.937764048576355, Accuracy: 0.9852941036224365, Computation time: 1.7688970565795898\n",
      "Step: 3513, Loss: 0.9160401821136475, Accuracy: 1.0, Computation time: 1.7060832977294922\n",
      "Step: 3514, Loss: 0.9378415942192078, Accuracy: 0.9791666865348816, Computation time: 1.3115601539611816\n",
      "Step: 3515, Loss: 0.916008710861206, Accuracy: 1.0, Computation time: 1.5892462730407715\n",
      "Step: 3516, Loss: 0.9179065823554993, Accuracy: 1.0, Computation time: 1.4300634860992432\n",
      "Step: 3517, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.4593627452850342\n",
      "Step: 3518, Loss: 0.9159674644470215, Accuracy: 1.0, Computation time: 1.531923532485962\n",
      "Step: 3519, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.5152692794799805\n",
      "Step: 3520, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.4799902439117432\n",
      "Step: 3521, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 2.234358072280884\n",
      "Step: 3522, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.529343605041504\n",
      "Step: 3523, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.9013094902038574\n",
      "Step: 3524, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.2006797790527344\n",
      "Step: 3525, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.523719072341919\n",
      "Step: 3526, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 1.8117947578430176\n",
      "Step: 3527, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.4748742580413818\n",
      "Step: 3528, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.5514166355133057\n",
      "Step: 3529, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.5742273330688477\n",
      "Step: 3530, Loss: 0.915932297706604, Accuracy: 1.0, Computation time: 1.3459255695343018\n",
      "Step: 3531, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.2872323989868164\n",
      "Step: 3532, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.2754263877868652\n",
      "Step: 3533, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 1.5993964672088623\n",
      "Step: 3534, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.46303391456604\n",
      "Step: 3535, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.621699571609497\n",
      "Step: 3536, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.451301097869873\n",
      "Step: 3537, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.4884693622589111\n",
      "Step: 3538, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.5434937477111816\n",
      "Step: 3539, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.3631658554077148\n",
      "Step: 3540, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.3075611591339111\n",
      "Step: 3541, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 2.048278570175171\n",
      "Step: 3542, Loss: 0.9185842871665955, Accuracy: 1.0, Computation time: 1.5073206424713135\n",
      "Step: 3543, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.6194636821746826\n",
      "Step: 3544, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.5025804042816162\n",
      "Step: 3545, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.6652190685272217\n",
      "Step: 3546, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 2.0588395595550537\n",
      "Step: 3547, Loss: 0.937531054019928, Accuracy: 0.96875, Computation time: 1.5213298797607422\n",
      "Step: 3548, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 1.8742942810058594\n",
      "Step: 3549, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.6696786880493164\n",
      "Step: 3550, Loss: 0.9172186851501465, Accuracy: 1.0, Computation time: 1.697251796722412\n",
      "Step: 3551, Loss: 0.938048243522644, Accuracy: 0.9583333730697632, Computation time: 1.5964939594268799\n",
      "Step: 3552, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.491011142730713\n",
      "Step: 3553, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.2374303340911865\n",
      "Step: 3554, Loss: 0.9159314036369324, Accuracy: 1.0, Computation time: 1.2745113372802734\n",
      "Step: 3555, Loss: 0.9164508581161499, Accuracy: 1.0, Computation time: 1.509162187576294\n",
      "Step: 3556, Loss: 0.9378198981285095, Accuracy: 0.9750000238418579, Computation time: 1.630643606185913\n",
      "Step: 3557, Loss: 0.9378641247749329, Accuracy: 0.9375, Computation time: 1.5712089538574219\n",
      "Step: 3558, Loss: 0.9159995317459106, Accuracy: 1.0, Computation time: 1.5487265586853027\n",
      "Step: 3559, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.4472136497497559\n",
      "Step: 3560, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.8676471710205078\n",
      "Step: 3561, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 1.464794397354126\n",
      "Step: 3562, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 1.475578784942627\n",
      "Step: 3563, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 1.564074993133545\n",
      "Step: 3564, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.4571821689605713\n",
      "Step: 3565, Loss: 0.9376040101051331, Accuracy: 0.9722222089767456, Computation time: 1.5042362213134766\n",
      "Step: 3566, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.4303736686706543\n",
      "Step: 3567, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 1.585458517074585\n",
      "Step: 3568, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.4159157276153564\n",
      "Step: 3569, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.43802809715271\n",
      "Step: 3570, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.4783880710601807\n",
      "Step: 3571, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 2.1216726303100586\n",
      "Step: 3572, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.3706324100494385\n",
      "Step: 3573, Loss: 0.9377412796020508, Accuracy: 0.96875, Computation time: 1.660090446472168\n",
      "Step: 3574, Loss: 0.9374820590019226, Accuracy: 0.9772727489471436, Computation time: 1.6667444705963135\n",
      "Step: 3575, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.371814489364624\n",
      "Step: 3576, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.5013422966003418\n",
      "Step: 3577, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 1.7500598430633545\n",
      "Step: 3578, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.5536479949951172\n",
      "Step: 3579, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 1.5005619525909424\n",
      "Step: 3580, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.466965913772583\n",
      "Step: 3581, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.3389041423797607\n",
      "Step: 3582, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.572190284729004\n",
      "Step: 3583, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.5361862182617188\n",
      "Step: 3584, Loss: 0.9160662889480591, Accuracy: 1.0, Computation time: 1.6505639553070068\n",
      "Step: 3585, Loss: 0.9357025027275085, Accuracy: 0.9642857313156128, Computation time: 1.970339059829712\n",
      "Step: 3586, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.7360739707946777\n",
      "Step: 3587, Loss: 0.9374946355819702, Accuracy: 0.9722222089767456, Computation time: 1.3540401458740234\n",
      "Step: 3588, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 1.4004173278808594\n",
      "Step: 3589, Loss: 0.9375821948051453, Accuracy: 0.9750000238418579, Computation time: 2.0748090744018555\n",
      "Step: 3590, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.5562505722045898\n",
      "Step: 3591, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.450432538986206\n",
      "Step: 3592, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.4601480960845947\n",
      "Step: 3593, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.2435193061828613\n",
      "Step: 3594, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.6625208854675293\n",
      "Step: 3595, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.2573585510253906\n",
      "Step: 3596, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.557469129562378\n",
      "Step: 3597, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.6442854404449463\n",
      "Step: 3598, Loss: 0.9374916553497314, Accuracy: 0.96875, Computation time: 1.4878926277160645\n",
      "Step: 3599, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.8215992450714111\n",
      "Step: 3600, Loss: 0.9375025629997253, Accuracy: 0.9791666865348816, Computation time: 1.7072503566741943\n",
      "Step: 3601, Loss: 0.9163296222686768, Accuracy: 1.0, Computation time: 1.5621120929718018\n",
      "Step: 3602, Loss: 0.9355460405349731, Accuracy: 0.96875, Computation time: 2.0510919094085693\n",
      "Step: 3603, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.3621244430541992\n",
      "Step: 3604, Loss: 0.9375184774398804, Accuracy: 0.96875, Computation time: 1.58986234664917\n",
      "Step: 3605, Loss: 0.9159846305847168, Accuracy: 1.0, Computation time: 1.5298120975494385\n",
      "Step: 3606, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.4720678329467773\n",
      "Step: 3607, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.8131122589111328\n",
      "Step: 3608, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.851651906967163\n",
      "Step: 3609, Loss: 0.9159433245658875, Accuracy: 1.0, Computation time: 2.27445125579834\n",
      "Step: 3610, Loss: 0.9272705912590027, Accuracy: 0.9642857313156128, Computation time: 1.9822628498077393\n",
      "Step: 3611, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 1.5543158054351807\n",
      "Step: 3612, Loss: 0.9163897633552551, Accuracy: 1.0, Computation time: 1.4742581844329834\n",
      "Step: 3613, Loss: 0.9170516729354858, Accuracy: 1.0, Computation time: 1.3888027667999268\n",
      "########################\n",
      "Test loss: 1.0725289583206177, Test Accuracy_epoch26: 0.760517954826355\n",
      "########################\n",
      "Step: 3614, Loss: 0.9163458943367004, Accuracy: 1.0, Computation time: 1.6578309535980225\n",
      "Step: 3615, Loss: 0.9164496064186096, Accuracy: 1.0, Computation time: 2.246265172958374\n",
      "Step: 3616, Loss: 0.9160844683647156, Accuracy: 1.0, Computation time: 1.447749137878418\n",
      "Step: 3617, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.4174981117248535\n",
      "Step: 3618, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 1.7000062465667725\n",
      "Step: 3619, Loss: 0.9159263968467712, Accuracy: 1.0, Computation time: 1.6539056301116943\n",
      "Step: 3620, Loss: 0.9374487996101379, Accuracy: 0.9750000238418579, Computation time: 1.9300193786621094\n",
      "Step: 3621, Loss: 0.9160386919975281, Accuracy: 1.0, Computation time: 1.3714048862457275\n",
      "Step: 3622, Loss: 0.9161068797111511, Accuracy: 1.0, Computation time: 1.5502893924713135\n",
      "Step: 3623, Loss: 0.9160904288291931, Accuracy: 1.0, Computation time: 1.2374317646026611\n",
      "Step: 3624, Loss: 0.9161014556884766, Accuracy: 1.0, Computation time: 1.1904315948486328\n",
      "Step: 3625, Loss: 0.919341504573822, Accuracy: 1.0, Computation time: 1.3029539585113525\n",
      "Step: 3626, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 1.4210619926452637\n",
      "Step: 3627, Loss: 0.9159874320030212, Accuracy: 1.0, Computation time: 1.5051329135894775\n",
      "Step: 3628, Loss: 0.9160650968551636, Accuracy: 1.0, Computation time: 1.1414029598236084\n",
      "Step: 3629, Loss: 0.9169807434082031, Accuracy: 1.0, Computation time: 1.2681477069854736\n",
      "Step: 3630, Loss: 0.9160161018371582, Accuracy: 1.0, Computation time: 1.1831514835357666\n",
      "Step: 3631, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.174731969833374\n",
      "Step: 3632, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.5740015506744385\n",
      "Step: 3633, Loss: 0.9159323573112488, Accuracy: 1.0, Computation time: 1.4079253673553467\n",
      "Step: 3634, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.5635135173797607\n",
      "Step: 3635, Loss: 0.9353427290916443, Accuracy: 0.9750000238418579, Computation time: 1.4308843612670898\n",
      "Step: 3636, Loss: 0.9180625081062317, Accuracy: 1.0, Computation time: 1.9628031253814697\n",
      "Step: 3637, Loss: 0.9160432815551758, Accuracy: 1.0, Computation time: 1.2520971298217773\n",
      "Step: 3638, Loss: 0.9290655851364136, Accuracy: 0.984375, Computation time: 1.1291394233703613\n",
      "Step: 3639, Loss: 0.9161366820335388, Accuracy: 1.0, Computation time: 1.3419268131256104\n",
      "Step: 3640, Loss: 0.9160662889480591, Accuracy: 1.0, Computation time: 1.2819902896881104\n",
      "Step: 3641, Loss: 0.9159673452377319, Accuracy: 1.0, Computation time: 1.267745018005371\n",
      "Step: 3642, Loss: 0.9160453081130981, Accuracy: 1.0, Computation time: 1.0909771919250488\n",
      "Step: 3643, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 1.6921017169952393\n",
      "Step: 3644, Loss: 0.9160014986991882, Accuracy: 1.0, Computation time: 1.291637897491455\n",
      "Step: 3645, Loss: 0.9240428805351257, Accuracy: 1.0, Computation time: 1.5137395858764648\n",
      "Step: 3646, Loss: 0.916056215763092, Accuracy: 1.0, Computation time: 1.5050270557403564\n",
      "Step: 3647, Loss: 0.9159450531005859, Accuracy: 1.0, Computation time: 1.4418230056762695\n",
      "Step: 3648, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 1.2720482349395752\n",
      "Step: 3649, Loss: 0.9176620841026306, Accuracy: 1.0, Computation time: 1.3277380466461182\n",
      "Step: 3650, Loss: 0.9160121083259583, Accuracy: 1.0, Computation time: 1.311493158340454\n",
      "Step: 3651, Loss: 0.9159446954727173, Accuracy: 1.0, Computation time: 1.2089428901672363\n",
      "Step: 3652, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.1856439113616943\n",
      "Step: 3653, Loss: 0.915974497795105, Accuracy: 1.0, Computation time: 1.4587233066558838\n",
      "Step: 3654, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.479588508605957\n",
      "Step: 3655, Loss: 0.9159795045852661, Accuracy: 1.0, Computation time: 1.6039128303527832\n",
      "Step: 3656, Loss: 0.9160305261611938, Accuracy: 1.0, Computation time: 1.4919943809509277\n",
      "Step: 3657, Loss: 0.9159596562385559, Accuracy: 1.0, Computation time: 1.3098292350769043\n",
      "Step: 3658, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.2613365650177002\n",
      "Step: 3659, Loss: 0.9181006550788879, Accuracy: 1.0, Computation time: 1.6967556476593018\n",
      "Step: 3660, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.7260499000549316\n",
      "Step: 3661, Loss: 0.917085587978363, Accuracy: 1.0, Computation time: 1.2336385250091553\n",
      "Step: 3662, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.4421706199645996\n",
      "Step: 3663, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.4602231979370117\n",
      "Step: 3664, Loss: 0.9159708023071289, Accuracy: 1.0, Computation time: 1.174424171447754\n",
      "Step: 3665, Loss: 0.918646514415741, Accuracy: 1.0, Computation time: 1.396409273147583\n",
      "Step: 3666, Loss: 0.9373002052307129, Accuracy: 0.9750000238418579, Computation time: 1.5269410610198975\n",
      "Step: 3667, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.3858213424682617\n",
      "Step: 3668, Loss: 0.9191068410873413, Accuracy: 1.0, Computation time: 1.4897229671478271\n",
      "Step: 3669, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.5997118949890137\n",
      "Step: 3670, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.231187343597412\n",
      "Step: 3671, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 1.3821327686309814\n",
      "Step: 3672, Loss: 0.9160016775131226, Accuracy: 1.0, Computation time: 1.7647039890289307\n",
      "Step: 3673, Loss: 0.915971577167511, Accuracy: 1.0, Computation time: 1.9209094047546387\n",
      "Step: 3674, Loss: 0.9160100221633911, Accuracy: 1.0, Computation time: 1.3643715381622314\n",
      "Step: 3675, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.6083080768585205\n",
      "Step: 3676, Loss: 0.9159891605377197, Accuracy: 1.0, Computation time: 1.32193922996521\n",
      "Step: 3677, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.3556809425354004\n",
      "Step: 3678, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 1.4764695167541504\n",
      "Step: 3679, Loss: 0.9238768219947815, Accuracy: 1.0, Computation time: 1.5511384010314941\n",
      "Step: 3680, Loss: 0.9200310707092285, Accuracy: 1.0, Computation time: 1.529942512512207\n",
      "Step: 3681, Loss: 0.9159834980964661, Accuracy: 1.0, Computation time: 1.238051176071167\n",
      "Step: 3682, Loss: 0.915995717048645, Accuracy: 1.0, Computation time: 1.5875787734985352\n",
      "Step: 3683, Loss: 0.916172206401825, Accuracy: 1.0, Computation time: 1.3947021961212158\n",
      "Step: 3684, Loss: 0.9166818261146545, Accuracy: 1.0, Computation time: 1.6759347915649414\n",
      "Step: 3685, Loss: 0.9165091514587402, Accuracy: 1.0, Computation time: 1.385627031326294\n",
      "Step: 3686, Loss: 0.915966808795929, Accuracy: 1.0, Computation time: 1.3585870265960693\n",
      "Step: 3687, Loss: 0.9160584807395935, Accuracy: 1.0, Computation time: 1.5153868198394775\n",
      "Step: 3688, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 1.3819313049316406\n",
      "Step: 3689, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 1.571077585220337\n",
      "Step: 3690, Loss: 0.9375990629196167, Accuracy: 0.9642857313156128, Computation time: 1.5326707363128662\n",
      "Step: 3691, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.4603028297424316\n",
      "Step: 3692, Loss: 0.9159379005432129, Accuracy: 1.0, Computation time: 1.2800090312957764\n",
      "Step: 3693, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.068960428237915\n",
      "Step: 3694, Loss: 0.9159033894538879, Accuracy: 1.0, Computation time: 1.357236623764038\n",
      "Step: 3695, Loss: 0.9166986346244812, Accuracy: 1.0, Computation time: 1.8044276237487793\n",
      "Step: 3696, Loss: 0.9375816583633423, Accuracy: 0.9750000238418579, Computation time: 1.6485862731933594\n",
      "Step: 3697, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.1258628368377686\n",
      "Step: 3698, Loss: 0.9178632497787476, Accuracy: 1.0, Computation time: 1.3319547176361084\n",
      "Step: 3699, Loss: 0.9376388788223267, Accuracy: 0.96875, Computation time: 1.384373664855957\n",
      "Step: 3700, Loss: 0.9580004215240479, Accuracy: 0.9375, Computation time: 1.547137975692749\n",
      "Step: 3701, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.43479585647583\n",
      "Step: 3702, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 1.2527263164520264\n",
      "Step: 3703, Loss: 0.9162377119064331, Accuracy: 1.0, Computation time: 1.2033815383911133\n",
      "Step: 3704, Loss: 0.9166592359542847, Accuracy: 1.0, Computation time: 1.4519875049591064\n",
      "Step: 3705, Loss: 0.9159415364265442, Accuracy: 1.0, Computation time: 1.266092300415039\n",
      "Step: 3706, Loss: 0.9162448644638062, Accuracy: 1.0, Computation time: 1.7105703353881836\n",
      "Step: 3707, Loss: 0.9159801006317139, Accuracy: 1.0, Computation time: 1.2527306079864502\n",
      "Step: 3708, Loss: 0.9160954356193542, Accuracy: 1.0, Computation time: 1.5029871463775635\n",
      "Step: 3709, Loss: 0.9159240126609802, Accuracy: 1.0, Computation time: 1.4216904640197754\n",
      "Step: 3710, Loss: 0.9161132574081421, Accuracy: 1.0, Computation time: 1.278055191040039\n",
      "Step: 3711, Loss: 0.9159784317016602, Accuracy: 1.0, Computation time: 1.6315762996673584\n",
      "Step: 3712, Loss: 0.9160212874412537, Accuracy: 1.0, Computation time: 1.2548332214355469\n",
      "Step: 3713, Loss: 0.9159905314445496, Accuracy: 1.0, Computation time: 1.9632611274719238\n",
      "Step: 3714, Loss: 0.9159781336784363, Accuracy: 1.0, Computation time: 1.538475751876831\n",
      "Step: 3715, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.4668827056884766\n",
      "Step: 3716, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.2858037948608398\n",
      "Step: 3717, Loss: 0.9377409219741821, Accuracy: 0.949999988079071, Computation time: 1.1444892883300781\n",
      "Step: 3718, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 1.282820224761963\n",
      "Step: 3719, Loss: 0.9376668334007263, Accuracy: 0.9772727489471436, Computation time: 1.4783568382263184\n",
      "Step: 3720, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 1.1327135562896729\n",
      "Step: 3721, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 1.4153571128845215\n",
      "Step: 3722, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.4380865097045898\n",
      "Step: 3723, Loss: 0.9167479276657104, Accuracy: 1.0, Computation time: 1.6043691635131836\n",
      "Step: 3724, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.4333274364471436\n",
      "Step: 3725, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 1.5553817749023438\n",
      "Step: 3726, Loss: 0.9158729910850525, Accuracy: 1.0, Computation time: 1.4966375827789307\n",
      "Step: 3727, Loss: 0.9163156747817993, Accuracy: 1.0, Computation time: 1.2329866886138916\n",
      "Step: 3728, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 1.8283710479736328\n",
      "Step: 3729, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 1.2143046855926514\n",
      "Step: 3730, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 1.126396894454956\n",
      "Step: 3731, Loss: 0.9159368276596069, Accuracy: 1.0, Computation time: 1.5519752502441406\n",
      "Step: 3732, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.6477127075195312\n",
      "Step: 3733, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.2195346355438232\n",
      "Step: 3734, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.745121717453003\n",
      "Step: 3735, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.5731945037841797\n",
      "Step: 3736, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 1.293976068496704\n",
      "Step: 3737, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.2036280632019043\n",
      "Step: 3738, Loss: 0.9337646961212158, Accuracy: 0.9807692766189575, Computation time: 1.7676761150360107\n",
      "Step: 3739, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.548203945159912\n",
      "Step: 3740, Loss: 0.9214332103729248, Accuracy: 1.0, Computation time: 1.682218313217163\n",
      "Step: 3741, Loss: 0.9374216198921204, Accuracy: 0.9807692766189575, Computation time: 1.4104318618774414\n",
      "Step: 3742, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 1.5026004314422607\n",
      "Step: 3743, Loss: 0.9376860857009888, Accuracy: 0.9791666865348816, Computation time: 1.3815257549285889\n",
      "Step: 3744, Loss: 0.9372643828392029, Accuracy: 0.9772727489471436, Computation time: 1.4057250022888184\n",
      "Step: 3745, Loss: 0.9160130620002747, Accuracy: 1.0, Computation time: 1.2490534782409668\n",
      "Step: 3746, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 1.3411619663238525\n",
      "Step: 3747, Loss: 0.9301720857620239, Accuracy: 0.9642857313156128, Computation time: 1.569551944732666\n",
      "Step: 3748, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.4147706031799316\n",
      "Step: 3749, Loss: 0.9158954620361328, Accuracy: 1.0, Computation time: 1.6297037601470947\n",
      "Step: 3750, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.9141838550567627\n",
      "Step: 3751, Loss: 0.9159023761749268, Accuracy: 1.0, Computation time: 1.4368088245391846\n",
      "Step: 3752, Loss: 0.9159901738166809, Accuracy: 1.0, Computation time: 1.7972707748413086\n",
      "########################\n",
      "Test loss: 1.071656346321106, Test Accuracy_epoch27: 0.760169506072998\n",
      "########################\n",
      "Step: 3753, Loss: 0.9159755110740662, Accuracy: 1.0, Computation time: 1.3449387550354004\n",
      "Step: 3754, Loss: 0.9159500002861023, Accuracy: 1.0, Computation time: 1.2273588180541992\n",
      "Step: 3755, Loss: 0.916706383228302, Accuracy: 1.0, Computation time: 1.4042999744415283\n",
      "Step: 3756, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 1.483665943145752\n",
      "Step: 3757, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 1.476313829421997\n",
      "Step: 3758, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.7042734622955322\n",
      "Step: 3759, Loss: 0.9160280227661133, Accuracy: 1.0, Computation time: 1.6734685897827148\n",
      "Step: 3760, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.3760793209075928\n",
      "Step: 3761, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.5174198150634766\n",
      "Step: 3762, Loss: 0.9159350395202637, Accuracy: 1.0, Computation time: 1.501720666885376\n",
      "Step: 3763, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.4380874633789062\n",
      "Step: 3764, Loss: 0.9165407419204712, Accuracy: 1.0, Computation time: 1.4406614303588867\n",
      "Step: 3765, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.5010714530944824\n",
      "Step: 3766, Loss: 0.9165452122688293, Accuracy: 1.0, Computation time: 1.5216617584228516\n",
      "Step: 3767, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.0683257579803467\n",
      "Step: 3768, Loss: 0.9159002900123596, Accuracy: 1.0, Computation time: 1.536705732345581\n",
      "Step: 3769, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 1.6626310348510742\n",
      "Step: 3770, Loss: 0.9177640676498413, Accuracy: 1.0, Computation time: 1.074728012084961\n",
      "Step: 3771, Loss: 0.9594259262084961, Accuracy: 0.9500000476837158, Computation time: 1.2067782878875732\n",
      "Step: 3772, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.2028441429138184\n",
      "Step: 3773, Loss: 0.9158948063850403, Accuracy: 1.0, Computation time: 1.3237476348876953\n",
      "Step: 3774, Loss: 0.9376305341720581, Accuracy: 0.96875, Computation time: 1.3897342681884766\n",
      "Step: 3775, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 1.5510292053222656\n",
      "Step: 3776, Loss: 0.9159121513366699, Accuracy: 1.0, Computation time: 1.4431922435760498\n",
      "Step: 3777, Loss: 0.9374459385871887, Accuracy: 0.96875, Computation time: 1.792701005935669\n",
      "Step: 3778, Loss: 0.916354238986969, Accuracy: 1.0, Computation time: 1.4598112106323242\n",
      "Step: 3779, Loss: 0.9183847308158875, Accuracy: 1.0, Computation time: 1.518131971359253\n",
      "Step: 3780, Loss: 0.916766881942749, Accuracy: 1.0, Computation time: 1.149587631225586\n",
      "Step: 3781, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.7129735946655273\n",
      "Step: 3782, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.163489818572998\n",
      "Step: 3783, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.3836033344268799\n",
      "Step: 3784, Loss: 0.9160272479057312, Accuracy: 1.0, Computation time: 1.4554808139801025\n",
      "Step: 3785, Loss: 0.9162840843200684, Accuracy: 1.0, Computation time: 1.3407773971557617\n",
      "Step: 3786, Loss: 0.9375678300857544, Accuracy: 0.9807692766189575, Computation time: 1.3364496231079102\n",
      "Step: 3787, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.3060879707336426\n",
      "Step: 3788, Loss: 0.937395453453064, Accuracy: 0.9791666865348816, Computation time: 1.160351276397705\n",
      "Step: 3789, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.534360647201538\n",
      "Step: 3790, Loss: 0.915931224822998, Accuracy: 1.0, Computation time: 1.900681972503662\n",
      "Step: 3791, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.5885496139526367\n",
      "Step: 3792, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.5204570293426514\n",
      "Step: 3793, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.4445300102233887\n",
      "Step: 3794, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.5464107990264893\n",
      "Step: 3795, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.2148752212524414\n",
      "Step: 3796, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.2534105777740479\n",
      "Step: 3797, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.389683723449707\n",
      "Step: 3798, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.627690076828003\n",
      "Step: 3799, Loss: 0.937502384185791, Accuracy: 0.96875, Computation time: 1.8130013942718506\n",
      "Step: 3800, Loss: 0.9376590847969055, Accuracy: 0.96875, Computation time: 1.356436014175415\n",
      "Step: 3801, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.3787415027618408\n",
      "Step: 3802, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.1995089054107666\n",
      "Step: 3803, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.414461612701416\n",
      "Step: 3804, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.6831822395324707\n",
      "Step: 3805, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 1.289724349975586\n",
      "Step: 3806, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.7473666667938232\n",
      "Step: 3807, Loss: 0.9375972151756287, Accuracy: 0.9722222089767456, Computation time: 1.5575478076934814\n",
      "Step: 3808, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.4687154293060303\n",
      "Step: 3809, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.3826532363891602\n",
      "Step: 3810, Loss: 0.9165874719619751, Accuracy: 1.0, Computation time: 1.390671730041504\n",
      "Step: 3811, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.3514461517333984\n",
      "Step: 3812, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 2.0238640308380127\n",
      "Step: 3813, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 1.586446762084961\n",
      "Step: 3814, Loss: 0.9165043830871582, Accuracy: 1.0, Computation time: 1.3752055168151855\n",
      "Step: 3815, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.5404870510101318\n",
      "Step: 3816, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 1.3094804286956787\n",
      "Step: 3817, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.2777173519134521\n",
      "Step: 3818, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.6494030952453613\n",
      "Step: 3819, Loss: 0.9177327752113342, Accuracy: 1.0, Computation time: 1.7456693649291992\n",
      "Step: 3820, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.2332732677459717\n",
      "Step: 3821, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.9037678241729736\n",
      "Step: 3822, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.0445239543914795\n",
      "Step: 3823, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.4223170280456543\n",
      "Step: 3824, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.5055763721466064\n",
      "Step: 3825, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 1.487546682357788\n",
      "Step: 3826, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.462674856185913\n",
      "Step: 3827, Loss: 0.916252851486206, Accuracy: 1.0, Computation time: 1.3920323848724365\n",
      "Step: 3828, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.5685067176818848\n",
      "Step: 3829, Loss: 0.9264957904815674, Accuracy: 0.9750000238418579, Computation time: 1.302412509918213\n",
      "Step: 3830, Loss: 0.9159753322601318, Accuracy: 1.0, Computation time: 1.1986191272735596\n",
      "Step: 3831, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.3194336891174316\n",
      "Step: 3832, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.334306240081787\n",
      "Step: 3833, Loss: 0.916411817073822, Accuracy: 1.0, Computation time: 1.4513487815856934\n",
      "Step: 3834, Loss: 0.9159129858016968, Accuracy: 1.0, Computation time: 1.1803538799285889\n",
      "Step: 3835, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.8312153816223145\n",
      "Step: 3836, Loss: 0.928274929523468, Accuracy: 0.9750000238418579, Computation time: 1.6373584270477295\n",
      "Step: 3837, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 1.4586946964263916\n",
      "Step: 3838, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.6326994895935059\n",
      "Step: 3839, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.3401198387145996\n",
      "Step: 3840, Loss: 0.9163064956665039, Accuracy: 1.0, Computation time: 1.652644395828247\n",
      "Step: 3841, Loss: 0.9161710739135742, Accuracy: 1.0, Computation time: 1.6467115879058838\n",
      "Step: 3842, Loss: 0.916047990322113, Accuracy: 1.0, Computation time: 1.5719966888427734\n",
      "Step: 3843, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.6689209938049316\n",
      "Step: 3844, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.6857059001922607\n",
      "Step: 3845, Loss: 0.916387140750885, Accuracy: 1.0, Computation time: 1.885690450668335\n",
      "Step: 3846, Loss: 0.9159939885139465, Accuracy: 1.0, Computation time: 1.517155408859253\n",
      "Step: 3847, Loss: 0.9160608649253845, Accuracy: 1.0, Computation time: 1.4410176277160645\n",
      "Step: 3848, Loss: 0.9162538051605225, Accuracy: 1.0, Computation time: 1.6136415004730225\n",
      "Step: 3849, Loss: 0.9180170893669128, Accuracy: 1.0, Computation time: 1.9541630744934082\n",
      "Step: 3850, Loss: 0.9205192923545837, Accuracy: 1.0, Computation time: 2.195122480392456\n",
      "Step: 3851, Loss: 0.9384787678718567, Accuracy: 0.96875, Computation time: 1.6437077522277832\n",
      "Step: 3852, Loss: 0.9162220358848572, Accuracy: 1.0, Computation time: 1.3907501697540283\n",
      "Step: 3853, Loss: 0.9160739183425903, Accuracy: 1.0, Computation time: 1.625554084777832\n",
      "Step: 3854, Loss: 0.9160940051078796, Accuracy: 1.0, Computation time: 1.2079548835754395\n",
      "Step: 3855, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 2.010211944580078\n",
      "Step: 3856, Loss: 0.915911078453064, Accuracy: 1.0, Computation time: 1.4220690727233887\n",
      "Step: 3857, Loss: 0.917791485786438, Accuracy: 1.0, Computation time: 1.3439300060272217\n",
      "Step: 3858, Loss: 0.9378379583358765, Accuracy: 0.9722222089767456, Computation time: 1.7482030391693115\n",
      "Step: 3859, Loss: 0.9160116910934448, Accuracy: 1.0, Computation time: 1.3499305248260498\n",
      "Step: 3860, Loss: 0.9170256853103638, Accuracy: 1.0, Computation time: 2.0888845920562744\n",
      "Step: 3861, Loss: 0.9345985651016235, Accuracy: 0.9375, Computation time: 1.357896327972412\n",
      "Step: 3862, Loss: 0.9160130023956299, Accuracy: 1.0, Computation time: 1.8154327869415283\n",
      "Step: 3863, Loss: 0.9161379337310791, Accuracy: 1.0, Computation time: 1.4546122550964355\n",
      "Step: 3864, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.2573513984680176\n",
      "Step: 3865, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 1.3083772659301758\n",
      "Step: 3866, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.3648569583892822\n",
      "Step: 3867, Loss: 0.9159027934074402, Accuracy: 1.0, Computation time: 1.3968539237976074\n",
      "Step: 3868, Loss: 0.9160841703414917, Accuracy: 1.0, Computation time: 1.2284064292907715\n",
      "Step: 3869, Loss: 0.9377323389053345, Accuracy: 0.9772727489471436, Computation time: 1.7208349704742432\n",
      "Step: 3870, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.672100305557251\n",
      "Step: 3871, Loss: 0.937799870967865, Accuracy: 0.9791666865348816, Computation time: 2.302558660507202\n",
      "Step: 3872, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 2.007877826690674\n",
      "Step: 3873, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.5334434509277344\n",
      "Step: 3874, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.5717601776123047\n",
      "Step: 3875, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.3044326305389404\n",
      "Step: 3876, Loss: 0.9226820468902588, Accuracy: 1.0, Computation time: 1.487983226776123\n",
      "Step: 3877, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.1962499618530273\n",
      "Step: 3878, Loss: 0.9159321784973145, Accuracy: 1.0, Computation time: 1.3689229488372803\n",
      "Step: 3879, Loss: 0.9160088896751404, Accuracy: 1.0, Computation time: 1.5090405941009521\n",
      "Step: 3880, Loss: 0.9377430081367493, Accuracy: 0.9791666865348816, Computation time: 1.252467155456543\n",
      "Step: 3881, Loss: 0.9160974621772766, Accuracy: 1.0, Computation time: 1.5070264339447021\n",
      "Step: 3882, Loss: 0.915928065776825, Accuracy: 1.0, Computation time: 1.3547117710113525\n",
      "Step: 3883, Loss: 0.9159913063049316, Accuracy: 1.0, Computation time: 1.4538555145263672\n",
      "Step: 3884, Loss: 0.9162549376487732, Accuracy: 1.0, Computation time: 1.2927954196929932\n",
      "Step: 3885, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.4847424030303955\n",
      "Step: 3886, Loss: 0.9377686977386475, Accuracy: 0.9750000238418579, Computation time: 1.5567116737365723\n",
      "Step: 3887, Loss: 0.9159278273582458, Accuracy: 1.0, Computation time: 1.324359655380249\n",
      "Step: 3888, Loss: 0.9375821352005005, Accuracy: 0.96875, Computation time: 1.3895001411437988\n",
      "Step: 3889, Loss: 0.9161249399185181, Accuracy: 1.0, Computation time: 1.158024549484253\n",
      "Step: 3890, Loss: 0.9159473180770874, Accuracy: 1.0, Computation time: 1.319096326828003\n",
      "Step: 3891, Loss: 0.9160207509994507, Accuracy: 1.0, Computation time: 1.2840569019317627\n",
      "########################\n",
      "Test loss: 1.0699326992034912, Test Accuracy_epoch28: 0.7664430141448975\n",
      "########################\n",
      "Step: 3892, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 1.0956990718841553\n",
      "Step: 3893, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 1.216038465499878\n",
      "Step: 3894, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.3013746738433838\n",
      "Step: 3895, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.2592616081237793\n",
      "Step: 3896, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.2716617584228516\n",
      "Step: 3897, Loss: 0.9159942269325256, Accuracy: 1.0, Computation time: 1.1819536685943604\n",
      "Step: 3898, Loss: 0.9163117408752441, Accuracy: 1.0, Computation time: 1.471339464187622\n",
      "Step: 3899, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.647853136062622\n",
      "Step: 3900, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.4554429054260254\n",
      "Step: 3901, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 1.3314995765686035\n",
      "Step: 3902, Loss: 0.9159426093101501, Accuracy: 1.0, Computation time: 2.3210127353668213\n",
      "Step: 3903, Loss: 0.9188845157623291, Accuracy: 1.0, Computation time: 1.5414605140686035\n",
      "Step: 3904, Loss: 0.9161224365234375, Accuracy: 1.0, Computation time: 1.3785569667816162\n",
      "Step: 3905, Loss: 0.9250491261482239, Accuracy: 1.0, Computation time: 2.2630808353424072\n",
      "Step: 3906, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.3256189823150635\n",
      "Step: 3907, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 1.279541254043579\n",
      "Step: 3908, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.25649094581604\n",
      "Step: 3909, Loss: 0.9376211762428284, Accuracy: 0.9807692766189575, Computation time: 1.4091968536376953\n",
      "Step: 3910, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.2742345333099365\n",
      "Step: 3911, Loss: 0.9368957877159119, Accuracy: 0.9642857313156128, Computation time: 1.6694667339324951\n",
      "Step: 3912, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 0.9956800937652588\n",
      "Step: 3913, Loss: 0.9160353541374207, Accuracy: 1.0, Computation time: 1.2549970149993896\n",
      "Step: 3914, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.6638586521148682\n",
      "Step: 3915, Loss: 0.9335307478904724, Accuracy: 0.9772727489471436, Computation time: 2.1762542724609375\n",
      "Step: 3916, Loss: 0.9159429669380188, Accuracy: 1.0, Computation time: 1.2824583053588867\n",
      "Step: 3917, Loss: 0.9160106778144836, Accuracy: 1.0, Computation time: 1.3911123275756836\n",
      "Step: 3918, Loss: 0.9159848690032959, Accuracy: 1.0, Computation time: 1.1113941669464111\n",
      "Step: 3919, Loss: 0.9182773232460022, Accuracy: 1.0, Computation time: 1.3159914016723633\n",
      "Step: 3920, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.1938281059265137\n",
      "Step: 3921, Loss: 0.9172095656394958, Accuracy: 1.0, Computation time: 1.4290556907653809\n",
      "Step: 3922, Loss: 0.9159849882125854, Accuracy: 1.0, Computation time: 1.4642865657806396\n",
      "Step: 3923, Loss: 0.915950357913971, Accuracy: 1.0, Computation time: 1.2140324115753174\n",
      "Step: 3924, Loss: 0.9160308241844177, Accuracy: 1.0, Computation time: 1.0446038246154785\n",
      "Step: 3925, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.2009789943695068\n",
      "Step: 3926, Loss: 0.9159073233604431, Accuracy: 1.0, Computation time: 1.1317074298858643\n",
      "Step: 3927, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.1112539768218994\n",
      "Step: 3928, Loss: 0.9159606695175171, Accuracy: 1.0, Computation time: 1.0163843631744385\n",
      "Step: 3929, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 1.4900918006896973\n",
      "Step: 3930, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.2747302055358887\n",
      "Step: 3931, Loss: 0.9369212985038757, Accuracy: 0.984375, Computation time: 1.5132715702056885\n",
      "Step: 3932, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 1.1189563274383545\n",
      "Step: 3933, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.046605110168457\n",
      "Step: 3934, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 1.162278175354004\n",
      "Step: 3935, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.2716012001037598\n",
      "Step: 3936, Loss: 0.9162994027137756, Accuracy: 1.0, Computation time: 1.2212049961090088\n",
      "Step: 3937, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 1.1928434371948242\n",
      "Step: 3938, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.2132983207702637\n",
      "Step: 3939, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.1889622211456299\n",
      "Step: 3940, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.3758254051208496\n",
      "Step: 3941, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.1353766918182373\n",
      "Step: 3942, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.1458406448364258\n",
      "Step: 3943, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.23209547996521\n",
      "Step: 3944, Loss: 0.9158512949943542, Accuracy: 1.0, Computation time: 1.3035147190093994\n",
      "Step: 3945, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.240602970123291\n",
      "Step: 3946, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.3985910415649414\n",
      "Step: 3947, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 2.065431833267212\n",
      "Step: 3948, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.2140522003173828\n",
      "Step: 3949, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 1.02595853805542\n",
      "Step: 3950, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.1267948150634766\n",
      "Step: 3951, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.1592013835906982\n",
      "Step: 3952, Loss: 0.9372625350952148, Accuracy: 0.9642857313156128, Computation time: 1.0760483741760254\n",
      "Step: 3953, Loss: 0.916441798210144, Accuracy: 1.0, Computation time: 1.29949951171875\n",
      "Step: 3954, Loss: 0.91584312915802, Accuracy: 1.0, Computation time: 1.3100388050079346\n",
      "Step: 3955, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.4147722721099854\n",
      "Step: 3956, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.6219189167022705\n",
      "Step: 3957, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.1658000946044922\n",
      "Step: 3958, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.4404714107513428\n",
      "Step: 3959, Loss: 0.9375404715538025, Accuracy: 0.9807692766189575, Computation time: 1.0825176239013672\n",
      "Step: 3960, Loss: 0.9165810346603394, Accuracy: 1.0, Computation time: 1.5746495723724365\n",
      "Step: 3961, Loss: 0.9167290925979614, Accuracy: 1.0, Computation time: 1.303173303604126\n",
      "Step: 3962, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.157987356185913\n",
      "Step: 3963, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.220038652420044\n",
      "Step: 3964, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.0558087825775146\n",
      "Step: 3965, Loss: 0.9374582767486572, Accuracy: nan, Computation time: 1.624375581741333\n",
      "Step: 3966, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.0838096141815186\n",
      "Step: 3967, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.660851240158081\n",
      "Step: 3968, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.1759538650512695\n",
      "Step: 3969, Loss: 0.9160318374633789, Accuracy: 1.0, Computation time: 1.4975252151489258\n",
      "Step: 3970, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 1.2927403450012207\n",
      "Step: 3971, Loss: 0.9243730306625366, Accuracy: 1.0, Computation time: 1.2976086139678955\n",
      "Step: 3972, Loss: 0.9160671830177307, Accuracy: 1.0, Computation time: 1.1756925582885742\n",
      "Step: 3973, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.1328790187835693\n",
      "Step: 3974, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.1120901107788086\n",
      "Step: 3975, Loss: 0.9159377217292786, Accuracy: 1.0, Computation time: 1.4086439609527588\n",
      "Step: 3976, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.2043652534484863\n",
      "Step: 3977, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 1.4596059322357178\n",
      "Step: 3978, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.3564927577972412\n",
      "Step: 3979, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 1.3502049446105957\n",
      "Step: 3980, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.2358043193817139\n",
      "Step: 3981, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.240368127822876\n",
      "Step: 3982, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.2062067985534668\n",
      "Step: 3983, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 1.0803446769714355\n",
      "Step: 3984, Loss: 0.9365028142929077, Accuracy: 0.9791666865348816, Computation time: 1.7560937404632568\n",
      "Step: 3985, Loss: 0.9162562489509583, Accuracy: 1.0, Computation time: 1.2235889434814453\n",
      "Step: 3986, Loss: 0.9348880052566528, Accuracy: 0.9750000238418579, Computation time: 1.5764448642730713\n",
      "Step: 3987, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 1.4255356788635254\n",
      "Step: 3988, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 1.7825653553009033\n",
      "Step: 3989, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 1.6399517059326172\n",
      "Step: 3990, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.314107894897461\n",
      "Step: 3991, Loss: 0.9159244298934937, Accuracy: 1.0, Computation time: 1.2904772758483887\n",
      "Step: 3992, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 1.8203277587890625\n",
      "Step: 3993, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 1.1231181621551514\n",
      "Step: 3994, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.2372255325317383\n",
      "Step: 3995, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.207143783569336\n",
      "Step: 3996, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.7689297199249268\n",
      "Step: 3997, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.3940649032592773\n",
      "Step: 3998, Loss: 0.9161345362663269, Accuracy: 1.0, Computation time: 1.5668349266052246\n",
      "Step: 3999, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.500025749206543\n",
      "Step: 4000, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.7056503295898438\n",
      "Step: 4001, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.8752858638763428\n",
      "Step: 4002, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 1.293041467666626\n",
      "Step: 4003, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.1867642402648926\n",
      "Step: 4004, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.24137544631958\n",
      "Step: 4005, Loss: 0.9159380197525024, Accuracy: 1.0, Computation time: 1.8971467018127441\n",
      "Step: 4006, Loss: 0.9158362746238708, Accuracy: 1.0, Computation time: 1.4978303909301758\n",
      "Step: 4007, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.672043800354004\n",
      "Step: 4008, Loss: 0.9380226135253906, Accuracy: 0.9750000238418579, Computation time: 1.406203031539917\n",
      "Step: 4009, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.5221118927001953\n",
      "Step: 4010, Loss: 0.9169361591339111, Accuracy: 1.0, Computation time: 1.8287646770477295\n",
      "Step: 4011, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.6368002891540527\n",
      "Step: 4012, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 2.2758727073669434\n",
      "Step: 4013, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.6753473281860352\n",
      "Step: 4014, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 1.447922945022583\n",
      "Step: 4015, Loss: 0.9334855079650879, Accuracy: 0.96875, Computation time: 1.422053337097168\n",
      "Step: 4016, Loss: 0.9159389734268188, Accuracy: 1.0, Computation time: 1.4864733219146729\n",
      "Step: 4017, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.6451973915100098\n",
      "Step: 4018, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.836414098739624\n",
      "Step: 4019, Loss: 0.9596239328384399, Accuracy: 0.9434524178504944, Computation time: 1.564997911453247\n",
      "Step: 4020, Loss: 0.9164166450500488, Accuracy: 1.0, Computation time: 1.845139741897583\n",
      "Step: 4021, Loss: 0.9172986745834351, Accuracy: 1.0, Computation time: 1.5556509494781494\n",
      "Step: 4022, Loss: 0.9162218570709229, Accuracy: 1.0, Computation time: 1.5144016742706299\n",
      "Step: 4023, Loss: 0.9285946488380432, Accuracy: 0.9642857313156128, Computation time: 1.5259613990783691\n",
      "Step: 4024, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.5865025520324707\n",
      "Step: 4025, Loss: 0.915945291519165, Accuracy: 1.0, Computation time: 1.3862323760986328\n",
      "Step: 4026, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 1.0710408687591553\n",
      "Step: 4027, Loss: 0.9376727938652039, Accuracy: 0.9642857313156128, Computation time: 1.695260763168335\n",
      "Step: 4028, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.3838601112365723\n",
      "Step: 4029, Loss: 0.9159889817237854, Accuracy: 1.0, Computation time: 1.9511969089508057\n",
      "Step: 4030, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 1.4484717845916748\n",
      "########################\n",
      "Test loss: 1.069765329360962, Test Accuracy_epoch29: 0.7659148573875427\n",
      "########################\n",
      "Step: 4031, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.46443510055542\n",
      "Step: 4032, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.333894968032837\n",
      "Step: 4033, Loss: 0.9592210650444031, Accuracy: 0.9305555820465088, Computation time: 1.376443862915039\n",
      "Step: 4034, Loss: 0.9592626094818115, Accuracy: 0.9615384340286255, Computation time: 1.4370853900909424\n",
      "Step: 4035, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.5347037315368652\n",
      "Step: 4036, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.4936039447784424\n",
      "Step: 4037, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 1.5933506488800049\n",
      "Step: 4038, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.238316297531128\n",
      "Step: 4039, Loss: 0.9165636897087097, Accuracy: 1.0, Computation time: 1.658839225769043\n",
      "Step: 4040, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.6331076622009277\n",
      "Step: 4041, Loss: 0.9372493028640747, Accuracy: 0.9807692766189575, Computation time: 2.185715436935425\n",
      "Step: 4042, Loss: 0.9159421920776367, Accuracy: 1.0, Computation time: 1.5871367454528809\n",
      "Step: 4043, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.361426830291748\n",
      "Step: 4044, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.2299158573150635\n",
      "Step: 4045, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.3803119659423828\n",
      "Step: 4046, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 1.427469253540039\n",
      "Step: 4047, Loss: 0.937722384929657, Accuracy: 0.9750000238418579, Computation time: 1.7395579814910889\n",
      "Step: 4048, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 1.2941808700561523\n",
      "Step: 4049, Loss: 0.9178889393806458, Accuracy: 1.0, Computation time: 1.5006966590881348\n",
      "Step: 4050, Loss: 0.9375142455101013, Accuracy: 0.9807692766189575, Computation time: 1.6077558994293213\n",
      "Step: 4051, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 1.5341362953186035\n",
      "Step: 4052, Loss: 0.9158779978752136, Accuracy: 1.0, Computation time: 1.524061918258667\n",
      "Step: 4053, Loss: 0.9160110950469971, Accuracy: 1.0, Computation time: 1.6061649322509766\n",
      "Step: 4054, Loss: 0.9159231781959534, Accuracy: 1.0, Computation time: 1.6777327060699463\n",
      "Step: 4055, Loss: 0.9374990463256836, Accuracy: 0.9821428656578064, Computation time: 1.5760555267333984\n",
      "Step: 4056, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.101140022277832\n",
      "Step: 4057, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.6686925888061523\n",
      "Step: 4058, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 1.237062692642212\n",
      "Step: 4059, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.624629259109497\n",
      "Step: 4060, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.3482491970062256\n",
      "Step: 4061, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.3694813251495361\n",
      "Step: 4062, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.2658257484436035\n",
      "Step: 4063, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.2531394958496094\n",
      "Step: 4064, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.3845431804656982\n",
      "Step: 4065, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 1.3552708625793457\n",
      "Step: 4066, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.270888090133667\n",
      "Step: 4067, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.5886456966400146\n",
      "Step: 4068, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.3084726333618164\n",
      "Step: 4069, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.3608059883117676\n",
      "Step: 4070, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.521371603012085\n",
      "Step: 4071, Loss: 0.9374381899833679, Accuracy: 0.9750000238418579, Computation time: 1.2598042488098145\n",
      "Step: 4072, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.3343346118927002\n",
      "Step: 4073, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.1962110996246338\n",
      "Step: 4074, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.8081245422363281\n",
      "Step: 4075, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.2441332340240479\n",
      "Step: 4076, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.93292236328125\n",
      "Step: 4077, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 1.6916420459747314\n",
      "Step: 4078, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 1.3526644706726074\n",
      "Step: 4079, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.2989003658294678\n",
      "Step: 4080, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.3659369945526123\n",
      "Step: 4081, Loss: 0.9159373044967651, Accuracy: 1.0, Computation time: 1.328674554824829\n",
      "Step: 4082, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.8341729640960693\n",
      "Step: 4083, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.029747486114502\n",
      "Step: 4084, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 1.4145786762237549\n",
      "Step: 4085, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.3827006816864014\n",
      "Step: 4086, Loss: 0.9374972581863403, Accuracy: 0.9807692766189575, Computation time: 1.459092617034912\n",
      "Step: 4087, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 1.2470097541809082\n",
      "Step: 4088, Loss: 0.9374669790267944, Accuracy: 0.9791666865348816, Computation time: 1.382556676864624\n",
      "Step: 4089, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.0461859703063965\n",
      "Step: 4090, Loss: 0.9377388954162598, Accuracy: 0.9791666865348816, Computation time: 1.0524959564208984\n",
      "Step: 4091, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.3075740337371826\n",
      "Step: 4092, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 1.4309401512145996\n",
      "Step: 4093, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 1.4116699695587158\n",
      "Step: 4094, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.5813398361206055\n",
      "Step: 4095, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.1151611804962158\n",
      "Step: 4096, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.3897273540496826\n",
      "Step: 4097, Loss: 0.915946900844574, Accuracy: 1.0, Computation time: 1.363572120666504\n",
      "Step: 4098, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.3513751029968262\n",
      "Step: 4099, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.351423740386963\n",
      "Step: 4100, Loss: 0.9158936738967896, Accuracy: 1.0, Computation time: 1.1534321308135986\n",
      "Step: 4101, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.4028277397155762\n",
      "Step: 4102, Loss: 0.9375805258750916, Accuracy: 0.9833333492279053, Computation time: 1.2578208446502686\n",
      "Step: 4103, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.257317066192627\n",
      "Step: 4104, Loss: 0.9374656677246094, Accuracy: 0.9772727489471436, Computation time: 1.6382224559783936\n",
      "Step: 4105, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 1.2435719966888428\n",
      "Step: 4106, Loss: 0.937532901763916, Accuracy: 0.9750000238418579, Computation time: 1.2438430786132812\n",
      "Step: 4107, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.0204546451568604\n",
      "Step: 4108, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.3346433639526367\n",
      "Step: 4109, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.4052741527557373\n",
      "Step: 4110, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 1.2874281406402588\n",
      "Step: 4111, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.2459440231323242\n",
      "Step: 4112, Loss: 0.9322435855865479, Accuracy: 0.9722222089767456, Computation time: 2.0056357383728027\n",
      "Step: 4113, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 1.1861145496368408\n",
      "Step: 4114, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.5466535091400146\n",
      "Step: 4115, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 1.4688396453857422\n",
      "Step: 4116, Loss: 0.9376303553581238, Accuracy: 0.9791666865348816, Computation time: 1.4594058990478516\n",
      "Step: 4117, Loss: 0.9236793518066406, Accuracy: 1.0, Computation time: 1.6756701469421387\n",
      "Step: 4118, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.5027058124542236\n",
      "Step: 4119, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.3654515743255615\n",
      "Step: 4120, Loss: 0.9376211166381836, Accuracy: 0.9821428656578064, Computation time: 1.3302958011627197\n",
      "Step: 4121, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.4100968837738037\n",
      "Step: 4122, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.1213555335998535\n",
      "Step: 4123, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 1.4412057399749756\n",
      "Step: 4124, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 1.2978825569152832\n",
      "Step: 4125, Loss: 0.9160270690917969, Accuracy: 1.0, Computation time: 1.4000232219696045\n",
      "Step: 4126, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.3119323253631592\n",
      "Step: 4127, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 1.444549798965454\n",
      "Step: 4128, Loss: 0.9164941906929016, Accuracy: 1.0, Computation time: 2.064120054244995\n",
      "Step: 4129, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.476808786392212\n",
      "Step: 4130, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 1.9869470596313477\n",
      "Step: 4131, Loss: 0.9159692525863647, Accuracy: 1.0, Computation time: 1.496887445449829\n",
      "Step: 4132, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.5225934982299805\n",
      "Step: 4133, Loss: 0.9174201488494873, Accuracy: 1.0, Computation time: 1.3448007106781006\n",
      "Step: 4134, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.2277684211730957\n",
      "Step: 4135, Loss: 0.9160698056221008, Accuracy: 1.0, Computation time: 1.6424639225006104\n",
      "Step: 4136, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 2.0469810962677\n",
      "Step: 4137, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 2.0726311206817627\n",
      "Step: 4138, Loss: 0.9160337448120117, Accuracy: 1.0, Computation time: 1.611194133758545\n",
      "Step: 4139, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.2970764636993408\n",
      "Step: 4140, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.2922489643096924\n",
      "Step: 4141, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.3618485927581787\n",
      "Step: 4142, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.227914571762085\n",
      "Step: 4143, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.407386064529419\n",
      "Step: 4144, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.6974844932556152\n",
      "Step: 4145, Loss: 0.9160575866699219, Accuracy: 1.0, Computation time: 1.459407091140747\n",
      "Step: 4146, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.4883298873901367\n",
      "Step: 4147, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.4514129161834717\n",
      "Step: 4148, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.3738195896148682\n",
      "Step: 4149, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.5659446716308594\n",
      "Step: 4150, Loss: 0.9375147223472595, Accuracy: 0.9807692766189575, Computation time: 1.6752307415008545\n",
      "Step: 4151, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.5978326797485352\n",
      "Step: 4152, Loss: 0.9269380569458008, Accuracy: 1.0, Computation time: 1.8559932708740234\n",
      "Step: 4153, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.3080840110778809\n",
      "Step: 4154, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.4818041324615479\n",
      "Step: 4155, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.7413663864135742\n",
      "Step: 4156, Loss: 0.9160377979278564, Accuracy: 1.0, Computation time: 1.571739912033081\n",
      "Step: 4157, Loss: 0.9375691413879395, Accuracy: 0.9791666865348816, Computation time: 1.5574145317077637\n",
      "Step: 4158, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.5793216228485107\n",
      "Step: 4159, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.5045416355133057\n",
      "Step: 4160, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 1.635714054107666\n",
      "Step: 4161, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 2.279233694076538\n",
      "Step: 4162, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 1.4588348865509033\n",
      "Step: 4163, Loss: 0.9376030564308167, Accuracy: 0.9833333492279053, Computation time: 1.3503563404083252\n",
      "Step: 4164, Loss: 0.9161708354949951, Accuracy: 1.0, Computation time: 1.7465271949768066\n",
      "Step: 4165, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.6975183486938477\n",
      "Step: 4166, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 1.6414518356323242\n",
      "Step: 4167, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.6935741901397705\n",
      "Step: 4168, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.819793462753296\n",
      "Step: 4169, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.5535359382629395\n",
      "########################\n",
      "Test loss: 1.0690679550170898, Test Accuracy_epoch30: 0.7648797035217285\n",
      "########################\n",
      "Step: 4170, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 1.2669506072998047\n",
      "Step: 4171, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 2.151294708251953\n",
      "Step: 4172, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 1.5591585636138916\n",
      "Step: 4173, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.942948818206787\n",
      "Step: 4174, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.7057256698608398\n",
      "Step: 4175, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 1.5852844715118408\n",
      "Step: 4176, Loss: 0.915834367275238, Accuracy: 1.0, Computation time: 1.4251434803009033\n",
      "Step: 4177, Loss: 0.9158351421356201, Accuracy: 1.0, Computation time: 1.5440289974212646\n",
      "Step: 4178, Loss: 0.9158368110656738, Accuracy: 1.0, Computation time: 1.8867182731628418\n",
      "Step: 4179, Loss: 0.916215181350708, Accuracy: 1.0, Computation time: 2.167581558227539\n",
      "Step: 4180, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.5239667892456055\n",
      "Step: 4181, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 2.36415433883667\n",
      "Step: 4182, Loss: 0.9260167479515076, Accuracy: 0.9722222089767456, Computation time: 3.080260992050171\n",
      "Step: 4183, Loss: 0.9161790609359741, Accuracy: 1.0, Computation time: 1.6717345714569092\n",
      "Step: 4184, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 1.5922939777374268\n",
      "Step: 4185, Loss: 0.9291257858276367, Accuracy: 0.949999988079071, Computation time: 1.3683161735534668\n",
      "Step: 4186, Loss: 0.9376227855682373, Accuracy: 0.96875, Computation time: 1.5718414783477783\n",
      "Step: 4187, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.4103789329528809\n",
      "Step: 4188, Loss: 0.9159401059150696, Accuracy: 1.0, Computation time: 1.7468791007995605\n",
      "Step: 4189, Loss: 0.9159559607505798, Accuracy: 1.0, Computation time: 1.474069595336914\n",
      "Step: 4190, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 1.3606572151184082\n",
      "Step: 4191, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 1.4847121238708496\n",
      "Step: 4192, Loss: 0.9159347414970398, Accuracy: 1.0, Computation time: 1.2757887840270996\n",
      "Step: 4193, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.2405664920806885\n",
      "Step: 4194, Loss: 0.9163308143615723, Accuracy: 1.0, Computation time: 1.3949894905090332\n",
      "Step: 4195, Loss: 0.9158980846405029, Accuracy: 1.0, Computation time: 1.3875269889831543\n",
      "Step: 4196, Loss: 0.9159621596336365, Accuracy: 1.0, Computation time: 1.5206670761108398\n",
      "Step: 4197, Loss: 0.9376422762870789, Accuracy: 0.9642857313156128, Computation time: 1.3599212169647217\n",
      "Step: 4198, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.4543402194976807\n",
      "Step: 4199, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.2401313781738281\n",
      "Step: 4200, Loss: 0.9159885048866272, Accuracy: 1.0, Computation time: 1.3992879390716553\n",
      "Step: 4201, Loss: 0.9376147389411926, Accuracy: 0.9833333492279053, Computation time: 1.2305824756622314\n",
      "Step: 4202, Loss: 0.9376465082168579, Accuracy: 0.9821428656578064, Computation time: 1.9489355087280273\n",
      "Step: 4203, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 1.06471586227417\n",
      "Step: 4204, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.1851239204406738\n",
      "Step: 4205, Loss: 0.916663646697998, Accuracy: 1.0, Computation time: 1.4382905960083008\n",
      "Step: 4206, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 1.1358425617218018\n",
      "Step: 4207, Loss: 0.9159282445907593, Accuracy: 1.0, Computation time: 1.1164131164550781\n",
      "Step: 4208, Loss: 0.9158779978752136, Accuracy: 1.0, Computation time: 1.396428108215332\n",
      "Step: 4209, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 0.9748716354370117\n",
      "Step: 4210, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.0613782405853271\n",
      "Step: 4211, Loss: 0.9376901984214783, Accuracy: 0.9791666865348816, Computation time: 1.02308988571167\n",
      "Step: 4212, Loss: 0.9160743355751038, Accuracy: 1.0, Computation time: 1.310960054397583\n",
      "Step: 4213, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 1.0980119705200195\n",
      "Step: 4214, Loss: 0.9376578330993652, Accuracy: 0.9772727489471436, Computation time: 1.2450292110443115\n",
      "Step: 4215, Loss: 0.9166550636291504, Accuracy: 1.0, Computation time: 1.2552707195281982\n",
      "Step: 4216, Loss: 0.917575478553772, Accuracy: 1.0, Computation time: 1.3360624313354492\n",
      "Step: 4217, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 2.398108959197998\n",
      "Step: 4218, Loss: 0.91600501537323, Accuracy: 1.0, Computation time: 1.1674070358276367\n",
      "Step: 4219, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.4683904647827148\n",
      "Step: 4220, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 1.062370777130127\n",
      "Step: 4221, Loss: 0.9160308241844177, Accuracy: 1.0, Computation time: 1.2114293575286865\n",
      "Step: 4222, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 0.9925806522369385\n",
      "Step: 4223, Loss: 0.9375778436660767, Accuracy: 0.96875, Computation time: 1.4489963054656982\n",
      "Step: 4224, Loss: 0.9176825881004333, Accuracy: 1.0, Computation time: 1.2539587020874023\n",
      "Step: 4225, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.0152995586395264\n",
      "Step: 4226, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.4328203201293945\n",
      "Step: 4227, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.3474090099334717\n",
      "Step: 4228, Loss: 0.9163656234741211, Accuracy: 1.0, Computation time: 1.436156988143921\n",
      "Step: 4229, Loss: 0.9162770509719849, Accuracy: 1.0, Computation time: 1.1280765533447266\n",
      "Step: 4230, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.0205132961273193\n",
      "Step: 4231, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.091398000717163\n",
      "Step: 4232, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 1.2682359218597412\n",
      "Step: 4233, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 1.2186651229858398\n",
      "Step: 4234, Loss: 0.9227527976036072, Accuracy: 1.0, Computation time: 1.617030382156372\n",
      "Step: 4235, Loss: 0.9161779880523682, Accuracy: 1.0, Computation time: 1.443033218383789\n",
      "Step: 4236, Loss: 0.9171119332313538, Accuracy: 1.0, Computation time: 1.628528356552124\n",
      "Step: 4237, Loss: 0.9159121513366699, Accuracy: 1.0, Computation time: 1.1934614181518555\n",
      "Step: 4238, Loss: 0.915968120098114, Accuracy: 1.0, Computation time: 1.6492547988891602\n",
      "Step: 4239, Loss: 0.9164717197418213, Accuracy: 1.0, Computation time: 1.764869213104248\n",
      "Step: 4240, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 1.4380710124969482\n",
      "Step: 4241, Loss: 0.9159225225448608, Accuracy: 1.0, Computation time: 1.3379712104797363\n",
      "Step: 4242, Loss: 0.9159463047981262, Accuracy: 1.0, Computation time: 1.0305922031402588\n",
      "Step: 4243, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.3060548305511475\n",
      "Step: 4244, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 1.2790396213531494\n",
      "Step: 4245, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 1.258664608001709\n",
      "Step: 4246, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 1.2480762004852295\n",
      "Step: 4247, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.818192958831787\n",
      "Step: 4248, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.0913796424865723\n",
      "Step: 4249, Loss: 0.9376314878463745, Accuracy: 0.9772727489471436, Computation time: 1.2221627235412598\n",
      "Step: 4250, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.1735446453094482\n",
      "Step: 4251, Loss: 0.9573184847831726, Accuracy: 0.9166666865348816, Computation time: 1.7959420680999756\n",
      "Step: 4252, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 1.079111099243164\n",
      "Step: 4253, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.4363336563110352\n",
      "Step: 4254, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 1.0668199062347412\n",
      "Step: 4255, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.481548547744751\n",
      "Step: 4256, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.1205904483795166\n",
      "Step: 4257, Loss: 0.9159492254257202, Accuracy: 1.0, Computation time: 1.7464990615844727\n",
      "Step: 4258, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 1.3004488945007324\n",
      "Step: 4259, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.344834327697754\n",
      "Step: 4260, Loss: 0.9376059174537659, Accuracy: 0.9642857313156128, Computation time: 1.2038118839263916\n",
      "Step: 4261, Loss: 0.9375640153884888, Accuracy: 0.9722222089767456, Computation time: 1.2116165161132812\n",
      "Step: 4262, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.4921848773956299\n",
      "Step: 4263, Loss: 0.9159489870071411, Accuracy: 1.0, Computation time: 1.4622235298156738\n",
      "Step: 4264, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.354872226715088\n",
      "Step: 4265, Loss: 0.9160510301589966, Accuracy: 1.0, Computation time: 1.2437477111816406\n",
      "Step: 4266, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.2786922454833984\n",
      "Step: 4267, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.2912635803222656\n",
      "Step: 4268, Loss: 0.9376580119132996, Accuracy: 0.9583333730697632, Computation time: 1.2540528774261475\n",
      "Step: 4269, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.1959149837493896\n",
      "Step: 4270, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.200901985168457\n",
      "Step: 4271, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.1261999607086182\n",
      "Step: 4272, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 1.105884075164795\n",
      "Step: 4273, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.337728500366211\n",
      "Step: 4274, Loss: 0.9170878529548645, Accuracy: 1.0, Computation time: 1.699979543685913\n",
      "Step: 4275, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.281736135482788\n",
      "Step: 4276, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.7233695983886719\n",
      "Step: 4277, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.3714032173156738\n",
      "Step: 4278, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.147627592086792\n",
      "Step: 4279, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.1662862300872803\n",
      "Step: 4280, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 1.4902033805847168\n",
      "Step: 4281, Loss: 0.9375290870666504, Accuracy: 0.9722222089767456, Computation time: 1.2474124431610107\n",
      "Step: 4282, Loss: 0.915846049785614, Accuracy: 1.0, Computation time: 1.3371131420135498\n",
      "Step: 4283, Loss: 0.9191503524780273, Accuracy: 1.0, Computation time: 1.6796462535858154\n",
      "Step: 4284, Loss: 0.9200680255889893, Accuracy: 1.0, Computation time: 1.4149482250213623\n",
      "Step: 4285, Loss: 0.9353408217430115, Accuracy: 0.9375, Computation time: 1.8592586517333984\n",
      "Step: 4286, Loss: 0.9331159591674805, Accuracy: 0.9772727489471436, Computation time: 2.151365280151367\n",
      "Step: 4287, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.188805103302002\n",
      "Step: 4288, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.1634588241577148\n",
      "Step: 4289, Loss: 0.9166135191917419, Accuracy: 1.0, Computation time: 1.8318939208984375\n",
      "Step: 4290, Loss: 0.9159651398658752, Accuracy: 1.0, Computation time: 1.1855700016021729\n",
      "Step: 4291, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.3942484855651855\n",
      "Step: 4292, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 1.3098974227905273\n",
      "Step: 4293, Loss: 0.9159113764762878, Accuracy: 1.0, Computation time: 1.3823401927947998\n",
      "Step: 4294, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.3413975238800049\n",
      "Step: 4295, Loss: 0.9375717639923096, Accuracy: 0.9722222089767456, Computation time: 1.1754608154296875\n",
      "Step: 4296, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 1.2149872779846191\n",
      "Step: 4297, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.310584545135498\n",
      "Step: 4298, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.1864595413208008\n",
      "Step: 4299, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.4644324779510498\n",
      "Step: 4300, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.329286813735962\n",
      "Step: 4301, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.4248435497283936\n",
      "Step: 4302, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.1990649700164795\n",
      "Step: 4303, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.20930814743042\n",
      "Step: 4304, Loss: 0.9166371822357178, Accuracy: 1.0, Computation time: 1.357487440109253\n",
      "Step: 4305, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 1.3254811763763428\n",
      "Step: 4306, Loss: 0.9159819483757019, Accuracy: 1.0, Computation time: 1.2719569206237793\n",
      "Step: 4307, Loss: 0.915961742401123, Accuracy: 1.0, Computation time: 1.4228520393371582\n",
      "Step: 4308, Loss: 0.9238029718399048, Accuracy: 1.0, Computation time: 1.7434489727020264\n",
      "########################\n",
      "Test loss: 1.071697473526001, Test Accuracy_epoch31: 0.7643014192581177\n",
      "########################\n",
      "Step: 4309, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.268561601638794\n",
      "Step: 4310, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.0819604396820068\n",
      "Step: 4311, Loss: 0.916058361530304, Accuracy: 1.0, Computation time: 1.4917995929718018\n",
      "Step: 4312, Loss: 0.916074812412262, Accuracy: 1.0, Computation time: 1.1886928081512451\n",
      "Step: 4313, Loss: 0.9159308671951294, Accuracy: 1.0, Computation time: 1.6174201965332031\n",
      "Step: 4314, Loss: 0.9177923202514648, Accuracy: 1.0, Computation time: 1.0327143669128418\n",
      "Step: 4315, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.243159532546997\n",
      "Step: 4316, Loss: 0.9159136414527893, Accuracy: 1.0, Computation time: 1.3591020107269287\n",
      "Step: 4317, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.1389145851135254\n",
      "Step: 4318, Loss: 0.9159546494483948, Accuracy: 1.0, Computation time: 1.167170763015747\n",
      "Step: 4319, Loss: 0.9161040782928467, Accuracy: 1.0, Computation time: 1.9948465824127197\n",
      "Step: 4320, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.4590654373168945\n",
      "Step: 4321, Loss: 0.9378561973571777, Accuracy: 0.9722222089767456, Computation time: 1.4981002807617188\n",
      "Step: 4322, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.6773085594177246\n",
      "Step: 4323, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.1998867988586426\n",
      "Step: 4324, Loss: 0.9376124739646912, Accuracy: 0.9750000238418579, Computation time: 1.2700002193450928\n",
      "Step: 4325, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.0549051761627197\n",
      "Step: 4326, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.0007834434509277\n",
      "Step: 4327, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.0786335468292236\n",
      "Step: 4328, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 1.014564037322998\n",
      "Step: 4329, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.0932440757751465\n",
      "Step: 4330, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 1.1906392574310303\n",
      "Step: 4331, Loss: 0.9159757494926453, Accuracy: 1.0, Computation time: 1.2373690605163574\n",
      "Step: 4332, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.4581575393676758\n",
      "Step: 4333, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.1555454730987549\n",
      "Step: 4334, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.3418357372283936\n",
      "Step: 4335, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.421858310699463\n",
      "Step: 4336, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.4224441051483154\n",
      "Step: 4337, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.4555175304412842\n",
      "Step: 4338, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.2112746238708496\n",
      "Step: 4339, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.4882259368896484\n",
      "Step: 4340, Loss: 0.9375344514846802, Accuracy: 0.9583333730697632, Computation time: 1.254763126373291\n",
      "Step: 4341, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 1.461352825164795\n",
      "Step: 4342, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.5760817527770996\n",
      "Step: 4343, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.3318390846252441\n",
      "Step: 4344, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.3279139995574951\n",
      "Step: 4345, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.412214756011963\n",
      "Step: 4346, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.1843822002410889\n",
      "Step: 4347, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 1.5898752212524414\n",
      "Step: 4348, Loss: 0.9158422350883484, Accuracy: nan, Computation time: 1.2942311763763428\n",
      "Step: 4349, Loss: 0.9160604476928711, Accuracy: 1.0, Computation time: 1.539283037185669\n",
      "Step: 4350, Loss: 0.9158368110656738, Accuracy: 1.0, Computation time: 1.3411214351654053\n",
      "Step: 4351, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.1910779476165771\n",
      "Step: 4352, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.0635292530059814\n",
      "Step: 4353, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.7953112125396729\n",
      "Step: 4354, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.0322391986846924\n",
      "Step: 4355, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.3434350490570068\n",
      "Step: 4356, Loss: 0.9158420562744141, Accuracy: 1.0, Computation time: 1.3572967052459717\n",
      "Step: 4357, Loss: 0.916065514087677, Accuracy: 1.0, Computation time: 1.3354952335357666\n",
      "Step: 4358, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.559124231338501\n",
      "Step: 4359, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 1.8022236824035645\n",
      "Step: 4360, Loss: 0.916058361530304, Accuracy: 1.0, Computation time: 1.2419764995574951\n",
      "Step: 4361, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.0359573364257812\n",
      "Step: 4362, Loss: 0.9592155814170837, Accuracy: 0.9444444179534912, Computation time: 1.458050012588501\n",
      "Step: 4363, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 1.207106351852417\n",
      "Step: 4364, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.7059593200683594\n",
      "Step: 4365, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.364133596420288\n",
      "Step: 4366, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.771012306213379\n",
      "Step: 4367, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.2393577098846436\n",
      "Step: 4368, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 2.0336389541625977\n",
      "Step: 4369, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.1966164112091064\n",
      "Step: 4370, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 1.3210456371307373\n",
      "Step: 4371, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 1.305476188659668\n",
      "Step: 4372, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.8816356658935547\n",
      "Step: 4373, Loss: 0.9158401489257812, Accuracy: 1.0, Computation time: 1.221693992614746\n",
      "Step: 4374, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.2696778774261475\n",
      "Step: 4375, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.0885629653930664\n",
      "Step: 4376, Loss: 0.9158385396003723, Accuracy: 1.0, Computation time: 1.3750982284545898\n",
      "Step: 4377, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.4177196025848389\n",
      "Step: 4378, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.184208869934082\n",
      "Step: 4379, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 0.9540810585021973\n",
      "Step: 4380, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.3198497295379639\n",
      "Step: 4381, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.1900238990783691\n",
      "Step: 4382, Loss: 0.9164209961891174, Accuracy: 1.0, Computation time: 2.180941343307495\n",
      "Step: 4383, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.4117095470428467\n",
      "Step: 4384, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.0776150226593018\n",
      "Step: 4385, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.55594801902771\n",
      "Step: 4386, Loss: 0.9360982179641724, Accuracy: 0.9750000238418579, Computation time: 1.6796503067016602\n",
      "Step: 4387, Loss: 0.9158371090888977, Accuracy: 1.0, Computation time: 1.2205772399902344\n",
      "Step: 4388, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 1.3902459144592285\n",
      "Step: 4389, Loss: 0.9161096215248108, Accuracy: 1.0, Computation time: 1.2139780521392822\n",
      "Step: 4390, Loss: 0.9376969337463379, Accuracy: 0.9772727489471436, Computation time: 1.3127341270446777\n",
      "Step: 4391, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 1.4631612300872803\n",
      "Step: 4392, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.6828808784484863\n",
      "Step: 4393, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.356015920639038\n",
      "Step: 4394, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.4470784664154053\n",
      "Step: 4395, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.2047069072723389\n",
      "Step: 4396, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.4149518013000488\n",
      "Step: 4397, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.2891335487365723\n",
      "Step: 4398, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.1876728534698486\n",
      "Step: 4399, Loss: 0.9592626690864563, Accuracy: 0.9409722089767456, Computation time: 1.2909660339355469\n",
      "Step: 4400, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.1863720417022705\n",
      "Step: 4401, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 1.2383344173431396\n",
      "Step: 4402, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.692322015762329\n",
      "Step: 4403, Loss: 0.9375836253166199, Accuracy: 0.9750000238418579, Computation time: 1.5931262969970703\n",
      "Step: 4404, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.1320888996124268\n",
      "Step: 4405, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.356835126876831\n",
      "Step: 4406, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.3486037254333496\n",
      "Step: 4407, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.3988208770751953\n",
      "Step: 4408, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.342348575592041\n",
      "Step: 4409, Loss: 0.9374966025352478, Accuracy: 0.9807692766189575, Computation time: 1.61208176612854\n",
      "Step: 4410, Loss: 0.9377163648605347, Accuracy: 0.9722222089767456, Computation time: 1.3012549877166748\n",
      "Step: 4411, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 1.2183475494384766\n",
      "Step: 4412, Loss: 0.9375853538513184, Accuracy: 0.9583333730697632, Computation time: 1.3715407848358154\n",
      "Step: 4413, Loss: 0.919634997844696, Accuracy: 1.0, Computation time: 1.4003777503967285\n",
      "Step: 4414, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.2360341548919678\n",
      "Step: 4415, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 1.4179916381835938\n",
      "Step: 4416, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 1.2184529304504395\n",
      "Step: 4417, Loss: 0.9163486957550049, Accuracy: 1.0, Computation time: 1.246079921722412\n",
      "Step: 4418, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.7246017456054688\n",
      "Step: 4419, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.3629462718963623\n",
      "Step: 4420, Loss: 0.9375009536743164, Accuracy: 0.9772727489471436, Computation time: 1.4243724346160889\n",
      "Step: 4421, Loss: 0.9168534874916077, Accuracy: 1.0, Computation time: 1.5637845993041992\n",
      "Step: 4422, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.5112268924713135\n",
      "Step: 4423, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.4839324951171875\n",
      "Step: 4424, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.3254146575927734\n",
      "Step: 4425, Loss: 0.9159669876098633, Accuracy: 1.0, Computation time: 1.5976157188415527\n",
      "Step: 4426, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.828298568725586\n",
      "Step: 4427, Loss: 0.9162207245826721, Accuracy: 1.0, Computation time: 1.3610985279083252\n",
      "Step: 4428, Loss: 0.9160630106925964, Accuracy: 1.0, Computation time: 1.6930222511291504\n",
      "Step: 4429, Loss: 0.9159114360809326, Accuracy: 1.0, Computation time: 1.5238077640533447\n",
      "Step: 4430, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 1.8115489482879639\n",
      "Step: 4431, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.5124034881591797\n",
      "Step: 4432, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.5296733379364014\n",
      "Step: 4433, Loss: 0.9178968071937561, Accuracy: 1.0, Computation time: 1.41044020652771\n",
      "Step: 4434, Loss: 0.9373339414596558, Accuracy: 0.9772727489471436, Computation time: 1.9162659645080566\n",
      "Step: 4435, Loss: 0.9160443544387817, Accuracy: 1.0, Computation time: 1.6245572566986084\n",
      "Step: 4436, Loss: 0.9159653782844543, Accuracy: 1.0, Computation time: 1.6554186344146729\n",
      "Step: 4437, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 1.4896867275238037\n",
      "Step: 4438, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 1.5436248779296875\n",
      "Step: 4439, Loss: 0.9160255789756775, Accuracy: 1.0, Computation time: 1.5959465503692627\n",
      "Step: 4440, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.3095991611480713\n",
      "Step: 4441, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.2976830005645752\n",
      "Step: 4442, Loss: 0.9376477599143982, Accuracy: 0.9852941036224365, Computation time: 1.4284510612487793\n",
      "Step: 4443, Loss: 0.9160416126251221, Accuracy: 1.0, Computation time: 1.4647283554077148\n",
      "Step: 4444, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 1.3942551612854004\n",
      "Step: 4445, Loss: 0.9172424077987671, Accuracy: 1.0, Computation time: 1.7017688751220703\n",
      "Step: 4446, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 1.3022606372833252\n",
      "########################\n",
      "Test loss: 1.0701936483383179, Test Accuracy_epoch32: 0.7658470869064331\n",
      "########################\n",
      "Step: 4447, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.262721061706543\n",
      "Step: 4448, Loss: 0.9180809259414673, Accuracy: 1.0, Computation time: 1.3701777458190918\n",
      "Step: 4449, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.400583028793335\n",
      "Step: 4450, Loss: 0.9159018993377686, Accuracy: 1.0, Computation time: 1.52479887008667\n",
      "Step: 4451, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.747572898864746\n",
      "Step: 4452, Loss: 0.9159541726112366, Accuracy: 1.0, Computation time: 1.4571723937988281\n",
      "Step: 4453, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 1.2130868434906006\n",
      "Step: 4454, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 2.156243324279785\n",
      "Step: 4455, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.1857819557189941\n",
      "Step: 4456, Loss: 0.9377033114433289, Accuracy: 0.9642857313156128, Computation time: 1.08839750289917\n",
      "Step: 4457, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.4790730476379395\n",
      "Step: 4458, Loss: 0.9206991791725159, Accuracy: 1.0, Computation time: 2.122119188308716\n",
      "Step: 4459, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.4798328876495361\n",
      "Step: 4460, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.3619773387908936\n",
      "Step: 4461, Loss: 0.9159400463104248, Accuracy: 1.0, Computation time: 1.5852010250091553\n",
      "Step: 4462, Loss: 0.9181153178215027, Accuracy: 1.0, Computation time: 1.305013656616211\n",
      "Step: 4463, Loss: 0.91591477394104, Accuracy: 1.0, Computation time: 1.0385680198669434\n",
      "Step: 4464, Loss: 0.9159571528434753, Accuracy: 1.0, Computation time: 1.5366201400756836\n",
      "Step: 4465, Loss: 0.916004478931427, Accuracy: 1.0, Computation time: 1.3869695663452148\n",
      "Step: 4466, Loss: 0.9159922003746033, Accuracy: 1.0, Computation time: 1.0329406261444092\n",
      "Step: 4467, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.5199494361877441\n",
      "Step: 4468, Loss: 0.9161041378974915, Accuracy: 1.0, Computation time: 1.5563361644744873\n",
      "Step: 4469, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.2228655815124512\n",
      "Step: 4470, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.0456433296203613\n",
      "Step: 4471, Loss: 0.9159250855445862, Accuracy: 1.0, Computation time: 1.2270827293395996\n",
      "Step: 4472, Loss: 0.9159634709358215, Accuracy: 1.0, Computation time: 1.0192663669586182\n",
      "Step: 4473, Loss: 0.9159713983535767, Accuracy: 1.0, Computation time: 1.276298999786377\n",
      "Step: 4474, Loss: 0.9159197211265564, Accuracy: 1.0, Computation time: 1.0884485244750977\n",
      "Step: 4475, Loss: 0.9161088466644287, Accuracy: 1.0, Computation time: 1.4438889026641846\n",
      "Step: 4476, Loss: 0.9159792065620422, Accuracy: 1.0, Computation time: 1.3363127708435059\n",
      "Step: 4477, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.2783563137054443\n",
      "Step: 4478, Loss: 0.9159592390060425, Accuracy: 1.0, Computation time: 1.113759994506836\n",
      "Step: 4479, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 1.9016797542572021\n",
      "Step: 4480, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 1.2283837795257568\n",
      "Step: 4481, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.2210865020751953\n",
      "Step: 4482, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 1.2329835891723633\n",
      "Step: 4483, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 1.4335641860961914\n",
      "Step: 4484, Loss: 0.9159289002418518, Accuracy: 1.0, Computation time: 1.5075035095214844\n",
      "Step: 4485, Loss: 0.9159670472145081, Accuracy: 1.0, Computation time: 1.0892367362976074\n",
      "Step: 4486, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.1434640884399414\n",
      "Step: 4487, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.1134552955627441\n",
      "Step: 4488, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.3233747482299805\n",
      "Step: 4489, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.3144443035125732\n",
      "Step: 4490, Loss: 0.9159300327301025, Accuracy: 1.0, Computation time: 1.3160414695739746\n",
      "Step: 4491, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 1.242074728012085\n",
      "Step: 4492, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.1713645458221436\n",
      "Step: 4493, Loss: 0.9377871751785278, Accuracy: 0.9791666865348816, Computation time: 1.3374059200286865\n",
      "Step: 4494, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.3081181049346924\n",
      "Step: 4495, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 1.2617721557617188\n",
      "Step: 4496, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.485511302947998\n",
      "Step: 4497, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.3465211391448975\n",
      "Step: 4498, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.6118450164794922\n",
      "Step: 4499, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 1.2328805923461914\n",
      "Step: 4500, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.1969430446624756\n",
      "Step: 4501, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.3584578037261963\n",
      "Step: 4502, Loss: 0.937590479850769, Accuracy: 0.9772727489471436, Computation time: 1.3889286518096924\n",
      "Step: 4503, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.614062786102295\n",
      "Step: 4504, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.4085273742675781\n",
      "Step: 4505, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.4712119102478027\n",
      "Step: 4506, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.1637027263641357\n",
      "Step: 4507, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.4925320148468018\n",
      "Step: 4508, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.2789359092712402\n",
      "Step: 4509, Loss: 0.9490547180175781, Accuracy: 0.9583333730697632, Computation time: 1.5524804592132568\n",
      "Step: 4510, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 1.455211877822876\n",
      "Step: 4511, Loss: 0.9196399450302124, Accuracy: 1.0, Computation time: 1.5429518222808838\n",
      "Step: 4512, Loss: 0.9160043001174927, Accuracy: 1.0, Computation time: 1.3066039085388184\n",
      "Step: 4513, Loss: 0.9160767197608948, Accuracy: 1.0, Computation time: 1.1604852676391602\n",
      "Step: 4514, Loss: 0.9162651300430298, Accuracy: 1.0, Computation time: 1.4677233695983887\n",
      "Step: 4515, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.4031527042388916\n",
      "Step: 4516, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.0541486740112305\n",
      "Step: 4517, Loss: 0.9376001954078674, Accuracy: 0.9772727489471436, Computation time: 1.8164393901824951\n",
      "Step: 4518, Loss: 0.9159582853317261, Accuracy: 1.0, Computation time: 1.192622423171997\n",
      "Step: 4519, Loss: 0.9160460829734802, Accuracy: 1.0, Computation time: 1.549591064453125\n",
      "Step: 4520, Loss: 0.9160457849502563, Accuracy: 1.0, Computation time: 1.7689268589019775\n",
      "Step: 4521, Loss: 0.9377919435501099, Accuracy: 0.96875, Computation time: 1.7079858779907227\n",
      "Step: 4522, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.3235702514648438\n",
      "Step: 4523, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.1956958770751953\n",
      "Step: 4524, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.2054967880249023\n",
      "Step: 4525, Loss: 0.9375193119049072, Accuracy: 0.9642857313156128, Computation time: 1.3564224243164062\n",
      "Step: 4526, Loss: 0.9161520600318909, Accuracy: 1.0, Computation time: 1.4937903881072998\n",
      "Step: 4527, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 1.7181768417358398\n",
      "Step: 4528, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.2788927555084229\n",
      "Step: 4529, Loss: 0.9375512599945068, Accuracy: 0.9750000238418579, Computation time: 1.2816071510314941\n",
      "Step: 4530, Loss: 0.9161905646324158, Accuracy: 1.0, Computation time: 2.1402080059051514\n",
      "Step: 4531, Loss: 0.9373897314071655, Accuracy: 0.9722222089767456, Computation time: 1.3993163108825684\n",
      "Step: 4532, Loss: 0.943854570388794, Accuracy: 0.9750000238418579, Computation time: 1.729283094406128\n",
      "Step: 4533, Loss: 0.9159980416297913, Accuracy: 1.0, Computation time: 1.0423674583435059\n",
      "Step: 4534, Loss: 0.9309794306755066, Accuracy: 0.9807692766189575, Computation time: 1.5805633068084717\n",
      "Step: 4535, Loss: 0.9159921407699585, Accuracy: 1.0, Computation time: 2.1651413440704346\n",
      "Step: 4536, Loss: 0.9160093069076538, Accuracy: 1.0, Computation time: 1.344268560409546\n",
      "Step: 4537, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 1.4082460403442383\n",
      "Step: 4538, Loss: 0.9159850478172302, Accuracy: 1.0, Computation time: 1.160222053527832\n",
      "Step: 4539, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.0297846794128418\n",
      "Step: 4540, Loss: 0.9159737825393677, Accuracy: 1.0, Computation time: 1.1711020469665527\n",
      "Step: 4541, Loss: 0.9159767031669617, Accuracy: 1.0, Computation time: 1.2520756721496582\n",
      "Step: 4542, Loss: 0.9377609491348267, Accuracy: 0.96875, Computation time: 1.1493439674377441\n",
      "Step: 4543, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.2166929244995117\n",
      "Step: 4544, Loss: 0.9167087078094482, Accuracy: 1.0, Computation time: 1.462327241897583\n",
      "Step: 4545, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.2343013286590576\n",
      "Step: 4546, Loss: 0.9165229201316833, Accuracy: 1.0, Computation time: 1.7502269744873047\n",
      "Step: 4547, Loss: 0.9160269498825073, Accuracy: 1.0, Computation time: 0.9778232574462891\n",
      "Step: 4548, Loss: 0.9184970259666443, Accuracy: 1.0, Computation time: 1.6584422588348389\n",
      "Step: 4549, Loss: 0.9377989768981934, Accuracy: 0.9772727489471436, Computation time: 1.3035860061645508\n",
      "Step: 4550, Loss: 0.915981113910675, Accuracy: 1.0, Computation time: 1.377474308013916\n",
      "Step: 4551, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.0689663887023926\n",
      "Step: 4552, Loss: 0.9376280307769775, Accuracy: 0.9583333730697632, Computation time: 1.2593662738800049\n",
      "Step: 4553, Loss: 0.9159626960754395, Accuracy: 1.0, Computation time: 1.6505770683288574\n",
      "Step: 4554, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.1702136993408203\n",
      "Step: 4555, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.6534063816070557\n",
      "Step: 4556, Loss: 0.9159658551216125, Accuracy: 1.0, Computation time: 1.149528980255127\n",
      "Step: 4557, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.1904630661010742\n",
      "Step: 4558, Loss: 0.915996253490448, Accuracy: 1.0, Computation time: 1.3874058723449707\n",
      "Step: 4559, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.3278870582580566\n",
      "Step: 4560, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.1138808727264404\n",
      "Step: 4561, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.0984926223754883\n",
      "Step: 4562, Loss: 0.916017472743988, Accuracy: 1.0, Computation time: 1.1645596027374268\n",
      "Step: 4563, Loss: 0.9160057902336121, Accuracy: 1.0, Computation time: 1.0656943321228027\n",
      "Step: 4564, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.1305885314941406\n",
      "Step: 4565, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 1.3857300281524658\n",
      "Step: 4566, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.1414544582366943\n",
      "Step: 4567, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.308248519897461\n",
      "Step: 4568, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 1.2700049877166748\n",
      "Step: 4569, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.2048001289367676\n",
      "Step: 4570, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.4224402904510498\n",
      "Step: 4571, Loss: 0.9161244034767151, Accuracy: 1.0, Computation time: 1.5618095397949219\n",
      "Step: 4572, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.147946834564209\n",
      "Step: 4573, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 1.3807027339935303\n",
      "Step: 4574, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.09208083152771\n",
      "Step: 4575, Loss: 0.9377049207687378, Accuracy: 0.96875, Computation time: 1.328829288482666\n",
      "Step: 4576, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.3027827739715576\n",
      "Step: 4577, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 1.1782171726226807\n",
      "Step: 4578, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.0282487869262695\n",
      "Step: 4579, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.235152006149292\n",
      "Step: 4580, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 1.2225525379180908\n",
      "Step: 4581, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.2383818626403809\n",
      "Step: 4582, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.801832675933838\n",
      "Step: 4583, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.3336615562438965\n",
      "Step: 4584, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 1.242452621459961\n",
      "Step: 4585, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.4017016887664795\n",
      "########################\n",
      "Test loss: 1.0712252855300903, Test Accuracy_epoch33: 0.764264702796936\n",
      "########################\n",
      "Step: 4586, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.723017692565918\n",
      "Step: 4587, Loss: 0.9161970019340515, Accuracy: 1.0, Computation time: 1.7713534832000732\n",
      "Step: 4588, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.977386236190796\n",
      "Step: 4589, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.2553367614746094\n",
      "Step: 4590, Loss: 0.9173526763916016, Accuracy: 1.0, Computation time: 1.2988791465759277\n",
      "Step: 4591, Loss: 0.9159579277038574, Accuracy: 1.0, Computation time: 1.019416093826294\n",
      "Step: 4592, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.0947003364562988\n",
      "Step: 4593, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 1.2657899856567383\n",
      "Step: 4594, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 1.0478241443634033\n",
      "Step: 4595, Loss: 0.9160171151161194, Accuracy: 1.0, Computation time: 1.4821062088012695\n",
      "Step: 4596, Loss: 0.9166010618209839, Accuracy: 1.0, Computation time: 1.4207990169525146\n",
      "Step: 4597, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.149085521697998\n",
      "Step: 4598, Loss: 0.9160187244415283, Accuracy: 1.0, Computation time: 1.4555292129516602\n",
      "Step: 4599, Loss: 0.9159916043281555, Accuracy: 1.0, Computation time: 1.188746690750122\n",
      "Step: 4600, Loss: 0.9160379767417908, Accuracy: 1.0, Computation time: 1.3526251316070557\n",
      "Step: 4601, Loss: 0.9160208702087402, Accuracy: 1.0, Computation time: 1.3108148574829102\n",
      "Step: 4602, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.1740596294403076\n",
      "Step: 4603, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.6872854232788086\n",
      "Step: 4604, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.267627477645874\n",
      "Step: 4605, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.227041482925415\n",
      "Step: 4606, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 1.2573964595794678\n",
      "Step: 4607, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.1211581230163574\n",
      "Step: 4608, Loss: 0.9376094341278076, Accuracy: 0.9722222089767456, Computation time: 1.1601066589355469\n",
      "Step: 4609, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.3285024166107178\n",
      "Step: 4610, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.1969752311706543\n",
      "Step: 4611, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.5153536796569824\n",
      "Step: 4612, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.2033960819244385\n",
      "Step: 4613, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.3920462131500244\n",
      "Step: 4614, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.4061918258666992\n",
      "Step: 4615, Loss: 0.9159191250801086, Accuracy: 1.0, Computation time: 1.6587331295013428\n",
      "Step: 4616, Loss: 0.9507168531417847, Accuracy: 0.949999988079071, Computation time: 1.5142834186553955\n",
      "Step: 4617, Loss: 0.9165199398994446, Accuracy: 1.0, Computation time: 1.0996904373168945\n",
      "Step: 4618, Loss: 0.9161427617073059, Accuracy: 1.0, Computation time: 1.579965591430664\n",
      "Step: 4619, Loss: 0.9371235370635986, Accuracy: 0.9722222089767456, Computation time: 2.064406394958496\n",
      "Step: 4620, Loss: 0.9160389304161072, Accuracy: 1.0, Computation time: 1.4130980968475342\n",
      "Step: 4621, Loss: 0.9374380707740784, Accuracy: 0.96875, Computation time: 1.2784698009490967\n",
      "Step: 4622, Loss: 0.9160555005073547, Accuracy: 1.0, Computation time: 1.328660011291504\n",
      "Step: 4623, Loss: 0.9160233736038208, Accuracy: 1.0, Computation time: 2.031062602996826\n",
      "Step: 4624, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.1825108528137207\n",
      "Step: 4625, Loss: 0.9332648515701294, Accuracy: 0.9791666865348816, Computation time: 1.1633672714233398\n",
      "Step: 4626, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 1.3676860332489014\n",
      "Step: 4627, Loss: 0.9162078499794006, Accuracy: 1.0, Computation time: 1.638803482055664\n",
      "Step: 4628, Loss: 0.916170060634613, Accuracy: 1.0, Computation time: 1.3329973220825195\n",
      "Step: 4629, Loss: 0.9159866571426392, Accuracy: 1.0, Computation time: 1.1672477722167969\n",
      "Step: 4630, Loss: 0.9377955794334412, Accuracy: 0.9807692766189575, Computation time: 1.525979995727539\n",
      "Step: 4631, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.2989437580108643\n",
      "Step: 4632, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.548872709274292\n",
      "Step: 4633, Loss: 0.9590819478034973, Accuracy: 0.9583333730697632, Computation time: 1.5273113250732422\n",
      "Step: 4634, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 1.101461410522461\n",
      "Step: 4635, Loss: 0.9159775972366333, Accuracy: 1.0, Computation time: 1.1863946914672852\n",
      "Step: 4636, Loss: 0.9161083102226257, Accuracy: 1.0, Computation time: 1.1224963665008545\n",
      "Step: 4637, Loss: 0.9160400032997131, Accuracy: 1.0, Computation time: 1.1344823837280273\n",
      "Step: 4638, Loss: 0.9159508943557739, Accuracy: 1.0, Computation time: 1.5595471858978271\n",
      "Step: 4639, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.1023790836334229\n",
      "Step: 4640, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.1610527038574219\n",
      "Step: 4641, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.1914317607879639\n",
      "Step: 4642, Loss: 0.9159297347068787, Accuracy: 1.0, Computation time: 1.367584466934204\n",
      "Step: 4643, Loss: 0.9165219068527222, Accuracy: 1.0, Computation time: 1.3609700202941895\n",
      "Step: 4644, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.1087312698364258\n",
      "Step: 4645, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.0521109104156494\n",
      "Step: 4646, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.279463529586792\n",
      "Step: 4647, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.4685790538787842\n",
      "Step: 4648, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.5080034732818604\n",
      "Step: 4649, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.3674123287200928\n",
      "Step: 4650, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.2987332344055176\n",
      "Step: 4651, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 1.1876366138458252\n",
      "Step: 4652, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.2392585277557373\n",
      "Step: 4653, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.320183277130127\n",
      "Step: 4654, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.2596673965454102\n",
      "Step: 4655, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.6296935081481934\n",
      "Step: 4656, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.3682632446289062\n",
      "Step: 4657, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.3495032787322998\n",
      "Step: 4658, Loss: 0.9374849200248718, Accuracy: 0.9583333730697632, Computation time: 1.3512060642242432\n",
      "Step: 4659, Loss: 0.9175976514816284, Accuracy: 1.0, Computation time: 1.314422369003296\n",
      "Step: 4660, Loss: 0.9159488081932068, Accuracy: 1.0, Computation time: 1.256800651550293\n",
      "Step: 4661, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.3326454162597656\n",
      "Step: 4662, Loss: 0.917315661907196, Accuracy: 1.0, Computation time: 1.2315139770507812\n",
      "Step: 4663, Loss: 0.9162708520889282, Accuracy: 1.0, Computation time: 1.656193494796753\n",
      "Step: 4664, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.1483550071716309\n",
      "Step: 4665, Loss: 0.9159389734268188, Accuracy: 1.0, Computation time: 1.41463303565979\n",
      "Step: 4666, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.6886043548583984\n",
      "Step: 4667, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.3177917003631592\n",
      "Step: 4668, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.232489824295044\n",
      "Step: 4669, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.3704478740692139\n",
      "Step: 4670, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.0781173706054688\n",
      "Step: 4671, Loss: 0.9160219430923462, Accuracy: 1.0, Computation time: 1.4938609600067139\n",
      "Step: 4672, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 1.428199052810669\n",
      "Step: 4673, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.2987267971038818\n",
      "Step: 4674, Loss: 0.9158541560173035, Accuracy: 1.0, Computation time: 1.210097074508667\n",
      "Step: 4675, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 1.40203857421875\n",
      "Step: 4676, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.4407639503479004\n",
      "Step: 4677, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.6163690090179443\n",
      "Step: 4678, Loss: 0.9163082242012024, Accuracy: 1.0, Computation time: 1.6831135749816895\n",
      "Step: 4679, Loss: 0.9160333275794983, Accuracy: 1.0, Computation time: 1.054039478302002\n",
      "Step: 4680, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.4062402248382568\n",
      "Step: 4681, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.4921929836273193\n",
      "Step: 4682, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.541008710861206\n",
      "Step: 4683, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.4110522270202637\n",
      "Step: 4684, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.4576780796051025\n",
      "Step: 4685, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.1525990962982178\n",
      "Step: 4686, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.1719260215759277\n",
      "Step: 4687, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.2388591766357422\n",
      "Step: 4688, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.3688304424285889\n",
      "Step: 4689, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 1.1832592487335205\n",
      "Step: 4690, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.560364007949829\n",
      "Step: 4691, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 1.2188663482666016\n",
      "Step: 4692, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 1.1984999179840088\n",
      "Step: 4693, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.4846603870391846\n",
      "Step: 4694, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.3542640209197998\n",
      "Step: 4695, Loss: 0.9158347249031067, Accuracy: 1.0, Computation time: 1.2509515285491943\n",
      "Step: 4696, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.4386637210845947\n",
      "Step: 4697, Loss: 0.9396070837974548, Accuracy: 0.949999988079071, Computation time: 1.5050828456878662\n",
      "Step: 4698, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.6202647686004639\n",
      "Step: 4699, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.5095748901367188\n",
      "Step: 4700, Loss: 0.9158551096916199, Accuracy: 1.0, Computation time: 1.7552428245544434\n",
      "Step: 4701, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.318744421005249\n",
      "Step: 4702, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.790281057357788\n",
      "Step: 4703, Loss: 0.9165486097335815, Accuracy: 1.0, Computation time: 1.9526939392089844\n",
      "Step: 4704, Loss: 0.9592276215553284, Accuracy: 0.9444444179534912, Computation time: 1.352651596069336\n",
      "Step: 4705, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.0833890438079834\n",
      "Step: 4706, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 1.3932816982269287\n",
      "Step: 4707, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.9533851146697998\n",
      "Step: 4708, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.1572632789611816\n",
      "Step: 4709, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.3933978080749512\n",
      "Step: 4710, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 1.4077293872833252\n",
      "Step: 4711, Loss: 0.9375831484794617, Accuracy: 0.9642857313156128, Computation time: 1.2723619937896729\n",
      "Step: 4712, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.3901925086975098\n",
      "Step: 4713, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.1182513236999512\n",
      "Step: 4714, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.1856839656829834\n",
      "Step: 4715, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.348172903060913\n",
      "Step: 4716, Loss: 0.9158415794372559, Accuracy: 1.0, Computation time: 1.173008918762207\n",
      "Step: 4717, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.2597064971923828\n",
      "Step: 4718, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.1228315830230713\n",
      "Step: 4719, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.2041411399841309\n",
      "Step: 4720, Loss: 0.9593527913093567, Accuracy: 0.9583333730697632, Computation time: 1.2540771961212158\n",
      "Step: 4721, Loss: 0.9158727526664734, Accuracy: nan, Computation time: 1.81976318359375\n",
      "Step: 4722, Loss: 0.916002631187439, Accuracy: 1.0, Computation time: 1.1660120487213135\n",
      "Step: 4723, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.332012414932251\n",
      "Step: 4724, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.4207351207733154\n",
      "########################\n",
      "Test loss: 1.0715117454528809, Test Accuracy_epoch34: 0.7628034353256226\n",
      "########################\n",
      "Step: 4725, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.419640064239502\n",
      "Step: 4726, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.296741008758545\n",
      "Step: 4727, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.1608784198760986\n",
      "Step: 4728, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 1.3856523036956787\n",
      "Step: 4729, Loss: 0.9158470034599304, Accuracy: 1.0, Computation time: 1.4129934310913086\n",
      "Step: 4730, Loss: 0.9161003828048706, Accuracy: 1.0, Computation time: 1.8011198043823242\n",
      "Step: 4731, Loss: 0.9375726580619812, Accuracy: 0.9833333492279053, Computation time: 1.275705337524414\n",
      "Step: 4732, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.344233751296997\n",
      "Step: 4733, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.3664398193359375\n",
      "Step: 4734, Loss: 0.9302101135253906, Accuracy: 0.96875, Computation time: 1.8226096630096436\n",
      "Step: 4735, Loss: 0.9162785410881042, Accuracy: 1.0, Computation time: 1.2369775772094727\n",
      "Step: 4736, Loss: 0.9377120733261108, Accuracy: 0.9642857313156128, Computation time: 1.2516999244689941\n",
      "Step: 4737, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 1.7537164688110352\n",
      "Step: 4738, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 1.4129440784454346\n",
      "Step: 4739, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.4248592853546143\n",
      "Step: 4740, Loss: 0.9160819053649902, Accuracy: 1.0, Computation time: 1.6720705032348633\n",
      "Step: 4741, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 1.4166853427886963\n",
      "Step: 4742, Loss: 0.9160003662109375, Accuracy: 1.0, Computation time: 1.3858110904693604\n",
      "Step: 4743, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.363215446472168\n",
      "Step: 4744, Loss: 0.937522828578949, Accuracy: 0.9583333730697632, Computation time: 1.4703757762908936\n",
      "Step: 4745, Loss: 0.9188656210899353, Accuracy: 1.0, Computation time: 1.7417330741882324\n",
      "Step: 4746, Loss: 0.9165189862251282, Accuracy: 1.0, Computation time: 1.3179082870483398\n",
      "Step: 4747, Loss: 0.915998101234436, Accuracy: 1.0, Computation time: 1.1653952598571777\n",
      "Step: 4748, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.147136926651001\n",
      "Step: 4749, Loss: 0.9160652756690979, Accuracy: 1.0, Computation time: 1.2243554592132568\n",
      "Step: 4750, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.204960584640503\n",
      "Step: 4751, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.413088083267212\n",
      "Step: 4752, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 1.024000883102417\n",
      "Step: 4753, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.2319514751434326\n",
      "Step: 4754, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.2700812816619873\n",
      "Step: 4755, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 1.5004701614379883\n",
      "Step: 4756, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 1.0903191566467285\n",
      "Step: 4757, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.3362257480621338\n",
      "Step: 4758, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.4043920040130615\n",
      "Step: 4759, Loss: 0.9159508347511292, Accuracy: 1.0, Computation time: 1.4504878520965576\n",
      "Step: 4760, Loss: 0.9160588383674622, Accuracy: 1.0, Computation time: 1.2589077949523926\n",
      "Step: 4761, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.2344374656677246\n",
      "Step: 4762, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 1.631096363067627\n",
      "Step: 4763, Loss: 0.9165322780609131, Accuracy: 1.0, Computation time: 1.2739043235778809\n",
      "Step: 4764, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.3231306076049805\n",
      "Step: 4765, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.5513765811920166\n",
      "Step: 4766, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 1.5286900997161865\n",
      "Step: 4767, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.371983289718628\n",
      "Step: 4768, Loss: 0.9159038662910461, Accuracy: 1.0, Computation time: 1.445763111114502\n",
      "Step: 4769, Loss: 0.9375177025794983, Accuracy: 0.9583333730697632, Computation time: 1.2732584476470947\n",
      "Step: 4770, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.119142770767212\n",
      "Step: 4771, Loss: 0.9191275835037231, Accuracy: 1.0, Computation time: 1.8126816749572754\n",
      "Step: 4772, Loss: 0.9515247344970703, Accuracy: 0.970588207244873, Computation time: 2.0748274326324463\n",
      "Step: 4773, Loss: 0.9159920811653137, Accuracy: 1.0, Computation time: 1.3019640445709229\n",
      "Step: 4774, Loss: 0.9162333607673645, Accuracy: 1.0, Computation time: 1.2788138389587402\n",
      "Step: 4775, Loss: 0.9162366390228271, Accuracy: 1.0, Computation time: 1.5306587219238281\n",
      "Step: 4776, Loss: 0.9163829684257507, Accuracy: 1.0, Computation time: 1.7434313297271729\n",
      "Step: 4777, Loss: 0.9160572290420532, Accuracy: 1.0, Computation time: 1.4799737930297852\n",
      "Step: 4778, Loss: 0.9159557223320007, Accuracy: 1.0, Computation time: 1.4587297439575195\n",
      "Step: 4779, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 1.370589256286621\n",
      "Step: 4780, Loss: 0.9292404055595398, Accuracy: 0.949999988079071, Computation time: 1.6866273880004883\n",
      "Step: 4781, Loss: 0.9163316488265991, Accuracy: 1.0, Computation time: 1.621013879776001\n",
      "Step: 4782, Loss: 0.9163369536399841, Accuracy: 1.0, Computation time: 1.3911287784576416\n",
      "Step: 4783, Loss: 0.9162261486053467, Accuracy: 1.0, Computation time: 1.0860812664031982\n",
      "Step: 4784, Loss: 0.9164339900016785, Accuracy: 1.0, Computation time: 1.487175464630127\n",
      "Step: 4785, Loss: 0.9161039590835571, Accuracy: 1.0, Computation time: 1.2798514366149902\n",
      "Step: 4786, Loss: 0.9159911870956421, Accuracy: 1.0, Computation time: 1.2994670867919922\n",
      "Step: 4787, Loss: 0.916093647480011, Accuracy: 1.0, Computation time: 1.3501369953155518\n",
      "Step: 4788, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 1.1752517223358154\n",
      "Step: 4789, Loss: 0.9186457991600037, Accuracy: 1.0, Computation time: 1.622960090637207\n",
      "Step: 4790, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 1.2442114353179932\n",
      "Step: 4791, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 1.3308262825012207\n",
      "Step: 4792, Loss: 0.9159486889839172, Accuracy: 1.0, Computation time: 1.140817403793335\n",
      "Step: 4793, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.0773286819458008\n",
      "Step: 4794, Loss: 0.9196813702583313, Accuracy: 1.0, Computation time: 1.158951759338379\n",
      "Step: 4795, Loss: 0.9162030220031738, Accuracy: 1.0, Computation time: 1.3183567523956299\n",
      "Step: 4796, Loss: 0.9159297943115234, Accuracy: 1.0, Computation time: 1.374159336090088\n",
      "Step: 4797, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 1.1836678981781006\n",
      "Step: 4798, Loss: 0.9159981608390808, Accuracy: 1.0, Computation time: 1.6630988121032715\n",
      "Step: 4799, Loss: 0.9161131381988525, Accuracy: 1.0, Computation time: 1.2120568752288818\n",
      "Step: 4800, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 1.539015531539917\n",
      "Step: 4801, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.1585805416107178\n",
      "Step: 4802, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 1.7170348167419434\n",
      "Step: 4803, Loss: 0.9159204363822937, Accuracy: 1.0, Computation time: 1.3186695575714111\n",
      "Step: 4804, Loss: 0.9159393310546875, Accuracy: 1.0, Computation time: 1.207127332687378\n",
      "Step: 4805, Loss: 0.9374043941497803, Accuracy: 0.96875, Computation time: 1.1094753742218018\n",
      "Step: 4806, Loss: 0.9374010562896729, Accuracy: 0.9642857313156128, Computation time: 1.148813009262085\n",
      "Step: 4807, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 1.485961675643921\n",
      "Step: 4808, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.7417230606079102\n",
      "Step: 4809, Loss: 0.9376106262207031, Accuracy: 0.9750000238418579, Computation time: 1.2312123775482178\n",
      "Step: 4810, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.1290218830108643\n",
      "Step: 4811, Loss: 0.9375501871109009, Accuracy: 0.9722222089767456, Computation time: 1.1602721214294434\n",
      "Step: 4812, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.2996630668640137\n",
      "Step: 4813, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.25467848777771\n",
      "Step: 4814, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 2.0746710300445557\n",
      "Step: 4815, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 1.210618019104004\n",
      "Step: 4816, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.1503496170043945\n",
      "Step: 4817, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.0388596057891846\n",
      "Step: 4818, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 1.2515599727630615\n",
      "Step: 4819, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.1185977458953857\n",
      "Step: 4820, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.5738177299499512\n",
      "Step: 4821, Loss: 0.9376013875007629, Accuracy: 0.9642857313156128, Computation time: 1.3641667366027832\n",
      "Step: 4822, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.5914456844329834\n",
      "Step: 4823, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.0099658966064453\n",
      "Step: 4824, Loss: 0.9159517288208008, Accuracy: 1.0, Computation time: 1.2353589534759521\n",
      "Step: 4825, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.1183874607086182\n",
      "Step: 4826, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.2503085136413574\n",
      "Step: 4827, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.3323240280151367\n",
      "Step: 4828, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.132204294204712\n",
      "Step: 4829, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.5212321281433105\n",
      "Step: 4830, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.045372724533081\n",
      "Step: 4831, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.132718801498413\n",
      "Step: 4832, Loss: 0.9374456405639648, Accuracy: 0.96875, Computation time: 1.3799760341644287\n",
      "Step: 4833, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.461735486984253\n",
      "Step: 4834, Loss: 0.9376358985900879, Accuracy: 0.9852941036224365, Computation time: 1.0181689262390137\n",
      "Step: 4835, Loss: 0.9159782528877258, Accuracy: 1.0, Computation time: 0.9861056804656982\n",
      "Step: 4836, Loss: 0.9159687161445618, Accuracy: 1.0, Computation time: 1.3243334293365479\n",
      "Step: 4837, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.178093433380127\n",
      "Step: 4838, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.3265507221221924\n",
      "Step: 4839, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.154237985610962\n",
      "Step: 4840, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.0225811004638672\n",
      "Step: 4841, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.2803642749786377\n",
      "Step: 4842, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.2918295860290527\n",
      "Step: 4843, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.2951290607452393\n",
      "Step: 4844, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.9425735473632812\n",
      "Step: 4845, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.292515754699707\n",
      "Step: 4846, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.3242051601409912\n",
      "Step: 4847, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.3247649669647217\n",
      "Step: 4848, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.515491008758545\n",
      "Step: 4849, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 1.0844528675079346\n",
      "Step: 4850, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.324354887008667\n",
      "Step: 4851, Loss: 0.9158727526664734, Accuracy: 1.0, Computation time: 1.4029452800750732\n",
      "Step: 4852, Loss: 0.9375454783439636, Accuracy: 0.9642857313156128, Computation time: 1.183347225189209\n",
      "Step: 4853, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 1.2551348209381104\n",
      "Step: 4854, Loss: 0.9159471392631531, Accuracy: 1.0, Computation time: 1.2292845249176025\n",
      "Step: 4855, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.351240873336792\n",
      "Step: 4856, Loss: 0.9287055730819702, Accuracy: 0.949999988079071, Computation time: 1.5743658542633057\n",
      "Step: 4857, Loss: 0.9164956212043762, Accuracy: 1.0, Computation time: 1.1407995223999023\n",
      "Step: 4858, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 1.1625499725341797\n",
      "Step: 4859, Loss: 0.9377146363258362, Accuracy: 0.9807692766189575, Computation time: 1.1499927043914795\n",
      "Step: 4860, Loss: 0.9159855842590332, Accuracy: 1.0, Computation time: 1.1434082984924316\n",
      "Step: 4861, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 1.1730282306671143\n",
      "Step: 4862, Loss: 0.9159151911735535, Accuracy: 1.0, Computation time: 1.3603284358978271\n",
      "Step: 4863, Loss: 0.9376965165138245, Accuracy: 0.9807692766189575, Computation time: 1.1586363315582275\n",
      "########################\n",
      "Test loss: 1.0710548162460327, Test Accuracy_epoch35: 0.7666483521461487\n",
      "########################\n",
      "Step: 4864, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.5210793018341064\n",
      "Step: 4865, Loss: 0.9158779382705688, Accuracy: 1.0, Computation time: 1.2993898391723633\n",
      "Step: 4866, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.322084665298462\n",
      "Step: 4867, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.0202512741088867\n",
      "Step: 4868, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.1051151752471924\n",
      "Step: 4869, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.4464142322540283\n",
      "Step: 4870, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 1.2499988079071045\n",
      "Step: 4871, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.1223711967468262\n",
      "Step: 4872, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.0743935108184814\n",
      "Step: 4873, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.4215829372406006\n",
      "Step: 4874, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.101649284362793\n",
      "Step: 4875, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 1.1218898296356201\n",
      "Step: 4876, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.3496766090393066\n",
      "Step: 4877, Loss: 0.9376619458198547, Accuracy: 0.9821428656578064, Computation time: 1.4005446434020996\n",
      "Step: 4878, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 1.1358048915863037\n",
      "Step: 4879, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.274000644683838\n",
      "Step: 4880, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.1540377140045166\n",
      "Step: 4881, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.083533525466919\n",
      "Step: 4882, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 1.318052053451538\n",
      "Step: 4883, Loss: 0.9159404039382935, Accuracy: 1.0, Computation time: 1.3219377994537354\n",
      "Step: 4884, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.0829105377197266\n",
      "Step: 4885, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.4290060997009277\n",
      "Step: 4886, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 1.353022813796997\n",
      "Step: 4887, Loss: 0.937492311000824, Accuracy: 0.96875, Computation time: 1.0838205814361572\n",
      "Step: 4888, Loss: 0.9159979224205017, Accuracy: 1.0, Computation time: 1.4268720149993896\n",
      "Step: 4889, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.5000457763671875\n",
      "Step: 4890, Loss: 0.9158881306648254, Accuracy: 1.0, Computation time: 1.1421575546264648\n",
      "Step: 4891, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.1068832874298096\n",
      "Step: 4892, Loss: 0.9191728830337524, Accuracy: 1.0, Computation time: 1.2315638065338135\n",
      "Step: 4893, Loss: 0.92855304479599, Accuracy: 0.9791666865348816, Computation time: 1.223970890045166\n",
      "Step: 4894, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.2016689777374268\n",
      "Step: 4895, Loss: 0.9159358739852905, Accuracy: 1.0, Computation time: 1.3418240547180176\n",
      "Step: 4896, Loss: 0.9160536527633667, Accuracy: 1.0, Computation time: 1.3471639156341553\n",
      "Step: 4897, Loss: 0.916102409362793, Accuracy: 1.0, Computation time: 1.249927043914795\n",
      "Step: 4898, Loss: 0.9160943627357483, Accuracy: 1.0, Computation time: 1.4791889190673828\n",
      "Step: 4899, Loss: 0.9159667491912842, Accuracy: 1.0, Computation time: 1.3845510482788086\n",
      "Step: 4900, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 2.2715749740600586\n",
      "Step: 4901, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.2514572143554688\n",
      "Step: 4902, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.3823280334472656\n",
      "Step: 4903, Loss: 0.9164182543754578, Accuracy: 1.0, Computation time: 1.0016469955444336\n",
      "Step: 4904, Loss: 0.9377069473266602, Accuracy: 0.96875, Computation time: 1.1395792961120605\n",
      "Step: 4905, Loss: 0.9159504175186157, Accuracy: 1.0, Computation time: 1.1497442722320557\n",
      "Step: 4906, Loss: 0.9377368688583374, Accuracy: 0.9821428656578064, Computation time: 1.1822080612182617\n",
      "Step: 4907, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.1217362880706787\n",
      "Step: 4908, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.4981915950775146\n",
      "Step: 4909, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 1.5311005115509033\n",
      "Step: 4910, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.0601325035095215\n",
      "Step: 4911, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.023758888244629\n",
      "Step: 4912, Loss: 0.9159510135650635, Accuracy: 1.0, Computation time: 1.0976941585540771\n",
      "Step: 4913, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.2608773708343506\n",
      "Step: 4914, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.1689343452453613\n",
      "Step: 4915, Loss: 0.9544907212257385, Accuracy: 0.9166666865348816, Computation time: 1.2918744087219238\n",
      "Step: 4916, Loss: 0.9321538209915161, Accuracy: 0.96875, Computation time: 1.288738489151001\n",
      "Step: 4917, Loss: 0.9159915447235107, Accuracy: 1.0, Computation time: 1.119018316268921\n",
      "Step: 4918, Loss: 0.9160078763961792, Accuracy: 1.0, Computation time: 1.5874824523925781\n",
      "Step: 4919, Loss: 0.9164674282073975, Accuracy: 1.0, Computation time: 1.4513192176818848\n",
      "Step: 4920, Loss: 0.9162358641624451, Accuracy: 1.0, Computation time: 1.3264427185058594\n",
      "Step: 4921, Loss: 0.9159682393074036, Accuracy: 1.0, Computation time: 1.1230111122131348\n",
      "Step: 4922, Loss: 0.9159426093101501, Accuracy: 1.0, Computation time: 1.3267722129821777\n",
      "Step: 4923, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.460944414138794\n",
      "Step: 4924, Loss: 0.9376394152641296, Accuracy: 0.9772727489471436, Computation time: 1.4110045433044434\n",
      "Step: 4925, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 1.3721094131469727\n",
      "Step: 4926, Loss: 0.9160090684890747, Accuracy: 1.0, Computation time: 1.216517448425293\n",
      "Step: 4927, Loss: 0.91603022813797, Accuracy: 1.0, Computation time: 1.3275458812713623\n",
      "Step: 4928, Loss: 0.9159243702888489, Accuracy: 1.0, Computation time: 1.5520906448364258\n",
      "Step: 4929, Loss: 0.9159797430038452, Accuracy: 1.0, Computation time: 1.2174005508422852\n",
      "Step: 4930, Loss: 0.9160375595092773, Accuracy: 1.0, Computation time: 1.1938765048980713\n",
      "Step: 4931, Loss: 0.9161083698272705, Accuracy: 1.0, Computation time: 1.4769785404205322\n",
      "Step: 4932, Loss: 0.9166719317436218, Accuracy: 1.0, Computation time: 1.2745509147644043\n",
      "Step: 4933, Loss: 0.91591876745224, Accuracy: 1.0, Computation time: 1.3145074844360352\n",
      "Step: 4934, Loss: 0.9160398840904236, Accuracy: 1.0, Computation time: 1.6561799049377441\n",
      "Step: 4935, Loss: 0.9159473180770874, Accuracy: 1.0, Computation time: 1.1663315296173096\n",
      "Step: 4936, Loss: 0.9377277493476868, Accuracy: 0.949999988079071, Computation time: 1.5926799774169922\n",
      "Step: 4937, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.265634536743164\n",
      "Step: 4938, Loss: 0.9166348576545715, Accuracy: 1.0, Computation time: 1.2233922481536865\n",
      "Step: 4939, Loss: 0.9372295141220093, Accuracy: 0.96875, Computation time: 1.2956533432006836\n",
      "Step: 4940, Loss: 0.9591411352157593, Accuracy: 0.9409722089767456, Computation time: 1.378312110900879\n",
      "Step: 4941, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.3057990074157715\n",
      "Step: 4942, Loss: 0.937629222869873, Accuracy: 0.9642857313156128, Computation time: 1.4472599029541016\n",
      "Step: 4943, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.373910665512085\n",
      "Step: 4944, Loss: 0.9298567771911621, Accuracy: 0.96875, Computation time: 1.1946158409118652\n",
      "Step: 4945, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 1.1472527980804443\n",
      "Step: 4946, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 1.4569144248962402\n",
      "Step: 4947, Loss: 0.915974497795105, Accuracy: 1.0, Computation time: 1.4972643852233887\n",
      "Step: 4948, Loss: 0.9176374673843384, Accuracy: 1.0, Computation time: 1.5917565822601318\n",
      "Step: 4949, Loss: 0.9158961176872253, Accuracy: 1.0, Computation time: 1.088087797164917\n",
      "Step: 4950, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 1.1715624332427979\n",
      "Step: 4951, Loss: 0.9164457321166992, Accuracy: 1.0, Computation time: 1.1252596378326416\n",
      "Step: 4952, Loss: 0.9376207590103149, Accuracy: 0.9821428656578064, Computation time: 1.4451773166656494\n",
      "Step: 4953, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.372009515762329\n",
      "Step: 4954, Loss: 0.9179142117500305, Accuracy: 1.0, Computation time: 1.1712515354156494\n",
      "Step: 4955, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 1.1000654697418213\n",
      "Step: 4956, Loss: 0.9161680936813354, Accuracy: 1.0, Computation time: 1.2873010635375977\n",
      "Step: 4957, Loss: 0.915971577167511, Accuracy: 1.0, Computation time: 1.1494812965393066\n",
      "Step: 4958, Loss: 0.9159968495368958, Accuracy: 1.0, Computation time: 1.251941442489624\n",
      "Step: 4959, Loss: 0.9375669360160828, Accuracy: 0.9772727489471436, Computation time: 1.0477228164672852\n",
      "Step: 4960, Loss: 0.9160215258598328, Accuracy: 1.0, Computation time: 2.0252139568328857\n",
      "Step: 4961, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.0512518882751465\n",
      "Step: 4962, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.136437177658081\n",
      "Step: 4963, Loss: 0.91620272397995, Accuracy: 1.0, Computation time: 1.196554183959961\n",
      "Step: 4964, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.1822540760040283\n",
      "Step: 4965, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.1021058559417725\n",
      "Step: 4966, Loss: 0.9161139130592346, Accuracy: 1.0, Computation time: 1.1061069965362549\n",
      "Step: 4967, Loss: 0.9372360706329346, Accuracy: 0.9750000238418579, Computation time: 1.3934648036956787\n",
      "Step: 4968, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.5643470287322998\n",
      "Step: 4969, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.2002232074737549\n",
      "Step: 4970, Loss: 0.9374946355819702, Accuracy: 0.9750000238418579, Computation time: 1.54315185546875\n",
      "Step: 4971, Loss: 0.9159250855445862, Accuracy: 1.0, Computation time: 1.066929578781128\n",
      "Step: 4972, Loss: 0.9378502368927002, Accuracy: 0.9642857313156128, Computation time: 1.3977367877960205\n",
      "Step: 4973, Loss: 0.9159307479858398, Accuracy: 1.0, Computation time: 1.4059269428253174\n",
      "Step: 4974, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.311478614807129\n",
      "Step: 4975, Loss: 0.9375123977661133, Accuracy: 0.9791666865348816, Computation time: 1.3037617206573486\n",
      "Step: 4976, Loss: 0.9178410768508911, Accuracy: 1.0, Computation time: 1.4141955375671387\n",
      "Step: 4977, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 0.9972231388092041\n",
      "Step: 4978, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.131605625152588\n",
      "Step: 4979, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.4066617488861084\n",
      "Step: 4980, Loss: 0.9160243272781372, Accuracy: 1.0, Computation time: 1.1158638000488281\n",
      "Step: 4981, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.2884328365325928\n",
      "Step: 4982, Loss: 0.9159186482429504, Accuracy: 1.0, Computation time: 1.0503430366516113\n",
      "Step: 4983, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 0.9480736255645752\n",
      "Step: 4984, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 0.9910473823547363\n",
      "Step: 4985, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.2469158172607422\n",
      "Step: 4986, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 1.111720085144043\n",
      "Step: 4987, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.278371810913086\n",
      "Step: 4988, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.3733861446380615\n",
      "Step: 4989, Loss: 0.9159986972808838, Accuracy: 1.0, Computation time: 1.3511996269226074\n",
      "Step: 4990, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.2943382263183594\n",
      "Step: 4991, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.080265760421753\n",
      "Step: 4992, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.3567228317260742\n",
      "Step: 4993, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.0341615676879883\n",
      "Step: 4994, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.4999806880950928\n",
      "Step: 4995, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.3534598350524902\n",
      "Step: 4996, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.0787160396575928\n",
      "Step: 4997, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.5972042083740234\n",
      "Step: 4998, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 0.880218505859375\n",
      "Step: 4999, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.1685240268707275\n",
      "Step: 5000, Loss: 0.9160221219062805, Accuracy: 1.0, Computation time: 1.2102324962615967\n",
      "Step: 5001, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.2957370281219482\n",
      "Step: 5002, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.3214268684387207\n",
      "########################\n",
      "Test loss: 1.0734105110168457, Test Accuracy_epoch36: 0.7678354382514954\n",
      "########################\n",
      "Step: 5003, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.2343499660491943\n",
      "Step: 5004, Loss: 0.9158607125282288, Accuracy: nan, Computation time: 0.9809672832489014\n",
      "Step: 5005, Loss: 0.9213500618934631, Accuracy: 1.0, Computation time: 1.7014598846435547\n",
      "Step: 5006, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.0768067836761475\n",
      "Step: 5007, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.2102241516113281\n",
      "Step: 5008, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.2310173511505127\n",
      "Step: 5009, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.2303977012634277\n",
      "Step: 5010, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.4180080890655518\n",
      "Step: 5011, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.108363151550293\n",
      "Step: 5012, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.4784207344055176\n",
      "Step: 5013, Loss: 0.9159848690032959, Accuracy: 1.0, Computation time: 1.2927308082580566\n",
      "Step: 5014, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.1438825130462646\n",
      "Step: 5015, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 2.4607133865356445\n",
      "Step: 5016, Loss: 0.9375196695327759, Accuracy: 0.9791666865348816, Computation time: 1.1912894248962402\n",
      "Step: 5017, Loss: 0.9158541560173035, Accuracy: 1.0, Computation time: 1.3185820579528809\n",
      "Step: 5018, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.1265084743499756\n",
      "Step: 5019, Loss: 0.9158409833908081, Accuracy: 1.0, Computation time: 1.1800239086151123\n",
      "Step: 5020, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 1.1172869205474854\n",
      "Step: 5021, Loss: 0.9374784231185913, Accuracy: 0.96875, Computation time: 1.4329478740692139\n",
      "Step: 5022, Loss: 0.9158381819725037, Accuracy: 1.0, Computation time: 1.1630792617797852\n",
      "Step: 5023, Loss: 0.9160974621772766, Accuracy: 1.0, Computation time: 1.5283596515655518\n",
      "Step: 5024, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.9972293376922607\n",
      "Step: 5025, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.201392650604248\n",
      "Step: 5026, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.1822187900543213\n",
      "Step: 5027, Loss: 0.9164817333221436, Accuracy: 1.0, Computation time: 1.4268360137939453\n",
      "Step: 5028, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 1.1534514427185059\n",
      "Step: 5029, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 1.4740736484527588\n",
      "Step: 5030, Loss: 0.917422354221344, Accuracy: 1.0, Computation time: 1.8134605884552002\n",
      "Step: 5031, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.042128086090088\n",
      "Step: 5032, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.6430151462554932\n",
      "Step: 5033, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.4398226737976074\n",
      "Step: 5034, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 1.4178340435028076\n",
      "Step: 5035, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 1.3575963973999023\n",
      "Step: 5036, Loss: 0.9163475036621094, Accuracy: 1.0, Computation time: 1.59944748878479\n",
      "Step: 5037, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.3048381805419922\n",
      "Step: 5038, Loss: 0.9167301058769226, Accuracy: 1.0, Computation time: 1.140493392944336\n",
      "Step: 5039, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 1.1024055480957031\n",
      "Step: 5040, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.8253633975982666\n",
      "Step: 5041, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.2987117767333984\n",
      "Step: 5042, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.3287599086761475\n",
      "Step: 5043, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.4957139492034912\n",
      "Step: 5044, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.3195533752441406\n",
      "Step: 5045, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 1.8543071746826172\n",
      "Step: 5046, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.3009626865386963\n",
      "Step: 5047, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.575150728225708\n",
      "Step: 5048, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.4995479583740234\n",
      "Step: 5049, Loss: 0.9172543883323669, Accuracy: 1.0, Computation time: 1.6153299808502197\n",
      "Step: 5050, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.4487857818603516\n",
      "Step: 5051, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.3712623119354248\n",
      "Step: 5052, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.1834216117858887\n",
      "Step: 5053, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.524003505706787\n",
      "Step: 5054, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.7069644927978516\n",
      "Step: 5055, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.2637975215911865\n",
      "Step: 5056, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.431668996810913\n",
      "Step: 5057, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 1.5473532676696777\n",
      "Step: 5058, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.6085846424102783\n",
      "Step: 5059, Loss: 0.9375447034835815, Accuracy: 0.96875, Computation time: 1.5462870597839355\n",
      "Step: 5060, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 1.9184765815734863\n",
      "Step: 5061, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.6508967876434326\n",
      "Step: 5062, Loss: 0.9158346056938171, Accuracy: 1.0, Computation time: 1.344233751296997\n",
      "Step: 5063, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.7356128692626953\n",
      "Step: 5064, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.2162024974822998\n",
      "Step: 5065, Loss: 0.9158323407173157, Accuracy: 1.0, Computation time: 1.4856829643249512\n",
      "Step: 5066, Loss: 0.9375402331352234, Accuracy: 0.9642857313156128, Computation time: 1.5840706825256348\n",
      "Step: 5067, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.1544649600982666\n",
      "Step: 5068, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 1.2331905364990234\n",
      "Step: 5069, Loss: 0.9158353805541992, Accuracy: 1.0, Computation time: 1.5709388256072998\n",
      "Step: 5070, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 1.3886735439300537\n",
      "Step: 5071, Loss: 0.915999710559845, Accuracy: 1.0, Computation time: 1.2601728439331055\n",
      "Step: 5072, Loss: 0.9335543513298035, Accuracy: 0.9642857313156128, Computation time: 2.015782594680786\n",
      "Step: 5073, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.7261979579925537\n",
      "Step: 5074, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.153355360031128\n",
      "Step: 5075, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 1.8217570781707764\n",
      "Step: 5076, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 1.3960916996002197\n",
      "Step: 5077, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.1216504573822021\n",
      "Step: 5078, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.262411117553711\n",
      "Step: 5079, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 1.0415661334991455\n",
      "Step: 5080, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.3267829418182373\n",
      "Step: 5081, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.4095523357391357\n",
      "Step: 5082, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.515556812286377\n",
      "Step: 5083, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.2936761379241943\n",
      "Step: 5084, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.2436165809631348\n",
      "Step: 5085, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.2938206195831299\n",
      "Step: 5086, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.2827489376068115\n",
      "Step: 5087, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.2358410358428955\n",
      "Step: 5088, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.2683112621307373\n",
      "Step: 5089, Loss: 0.9158603549003601, Accuracy: 1.0, Computation time: 1.4538602828979492\n",
      "Step: 5090, Loss: 0.9159160852432251, Accuracy: 1.0, Computation time: 1.8956987857818604\n",
      "Step: 5091, Loss: 0.9158348441123962, Accuracy: 1.0, Computation time: 1.0828776359558105\n",
      "Step: 5092, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.2905504703521729\n",
      "Step: 5093, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.57234787940979\n",
      "Step: 5094, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 1.2276942729949951\n",
      "Step: 5095, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.1934785842895508\n",
      "Step: 5096, Loss: 0.9158415794372559, Accuracy: 1.0, Computation time: 1.2290687561035156\n",
      "Step: 5097, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.2864561080932617\n",
      "Step: 5098, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.5458638668060303\n",
      "Step: 5099, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.2965188026428223\n",
      "Step: 5100, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 1.1955955028533936\n",
      "Step: 5101, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.1204423904418945\n",
      "Step: 5102, Loss: 0.9380477070808411, Accuracy: 0.9583333730697632, Computation time: 1.579507827758789\n",
      "Step: 5103, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.1859095096588135\n",
      "Step: 5104, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.1598446369171143\n",
      "Step: 5105, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.4062438011169434\n",
      "Step: 5106, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.0978403091430664\n",
      "Step: 5107, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.1295979022979736\n",
      "Step: 5108, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.3084075450897217\n",
      "Step: 5109, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.1933746337890625\n",
      "Step: 5110, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.5051708221435547\n",
      "Step: 5111, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.1922383308410645\n",
      "Step: 5112, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.2447102069854736\n",
      "Step: 5113, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 1.2701036930084229\n",
      "Step: 5114, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 1.525223970413208\n",
      "Step: 5115, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 1.3118598461151123\n",
      "Step: 5116, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 1.2821159362792969\n",
      "Step: 5117, Loss: 0.9375544786453247, Accuracy: 0.9642857313156128, Computation time: 1.3604984283447266\n",
      "Step: 5118, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 1.270705223083496\n",
      "Step: 5119, Loss: 0.9393956661224365, Accuracy: 0.9772727489471436, Computation time: 1.7003772258758545\n",
      "Step: 5120, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.1357240676879883\n",
      "Step: 5121, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 1.199279546737671\n",
      "Step: 5122, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 1.4608454704284668\n",
      "Step: 5123, Loss: 0.9375094175338745, Accuracy: 0.9791666865348816, Computation time: 1.2974717617034912\n",
      "Step: 5124, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.174802541732788\n",
      "Step: 5125, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.3083064556121826\n",
      "Step: 5126, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.4913125038146973\n",
      "Step: 5127, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.1659071445465088\n",
      "Step: 5128, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 1.2117512226104736\n",
      "Step: 5129, Loss: 0.9162667989730835, Accuracy: 1.0, Computation time: 1.2927634716033936\n",
      "Step: 5130, Loss: 0.9375381469726562, Accuracy: 0.96875, Computation time: 1.3587183952331543\n",
      "Step: 5131, Loss: 0.9376109838485718, Accuracy: 0.9807692766189575, Computation time: 1.1533324718475342\n",
      "Step: 5132, Loss: 0.9158366918563843, Accuracy: 1.0, Computation time: 1.2427423000335693\n",
      "Step: 5133, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.2215418815612793\n",
      "Step: 5134, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.2726449966430664\n",
      "Step: 5135, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 1.1882588863372803\n",
      "Step: 5136, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.2384085655212402\n",
      "Step: 5137, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.1559808254241943\n",
      "Step: 5138, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.495267391204834\n",
      "Step: 5139, Loss: 0.9374367594718933, Accuracy: 0.9821428656578064, Computation time: 1.126927375793457\n",
      "Step: 5140, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 1.0866544246673584\n",
      "Step: 5141, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 1.394843578338623\n",
      "########################\n",
      "Test loss: 1.070935845375061, Test Accuracy_epoch37: 0.7665502429008484\n",
      "########################\n",
      "Step: 5142, Loss: 0.9375308156013489, Accuracy: 0.9642857313156128, Computation time: 1.6761503219604492\n",
      "Step: 5143, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.355811595916748\n",
      "Step: 5144, Loss: 0.9159612655639648, Accuracy: 1.0, Computation time: 1.3915510177612305\n",
      "Step: 5145, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.2843196392059326\n",
      "Step: 5146, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 1.2223823070526123\n",
      "Step: 5147, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.1043896675109863\n",
      "Step: 5148, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.0427157878875732\n",
      "Step: 5149, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.2343144416809082\n",
      "Step: 5150, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.3333704471588135\n",
      "Step: 5151, Loss: 0.9158509969711304, Accuracy: 1.0, Computation time: 1.175546646118164\n",
      "Step: 5152, Loss: 0.915836751461029, Accuracy: 1.0, Computation time: 1.2958290576934814\n",
      "Step: 5153, Loss: 0.9255651235580444, Accuracy: 1.0, Computation time: 1.268371820449829\n",
      "Step: 5154, Loss: 0.9158894419670105, Accuracy: 1.0, Computation time: 1.5761768817901611\n",
      "Step: 5155, Loss: 0.9376835227012634, Accuracy: 0.9750000238418579, Computation time: 1.49546480178833\n",
      "Step: 5156, Loss: 0.9160272479057312, Accuracy: 1.0, Computation time: 1.3216142654418945\n",
      "Step: 5157, Loss: 0.9160478711128235, Accuracy: 1.0, Computation time: 1.600764513015747\n",
      "Step: 5158, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.3928751945495605\n",
      "Step: 5159, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 1.4824399948120117\n",
      "Step: 5160, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.4861407279968262\n",
      "Step: 5161, Loss: 0.9376694560050964, Accuracy: 0.9722222089767456, Computation time: 1.5876374244689941\n",
      "Step: 5162, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.3215372562408447\n",
      "Step: 5163, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.5553789138793945\n",
      "Step: 5164, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 1.2690315246582031\n",
      "Step: 5165, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.2529888153076172\n",
      "Step: 5166, Loss: 0.916019082069397, Accuracy: 1.0, Computation time: 1.3994276523590088\n",
      "Step: 5167, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.5688121318817139\n",
      "Step: 5168, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.5653471946716309\n",
      "Step: 5169, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.4241535663604736\n",
      "Step: 5170, Loss: 0.9375197887420654, Accuracy: 0.9722222089767456, Computation time: 1.515211582183838\n",
      "Step: 5171, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 1.2515428066253662\n",
      "Step: 5172, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.424943447113037\n",
      "Step: 5173, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.4239745140075684\n",
      "Step: 5174, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.643937110900879\n",
      "Step: 5175, Loss: 0.9158408045768738, Accuracy: 1.0, Computation time: 1.6140310764312744\n",
      "Step: 5176, Loss: 0.937462568283081, Accuracy: 0.9807692766189575, Computation time: 1.4848439693450928\n",
      "Step: 5177, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.5466570854187012\n",
      "Step: 5178, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.3575890064239502\n",
      "Step: 5179, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.3532588481903076\n",
      "Step: 5180, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.6404869556427002\n",
      "Step: 5181, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.0929253101348877\n",
      "Step: 5182, Loss: 0.9158425331115723, Accuracy: 1.0, Computation time: 1.4476559162139893\n",
      "Step: 5183, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 1.3585238456726074\n",
      "Step: 5184, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 1.3758461475372314\n",
      "Step: 5185, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 1.5986347198486328\n",
      "Step: 5186, Loss: 0.9377113580703735, Accuracy: 0.9791666865348816, Computation time: 1.6893503665924072\n",
      "Step: 5187, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.4432501792907715\n",
      "Step: 5188, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 1.1505122184753418\n",
      "Step: 5189, Loss: 0.9165692925453186, Accuracy: 1.0, Computation time: 1.780379056930542\n",
      "Step: 5190, Loss: 0.9375080466270447, Accuracy: 0.9750000238418579, Computation time: 1.3772273063659668\n",
      "Step: 5191, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.7337186336517334\n",
      "Step: 5192, Loss: 0.9159412980079651, Accuracy: 1.0, Computation time: 1.7137455940246582\n",
      "Step: 5193, Loss: 0.9161939024925232, Accuracy: 1.0, Computation time: 1.6303305625915527\n",
      "Step: 5194, Loss: 0.9375725388526917, Accuracy: 0.9772727489471436, Computation time: 1.767317771911621\n",
      "Step: 5195, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 1.357048511505127\n",
      "Step: 5196, Loss: 0.9375969171524048, Accuracy: 0.9791666865348816, Computation time: 1.4109714031219482\n",
      "Step: 5197, Loss: 0.9158482551574707, Accuracy: 1.0, Computation time: 1.4109976291656494\n",
      "Step: 5198, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.3811852931976318\n",
      "Step: 5199, Loss: 0.9594443440437317, Accuracy: 0.9444444179534912, Computation time: 1.3051316738128662\n",
      "Step: 5200, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.5497479438781738\n",
      "Step: 5201, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.291032075881958\n",
      "Step: 5202, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.3557519912719727\n",
      "Step: 5203, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 1.4342706203460693\n",
      "Step: 5204, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 1.443225622177124\n",
      "Step: 5205, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.3154118061065674\n",
      "Step: 5206, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.3672943115234375\n",
      "Step: 5207, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 1.2628908157348633\n",
      "Step: 5208, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.1815130710601807\n",
      "Step: 5209, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.4382381439208984\n",
      "Step: 5210, Loss: 0.9375435709953308, Accuracy: 0.9807692766189575, Computation time: 1.205735206604004\n",
      "Step: 5211, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.456178903579712\n",
      "Step: 5212, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 1.1187717914581299\n",
      "Step: 5213, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.305753231048584\n",
      "Step: 5214, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.3514399528503418\n",
      "Step: 5215, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.0531363487243652\n",
      "Step: 5216, Loss: 0.9158428907394409, Accuracy: 1.0, Computation time: 1.6921463012695312\n",
      "Step: 5217, Loss: 0.9375631213188171, Accuracy: 0.9807692766189575, Computation time: 1.360180377960205\n",
      "Step: 5218, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.1830592155456543\n",
      "Step: 5219, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 1.3385775089263916\n",
      "Step: 5220, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.4666099548339844\n",
      "Step: 5221, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.6383545398712158\n",
      "Step: 5222, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.165971040725708\n",
      "Step: 5223, Loss: 0.9375404715538025, Accuracy: 0.9791666865348816, Computation time: 1.3540441989898682\n",
      "Step: 5224, Loss: 0.9161391854286194, Accuracy: 1.0, Computation time: 1.6295392513275146\n",
      "Step: 5225, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.3004961013793945\n",
      "Step: 5226, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.2601943016052246\n",
      "Step: 5227, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.261756420135498\n",
      "Step: 5228, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.311859369277954\n",
      "Step: 5229, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.5082740783691406\n",
      "Step: 5230, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.47098708152771\n",
      "Step: 5231, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 1.4182021617889404\n",
      "Step: 5232, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 1.476276159286499\n",
      "Step: 5233, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 1.5141775608062744\n",
      "Step: 5234, Loss: 0.9260739684104919, Accuracy: 0.9642857313156128, Computation time: 1.3890643119812012\n",
      "Step: 5235, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.5990324020385742\n",
      "Step: 5236, Loss: 0.9159307479858398, Accuracy: 1.0, Computation time: 1.3353829383850098\n",
      "Step: 5237, Loss: 0.9159668684005737, Accuracy: 1.0, Computation time: 1.6747360229492188\n",
      "Step: 5238, Loss: 0.937752366065979, Accuracy: 0.96875, Computation time: 1.2847235202789307\n",
      "Step: 5239, Loss: 0.9159392714500427, Accuracy: 1.0, Computation time: 1.1454088687896729\n",
      "Step: 5240, Loss: 0.9159061312675476, Accuracy: 1.0, Computation time: 1.0364887714385986\n",
      "Step: 5241, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.455418586730957\n",
      "Step: 5242, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.1177978515625\n",
      "Step: 5243, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.478515625\n",
      "Step: 5244, Loss: 0.9159497022628784, Accuracy: 1.0, Computation time: 1.2181289196014404\n",
      "Step: 5245, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.57806396484375\n",
      "Step: 5246, Loss: 0.9336149096488953, Accuracy: 0.9791666865348816, Computation time: 1.630859613418579\n",
      "Step: 5247, Loss: 0.915911078453064, Accuracy: 1.0, Computation time: 1.32047700881958\n",
      "Step: 5248, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 1.377460241317749\n",
      "Step: 5249, Loss: 0.9159924387931824, Accuracy: 1.0, Computation time: 1.7194125652313232\n",
      "Step: 5250, Loss: 0.9168480634689331, Accuracy: 1.0, Computation time: 1.371732234954834\n",
      "Step: 5251, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.441617727279663\n",
      "Step: 5252, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.4559886455535889\n",
      "Step: 5253, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.1397101879119873\n",
      "Step: 5254, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.24061918258667\n",
      "Step: 5255, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.2613005638122559\n",
      "Step: 5256, Loss: 0.9158821105957031, Accuracy: 1.0, Computation time: 1.1712286472320557\n",
      "Step: 5257, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.3224666118621826\n",
      "Step: 5258, Loss: 0.9158753156661987, Accuracy: 1.0, Computation time: 1.5039727687835693\n",
      "Step: 5259, Loss: 0.9159165024757385, Accuracy: 1.0, Computation time: 1.4291880130767822\n",
      "Step: 5260, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.1806955337524414\n",
      "Step: 5261, Loss: 0.9160687923431396, Accuracy: 1.0, Computation time: 1.4592628479003906\n",
      "Step: 5262, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.4097480773925781\n",
      "Step: 5263, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.1954896450042725\n",
      "Step: 5264, Loss: 0.937551736831665, Accuracy: 0.9821428656578064, Computation time: 1.4298486709594727\n",
      "Step: 5265, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.4115278720855713\n",
      "Step: 5266, Loss: 0.9159286022186279, Accuracy: 1.0, Computation time: 1.3027184009552002\n",
      "Step: 5267, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.75868821144104\n",
      "Step: 5268, Loss: 0.9159424304962158, Accuracy: 1.0, Computation time: 1.168334722518921\n",
      "Step: 5269, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.3160624504089355\n",
      "Step: 5270, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.3609414100646973\n",
      "Step: 5271, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 1.2713842391967773\n",
      "Step: 5272, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 1.7075648307800293\n",
      "Step: 5273, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.0406534671783447\n",
      "Step: 5274, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 1.111536979675293\n",
      "Step: 5275, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.2303366661071777\n",
      "Step: 5276, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.2806541919708252\n",
      "Step: 5277, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.204981803894043\n",
      "Step: 5278, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.3694438934326172\n",
      "Step: 5279, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.1523747444152832\n",
      "Step: 5280, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.2212326526641846\n",
      "########################\n",
      "Test loss: 1.0691089630126953, Test Accuracy_epoch38: 0.7686321139335632\n",
      "########################\n",
      "Step: 5281, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.9219508171081543\n",
      "Step: 5282, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.4754807949066162\n",
      "Step: 5283, Loss: 0.915838360786438, Accuracy: 1.0, Computation time: 1.2499127388000488\n",
      "Step: 5284, Loss: 0.9375004768371582, Accuracy: 0.9750000238418579, Computation time: 1.5613703727722168\n",
      "Step: 5285, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.3114848136901855\n",
      "Step: 5286, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.4825708866119385\n",
      "Step: 5287, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.239302635192871\n",
      "Step: 5288, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 1.4686121940612793\n",
      "Step: 5289, Loss: 0.9158359169960022, Accuracy: 1.0, Computation time: 1.2911837100982666\n",
      "Step: 5290, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.4879050254821777\n",
      "Step: 5291, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.2510180473327637\n",
      "Step: 5292, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 1.3428189754486084\n",
      "Step: 5293, Loss: 0.9375495910644531, Accuracy: 0.9791666865348816, Computation time: 1.136963129043579\n",
      "Step: 5294, Loss: 0.9158331155776978, Accuracy: 1.0, Computation time: 1.4813227653503418\n",
      "Step: 5295, Loss: 0.935312032699585, Accuracy: 0.9750000238418579, Computation time: 1.6387436389923096\n",
      "Step: 5296, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.377333641052246\n",
      "Step: 5297, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.1318986415863037\n",
      "Step: 5298, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.8249919414520264\n",
      "Step: 5299, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 1.676154375076294\n",
      "Step: 5300, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 1.8053991794586182\n",
      "Step: 5301, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.3679401874542236\n",
      "Step: 5302, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.26759934425354\n",
      "Step: 5303, Loss: 0.9375171661376953, Accuracy: 0.96875, Computation time: 1.6678082942962646\n",
      "Step: 5304, Loss: 0.9158366322517395, Accuracy: 1.0, Computation time: 1.2467567920684814\n",
      "Step: 5305, Loss: 0.9158312082290649, Accuracy: 1.0, Computation time: 1.6769664287567139\n",
      "Step: 5306, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.6152317523956299\n",
      "Step: 5307, Loss: 0.9158338308334351, Accuracy: 1.0, Computation time: 1.241633415222168\n",
      "Step: 5308, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 1.4959778785705566\n",
      "Step: 5309, Loss: 0.9158366918563843, Accuracy: 1.0, Computation time: 1.2159833908081055\n",
      "Step: 5310, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.3570289611816406\n",
      "Step: 5311, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.369981050491333\n",
      "Step: 5312, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.3870539665222168\n",
      "Step: 5313, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.6046640872955322\n",
      "Step: 5314, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.4856221675872803\n",
      "Step: 5315, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 1.3635730743408203\n",
      "Step: 5316, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.3402526378631592\n",
      "Step: 5317, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.3436899185180664\n",
      "Step: 5318, Loss: 0.9158307313919067, Accuracy: 1.0, Computation time: 0.9988501071929932\n",
      "Step: 5319, Loss: 0.9158359169960022, Accuracy: 1.0, Computation time: 1.3594136238098145\n",
      "Step: 5320, Loss: 0.9158363938331604, Accuracy: 1.0, Computation time: 1.2911264896392822\n",
      "Step: 5321, Loss: 0.9158355593681335, Accuracy: 1.0, Computation time: 1.193450689315796\n",
      "Step: 5322, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.4406628608703613\n",
      "Step: 5323, Loss: 0.9375437498092651, Accuracy: 0.9642857313156128, Computation time: 1.049774408340454\n",
      "Step: 5324, Loss: 0.9158366918563843, Accuracy: 1.0, Computation time: 1.0557348728179932\n",
      "Step: 5325, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.1825385093688965\n",
      "Step: 5326, Loss: 0.915834903717041, Accuracy: 1.0, Computation time: 1.2134642601013184\n",
      "Step: 5327, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.388993740081787\n",
      "Step: 5328, Loss: 0.9158349633216858, Accuracy: 1.0, Computation time: 1.2829103469848633\n",
      "Step: 5329, Loss: 0.9158356785774231, Accuracy: 1.0, Computation time: 1.0915498733520508\n",
      "Step: 5330, Loss: 0.9158337116241455, Accuracy: 1.0, Computation time: 1.3163986206054688\n",
      "Step: 5331, Loss: 0.9161209464073181, Accuracy: 1.0, Computation time: 1.2321076393127441\n",
      "Step: 5332, Loss: 0.9158310890197754, Accuracy: 1.0, Computation time: 1.5133552551269531\n",
      "Step: 5333, Loss: 0.9158360958099365, Accuracy: 1.0, Computation time: 1.1286070346832275\n",
      "Step: 5334, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 1.572021484375\n",
      "Step: 5335, Loss: 0.9158363342285156, Accuracy: 1.0, Computation time: 1.5035767555236816\n",
      "Step: 5336, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.439455270767212\n",
      "Step: 5337, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.5306496620178223\n",
      "Step: 5338, Loss: 0.9375180602073669, Accuracy: 0.9791666865348816, Computation time: 1.3268136978149414\n",
      "Step: 5339, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.0564172267913818\n",
      "Step: 5340, Loss: 0.9158390760421753, Accuracy: 1.0, Computation time: 1.4495198726654053\n",
      "Step: 5341, Loss: 0.9158377647399902, Accuracy: 1.0, Computation time: 1.1219334602355957\n",
      "Step: 5342, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 1.3299648761749268\n",
      "Step: 5343, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.1863398551940918\n",
      "Step: 5344, Loss: 0.9158350825309753, Accuracy: 1.0, Computation time: 1.2824623584747314\n",
      "Step: 5345, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.208115577697754\n",
      "Step: 5346, Loss: 0.9158327579498291, Accuracy: 1.0, Computation time: 1.0979318618774414\n",
      "Step: 5347, Loss: 0.9158319234848022, Accuracy: 1.0, Computation time: 1.338489294052124\n",
      "Step: 5348, Loss: 0.9374975562095642, Accuracy: 0.9791666865348816, Computation time: 1.0365102291107178\n",
      "Step: 5349, Loss: 0.9173265099525452, Accuracy: 1.0, Computation time: 1.3921916484832764\n",
      "Step: 5350, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 1.3108224868774414\n",
      "Step: 5351, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.0893080234527588\n",
      "Step: 5352, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.3073325157165527\n",
      "Step: 5353, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.492563247680664\n",
      "Step: 5354, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.4534807205200195\n",
      "Step: 5355, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 1.2264888286590576\n",
      "Step: 5356, Loss: 0.9158371686935425, Accuracy: 1.0, Computation time: 1.7144277095794678\n",
      "Step: 5357, Loss: 0.9375719428062439, Accuracy: 0.949999988079071, Computation time: 1.3930673599243164\n",
      "Step: 5358, Loss: 0.9158361554145813, Accuracy: 1.0, Computation time: 1.0932707786560059\n",
      "Step: 5359, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.3461153507232666\n",
      "Step: 5360, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 1.3003149032592773\n",
      "Step: 5361, Loss: 0.9158414006233215, Accuracy: 1.0, Computation time: 1.8613040447235107\n",
      "Step: 5362, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 1.5556995868682861\n",
      "Step: 5363, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.4511513710021973\n",
      "Step: 5364, Loss: 0.9158408045768738, Accuracy: 1.0, Computation time: 1.180462121963501\n",
      "Step: 5365, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.221872329711914\n",
      "Step: 5366, Loss: 0.9375155568122864, Accuracy: 0.96875, Computation time: 1.2127740383148193\n",
      "Step: 5367, Loss: 0.9375263452529907, Accuracy: 0.9807692766189575, Computation time: 1.3071646690368652\n",
      "Step: 5368, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 1.3822154998779297\n",
      "Step: 5369, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.4063336849212646\n",
      "Step: 5370, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 1.5115928649902344\n",
      "Step: 5371, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 1.0984408855438232\n",
      "Step: 5372, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.495142936706543\n",
      "Step: 5373, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.6150188446044922\n",
      "Step: 5374, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 1.539546012878418\n",
      "Step: 5375, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.5828053951263428\n",
      "Step: 5376, Loss: 0.9374712109565735, Accuracy: 0.9642857313156128, Computation time: 1.2716712951660156\n",
      "Step: 5377, Loss: 0.9158329963684082, Accuracy: 1.0, Computation time: 1.4207243919372559\n",
      "Step: 5378, Loss: 0.9158344864845276, Accuracy: 1.0, Computation time: 1.322730302810669\n",
      "Step: 5379, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.2477123737335205\n",
      "Step: 5380, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 1.4980907440185547\n",
      "Step: 5381, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.660323143005371\n",
      "Step: 5382, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.7072806358337402\n",
      "Step: 5383, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 1.68906569480896\n",
      "Step: 5384, Loss: 0.9374960064888, Accuracy: 0.9821428656578064, Computation time: 1.6791856288909912\n",
      "Step: 5385, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 1.9098315238952637\n",
      "Step: 5386, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.3629534244537354\n",
      "Step: 5387, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 1.9625656604766846\n",
      "Step: 5388, Loss: 0.9158312082290649, Accuracy: 1.0, Computation time: 1.2464067935943604\n",
      "Step: 5389, Loss: 0.9158371686935425, Accuracy: 1.0, Computation time: 1.2460415363311768\n",
      "Step: 5390, Loss: 0.915836751461029, Accuracy: 1.0, Computation time: 1.5364270210266113\n",
      "Step: 5391, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.504798173904419\n",
      "Step: 5392, Loss: 0.9158337116241455, Accuracy: 1.0, Computation time: 1.1868951320648193\n",
      "Step: 5393, Loss: 0.9158341884613037, Accuracy: 1.0, Computation time: 1.5060882568359375\n",
      "Step: 5394, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.4180736541748047\n",
      "Step: 5395, Loss: 0.9268883466720581, Accuracy: 0.9722222089767456, Computation time: 1.9893455505371094\n",
      "Step: 5396, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.0489716529846191\n",
      "Step: 5397, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.331496000289917\n",
      "Step: 5398, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.4975945949554443\n",
      "Step: 5399, Loss: 0.915890634059906, Accuracy: 1.0, Computation time: 1.2386820316314697\n",
      "Step: 5400, Loss: 0.9257543683052063, Accuracy: 0.9772727489471436, Computation time: 1.802210807800293\n",
      "Step: 5401, Loss: 0.9159286618232727, Accuracy: 1.0, Computation time: 1.2615175247192383\n",
      "Step: 5402, Loss: 0.9166026711463928, Accuracy: 1.0, Computation time: 1.55259370803833\n",
      "Step: 5403, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.4701354503631592\n",
      "Step: 5404, Loss: 0.9374969601631165, Accuracy: 0.9791666865348816, Computation time: 1.1419434547424316\n",
      "Step: 5405, Loss: 0.9160248041152954, Accuracy: 1.0, Computation time: 1.3015687465667725\n",
      "Step: 5406, Loss: 0.916027307510376, Accuracy: 1.0, Computation time: 1.2053554058074951\n",
      "Step: 5407, Loss: 0.9166890382766724, Accuracy: 1.0, Computation time: 1.479485273361206\n",
      "Step: 5408, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.2171568870544434\n",
      "Step: 5409, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.186053991317749\n",
      "Step: 5410, Loss: 0.9376391172409058, Accuracy: 0.9722222089767456, Computation time: 1.2628655433654785\n",
      "Step: 5411, Loss: 0.915939211845398, Accuracy: 1.0, Computation time: 1.2510430812835693\n",
      "Step: 5412, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 1.4777772426605225\n",
      "Step: 5413, Loss: 0.9381341338157654, Accuracy: 0.9821428656578064, Computation time: 1.6605186462402344\n",
      "Step: 5414, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 1.4630520343780518\n",
      "Step: 5415, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.350583791732788\n",
      "Step: 5416, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 1.9505233764648438\n",
      "Step: 5417, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.3502767086029053\n",
      "Step: 5418, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 1.2638792991638184\n",
      "Step: 5419, Loss: 0.9160041809082031, Accuracy: 1.0, Computation time: 1.4996395111083984\n",
      "########################\n",
      "Test loss: 1.0722689628601074, Test Accuracy_epoch39: 0.7654756903648376\n",
      "########################\n",
      "Step: 5420, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.3029634952545166\n",
      "Step: 5421, Loss: 0.9220106601715088, Accuracy: 1.0, Computation time: 1.3237667083740234\n",
      "Step: 5422, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.2965984344482422\n",
      "Step: 5423, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 1.4542179107666016\n",
      "Step: 5424, Loss: 0.9161727428436279, Accuracy: 1.0, Computation time: 1.332805871963501\n",
      "Step: 5425, Loss: 0.9159534573554993, Accuracy: 1.0, Computation time: 1.5295720100402832\n",
      "Step: 5426, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.1893584728240967\n",
      "Step: 5427, Loss: 0.9163473844528198, Accuracy: 1.0, Computation time: 1.7048389911651611\n",
      "Step: 5428, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.4039244651794434\n",
      "Step: 5429, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.674710988998413\n",
      "Step: 5430, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.2315664291381836\n",
      "Step: 5431, Loss: 0.916887640953064, Accuracy: 1.0, Computation time: 1.8860511779785156\n",
      "Step: 5432, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.3271796703338623\n",
      "Step: 5433, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.2971768379211426\n",
      "Step: 5434, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.1711368560791016\n",
      "Step: 5435, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.3465218544006348\n",
      "Step: 5436, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 1.5147104263305664\n",
      "Step: 5437, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.14277982711792\n",
      "Step: 5438, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.6758639812469482\n",
      "Step: 5439, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.4004294872283936\n",
      "Step: 5440, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.3127093315124512\n",
      "Step: 5441, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.3208458423614502\n",
      "Step: 5442, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.339334487915039\n",
      "Step: 5443, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.7624549865722656\n",
      "Step: 5444, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.5063979625701904\n",
      "Step: 5445, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.3472216129302979\n",
      "Step: 5446, Loss: 0.9158421158790588, Accuracy: 1.0, Computation time: 1.4549875259399414\n",
      "Step: 5447, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 1.6556289196014404\n",
      "Step: 5448, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 1.425800085067749\n",
      "Step: 5449, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 1.4781537055969238\n",
      "Step: 5450, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.4184491634368896\n",
      "Step: 5451, Loss: 0.9158395528793335, Accuracy: 1.0, Computation time: 1.4470722675323486\n",
      "Step: 5452, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.3812298774719238\n",
      "Step: 5453, Loss: 0.9375783801078796, Accuracy: 0.949999988079071, Computation time: 1.216437816619873\n",
      "Step: 5454, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.546447515487671\n",
      "Step: 5455, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.3463914394378662\n",
      "Step: 5456, Loss: 0.9375250935554504, Accuracy: 0.9807692766189575, Computation time: 2.0073940753936768\n",
      "Step: 5457, Loss: 0.9376788139343262, Accuracy: 0.9750000238418579, Computation time: 1.4634933471679688\n",
      "Step: 5458, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.5071163177490234\n",
      "Step: 5459, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.5452542304992676\n",
      "Step: 5460, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.441105604171753\n",
      "Step: 5461, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.3746559619903564\n",
      "Step: 5462, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.3112130165100098\n",
      "Step: 5463, Loss: 0.9593903422355652, Accuracy: 0.9571428894996643, Computation time: 1.4182250499725342\n",
      "Step: 5464, Loss: 0.937628448009491, Accuracy: 0.96875, Computation time: 1.8387377262115479\n",
      "Step: 5465, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.3908746242523193\n",
      "Step: 5466, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 1.5605642795562744\n",
      "Step: 5467, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 1.302032470703125\n",
      "Step: 5468, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.8409302234649658\n",
      "Step: 5469, Loss: 0.9161866307258606, Accuracy: 1.0, Computation time: 1.227910041809082\n",
      "Step: 5470, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 1.8394107818603516\n",
      "Step: 5471, Loss: 0.9165815114974976, Accuracy: 1.0, Computation time: 1.5210833549499512\n",
      "Step: 5472, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 1.5056376457214355\n",
      "Step: 5473, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 1.5071041584014893\n",
      "Step: 5474, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 1.4217960834503174\n",
      "Step: 5475, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.643275260925293\n",
      "Step: 5476, Loss: 0.9161047339439392, Accuracy: 1.0, Computation time: 1.5748608112335205\n",
      "Step: 5477, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.3861262798309326\n",
      "Step: 5478, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.4392249584197998\n",
      "Step: 5479, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.1925904750823975\n",
      "Step: 5480, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.6752245426177979\n",
      "Step: 5481, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.124753713607788\n",
      "Step: 5482, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.277505874633789\n",
      "Step: 5483, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.555281400680542\n",
      "Step: 5484, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 1.3027775287628174\n",
      "Step: 5485, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.4230830669403076\n",
      "Step: 5486, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.5198488235473633\n",
      "Step: 5487, Loss: 0.9376224875450134, Accuracy: 0.9722222089767456, Computation time: 1.261397361755371\n",
      "Step: 5488, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 1.4642648696899414\n",
      "Step: 5489, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.3138859272003174\n",
      "Step: 5490, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.4805333614349365\n",
      "Step: 5491, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 1.6155054569244385\n",
      "Step: 5492, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.6079342365264893\n",
      "Step: 5493, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.3581454753875732\n",
      "Step: 5494, Loss: 0.9159278869628906, Accuracy: 1.0, Computation time: 1.6369142532348633\n",
      "Step: 5495, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 1.239281415939331\n",
      "Step: 5496, Loss: 0.9161092638969421, Accuracy: 1.0, Computation time: 1.8539683818817139\n",
      "Step: 5497, Loss: 0.9158420562744141, Accuracy: 1.0, Computation time: 1.322632074356079\n",
      "Step: 5498, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.7948086261749268\n",
      "Step: 5499, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.5303990840911865\n",
      "Step: 5500, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 1.5868573188781738\n",
      "Step: 5501, Loss: 0.9158423542976379, Accuracy: 1.0, Computation time: 1.739490270614624\n",
      "Step: 5502, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.6288444995880127\n",
      "Step: 5503, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 1.1932010650634766\n",
      "Step: 5504, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.8340895175933838\n",
      "Step: 5505, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.377032995223999\n",
      "Step: 5506, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.6891050338745117\n",
      "Step: 5507, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.4018404483795166\n",
      "Step: 5508, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.505382776260376\n",
      "Step: 5509, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 1.2581818103790283\n",
      "Step: 5510, Loss: 0.937586784362793, Accuracy: 0.9772727489471436, Computation time: 1.4057211875915527\n",
      "Step: 5511, Loss: 0.9377920031547546, Accuracy: 0.96875, Computation time: 1.4040210247039795\n",
      "Step: 5512, Loss: 0.915834367275238, Accuracy: 1.0, Computation time: 1.5338668823242188\n",
      "Step: 5513, Loss: 0.9159464836120605, Accuracy: 1.0, Computation time: 1.6098837852478027\n",
      "Step: 5514, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.678415298461914\n",
      "Step: 5515, Loss: 0.9159322381019592, Accuracy: 1.0, Computation time: 1.1822643280029297\n",
      "Step: 5516, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.5714945793151855\n",
      "Step: 5517, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.4022834300994873\n",
      "Step: 5518, Loss: 0.9158424139022827, Accuracy: 1.0, Computation time: 1.2378699779510498\n",
      "Step: 5519, Loss: 0.9163703322410583, Accuracy: 1.0, Computation time: 1.4782309532165527\n",
      "Step: 5520, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 1.3954977989196777\n",
      "Step: 5521, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 1.3453381061553955\n",
      "Step: 5522, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.300724983215332\n",
      "Step: 5523, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.4415879249572754\n",
      "Step: 5524, Loss: 0.9158398509025574, Accuracy: 1.0, Computation time: 1.280921459197998\n",
      "Step: 5525, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.3656284809112549\n",
      "Step: 5526, Loss: 0.9158397912979126, Accuracy: 1.0, Computation time: 1.4624876976013184\n",
      "Step: 5527, Loss: 0.9374504685401917, Accuracy: 0.9772727489471436, Computation time: 1.6358561515808105\n",
      "Step: 5528, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.3543672561645508\n",
      "Step: 5529, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.3878240585327148\n",
      "Step: 5530, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.235335111618042\n",
      "Step: 5531, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.382605791091919\n",
      "Step: 5532, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 1.427687168121338\n",
      "Step: 5533, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.2688863277435303\n",
      "Step: 5534, Loss: 0.9158369898796082, Accuracy: 1.0, Computation time: 1.2930593490600586\n",
      "Step: 5535, Loss: 0.9593371152877808, Accuracy: 0.9375, Computation time: 1.5745906829833984\n",
      "Step: 5536, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 1.2625792026519775\n",
      "Step: 5537, Loss: 0.9374955296516418, Accuracy: 0.949999988079071, Computation time: 1.4930088520050049\n",
      "Step: 5538, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.4219448566436768\n",
      "Step: 5539, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.2076334953308105\n",
      "Step: 5540, Loss: 0.9158400893211365, Accuracy: 1.0, Computation time: 1.74070405960083\n",
      "Step: 5541, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.6951265335083008\n",
      "Step: 5542, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.495736837387085\n",
      "Step: 5543, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 1.4070501327514648\n",
      "Step: 5544, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 1.2736279964447021\n",
      "Step: 5545, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.3259062767028809\n",
      "Step: 5546, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.6035394668579102\n",
      "Step: 5547, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.6035950183868408\n",
      "Step: 5548, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.5236437320709229\n",
      "Step: 5549, Loss: 0.9158393740653992, Accuracy: 1.0, Computation time: 1.2549331188201904\n",
      "Step: 5550, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 1.344451904296875\n",
      "Step: 5551, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.2878291606903076\n",
      "Step: 5552, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 1.177267074584961\n",
      "Step: 5553, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.460209846496582\n",
      "Step: 5554, Loss: 0.9158363938331604, Accuracy: 1.0, Computation time: 1.5373599529266357\n",
      "Step: 5555, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 1.318131685256958\n",
      "Step: 5556, Loss: 0.9158349633216858, Accuracy: 1.0, Computation time: 1.8738071918487549\n",
      "Step: 5557, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.192131757736206\n",
      "Step: 5558, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 2.082146167755127\n",
      "########################\n",
      "Test loss: 1.0711970329284668, Test Accuracy_epoch40: 0.7665502429008484\n",
      "########################\n",
      "Step: 5559, Loss: 0.915833592414856, Accuracy: 1.0, Computation time: 1.2364850044250488\n",
      "Step: 5560, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.2898876667022705\n",
      "Step: 5561, Loss: 0.9300780892372131, Accuracy: 0.9583333730697632, Computation time: 1.5464599132537842\n",
      "Step: 5562, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.319305181503296\n",
      "Step: 5563, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 1.4560437202453613\n",
      "Step: 5564, Loss: 0.9159610867500305, Accuracy: 1.0, Computation time: 1.2658805847167969\n",
      "Step: 5565, Loss: 0.9376314878463745, Accuracy: 0.9772727489471436, Computation time: 1.902078628540039\n",
      "Step: 5566, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 1.4883027076721191\n",
      "Step: 5567, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.0507938861846924\n",
      "Step: 5568, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 1.5733146667480469\n",
      "Step: 5569, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.2447960376739502\n",
      "Step: 5570, Loss: 0.9159282445907593, Accuracy: 1.0, Computation time: 1.3702812194824219\n",
      "Step: 5571, Loss: 0.9374721050262451, Accuracy: 0.9807692766189575, Computation time: 1.3581223487854004\n",
      "Step: 5572, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.2477972507476807\n",
      "Step: 5573, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 1.3294167518615723\n",
      "Step: 5574, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 1.4784560203552246\n",
      "Step: 5575, Loss: 0.9159438014030457, Accuracy: 1.0, Computation time: 1.4665024280548096\n",
      "Step: 5576, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.1237432956695557\n",
      "Step: 5577, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.4467980861663818\n",
      "Step: 5578, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.4455323219299316\n",
      "Step: 5579, Loss: 0.9375225305557251, Accuracy: 0.9750000238418579, Computation time: 1.1872661113739014\n",
      "Step: 5580, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.5023193359375\n",
      "Step: 5581, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.5804009437561035\n",
      "Step: 5582, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 1.740527868270874\n",
      "Step: 5583, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.3880181312561035\n",
      "Step: 5584, Loss: 0.91590815782547, Accuracy: 1.0, Computation time: 1.296565294265747\n",
      "Step: 5585, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 1.2580883502960205\n",
      "Step: 5586, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.1788265705108643\n",
      "Step: 5587, Loss: 0.9222921133041382, Accuracy: 1.0, Computation time: 1.6893811225891113\n",
      "Step: 5588, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.097794532775879\n",
      "Step: 5589, Loss: 0.9374837875366211, Accuracy: 0.9821428656578064, Computation time: 1.3600664138793945\n",
      "Step: 5590, Loss: 0.9160086512565613, Accuracy: 1.0, Computation time: 1.5687439441680908\n",
      "Step: 5591, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.2040824890136719\n",
      "Step: 5592, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 1.5554182529449463\n",
      "Step: 5593, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.3698368072509766\n",
      "Step: 5594, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.3492159843444824\n",
      "Step: 5595, Loss: 0.9183940887451172, Accuracy: 1.0, Computation time: 1.4776837825775146\n",
      "Step: 5596, Loss: 0.9377307891845703, Accuracy: 0.96875, Computation time: 1.431079626083374\n",
      "Step: 5597, Loss: 0.9160458445549011, Accuracy: 1.0, Computation time: 2.106619119644165\n",
      "Step: 5598, Loss: 0.9161072969436646, Accuracy: 1.0, Computation time: 1.1378462314605713\n",
      "Step: 5599, Loss: 0.9160041809082031, Accuracy: 1.0, Computation time: 1.6096630096435547\n",
      "Step: 5600, Loss: 0.915961742401123, Accuracy: 1.0, Computation time: 1.4504141807556152\n",
      "Step: 5601, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.2591779232025146\n",
      "Step: 5602, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 1.3258965015411377\n",
      "Step: 5603, Loss: 0.9160089492797852, Accuracy: 1.0, Computation time: 1.2111196517944336\n",
      "Step: 5604, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 1.2097127437591553\n",
      "Step: 5605, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.4946625232696533\n",
      "Step: 5606, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.4732589721679688\n",
      "Step: 5607, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.4622952938079834\n",
      "Step: 5608, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 1.2157683372497559\n",
      "Step: 5609, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.1148204803466797\n",
      "Step: 5610, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 1.7245357036590576\n",
      "Step: 5611, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.5152919292449951\n",
      "Step: 5612, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.6628899574279785\n",
      "Step: 5613, Loss: 0.9375721216201782, Accuracy: 0.9772727489471436, Computation time: 1.4780657291412354\n",
      "Step: 5614, Loss: 0.9163965582847595, Accuracy: 1.0, Computation time: 1.3136606216430664\n",
      "Step: 5615, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 1.2827563285827637\n",
      "Step: 5616, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.5429153442382812\n",
      "Step: 5617, Loss: 0.9375435709953308, Accuracy: 0.9807692766189575, Computation time: 1.381528377532959\n",
      "Step: 5618, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 1.278007984161377\n",
      "Step: 5619, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.3593599796295166\n",
      "Step: 5620, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.7779088020324707\n",
      "Step: 5621, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 1.6872284412384033\n",
      "Step: 5622, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.3852858543395996\n",
      "Step: 5623, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.6237461566925049\n",
      "Step: 5624, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.2834949493408203\n",
      "Step: 5625, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.1958653926849365\n",
      "Step: 5626, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.2992424964904785\n",
      "Step: 5627, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.3645415306091309\n",
      "Step: 5628, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 1.277897596359253\n",
      "Step: 5629, Loss: 0.9158353805541992, Accuracy: 1.0, Computation time: 1.5779800415039062\n",
      "Step: 5630, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.259467601776123\n",
      "Step: 5631, Loss: 0.9161935448646545, Accuracy: 1.0, Computation time: 1.6527886390686035\n",
      "Step: 5632, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.173668384552002\n",
      "Step: 5633, Loss: 0.915836751461029, Accuracy: 1.0, Computation time: 1.5641770362854004\n",
      "Step: 5634, Loss: 0.9163017868995667, Accuracy: 1.0, Computation time: 1.6516497135162354\n",
      "Step: 5635, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.4065697193145752\n",
      "Step: 5636, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 1.2061049938201904\n",
      "Step: 5637, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 1.3326232433319092\n",
      "Step: 5638, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.6417720317840576\n",
      "Step: 5639, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.4323489665985107\n",
      "Step: 5640, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.4330787658691406\n",
      "Step: 5641, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.3128383159637451\n",
      "Step: 5642, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.5265495777130127\n",
      "Step: 5643, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.457902193069458\n",
      "Step: 5644, Loss: 0.9214057326316833, Accuracy: 1.0, Computation time: 1.351428508758545\n",
      "Step: 5645, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 1.3712694644927979\n",
      "Step: 5646, Loss: 0.915922999382019, Accuracy: 1.0, Computation time: 1.333573579788208\n",
      "Step: 5647, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 1.407762050628662\n",
      "Step: 5648, Loss: 0.9159569144248962, Accuracy: 1.0, Computation time: 1.623037338256836\n",
      "Step: 5649, Loss: 0.9160416126251221, Accuracy: 1.0, Computation time: 1.4058363437652588\n",
      "Step: 5650, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.4298858642578125\n",
      "Step: 5651, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.3954534530639648\n",
      "Step: 5652, Loss: 0.9160303473472595, Accuracy: 1.0, Computation time: 2.3293464183807373\n",
      "Step: 5653, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.7994797229766846\n",
      "Step: 5654, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 2.006932020187378\n",
      "Step: 5655, Loss: 0.9159339666366577, Accuracy: 1.0, Computation time: 1.5691368579864502\n",
      "Step: 5656, Loss: 0.9159477949142456, Accuracy: 1.0, Computation time: 1.480752944946289\n",
      "Step: 5657, Loss: 0.9158961176872253, Accuracy: 1.0, Computation time: 1.4429607391357422\n",
      "Step: 5658, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.3859617710113525\n",
      "Step: 5659, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.5231890678405762\n",
      "Step: 5660, Loss: 0.9375773668289185, Accuracy: 0.9807692766189575, Computation time: 1.5780360698699951\n",
      "Step: 5661, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.4459974765777588\n",
      "Step: 5662, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 1.2882304191589355\n",
      "Step: 5663, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.4421122074127197\n",
      "Step: 5664, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 2.224182605743408\n",
      "Step: 5665, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.6333062648773193\n",
      "Step: 5666, Loss: 0.9161872863769531, Accuracy: 1.0, Computation time: 1.6210618019104004\n",
      "Step: 5667, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.4538414478302002\n",
      "Step: 5668, Loss: 0.9376458525657654, Accuracy: 0.9750000238418579, Computation time: 1.278270959854126\n",
      "Step: 5669, Loss: 0.9375628232955933, Accuracy: 0.9722222089767456, Computation time: 1.4147999286651611\n",
      "Step: 5670, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.8222174644470215\n",
      "Step: 5671, Loss: 0.9158674478530884, Accuracy: 1.0, Computation time: 1.534508228302002\n",
      "Step: 5672, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 1.764568567276001\n",
      "Step: 5673, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.5151617527008057\n",
      "Step: 5674, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.5522022247314453\n",
      "Step: 5675, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.64955735206604\n",
      "Step: 5676, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.851252794265747\n",
      "Step: 5677, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.6378192901611328\n",
      "Step: 5678, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.491546869277954\n",
      "Step: 5679, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.6798114776611328\n",
      "Step: 5680, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.4286375045776367\n",
      "Step: 5681, Loss: 0.9158350229263306, Accuracy: 1.0, Computation time: 1.3698389530181885\n",
      "Step: 5682, Loss: 0.9158366322517395, Accuracy: 1.0, Computation time: 1.8117961883544922\n",
      "Step: 5683, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 1.5406458377838135\n",
      "Step: 5684, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 1.4327592849731445\n",
      "Step: 5685, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.2729756832122803\n",
      "Step: 5686, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.7104263305664062\n",
      "Step: 5687, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 1.31919264793396\n",
      "Step: 5688, Loss: 0.9158352017402649, Accuracy: 1.0, Computation time: 1.4592084884643555\n",
      "Step: 5689, Loss: 0.9158360958099365, Accuracy: 1.0, Computation time: 1.4150116443634033\n",
      "Step: 5690, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 2.002842903137207\n",
      "Step: 5691, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.5430607795715332\n",
      "Step: 5692, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.735482931137085\n",
      "Step: 5693, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.5672545433044434\n",
      "Step: 5694, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.4406769275665283\n",
      "Step: 5695, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.6878688335418701\n",
      "Step: 5696, Loss: 0.9158331155776978, Accuracy: 1.0, Computation time: 1.3775336742401123\n",
      "Step: 5697, Loss: 0.9158311486244202, Accuracy: 1.0, Computation time: 1.5665090084075928\n",
      "########################\n",
      "Test loss: 1.0719128847122192, Test Accuracy_epoch41: 0.7633415460586548\n",
      "########################\n",
      "Step: 5698, Loss: 0.9158346056938171, Accuracy: 1.0, Computation time: 1.5593879222869873\n",
      "Step: 5699, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 1.7014338970184326\n",
      "Step: 5700, Loss: 0.9158315658569336, Accuracy: 1.0, Computation time: 1.4205315113067627\n",
      "Step: 5701, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.985827922821045\n",
      "Step: 5702, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.322195053100586\n",
      "Step: 5703, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.3853943347930908\n",
      "Step: 5704, Loss: 0.9158320426940918, Accuracy: 1.0, Computation time: 1.5134267807006836\n",
      "Step: 5705, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.2413709163665771\n",
      "Step: 5706, Loss: 0.915831983089447, Accuracy: 1.0, Computation time: 1.4888169765472412\n",
      "Step: 5707, Loss: 0.9158347249031067, Accuracy: 1.0, Computation time: 1.2087903022766113\n",
      "Step: 5708, Loss: 0.9158341288566589, Accuracy: 1.0, Computation time: 1.4149868488311768\n",
      "Step: 5709, Loss: 0.9375187158584595, Accuracy: 0.9791666865348816, Computation time: 1.4898457527160645\n",
      "Step: 5710, Loss: 0.9375669360160828, Accuracy: 0.949999988079071, Computation time: 1.470907211303711\n",
      "Step: 5711, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.5108773708343506\n",
      "Step: 5712, Loss: 0.915836751461029, Accuracy: 1.0, Computation time: 1.5121042728424072\n",
      "Step: 5713, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.1756277084350586\n",
      "Step: 5714, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.2356159687042236\n",
      "Step: 5715, Loss: 0.9158349633216858, Accuracy: 1.0, Computation time: 1.346327543258667\n",
      "Step: 5716, Loss: 0.9169464707374573, Accuracy: 1.0, Computation time: 1.4536716938018799\n",
      "Step: 5717, Loss: 0.9592875242233276, Accuracy: 0.9365079402923584, Computation time: 1.7694780826568604\n",
      "Step: 5718, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.4526050090789795\n",
      "Step: 5719, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 1.331157922744751\n",
      "Step: 5720, Loss: 0.9376445412635803, Accuracy: 0.9642857313156128, Computation time: 1.4330220222473145\n",
      "Step: 5721, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.526838779449463\n",
      "Step: 5722, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.3752379417419434\n",
      "Step: 5723, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.7397136688232422\n",
      "Step: 5724, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.3292815685272217\n",
      "Step: 5725, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.7022554874420166\n",
      "Step: 5726, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.4075238704681396\n",
      "Step: 5727, Loss: 0.937559962272644, Accuracy: 0.9791666865348816, Computation time: 1.768580675125122\n",
      "Step: 5728, Loss: 0.91597980260849, Accuracy: 1.0, Computation time: 1.2334918975830078\n",
      "Step: 5729, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.4950497150421143\n",
      "Step: 5730, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 1.9664974212646484\n",
      "Step: 5731, Loss: 0.9375604391098022, Accuracy: 0.9821428656578064, Computation time: 1.2279126644134521\n",
      "Step: 5732, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.2905552387237549\n",
      "Step: 5733, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.4018421173095703\n",
      "Step: 5734, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.4608032703399658\n",
      "Step: 5735, Loss: 0.9375181794166565, Accuracy: 0.9583333730697632, Computation time: 1.4298124313354492\n",
      "Step: 5736, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 1.5557658672332764\n",
      "Step: 5737, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.5439562797546387\n",
      "Step: 5738, Loss: 0.9375753402709961, Accuracy: 0.9750000238418579, Computation time: 1.4770362377166748\n",
      "Step: 5739, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.5276029109954834\n",
      "Step: 5740, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.4464006423950195\n",
      "Step: 5741, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.6841325759887695\n",
      "Step: 5742, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.6380212306976318\n",
      "Step: 5743, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 1.2256200313568115\n",
      "Step: 5744, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.8221874237060547\n",
      "Step: 5745, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.603342056274414\n",
      "Step: 5746, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.3544960021972656\n",
      "Step: 5747, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.4631273746490479\n",
      "Step: 5748, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.442542314529419\n",
      "Step: 5749, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 1.301882028579712\n",
      "Step: 5750, Loss: 0.915841281414032, Accuracy: 1.0, Computation time: 1.271498441696167\n",
      "Step: 5751, Loss: 0.9158400297164917, Accuracy: 1.0, Computation time: 1.4349873065948486\n",
      "Step: 5752, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 1.3575856685638428\n",
      "Step: 5753, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.538560152053833\n",
      "Step: 5754, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.326439619064331\n",
      "Step: 5755, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.2736191749572754\n",
      "Step: 5756, Loss: 0.9158353805541992, Accuracy: 1.0, Computation time: 1.4327681064605713\n",
      "Step: 5757, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 1.5807723999023438\n",
      "Step: 5758, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 1.4949607849121094\n",
      "Step: 5759, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 1.4258434772491455\n",
      "Step: 5760, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.7638003826141357\n",
      "Step: 5761, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 1.2552876472473145\n",
      "Step: 5762, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.3222908973693848\n",
      "Step: 5763, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.352522850036621\n",
      "Step: 5764, Loss: 0.915831983089447, Accuracy: 1.0, Computation time: 1.3275861740112305\n",
      "Step: 5765, Loss: 0.9158326983451843, Accuracy: 1.0, Computation time: 1.1815385818481445\n",
      "Step: 5766, Loss: 0.9158371686935425, Accuracy: 1.0, Computation time: 1.4200599193572998\n",
      "Step: 5767, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.366670846939087\n",
      "Step: 5768, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.873755693435669\n",
      "Step: 5769, Loss: 0.9158352017402649, Accuracy: 1.0, Computation time: 1.3506462574005127\n",
      "Step: 5770, Loss: 0.9158342480659485, Accuracy: 1.0, Computation time: 1.2326982021331787\n",
      "Step: 5771, Loss: 0.9374363422393799, Accuracy: 0.9642857313156128, Computation time: 1.478109359741211\n",
      "Step: 5772, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.3718829154968262\n",
      "Step: 5773, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.391530990600586\n",
      "Step: 5774, Loss: 0.9375805258750916, Accuracy: 0.9583333730697632, Computation time: 1.391676425933838\n",
      "Step: 5775, Loss: 0.9158371090888977, Accuracy: 1.0, Computation time: 1.112311840057373\n",
      "Step: 5776, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.5961766242980957\n",
      "Step: 5777, Loss: 0.9158341288566589, Accuracy: 1.0, Computation time: 1.4007937908172607\n",
      "Step: 5778, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 1.522339105606079\n",
      "Step: 5779, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 1.2935538291931152\n",
      "Step: 5780, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 1.6656429767608643\n",
      "Step: 5781, Loss: 0.9158336520195007, Accuracy: 1.0, Computation time: 1.329512119293213\n",
      "Step: 5782, Loss: 0.9158368706703186, Accuracy: 1.0, Computation time: 1.4181787967681885\n",
      "Step: 5783, Loss: 0.9158324599266052, Accuracy: 1.0, Computation time: 1.3642089366912842\n",
      "Step: 5784, Loss: 0.9158352017402649, Accuracy: 1.0, Computation time: 1.0891857147216797\n",
      "Step: 5785, Loss: 0.9158344268798828, Accuracy: 1.0, Computation time: 1.2821576595306396\n",
      "Step: 5786, Loss: 0.9158316254615784, Accuracy: 1.0, Computation time: 1.282346487045288\n",
      "Step: 5787, Loss: 0.9158322215080261, Accuracy: 1.0, Computation time: 1.625535488128662\n",
      "Step: 5788, Loss: 0.9158341884613037, Accuracy: 1.0, Computation time: 1.3928377628326416\n",
      "Step: 5789, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 1.183121919631958\n",
      "Step: 5790, Loss: 0.9374811053276062, Accuracy: 0.9750000238418579, Computation time: 1.3536622524261475\n",
      "Step: 5791, Loss: 0.9159516096115112, Accuracy: 1.0, Computation time: 1.4789915084838867\n",
      "Step: 5792, Loss: 0.939274787902832, Accuracy: 0.9821428656578064, Computation time: 1.5704069137573242\n",
      "Step: 5793, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.4208533763885498\n",
      "Step: 5794, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.437027931213379\n",
      "Step: 5795, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.636436939239502\n",
      "Step: 5796, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.745387077331543\n",
      "Step: 5797, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.1886684894561768\n",
      "Step: 5798, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.9201886653900146\n",
      "Step: 5799, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 1.1640455722808838\n",
      "Step: 5800, Loss: 0.9160351157188416, Accuracy: 1.0, Computation time: 1.6135241985321045\n",
      "Step: 5801, Loss: 0.9376140236854553, Accuracy: 0.9791666865348816, Computation time: 1.3939669132232666\n",
      "Step: 5802, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.4131150245666504\n",
      "Step: 5803, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.588693380355835\n",
      "Step: 5804, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.72819185256958\n",
      "Step: 5805, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.5804510116577148\n",
      "Step: 5806, Loss: 0.9374786615371704, Accuracy: 0.9833333492279053, Computation time: 1.5054564476013184\n",
      "Step: 5807, Loss: 0.915905773639679, Accuracy: nan, Computation time: 2.0639541149139404\n",
      "Step: 5808, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.3488361835479736\n",
      "Step: 5809, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.6719224452972412\n",
      "Step: 5810, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.6014015674591064\n",
      "Step: 5811, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.5368189811706543\n",
      "Step: 5812, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.4889564514160156\n",
      "Step: 5813, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 1.560225248336792\n",
      "Step: 5814, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.6256117820739746\n",
      "Step: 5815, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 1.806473970413208\n",
      "Step: 5816, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.4779605865478516\n",
      "Step: 5817, Loss: 0.9376452565193176, Accuracy: 0.9772727489471436, Computation time: 2.070521116256714\n",
      "Step: 5818, Loss: 0.9158499836921692, Accuracy: 1.0, Computation time: 1.829101324081421\n",
      "Step: 5819, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 1.9233882427215576\n",
      "Step: 5820, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.607001781463623\n",
      "Step: 5821, Loss: 0.9376621842384338, Accuracy: 0.9750000238418579, Computation time: 1.8414173126220703\n",
      "Step: 5822, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.5206732749938965\n",
      "Step: 5823, Loss: 0.9158757925033569, Accuracy: nan, Computation time: 1.8576006889343262\n",
      "Step: 5824, Loss: 0.9375437498092651, Accuracy: 0.96875, Computation time: 1.7155804634094238\n",
      "Step: 5825, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.9686510562896729\n",
      "Step: 5826, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.455124855041504\n",
      "Step: 5827, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.7659285068511963\n",
      "Step: 5828, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.7817485332489014\n",
      "Step: 5829, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 1.6817843914031982\n",
      "Step: 5830, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 1.3757472038269043\n",
      "Step: 5831, Loss: 0.9375534057617188, Accuracy: 0.9772727489471436, Computation time: 1.7253999710083008\n",
      "Step: 5832, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.5099592208862305\n",
      "Step: 5833, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.9492173194885254\n",
      "Step: 5834, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.5947260856628418\n",
      "Step: 5835, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.5833616256713867\n",
      "Step: 5836, Loss: 0.9374866485595703, Accuracy: 0.9807692766189575, Computation time: 1.5580193996429443\n",
      "########################\n",
      "Test loss: 1.0703922510147095, Test Accuracy_epoch42: 0.7660588026046753\n",
      "########################\n",
      "Step: 5837, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.821171522140503\n",
      "Step: 5838, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.5209417343139648\n",
      "Step: 5839, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.6618411540985107\n",
      "Step: 5840, Loss: 0.9375954866409302, Accuracy: 0.9750000238418579, Computation time: 2.2726261615753174\n",
      "Step: 5841, Loss: 0.9158337712287903, Accuracy: 1.0, Computation time: 1.5138390064239502\n",
      "Step: 5842, Loss: 0.9376246929168701, Accuracy: 0.9772727489471436, Computation time: 1.6255300045013428\n",
      "Step: 5843, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.5648927688598633\n",
      "Step: 5844, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.7672135829925537\n",
      "Step: 5845, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.5522422790527344\n",
      "Step: 5846, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 1.3902535438537598\n",
      "Step: 5847, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 1.5122058391571045\n",
      "Step: 5848, Loss: 0.9158361554145813, Accuracy: 1.0, Computation time: 1.667633295059204\n",
      "Step: 5849, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 1.841242790222168\n",
      "Step: 5850, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 1.5532190799713135\n",
      "Step: 5851, Loss: 0.9158377647399902, Accuracy: 1.0, Computation time: 1.7327227592468262\n",
      "Step: 5852, Loss: 0.9158370494842529, Accuracy: 1.0, Computation time: 1.3809864521026611\n",
      "Step: 5853, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.8623745441436768\n",
      "Step: 5854, Loss: 0.9158371090888977, Accuracy: 1.0, Computation time: 1.76267409324646\n",
      "Step: 5855, Loss: 0.9158359169960022, Accuracy: 1.0, Computation time: 1.3276855945587158\n",
      "Step: 5856, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.4159071445465088\n",
      "Step: 5857, Loss: 0.9158326387405396, Accuracy: 1.0, Computation time: 1.861015796661377\n",
      "Step: 5858, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.2577600479125977\n",
      "Step: 5859, Loss: 0.9158329367637634, Accuracy: 1.0, Computation time: 1.4980947971343994\n",
      "Step: 5860, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.5765864849090576\n",
      "Step: 5861, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 1.714064359664917\n",
      "Step: 5862, Loss: 0.9166625142097473, Accuracy: nan, Computation time: 2.1073074340820312\n",
      "Step: 5863, Loss: 0.915905237197876, Accuracy: 1.0, Computation time: 1.8046579360961914\n",
      "Step: 5864, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.5921094417572021\n",
      "Step: 5865, Loss: 0.9159609079360962, Accuracy: 1.0, Computation time: 1.3475406169891357\n",
      "Step: 5866, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.1038484573364258\n",
      "Step: 5867, Loss: 0.9375197291374207, Accuracy: 0.96875, Computation time: 1.2696657180786133\n",
      "Step: 5868, Loss: 0.9158332347869873, Accuracy: 1.0, Computation time: 1.311671495437622\n",
      "Step: 5869, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.8150408267974854\n",
      "Step: 5870, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.6374015808105469\n",
      "Step: 5871, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.6317589282989502\n",
      "Step: 5872, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.3404476642608643\n",
      "Step: 5873, Loss: 0.916126549243927, Accuracy: 1.0, Computation time: 1.885051965713501\n",
      "Step: 5874, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.597748041152954\n",
      "Step: 5875, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 1.4164953231811523\n",
      "Step: 5876, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.2455296516418457\n",
      "Step: 5877, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.331026554107666\n",
      "Step: 5878, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 1.6240973472595215\n",
      "Step: 5879, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.807037115097046\n",
      "Step: 5880, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.3287193775177002\n",
      "Step: 5881, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.1999454498291016\n",
      "Step: 5882, Loss: 0.93759685754776, Accuracy: 0.96875, Computation time: 1.8174240589141846\n",
      "Step: 5883, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.1042945384979248\n",
      "Step: 5884, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 1.5672166347503662\n",
      "Step: 5885, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.7586164474487305\n",
      "Step: 5886, Loss: 0.9247949719429016, Accuracy: 1.0, Computation time: 1.4137022495269775\n",
      "Step: 5887, Loss: 0.930874764919281, Accuracy: 0.9807692766189575, Computation time: 1.643200397491455\n",
      "Step: 5888, Loss: 0.9163957238197327, Accuracy: 1.0, Computation time: 1.612551212310791\n",
      "Step: 5889, Loss: 0.9163795709609985, Accuracy: 1.0, Computation time: 1.250436782836914\n",
      "Step: 5890, Loss: 0.9171270132064819, Accuracy: 1.0, Computation time: 1.3383989334106445\n",
      "Step: 5891, Loss: 0.9163507223129272, Accuracy: 1.0, Computation time: 1.11958646774292\n",
      "Step: 5892, Loss: 0.9160155057907104, Accuracy: 1.0, Computation time: 1.566619873046875\n",
      "Step: 5893, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 1.1645786762237549\n",
      "Step: 5894, Loss: 0.9160482883453369, Accuracy: 1.0, Computation time: 1.3159441947937012\n",
      "Step: 5895, Loss: 0.9161916375160217, Accuracy: 1.0, Computation time: 1.7784080505371094\n",
      "Step: 5896, Loss: 0.9404688477516174, Accuracy: 0.9583333730697632, Computation time: 1.4737129211425781\n",
      "Step: 5897, Loss: 0.9167417883872986, Accuracy: 1.0, Computation time: 1.453432321548462\n",
      "Step: 5898, Loss: 0.937663733959198, Accuracy: 0.875, Computation time: 1.188072919845581\n",
      "Step: 5899, Loss: 0.9352573752403259, Accuracy: 0.9583333730697632, Computation time: 1.714726448059082\n",
      "Step: 5900, Loss: 0.916536808013916, Accuracy: 1.0, Computation time: 1.840080738067627\n",
      "Step: 5901, Loss: 0.9377040863037109, Accuracy: 0.96875, Computation time: 1.5697112083435059\n",
      "Step: 5902, Loss: 0.9161753058433533, Accuracy: 1.0, Computation time: 1.3115973472595215\n",
      "Step: 5903, Loss: 0.9163157343864441, Accuracy: 1.0, Computation time: 1.132577896118164\n",
      "Step: 5904, Loss: 0.91603022813797, Accuracy: 1.0, Computation time: 1.3124690055847168\n",
      "Step: 5905, Loss: 0.9232542514801025, Accuracy: 1.0, Computation time: 1.3187854290008545\n",
      "Step: 5906, Loss: 0.9167320132255554, Accuracy: 1.0, Computation time: 1.4229187965393066\n",
      "Step: 5907, Loss: 0.9353027939796448, Accuracy: 0.9722222089767456, Computation time: 1.2027475833892822\n",
      "Step: 5908, Loss: 0.915984570980072, Accuracy: 1.0, Computation time: 1.4539306163787842\n",
      "Step: 5909, Loss: 0.9159712791442871, Accuracy: 1.0, Computation time: 1.7948591709136963\n",
      "Step: 5910, Loss: 0.9368368983268738, Accuracy: 0.9375, Computation time: 1.287214756011963\n",
      "Step: 5911, Loss: 0.9185784459114075, Accuracy: 1.0, Computation time: 1.3846898078918457\n",
      "Step: 5912, Loss: 0.9160533547401428, Accuracy: 1.0, Computation time: 1.1597700119018555\n",
      "Step: 5913, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.0772244930267334\n",
      "Step: 5914, Loss: 0.9377599358558655, Accuracy: 0.9642857313156128, Computation time: 1.1693744659423828\n",
      "Step: 5915, Loss: 0.9160518050193787, Accuracy: 1.0, Computation time: 1.2009217739105225\n",
      "Step: 5916, Loss: 0.9363219141960144, Accuracy: 0.9807692766189575, Computation time: 1.1994752883911133\n",
      "Step: 5917, Loss: 0.9327116012573242, Accuracy: 0.96875, Computation time: 1.1034595966339111\n",
      "Step: 5918, Loss: 0.9160956740379333, Accuracy: 1.0, Computation time: 1.4181883335113525\n",
      "Step: 5919, Loss: 0.9160035848617554, Accuracy: 1.0, Computation time: 1.2313752174377441\n",
      "Step: 5920, Loss: 0.9159674644470215, Accuracy: 1.0, Computation time: 1.1105577945709229\n",
      "Step: 5921, Loss: 0.9160498380661011, Accuracy: 1.0, Computation time: 1.3553662300109863\n",
      "Step: 5922, Loss: 0.916024923324585, Accuracy: 1.0, Computation time: 1.1769163608551025\n",
      "Step: 5923, Loss: 0.9160082936286926, Accuracy: 1.0, Computation time: 1.1326775550842285\n",
      "Step: 5924, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 1.1051712036132812\n",
      "Step: 5925, Loss: 0.9161297082901001, Accuracy: 1.0, Computation time: 1.1390962600708008\n",
      "Step: 5926, Loss: 0.935217559337616, Accuracy: 0.9722222089767456, Computation time: 1.3951432704925537\n",
      "Step: 5927, Loss: 0.9160156846046448, Accuracy: 1.0, Computation time: 1.3609848022460938\n",
      "Step: 5928, Loss: 0.9181294441223145, Accuracy: 1.0, Computation time: 1.448256015777588\n",
      "Step: 5929, Loss: 0.9159964919090271, Accuracy: 1.0, Computation time: 1.0880086421966553\n",
      "Step: 5930, Loss: 0.9167719483375549, Accuracy: 1.0, Computation time: 1.1136064529418945\n",
      "Step: 5931, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.0266153812408447\n",
      "Step: 5932, Loss: 0.9382063150405884, Accuracy: 0.9821428656578064, Computation time: 1.1398184299468994\n",
      "Step: 5933, Loss: 0.9160860180854797, Accuracy: 1.0, Computation time: 1.4078185558319092\n",
      "Step: 5934, Loss: 0.916776716709137, Accuracy: 1.0, Computation time: 1.2314398288726807\n",
      "Step: 5935, Loss: 0.9159539937973022, Accuracy: 1.0, Computation time: 1.488722801208496\n",
      "Step: 5936, Loss: 0.9159581661224365, Accuracy: 1.0, Computation time: 1.2833905220031738\n",
      "Step: 5937, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 1.1875290870666504\n",
      "Step: 5938, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.2291326522827148\n",
      "Step: 5939, Loss: 0.959625244140625, Accuracy: 0.9479166865348816, Computation time: 1.1525025367736816\n",
      "Step: 5940, Loss: 0.9161347150802612, Accuracy: 1.0, Computation time: 1.62937331199646\n",
      "Step: 5941, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.075659990310669\n",
      "Step: 5942, Loss: 0.9160127639770508, Accuracy: 1.0, Computation time: 1.027418851852417\n",
      "Step: 5943, Loss: 0.9174113273620605, Accuracy: 1.0, Computation time: 1.3718628883361816\n",
      "Step: 5944, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.193296194076538\n",
      "Step: 5945, Loss: 0.9160520434379578, Accuracy: 1.0, Computation time: 1.118335485458374\n",
      "Step: 5946, Loss: 0.9159485101699829, Accuracy: 1.0, Computation time: 1.0203871726989746\n",
      "Step: 5947, Loss: 0.9160094261169434, Accuracy: 1.0, Computation time: 1.355609655380249\n",
      "Step: 5948, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 1.1750645637512207\n",
      "Step: 5949, Loss: 0.915938675403595, Accuracy: 1.0, Computation time: 1.2080729007720947\n",
      "Step: 5950, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 1.0494043827056885\n",
      "Step: 5951, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.0144197940826416\n",
      "Step: 5952, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.4248533248901367\n",
      "Step: 5953, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 1.1208207607269287\n",
      "Step: 5954, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.0513055324554443\n",
      "Step: 5955, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.088449478149414\n",
      "Step: 5956, Loss: 0.9285777807235718, Accuracy: 0.9791666865348816, Computation time: 1.4895813465118408\n",
      "Step: 5957, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.2766029834747314\n",
      "Step: 5958, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 1.2528269290924072\n",
      "Step: 5959, Loss: 0.9267799258232117, Accuracy: 0.96875, Computation time: 1.2317321300506592\n",
      "Step: 5960, Loss: 0.9160050749778748, Accuracy: 1.0, Computation time: 1.1863813400268555\n",
      "Step: 5961, Loss: 0.937610387802124, Accuracy: 0.9750000238418579, Computation time: 1.206085443496704\n",
      "Step: 5962, Loss: 0.9161292314529419, Accuracy: 1.0, Computation time: 1.1234257221221924\n",
      "Step: 5963, Loss: 0.9159368276596069, Accuracy: 1.0, Computation time: 1.071610927581787\n",
      "Step: 5964, Loss: 0.9167319536209106, Accuracy: 1.0, Computation time: 1.4148688316345215\n",
      "Step: 5965, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.1583292484283447\n",
      "Step: 5966, Loss: 0.9169808030128479, Accuracy: 1.0, Computation time: 1.2869987487792969\n",
      "Step: 5967, Loss: 0.9374725818634033, Accuracy: 0.9722222089767456, Computation time: 1.1963648796081543\n",
      "Step: 5968, Loss: 0.9382869601249695, Accuracy: 0.9807692766189575, Computation time: 1.1226117610931396\n",
      "Step: 5969, Loss: 0.9162743091583252, Accuracy: 1.0, Computation time: 1.2811799049377441\n",
      "Step: 5970, Loss: 0.9164201021194458, Accuracy: 1.0, Computation time: 1.1383380889892578\n",
      "Step: 5971, Loss: 0.9159756302833557, Accuracy: 1.0, Computation time: 1.1281251907348633\n",
      "Step: 5972, Loss: 0.9245358109474182, Accuracy: 1.0, Computation time: 1.0999572277069092\n",
      "Step: 5973, Loss: 0.915973424911499, Accuracy: 1.0, Computation time: 1.0577664375305176\n",
      "Step: 5974, Loss: 0.916019082069397, Accuracy: 1.0, Computation time: 1.1087563037872314\n",
      "Step: 5975, Loss: 0.916344165802002, Accuracy: 1.0, Computation time: 1.3732151985168457\n",
      "########################\n",
      "Test loss: 1.0743390321731567, Test Accuracy_epoch43: 0.7598647475242615\n",
      "########################\n",
      "Step: 5976, Loss: 0.9161288738250732, Accuracy: 1.0, Computation time: 1.0256240367889404\n",
      "Step: 5977, Loss: 0.9160168766975403, Accuracy: 1.0, Computation time: 1.0245981216430664\n",
      "Step: 5978, Loss: 0.9159901142120361, Accuracy: 1.0, Computation time: 1.2645840644836426\n",
      "Step: 5979, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.1414446830749512\n",
      "Step: 5980, Loss: 0.9160060286521912, Accuracy: 1.0, Computation time: 1.0817351341247559\n",
      "Step: 5981, Loss: 0.9205899834632874, Accuracy: 1.0, Computation time: 1.0342164039611816\n",
      "Step: 5982, Loss: 0.9185070991516113, Accuracy: 1.0, Computation time: 1.2571923732757568\n",
      "Step: 5983, Loss: 0.916298508644104, Accuracy: 1.0, Computation time: 1.3978731632232666\n",
      "Step: 5984, Loss: 0.9165736436843872, Accuracy: 1.0, Computation time: 1.747274398803711\n",
      "Step: 5985, Loss: 0.9161206483840942, Accuracy: 1.0, Computation time: 1.127532720565796\n",
      "Step: 5986, Loss: 0.9377548098564148, Accuracy: 0.9722222089767456, Computation time: 0.9799506664276123\n",
      "Step: 5987, Loss: 0.9161702394485474, Accuracy: 1.0, Computation time: 1.0905086994171143\n",
      "Step: 5988, Loss: 0.9160400032997131, Accuracy: 1.0, Computation time: 1.1053447723388672\n",
      "Step: 5989, Loss: 0.9159261584281921, Accuracy: 1.0, Computation time: 1.2597429752349854\n",
      "Step: 5990, Loss: 0.9185099005699158, Accuracy: 1.0, Computation time: 1.3094356060028076\n",
      "Step: 5991, Loss: 0.9346397519111633, Accuracy: 0.9642857313156128, Computation time: 1.4680674076080322\n",
      "Step: 5992, Loss: 0.9160193800926208, Accuracy: 1.0, Computation time: 1.2228538990020752\n",
      "Step: 5993, Loss: 0.9304552674293518, Accuracy: 0.9791666865348816, Computation time: 1.0853066444396973\n",
      "Step: 5994, Loss: 0.9355577230453491, Accuracy: 0.9772727489471436, Computation time: 1.1762878894805908\n",
      "Step: 5995, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.0111186504364014\n",
      "Step: 5996, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 1.2089076042175293\n",
      "Step: 5997, Loss: 0.937571108341217, Accuracy: 0.9722222089767456, Computation time: 1.3865704536437988\n",
      "Step: 5998, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.183800220489502\n",
      "Step: 5999, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 1.2194609642028809\n",
      "Step: 6000, Loss: 0.9158888459205627, Accuracy: 1.0, Computation time: 1.277036428451538\n",
      "Step: 6001, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.3439791202545166\n",
      "Step: 6002, Loss: 0.9206594824790955, Accuracy: 1.0, Computation time: 1.7641563415527344\n",
      "Step: 6003, Loss: 0.9160816669464111, Accuracy: 1.0, Computation time: 1.2383854389190674\n",
      "Step: 6004, Loss: 0.915996253490448, Accuracy: 1.0, Computation time: 1.088686227798462\n",
      "Step: 6005, Loss: 0.9162265658378601, Accuracy: 1.0, Computation time: 1.3795287609100342\n",
      "Step: 6006, Loss: 0.9162396192550659, Accuracy: 1.0, Computation time: 1.1232101917266846\n",
      "Step: 6007, Loss: 0.9379473924636841, Accuracy: 0.96875, Computation time: 1.540100336074829\n",
      "Step: 6008, Loss: 0.9279197454452515, Accuracy: 0.949999988079071, Computation time: 1.0557737350463867\n",
      "Step: 6009, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.3007662296295166\n",
      "Step: 6010, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 0.9784975051879883\n",
      "Step: 6011, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 1.1468095779418945\n",
      "Step: 6012, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.0262706279754639\n",
      "Step: 6013, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 0.987499475479126\n",
      "Step: 6014, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.0858922004699707\n",
      "Step: 6015, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.1095588207244873\n",
      "Step: 6016, Loss: 0.9201980829238892, Accuracy: 1.0, Computation time: 1.5700860023498535\n",
      "Step: 6017, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 0.9508452415466309\n",
      "Step: 6018, Loss: 0.916144609451294, Accuracy: 1.0, Computation time: 1.468043327331543\n",
      "Step: 6019, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.2812755107879639\n",
      "Step: 6020, Loss: 0.9159945845603943, Accuracy: 1.0, Computation time: 1.1139552593231201\n",
      "Step: 6021, Loss: 0.9377325177192688, Accuracy: 0.9722222089767456, Computation time: 1.1422569751739502\n",
      "Step: 6022, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 1.2507576942443848\n",
      "Step: 6023, Loss: 0.9363400340080261, Accuracy: 0.9750000238418579, Computation time: 1.096709966659546\n",
      "Step: 6024, Loss: 0.9167457818984985, Accuracy: 1.0, Computation time: 2.297098159790039\n",
      "Step: 6025, Loss: 0.9159666895866394, Accuracy: 1.0, Computation time: 1.1470024585723877\n",
      "Step: 6026, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.3419923782348633\n",
      "Step: 6027, Loss: 0.9201237559318542, Accuracy: 1.0, Computation time: 1.983649492263794\n",
      "Step: 6028, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 1.2389545440673828\n",
      "Step: 6029, Loss: 0.9159965515136719, Accuracy: 1.0, Computation time: 1.3668420314788818\n",
      "Step: 6030, Loss: 0.9377931356430054, Accuracy: 0.9750000238418579, Computation time: 1.0940790176391602\n",
      "Step: 6031, Loss: 0.9159975051879883, Accuracy: 1.0, Computation time: 1.175309181213379\n",
      "Step: 6032, Loss: 0.9161269664764404, Accuracy: 1.0, Computation time: 1.260643720626831\n",
      "Step: 6033, Loss: 0.9159116744995117, Accuracy: 1.0, Computation time: 1.5107421875\n",
      "Step: 6034, Loss: 0.9376368522644043, Accuracy: 0.9807692766189575, Computation time: 1.6485402584075928\n",
      "Step: 6035, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.284315586090088\n",
      "Step: 6036, Loss: 0.9375230669975281, Accuracy: 0.984375, Computation time: 1.2258543968200684\n",
      "Step: 6037, Loss: 0.9377057552337646, Accuracy: 0.9750000238418579, Computation time: 1.2799384593963623\n",
      "Step: 6038, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.3354883193969727\n",
      "Step: 6039, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.2724552154541016\n",
      "Step: 6040, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.737412929534912\n",
      "Step: 6041, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.2137377262115479\n",
      "Step: 6042, Loss: 0.915960431098938, Accuracy: 1.0, Computation time: 1.429854393005371\n",
      "Step: 6043, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.4065749645233154\n",
      "Step: 6044, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.5852468013763428\n",
      "Step: 6045, Loss: 0.9375532865524292, Accuracy: 0.984375, Computation time: 1.193925142288208\n",
      "Step: 6046, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.7721855640411377\n",
      "Step: 6047, Loss: 0.9161123633384705, Accuracy: 1.0, Computation time: 1.65484619140625\n",
      "Step: 6048, Loss: 0.9353266954421997, Accuracy: 0.9791666865348816, Computation time: 1.154325246810913\n",
      "Step: 6049, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 0.9663500785827637\n",
      "Step: 6050, Loss: 0.9374857544898987, Accuracy: 0.9791666865348816, Computation time: 1.100928783416748\n",
      "Step: 6051, Loss: 0.9197246432304382, Accuracy: 1.0, Computation time: 1.4217751026153564\n",
      "Step: 6052, Loss: 0.9197202324867249, Accuracy: 1.0, Computation time: 1.8418474197387695\n",
      "Step: 6053, Loss: 0.9546194672584534, Accuracy: 0.9437500238418579, Computation time: 1.4269766807556152\n",
      "Step: 6054, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 1.0834996700286865\n",
      "Step: 6055, Loss: 0.9159762859344482, Accuracy: 1.0, Computation time: 0.9052340984344482\n",
      "Step: 6056, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 1.2872676849365234\n",
      "Step: 6057, Loss: 0.9380676746368408, Accuracy: 0.984375, Computation time: 1.0816526412963867\n",
      "Step: 6058, Loss: 0.9161297082901001, Accuracy: 1.0, Computation time: 1.0895309448242188\n",
      "Step: 6059, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.2836952209472656\n",
      "Step: 6060, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.597978115081787\n",
      "Step: 6061, Loss: 0.9170978665351868, Accuracy: 1.0, Computation time: 1.0661301612854004\n",
      "Step: 6062, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.3005273342132568\n",
      "Step: 6063, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 1.3969264030456543\n",
      "Step: 6064, Loss: 0.9191654920578003, Accuracy: 1.0, Computation time: 1.316908359527588\n",
      "Step: 6065, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.1866135597229004\n",
      "Step: 6066, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.3346447944641113\n",
      "Step: 6067, Loss: 0.9159770011901855, Accuracy: 1.0, Computation time: 1.3352134227752686\n",
      "Step: 6068, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.2004790306091309\n",
      "Step: 6069, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.1045973300933838\n",
      "Step: 6070, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.2850878238677979\n",
      "Step: 6071, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.1295380592346191\n",
      "Step: 6072, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.2532949447631836\n",
      "Step: 6073, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.1766252517700195\n",
      "Step: 6074, Loss: 0.9159504175186157, Accuracy: 1.0, Computation time: 1.17964506149292\n",
      "Step: 6075, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 1.2380812168121338\n",
      "Step: 6076, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 0.987163782119751\n",
      "Step: 6077, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.155562162399292\n",
      "Step: 6078, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.2553000450134277\n",
      "Step: 6079, Loss: 0.9160493016242981, Accuracy: 1.0, Computation time: 1.3484346866607666\n",
      "Step: 6080, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 0.8958477973937988\n",
      "Step: 6081, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.1643168926239014\n",
      "Step: 6082, Loss: 0.915979266166687, Accuracy: 1.0, Computation time: 0.9920833110809326\n",
      "Step: 6083, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.0716941356658936\n",
      "Step: 6084, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 0.9594783782958984\n",
      "Step: 6085, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 0.9333131313323975\n",
      "Step: 6086, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.3891310691833496\n",
      "Step: 6087, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.0255825519561768\n",
      "Step: 6088, Loss: 0.9354388117790222, Accuracy: 0.9772727489471436, Computation time: 1.3149797916412354\n",
      "Step: 6089, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.1419122219085693\n",
      "Step: 6090, Loss: 0.9161021709442139, Accuracy: 1.0, Computation time: 1.0084104537963867\n",
      "Step: 6091, Loss: 0.916225254535675, Accuracy: 1.0, Computation time: 1.3552958965301514\n",
      "Step: 6092, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.1313951015472412\n",
      "Step: 6093, Loss: 0.9159124493598938, Accuracy: 1.0, Computation time: 1.116342306137085\n",
      "Step: 6094, Loss: 0.9181421399116516, Accuracy: 1.0, Computation time: 1.235593557357788\n",
      "Step: 6095, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.167069673538208\n",
      "Step: 6096, Loss: 0.9177687168121338, Accuracy: 1.0, Computation time: 1.4738502502441406\n",
      "Step: 6097, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.355475664138794\n",
      "Step: 6098, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 0.939598798751831\n",
      "Step: 6099, Loss: 0.9376729130744934, Accuracy: 0.9772727489471436, Computation time: 1.128443956375122\n",
      "Step: 6100, Loss: 0.9159583449363708, Accuracy: 1.0, Computation time: 1.0930862426757812\n",
      "Step: 6101, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 0.9478199481964111\n",
      "Step: 6102, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 1.0895311832427979\n",
      "Step: 6103, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.1247210502624512\n",
      "Step: 6104, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.1066100597381592\n",
      "Step: 6105, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.1815135478973389\n",
      "Step: 6106, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.258474349975586\n",
      "Step: 6107, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 1.4242329597473145\n",
      "Step: 6108, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 0.9757335186004639\n",
      "Step: 6109, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.5521714687347412\n",
      "Step: 6110, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.1501727104187012\n",
      "Step: 6111, Loss: 0.916068971157074, Accuracy: 1.0, Computation time: 1.3932905197143555\n",
      "Step: 6112, Loss: 0.9163419008255005, Accuracy: 1.0, Computation time: 1.4267306327819824\n",
      "Step: 6113, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 1.3643794059753418\n",
      "Step: 6114, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.6158294677734375\n",
      "########################\n",
      "Test loss: 1.0733250379562378, Test Accuracy_epoch44: 0.7638394236564636\n",
      "########################\n",
      "Step: 6115, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.41748046875\n",
      "Step: 6116, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.608654260635376\n",
      "Step: 6117, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.2456395626068115\n",
      "Step: 6118, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.2819063663482666\n",
      "Step: 6119, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.0482800006866455\n",
      "Step: 6120, Loss: 0.9160538911819458, Accuracy: 1.0, Computation time: 1.4739298820495605\n",
      "Step: 6121, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 1.3046696186065674\n",
      "Step: 6122, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.212998390197754\n",
      "Step: 6123, Loss: 0.9331477284431458, Accuracy: 0.96875, Computation time: 1.5866994857788086\n",
      "Step: 6124, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 1.4110491275787354\n",
      "Step: 6125, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.2215168476104736\n",
      "Step: 6126, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.4972727298736572\n",
      "Step: 6127, Loss: 0.9159253239631653, Accuracy: 1.0, Computation time: 1.1575360298156738\n",
      "Step: 6128, Loss: 0.9159447550773621, Accuracy: 1.0, Computation time: 1.3135781288146973\n",
      "Step: 6129, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 1.1746013164520264\n",
      "Step: 6130, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.2760939598083496\n",
      "Step: 6131, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.4092671871185303\n",
      "Step: 6132, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.2932524681091309\n",
      "Step: 6133, Loss: 0.9163031578063965, Accuracy: 1.0, Computation time: 1.6359915733337402\n",
      "Step: 6134, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.4125795364379883\n",
      "Step: 6135, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.3912746906280518\n",
      "Step: 6136, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.5568089485168457\n",
      "Step: 6137, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.1515698432922363\n",
      "Step: 6138, Loss: 0.9207444190979004, Accuracy: 1.0, Computation time: 1.4030463695526123\n",
      "Step: 6139, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 1.1791889667510986\n",
      "Step: 6140, Loss: 0.9161281585693359, Accuracy: 1.0, Computation time: 1.3067643642425537\n",
      "Step: 6141, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.4259705543518066\n",
      "Step: 6142, Loss: 0.915963351726532, Accuracy: 1.0, Computation time: 1.397672176361084\n",
      "Step: 6143, Loss: 0.9159859418869019, Accuracy: 1.0, Computation time: 1.521662950515747\n",
      "Step: 6144, Loss: 0.915935754776001, Accuracy: 1.0, Computation time: 1.5434622764587402\n",
      "Step: 6145, Loss: 0.9375722408294678, Accuracy: 0.9722222089767456, Computation time: 1.4549286365509033\n",
      "Step: 6146, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 1.18049955368042\n",
      "Step: 6147, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.2800157070159912\n",
      "Step: 6148, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.1634266376495361\n",
      "Step: 6149, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.079296588897705\n",
      "Step: 6150, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.283198595046997\n",
      "Step: 6151, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.4459693431854248\n",
      "Step: 6152, Loss: 0.9159083962440491, Accuracy: 1.0, Computation time: 1.2044425010681152\n",
      "Step: 6153, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 1.0958142280578613\n",
      "Step: 6154, Loss: 0.9213439226150513, Accuracy: 1.0, Computation time: 1.6506187915802002\n",
      "Step: 6155, Loss: 0.915911078453064, Accuracy: 1.0, Computation time: 1.3440654277801514\n",
      "Step: 6156, Loss: 0.949659526348114, Accuracy: 0.9513888955116272, Computation time: 1.1911261081695557\n",
      "Step: 6157, Loss: 0.9162290692329407, Accuracy: 1.0, Computation time: 1.5200340747833252\n",
      "Step: 6158, Loss: 0.9380969405174255, Accuracy: 0.9772727489471436, Computation time: 1.282282829284668\n",
      "Step: 6159, Loss: 0.9160102009773254, Accuracy: 1.0, Computation time: 1.49320650100708\n",
      "Step: 6160, Loss: 0.9159985184669495, Accuracy: 1.0, Computation time: 1.12729811668396\n",
      "Step: 6161, Loss: 0.9159862995147705, Accuracy: 1.0, Computation time: 2.1138784885406494\n",
      "Step: 6162, Loss: 0.9165312647819519, Accuracy: 1.0, Computation time: 1.382232666015625\n",
      "Step: 6163, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.3465161323547363\n",
      "Step: 6164, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.229468584060669\n",
      "Step: 6165, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.2703063488006592\n",
      "Step: 6166, Loss: 0.9375103712081909, Accuracy: 0.9791666865348816, Computation time: 1.1401643753051758\n",
      "Step: 6167, Loss: 0.9159702062606812, Accuracy: 1.0, Computation time: 1.1136560440063477\n",
      "Step: 6168, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 1.1684215068817139\n",
      "Step: 6169, Loss: 0.9376227259635925, Accuracy: 0.9583333730697632, Computation time: 1.2136294841766357\n",
      "Step: 6170, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.111175298690796\n",
      "Step: 6171, Loss: 0.9375597834587097, Accuracy: 0.9642857313156128, Computation time: 1.068378210067749\n",
      "Step: 6172, Loss: 0.9595250487327576, Accuracy: 0.9409722089767456, Computation time: 1.525636911392212\n",
      "Step: 6173, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.0969712734222412\n",
      "Step: 6174, Loss: 0.9236725568771362, Accuracy: 1.0, Computation time: 1.269096851348877\n",
      "Step: 6175, Loss: 0.915958821773529, Accuracy: 1.0, Computation time: 1.142383098602295\n",
      "Step: 6176, Loss: 0.9166346192359924, Accuracy: 1.0, Computation time: 1.0190610885620117\n",
      "Step: 6177, Loss: 0.9159372448921204, Accuracy: 1.0, Computation time: 0.9657485485076904\n",
      "Step: 6178, Loss: 0.9160923361778259, Accuracy: 1.0, Computation time: 1.1424858570098877\n",
      "Step: 6179, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 1.139674186706543\n",
      "Step: 6180, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 1.0963401794433594\n",
      "Step: 6181, Loss: 0.9166876077651978, Accuracy: 1.0, Computation time: 1.3957834243774414\n",
      "Step: 6182, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 1.359947681427002\n",
      "Step: 6183, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.1834003925323486\n",
      "Step: 6184, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.198781967163086\n",
      "Step: 6185, Loss: 0.9159411787986755, Accuracy: 1.0, Computation time: 1.3045053482055664\n",
      "Step: 6186, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 1.7159099578857422\n",
      "Step: 6187, Loss: 0.9160283803939819, Accuracy: 1.0, Computation time: 1.3934721946716309\n",
      "Step: 6188, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.4735937118530273\n",
      "Step: 6189, Loss: 0.9289634227752686, Accuracy: 0.9750000238418579, Computation time: 1.513092041015625\n",
      "Step: 6190, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.4944584369659424\n",
      "Step: 6191, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.171316385269165\n",
      "Step: 6192, Loss: 0.9160594940185547, Accuracy: 1.0, Computation time: 1.4458632469177246\n",
      "Step: 6193, Loss: 0.9160438179969788, Accuracy: 1.0, Computation time: 1.3164868354797363\n",
      "Step: 6194, Loss: 0.916133463382721, Accuracy: 1.0, Computation time: 1.7576422691345215\n",
      "Step: 6195, Loss: 0.9164716601371765, Accuracy: 1.0, Computation time: 1.4468939304351807\n",
      "Step: 6196, Loss: 0.9376263618469238, Accuracy: 0.984375, Computation time: 1.9265577793121338\n",
      "Step: 6197, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.5938100814819336\n",
      "Step: 6198, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.585418939590454\n",
      "Step: 6199, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.979698896408081\n",
      "Step: 6200, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 1.2718379497528076\n",
      "Step: 6201, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.2769017219543457\n",
      "Step: 6202, Loss: 0.9281876087188721, Accuracy: 0.949999988079071, Computation time: 1.285008430480957\n",
      "Step: 6203, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 1.3704948425292969\n",
      "Step: 6204, Loss: 0.9159941673278809, Accuracy: 1.0, Computation time: 1.020343542098999\n",
      "Step: 6205, Loss: 0.9205594658851624, Accuracy: 1.0, Computation time: 2.423072099685669\n",
      "Step: 6206, Loss: 0.9591332077980042, Accuracy: 0.970588207244873, Computation time: 2.024731159210205\n",
      "Step: 6207, Loss: 0.916070282459259, Accuracy: 1.0, Computation time: 1.3299262523651123\n",
      "Step: 6208, Loss: 0.9162777662277222, Accuracy: 1.0, Computation time: 1.15293550491333\n",
      "Step: 6209, Loss: 0.9168612957000732, Accuracy: 1.0, Computation time: 1.2112538814544678\n",
      "Step: 6210, Loss: 0.9159488677978516, Accuracy: 1.0, Computation time: 1.0964770317077637\n",
      "Step: 6211, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 1.2095983028411865\n",
      "Step: 6212, Loss: 0.9381511807441711, Accuracy: 0.9722222089767456, Computation time: 1.157583475112915\n",
      "Step: 6213, Loss: 0.9160323739051819, Accuracy: 1.0, Computation time: 1.147848129272461\n",
      "Step: 6214, Loss: 0.9181217551231384, Accuracy: 1.0, Computation time: 1.5092132091522217\n",
      "Step: 6215, Loss: 0.9160417318344116, Accuracy: 1.0, Computation time: 1.2077486515045166\n",
      "Step: 6216, Loss: 0.9161064028739929, Accuracy: 1.0, Computation time: 1.4083127975463867\n",
      "Step: 6217, Loss: 0.9162012934684753, Accuracy: 1.0, Computation time: 1.1606228351593018\n",
      "Step: 6218, Loss: 0.9160639047622681, Accuracy: 1.0, Computation time: 1.2944705486297607\n",
      "Step: 6219, Loss: 0.9160478115081787, Accuracy: 1.0, Computation time: 1.2040014266967773\n",
      "Step: 6220, Loss: 0.9160622954368591, Accuracy: 1.0, Computation time: 1.4821367263793945\n",
      "Step: 6221, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.2437188625335693\n",
      "Step: 6222, Loss: 0.9380064010620117, Accuracy: 0.9583333730697632, Computation time: 1.2812888622283936\n",
      "Step: 6223, Loss: 0.9159569144248962, Accuracy: 1.0, Computation time: 1.2770626544952393\n",
      "Step: 6224, Loss: 0.9163604974746704, Accuracy: 1.0, Computation time: 1.0670998096466064\n",
      "Step: 6225, Loss: 0.9160444736480713, Accuracy: 1.0, Computation time: 1.1635031700134277\n",
      "Step: 6226, Loss: 0.9364774823188782, Accuracy: 0.9772727489471436, Computation time: 1.3851816654205322\n",
      "Step: 6227, Loss: 0.9163939952850342, Accuracy: 1.0, Computation time: 1.144017219543457\n",
      "Step: 6228, Loss: 0.916252076625824, Accuracy: 1.0, Computation time: 1.093886137008667\n",
      "Step: 6229, Loss: 0.9164338111877441, Accuracy: 1.0, Computation time: 1.055812120437622\n",
      "Step: 6230, Loss: 0.9162750244140625, Accuracy: 1.0, Computation time: 1.1988637447357178\n",
      "Step: 6231, Loss: 0.9161034822463989, Accuracy: 1.0, Computation time: 1.3317222595214844\n",
      "Step: 6232, Loss: 0.91606605052948, Accuracy: 1.0, Computation time: 1.2069733142852783\n",
      "Step: 6233, Loss: 0.9161571860313416, Accuracy: 1.0, Computation time: 1.1759939193725586\n",
      "Step: 6234, Loss: 0.9161684513092041, Accuracy: 1.0, Computation time: 1.3196074962615967\n",
      "Step: 6235, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.1032567024230957\n",
      "Step: 6236, Loss: 0.9160069227218628, Accuracy: 1.0, Computation time: 1.1534171104431152\n",
      "Step: 6237, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 1.119290828704834\n",
      "Step: 6238, Loss: 0.9511733651161194, Accuracy: 0.9583333730697632, Computation time: 1.2889809608459473\n",
      "Step: 6239, Loss: 0.9163548350334167, Accuracy: 1.0, Computation time: 1.5249311923980713\n",
      "Step: 6240, Loss: 0.9177154302597046, Accuracy: 1.0, Computation time: 1.2453045845031738\n",
      "Step: 6241, Loss: 0.9182500839233398, Accuracy: 1.0, Computation time: 1.6937813758850098\n",
      "Step: 6242, Loss: 0.9166876673698425, Accuracy: 1.0, Computation time: 1.123373031616211\n",
      "Step: 6243, Loss: 0.9177973866462708, Accuracy: 1.0, Computation time: 1.105459451675415\n",
      "Step: 6244, Loss: 0.9159427881240845, Accuracy: 1.0, Computation time: 1.1934943199157715\n",
      "Step: 6245, Loss: 0.9161673188209534, Accuracy: 1.0, Computation time: 1.33646559715271\n",
      "Step: 6246, Loss: 0.9162753224372864, Accuracy: 1.0, Computation time: 1.2380313873291016\n",
      "Step: 6247, Loss: 0.9169210195541382, Accuracy: 1.0, Computation time: 1.14121675491333\n",
      "Step: 6248, Loss: 0.9180013537406921, Accuracy: 1.0, Computation time: 1.1340742111206055\n",
      "Step: 6249, Loss: 0.9171542525291443, Accuracy: 1.0, Computation time: 1.2493529319763184\n",
      "Step: 6250, Loss: 0.9176483154296875, Accuracy: 1.0, Computation time: 1.515629768371582\n",
      "Step: 6251, Loss: 0.9161761403083801, Accuracy: 1.0, Computation time: 1.1684153079986572\n",
      "Step: 6252, Loss: 0.9161204695701599, Accuracy: 1.0, Computation time: 0.9903769493103027\n",
      "Step: 6253, Loss: 0.9259493947029114, Accuracy: 0.9722222089767456, Computation time: 1.2709829807281494\n",
      "########################\n",
      "Test loss: 1.0686687231063843, Test Accuracy_epoch45: 0.7686551809310913\n",
      "########################\n",
      "Step: 6254, Loss: 0.9162464737892151, Accuracy: 1.0, Computation time: 1.1538591384887695\n",
      "Step: 6255, Loss: 0.9161660075187683, Accuracy: 1.0, Computation time: 1.497389554977417\n",
      "Step: 6256, Loss: 0.9167726635932922, Accuracy: 1.0, Computation time: 1.3272936344146729\n",
      "Step: 6257, Loss: 0.9372537732124329, Accuracy: 0.9772727489471436, Computation time: 1.1777374744415283\n",
      "Step: 6258, Loss: 0.9166759252548218, Accuracy: 1.0, Computation time: 1.313405990600586\n",
      "Step: 6259, Loss: 0.9219608306884766, Accuracy: 1.0, Computation time: 1.6142220497131348\n",
      "Step: 6260, Loss: 0.9159484505653381, Accuracy: 1.0, Computation time: 1.0588312149047852\n",
      "Step: 6261, Loss: 0.9163722395896912, Accuracy: 1.0, Computation time: 1.059650182723999\n",
      "Step: 6262, Loss: 0.9608862996101379, Accuracy: 0.9375, Computation time: 1.012547492980957\n",
      "Step: 6263, Loss: 0.9169722199440002, Accuracy: 1.0, Computation time: 1.0942556858062744\n",
      "Step: 6264, Loss: 0.9174273014068604, Accuracy: 1.0, Computation time: 1.4182333946228027\n",
      "Step: 6265, Loss: 0.9170040488243103, Accuracy: 1.0, Computation time: 1.1055879592895508\n",
      "Step: 6266, Loss: 0.917250394821167, Accuracy: 1.0, Computation time: 1.119581699371338\n",
      "Step: 6267, Loss: 0.938457190990448, Accuracy: 0.9642857313156128, Computation time: 1.5606825351715088\n",
      "Step: 6268, Loss: 0.9159881472587585, Accuracy: 1.0, Computation time: 1.34442138671875\n",
      "Step: 6269, Loss: 0.9161800742149353, Accuracy: 1.0, Computation time: 1.0409824848175049\n",
      "Step: 6270, Loss: 0.9163552522659302, Accuracy: 1.0, Computation time: 1.3420825004577637\n",
      "Step: 6271, Loss: 0.9163857698440552, Accuracy: 1.0, Computation time: 1.2319989204406738\n",
      "Step: 6272, Loss: 0.9164987802505493, Accuracy: 1.0, Computation time: 1.3941364288330078\n",
      "Step: 6273, Loss: 0.9166794419288635, Accuracy: 1.0, Computation time: 1.481191873550415\n",
      "Step: 6274, Loss: 0.9165963530540466, Accuracy: 1.0, Computation time: 1.4334819316864014\n",
      "Step: 6275, Loss: 0.9159932136535645, Accuracy: 1.0, Computation time: 1.0159130096435547\n",
      "Step: 6276, Loss: 0.916350245475769, Accuracy: 1.0, Computation time: 0.9505786895751953\n",
      "Step: 6277, Loss: 0.91603022813797, Accuracy: 1.0, Computation time: 1.086186408996582\n",
      "Step: 6278, Loss: 0.915996789932251, Accuracy: 1.0, Computation time: 0.9674072265625\n",
      "Step: 6279, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.2670588493347168\n",
      "Step: 6280, Loss: 0.916301965713501, Accuracy: 1.0, Computation time: 1.1753849983215332\n",
      "Step: 6281, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 0.9570348262786865\n",
      "Step: 6282, Loss: 0.9160907864570618, Accuracy: 1.0, Computation time: 1.0542967319488525\n",
      "Step: 6283, Loss: 0.9160438179969788, Accuracy: 1.0, Computation time: 1.2306077480316162\n",
      "Step: 6284, Loss: 0.9160466194152832, Accuracy: 1.0, Computation time: 1.0335376262664795\n",
      "Step: 6285, Loss: 0.9184569716453552, Accuracy: 1.0, Computation time: 1.2567503452301025\n",
      "Step: 6286, Loss: 0.9160611033439636, Accuracy: 1.0, Computation time: 0.9579143524169922\n",
      "Step: 6287, Loss: 0.9163061380386353, Accuracy: 1.0, Computation time: 1.3217225074768066\n",
      "Step: 6288, Loss: 0.916075587272644, Accuracy: 1.0, Computation time: 1.0329387187957764\n",
      "Step: 6289, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.100236177444458\n",
      "Step: 6290, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.292933702468872\n",
      "Step: 6291, Loss: 0.9159070253372192, Accuracy: 1.0, Computation time: 1.0767910480499268\n",
      "Step: 6292, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 0.9851667881011963\n",
      "Step: 6293, Loss: 0.9376527070999146, Accuracy: 0.984375, Computation time: 1.0008947849273682\n",
      "Step: 6294, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.2279448509216309\n",
      "Step: 6295, Loss: 0.9160338044166565, Accuracy: 1.0, Computation time: 0.9170396327972412\n",
      "Step: 6296, Loss: 0.9167197942733765, Accuracy: 1.0, Computation time: 1.2397689819335938\n",
      "Step: 6297, Loss: 0.9180880784988403, Accuracy: 1.0, Computation time: 1.248328685760498\n",
      "Step: 6298, Loss: 0.915946364402771, Accuracy: 1.0, Computation time: 1.0733225345611572\n",
      "Step: 6299, Loss: 0.9162470102310181, Accuracy: 1.0, Computation time: 1.2545137405395508\n",
      "Step: 6300, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 0.966362476348877\n",
      "Step: 6301, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.1181221008300781\n",
      "Step: 6302, Loss: 0.9164463877677917, Accuracy: 1.0, Computation time: 1.1088619232177734\n",
      "Step: 6303, Loss: 0.9159290194511414, Accuracy: 1.0, Computation time: 0.9464209079742432\n",
      "Step: 6304, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 0.9370980262756348\n",
      "Step: 6305, Loss: 0.9160926938056946, Accuracy: 1.0, Computation time: 1.043743371963501\n",
      "Step: 6306, Loss: 0.9163495302200317, Accuracy: 1.0, Computation time: 0.9756617546081543\n",
      "Step: 6307, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.0513577461242676\n",
      "Step: 6308, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 0.950946569442749\n",
      "Step: 6309, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.1628313064575195\n",
      "Step: 6310, Loss: 0.916813313961029, Accuracy: 1.0, Computation time: 1.0999326705932617\n",
      "Step: 6311, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.151818037033081\n",
      "Step: 6312, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.363420009613037\n",
      "Step: 6313, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 1.1199417114257812\n",
      "Step: 6314, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 0.9379115104675293\n",
      "Step: 6315, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.1697869300842285\n",
      "Step: 6316, Loss: 0.9165597558021545, Accuracy: 1.0, Computation time: 0.9763514995574951\n",
      "Step: 6317, Loss: 0.91729736328125, Accuracy: 1.0, Computation time: 1.1199467182159424\n",
      "Step: 6318, Loss: 0.9161875247955322, Accuracy: 1.0, Computation time: 1.3706493377685547\n",
      "Step: 6319, Loss: 0.9376404881477356, Accuracy: 0.9642857313156128, Computation time: 1.1759276390075684\n",
      "Step: 6320, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.10014009475708\n",
      "Step: 6321, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 0.9790611267089844\n",
      "Step: 6322, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.0111322402954102\n",
      "Step: 6323, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 0.9886550903320312\n",
      "Step: 6324, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 0.9365723133087158\n",
      "Step: 6325, Loss: 0.9159817695617676, Accuracy: 1.0, Computation time: 0.9410181045532227\n",
      "Step: 6326, Loss: 0.9375038146972656, Accuracy: 0.9807692766189575, Computation time: 0.901519775390625\n",
      "Step: 6327, Loss: 0.9374169707298279, Accuracy: 0.9750000238418579, Computation time: 1.0341317653656006\n",
      "Step: 6328, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 0.9908173084259033\n",
      "Step: 6329, Loss: 0.9159674644470215, Accuracy: 1.0, Computation time: 1.1415255069732666\n",
      "Step: 6330, Loss: 0.9179790616035461, Accuracy: 1.0, Computation time: 1.4383089542388916\n",
      "Step: 6331, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.1099798679351807\n",
      "Step: 6332, Loss: 0.9395056962966919, Accuracy: 0.9772727489471436, Computation time: 1.0167760848999023\n",
      "Step: 6333, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 0.9106965065002441\n",
      "Step: 6334, Loss: 0.9166263937950134, Accuracy: 1.0, Computation time: 1.1979403495788574\n",
      "Step: 6335, Loss: 0.9375000596046448, Accuracy: 0.9861111044883728, Computation time: 0.9454967975616455\n",
      "Step: 6336, Loss: 0.9159632325172424, Accuracy: 1.0, Computation time: 0.962505578994751\n",
      "Step: 6337, Loss: 0.9161032438278198, Accuracy: 1.0, Computation time: 1.0549452304840088\n",
      "Step: 6338, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.148221731185913\n",
      "Step: 6339, Loss: 0.9377781748771667, Accuracy: 0.9807692766189575, Computation time: 1.0709006786346436\n",
      "Step: 6340, Loss: 0.9207229614257812, Accuracy: 1.0, Computation time: 1.5798771381378174\n",
      "Step: 6341, Loss: 0.9375726580619812, Accuracy: 0.9750000238418579, Computation time: 1.1035361289978027\n",
      "Step: 6342, Loss: 0.9159712791442871, Accuracy: 1.0, Computation time: 1.0636043548583984\n",
      "Step: 6343, Loss: 0.9160043597221375, Accuracy: 1.0, Computation time: 1.1856663227081299\n",
      "Step: 6344, Loss: 0.9160168766975403, Accuracy: 1.0, Computation time: 1.0412969589233398\n",
      "Step: 6345, Loss: 0.9159299731254578, Accuracy: 1.0, Computation time: 1.2375195026397705\n",
      "Step: 6346, Loss: 0.9161310791969299, Accuracy: 1.0, Computation time: 1.0018532276153564\n",
      "Step: 6347, Loss: 0.9160075187683105, Accuracy: 1.0, Computation time: 1.0716640949249268\n",
      "Step: 6348, Loss: 0.9159942269325256, Accuracy: 1.0, Computation time: 1.048902988433838\n",
      "Step: 6349, Loss: 0.9159727692604065, Accuracy: 1.0, Computation time: 1.0319747924804688\n",
      "Step: 6350, Loss: 0.9162724018096924, Accuracy: 1.0, Computation time: 1.3805034160614014\n",
      "Step: 6351, Loss: 0.9159379601478577, Accuracy: 1.0, Computation time: 1.1083741188049316\n",
      "Step: 6352, Loss: 0.9172194004058838, Accuracy: 1.0, Computation time: 1.1285486221313477\n",
      "Step: 6353, Loss: 0.9162850379943848, Accuracy: 1.0, Computation time: 0.9477288722991943\n",
      "Step: 6354, Loss: 0.915928304195404, Accuracy: 1.0, Computation time: 0.920196533203125\n",
      "Step: 6355, Loss: 0.9160165190696716, Accuracy: 1.0, Computation time: 1.1374273300170898\n",
      "Step: 6356, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 0.9368948936462402\n",
      "Step: 6357, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 0.981651782989502\n",
      "Step: 6358, Loss: 0.9159471392631531, Accuracy: 1.0, Computation time: 1.0379235744476318\n",
      "Step: 6359, Loss: 0.9336845874786377, Accuracy: 0.9722222089767456, Computation time: 1.8037939071655273\n",
      "Step: 6360, Loss: 0.9160726070404053, Accuracy: 1.0, Computation time: 1.3574066162109375\n",
      "Step: 6361, Loss: 0.9162260293960571, Accuracy: 1.0, Computation time: 1.3055834770202637\n",
      "Step: 6362, Loss: 0.9161855578422546, Accuracy: 1.0, Computation time: 1.3430736064910889\n",
      "Step: 6363, Loss: 0.9169458150863647, Accuracy: 1.0, Computation time: 1.2930941581726074\n",
      "Step: 6364, Loss: 0.9161573052406311, Accuracy: 1.0, Computation time: 1.0632078647613525\n",
      "Step: 6365, Loss: 0.9160618782043457, Accuracy: 1.0, Computation time: 1.0230960845947266\n",
      "Step: 6366, Loss: 0.9159885048866272, Accuracy: 1.0, Computation time: 1.5022342205047607\n",
      "Step: 6367, Loss: 0.9248557686805725, Accuracy: 1.0, Computation time: 1.2814037799835205\n",
      "Step: 6368, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.0234401226043701\n",
      "Step: 6369, Loss: 0.9163114428520203, Accuracy: 1.0, Computation time: 1.3571836948394775\n",
      "Step: 6370, Loss: 0.9378395676612854, Accuracy: 0.9722222089767456, Computation time: 1.1729340553283691\n",
      "Step: 6371, Loss: 0.938051700592041, Accuracy: 0.9772727489471436, Computation time: 1.1582591533660889\n",
      "Step: 6372, Loss: 0.9166510105133057, Accuracy: 1.0, Computation time: 1.345625400543213\n",
      "Step: 6373, Loss: 0.9166632890701294, Accuracy: 1.0, Computation time: 1.0286719799041748\n",
      "Step: 6374, Loss: 0.9165844917297363, Accuracy: 1.0, Computation time: 1.3577353954315186\n",
      "Step: 6375, Loss: 0.9164127707481384, Accuracy: 1.0, Computation time: 1.826338768005371\n",
      "Step: 6376, Loss: 0.9161530137062073, Accuracy: 1.0, Computation time: 1.1560680866241455\n",
      "Step: 6377, Loss: 0.9159353971481323, Accuracy: 1.0, Computation time: 1.6375787258148193\n",
      "Step: 6378, Loss: 0.9161454439163208, Accuracy: 1.0, Computation time: 1.0825090408325195\n",
      "Step: 6379, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 1.2031457424163818\n",
      "Step: 6380, Loss: 0.916138231754303, Accuracy: 1.0, Computation time: 1.0864458084106445\n",
      "Step: 6381, Loss: 0.9163135290145874, Accuracy: 1.0, Computation time: 1.1091573238372803\n",
      "Step: 6382, Loss: 0.9160308837890625, Accuracy: 1.0, Computation time: 1.0456442832946777\n",
      "Step: 6383, Loss: 0.9161310195922852, Accuracy: 1.0, Computation time: 1.051701307296753\n",
      "Step: 6384, Loss: 0.9160711765289307, Accuracy: 1.0, Computation time: 1.1626856327056885\n",
      "Step: 6385, Loss: 0.9161247611045837, Accuracy: 1.0, Computation time: 1.144096851348877\n",
      "Step: 6386, Loss: 0.9378016591072083, Accuracy: 0.9807692766189575, Computation time: 1.1094725131988525\n",
      "Step: 6387, Loss: 0.9160059094429016, Accuracy: 1.0, Computation time: 1.1527035236358643\n",
      "Step: 6388, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 1.503908634185791\n",
      "Step: 6389, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.1300156116485596\n",
      "Step: 6390, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.044909954071045\n",
      "Step: 6391, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 1.2826752662658691\n",
      "Step: 6392, Loss: 0.9159307479858398, Accuracy: 1.0, Computation time: 1.1279959678649902\n",
      "########################\n",
      "Test loss: 1.0699656009674072, Test Accuracy_epoch46: 0.7688365578651428\n",
      "########################\n",
      "Step: 6393, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.0582735538482666\n",
      "Step: 6394, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 1.3059637546539307\n",
      "Step: 6395, Loss: 0.9160027503967285, Accuracy: 1.0, Computation time: 1.473287582397461\n",
      "Step: 6396, Loss: 0.9285659193992615, Accuracy: 0.9722222089767456, Computation time: 1.3714141845703125\n",
      "Step: 6397, Loss: 0.9159458875656128, Accuracy: 1.0, Computation time: 1.0226972103118896\n",
      "Step: 6398, Loss: 0.9160482883453369, Accuracy: 1.0, Computation time: 1.3840131759643555\n",
      "Step: 6399, Loss: 0.9376248121261597, Accuracy: 0.9807692766189575, Computation time: 1.2221314907073975\n",
      "Step: 6400, Loss: 0.9160459637641907, Accuracy: 1.0, Computation time: 1.0316708087921143\n",
      "Step: 6401, Loss: 0.9159631133079529, Accuracy: 1.0, Computation time: 1.0229229927062988\n",
      "Step: 6402, Loss: 0.9159668684005737, Accuracy: 1.0, Computation time: 1.0659217834472656\n",
      "Step: 6403, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 0.994823694229126\n",
      "Step: 6404, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.0992357730865479\n",
      "Step: 6405, Loss: 0.9159907698631287, Accuracy: 1.0, Computation time: 1.025719404220581\n",
      "Step: 6406, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.0243306159973145\n",
      "Step: 6407, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 1.0557365417480469\n",
      "Step: 6408, Loss: 0.9173353314399719, Accuracy: 1.0, Computation time: 1.2490808963775635\n",
      "Step: 6409, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.4044506549835205\n",
      "Step: 6410, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 1.1064739227294922\n",
      "Step: 6411, Loss: 0.9212815165519714, Accuracy: 1.0, Computation time: 1.692941665649414\n",
      "Step: 6412, Loss: 0.9160712957382202, Accuracy: 1.0, Computation time: 0.8804776668548584\n",
      "Step: 6413, Loss: 0.916541576385498, Accuracy: 1.0, Computation time: 1.9031825065612793\n",
      "Step: 6414, Loss: 0.9438253045082092, Accuracy: 0.96875, Computation time: 1.2709684371948242\n",
      "Step: 6415, Loss: 0.9160284399986267, Accuracy: 1.0, Computation time: 1.0953326225280762\n",
      "Step: 6416, Loss: 0.9161253571510315, Accuracy: 1.0, Computation time: 1.0593171119689941\n",
      "Step: 6417, Loss: 0.9160487055778503, Accuracy: 1.0, Computation time: 1.2066028118133545\n",
      "Step: 6418, Loss: 0.916325032711029, Accuracy: 1.0, Computation time: 1.0784447193145752\n",
      "Step: 6419, Loss: 0.9162219762802124, Accuracy: 1.0, Computation time: 1.2738478183746338\n",
      "Step: 6420, Loss: 0.916053831577301, Accuracy: 1.0, Computation time: 1.1825273036956787\n",
      "Step: 6421, Loss: 0.9160344004631042, Accuracy: 1.0, Computation time: 1.2501871585845947\n",
      "Step: 6422, Loss: 0.916260302066803, Accuracy: 1.0, Computation time: 1.1530754566192627\n",
      "Step: 6423, Loss: 0.9159932136535645, Accuracy: 1.0, Computation time: 1.0917179584503174\n",
      "Step: 6424, Loss: 0.9159390330314636, Accuracy: 1.0, Computation time: 1.0529847145080566\n",
      "Step: 6425, Loss: 0.9159817695617676, Accuracy: 1.0, Computation time: 1.1048855781555176\n",
      "Step: 6426, Loss: 0.9160804748535156, Accuracy: 1.0, Computation time: 1.0521140098571777\n",
      "Step: 6427, Loss: 0.9596133232116699, Accuracy: 0.9472222328186035, Computation time: 1.1715688705444336\n",
      "Step: 6428, Loss: 0.9161052703857422, Accuracy: 1.0, Computation time: 1.086055040359497\n",
      "Step: 6429, Loss: 0.9374213218688965, Accuracy: 0.96875, Computation time: 1.5166833400726318\n",
      "Step: 6430, Loss: 0.9159629940986633, Accuracy: 1.0, Computation time: 1.0527076721191406\n",
      "Step: 6431, Loss: 0.9159539937973022, Accuracy: 1.0, Computation time: 1.011101484298706\n",
      "Step: 6432, Loss: 0.9160081744194031, Accuracy: 1.0, Computation time: 1.2974052429199219\n",
      "Step: 6433, Loss: 0.9159456491470337, Accuracy: 1.0, Computation time: 1.1700551509857178\n",
      "Step: 6434, Loss: 0.9159365296363831, Accuracy: 1.0, Computation time: 1.0253016948699951\n",
      "Step: 6435, Loss: 0.9173937439918518, Accuracy: 1.0, Computation time: 1.0771815776824951\n",
      "Step: 6436, Loss: 0.9159107804298401, Accuracy: 1.0, Computation time: 1.216968297958374\n",
      "Step: 6437, Loss: 0.9162042140960693, Accuracy: 1.0, Computation time: 1.2086975574493408\n",
      "Step: 6438, Loss: 0.9197325706481934, Accuracy: 1.0, Computation time: 1.7681422233581543\n",
      "Step: 6439, Loss: 0.9159075617790222, Accuracy: 1.0, Computation time: 1.2309811115264893\n",
      "Step: 6440, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.250457763671875\n",
      "Step: 6441, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 1.018378496170044\n",
      "Step: 6442, Loss: 0.9160271286964417, Accuracy: 1.0, Computation time: 1.0603175163269043\n",
      "Step: 6443, Loss: 0.9159318208694458, Accuracy: 1.0, Computation time: 1.2935409545898438\n",
      "Step: 6444, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.1399657726287842\n",
      "Step: 6445, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 1.177704095840454\n",
      "Step: 6446, Loss: 0.9376402497291565, Accuracy: 0.9821428656578064, Computation time: 1.0156066417694092\n",
      "Step: 6447, Loss: 0.9376326203346252, Accuracy: 0.9722222089767456, Computation time: 1.2938649654388428\n",
      "Step: 6448, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 1.127551555633545\n",
      "Step: 6449, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 1.0050320625305176\n",
      "Step: 6450, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.8102989196777344\n",
      "Step: 6451, Loss: 0.915939211845398, Accuracy: 1.0, Computation time: 1.471590518951416\n",
      "Step: 6452, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 1.4456164836883545\n",
      "Step: 6453, Loss: 0.9163106083869934, Accuracy: 1.0, Computation time: 1.1429967880249023\n",
      "Step: 6454, Loss: 0.9158870577812195, Accuracy: 1.0, Computation time: 1.2742819786071777\n",
      "Step: 6455, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.0361549854278564\n",
      "Step: 6456, Loss: 0.9159404635429382, Accuracy: 1.0, Computation time: 1.0180246829986572\n",
      "Step: 6457, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.0823874473571777\n",
      "Step: 6458, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 1.0937659740447998\n",
      "Step: 6459, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.4628145694732666\n",
      "Step: 6460, Loss: 0.9377448558807373, Accuracy: 0.9791666865348816, Computation time: 1.1651005744934082\n",
      "Step: 6461, Loss: 0.9158712029457092, Accuracy: 1.0, Computation time: 1.1422500610351562\n",
      "Step: 6462, Loss: 0.9162769317626953, Accuracy: 1.0, Computation time: 1.2615537643432617\n",
      "Step: 6463, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.270230770111084\n",
      "Step: 6464, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 0.8899333477020264\n",
      "Step: 6465, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 1.065999984741211\n",
      "Step: 6466, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.6965444087982178\n",
      "Step: 6467, Loss: 0.9375422596931458, Accuracy: 0.9750000238418579, Computation time: 1.1524348258972168\n",
      "Step: 6468, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.082780122756958\n",
      "Step: 6469, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.1329245567321777\n",
      "Step: 6470, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.040409803390503\n",
      "Step: 6471, Loss: 0.9160569906234741, Accuracy: 1.0, Computation time: 1.167121410369873\n",
      "Step: 6472, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.0726022720336914\n",
      "Step: 6473, Loss: 0.9160717129707336, Accuracy: 1.0, Computation time: 1.1691358089447021\n",
      "Step: 6474, Loss: 0.9186335802078247, Accuracy: 1.0, Computation time: 1.3368680477142334\n",
      "Step: 6475, Loss: 0.9376274347305298, Accuracy: 0.9772727489471436, Computation time: 1.0719411373138428\n",
      "Step: 6476, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 1.1034417152404785\n",
      "Step: 6477, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.3981056213378906\n",
      "Step: 6478, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.327463150024414\n",
      "Step: 6479, Loss: 0.9375395178794861, Accuracy: 0.9750000238418579, Computation time: 1.5212230682373047\n",
      "Step: 6480, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.0009651184082031\n",
      "Step: 6481, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 1.404862880706787\n",
      "Step: 6482, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.6152324676513672\n",
      "Step: 6483, Loss: 0.9160358905792236, Accuracy: 1.0, Computation time: 0.9449915885925293\n",
      "Step: 6484, Loss: 0.9158920049667358, Accuracy: nan, Computation time: 0.9181196689605713\n",
      "Step: 6485, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.410353660583496\n",
      "Step: 6486, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.129606008529663\n",
      "Step: 6487, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.0255963802337646\n",
      "Step: 6488, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 1.0502080917358398\n",
      "Step: 6489, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.1082618236541748\n",
      "Step: 6490, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.079312801361084\n",
      "Step: 6491, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.1178786754608154\n",
      "Step: 6492, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 1.4831717014312744\n",
      "Step: 6493, Loss: 0.9161603450775146, Accuracy: 1.0, Computation time: 1.339742660522461\n",
      "Step: 6494, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 0.9410233497619629\n",
      "Step: 6495, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 1.0314538478851318\n",
      "Step: 6496, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.2295186519622803\n",
      "Step: 6497, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.2164177894592285\n",
      "Step: 6498, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.1745612621307373\n",
      "Step: 6499, Loss: 0.9162437319755554, Accuracy: 1.0, Computation time: 1.3114545345306396\n",
      "Step: 6500, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 1.5728073120117188\n",
      "Step: 6501, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 1.1756305694580078\n",
      "Step: 6502, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 1.3460679054260254\n",
      "Step: 6503, Loss: 0.9160346984863281, Accuracy: 1.0, Computation time: 1.6991267204284668\n",
      "Step: 6504, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.22829008102417\n",
      "Step: 6505, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.1330389976501465\n",
      "Step: 6506, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 1.1517388820648193\n",
      "Step: 6507, Loss: 0.9592026472091675, Accuracy: 0.935606062412262, Computation time: 1.4060771465301514\n",
      "Step: 6508, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.1669480800628662\n",
      "Step: 6509, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.1629080772399902\n",
      "Step: 6510, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.6291580200195312\n",
      "Step: 6511, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.0939092636108398\n",
      "Step: 6512, Loss: 0.937491774559021, Accuracy: 0.96875, Computation time: 1.57462739944458\n",
      "Step: 6513, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.6008288860321045\n",
      "Step: 6514, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.6252665519714355\n",
      "Step: 6515, Loss: 0.9159042239189148, Accuracy: 1.0, Computation time: 1.6365983486175537\n",
      "Step: 6516, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 1.2509911060333252\n",
      "Step: 6517, Loss: 0.915913999080658, Accuracy: 1.0, Computation time: 1.1951723098754883\n",
      "Step: 6518, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.0828843116760254\n",
      "Step: 6519, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.0946660041809082\n",
      "Step: 6520, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 1.1187829971313477\n",
      "Step: 6521, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 1.0554959774017334\n",
      "Step: 6522, Loss: 0.9158380627632141, Accuracy: 1.0, Computation time: 1.4382028579711914\n",
      "Step: 6523, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 1.3132152557373047\n",
      "Step: 6524, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.0170350074768066\n",
      "Step: 6525, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.1556339263916016\n",
      "Step: 6526, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 1.2729218006134033\n",
      "Step: 6527, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.0196270942687988\n",
      "Step: 6528, Loss: 0.9167840480804443, Accuracy: 1.0, Computation time: 1.1734776496887207\n",
      "Step: 6529, Loss: 0.9158434271812439, Accuracy: 1.0, Computation time: 1.0930609703063965\n",
      "Step: 6530, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 1.086127758026123\n",
      "Step: 6531, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 1.3408739566802979\n",
      "########################\n",
      "Test loss: 1.0716086626052856, Test Accuracy_epoch47: 0.7641208171844482\n",
      "########################\n",
      "Step: 6532, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.1144986152648926\n",
      "Step: 6533, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 0.98093581199646\n",
      "Step: 6534, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.2583580017089844\n",
      "Step: 6535, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 1.644819974899292\n",
      "Step: 6536, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.2650699615478516\n",
      "Step: 6537, Loss: 0.9250587224960327, Accuracy: 1.0, Computation time: 1.077622652053833\n",
      "Step: 6538, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 1.3999149799346924\n",
      "Step: 6539, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 1.1858985424041748\n",
      "Step: 6540, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 1.0439810752868652\n",
      "Step: 6541, Loss: 0.9160711169242859, Accuracy: 1.0, Computation time: 1.6142010688781738\n",
      "Step: 6542, Loss: 0.9375157356262207, Accuracy: 0.9833333492279053, Computation time: 1.4368999004364014\n",
      "Step: 6543, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 1.523808240890503\n",
      "Step: 6544, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.1641759872436523\n",
      "Step: 6545, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.0824952125549316\n",
      "Step: 6546, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 1.4343481063842773\n",
      "Step: 6547, Loss: 0.9375686049461365, Accuracy: 0.9750000238418579, Computation time: 1.1941230297088623\n",
      "Step: 6548, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 1.0353269577026367\n",
      "Step: 6549, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.251741886138916\n",
      "Step: 6550, Loss: 0.9374973177909851, Accuracy: 0.9791666865348816, Computation time: 1.1409800052642822\n",
      "Step: 6551, Loss: 0.9159601926803589, Accuracy: 1.0, Computation time: 1.161954402923584\n",
      "Step: 6552, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 0.9862234592437744\n",
      "Step: 6553, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.0647006034851074\n",
      "Step: 6554, Loss: 0.9375187754631042, Accuracy: 0.9750000238418579, Computation time: 0.9746034145355225\n",
      "Step: 6555, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 1.0343003273010254\n",
      "Step: 6556, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.0446031093597412\n",
      "Step: 6557, Loss: 0.9374872446060181, Accuracy: 0.949999988079071, Computation time: 1.1379365921020508\n",
      "Step: 6558, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 0.906308650970459\n",
      "Step: 6559, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 1.3081464767456055\n",
      "Step: 6560, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.1347320079803467\n",
      "Step: 6561, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 0.9322676658630371\n",
      "Step: 6562, Loss: 0.9158603549003601, Accuracy: 1.0, Computation time: 1.1171159744262695\n",
      "Step: 6563, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 1.0549347400665283\n",
      "Step: 6564, Loss: 0.9161288738250732, Accuracy: 1.0, Computation time: 1.1827640533447266\n",
      "Step: 6565, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.2904205322265625\n",
      "Step: 6566, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 1.6117162704467773\n",
      "Step: 6567, Loss: 0.9158447980880737, Accuracy: 1.0, Computation time: 1.0002405643463135\n",
      "Step: 6568, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.143141269683838\n",
      "Step: 6569, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.1788914203643799\n",
      "Step: 6570, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.130549669265747\n",
      "Step: 6571, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.0515239238739014\n",
      "Step: 6572, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.0444915294647217\n",
      "Step: 6573, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.0367364883422852\n",
      "Step: 6574, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.2919094562530518\n",
      "Step: 6575, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 0.8827576637268066\n",
      "Step: 6576, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.0831987857818604\n",
      "Step: 6577, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.0936765670776367\n",
      "Step: 6578, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.0365307331085205\n",
      "Step: 6579, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.0070171356201172\n",
      "Step: 6580, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.992173433303833\n",
      "Step: 6581, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.1346721649169922\n",
      "Step: 6582, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 1.4670295715332031\n",
      "Step: 6583, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.0745353698730469\n",
      "Step: 6584, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 1.0523717403411865\n",
      "Step: 6585, Loss: 0.9374833106994629, Accuracy: 0.9750000238418579, Computation time: 1.124164342880249\n",
      "Step: 6586, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.0174720287322998\n",
      "Step: 6587, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.3180723190307617\n",
      "Step: 6588, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.5872457027435303\n",
      "Step: 6589, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.4570417404174805\n",
      "Step: 6590, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.32594633102417\n",
      "Step: 6591, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 1.491337776184082\n",
      "Step: 6592, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.2483632564544678\n",
      "Step: 6593, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.1348483562469482\n",
      "Step: 6594, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 1.0936496257781982\n",
      "Step: 6595, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.0946662425994873\n",
      "Step: 6596, Loss: 0.9158443808555603, Accuracy: 1.0, Computation time: 0.9508190155029297\n",
      "Step: 6597, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.3034791946411133\n",
      "Step: 6598, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.588911533355713\n",
      "Step: 6599, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 1.0377540588378906\n",
      "Step: 6600, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.0110487937927246\n",
      "Step: 6601, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.0445826053619385\n",
      "Step: 6602, Loss: 0.9158448576927185, Accuracy: 1.0, Computation time: 1.209137201309204\n",
      "Step: 6603, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 1.1499054431915283\n",
      "Step: 6604, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.2483711242675781\n",
      "Step: 6605, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.1462619304656982\n",
      "Step: 6606, Loss: 0.9374903440475464, Accuracy: 0.9807692766189575, Computation time: 1.1368463039398193\n",
      "Step: 6607, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.2447872161865234\n",
      "Step: 6608, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.0884010791778564\n",
      "Step: 6609, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.1868643760681152\n",
      "Step: 6610, Loss: 0.9159616231918335, Accuracy: 1.0, Computation time: 1.2929649353027344\n",
      "Step: 6611, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 1.0804193019866943\n",
      "Step: 6612, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 1.1426396369934082\n",
      "Step: 6613, Loss: 0.9375941157341003, Accuracy: 0.9807692766189575, Computation time: 1.0688376426696777\n",
      "Step: 6614, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.072005271911621\n",
      "Step: 6615, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.042494297027588\n",
      "Step: 6616, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.092390775680542\n",
      "Step: 6617, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.6547021865844727\n",
      "Step: 6618, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.1549012660980225\n",
      "Step: 6619, Loss: 0.9158425331115723, Accuracy: 1.0, Computation time: 1.1429007053375244\n",
      "Step: 6620, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.183394432067871\n",
      "Step: 6621, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.3638861179351807\n",
      "Step: 6622, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 0.9250216484069824\n",
      "Step: 6623, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.065222978591919\n",
      "Step: 6624, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.1207654476165771\n",
      "Step: 6625, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.0935816764831543\n",
      "Step: 6626, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.3030924797058105\n",
      "Step: 6627, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.1083123683929443\n",
      "Step: 6628, Loss: 0.9158410429954529, Accuracy: 1.0, Computation time: 1.087303638458252\n",
      "Step: 6629, Loss: 0.9158389568328857, Accuracy: 1.0, Computation time: 1.0972347259521484\n",
      "Step: 6630, Loss: 0.9159790277481079, Accuracy: 1.0, Computation time: 1.19584059715271\n",
      "Step: 6631, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.1278343200683594\n",
      "Step: 6632, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.2556979656219482\n",
      "Step: 6633, Loss: 0.9158386588096619, Accuracy: 1.0, Computation time: 1.1735684871673584\n",
      "Step: 6634, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.9812581539154053\n",
      "Step: 6635, Loss: 0.915841817855835, Accuracy: nan, Computation time: 1.0184590816497803\n",
      "Step: 6636, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 0.9638888835906982\n",
      "Step: 6637, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.1277940273284912\n",
      "Step: 6638, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.2318902015686035\n",
      "Step: 6639, Loss: 0.9158355593681335, Accuracy: 1.0, Computation time: 1.295893669128418\n",
      "Step: 6640, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 1.1019184589385986\n",
      "Step: 6641, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 1.2797791957855225\n",
      "Step: 6642, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 1.2452633380889893\n",
      "Step: 6643, Loss: 0.9375376105308533, Accuracy: 0.9750000238418579, Computation time: 1.291107177734375\n",
      "Step: 6644, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.1377530097961426\n",
      "Step: 6645, Loss: 0.9158364534378052, Accuracy: 1.0, Computation time: 1.1517093181610107\n",
      "Step: 6646, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 1.3934078216552734\n",
      "Step: 6647, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.3466713428497314\n",
      "Step: 6648, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.0506489276885986\n",
      "Step: 6649, Loss: 0.9158438444137573, Accuracy: 1.0, Computation time: 1.3253846168518066\n",
      "Step: 6650, Loss: 0.9159786701202393, Accuracy: 1.0, Computation time: 1.0774445533752441\n",
      "Step: 6651, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.102311134338379\n",
      "Step: 6652, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 1.116870641708374\n",
      "Step: 6653, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.0205764770507812\n",
      "Step: 6654, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.0298223495483398\n",
      "Step: 6655, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.059352159500122\n",
      "Step: 6656, Loss: 0.9375369548797607, Accuracy: 0.9791666865348816, Computation time: 1.236617088317871\n",
      "Step: 6657, Loss: 0.9596495032310486, Accuracy: 0.925000011920929, Computation time: 1.3146662712097168\n",
      "Step: 6658, Loss: 0.9158360362052917, Accuracy: 1.0, Computation time: 1.1703989505767822\n",
      "Step: 6659, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.4535455703735352\n",
      "Step: 6660, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 1.1288375854492188\n",
      "Step: 6661, Loss: 0.9162577390670776, Accuracy: 1.0, Computation time: 1.4267172813415527\n",
      "Step: 6662, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 1.3624742031097412\n",
      "Step: 6663, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.090700387954712\n",
      "Step: 6664, Loss: 0.9374507069587708, Accuracy: 0.9807692766189575, Computation time: 1.6632978916168213\n",
      "Step: 6665, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.2492263317108154\n",
      "Step: 6666, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.1295557022094727\n",
      "Step: 6667, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.2101037502288818\n",
      "Step: 6668, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 1.1140530109405518\n",
      "Step: 6669, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.1101839542388916\n",
      "########################\n",
      "Test loss: 1.0723068714141846, Test Accuracy_epoch48: 0.7651878595352173\n",
      "########################\n",
      "Step: 6670, Loss: 0.9158357381820679, Accuracy: 1.0, Computation time: 1.0273005962371826\n",
      "Step: 6671, Loss: 0.9158399105072021, Accuracy: 1.0, Computation time: 1.2682161331176758\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 98\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataset_size, dataset_test_size, batch_size, lr, hidden_hidden_channels, num_hidden_layers, steps, hidden_size, width_size, depth, seed, dropout_rate)\u001b[0m\n\u001b[1;32m     96\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     97\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 98\u001b[0m bxe, acc, model, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mmake_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbxe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Computation time: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/jaxpy39/lib/python3.9/site-packages/equinox/_jit.py:162\u001b[0m, in \u001b[0;36m_JitWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m XlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# Catch Equinox's runtime errors, and strip the more intimidating parts of\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;66;03m# the error message.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/jaxpy39/lib/python3.9/site-packages/equinox/_module.py:861\u001b[0m, in \u001b[0;36mBoundMethod.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__func__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__self__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/jaxpy39/lib/python3.9/site-packages/equinox/_jit.py:157\u001b[0m, in \u001b[0;36m_JitWrapper._call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached(dynamic, static)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _postprocess(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/jaxpy39/lib/python3.9/site-packages/equinox/_module.py:719\u001b[0m, in \u001b[0;36m_unflatten_module\u001b[0;34m(cls, aux, dynamic_field_values)\u001b[0m\n\u001b[1;32m    709\u001b[0m     aux \u001b[38;5;241m=\u001b[39m _FlattenedData(\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(dynamic_field_names),\n\u001b[1;32m    711\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(static_field_names),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(wrapper_field_values),\n\u001b[1;32m    715\u001b[0m     )\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(dynamic_field_values), aux\n\u001b[0;32m--> 719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unflatten_module\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m], aux: _FlattenedData, dynamic_field_values):\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# This doesn't go via `__init__`. A user may have done something nontrivial there,\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# and the field values may be dummy values as used in various places throughout JAX.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# See also\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# https://jax.readthedocs.io/en/latest/pytrees.html#custom-pytrees-and-initialization,\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m# which was (I believe) inspired by Equinox's approach here.\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(aux\u001b[38;5;241m.\u001b[39mdynamic_field_names, dynamic_field_values):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5797e3-7c51-450f-81a7-36dc76dcdc07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
