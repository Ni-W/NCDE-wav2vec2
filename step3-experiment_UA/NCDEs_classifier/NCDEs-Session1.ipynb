{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2942c3-3afd-4cf3-9174-d5f98bfff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4351d28-4af4-437b-a8cc-b0a35cc21810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session1_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc0ff5c-6db2-4fd7-a2c2-528cc79ec706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4446, 256, 768) (4446,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last2, wav2vec_last3, wav2vec_last4, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last2,label_last3,label_last4,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7efa41-b81f-46c5-95c5-e3795a7fc776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afacc760-13e0-43ab-ae32-2707cc73018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3c6fbe-39f0-4193-919e-837984bafbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        #先将数据流降维再放入模型中训练\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0) \n",
    "            pred_var=prediction.var(axis=0)   \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)    \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ae516d-2dd8-4dde-ac95-c048490fb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3970321-4a45-4ec5-ad66-772d11a9febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last1], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last1\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76eaf028-8d89-4312-a31e-ba80fc0b9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903cb7ff-127c-443b-bf47-ca4b0e0c80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ac5595-4b4d-4b24-ae4c-3ea1cacf7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4446,\n",
    "    dataset_test_size=1085,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=2456,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    import jax.numpy as jnp\n",
    "    from jax import jit\n",
    "    def calculate_confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "        true_labels = true_labels.astype(jnp.int32)\n",
    "        pred_labels = pred_labels.astype(jnp.int32)\n",
    "        conf_matrix = jnp.zeros((num_classes, num_classes), dtype=jnp.int32)\n",
    "        for t, p in zip(true_labels, pred_labels):\n",
    "            conf_matrix = conf_matrix.at[t, p].add(1)\n",
    "        return conf_matrix\n",
    "\n",
    "    @jit\n",
    "    def calculate_ua(conf_matrix):\n",
    "        class_accuracy = jnp.diag(conf_matrix) / jnp.sum(conf_matrix, axis=1)\n",
    "        UA = jnp.mean(class_accuracy)\n",
    "        return UA\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        conf_matrix = calculate_confusion_matrix(label_i, pred, num_classes=4)\n",
    "        UA = calculate_ua(conf_matrix)\n",
    "        return UA\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        #y_pred = jnp.array(pred)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')   \n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878317f-af44-4632-8f3a-a00da9852fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.4311494827270508, Accuracy: 0.25, Computation time: 13.052272081375122\n",
      "Step: 1, Loss: 1.4252451658248901, Accuracy: 0.25, Computation time: 2.9613564014434814\n",
      "Step: 2, Loss: 1.4722480773925781, Accuracy: 0.25, Computation time: 3.0413146018981934\n",
      "Step: 3, Loss: 1.4022595882415771, Accuracy: 0.30000001192092896, Computation time: 3.398405075073242\n",
      "Step: 4, Loss: 1.3898096084594727, Accuracy: 0.5333333611488342, Computation time: 2.280250072479248\n",
      "Step: 5, Loss: 1.4013949632644653, Accuracy: 0.25, Computation time: 3.2327046394348145\n",
      "Step: 6, Loss: 1.4528671503067017, Accuracy: 0.25, Computation time: 2.991748809814453\n",
      "Step: 7, Loss: 1.42206871509552, Accuracy: 0.25, Computation time: 3.0423452854156494\n",
      "Step: 8, Loss: 1.3643641471862793, Accuracy: 0.25, Computation time: 2.805729627609253\n",
      "Step: 9, Loss: 1.3031450510025024, Accuracy: 0.25, Computation time: 2.3586668968200684\n",
      "Step: 10, Loss: 1.3643276691436768, Accuracy: 0.5, Computation time: 3.0676989555358887\n",
      "Step: 11, Loss: 1.4480628967285156, Accuracy: 0.25, Computation time: 2.544748067855835\n",
      "Step: 12, Loss: 1.269354224205017, Accuracy: 0.5, Computation time: 2.405519485473633\n",
      "Step: 13, Loss: 1.3211591243743896, Accuracy: 0.5, Computation time: 2.5881993770599365\n",
      "Step: 14, Loss: 1.3004165887832642, Accuracy: 0.5, Computation time: 2.6397054195404053\n",
      "Step: 15, Loss: 1.2275875806808472, Accuracy: 0.5, Computation time: 2.809877634048462\n",
      "Step: 16, Loss: 1.2365927696228027, Accuracy: 0.5, Computation time: 3.962155818939209\n",
      "Step: 17, Loss: 1.278512716293335, Accuracy: 0.5, Computation time: 2.879453420639038\n",
      "Step: 18, Loss: 1.0551226139068604, Accuracy: 0.75, Computation time: 2.9351248741149902\n",
      "Step: 19, Loss: 1.0835390090942383, Accuracy: 0.75, Computation time: 2.6751158237457275\n",
      "Step: 20, Loss: 1.0932121276855469, Accuracy: 0.7333333492279053, Computation time: 3.1435961723327637\n",
      "Step: 21, Loss: 1.1252282857894897, Accuracy: 0.75, Computation time: 2.8475301265716553\n",
      "Step: 22, Loss: 1.1055424213409424, Accuracy: 0.7727272510528564, Computation time: 2.869264602661133\n",
      "Step: 23, Loss: 1.0730605125427246, Accuracy: 0.8892857432365417, Computation time: 2.951411008834839\n",
      "Step: 24, Loss: 1.0172197818756104, Accuracy: 0.9772727489471436, Computation time: 3.2682535648345947\n",
      "Step: 25, Loss: 0.978370726108551, Accuracy: 1.0, Computation time: 3.6312477588653564\n",
      "Step: 26, Loss: 1.005716323852539, Accuracy: 1.0, Computation time: 3.1196131706237793\n",
      "Step: 27, Loss: 1.0108680725097656, Accuracy: 0.942307710647583, Computation time: 3.047534704208374\n",
      "Step: 28, Loss: 0.9951657652854919, Accuracy: 0.96875, Computation time: 3.405360698699951\n",
      "Step: 29, Loss: 0.9798375964164734, Accuracy: 0.9272727370262146, Computation time: 3.598215103149414\n",
      "Step: 30, Loss: 0.9698197245597839, Accuracy: 0.949999988079071, Computation time: 2.803142786026001\n",
      "Step: 31, Loss: 0.9739822149276733, Accuracy: 0.949999988079071, Computation time: 3.2901148796081543\n",
      "Step: 32, Loss: 0.9343714118003845, Accuracy: 1.0, Computation time: 3.227717638015747\n",
      "Step: 33, Loss: 0.9877347350120544, Accuracy: 0.96875, Computation time: 3.476865530014038\n",
      "Step: 34, Loss: 0.9663472175598145, Accuracy: 0.9615384340286255, Computation time: 3.122187614440918\n",
      "Step: 35, Loss: 0.9705859422683716, Accuracy: 0.9583333730697632, Computation time: 3.1051268577575684\n",
      "Step: 36, Loss: 0.9275840520858765, Accuracy: 1.0, Computation time: 2.9996464252471924\n",
      "Step: 37, Loss: 0.9352527856826782, Accuracy: 1.0, Computation time: 2.7119219303131104\n",
      "Step: 38, Loss: 0.9501588940620422, Accuracy: 0.96875, Computation time: 3.393947124481201\n",
      "Step: 39, Loss: 0.9268126487731934, Accuracy: 1.0, Computation time: 2.526470422744751\n",
      "Step: 40, Loss: 0.9456766843795776, Accuracy: 0.9833333492279053, Computation time: 3.2140231132507324\n",
      "Step: 41, Loss: 0.923302173614502, Accuracy: 1.0, Computation time: 2.873483657836914\n",
      "Step: 42, Loss: 0.9240654706954956, Accuracy: 1.0, Computation time: 2.4496562480926514\n",
      "Step: 43, Loss: 0.9233665466308594, Accuracy: 1.0, Computation time: 2.2613232135772705\n",
      "Step: 44, Loss: 0.9529768228530884, Accuracy: 0.9615384340286255, Computation time: 2.6314377784729004\n",
      "Step: 45, Loss: 0.925947904586792, Accuracy: 1.0, Computation time: 2.4770843982696533\n",
      "Step: 46, Loss: 0.9215394854545593, Accuracy: 1.0, Computation time: 2.8499557971954346\n",
      "Step: 47, Loss: 0.9192102551460266, Accuracy: 1.0, Computation time: 2.661958932876587\n",
      "Step: 48, Loss: 0.9277427196502686, Accuracy: 1.0, Computation time: 2.480741024017334\n",
      "Step: 49, Loss: 0.9221502542495728, Accuracy: 1.0, Computation time: 2.44328236579895\n",
      "Step: 50, Loss: 0.9184707999229431, Accuracy: 1.0, Computation time: 2.0313565731048584\n",
      "Step: 51, Loss: 0.9222390651702881, Accuracy: 1.0, Computation time: 2.341919183731079\n",
      "Step: 52, Loss: 0.919125497341156, Accuracy: 1.0, Computation time: 2.272402048110962\n",
      "Step: 53, Loss: 0.917995035648346, Accuracy: 1.0, Computation time: 2.3871536254882812\n",
      "Step: 54, Loss: 0.9187597632408142, Accuracy: 1.0, Computation time: 2.53682804107666\n",
      "Step: 55, Loss: 0.9191994071006775, Accuracy: 1.0, Computation time: 2.2082583904266357\n",
      "Step: 56, Loss: 0.9197551608085632, Accuracy: 1.0, Computation time: 2.019376277923584\n",
      "Step: 57, Loss: 0.9247208833694458, Accuracy: 1.0, Computation time: 1.9637911319732666\n",
      "Step: 58, Loss: 0.9302466511726379, Accuracy: 1.0, Computation time: 2.160414695739746\n",
      "Step: 59, Loss: 0.9186831116676331, Accuracy: 1.0, Computation time: 2.043917417526245\n",
      "Step: 60, Loss: 0.9389661550521851, Accuracy: 0.9772727489471436, Computation time: 2.104506492614746\n",
      "Step: 61, Loss: 0.9240270853042603, Accuracy: 1.0, Computation time: 2.0442848205566406\n",
      "Step: 62, Loss: 0.9202656745910645, Accuracy: 1.0, Computation time: 2.5226545333862305\n",
      "Step: 63, Loss: 0.9249522089958191, Accuracy: 1.0, Computation time: 1.8955237865447998\n",
      "Step: 64, Loss: 0.9202414751052856, Accuracy: 1.0, Computation time: 2.174984931945801\n",
      "Step: 65, Loss: 0.9433198571205139, Accuracy: 0.9791666865348816, Computation time: 2.0178678035736084\n",
      "Step: 66, Loss: 0.9590627551078796, Accuracy: 0.9583333730697632, Computation time: 2.0948102474212646\n",
      "Step: 67, Loss: 0.9239873290061951, Accuracy: 1.0, Computation time: 2.2012345790863037\n",
      "Step: 68, Loss: 0.9454843401908875, Accuracy: 0.9772727489471436, Computation time: 2.0513226985931396\n",
      "Step: 69, Loss: 0.9525526165962219, Accuracy: 0.9625000357627869, Computation time: 2.1872777938842773\n",
      "Step: 70, Loss: 0.9190079569816589, Accuracy: 1.0, Computation time: 2.0437936782836914\n",
      "Step: 71, Loss: 0.9237737655639648, Accuracy: 1.0, Computation time: 2.0579967498779297\n",
      "Step: 72, Loss: 0.9348201751708984, Accuracy: 0.9583333730697632, Computation time: 2.032736301422119\n",
      "Step: 73, Loss: 0.9399436116218567, Accuracy: 0.96875, Computation time: 2.184711456298828\n",
      "Step: 74, Loss: 0.9193229675292969, Accuracy: 1.0, Computation time: 2.6176652908325195\n",
      "Step: 75, Loss: 0.9203897714614868, Accuracy: 1.0, Computation time: 2.4072377681732178\n",
      "Step: 76, Loss: 0.9176183938980103, Accuracy: 1.0, Computation time: 2.1159064769744873\n",
      "Step: 77, Loss: 0.9251930713653564, Accuracy: 1.0, Computation time: 1.9281225204467773\n",
      "Step: 78, Loss: 0.9173877239227295, Accuracy: 1.0, Computation time: 2.1533901691436768\n",
      "Step: 79, Loss: 0.9284822940826416, Accuracy: 0.9791666865348816, Computation time: 2.468362331390381\n",
      "Step: 80, Loss: 0.9419388771057129, Accuracy: 0.9642857313156128, Computation time: 1.986400842666626\n",
      "Step: 81, Loss: 0.9204034805297852, Accuracy: 1.0, Computation time: 1.8928906917572021\n",
      "Step: 82, Loss: 0.9185985326766968, Accuracy: 1.0, Computation time: 2.1313838958740234\n",
      "Step: 83, Loss: 0.9181638956069946, Accuracy: 1.0, Computation time: 2.6954987049102783\n",
      "Step: 84, Loss: 0.9371540546417236, Accuracy: 0.9375, Computation time: 1.830054759979248\n",
      "Step: 85, Loss: 0.9178653955459595, Accuracy: 1.0, Computation time: 1.984804630279541\n",
      "Step: 86, Loss: 0.9182701110839844, Accuracy: 1.0, Computation time: 1.6939606666564941\n",
      "Step: 87, Loss: 0.9182466268539429, Accuracy: 1.0, Computation time: 2.1792941093444824\n",
      "Step: 88, Loss: 0.9181023836135864, Accuracy: 1.0, Computation time: 2.1249005794525146\n",
      "Step: 89, Loss: 0.9194066524505615, Accuracy: 1.0, Computation time: 2.146498918533325\n",
      "Step: 90, Loss: 0.9380548596382141, Accuracy: 0.9642857313156128, Computation time: 2.025651216506958\n",
      "Step: 91, Loss: 0.9178246259689331, Accuracy: 1.0, Computation time: 2.4599380493164062\n",
      "Step: 92, Loss: 0.9378439784049988, Accuracy: 0.9642857313156128, Computation time: 2.2963509559631348\n",
      "Step: 93, Loss: 0.917132556438446, Accuracy: 1.0, Computation time: 1.7577850818634033\n",
      "Step: 94, Loss: 0.9322193264961243, Accuracy: 0.9722222089767456, Computation time: 1.9731361865997314\n",
      "Step: 95, Loss: 0.9223240613937378, Accuracy: 1.0, Computation time: 2.431143283843994\n",
      "Step: 96, Loss: 0.9391887784004211, Accuracy: 0.9852941036224365, Computation time: 2.079392433166504\n",
      "Step: 97, Loss: 0.9340347051620483, Accuracy: 0.96875, Computation time: 2.165362596511841\n",
      "Step: 98, Loss: 0.9177696704864502, Accuracy: 1.0, Computation time: 2.1471755504608154\n",
      "Step: 99, Loss: 0.9180329442024231, Accuracy: 1.0, Computation time: 2.12990665435791\n",
      "Step: 100, Loss: 0.9190695285797119, Accuracy: 1.0, Computation time: 1.866455316543579\n",
      "Step: 101, Loss: 0.9185344576835632, Accuracy: 1.0, Computation time: 2.145570755004883\n",
      "Step: 102, Loss: 0.9195671081542969, Accuracy: 1.0, Computation time: 2.0042355060577393\n",
      "Step: 103, Loss: 0.9203292727470398, Accuracy: 1.0, Computation time: 2.4397966861724854\n",
      "Step: 104, Loss: 0.9170622229576111, Accuracy: 1.0, Computation time: 1.9251701831817627\n",
      "Step: 105, Loss: 0.9285397529602051, Accuracy: 1.0, Computation time: 2.4289939403533936\n",
      "Step: 106, Loss: 0.9168263077735901, Accuracy: 1.0, Computation time: 1.9320075511932373\n",
      "Step: 107, Loss: 0.919215977191925, Accuracy: 1.0, Computation time: 2.013430595397949\n",
      "Step: 108, Loss: 0.921048641204834, Accuracy: 1.0, Computation time: 2.0590474605560303\n",
      "Step: 109, Loss: 0.9362913966178894, Accuracy: 0.949999988079071, Computation time: 2.044630289077759\n",
      "Step: 110, Loss: 0.9177082180976868, Accuracy: 1.0, Computation time: 2.4356184005737305\n",
      "Step: 111, Loss: 0.9168689846992493, Accuracy: 1.0, Computation time: 2.0357825756073\n",
      "Step: 112, Loss: 0.9177343845367432, Accuracy: 1.0, Computation time: 2.371689558029175\n",
      "Step: 113, Loss: 0.9175502061843872, Accuracy: 1.0, Computation time: 2.030813455581665\n",
      "Step: 114, Loss: 0.9207245111465454, Accuracy: 1.0, Computation time: 2.3138954639434814\n",
      "Step: 115, Loss: 0.9284073114395142, Accuracy: 0.9772727489471436, Computation time: 2.2056872844696045\n",
      "Step: 116, Loss: 0.9173287749290466, Accuracy: 1.0, Computation time: 2.216003894805908\n",
      "Step: 117, Loss: 0.9282464385032654, Accuracy: 0.9722222089767456, Computation time: 2.241473436355591\n",
      "Step: 118, Loss: 0.9166672229766846, Accuracy: 1.0, Computation time: 2.2446329593658447\n",
      "Step: 119, Loss: 0.9167885184288025, Accuracy: 1.0, Computation time: 1.9122130870819092\n",
      "Step: 120, Loss: 0.9378604292869568, Accuracy: 0.9375, Computation time: 2.3464725017547607\n",
      "Step: 121, Loss: 0.9180246591567993, Accuracy: 1.0, Computation time: 2.187790870666504\n",
      "Step: 122, Loss: 0.9174267053604126, Accuracy: 1.0, Computation time: 1.861159324645996\n",
      "Step: 123, Loss: 0.9369134902954102, Accuracy: 0.9722222089767456, Computation time: 2.17238712310791\n",
      "Step: 124, Loss: 0.9229201078414917, Accuracy: 1.0, Computation time: 2.261936902999878\n",
      "Step: 125, Loss: 0.9178992509841919, Accuracy: 1.0, Computation time: 1.911529302597046\n",
      "Step: 126, Loss: 0.9174821972846985, Accuracy: 1.0, Computation time: 2.012300968170166\n",
      "Step: 127, Loss: 0.9170621633529663, Accuracy: 1.0, Computation time: 2.261920690536499\n",
      "Step: 128, Loss: 0.9180100560188293, Accuracy: 1.0, Computation time: 2.0460431575775146\n",
      "Step: 129, Loss: 0.9167581796646118, Accuracy: 1.0, Computation time: 1.856604814529419\n",
      "Step: 130, Loss: 0.941668689250946, Accuracy: 0.9642857313156128, Computation time: 2.05008602142334\n",
      "Step: 131, Loss: 0.9171064496040344, Accuracy: 1.0, Computation time: 1.6426146030426025\n",
      "Step: 132, Loss: 0.919039785861969, Accuracy: 1.0, Computation time: 2.0470283031463623\n",
      "Step: 133, Loss: 0.9174054861068726, Accuracy: 1.0, Computation time: 2.019808053970337\n",
      "Step: 134, Loss: 0.9195472002029419, Accuracy: 1.0, Computation time: 1.9581034183502197\n",
      "Step: 135, Loss: 0.9184831380844116, Accuracy: 1.0, Computation time: 2.0650954246520996\n",
      "Step: 136, Loss: 0.9181072115898132, Accuracy: 1.0, Computation time: 1.8906219005584717\n",
      "Step: 137, Loss: 0.9543102979660034, Accuracy: 0.9444444179534912, Computation time: 2.157261848449707\n",
      "Step: 138, Loss: 0.9181280136108398, Accuracy: 1.0, Computation time: 2.1206555366516113\n",
      "Step: 139, Loss: 0.9190123081207275, Accuracy: 1.0, Computation time: 2.026400566101074\n",
      "########################\n",
      "Test loss: 1.1254160404205322, Test Accuracy_epoch1: 0.7222479581832886\n",
      "########################\n",
      "Step: 140, Loss: 0.9175225496292114, Accuracy: 1.0, Computation time: 1.9663922786712646\n",
      "Step: 141, Loss: 0.919110119342804, Accuracy: 1.0, Computation time: 1.9274775981903076\n",
      "Step: 142, Loss: 0.9232816100120544, Accuracy: 1.0, Computation time: 1.8920702934265137\n",
      "Step: 143, Loss: 0.9173122048377991, Accuracy: 1.0, Computation time: 2.207148790359497\n",
      "Step: 144, Loss: 0.9171773791313171, Accuracy: 1.0, Computation time: 1.8203227519989014\n",
      "Step: 145, Loss: 0.9175885319709778, Accuracy: 1.0, Computation time: 1.7810697555541992\n",
      "Step: 146, Loss: 0.9215722680091858, Accuracy: 1.0, Computation time: 2.032977342605591\n",
      "Step: 147, Loss: 0.9233503341674805, Accuracy: 1.0, Computation time: 2.348301887512207\n",
      "Step: 148, Loss: 0.9237939119338989, Accuracy: 1.0, Computation time: 2.5223019123077393\n",
      "Step: 149, Loss: 0.9404098987579346, Accuracy: 0.9791666865348816, Computation time: 1.957756519317627\n",
      "Step: 150, Loss: 0.9204562902450562, Accuracy: 1.0, Computation time: 2.611060857772827\n",
      "Step: 151, Loss: 0.9170322418212891, Accuracy: 1.0, Computation time: 1.7590465545654297\n",
      "Step: 152, Loss: 0.9188661575317383, Accuracy: 1.0, Computation time: 1.9987666606903076\n",
      "Step: 153, Loss: 0.9180364608764648, Accuracy: 1.0, Computation time: 2.6303861141204834\n",
      "Step: 154, Loss: 0.9181089401245117, Accuracy: 1.0, Computation time: 1.992262840270996\n",
      "Step: 155, Loss: 0.9174259305000305, Accuracy: 1.0, Computation time: 2.1546013355255127\n",
      "Step: 156, Loss: 0.9176468253135681, Accuracy: 1.0, Computation time: 2.1541292667388916\n",
      "Step: 157, Loss: 0.9373913407325745, Accuracy: 0.9642857313156128, Computation time: 2.248566150665283\n",
      "Step: 158, Loss: 0.9545996785163879, Accuracy: 0.96875, Computation time: 2.139227867126465\n",
      "Step: 159, Loss: 0.9165230989456177, Accuracy: 1.0, Computation time: 1.9492988586425781\n",
      "Step: 160, Loss: 0.9199224710464478, Accuracy: 1.0, Computation time: 2.142869710922241\n",
      "Step: 161, Loss: 0.9173356890678406, Accuracy: 1.0, Computation time: 2.4102795124053955\n",
      "Step: 162, Loss: 0.9530804753303528, Accuracy: 0.9583333730697632, Computation time: 1.9526684284210205\n",
      "Step: 163, Loss: 0.9169982671737671, Accuracy: 1.0, Computation time: 1.8994271755218506\n",
      "Step: 164, Loss: 0.9353590607643127, Accuracy: 0.9772727489471436, Computation time: 2.0080461502075195\n",
      "Step: 165, Loss: 0.9186770915985107, Accuracy: 1.0, Computation time: 1.9712655544281006\n",
      "Step: 166, Loss: 0.9179494380950928, Accuracy: 1.0, Computation time: 1.847419261932373\n",
      "Step: 167, Loss: 0.9213717579841614, Accuracy: 1.0, Computation time: 1.817795991897583\n",
      "Step: 168, Loss: 0.9392160773277283, Accuracy: 0.9791666865348816, Computation time: 2.110961437225342\n",
      "Step: 169, Loss: 0.9391487836837769, Accuracy: 0.9642857313156128, Computation time: 2.052064895629883\n",
      "Step: 170, Loss: 0.9221141934394836, Accuracy: 1.0, Computation time: 1.882591962814331\n",
      "Step: 171, Loss: 0.9193007946014404, Accuracy: 1.0, Computation time: 2.1269664764404297\n",
      "Step: 172, Loss: 0.9338473081588745, Accuracy: 0.949999988079071, Computation time: 2.097200870513916\n",
      "Step: 173, Loss: 0.9172568917274475, Accuracy: 1.0, Computation time: 1.833315372467041\n",
      "Step: 174, Loss: 0.9214735627174377, Accuracy: 1.0, Computation time: 1.8117187023162842\n",
      "Step: 175, Loss: 0.9172185659408569, Accuracy: 1.0, Computation time: 1.9293208122253418\n",
      "Step: 176, Loss: 0.9228370785713196, Accuracy: 1.0, Computation time: 1.7475860118865967\n",
      "Step: 177, Loss: 0.945626974105835, Accuracy: 0.9583333730697632, Computation time: 1.8954286575317383\n",
      "Step: 178, Loss: 0.931247353553772, Accuracy: 0.984375, Computation time: 2.0631606578826904\n",
      "Step: 179, Loss: 0.9186880588531494, Accuracy: 1.0, Computation time: 1.925537109375\n",
      "Step: 180, Loss: 0.9253281950950623, Accuracy: 1.0, Computation time: 2.241546392440796\n",
      "Step: 181, Loss: 0.9170297384262085, Accuracy: 1.0, Computation time: 1.5876784324645996\n",
      "Step: 182, Loss: 0.9182567596435547, Accuracy: 1.0, Computation time: 1.947758674621582\n",
      "Step: 183, Loss: 0.9167178273200989, Accuracy: 1.0, Computation time: 2.0555331707000732\n",
      "Step: 184, Loss: 0.9184129238128662, Accuracy: 1.0, Computation time: 1.8523671627044678\n",
      "Step: 185, Loss: 0.9178211688995361, Accuracy: 1.0, Computation time: 1.6994061470031738\n",
      "Step: 186, Loss: 0.9333131909370422, Accuracy: 0.9722222089767456, Computation time: 1.9971270561218262\n",
      "Step: 187, Loss: 0.9324145317077637, Accuracy: 0.9750000238418579, Computation time: 2.1656272411346436\n",
      "Step: 188, Loss: 0.9165976047515869, Accuracy: 1.0, Computation time: 1.7123053073883057\n",
      "Step: 189, Loss: 0.917552649974823, Accuracy: 1.0, Computation time: 1.7190327644348145\n",
      "Step: 190, Loss: 0.9194331765174866, Accuracy: 1.0, Computation time: 1.794313907623291\n",
      "Step: 191, Loss: 0.9291635155677795, Accuracy: 1.0, Computation time: 1.9968187808990479\n",
      "Step: 192, Loss: 0.9279760122299194, Accuracy: 1.0, Computation time: 1.9601237773895264\n",
      "Step: 193, Loss: 0.916580319404602, Accuracy: 1.0, Computation time: 1.6829354763031006\n",
      "Step: 194, Loss: 0.9174681901931763, Accuracy: 1.0, Computation time: 2.0129575729370117\n",
      "Step: 195, Loss: 0.9178560972213745, Accuracy: 1.0, Computation time: 2.129992961883545\n",
      "Step: 196, Loss: 0.9214180707931519, Accuracy: 1.0, Computation time: 2.105294942855835\n",
      "Step: 197, Loss: 0.9205055236816406, Accuracy: 1.0, Computation time: 1.8137640953063965\n",
      "Step: 198, Loss: 0.9219487905502319, Accuracy: 1.0, Computation time: 1.9335410594940186\n",
      "Step: 199, Loss: 0.9220900535583496, Accuracy: 1.0, Computation time: 2.1947648525238037\n",
      "Step: 200, Loss: 0.9243406057357788, Accuracy: 1.0, Computation time: 2.360318183898926\n",
      "Step: 201, Loss: 0.9180603623390198, Accuracy: 1.0, Computation time: 1.9365911483764648\n",
      "Step: 202, Loss: 0.92405104637146, Accuracy: 1.0, Computation time: 2.2169578075408936\n",
      "Step: 203, Loss: 0.939310610294342, Accuracy: 0.9791666865348816, Computation time: 1.684685230255127\n",
      "Step: 204, Loss: 0.9324040412902832, Accuracy: 0.9583333730697632, Computation time: 1.8437931537628174\n",
      "Step: 205, Loss: 0.9178194999694824, Accuracy: 1.0, Computation time: 1.8770852088928223\n",
      "Step: 206, Loss: 0.9219551086425781, Accuracy: 1.0, Computation time: 1.9797945022583008\n",
      "Step: 207, Loss: 0.9256213903427124, Accuracy: 1.0, Computation time: 1.9140098094940186\n",
      "Step: 208, Loss: 0.9403736591339111, Accuracy: 0.9583333730697632, Computation time: 2.012626886367798\n",
      "Step: 209, Loss: 0.916680097579956, Accuracy: 1.0, Computation time: 1.7311410903930664\n",
      "Step: 210, Loss: 0.9203243851661682, Accuracy: nan, Computation time: 1.6241824626922607\n",
      "Step: 211, Loss: 0.9293957948684692, Accuracy: 0.9583333730697632, Computation time: 2.2338521480560303\n",
      "Step: 212, Loss: 0.917933464050293, Accuracy: 1.0, Computation time: 2.5082998275756836\n",
      "Step: 213, Loss: 0.9183288812637329, Accuracy: 1.0, Computation time: 1.7059071063995361\n",
      "Step: 214, Loss: 0.9386762380599976, Accuracy: 0.9791666865348816, Computation time: 2.2686681747436523\n",
      "Step: 215, Loss: 0.9209810495376587, Accuracy: 1.0, Computation time: 2.0400290489196777\n",
      "Step: 216, Loss: 0.9538670778274536, Accuracy: 0.9333333969116211, Computation time: 2.085134267807007\n",
      "Step: 217, Loss: 0.9171183705329895, Accuracy: 1.0, Computation time: 2.2224864959716797\n",
      "Step: 218, Loss: 0.9165945053100586, Accuracy: 1.0, Computation time: 2.186403512954712\n",
      "Step: 219, Loss: 0.9361786842346191, Accuracy: 0.9833333492279053, Computation time: 1.8989825248718262\n",
      "Step: 220, Loss: 0.9371097683906555, Accuracy: 0.96875, Computation time: 1.4873478412628174\n",
      "Step: 221, Loss: 0.916389524936676, Accuracy: 1.0, Computation time: 1.7853474617004395\n",
      "Step: 222, Loss: 0.9194848537445068, Accuracy: 1.0, Computation time: 1.8459227085113525\n",
      "Step: 223, Loss: 0.9165759682655334, Accuracy: 1.0, Computation time: 1.8069801330566406\n",
      "Step: 224, Loss: 0.9163295030593872, Accuracy: 1.0, Computation time: 1.9486682415008545\n",
      "Step: 225, Loss: 0.9278554916381836, Accuracy: 0.9642857313156128, Computation time: 1.8941283226013184\n",
      "Step: 226, Loss: 0.9197719693183899, Accuracy: 1.0, Computation time: 1.7189569473266602\n",
      "Step: 227, Loss: 0.9272364974021912, Accuracy: 0.9642857313156128, Computation time: 1.9363341331481934\n",
      "Step: 228, Loss: 0.9168144464492798, Accuracy: 1.0, Computation time: 2.1468427181243896\n",
      "Step: 229, Loss: 0.9167309999465942, Accuracy: 1.0, Computation time: 2.125598669052124\n",
      "Step: 230, Loss: 0.9418007731437683, Accuracy: 0.9807692766189575, Computation time: 1.9095773696899414\n",
      "Step: 231, Loss: 0.9161786437034607, Accuracy: 1.0, Computation time: 1.8033289909362793\n",
      "Step: 232, Loss: 0.9176221489906311, Accuracy: 1.0, Computation time: 1.8109734058380127\n",
      "Step: 233, Loss: 0.9171329736709595, Accuracy: 1.0, Computation time: 2.0683202743530273\n",
      "Step: 234, Loss: 0.927720308303833, Accuracy: 0.9722222089767456, Computation time: 1.8011424541473389\n",
      "Step: 235, Loss: 0.9189923405647278, Accuracy: 1.0, Computation time: 2.1078999042510986\n",
      "Step: 236, Loss: 0.9163596034049988, Accuracy: 1.0, Computation time: 1.9654736518859863\n",
      "Step: 237, Loss: 0.9178925156593323, Accuracy: 1.0, Computation time: 1.7221834659576416\n",
      "Step: 238, Loss: 0.9163562655448914, Accuracy: 1.0, Computation time: 1.9168503284454346\n",
      "Step: 239, Loss: 0.916844367980957, Accuracy: 1.0, Computation time: 2.339378595352173\n",
      "Step: 240, Loss: 0.9386952519416809, Accuracy: 0.9807692766189575, Computation time: 2.0301692485809326\n",
      "Step: 241, Loss: 0.9165932536125183, Accuracy: 1.0, Computation time: 2.1153883934020996\n",
      "Step: 242, Loss: 0.9171544313430786, Accuracy: 1.0, Computation time: 2.305335760116577\n",
      "Step: 243, Loss: 0.9307213425636292, Accuracy: 0.9722222089767456, Computation time: 1.8077406883239746\n",
      "Step: 244, Loss: 0.9161754250526428, Accuracy: 1.0, Computation time: 2.107912302017212\n",
      "Step: 245, Loss: 0.936147153377533, Accuracy: 0.9722222089767456, Computation time: 1.9078295230865479\n",
      "Step: 246, Loss: 0.9164270758628845, Accuracy: 1.0, Computation time: 1.7850184440612793\n",
      "Step: 247, Loss: 0.9320181608200073, Accuracy: 0.9807692766189575, Computation time: 1.9602863788604736\n",
      "Step: 248, Loss: 0.9260668754577637, Accuracy: 0.9750000238418579, Computation time: 2.1217188835144043\n",
      "Step: 249, Loss: 0.9174210429191589, Accuracy: 1.0, Computation time: 1.7649147510528564\n",
      "Step: 250, Loss: 0.9169328212738037, Accuracy: 1.0, Computation time: 1.9059720039367676\n",
      "Step: 251, Loss: 0.9169493317604065, Accuracy: 1.0, Computation time: 2.2501630783081055\n",
      "Step: 252, Loss: 0.9168341159820557, Accuracy: 1.0, Computation time: 1.6889946460723877\n",
      "Step: 253, Loss: 0.9396597146987915, Accuracy: 0.9772727489471436, Computation time: 1.7354910373687744\n",
      "Step: 254, Loss: 0.9199002385139465, Accuracy: 1.0, Computation time: 1.698798656463623\n",
      "Step: 255, Loss: 0.916747510433197, Accuracy: 1.0, Computation time: 1.9472770690917969\n",
      "Step: 256, Loss: 0.9179443717002869, Accuracy: 1.0, Computation time: 1.9810068607330322\n",
      "Step: 257, Loss: 0.9163230061531067, Accuracy: 1.0, Computation time: 1.884239673614502\n",
      "Step: 258, Loss: 0.9314839839935303, Accuracy: 0.9772727489471436, Computation time: 2.313107967376709\n",
      "Step: 259, Loss: 0.9383406639099121, Accuracy: 0.9750000238418579, Computation time: 1.8012416362762451\n",
      "Step: 260, Loss: 0.9165622591972351, Accuracy: 1.0, Computation time: 1.634800910949707\n",
      "Step: 261, Loss: 0.9168943166732788, Accuracy: 1.0, Computation time: 1.5552105903625488\n",
      "Step: 262, Loss: 0.9181698560714722, Accuracy: 1.0, Computation time: 1.8309156894683838\n",
      "Step: 263, Loss: 0.9174535274505615, Accuracy: 1.0, Computation time: 1.8629631996154785\n",
      "Step: 264, Loss: 0.9166602492332458, Accuracy: 1.0, Computation time: 1.6230103969573975\n",
      "Step: 265, Loss: 0.9168181419372559, Accuracy: 1.0, Computation time: 1.6476666927337646\n",
      "Step: 266, Loss: 0.9162284731864929, Accuracy: 1.0, Computation time: 1.7611610889434814\n",
      "Step: 267, Loss: 0.9348989129066467, Accuracy: 0.9772727489471436, Computation time: 1.9346144199371338\n",
      "Step: 268, Loss: 0.916401743888855, Accuracy: 1.0, Computation time: 1.824571132659912\n",
      "Step: 269, Loss: 0.9165612459182739, Accuracy: 1.0, Computation time: 1.9672760963439941\n",
      "Step: 270, Loss: 0.9217420816421509, Accuracy: 1.0, Computation time: 1.9044947624206543\n",
      "Step: 271, Loss: 0.917056679725647, Accuracy: 1.0, Computation time: 1.8373074531555176\n",
      "Step: 272, Loss: 0.9176571369171143, Accuracy: 1.0, Computation time: 2.0587832927703857\n",
      "Step: 273, Loss: 0.9162753820419312, Accuracy: 1.0, Computation time: 1.739877700805664\n",
      "Step: 274, Loss: 0.9170302748680115, Accuracy: 1.0, Computation time: 1.803231954574585\n",
      "Step: 275, Loss: 0.9164833426475525, Accuracy: 1.0, Computation time: 1.8236262798309326\n",
      "Step: 276, Loss: 0.9377892017364502, Accuracy: 0.9750000238418579, Computation time: 1.8100388050079346\n",
      "Step: 277, Loss: 0.9162307381629944, Accuracy: 1.0, Computation time: 1.9157462120056152\n",
      "Step: 278, Loss: 0.9376050233840942, Accuracy: 0.9791666865348816, Computation time: 1.5624973773956299\n",
      "########################\n",
      "Test loss: 1.119320273399353, Test Accuracy_epoch2: 0.7311297059059143\n",
      "########################\n",
      "Step: 279, Loss: 0.9162908792495728, Accuracy: 1.0, Computation time: 1.7763972282409668\n",
      "Step: 280, Loss: 0.9163581132888794, Accuracy: 1.0, Computation time: 1.7481095790863037\n",
      "Step: 281, Loss: 0.9382890462875366, Accuracy: 0.9750000238418579, Computation time: 2.0945589542388916\n",
      "Step: 282, Loss: 0.9165104627609253, Accuracy: 1.0, Computation time: 1.7994539737701416\n",
      "Step: 283, Loss: 0.9261619448661804, Accuracy: 1.0, Computation time: 1.8681602478027344\n",
      "Step: 284, Loss: 0.9164318442344666, Accuracy: 1.0, Computation time: 1.8147540092468262\n",
      "Step: 285, Loss: 0.9163532853126526, Accuracy: 1.0, Computation time: 1.904780626296997\n",
      "Step: 286, Loss: 0.9360880255699158, Accuracy: 0.9722222089767456, Computation time: 2.7620301246643066\n",
      "Step: 287, Loss: 0.9161645174026489, Accuracy: 1.0, Computation time: 1.906367301940918\n",
      "Step: 288, Loss: 0.9250701665878296, Accuracy: 1.0, Computation time: 2.1199960708618164\n",
      "Step: 289, Loss: 0.916422426700592, Accuracy: 1.0, Computation time: 2.3579440116882324\n",
      "Step: 290, Loss: 0.9163469672203064, Accuracy: 1.0, Computation time: 1.6395151615142822\n",
      "Step: 291, Loss: 0.9385509490966797, Accuracy: 0.9722222089767456, Computation time: 2.034010648727417\n",
      "Step: 292, Loss: 0.9160254001617432, Accuracy: 1.0, Computation time: 1.8592495918273926\n",
      "Step: 293, Loss: 0.9161263108253479, Accuracy: 1.0, Computation time: 1.6185133457183838\n",
      "Step: 294, Loss: 0.9175852537155151, Accuracy: 1.0, Computation time: 1.7825264930725098\n",
      "Step: 295, Loss: 0.9164402484893799, Accuracy: 1.0, Computation time: 2.2273380756378174\n",
      "Step: 296, Loss: 0.9384760856628418, Accuracy: 0.9807692766189575, Computation time: 1.891087293624878\n",
      "Step: 297, Loss: 0.9299321174621582, Accuracy: 1.0, Computation time: 2.026810646057129\n",
      "Step: 298, Loss: 0.9346808791160583, Accuracy: 0.9583333730697632, Computation time: 1.9968922138214111\n",
      "Step: 299, Loss: 0.9160705804824829, Accuracy: 1.0, Computation time: 1.9140527248382568\n",
      "Step: 300, Loss: 0.9206868410110474, Accuracy: 1.0, Computation time: 1.7762439250946045\n",
      "Step: 301, Loss: 0.9161602854728699, Accuracy: 1.0, Computation time: 1.7268791198730469\n",
      "Step: 302, Loss: 0.9317154288291931, Accuracy: 1.0, Computation time: 1.8618686199188232\n",
      "Step: 303, Loss: 0.9164708852767944, Accuracy: 1.0, Computation time: 1.953995704650879\n",
      "Step: 304, Loss: 0.9189391732215881, Accuracy: 1.0, Computation time: 1.7251801490783691\n",
      "Step: 305, Loss: 0.9164164066314697, Accuracy: 1.0, Computation time: 1.874938726425171\n",
      "Step: 306, Loss: 0.9203102588653564, Accuracy: 1.0, Computation time: 2.1412301063537598\n",
      "Step: 307, Loss: 0.9207778573036194, Accuracy: 1.0, Computation time: 2.165015697479248\n",
      "Step: 308, Loss: 0.9173088669776917, Accuracy: 1.0, Computation time: 1.7737250328063965\n",
      "Step: 309, Loss: 0.9168466329574585, Accuracy: 1.0, Computation time: 1.9900193214416504\n",
      "Step: 310, Loss: 0.9396740794181824, Accuracy: 0.9642857313156128, Computation time: 1.9068667888641357\n",
      "Step: 311, Loss: 0.9170477986335754, Accuracy: 1.0, Computation time: 1.9932866096496582\n",
      "Step: 312, Loss: 0.9179598093032837, Accuracy: 1.0, Computation time: 1.9228370189666748\n",
      "Step: 313, Loss: 0.916144609451294, Accuracy: 1.0, Computation time: 1.98227858543396\n",
      "Step: 314, Loss: 0.9168723225593567, Accuracy: 1.0, Computation time: 1.77455472946167\n",
      "Step: 315, Loss: 0.9164547324180603, Accuracy: 1.0, Computation time: 2.094069719314575\n",
      "Step: 316, Loss: 0.91681969165802, Accuracy: 1.0, Computation time: 1.947434663772583\n",
      "Step: 317, Loss: 0.9169727563858032, Accuracy: 1.0, Computation time: 1.8800241947174072\n",
      "Step: 318, Loss: 0.9164633750915527, Accuracy: 1.0, Computation time: 1.643622636795044\n",
      "Step: 319, Loss: 0.9166653752326965, Accuracy: 1.0, Computation time: 2.275996446609497\n",
      "Step: 320, Loss: 0.9164644479751587, Accuracy: 1.0, Computation time: 1.6496753692626953\n",
      "Step: 321, Loss: 0.9342145323753357, Accuracy: 0.96875, Computation time: 2.4222450256347656\n",
      "Step: 322, Loss: 0.9162052273750305, Accuracy: 1.0, Computation time: 1.6523730754852295\n",
      "Step: 323, Loss: 0.9162369966506958, Accuracy: 1.0, Computation time: 1.9451251029968262\n",
      "Step: 324, Loss: 0.9164930582046509, Accuracy: 1.0, Computation time: 1.9380168914794922\n",
      "Step: 325, Loss: 0.9173757433891296, Accuracy: 1.0, Computation time: 2.048084259033203\n",
      "Step: 326, Loss: 0.9164047241210938, Accuracy: 1.0, Computation time: 1.6730315685272217\n",
      "Step: 327, Loss: 0.916233241558075, Accuracy: 1.0, Computation time: 2.043888807296753\n",
      "Step: 328, Loss: 0.9170633554458618, Accuracy: 1.0, Computation time: 2.220924139022827\n",
      "Step: 329, Loss: 0.9381263852119446, Accuracy: 0.9821428656578064, Computation time: 1.5209252834320068\n",
      "Step: 330, Loss: 0.937942624092102, Accuracy: 0.9750000238418579, Computation time: 1.7095544338226318\n",
      "Step: 331, Loss: 0.9162046313285828, Accuracy: 1.0, Computation time: 1.791391372680664\n",
      "Step: 332, Loss: 0.9181069731712341, Accuracy: 1.0, Computation time: 1.7094974517822266\n",
      "Step: 333, Loss: 0.9165344834327698, Accuracy: 1.0, Computation time: 1.5795204639434814\n",
      "Step: 334, Loss: 0.9325012564659119, Accuracy: 0.9821428656578064, Computation time: 2.278676748275757\n",
      "Step: 335, Loss: 0.9161949753761292, Accuracy: 1.0, Computation time: 1.7291126251220703\n",
      "Step: 336, Loss: 0.9159993529319763, Accuracy: 1.0, Computation time: 1.9712979793548584\n",
      "Step: 337, Loss: 0.9173266291618347, Accuracy: 1.0, Computation time: 1.9199299812316895\n",
      "Step: 338, Loss: 0.9248018860816956, Accuracy: 1.0, Computation time: 1.754892349243164\n",
      "Step: 339, Loss: 0.9180271029472351, Accuracy: 1.0, Computation time: 1.9056859016418457\n",
      "Step: 340, Loss: 0.9168074131011963, Accuracy: 1.0, Computation time: 1.8159997463226318\n",
      "Step: 341, Loss: 0.9306272268295288, Accuracy: 0.9722222089767456, Computation time: 2.0201656818389893\n",
      "Step: 342, Loss: 0.9168713092803955, Accuracy: 1.0, Computation time: 1.735426664352417\n",
      "Step: 343, Loss: 0.9246713519096375, Accuracy: 1.0, Computation time: 1.7967004776000977\n",
      "Step: 344, Loss: 0.9191507697105408, Accuracy: 1.0, Computation time: 1.677565336227417\n",
      "Step: 345, Loss: 0.9165803790092468, Accuracy: 1.0, Computation time: 2.1192877292633057\n",
      "Step: 346, Loss: 0.9185335636138916, Accuracy: 1.0, Computation time: 1.7042772769927979\n",
      "Step: 347, Loss: 0.9534440040588379, Accuracy: 0.90625, Computation time: 1.946239948272705\n",
      "Step: 348, Loss: 0.9211217761039734, Accuracy: 1.0, Computation time: 2.324456214904785\n",
      "Step: 349, Loss: 0.9171849489212036, Accuracy: 1.0, Computation time: 2.006667137145996\n",
      "Step: 350, Loss: 0.9166321754455566, Accuracy: 1.0, Computation time: 1.7953836917877197\n",
      "Step: 351, Loss: 0.9176440238952637, Accuracy: 1.0, Computation time: 1.7898085117340088\n",
      "Step: 352, Loss: 0.9177730083465576, Accuracy: 1.0, Computation time: 1.89080810546875\n",
      "Step: 353, Loss: 0.9439022541046143, Accuracy: 0.9583333730697632, Computation time: 1.7814562320709229\n",
      "Step: 354, Loss: 0.9174073934555054, Accuracy: 1.0, Computation time: 2.0417587757110596\n",
      "Step: 355, Loss: 0.9183027744293213, Accuracy: 1.0, Computation time: 2.074432611465454\n",
      "Step: 356, Loss: 0.9178274273872375, Accuracy: 1.0, Computation time: 2.070720911026001\n",
      "Step: 357, Loss: 0.939159095287323, Accuracy: 0.9583333730697632, Computation time: 2.2648324966430664\n",
      "Step: 358, Loss: 0.9173438549041748, Accuracy: 1.0, Computation time: 2.213273286819458\n",
      "Step: 359, Loss: 0.9164688587188721, Accuracy: 1.0, Computation time: 2.124009370803833\n",
      "Step: 360, Loss: 0.9165964126586914, Accuracy: 1.0, Computation time: 1.7958838939666748\n",
      "Step: 361, Loss: 0.9163281917572021, Accuracy: 1.0, Computation time: 2.0863800048828125\n",
      "Step: 362, Loss: 0.9166341423988342, Accuracy: 1.0, Computation time: 1.7157423496246338\n",
      "Step: 363, Loss: 0.9166398048400879, Accuracy: 1.0, Computation time: 1.9906346797943115\n",
      "Step: 364, Loss: 0.9179636836051941, Accuracy: 1.0, Computation time: 2.508873462677002\n",
      "Step: 365, Loss: 0.9164284467697144, Accuracy: 1.0, Computation time: 1.9389839172363281\n",
      "Step: 366, Loss: 0.9174298644065857, Accuracy: 1.0, Computation time: 1.652360200881958\n",
      "Step: 367, Loss: 0.9191519618034363, Accuracy: 1.0, Computation time: 2.0917820930480957\n",
      "Step: 368, Loss: 0.9298600554466248, Accuracy: 0.9375, Computation time: 2.7485697269439697\n",
      "Step: 369, Loss: 0.918628990650177, Accuracy: 1.0, Computation time: 2.1680283546447754\n",
      "Step: 370, Loss: 0.9176005721092224, Accuracy: 1.0, Computation time: 1.6426551342010498\n",
      "Step: 371, Loss: 0.9178012013435364, Accuracy: 1.0, Computation time: 2.1022372245788574\n",
      "Step: 372, Loss: 0.9312906861305237, Accuracy: 0.9821428656578064, Computation time: 2.116450786590576\n",
      "Step: 373, Loss: 0.9417213797569275, Accuracy: 0.9583333730697632, Computation time: 2.0577821731567383\n",
      "Step: 374, Loss: 0.9169085621833801, Accuracy: 1.0, Computation time: 1.9339869022369385\n",
      "Step: 375, Loss: 0.9166752099990845, Accuracy: 1.0, Computation time: 1.774444580078125\n",
      "Step: 376, Loss: 0.9176028966903687, Accuracy: 1.0, Computation time: 2.0441696643829346\n",
      "Step: 377, Loss: 0.9166162610054016, Accuracy: 1.0, Computation time: 2.110011100769043\n",
      "Step: 378, Loss: 0.9162605404853821, Accuracy: 1.0, Computation time: 1.7286767959594727\n",
      "Step: 379, Loss: 0.9352564811706543, Accuracy: 0.9772727489471436, Computation time: 1.9046285152435303\n",
      "Step: 380, Loss: 0.916436493396759, Accuracy: 1.0, Computation time: 1.9000585079193115\n",
      "Step: 381, Loss: 0.9164234399795532, Accuracy: 1.0, Computation time: 2.1842880249023438\n",
      "Step: 382, Loss: 0.9241300821304321, Accuracy: 1.0, Computation time: 1.9747529029846191\n",
      "Step: 383, Loss: 0.9246222376823425, Accuracy: 1.0, Computation time: 2.0134029388427734\n",
      "Step: 384, Loss: 0.9174195528030396, Accuracy: 1.0, Computation time: 1.8989522457122803\n",
      "Step: 385, Loss: 0.9371415376663208, Accuracy: 0.9807692766189575, Computation time: 1.657578706741333\n",
      "Step: 386, Loss: 0.926322877407074, Accuracy: 1.0, Computation time: 1.8246400356292725\n",
      "Step: 387, Loss: 0.9165053963661194, Accuracy: 1.0, Computation time: 1.9279935359954834\n",
      "Step: 388, Loss: 0.9162026643753052, Accuracy: 1.0, Computation time: 1.7550971508026123\n",
      "Step: 389, Loss: 0.9258434772491455, Accuracy: 1.0, Computation time: 1.8144395351409912\n",
      "Step: 390, Loss: 0.9161491990089417, Accuracy: 1.0, Computation time: 1.616119623184204\n",
      "Step: 391, Loss: 0.9165512323379517, Accuracy: 1.0, Computation time: 1.8336474895477295\n",
      "Step: 392, Loss: 0.9341297745704651, Accuracy: 0.9722222089767456, Computation time: 2.8587486743927\n",
      "Step: 393, Loss: 0.9171302914619446, Accuracy: 1.0, Computation time: 1.8281419277191162\n",
      "Step: 394, Loss: 0.9170807003974915, Accuracy: 1.0, Computation time: 1.9847729206085205\n",
      "Step: 395, Loss: 0.9161415696144104, Accuracy: 1.0, Computation time: 1.8539352416992188\n",
      "Step: 396, Loss: 0.9160267114639282, Accuracy: 1.0, Computation time: 1.933243989944458\n",
      "Step: 397, Loss: 0.9300949573516846, Accuracy: 1.0, Computation time: 2.3917839527130127\n",
      "Step: 398, Loss: 0.9163557887077332, Accuracy: 1.0, Computation time: 2.1711721420288086\n",
      "Step: 399, Loss: 0.9171371459960938, Accuracy: 1.0, Computation time: 1.5689129829406738\n",
      "Step: 400, Loss: 0.9169949889183044, Accuracy: 1.0, Computation time: 1.870483160018921\n",
      "Step: 401, Loss: 0.9161369800567627, Accuracy: 1.0, Computation time: 1.941023349761963\n",
      "Step: 402, Loss: 0.9164825081825256, Accuracy: 1.0, Computation time: 1.834782600402832\n",
      "Step: 403, Loss: 0.9332246780395508, Accuracy: 0.9642857313156128, Computation time: 1.7909796237945557\n",
      "Step: 404, Loss: 0.9318148493766785, Accuracy: 0.9750000238418579, Computation time: 1.9006357192993164\n",
      "Step: 405, Loss: 0.9161538481712341, Accuracy: 1.0, Computation time: 1.8701906204223633\n",
      "Step: 406, Loss: 0.9162089824676514, Accuracy: 1.0, Computation time: 1.9037270545959473\n",
      "Step: 407, Loss: 0.9381741881370544, Accuracy: 0.96875, Computation time: 1.812049388885498\n",
      "Step: 408, Loss: 0.9164378046989441, Accuracy: 1.0, Computation time: 2.007549524307251\n",
      "Step: 409, Loss: 0.937423050403595, Accuracy: 0.9750000238418579, Computation time: 2.0180134773254395\n",
      "Step: 410, Loss: 0.9204356074333191, Accuracy: 1.0, Computation time: 1.8092284202575684\n",
      "Step: 411, Loss: 0.9163369536399841, Accuracy: 1.0, Computation time: 2.1434168815612793\n",
      "Step: 412, Loss: 0.916385293006897, Accuracy: 1.0, Computation time: 2.162447452545166\n",
      "Step: 413, Loss: 0.9195835590362549, Accuracy: 1.0, Computation time: 2.0542781352996826\n",
      "Step: 414, Loss: 0.9160329699516296, Accuracy: 1.0, Computation time: 1.6923959255218506\n",
      "Step: 415, Loss: 0.940662145614624, Accuracy: 0.9772727489471436, Computation time: 2.071861982345581\n",
      "Step: 416, Loss: 0.9542915225028992, Accuracy: 0.9522727727890015, Computation time: 1.9691526889801025\n",
      "Step: 417, Loss: 0.9238306283950806, Accuracy: 1.0, Computation time: 1.851935863494873\n",
      "########################\n",
      "Test loss: 1.1246442794799805, Test Accuracy_epoch3: 0.7273173332214355\n",
      "########################\n",
      "Step: 418, Loss: 0.9161523580551147, Accuracy: 1.0, Computation time: 1.882669448852539\n",
      "Step: 419, Loss: 0.9167785048484802, Accuracy: 1.0, Computation time: 1.84938645362854\n",
      "Step: 420, Loss: 0.9165600538253784, Accuracy: 1.0, Computation time: 2.074665069580078\n",
      "Step: 421, Loss: 0.9166279435157776, Accuracy: 1.0, Computation time: 1.859809398651123\n",
      "Step: 422, Loss: 0.9215006828308105, Accuracy: 1.0, Computation time: 2.179797887802124\n",
      "Step: 423, Loss: 0.9168276786804199, Accuracy: 1.0, Computation time: 2.2503950595855713\n",
      "Step: 424, Loss: 0.9392537474632263, Accuracy: 0.9642857313156128, Computation time: 2.0174341201782227\n",
      "Step: 425, Loss: 0.9160906672477722, Accuracy: 1.0, Computation time: 1.9041879177093506\n",
      "Step: 426, Loss: 0.9162722229957581, Accuracy: 1.0, Computation time: 2.361452579498291\n",
      "Step: 427, Loss: 0.9175585508346558, Accuracy: 1.0, Computation time: 1.9081625938415527\n",
      "Step: 428, Loss: 0.9350000023841858, Accuracy: 0.9642857313156128, Computation time: 2.27907395362854\n",
      "Step: 429, Loss: 0.9168381094932556, Accuracy: 1.0, Computation time: 1.871891975402832\n",
      "Step: 430, Loss: 0.9397466778755188, Accuracy: 0.9642857313156128, Computation time: 1.880638599395752\n",
      "Step: 431, Loss: 0.9162125587463379, Accuracy: 1.0, Computation time: 2.3823025226593018\n",
      "Step: 432, Loss: 0.9324520826339722, Accuracy: 0.9583333730697632, Computation time: 1.7365186214447021\n",
      "Step: 433, Loss: 0.9431187510490417, Accuracy: 0.9772727489471436, Computation time: 2.1046600341796875\n",
      "Step: 434, Loss: 0.9169661402702332, Accuracy: 1.0, Computation time: 2.1524665355682373\n",
      "Step: 435, Loss: 0.9162133932113647, Accuracy: 1.0, Computation time: 1.9362022876739502\n",
      "Step: 436, Loss: 0.9164415597915649, Accuracy: 1.0, Computation time: 1.9649670124053955\n",
      "Step: 437, Loss: 0.949166476726532, Accuracy: 0.9583333730697632, Computation time: 1.8394389152526855\n",
      "Step: 438, Loss: 0.9162602424621582, Accuracy: 1.0, Computation time: 2.0344035625457764\n",
      "Step: 439, Loss: 0.9162688255310059, Accuracy: 1.0, Computation time: 2.037897825241089\n",
      "Step: 440, Loss: 0.916806697845459, Accuracy: 1.0, Computation time: 2.238623857498169\n",
      "Step: 441, Loss: 0.9359811544418335, Accuracy: 0.96875, Computation time: 1.8009922504425049\n",
      "Step: 442, Loss: 0.9175369739532471, Accuracy: 1.0, Computation time: 1.9748811721801758\n",
      "Step: 443, Loss: 0.9162030220031738, Accuracy: 1.0, Computation time: 1.9892857074737549\n",
      "Step: 444, Loss: 0.9163294434547424, Accuracy: 1.0, Computation time: 2.4654459953308105\n",
      "Step: 445, Loss: 0.9161574840545654, Accuracy: 1.0, Computation time: 2.0269010066986084\n",
      "Step: 446, Loss: 0.9162208437919617, Accuracy: 1.0, Computation time: 1.7328519821166992\n",
      "Step: 447, Loss: 0.9168674945831299, Accuracy: 1.0, Computation time: 2.0186173915863037\n",
      "Step: 448, Loss: 0.9194303154945374, Accuracy: 1.0, Computation time: 2.0988872051239014\n",
      "Step: 449, Loss: 0.9163866639137268, Accuracy: 1.0, Computation time: 2.071268320083618\n",
      "Step: 450, Loss: 0.9177637696266174, Accuracy: 1.0, Computation time: 1.9333040714263916\n",
      "Step: 451, Loss: 0.9160047173500061, Accuracy: 1.0, Computation time: 1.8644955158233643\n",
      "Step: 452, Loss: 0.9407308101654053, Accuracy: 0.949999988079071, Computation time: 1.7434697151184082\n",
      "Step: 453, Loss: 0.9165100455284119, Accuracy: 1.0, Computation time: 2.279832601547241\n",
      "Step: 454, Loss: 0.916003406047821, Accuracy: 1.0, Computation time: 1.7503242492675781\n",
      "Step: 455, Loss: 0.9162625670433044, Accuracy: 1.0, Computation time: 1.9515857696533203\n",
      "Step: 456, Loss: 0.9161264300346375, Accuracy: 1.0, Computation time: 2.076157569885254\n",
      "Step: 457, Loss: 0.9160565137863159, Accuracy: 1.0, Computation time: 1.6510522365570068\n",
      "Step: 458, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.9595017433166504\n",
      "Step: 459, Loss: 0.9159836173057556, Accuracy: 1.0, Computation time: 2.237238883972168\n",
      "Step: 460, Loss: 0.9164177179336548, Accuracy: 1.0, Computation time: 1.7348592281341553\n",
      "Step: 461, Loss: 0.9378939867019653, Accuracy: 0.9791666865348816, Computation time: 1.684046983718872\n",
      "Step: 462, Loss: 0.9184679388999939, Accuracy: 1.0, Computation time: 2.0341551303863525\n",
      "Step: 463, Loss: 0.9160768985748291, Accuracy: 1.0, Computation time: 2.3679611682891846\n",
      "Step: 464, Loss: 0.9164301753044128, Accuracy: 1.0, Computation time: 1.7797696590423584\n",
      "Step: 465, Loss: 0.9350946545600891, Accuracy: 0.96875, Computation time: 1.8331563472747803\n",
      "Step: 466, Loss: 0.9160192012786865, Accuracy: 1.0, Computation time: 1.830547571182251\n",
      "Step: 467, Loss: 0.9290201663970947, Accuracy: 0.9791666865348816, Computation time: 1.849252700805664\n",
      "Step: 468, Loss: 0.9161680936813354, Accuracy: 1.0, Computation time: 2.9144978523254395\n",
      "Step: 469, Loss: 0.9161289930343628, Accuracy: 1.0, Computation time: 1.9941439628601074\n",
      "Step: 470, Loss: 0.916242778301239, Accuracy: 1.0, Computation time: 2.225863456726074\n",
      "Step: 471, Loss: 0.9164809584617615, Accuracy: 1.0, Computation time: 1.8544843196868896\n",
      "Step: 472, Loss: 0.9180821776390076, Accuracy: 1.0, Computation time: 1.9089434146881104\n",
      "Step: 473, Loss: 0.9168024659156799, Accuracy: 1.0, Computation time: 2.0851991176605225\n",
      "Step: 474, Loss: 0.9159932732582092, Accuracy: 1.0, Computation time: 2.0450475215911865\n",
      "Step: 475, Loss: 0.9159635305404663, Accuracy: 1.0, Computation time: 1.80751633644104\n",
      "Step: 476, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.9413301944732666\n",
      "Step: 477, Loss: 0.9163278341293335, Accuracy: 1.0, Computation time: 1.8271484375\n",
      "Step: 478, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.8527040481567383\n",
      "Step: 479, Loss: 0.9159685373306274, Accuracy: 1.0, Computation time: 1.914604902267456\n",
      "Step: 480, Loss: 0.9168382883071899, Accuracy: 1.0, Computation time: 1.8233399391174316\n",
      "Step: 481, Loss: 0.9160276651382446, Accuracy: 1.0, Computation time: 1.690037727355957\n",
      "Step: 482, Loss: 0.9169866442680359, Accuracy: 1.0, Computation time: 1.9287302494049072\n",
      "Step: 483, Loss: 0.9206724762916565, Accuracy: 1.0, Computation time: 2.3057332038879395\n",
      "Step: 484, Loss: 0.9162129759788513, Accuracy: 1.0, Computation time: 2.0980281829833984\n",
      "Step: 485, Loss: 0.9185513854026794, Accuracy: 1.0, Computation time: 1.8385865688323975\n",
      "Step: 486, Loss: 0.9308594465255737, Accuracy: 0.949999988079071, Computation time: 2.1293981075286865\n",
      "Step: 487, Loss: 0.9160304665565491, Accuracy: 1.0, Computation time: 1.7044601440429688\n",
      "Step: 488, Loss: 0.9162882566452026, Accuracy: 1.0, Computation time: 1.6975154876708984\n",
      "Step: 489, Loss: 0.9218932390213013, Accuracy: 1.0, Computation time: 2.096204996109009\n",
      "Step: 490, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 1.6787853240966797\n",
      "Step: 491, Loss: 0.916071891784668, Accuracy: 1.0, Computation time: 1.8375744819641113\n",
      "Step: 492, Loss: 0.9163028001785278, Accuracy: 1.0, Computation time: 1.8001387119293213\n",
      "Step: 493, Loss: 0.9160469770431519, Accuracy: 1.0, Computation time: 2.1663522720336914\n",
      "Step: 494, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 1.6389694213867188\n",
      "Step: 495, Loss: 0.9159811735153198, Accuracy: 1.0, Computation time: 1.6799328327178955\n",
      "Step: 496, Loss: 0.9162912964820862, Accuracy: 1.0, Computation time: 1.9154667854309082\n",
      "Step: 497, Loss: 0.9253934025764465, Accuracy: 0.9750000238418579, Computation time: 1.6920077800750732\n",
      "Step: 498, Loss: 0.9159952402114868, Accuracy: 1.0, Computation time: 1.7296221256256104\n",
      "Step: 499, Loss: 0.9159964919090271, Accuracy: 1.0, Computation time: 1.7657344341278076\n",
      "Step: 500, Loss: 0.9171053171157837, Accuracy: 1.0, Computation time: 1.6626183986663818\n",
      "Step: 501, Loss: 0.9162299036979675, Accuracy: 1.0, Computation time: 2.20993709564209\n",
      "Step: 502, Loss: 0.9164290428161621, Accuracy: 1.0, Computation time: 2.093194007873535\n",
      "Step: 503, Loss: 0.9159904718399048, Accuracy: 1.0, Computation time: 1.8462209701538086\n",
      "Step: 504, Loss: 0.9301384091377258, Accuracy: 0.9772727489471436, Computation time: 1.6879551410675049\n",
      "Step: 505, Loss: 0.9161520600318909, Accuracy: 1.0, Computation time: 2.1444053649902344\n",
      "Step: 506, Loss: 0.9376332759857178, Accuracy: 0.9583333730697632, Computation time: 1.7695488929748535\n",
      "Step: 507, Loss: 0.9161671996116638, Accuracy: 1.0, Computation time: 1.7956202030181885\n",
      "Step: 508, Loss: 0.9542499780654907, Accuracy: 0.9479166865348816, Computation time: 1.9495127201080322\n",
      "Step: 509, Loss: 0.9249578714370728, Accuracy: 1.0, Computation time: 1.6856439113616943\n",
      "Step: 510, Loss: 0.9161897301673889, Accuracy: 1.0, Computation time: 2.032721757888794\n",
      "Step: 511, Loss: 0.9161097407341003, Accuracy: 1.0, Computation time: 1.8759942054748535\n",
      "Step: 512, Loss: 0.916098952293396, Accuracy: 1.0, Computation time: 1.8523995876312256\n",
      "Step: 513, Loss: 0.9379213452339172, Accuracy: 0.9772727489471436, Computation time: 1.7890806198120117\n",
      "Step: 514, Loss: 0.9166064858436584, Accuracy: 1.0, Computation time: 2.499072313308716\n",
      "Step: 515, Loss: 0.9161339998245239, Accuracy: 1.0, Computation time: 1.8539209365844727\n",
      "Step: 516, Loss: 0.9163963198661804, Accuracy: 1.0, Computation time: 1.9456911087036133\n",
      "Step: 517, Loss: 0.916663646697998, Accuracy: 1.0, Computation time: 1.698796272277832\n",
      "Step: 518, Loss: 0.9161228537559509, Accuracy: 1.0, Computation time: 1.6754975318908691\n",
      "Step: 519, Loss: 0.9160564541816711, Accuracy: 1.0, Computation time: 1.7715013027191162\n",
      "Step: 520, Loss: 0.9179978370666504, Accuracy: 1.0, Computation time: 2.1037912368774414\n",
      "Step: 521, Loss: 0.935796856880188, Accuracy: 0.9807692766189575, Computation time: 1.917196273803711\n",
      "Step: 522, Loss: 0.9362528920173645, Accuracy: 0.96875, Computation time: 2.207321882247925\n",
      "Step: 523, Loss: 0.9159682393074036, Accuracy: 1.0, Computation time: 2.1870105266571045\n",
      "Step: 524, Loss: 0.9161656498908997, Accuracy: 1.0, Computation time: 1.9685478210449219\n",
      "Step: 525, Loss: 0.9190808534622192, Accuracy: 1.0, Computation time: 1.7678697109222412\n",
      "Step: 526, Loss: 0.923028290271759, Accuracy: 1.0, Computation time: 2.0565929412841797\n",
      "Step: 527, Loss: 0.9162918329238892, Accuracy: 1.0, Computation time: 1.6630198955535889\n",
      "Step: 528, Loss: 0.9164898991584778, Accuracy: 1.0, Computation time: 1.7691457271575928\n",
      "Step: 529, Loss: 0.9163578152656555, Accuracy: 1.0, Computation time: 1.6603844165802002\n",
      "Step: 530, Loss: 0.9163428544998169, Accuracy: 1.0, Computation time: 1.5977492332458496\n",
      "Step: 531, Loss: 0.9176216125488281, Accuracy: 1.0, Computation time: 1.8235259056091309\n",
      "Step: 532, Loss: 0.9373711347579956, Accuracy: 0.9750000238418579, Computation time: 1.957284688949585\n",
      "Step: 533, Loss: 0.916279137134552, Accuracy: 1.0, Computation time: 2.8133983612060547\n",
      "Step: 534, Loss: 0.9164367914199829, Accuracy: 1.0, Computation time: 1.5751845836639404\n",
      "Step: 535, Loss: 0.9163002967834473, Accuracy: 1.0, Computation time: 1.896817684173584\n",
      "Step: 536, Loss: 0.9160801768302917, Accuracy: 1.0, Computation time: 1.9026234149932861\n",
      "Step: 537, Loss: 0.9160556793212891, Accuracy: 1.0, Computation time: 1.9571177959442139\n",
      "Step: 538, Loss: 0.9169747233390808, Accuracy: 1.0, Computation time: 1.968883991241455\n",
      "Step: 539, Loss: 0.9161295294761658, Accuracy: 1.0, Computation time: 1.9547219276428223\n",
      "Step: 540, Loss: 0.9190731644630432, Accuracy: 1.0, Computation time: 2.0704257488250732\n",
      "Step: 541, Loss: 0.9208258390426636, Accuracy: 1.0, Computation time: 2.059375286102295\n",
      "Step: 542, Loss: 0.9162716269493103, Accuracy: 1.0, Computation time: 2.1924328804016113\n",
      "Step: 543, Loss: 0.9160324335098267, Accuracy: 1.0, Computation time: 2.3372817039489746\n",
      "Step: 544, Loss: 0.9159855842590332, Accuracy: 1.0, Computation time: 1.6812989711761475\n",
      "Step: 545, Loss: 0.9538425803184509, Accuracy: 0.9196428656578064, Computation time: 2.000662088394165\n",
      "Step: 546, Loss: 0.9168238639831543, Accuracy: 1.0, Computation time: 1.7423276901245117\n",
      "Step: 547, Loss: 0.9223621487617493, Accuracy: 1.0, Computation time: 2.0368731021881104\n",
      "Step: 548, Loss: 0.9170545339584351, Accuracy: 1.0, Computation time: 2.028803586959839\n",
      "Step: 549, Loss: 0.9162127375602722, Accuracy: 1.0, Computation time: 1.7345647811889648\n",
      "Step: 550, Loss: 0.9236465096473694, Accuracy: 1.0, Computation time: 2.1469671726226807\n",
      "Step: 551, Loss: 0.9160990118980408, Accuracy: 1.0, Computation time: 1.8886198997497559\n",
      "Step: 552, Loss: 0.9331142902374268, Accuracy: 0.9750000238418579, Computation time: 1.893362283706665\n",
      "Step: 553, Loss: 0.915996253490448, Accuracy: 1.0, Computation time: 1.910205602645874\n",
      "Step: 554, Loss: 0.9159725904464722, Accuracy: 1.0, Computation time: 1.8510189056396484\n",
      "Step: 555, Loss: 0.9207223653793335, Accuracy: 1.0, Computation time: 2.5057878494262695\n",
      "Step: 556, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.6984272003173828\n",
      "########################\n",
      "Test loss: 1.1208009719848633, Test Accuracy_epoch4: 0.7281585931777954\n",
      "########################\n",
      "Step: 557, Loss: 0.937433660030365, Accuracy: 0.9722222089767456, Computation time: 1.8755571842193604\n",
      "Step: 558, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 1.989675521850586\n",
      "Step: 559, Loss: 0.9469678997993469, Accuracy: 0.9495192766189575, Computation time: 2.0682172775268555\n",
      "Step: 560, Loss: 0.9162774085998535, Accuracy: 1.0, Computation time: 2.1995813846588135\n",
      "Step: 561, Loss: 0.9162591099739075, Accuracy: 1.0, Computation time: 1.8644027709960938\n",
      "Step: 562, Loss: 0.9168372750282288, Accuracy: 1.0, Computation time: 1.9003756046295166\n",
      "Step: 563, Loss: 0.9240981340408325, Accuracy: 1.0, Computation time: 2.2668867111206055\n",
      "Step: 564, Loss: 0.9363482594490051, Accuracy: 0.9375, Computation time: 1.8363580703735352\n",
      "Step: 565, Loss: 0.916251540184021, Accuracy: 1.0, Computation time: 2.344576120376587\n",
      "Step: 566, Loss: 0.9164232611656189, Accuracy: 1.0, Computation time: 2.3002772331237793\n",
      "Step: 567, Loss: 0.9170311689376831, Accuracy: 1.0, Computation time: 1.8858320713043213\n",
      "Step: 568, Loss: 0.916408360004425, Accuracy: 1.0, Computation time: 2.1731629371643066\n",
      "Step: 569, Loss: 0.9165078997612, Accuracy: 1.0, Computation time: 2.0429532527923584\n",
      "Step: 570, Loss: 0.9161786437034607, Accuracy: 1.0, Computation time: 1.9405620098114014\n",
      "Step: 571, Loss: 0.9281957149505615, Accuracy: 1.0, Computation time: 2.118562936782837\n",
      "Step: 572, Loss: 0.9169013500213623, Accuracy: 1.0, Computation time: 1.905771017074585\n",
      "Step: 573, Loss: 0.9166665077209473, Accuracy: 1.0, Computation time: 2.2144641876220703\n",
      "Step: 574, Loss: 0.91617751121521, Accuracy: 1.0, Computation time: 2.0679526329040527\n",
      "Step: 575, Loss: 0.9382376670837402, Accuracy: 0.9772727489471436, Computation time: 1.9857468605041504\n",
      "Step: 576, Loss: 0.9163307547569275, Accuracy: 1.0, Computation time: 2.084730863571167\n",
      "Step: 577, Loss: 0.9165812134742737, Accuracy: 1.0, Computation time: 1.935558795928955\n",
      "Step: 578, Loss: 0.9163142442703247, Accuracy: 1.0, Computation time: 2.0827929973602295\n",
      "Step: 579, Loss: 0.9163244366645813, Accuracy: 1.0, Computation time: 2.305506706237793\n",
      "Step: 580, Loss: 0.9160851240158081, Accuracy: 1.0, Computation time: 2.009777069091797\n",
      "Step: 581, Loss: 0.9168205261230469, Accuracy: 1.0, Computation time: 1.81256103515625\n",
      "Step: 582, Loss: 0.9159759283065796, Accuracy: 1.0, Computation time: 2.00878643989563\n",
      "Step: 583, Loss: 0.937821626663208, Accuracy: 0.949999988079071, Computation time: 2.0457143783569336\n",
      "Step: 584, Loss: 0.9179475903511047, Accuracy: 1.0, Computation time: 2.102937698364258\n",
      "Step: 585, Loss: 0.9160511493682861, Accuracy: 1.0, Computation time: 1.9354801177978516\n",
      "Step: 586, Loss: 0.9375906586647034, Accuracy: 0.9791666865348816, Computation time: 1.7975006103515625\n",
      "Step: 587, Loss: 0.916496753692627, Accuracy: 1.0, Computation time: 1.8459875583648682\n",
      "Step: 588, Loss: 0.9168099164962769, Accuracy: 1.0, Computation time: 1.8911771774291992\n",
      "Step: 589, Loss: 0.9162724614143372, Accuracy: 1.0, Computation time: 1.9955353736877441\n",
      "Step: 590, Loss: 0.9162503480911255, Accuracy: 1.0, Computation time: 2.180144786834717\n",
      "Step: 591, Loss: 0.9163382649421692, Accuracy: 1.0, Computation time: 2.0296170711517334\n",
      "Step: 592, Loss: 0.9160327911376953, Accuracy: 1.0, Computation time: 2.049872398376465\n",
      "Step: 593, Loss: 0.9160142540931702, Accuracy: 1.0, Computation time: 1.8136250972747803\n",
      "Step: 594, Loss: 0.9160499572753906, Accuracy: 1.0, Computation time: 2.221738576889038\n",
      "Step: 595, Loss: 0.9200003147125244, Accuracy: 1.0, Computation time: 2.5956060886383057\n",
      "Step: 596, Loss: 0.9377102851867676, Accuracy: 0.9722222089767456, Computation time: 2.0978903770446777\n",
      "Step: 597, Loss: 0.9160957932472229, Accuracy: 1.0, Computation time: 1.8477678298950195\n",
      "Step: 598, Loss: 0.9192245602607727, Accuracy: 1.0, Computation time: 2.889744997024536\n",
      "Step: 599, Loss: 0.9163557291030884, Accuracy: 1.0, Computation time: 1.766012191772461\n",
      "Step: 600, Loss: 0.9494675993919373, Accuracy: 0.949999988079071, Computation time: 2.1984009742736816\n",
      "Step: 601, Loss: 0.9282430410385132, Accuracy: 0.9750000238418579, Computation time: 1.939516544342041\n",
      "Step: 602, Loss: 0.9161273837089539, Accuracy: 1.0, Computation time: 1.909961462020874\n",
      "Step: 603, Loss: 0.9161661863327026, Accuracy: 1.0, Computation time: 1.7257936000823975\n",
      "Step: 604, Loss: 0.9165199995040894, Accuracy: 1.0, Computation time: 1.9483511447906494\n",
      "Step: 605, Loss: 0.9162964224815369, Accuracy: 1.0, Computation time: 2.029716968536377\n",
      "Step: 606, Loss: 0.9162485599517822, Accuracy: 1.0, Computation time: 2.305415153503418\n",
      "Step: 607, Loss: 0.9172348976135254, Accuracy: 1.0, Computation time: 1.952397108078003\n",
      "Step: 608, Loss: 0.9162140488624573, Accuracy: 1.0, Computation time: 1.8334200382232666\n",
      "Step: 609, Loss: 0.9161196351051331, Accuracy: 1.0, Computation time: 2.0179007053375244\n",
      "Step: 610, Loss: 0.9163323640823364, Accuracy: 1.0, Computation time: 1.9482741355895996\n",
      "Step: 611, Loss: 0.9299910664558411, Accuracy: 0.9722222089767456, Computation time: 2.3724772930145264\n",
      "Step: 612, Loss: 0.9217509627342224, Accuracy: 1.0, Computation time: 2.435314178466797\n",
      "Step: 613, Loss: 0.9163457155227661, Accuracy: 1.0, Computation time: 2.0607309341430664\n",
      "Step: 614, Loss: 0.936235249042511, Accuracy: 0.9166666865348816, Computation time: 2.5695252418518066\n",
      "Step: 615, Loss: 0.9164515733718872, Accuracy: 1.0, Computation time: 1.78070068359375\n",
      "Step: 616, Loss: 0.921722412109375, Accuracy: 1.0, Computation time: 1.9980292320251465\n",
      "Step: 617, Loss: 0.9161790013313293, Accuracy: 1.0, Computation time: 2.065155029296875\n",
      "Step: 618, Loss: 0.9359800219535828, Accuracy: 0.9821428656578064, Computation time: 2.147407293319702\n",
      "Step: 619, Loss: 0.9163874387741089, Accuracy: 1.0, Computation time: 1.6476328372955322\n",
      "Step: 620, Loss: 0.9162878394126892, Accuracy: 1.0, Computation time: 2.15336275100708\n",
      "Step: 621, Loss: 0.916630208492279, Accuracy: 1.0, Computation time: 1.6778035163879395\n",
      "Step: 622, Loss: 0.91628098487854, Accuracy: 1.0, Computation time: 1.6154377460479736\n",
      "Step: 623, Loss: 0.9171074032783508, Accuracy: 1.0, Computation time: 1.6246535778045654\n",
      "Step: 624, Loss: 0.9162262082099915, Accuracy: 1.0, Computation time: 1.6133067607879639\n",
      "Step: 625, Loss: 0.9162763953208923, Accuracy: 1.0, Computation time: 1.9503848552703857\n",
      "Step: 626, Loss: 0.923892617225647, Accuracy: 1.0, Computation time: 2.247818946838379\n",
      "Step: 627, Loss: 0.9161067605018616, Accuracy: 1.0, Computation time: 1.8794476985931396\n",
      "Step: 628, Loss: 0.9161713123321533, Accuracy: 1.0, Computation time: 1.6828086376190186\n",
      "Step: 629, Loss: 0.916452944278717, Accuracy: 1.0, Computation time: 1.3886916637420654\n",
      "Step: 630, Loss: 0.9193819761276245, Accuracy: 1.0, Computation time: 1.69834566116333\n",
      "Step: 631, Loss: 0.916371762752533, Accuracy: 1.0, Computation time: 1.6778438091278076\n",
      "Step: 632, Loss: 0.9163624048233032, Accuracy: 1.0, Computation time: 1.4447252750396729\n",
      "Step: 633, Loss: 0.938234806060791, Accuracy: 0.9583333730697632, Computation time: 2.257023572921753\n",
      "Step: 634, Loss: 0.9161033034324646, Accuracy: 1.0, Computation time: 1.4760375022888184\n",
      "Step: 635, Loss: 0.91642826795578, Accuracy: 1.0, Computation time: 1.7159090042114258\n",
      "Step: 636, Loss: 0.9161295294761658, Accuracy: 1.0, Computation time: 1.7388312816619873\n",
      "Step: 637, Loss: 0.9161912202835083, Accuracy: 1.0, Computation time: 2.060082197189331\n",
      "Step: 638, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 1.914724588394165\n",
      "Step: 639, Loss: 0.9171855449676514, Accuracy: 1.0, Computation time: 2.0773563385009766\n",
      "Step: 640, Loss: 0.9245406985282898, Accuracy: 1.0, Computation time: 1.8588504791259766\n",
      "Step: 641, Loss: 0.9195361137390137, Accuracy: 1.0, Computation time: 1.8019740581512451\n",
      "Step: 642, Loss: 0.9398841261863708, Accuracy: 0.96875, Computation time: 1.7436251640319824\n",
      "Step: 643, Loss: 0.9166025519371033, Accuracy: 1.0, Computation time: 1.7735881805419922\n",
      "Step: 644, Loss: 0.9167982935905457, Accuracy: 1.0, Computation time: 1.8356285095214844\n",
      "Step: 645, Loss: 0.917056679725647, Accuracy: 1.0, Computation time: 1.6549267768859863\n",
      "Step: 646, Loss: 0.9377907514572144, Accuracy: 0.9861111044883728, Computation time: 1.724738597869873\n",
      "Step: 647, Loss: 0.916834831237793, Accuracy: 1.0, Computation time: 1.8762645721435547\n",
      "Step: 648, Loss: 0.9162858724594116, Accuracy: 1.0, Computation time: 1.8459136486053467\n",
      "Step: 649, Loss: 0.9356537461280823, Accuracy: 0.9642857313156128, Computation time: 2.0453271865844727\n",
      "Step: 650, Loss: 0.9401969313621521, Accuracy: 0.96875, Computation time: 2.171043634414673\n",
      "Step: 651, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 1.9653828144073486\n",
      "Step: 652, Loss: 0.9195753335952759, Accuracy: 1.0, Computation time: 2.118636131286621\n",
      "Step: 653, Loss: 0.9172452092170715, Accuracy: 1.0, Computation time: 2.0092785358428955\n",
      "Step: 654, Loss: 0.9204747676849365, Accuracy: 1.0, Computation time: 2.1703388690948486\n",
      "Step: 655, Loss: 0.9175872802734375, Accuracy: 1.0, Computation time: 2.2021074295043945\n",
      "Step: 656, Loss: 0.9484626054763794, Accuracy: 0.9571428894996643, Computation time: 2.511672019958496\n",
      "Step: 657, Loss: 0.9164696931838989, Accuracy: 1.0, Computation time: 2.09066104888916\n",
      "Step: 658, Loss: 0.9175587296485901, Accuracy: 1.0, Computation time: 2.3199305534362793\n",
      "Step: 659, Loss: 0.9163976311683655, Accuracy: 1.0, Computation time: 2.1357576847076416\n",
      "Step: 660, Loss: 0.9352763891220093, Accuracy: 0.96875, Computation time: 2.2867274284362793\n",
      "Step: 661, Loss: 0.9167032241821289, Accuracy: 1.0, Computation time: 2.51729679107666\n",
      "Step: 662, Loss: 0.9163969159126282, Accuracy: 1.0, Computation time: 1.9671943187713623\n",
      "Step: 663, Loss: 0.9169476628303528, Accuracy: 1.0, Computation time: 2.179333209991455\n",
      "Step: 664, Loss: 0.9166520833969116, Accuracy: 1.0, Computation time: 1.9555115699768066\n",
      "Step: 665, Loss: 0.9292833209037781, Accuracy: 0.9642857313156128, Computation time: 2.3942036628723145\n",
      "Step: 666, Loss: 0.9163137078285217, Accuracy: 1.0, Computation time: 2.566898822784424\n",
      "Step: 667, Loss: 0.9163370132446289, Accuracy: 1.0, Computation time: 2.818657636642456\n",
      "Step: 668, Loss: 0.939124584197998, Accuracy: 0.9722222089767456, Computation time: 2.089010000228882\n",
      "Step: 669, Loss: 0.9164645671844482, Accuracy: 1.0, Computation time: 1.970940113067627\n",
      "Step: 670, Loss: 0.9173615574836731, Accuracy: 1.0, Computation time: 2.675114870071411\n",
      "Step: 671, Loss: 0.9356434345245361, Accuracy: 0.949999988079071, Computation time: 1.9546380043029785\n",
      "Step: 672, Loss: 0.9182324409484863, Accuracy: 1.0, Computation time: 2.3306350708007812\n",
      "Step: 673, Loss: 0.916407585144043, Accuracy: 1.0, Computation time: 2.300877571105957\n",
      "Step: 674, Loss: 0.9166033267974854, Accuracy: nan, Computation time: 2.2095844745635986\n",
      "Step: 675, Loss: 0.916856586933136, Accuracy: 1.0, Computation time: 2.115269660949707\n",
      "Step: 676, Loss: 0.9163150787353516, Accuracy: 1.0, Computation time: 2.0594301223754883\n",
      "Step: 677, Loss: 0.916886031627655, Accuracy: 1.0, Computation time: 2.4503324031829834\n",
      "Step: 678, Loss: 0.916187047958374, Accuracy: 1.0, Computation time: 2.3546061515808105\n",
      "Step: 679, Loss: 0.9165432453155518, Accuracy: 1.0, Computation time: 2.248634099960327\n",
      "Step: 680, Loss: 0.9160213470458984, Accuracy: 1.0, Computation time: 2.288090467453003\n",
      "Step: 681, Loss: 0.9324825406074524, Accuracy: 0.9583333730697632, Computation time: 2.2968714237213135\n",
      "Step: 682, Loss: 0.9161855578422546, Accuracy: 1.0, Computation time: 1.884927749633789\n",
      "Step: 683, Loss: 0.9161611795425415, Accuracy: 1.0, Computation time: 1.872025489807129\n",
      "Step: 684, Loss: 0.9160863161087036, Accuracy: 1.0, Computation time: 2.34761118888855\n",
      "Step: 685, Loss: 0.9164663553237915, Accuracy: 1.0, Computation time: 2.0035805702209473\n",
      "Step: 686, Loss: 0.9164612293243408, Accuracy: 1.0, Computation time: 1.938269853591919\n",
      "Step: 687, Loss: 0.9167903065681458, Accuracy: 1.0, Computation time: 1.859961748123169\n",
      "Step: 688, Loss: 0.9159839153289795, Accuracy: 1.0, Computation time: 1.7752528190612793\n",
      "Step: 689, Loss: 0.936118483543396, Accuracy: 0.9642857313156128, Computation time: 2.031242609024048\n",
      "Step: 690, Loss: 0.9168311953544617, Accuracy: 1.0, Computation time: 1.7960870265960693\n",
      "Step: 691, Loss: 0.9372649192810059, Accuracy: 0.96875, Computation time: 2.033816337585449\n",
      "Step: 692, Loss: 0.9161666035652161, Accuracy: 1.0, Computation time: 1.7639472484588623\n",
      "Step: 693, Loss: 0.9200454354286194, Accuracy: 1.0, Computation time: 2.005357265472412\n",
      "Step: 694, Loss: 0.9163416028022766, Accuracy: 1.0, Computation time: 1.9138965606689453\n",
      "Step: 695, Loss: 0.9339596033096313, Accuracy: 0.949999988079071, Computation time: 2.08001708984375\n",
      "########################\n",
      "Test loss: 1.1210875511169434, Test Accuracy_epoch5: 0.7239526510238647\n",
      "########################\n",
      "Step: 696, Loss: 0.9161055088043213, Accuracy: 1.0, Computation time: 2.084152936935425\n",
      "Step: 697, Loss: 0.9160916805267334, Accuracy: 1.0, Computation time: 2.021162271499634\n",
      "Step: 698, Loss: 0.9175833463668823, Accuracy: 1.0, Computation time: 1.7892694473266602\n",
      "Step: 699, Loss: 0.9160180687904358, Accuracy: 1.0, Computation time: 1.9986109733581543\n",
      "Step: 700, Loss: 0.9410555362701416, Accuracy: 0.96875, Computation time: 2.396636724472046\n",
      "Step: 701, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 2.149012565612793\n",
      "Step: 702, Loss: 0.916000485420227, Accuracy: 1.0, Computation time: 2.1394989490509033\n",
      "Step: 703, Loss: 0.9159668684005737, Accuracy: 1.0, Computation time: 2.1454527378082275\n",
      "Step: 704, Loss: 0.9338287115097046, Accuracy: 0.9750000238418579, Computation time: 2.440817356109619\n",
      "Step: 705, Loss: 0.925309419631958, Accuracy: 0.9772727489471436, Computation time: 2.18925142288208\n",
      "Step: 706, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 2.012723207473755\n",
      "Step: 707, Loss: 0.9160259962081909, Accuracy: 1.0, Computation time: 2.2860445976257324\n",
      "Step: 708, Loss: 0.9159498810768127, Accuracy: 1.0, Computation time: 1.7145967483520508\n",
      "Step: 709, Loss: 0.9160330891609192, Accuracy: 1.0, Computation time: 2.033231258392334\n",
      "Step: 710, Loss: 0.915976345539093, Accuracy: 1.0, Computation time: 2.116712808609009\n",
      "Step: 711, Loss: 0.9160109758377075, Accuracy: 1.0, Computation time: 1.9554321765899658\n",
      "Step: 712, Loss: 0.9171936511993408, Accuracy: 1.0, Computation time: 2.051888942718506\n",
      "Step: 713, Loss: 0.9161846041679382, Accuracy: 1.0, Computation time: 1.8004071712493896\n",
      "Step: 714, Loss: 0.9208964705467224, Accuracy: 1.0, Computation time: 2.2586944103240967\n",
      "Step: 715, Loss: 0.9159521460533142, Accuracy: 1.0, Computation time: 2.021540403366089\n",
      "Step: 716, Loss: 0.916093111038208, Accuracy: 1.0, Computation time: 2.16058087348938\n",
      "Step: 717, Loss: 0.9177044630050659, Accuracy: 1.0, Computation time: 2.596341848373413\n",
      "Step: 718, Loss: 0.916143536567688, Accuracy: 1.0, Computation time: 2.373047113418579\n",
      "Step: 719, Loss: 0.9160324931144714, Accuracy: 1.0, Computation time: 2.1933562755584717\n",
      "Step: 720, Loss: 0.9159871935844421, Accuracy: 1.0, Computation time: 2.1404335498809814\n",
      "Step: 721, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 1.815173625946045\n",
      "Step: 722, Loss: 0.9369544386863708, Accuracy: 0.96875, Computation time: 2.357508659362793\n",
      "Step: 723, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.9338839054107666\n",
      "Step: 724, Loss: 0.9165987968444824, Accuracy: 1.0, Computation time: 2.087052583694458\n",
      "Step: 725, Loss: 0.9346689581871033, Accuracy: 0.9722222089767456, Computation time: 2.2285473346710205\n",
      "Step: 726, Loss: 0.9167342185974121, Accuracy: 1.0, Computation time: 2.333038806915283\n",
      "Step: 727, Loss: 0.9206792712211609, Accuracy: 1.0, Computation time: 2.25244140625\n",
      "Step: 728, Loss: 0.9161703586578369, Accuracy: 1.0, Computation time: 2.6482245922088623\n",
      "Step: 729, Loss: 0.9186019897460938, Accuracy: 1.0, Computation time: 2.1864845752716064\n",
      "Step: 730, Loss: 0.9161998629570007, Accuracy: 1.0, Computation time: 1.6959125995635986\n",
      "Step: 731, Loss: 0.9169309139251709, Accuracy: 1.0, Computation time: 1.9120848178863525\n",
      "Step: 732, Loss: 0.9378572702407837, Accuracy: 0.9807692766189575, Computation time: 2.130647659301758\n",
      "Step: 733, Loss: 0.9177616238594055, Accuracy: 1.0, Computation time: 2.138554096221924\n",
      "Step: 734, Loss: 0.916856050491333, Accuracy: 1.0, Computation time: 1.861337661743164\n",
      "Step: 735, Loss: 0.9163708686828613, Accuracy: 1.0, Computation time: 1.7782924175262451\n",
      "Step: 736, Loss: 0.9378880262374878, Accuracy: 0.9642857313156128, Computation time: 2.006588935852051\n",
      "Step: 737, Loss: 0.9229040741920471, Accuracy: 1.0, Computation time: 1.9640462398529053\n",
      "Step: 738, Loss: 0.9162296056747437, Accuracy: 1.0, Computation time: 2.016951322555542\n",
      "Step: 739, Loss: 0.9161505699157715, Accuracy: 1.0, Computation time: 2.138820171356201\n",
      "Step: 740, Loss: 0.9165357351303101, Accuracy: 1.0, Computation time: 1.7199785709381104\n",
      "Step: 741, Loss: 0.9383650422096252, Accuracy: 0.9772727489471436, Computation time: 2.157536268234253\n",
      "Step: 742, Loss: 0.9231418967247009, Accuracy: 1.0, Computation time: 1.803196668624878\n",
      "Step: 743, Loss: 0.9170774221420288, Accuracy: 1.0, Computation time: 2.2795608043670654\n",
      "Step: 744, Loss: 0.917299211025238, Accuracy: 1.0, Computation time: 1.817579984664917\n",
      "Step: 745, Loss: 0.9165153503417969, Accuracy: 1.0, Computation time: 1.7678487300872803\n",
      "Step: 746, Loss: 0.9166247844696045, Accuracy: 1.0, Computation time: 2.3245813846588135\n",
      "Step: 747, Loss: 0.9373244643211365, Accuracy: 0.9583333730697632, Computation time: 2.253527879714966\n",
      "Step: 748, Loss: 0.916467547416687, Accuracy: 1.0, Computation time: 1.9987728595733643\n",
      "Step: 749, Loss: 0.9166194200515747, Accuracy: 1.0, Computation time: 2.1170215606689453\n",
      "Step: 750, Loss: 0.9165359735488892, Accuracy: 1.0, Computation time: 1.8478927612304688\n",
      "Step: 751, Loss: 0.9170722961425781, Accuracy: 1.0, Computation time: 1.7325451374053955\n",
      "Step: 752, Loss: 0.9164254665374756, Accuracy: 1.0, Computation time: 2.3628413677215576\n",
      "Step: 753, Loss: 0.9160479307174683, Accuracy: 1.0, Computation time: 1.5167884826660156\n",
      "Step: 754, Loss: 0.9365411400794983, Accuracy: 0.9833333492279053, Computation time: 1.803539752960205\n",
      "Step: 755, Loss: 0.9160479307174683, Accuracy: 1.0, Computation time: 1.7240960597991943\n",
      "Step: 756, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 2.049316644668579\n",
      "Step: 757, Loss: 0.9386879801750183, Accuracy: 0.9750000238418579, Computation time: 2.4281036853790283\n",
      "Step: 758, Loss: 0.9160868525505066, Accuracy: 1.0, Computation time: 2.0263450145721436\n",
      "Step: 759, Loss: 0.9161026477813721, Accuracy: 1.0, Computation time: 1.756347417831421\n",
      "Step: 760, Loss: 0.9161293506622314, Accuracy: 1.0, Computation time: 1.7782304286956787\n",
      "Step: 761, Loss: 0.9161406755447388, Accuracy: 1.0, Computation time: 2.023061990737915\n",
      "Step: 762, Loss: 0.9161643981933594, Accuracy: 1.0, Computation time: 1.7105042934417725\n",
      "Step: 763, Loss: 0.932866632938385, Accuracy: 0.9722222089767456, Computation time: 1.7555105686187744\n",
      "Step: 764, Loss: 0.916865885257721, Accuracy: 1.0, Computation time: 2.1317927837371826\n",
      "Step: 765, Loss: 0.9162620902061462, Accuracy: 1.0, Computation time: 1.5870862007141113\n",
      "Step: 766, Loss: 0.9348483681678772, Accuracy: 0.9807692766189575, Computation time: 2.141982316970825\n",
      "Step: 767, Loss: 0.9378353953361511, Accuracy: 0.9722222089767456, Computation time: 2.2864577770233154\n",
      "Step: 768, Loss: 0.9421255588531494, Accuracy: 0.9772727489471436, Computation time: 2.042851448059082\n",
      "Step: 769, Loss: 0.9165627360343933, Accuracy: 1.0, Computation time: 1.8573179244995117\n",
      "Step: 770, Loss: 0.9163517951965332, Accuracy: 1.0, Computation time: 2.0199475288391113\n",
      "Step: 771, Loss: 0.9170889854431152, Accuracy: 1.0, Computation time: 1.8274073600769043\n",
      "Step: 772, Loss: 0.9170319437980652, Accuracy: 1.0, Computation time: 1.802048683166504\n",
      "Step: 773, Loss: 0.9167544841766357, Accuracy: 1.0, Computation time: 2.364753007888794\n",
      "Step: 774, Loss: 0.9165050387382507, Accuracy: 1.0, Computation time: 1.8739566802978516\n",
      "Step: 775, Loss: 0.9390761852264404, Accuracy: 0.9642857313156128, Computation time: 2.5372068881988525\n",
      "Step: 776, Loss: 0.9165569543838501, Accuracy: 1.0, Computation time: 1.9338512420654297\n",
      "Step: 777, Loss: 0.9170852899551392, Accuracy: 1.0, Computation time: 1.9741840362548828\n",
      "Step: 778, Loss: 0.9169282913208008, Accuracy: 1.0, Computation time: 2.1330037117004395\n",
      "Step: 779, Loss: 0.9207243919372559, Accuracy: 1.0, Computation time: 2.0799901485443115\n",
      "Step: 780, Loss: 0.9168788194656372, Accuracy: 1.0, Computation time: 1.9905378818511963\n",
      "Step: 781, Loss: 0.9168462753295898, Accuracy: 1.0, Computation time: 2.2829623222351074\n",
      "Step: 782, Loss: 0.916454017162323, Accuracy: 1.0, Computation time: 2.175863265991211\n",
      "Step: 783, Loss: 0.938201904296875, Accuracy: 0.9772727489471436, Computation time: 1.7905213832855225\n",
      "Step: 784, Loss: 0.9160348176956177, Accuracy: 1.0, Computation time: 1.8608622550964355\n",
      "Step: 785, Loss: 0.919439435005188, Accuracy: 1.0, Computation time: 2.421976327896118\n",
      "Step: 786, Loss: 0.9161642789840698, Accuracy: 1.0, Computation time: 2.2264699935913086\n",
      "Step: 787, Loss: 0.9300895929336548, Accuracy: 0.96875, Computation time: 2.2074084281921387\n",
      "Step: 788, Loss: 0.9161689281463623, Accuracy: 1.0, Computation time: 2.0813868045806885\n",
      "Step: 789, Loss: 0.9161401987075806, Accuracy: 1.0, Computation time: 1.739445686340332\n",
      "Step: 790, Loss: 0.9380086660385132, Accuracy: 0.9750000238418579, Computation time: 2.207944631576538\n",
      "Step: 791, Loss: 0.9163395166397095, Accuracy: 1.0, Computation time: 2.102944850921631\n",
      "Step: 792, Loss: 0.9163545966148376, Accuracy: 1.0, Computation time: 2.0270419120788574\n",
      "Step: 793, Loss: 0.9159802198410034, Accuracy: 1.0, Computation time: 2.0180015563964844\n",
      "Step: 794, Loss: 0.9162012934684753, Accuracy: 1.0, Computation time: 2.296025037765503\n",
      "Step: 795, Loss: 0.9160524010658264, Accuracy: 1.0, Computation time: 1.9700284004211426\n",
      "Step: 796, Loss: 0.916120171546936, Accuracy: 1.0, Computation time: 2.5057899951934814\n",
      "Step: 797, Loss: 0.9160561561584473, Accuracy: 1.0, Computation time: 1.845625400543213\n",
      "Step: 798, Loss: 0.9378823637962341, Accuracy: 0.9791666865348816, Computation time: 1.8769745826721191\n",
      "Step: 799, Loss: 0.9159519672393799, Accuracy: 1.0, Computation time: 1.8212082386016846\n",
      "Step: 800, Loss: 0.9160158634185791, Accuracy: 1.0, Computation time: 1.8903419971466064\n",
      "Step: 801, Loss: 0.9376546144485474, Accuracy: 0.9772727489471436, Computation time: 1.901855230331421\n",
      "Step: 802, Loss: 0.9171770811080933, Accuracy: 1.0, Computation time: 2.0310752391815186\n",
      "Step: 803, Loss: 0.9160317182540894, Accuracy: 1.0, Computation time: 2.010777473449707\n",
      "Step: 804, Loss: 0.9296596050262451, Accuracy: 0.9772727489471436, Computation time: 2.1261556148529053\n",
      "Step: 805, Loss: 0.9166401624679565, Accuracy: 1.0, Computation time: 1.930142879486084\n",
      "Step: 806, Loss: 0.9165710806846619, Accuracy: 1.0, Computation time: 1.8929522037506104\n",
      "Step: 807, Loss: 0.9160796999931335, Accuracy: 1.0, Computation time: 1.8275330066680908\n",
      "Step: 808, Loss: 0.9161041975021362, Accuracy: 1.0, Computation time: 1.977403163909912\n",
      "Step: 809, Loss: 0.9179264307022095, Accuracy: 1.0, Computation time: 2.697446346282959\n",
      "Step: 810, Loss: 0.9160999655723572, Accuracy: 1.0, Computation time: 1.7270808219909668\n",
      "Step: 811, Loss: 0.9161683917045593, Accuracy: 1.0, Computation time: 1.9262444972991943\n",
      "Step: 812, Loss: 0.916432797908783, Accuracy: 1.0, Computation time: 1.923492670059204\n",
      "Step: 813, Loss: 0.9160857200622559, Accuracy: 1.0, Computation time: 2.1265456676483154\n",
      "Step: 814, Loss: 0.916100800037384, Accuracy: 1.0, Computation time: 2.094957113265991\n",
      "Step: 815, Loss: 0.9220883250236511, Accuracy: 1.0, Computation time: 1.9264369010925293\n",
      "Step: 816, Loss: 0.9159321784973145, Accuracy: 1.0, Computation time: 1.7236254215240479\n",
      "Step: 817, Loss: 0.9369295239448547, Accuracy: 0.9722222089767456, Computation time: 1.7597811222076416\n",
      "Step: 818, Loss: 0.9244016408920288, Accuracy: 1.0, Computation time: 2.1426548957824707\n",
      "Step: 819, Loss: 0.9164924025535583, Accuracy: 1.0, Computation time: 1.9979217052459717\n",
      "Step: 820, Loss: 0.9160141944885254, Accuracy: 1.0, Computation time: 2.1583118438720703\n",
      "Step: 821, Loss: 0.9161465764045715, Accuracy: 1.0, Computation time: 1.8213846683502197\n",
      "Step: 822, Loss: 0.9161706566810608, Accuracy: 1.0, Computation time: 2.1944844722747803\n",
      "Step: 823, Loss: 0.916352391242981, Accuracy: 1.0, Computation time: 2.1883602142333984\n",
      "Step: 824, Loss: 0.9160057306289673, Accuracy: 1.0, Computation time: 1.856776475906372\n",
      "Step: 825, Loss: 0.9160480499267578, Accuracy: 1.0, Computation time: 2.2052111625671387\n",
      "Step: 826, Loss: 0.915963888168335, Accuracy: 1.0, Computation time: 1.8371622562408447\n",
      "Step: 827, Loss: 0.915984034538269, Accuracy: 1.0, Computation time: 2.372769832611084\n",
      "Step: 828, Loss: 0.9160270690917969, Accuracy: 1.0, Computation time: 2.216564178466797\n",
      "Step: 829, Loss: 0.9160363674163818, Accuracy: 1.0, Computation time: 2.1267597675323486\n",
      "Step: 830, Loss: 0.9160929918289185, Accuracy: 1.0, Computation time: 2.484825849533081\n",
      "Step: 831, Loss: 0.9160961508750916, Accuracy: 1.0, Computation time: 2.237185001373291\n",
      "Step: 832, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 1.9009900093078613\n",
      "Step: 833, Loss: 0.91602623462677, Accuracy: 1.0, Computation time: 2.1308162212371826\n",
      "Step: 834, Loss: 0.916131317615509, Accuracy: 1.0, Computation time: 1.6928279399871826\n",
      "########################\n",
      "Test loss: 1.1192266941070557, Test Accuracy_epoch6: 0.7289553880691528\n",
      "########################\n",
      "Step: 835, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 2.037609100341797\n",
      "Step: 836, Loss: 0.937778651714325, Accuracy: 0.9722222089767456, Computation time: 2.2811341285705566\n",
      "Step: 837, Loss: 0.9414398074150085, Accuracy: 0.9642857313156128, Computation time: 2.004833221435547\n",
      "Step: 838, Loss: 0.916022539138794, Accuracy: 1.0, Computation time: 2.085649251937866\n",
      "Step: 839, Loss: 0.9159981608390808, Accuracy: 1.0, Computation time: 2.1737020015716553\n",
      "Step: 840, Loss: 0.9159621000289917, Accuracy: 1.0, Computation time: 2.014556646347046\n",
      "Step: 841, Loss: 0.9165538549423218, Accuracy: 1.0, Computation time: 1.983966588973999\n",
      "Step: 842, Loss: 0.9314917922019958, Accuracy: 0.96875, Computation time: 2.3292999267578125\n",
      "Step: 843, Loss: 0.9216398596763611, Accuracy: 1.0, Computation time: 1.8892343044281006\n",
      "Step: 844, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.6853432655334473\n",
      "Step: 845, Loss: 0.9159376621246338, Accuracy: 1.0, Computation time: 1.7250092029571533\n",
      "Step: 846, Loss: 0.9159987568855286, Accuracy: 1.0, Computation time: 1.9951846599578857\n",
      "Step: 847, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.8642733097076416\n",
      "Step: 848, Loss: 0.938869833946228, Accuracy: 0.9722222089767456, Computation time: 1.7476539611816406\n",
      "Step: 849, Loss: 0.9551376700401306, Accuracy: 0.9375, Computation time: 2.0051066875457764\n",
      "Step: 850, Loss: 0.9160066843032837, Accuracy: 1.0, Computation time: 1.6466515064239502\n",
      "Step: 851, Loss: 0.9160295128822327, Accuracy: 1.0, Computation time: 1.9858934879302979\n",
      "Step: 852, Loss: 0.9159936904907227, Accuracy: 1.0, Computation time: 1.7482271194458008\n",
      "Step: 853, Loss: 0.916006863117218, Accuracy: 1.0, Computation time: 2.073265552520752\n",
      "Step: 854, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 2.4063806533813477\n",
      "Step: 855, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.9101753234863281\n",
      "Step: 856, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 1.8846526145935059\n",
      "Step: 857, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.7242817878723145\n",
      "Step: 858, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 1.9388635158538818\n",
      "Step: 859, Loss: 0.9173365235328674, Accuracy: 1.0, Computation time: 2.0111472606658936\n",
      "Step: 860, Loss: 0.9160351157188416, Accuracy: 1.0, Computation time: 1.9987504482269287\n",
      "Step: 861, Loss: 0.9359495043754578, Accuracy: 0.9791666865348816, Computation time: 1.8757522106170654\n",
      "Step: 862, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.8269822597503662\n",
      "Step: 863, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.850628137588501\n",
      "Step: 864, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 1.7899136543273926\n",
      "Step: 865, Loss: 0.9159653782844543, Accuracy: 1.0, Computation time: 1.841766119003296\n",
      "Step: 866, Loss: 0.9363348484039307, Accuracy: 0.9642857313156128, Computation time: 1.979433298110962\n",
      "Step: 867, Loss: 0.9375771284103394, Accuracy: 0.96875, Computation time: 2.076566219329834\n",
      "Step: 868, Loss: 0.9159576892852783, Accuracy: 1.0, Computation time: 1.6524324417114258\n",
      "Step: 869, Loss: 0.937480628490448, Accuracy: 0.9791666865348816, Computation time: 1.9883196353912354\n",
      "Step: 870, Loss: 0.9160153269767761, Accuracy: 1.0, Computation time: 1.8933939933776855\n",
      "Step: 871, Loss: 0.9159916043281555, Accuracy: 1.0, Computation time: 2.002739191055298\n",
      "Step: 872, Loss: 0.9449315667152405, Accuracy: 0.949999988079071, Computation time: 1.9926657676696777\n",
      "Step: 873, Loss: 0.9169974327087402, Accuracy: 1.0, Computation time: 2.0577216148376465\n",
      "Step: 874, Loss: 0.9394129514694214, Accuracy: 0.9772727489471436, Computation time: 1.9123897552490234\n",
      "Step: 875, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 2.007425546646118\n",
      "Step: 876, Loss: 0.9181751608848572, Accuracy: 1.0, Computation time: 1.991175651550293\n",
      "Step: 877, Loss: 0.930188000202179, Accuracy: 0.9642857313156128, Computation time: 1.9725110530853271\n",
      "Step: 878, Loss: 0.9164654016494751, Accuracy: 1.0, Computation time: 1.8545455932617188\n",
      "Step: 879, Loss: 0.9160426259040833, Accuracy: 1.0, Computation time: 1.8433103561401367\n",
      "Step: 880, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.8803415298461914\n",
      "Step: 881, Loss: 0.9181005954742432, Accuracy: 1.0, Computation time: 2.502397060394287\n",
      "Step: 882, Loss: 0.9160743951797485, Accuracy: 1.0, Computation time: 2.158787727355957\n",
      "Step: 883, Loss: 0.937292218208313, Accuracy: 0.9375, Computation time: 2.0022199153900146\n",
      "Step: 884, Loss: 0.9159634709358215, Accuracy: 1.0, Computation time: 1.994560718536377\n",
      "Step: 885, Loss: 0.9159505367279053, Accuracy: 1.0, Computation time: 2.244920253753662\n",
      "Step: 886, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.7298057079315186\n",
      "Step: 887, Loss: 0.9379774928092957, Accuracy: 0.9642857313156128, Computation time: 2.074739933013916\n",
      "Step: 888, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.8607099056243896\n",
      "Step: 889, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.696758508682251\n",
      "Step: 890, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 2.1496944427490234\n",
      "Step: 891, Loss: 0.9160324335098267, Accuracy: 1.0, Computation time: 1.9512414932250977\n",
      "Step: 892, Loss: 0.9162417054176331, Accuracy: 1.0, Computation time: 2.312229871749878\n",
      "Step: 893, Loss: 0.9168050289154053, Accuracy: 1.0, Computation time: 1.6596686840057373\n",
      "Step: 894, Loss: 0.9167472124099731, Accuracy: 1.0, Computation time: 1.79667329788208\n",
      "Step: 895, Loss: 0.9376360774040222, Accuracy: 0.9583333730697632, Computation time: 2.242182731628418\n",
      "Step: 896, Loss: 0.9159338474273682, Accuracy: 1.0, Computation time: 1.91025972366333\n",
      "Step: 897, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.669795274734497\n",
      "Step: 898, Loss: 0.9161800146102905, Accuracy: 1.0, Computation time: 1.651928424835205\n",
      "Step: 899, Loss: 0.9167773723602295, Accuracy: 1.0, Computation time: 2.215703010559082\n",
      "Step: 900, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 1.799187183380127\n",
      "Step: 901, Loss: 0.9161986112594604, Accuracy: 1.0, Computation time: 2.1331987380981445\n",
      "Step: 902, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.7660231590270996\n",
      "Step: 903, Loss: 0.9159467220306396, Accuracy: 1.0, Computation time: 1.7168548107147217\n",
      "Step: 904, Loss: 0.9169909954071045, Accuracy: 1.0, Computation time: 1.9175729751586914\n",
      "Step: 905, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.6243443489074707\n",
      "Step: 906, Loss: 0.9160127639770508, Accuracy: 1.0, Computation time: 1.8441731929779053\n",
      "Step: 907, Loss: 0.9161150455474854, Accuracy: 1.0, Computation time: 1.8048572540283203\n",
      "Step: 908, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.973158597946167\n",
      "Step: 909, Loss: 0.9363420009613037, Accuracy: 0.9583333730697632, Computation time: 1.9000425338745117\n",
      "Step: 910, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.6938252449035645\n",
      "Step: 911, Loss: 0.918545126914978, Accuracy: 1.0, Computation time: 1.8649873733520508\n",
      "Step: 912, Loss: 0.9246597290039062, Accuracy: 1.0, Computation time: 1.7508904933929443\n",
      "Step: 913, Loss: 0.9225618839263916, Accuracy: 1.0, Computation time: 1.9471559524536133\n",
      "Step: 914, Loss: 0.9163507223129272, Accuracy: 1.0, Computation time: 2.365800619125366\n",
      "Step: 915, Loss: 0.9245690107345581, Accuracy: 1.0, Computation time: 1.7481257915496826\n",
      "Step: 916, Loss: 0.9162746667861938, Accuracy: 1.0, Computation time: 2.1484248638153076\n",
      "Step: 917, Loss: 0.9229170680046082, Accuracy: 1.0, Computation time: 2.1729066371917725\n",
      "Step: 918, Loss: 0.9162150025367737, Accuracy: 1.0, Computation time: 1.8852298259735107\n",
      "Step: 919, Loss: 0.9164307713508606, Accuracy: 1.0, Computation time: 2.4124886989593506\n",
      "Step: 920, Loss: 0.93799889087677, Accuracy: 0.9791666865348816, Computation time: 1.9540443420410156\n",
      "Step: 921, Loss: 0.9166173934936523, Accuracy: 1.0, Computation time: 1.8672537803649902\n",
      "Step: 922, Loss: 0.9251716136932373, Accuracy: 1.0, Computation time: 2.0469017028808594\n",
      "Step: 923, Loss: 0.9160211682319641, Accuracy: 1.0, Computation time: 1.600884199142456\n",
      "Step: 924, Loss: 0.9160203337669373, Accuracy: 1.0, Computation time: 1.9130065441131592\n",
      "Step: 925, Loss: 0.9161840677261353, Accuracy: 1.0, Computation time: 2.1774652004241943\n",
      "Step: 926, Loss: 0.9164016842842102, Accuracy: 1.0, Computation time: 2.079558849334717\n",
      "Step: 927, Loss: 0.9162201285362244, Accuracy: 1.0, Computation time: 2.2213051319122314\n",
      "Step: 928, Loss: 0.9161465167999268, Accuracy: 1.0, Computation time: 1.9741907119750977\n",
      "Step: 929, Loss: 0.9375976920127869, Accuracy: 0.9750000238418579, Computation time: 1.7204651832580566\n",
      "Step: 930, Loss: 0.9161139726638794, Accuracy: 1.0, Computation time: 2.2732791900634766\n",
      "Step: 931, Loss: 0.9195815324783325, Accuracy: 1.0, Computation time: 2.1116344928741455\n",
      "Step: 932, Loss: 0.9318942427635193, Accuracy: 0.9642857313156128, Computation time: 1.8308124542236328\n",
      "Step: 933, Loss: 0.9201434254646301, Accuracy: 1.0, Computation time: 2.11114239692688\n",
      "Step: 934, Loss: 0.9271501302719116, Accuracy: 0.9791666865348816, Computation time: 1.9186575412750244\n",
      "Step: 935, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 1.8304274082183838\n",
      "Step: 936, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 2.0317931175231934\n",
      "Step: 937, Loss: 0.9185070395469666, Accuracy: 1.0, Computation time: 2.4558968544006348\n",
      "Step: 938, Loss: 0.9161185026168823, Accuracy: 1.0, Computation time: 2.102871894836426\n",
      "Step: 939, Loss: 0.9160058498382568, Accuracy: 1.0, Computation time: 2.170409679412842\n",
      "Step: 940, Loss: 0.9373708367347717, Accuracy: 0.9722222089767456, Computation time: 2.1780593395233154\n",
      "Step: 941, Loss: 0.916154146194458, Accuracy: 1.0, Computation time: 2.3383758068084717\n",
      "Step: 942, Loss: 0.9159660339355469, Accuracy: 1.0, Computation time: 1.9393959045410156\n",
      "Step: 943, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 2.1833066940307617\n",
      "Step: 944, Loss: 0.9160286784172058, Accuracy: 1.0, Computation time: 1.8739306926727295\n",
      "Step: 945, Loss: 0.9184414744377136, Accuracy: 1.0, Computation time: 2.2958457469940186\n",
      "Step: 946, Loss: 0.9159659147262573, Accuracy: 1.0, Computation time: 1.9274086952209473\n",
      "Step: 947, Loss: 0.916128933429718, Accuracy: 1.0, Computation time: 2.103952407836914\n",
      "Step: 948, Loss: 0.9160522818565369, Accuracy: 1.0, Computation time: 2.4833312034606934\n",
      "Step: 949, Loss: 0.9470764398574829, Accuracy: 0.9750000238418579, Computation time: 2.4045913219451904\n",
      "Step: 950, Loss: 0.9161758422851562, Accuracy: 1.0, Computation time: 2.1119577884674072\n",
      "Step: 951, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 2.017723798751831\n",
      "Step: 952, Loss: 0.9161514043807983, Accuracy: 1.0, Computation time: 2.1566920280456543\n",
      "Step: 953, Loss: 0.9160375595092773, Accuracy: 1.0, Computation time: 2.1439168453216553\n",
      "Step: 954, Loss: 0.9161379337310791, Accuracy: 1.0, Computation time: 2.322216510772705\n",
      "Step: 955, Loss: 0.9161622524261475, Accuracy: 1.0, Computation time: 2.0495548248291016\n",
      "Step: 956, Loss: 0.9160422682762146, Accuracy: 1.0, Computation time: 2.2558083534240723\n",
      "Step: 957, Loss: 0.9160841107368469, Accuracy: 1.0, Computation time: 2.2162082195281982\n",
      "Step: 958, Loss: 0.9160071611404419, Accuracy: 1.0, Computation time: 2.074051856994629\n",
      "Step: 959, Loss: 0.9378001093864441, Accuracy: 0.9722222089767456, Computation time: 2.012787103652954\n",
      "Step: 960, Loss: 0.9159669876098633, Accuracy: 1.0, Computation time: 2.1482346057891846\n",
      "Step: 961, Loss: 0.9189707040786743, Accuracy: 1.0, Computation time: 2.0860822200775146\n",
      "Step: 962, Loss: 0.9169802069664001, Accuracy: 1.0, Computation time: 2.467642307281494\n",
      "Step: 963, Loss: 0.918791651725769, Accuracy: 1.0, Computation time: 2.008049249649048\n",
      "Step: 964, Loss: 0.9159932136535645, Accuracy: 1.0, Computation time: 2.2577531337738037\n",
      "Step: 965, Loss: 0.9175630807876587, Accuracy: 1.0, Computation time: 2.2798428535461426\n",
      "Step: 966, Loss: 0.9160867929458618, Accuracy: 1.0, Computation time: 1.876028060913086\n",
      "Step: 967, Loss: 0.9160573482513428, Accuracy: 1.0, Computation time: 1.7605488300323486\n",
      "Step: 968, Loss: 0.934677243232727, Accuracy: 0.9807692766189575, Computation time: 1.9220573902130127\n",
      "Step: 969, Loss: 0.9162497520446777, Accuracy: 1.0, Computation time: 1.8696990013122559\n",
      "Step: 970, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 1.7608826160430908\n",
      "Step: 971, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 1.9459764957427979\n",
      "Step: 972, Loss: 0.9163306951522827, Accuracy: 1.0, Computation time: 2.1136834621429443\n",
      "Step: 973, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.8370428085327148\n",
      "########################\n",
      "Test loss: 1.121600866317749, Test Accuracy_epoch7: 0.7212971448898315\n",
      "########################\n",
      "Step: 974, Loss: 0.9179810881614685, Accuracy: 1.0, Computation time: 1.9872419834136963\n",
      "Step: 975, Loss: 0.9160402417182922, Accuracy: 1.0, Computation time: 2.0286266803741455\n",
      "Step: 976, Loss: 0.9374313950538635, Accuracy: 0.9750000238418579, Computation time: 2.2048773765563965\n",
      "Step: 977, Loss: 0.9159422516822815, Accuracy: 1.0, Computation time: 2.063342571258545\n",
      "Step: 978, Loss: 0.915953516960144, Accuracy: 1.0, Computation time: 1.9397528171539307\n",
      "Step: 979, Loss: 0.9159284830093384, Accuracy: 1.0, Computation time: 2.11374831199646\n",
      "Step: 980, Loss: 0.9161974191665649, Accuracy: 1.0, Computation time: 1.866396427154541\n",
      "Step: 981, Loss: 0.9159768223762512, Accuracy: 1.0, Computation time: 1.8378899097442627\n",
      "Step: 982, Loss: 0.9159401655197144, Accuracy: 1.0, Computation time: 2.133951425552368\n",
      "Step: 983, Loss: 0.9159137606620789, Accuracy: 1.0, Computation time: 1.775517225265503\n",
      "Step: 984, Loss: 0.9159687161445618, Accuracy: 1.0, Computation time: 1.8250715732574463\n",
      "Step: 985, Loss: 0.9165786504745483, Accuracy: 1.0, Computation time: 1.8914248943328857\n",
      "Step: 986, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.9624269008636475\n",
      "Step: 987, Loss: 0.9159906506538391, Accuracy: 1.0, Computation time: 2.018954277038574\n",
      "Step: 988, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 1.8325679302215576\n",
      "Step: 989, Loss: 0.9159255623817444, Accuracy: 1.0, Computation time: 1.6838300228118896\n",
      "Step: 990, Loss: 0.9161140322685242, Accuracy: 1.0, Computation time: 2.2655844688415527\n",
      "Step: 991, Loss: 0.9163674712181091, Accuracy: 1.0, Computation time: 1.865933895111084\n",
      "Step: 992, Loss: 0.9160805344581604, Accuracy: 1.0, Computation time: 1.7190628051757812\n",
      "Step: 993, Loss: 0.9161349534988403, Accuracy: 1.0, Computation time: 1.9898462295532227\n",
      "Step: 994, Loss: 0.9163250923156738, Accuracy: 1.0, Computation time: 2.055521011352539\n",
      "Step: 995, Loss: 0.9159566760063171, Accuracy: 1.0, Computation time: 1.7870903015136719\n",
      "Step: 996, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.8365352153778076\n",
      "Step: 997, Loss: 0.9781072735786438, Accuracy: 0.938811182975769, Computation time: 1.867391586303711\n",
      "Step: 998, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.889770269393921\n",
      "Step: 999, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.9047222137451172\n",
      "Step: 1000, Loss: 0.915926992893219, Accuracy: 1.0, Computation time: 2.0185494422912598\n",
      "Step: 1001, Loss: 0.915983259677887, Accuracy: 1.0, Computation time: 1.5576279163360596\n",
      "Step: 1002, Loss: 0.9160300493240356, Accuracy: 1.0, Computation time: 1.9945385456085205\n",
      "Step: 1003, Loss: 0.9203526377677917, Accuracy: 1.0, Computation time: 2.303222417831421\n",
      "Step: 1004, Loss: 0.9269633293151855, Accuracy: 0.9750000238418579, Computation time: 2.227278709411621\n",
      "Step: 1005, Loss: 0.9162169694900513, Accuracy: 1.0, Computation time: 2.000718355178833\n",
      "Step: 1006, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 2.3048977851867676\n",
      "Step: 1007, Loss: 0.9159519672393799, Accuracy: 1.0, Computation time: 1.7380027770996094\n",
      "Step: 1008, Loss: 0.9160518646240234, Accuracy: 1.0, Computation time: 1.7327585220336914\n",
      "Step: 1009, Loss: 0.9160252809524536, Accuracy: 1.0, Computation time: 2.2230076789855957\n",
      "Step: 1010, Loss: 0.9160400629043579, Accuracy: 1.0, Computation time: 1.8986141681671143\n",
      "Step: 1011, Loss: 0.9159470796585083, Accuracy: 1.0, Computation time: 2.2517876625061035\n",
      "Step: 1012, Loss: 0.916034460067749, Accuracy: 1.0, Computation time: 2.086909294128418\n",
      "Step: 1013, Loss: 0.916263222694397, Accuracy: 1.0, Computation time: 2.196902275085449\n",
      "Step: 1014, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 1.878727912902832\n",
      "Step: 1015, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 1.9070870876312256\n",
      "Step: 1016, Loss: 0.916811466217041, Accuracy: 1.0, Computation time: 1.9365870952606201\n",
      "Step: 1017, Loss: 0.9375929236412048, Accuracy: 0.9375, Computation time: 1.9123709201812744\n",
      "Step: 1018, Loss: 0.916026771068573, Accuracy: 1.0, Computation time: 2.7556962966918945\n",
      "Step: 1019, Loss: 0.9375854134559631, Accuracy: 0.9807692766189575, Computation time: 2.0394341945648193\n",
      "Step: 1020, Loss: 0.9378243684768677, Accuracy: 0.9583333730697632, Computation time: 1.8097186088562012\n",
      "Step: 1021, Loss: 0.9159632325172424, Accuracy: 1.0, Computation time: 1.6290838718414307\n",
      "Step: 1022, Loss: 0.9168485999107361, Accuracy: 1.0, Computation time: 1.9520297050476074\n",
      "Step: 1023, Loss: 0.9160125255584717, Accuracy: 1.0, Computation time: 1.723118543624878\n",
      "Step: 1024, Loss: 0.9160677194595337, Accuracy: 1.0, Computation time: 2.1968846321105957\n",
      "Step: 1025, Loss: 0.9160109162330627, Accuracy: 1.0, Computation time: 1.7688801288604736\n",
      "Step: 1026, Loss: 0.9159693121910095, Accuracy: 1.0, Computation time: 1.9326965808868408\n",
      "Step: 1027, Loss: 0.9160065054893494, Accuracy: 1.0, Computation time: 1.8677210807800293\n",
      "Step: 1028, Loss: 0.9160370230674744, Accuracy: 1.0, Computation time: 1.7358274459838867\n",
      "Step: 1029, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 1.9235289096832275\n",
      "Step: 1030, Loss: 0.917740523815155, Accuracy: 1.0, Computation time: 1.7905502319335938\n",
      "Step: 1031, Loss: 0.915972113609314, Accuracy: 1.0, Computation time: 1.876295804977417\n",
      "Step: 1032, Loss: 0.9163492918014526, Accuracy: 1.0, Computation time: 1.906933069229126\n",
      "Step: 1033, Loss: 0.9159963726997375, Accuracy: 1.0, Computation time: 2.117253541946411\n",
      "Step: 1034, Loss: 0.9169554114341736, Accuracy: 1.0, Computation time: 1.76540207862854\n",
      "Step: 1035, Loss: 0.9165037870407104, Accuracy: 1.0, Computation time: 1.860666275024414\n",
      "Step: 1036, Loss: 0.91646808385849, Accuracy: 1.0, Computation time: 1.675278663635254\n",
      "Step: 1037, Loss: 0.916057825088501, Accuracy: 1.0, Computation time: 2.0100932121276855\n",
      "Step: 1038, Loss: 0.9160410165786743, Accuracy: 1.0, Computation time: 1.9368581771850586\n",
      "Step: 1039, Loss: 0.9160101413726807, Accuracy: 1.0, Computation time: 1.9086735248565674\n",
      "Step: 1040, Loss: 0.9160264730453491, Accuracy: 1.0, Computation time: 2.020118236541748\n",
      "Step: 1041, Loss: 0.9160431623458862, Accuracy: 1.0, Computation time: 1.8196332454681396\n",
      "Step: 1042, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 2.135809898376465\n",
      "Step: 1043, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.6680734157562256\n",
      "Step: 1044, Loss: 0.9161686897277832, Accuracy: 1.0, Computation time: 2.0170278549194336\n",
      "Step: 1045, Loss: 0.9162208437919617, Accuracy: 1.0, Computation time: 2.1448323726654053\n",
      "Step: 1046, Loss: 0.9375766515731812, Accuracy: 0.9642857313156128, Computation time: 1.7909557819366455\n",
      "Step: 1047, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.8585419654846191\n",
      "Step: 1048, Loss: 0.9376264810562134, Accuracy: 0.9791666865348816, Computation time: 2.436673402786255\n",
      "Step: 1049, Loss: 0.9395844340324402, Accuracy: 0.9642857313156128, Computation time: 1.8223488330841064\n",
      "Step: 1050, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.8895809650421143\n",
      "Step: 1051, Loss: 0.9335346221923828, Accuracy: 0.9852941036224365, Computation time: 1.7728595733642578\n",
      "Step: 1052, Loss: 0.9159435033798218, Accuracy: 1.0, Computation time: 2.106135606765747\n",
      "Step: 1053, Loss: 0.9374544024467468, Accuracy: 0.949999988079071, Computation time: 1.7513337135314941\n",
      "Step: 1054, Loss: 0.9159456491470337, Accuracy: 1.0, Computation time: 1.8543381690979004\n",
      "Step: 1055, Loss: 0.9161549210548401, Accuracy: 1.0, Computation time: 1.8668341636657715\n",
      "Step: 1056, Loss: 0.9164882302284241, Accuracy: 1.0, Computation time: 2.075387716293335\n",
      "Step: 1057, Loss: 0.915957510471344, Accuracy: 1.0, Computation time: 1.8422791957855225\n",
      "Step: 1058, Loss: 0.9160855412483215, Accuracy: 1.0, Computation time: 1.8808541297912598\n",
      "Step: 1059, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.7846670150756836\n",
      "Step: 1060, Loss: 0.9208207130432129, Accuracy: 1.0, Computation time: 1.8329918384552002\n",
      "Step: 1061, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 2.0648858547210693\n",
      "Step: 1062, Loss: 0.9161000847816467, Accuracy: 1.0, Computation time: 2.065242052078247\n",
      "Step: 1063, Loss: 0.9160792231559753, Accuracy: 1.0, Computation time: 1.7689001560211182\n",
      "Step: 1064, Loss: 0.9203665256500244, Accuracy: 1.0, Computation time: 2.398142099380493\n",
      "Step: 1065, Loss: 0.9161156415939331, Accuracy: 1.0, Computation time: 1.9372601509094238\n",
      "Step: 1066, Loss: 0.9395431280136108, Accuracy: 0.9750000238418579, Computation time: 2.1006851196289062\n",
      "Step: 1067, Loss: 0.9167872667312622, Accuracy: 1.0, Computation time: 1.9187138080596924\n",
      "Step: 1068, Loss: 0.9160107970237732, Accuracy: 1.0, Computation time: 1.9938948154449463\n",
      "Step: 1069, Loss: 0.9163115620613098, Accuracy: 1.0, Computation time: 1.8093712329864502\n",
      "Step: 1070, Loss: 0.9159572720527649, Accuracy: 1.0, Computation time: 1.7551627159118652\n",
      "Step: 1071, Loss: 0.9159756898880005, Accuracy: 1.0, Computation time: 1.8384613990783691\n",
      "Step: 1072, Loss: 0.9159473180770874, Accuracy: 1.0, Computation time: 1.774773359298706\n",
      "Step: 1073, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 2.161264657974243\n",
      "Step: 1074, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.5464799404144287\n",
      "Step: 1075, Loss: 0.9159958958625793, Accuracy: 1.0, Computation time: 2.156934976577759\n",
      "Step: 1076, Loss: 0.9160454273223877, Accuracy: 1.0, Computation time: 1.8100650310516357\n",
      "Step: 1077, Loss: 0.9181955456733704, Accuracy: 1.0, Computation time: 2.020056962966919\n",
      "Step: 1078, Loss: 0.931268036365509, Accuracy: 0.9791666865348816, Computation time: 2.166001558303833\n",
      "Step: 1079, Loss: 0.9159477949142456, Accuracy: 1.0, Computation time: 1.631185531616211\n",
      "Step: 1080, Loss: 0.9159486889839172, Accuracy: 1.0, Computation time: 1.7429101467132568\n",
      "Step: 1081, Loss: 0.9161320328712463, Accuracy: 1.0, Computation time: 1.884735345840454\n",
      "Step: 1082, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.8881869316101074\n",
      "Step: 1083, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 2.0433783531188965\n",
      "Step: 1084, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.5845673084259033\n",
      "Step: 1085, Loss: 0.9277744293212891, Accuracy: 0.96875, Computation time: 2.1706504821777344\n",
      "Step: 1086, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 1.785254716873169\n",
      "Step: 1087, Loss: 0.9375348687171936, Accuracy: 0.9821428656578064, Computation time: 1.8402678966522217\n",
      "Step: 1088, Loss: 0.9164149165153503, Accuracy: 1.0, Computation time: 1.9869229793548584\n",
      "Step: 1089, Loss: 0.925823450088501, Accuracy: 1.0, Computation time: 2.1079261302948\n",
      "Step: 1090, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 1.660412073135376\n",
      "Step: 1091, Loss: 0.9161704182624817, Accuracy: 1.0, Computation time: 1.5227224826812744\n",
      "Step: 1092, Loss: 0.9268406629562378, Accuracy: 0.9772727489471436, Computation time: 1.9291703701019287\n",
      "Step: 1093, Loss: 0.9160779118537903, Accuracy: 1.0, Computation time: 2.0263164043426514\n",
      "Step: 1094, Loss: 0.9169962406158447, Accuracy: 1.0, Computation time: 2.041832685470581\n",
      "Step: 1095, Loss: 0.9385287761688232, Accuracy: 0.96875, Computation time: 2.323817014694214\n",
      "Step: 1096, Loss: 0.9246540665626526, Accuracy: 1.0, Computation time: 2.182861804962158\n",
      "Step: 1097, Loss: 0.9167670607566833, Accuracy: 1.0, Computation time: 2.0564804077148438\n",
      "Step: 1098, Loss: 0.9597911238670349, Accuracy: 0.8999999761581421, Computation time: 2.197504997253418\n",
      "Step: 1099, Loss: 0.9171379208564758, Accuracy: 1.0, Computation time: 1.9611096382141113\n",
      "Step: 1100, Loss: 0.938682496547699, Accuracy: 0.96875, Computation time: 2.5518405437469482\n",
      "Step: 1101, Loss: 0.9164817333221436, Accuracy: 1.0, Computation time: 2.0599703788757324\n",
      "Step: 1102, Loss: 0.9417119026184082, Accuracy: 0.9750000238418579, Computation time: 2.3309543132781982\n",
      "Step: 1103, Loss: 0.9170330762863159, Accuracy: 1.0, Computation time: 2.0750691890716553\n",
      "Step: 1104, Loss: 0.9184979200363159, Accuracy: 1.0, Computation time: 2.3686599731445312\n",
      "Step: 1105, Loss: 0.9392940402030945, Accuracy: 0.96875, Computation time: 2.3143296241760254\n",
      "Step: 1106, Loss: 0.9393914341926575, Accuracy: 0.9791666865348816, Computation time: 1.7877233028411865\n",
      "Step: 1107, Loss: 0.917066216468811, Accuracy: 1.0, Computation time: 2.0860228538513184\n",
      "Step: 1108, Loss: 0.9184054732322693, Accuracy: 1.0, Computation time: 1.973672866821289\n",
      "Step: 1109, Loss: 0.9162424802780151, Accuracy: 1.0, Computation time: 1.9003033638000488\n",
      "Step: 1110, Loss: 0.918595552444458, Accuracy: 1.0, Computation time: 2.0006542205810547\n",
      "Step: 1111, Loss: 0.9629858136177063, Accuracy: 0.925000011920929, Computation time: 1.953930377960205\n",
      "Step: 1112, Loss: 0.9167696833610535, Accuracy: 1.0, Computation time: 2.079845666885376\n",
      "########################\n",
      "Test loss: 1.1323641538619995, Test Accuracy_epoch8: 0.7191227674484253\n",
      "########################\n",
      "Step: 1113, Loss: 0.9171879291534424, Accuracy: 1.0, Computation time: 1.9533560276031494\n",
      "Step: 1114, Loss: 0.9170292019844055, Accuracy: 1.0, Computation time: 1.9326176643371582\n",
      "Step: 1115, Loss: 0.9170321226119995, Accuracy: 1.0, Computation time: 2.0361244678497314\n",
      "Step: 1116, Loss: 0.937006950378418, Accuracy: 0.949999988079071, Computation time: 2.086618185043335\n",
      "Step: 1117, Loss: 0.9373984932899475, Accuracy: 0.9750000238418579, Computation time: 1.788271427154541\n",
      "Step: 1118, Loss: 0.9161839485168457, Accuracy: 1.0, Computation time: 1.523078203201294\n",
      "Step: 1119, Loss: 0.924857497215271, Accuracy: 1.0, Computation time: 2.2375574111938477\n",
      "Step: 1120, Loss: 0.916254997253418, Accuracy: 1.0, Computation time: 1.5248188972473145\n",
      "Step: 1121, Loss: 0.9165566563606262, Accuracy: 1.0, Computation time: 1.8974609375\n",
      "Step: 1122, Loss: 0.9166340231895447, Accuracy: 1.0, Computation time: 2.0346145629882812\n",
      "Step: 1123, Loss: 0.9165209531784058, Accuracy: 1.0, Computation time: 1.800180435180664\n",
      "Step: 1124, Loss: 0.9223339557647705, Accuracy: 1.0, Computation time: 2.620939016342163\n",
      "Step: 1125, Loss: 0.9161695837974548, Accuracy: 1.0, Computation time: 1.7664730548858643\n",
      "Step: 1126, Loss: 0.916610598564148, Accuracy: 1.0, Computation time: 2.0449509620666504\n",
      "Step: 1127, Loss: 0.916258692741394, Accuracy: 1.0, Computation time: 1.9207282066345215\n",
      "Step: 1128, Loss: 0.9163821935653687, Accuracy: 1.0, Computation time: 2.014516830444336\n",
      "Step: 1129, Loss: 0.9163793325424194, Accuracy: 1.0, Computation time: 1.7709019184112549\n",
      "Step: 1130, Loss: 0.9163902401924133, Accuracy: 1.0, Computation time: 1.6501879692077637\n",
      "Step: 1131, Loss: 0.9162067174911499, Accuracy: 1.0, Computation time: 2.032642364501953\n",
      "Step: 1132, Loss: 0.9416587352752686, Accuracy: 0.9722222089767456, Computation time: 1.9225001335144043\n",
      "Step: 1133, Loss: 0.9161201119422913, Accuracy: 1.0, Computation time: 1.7876980304718018\n",
      "Step: 1134, Loss: 0.9164348840713501, Accuracy: 1.0, Computation time: 1.6505846977233887\n",
      "Step: 1135, Loss: 0.9162653684616089, Accuracy: 1.0, Computation time: 2.1108734607696533\n",
      "Step: 1136, Loss: 0.9161222577095032, Accuracy: 1.0, Computation time: 1.684079647064209\n",
      "Step: 1137, Loss: 0.9161462783813477, Accuracy: 1.0, Computation time: 2.0278384685516357\n",
      "Step: 1138, Loss: 0.9161118865013123, Accuracy: 1.0, Computation time: 1.6555874347686768\n",
      "Step: 1139, Loss: 0.9160143136978149, Accuracy: 1.0, Computation time: 1.8430595397949219\n",
      "Step: 1140, Loss: 0.9165288209915161, Accuracy: 1.0, Computation time: 1.4939236640930176\n",
      "Step: 1141, Loss: 0.9160410761833191, Accuracy: 1.0, Computation time: 1.5805919170379639\n",
      "Step: 1142, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.7501509189605713\n",
      "Step: 1143, Loss: 0.9160561561584473, Accuracy: 1.0, Computation time: 1.6697399616241455\n",
      "Step: 1144, Loss: 0.9338484406471252, Accuracy: 0.9722222089767456, Computation time: 1.582080602645874\n",
      "Step: 1145, Loss: 0.9160393476486206, Accuracy: 1.0, Computation time: 2.1054511070251465\n",
      "Step: 1146, Loss: 0.9171009659767151, Accuracy: 1.0, Computation time: 2.3428666591644287\n",
      "Step: 1147, Loss: 0.9164677858352661, Accuracy: 1.0, Computation time: 1.9760427474975586\n",
      "Step: 1148, Loss: 0.9160436391830444, Accuracy: 1.0, Computation time: 1.9571139812469482\n",
      "Step: 1149, Loss: 0.9159842729568481, Accuracy: 1.0, Computation time: 2.215869903564453\n",
      "Step: 1150, Loss: 0.9164953827857971, Accuracy: 1.0, Computation time: 1.6898164749145508\n",
      "Step: 1151, Loss: 0.9246588945388794, Accuracy: 1.0, Computation time: 2.6255240440368652\n",
      "Step: 1152, Loss: 0.9160144925117493, Accuracy: 1.0, Computation time: 1.9615106582641602\n",
      "Step: 1153, Loss: 0.9375476241111755, Accuracy: 0.9791666865348816, Computation time: 2.061312437057495\n",
      "Step: 1154, Loss: 0.9161739945411682, Accuracy: 1.0, Computation time: 1.966780185699463\n",
      "Step: 1155, Loss: 0.9161176085472107, Accuracy: 1.0, Computation time: 1.5554900169372559\n",
      "Step: 1156, Loss: 0.9162331819534302, Accuracy: 1.0, Computation time: 2.215951681137085\n",
      "Step: 1157, Loss: 0.9160956144332886, Accuracy: 1.0, Computation time: 1.6250157356262207\n",
      "Step: 1158, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 1.4846782684326172\n",
      "Step: 1159, Loss: 0.9160771369934082, Accuracy: 1.0, Computation time: 1.5817453861236572\n",
      "Step: 1160, Loss: 0.9376930594444275, Accuracy: 0.9772727489471436, Computation time: 1.5430448055267334\n",
      "Step: 1161, Loss: 0.9161078333854675, Accuracy: 1.0, Computation time: 1.7327704429626465\n",
      "Step: 1162, Loss: 0.9167091846466064, Accuracy: 1.0, Computation time: 1.7275912761688232\n",
      "Step: 1163, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 1.5137641429901123\n",
      "Step: 1164, Loss: 0.9160933494567871, Accuracy: 1.0, Computation time: 1.4213969707489014\n",
      "Step: 1165, Loss: 0.9159982204437256, Accuracy: 1.0, Computation time: 1.6827542781829834\n",
      "Step: 1166, Loss: 0.9159966707229614, Accuracy: 1.0, Computation time: 1.995877742767334\n",
      "Step: 1167, Loss: 0.9162672758102417, Accuracy: 1.0, Computation time: 1.8043782711029053\n",
      "Step: 1168, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.6637606620788574\n",
      "Step: 1169, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.482144832611084\n",
      "Step: 1170, Loss: 0.9320072531700134, Accuracy: 0.9642857313156128, Computation time: 2.004979133605957\n",
      "Step: 1171, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.939547061920166\n",
      "Step: 1172, Loss: 0.916002094745636, Accuracy: 1.0, Computation time: 1.7788491249084473\n",
      "Step: 1173, Loss: 0.9164589047431946, Accuracy: 1.0, Computation time: 1.9540772438049316\n",
      "Step: 1174, Loss: 0.9161601066589355, Accuracy: 1.0, Computation time: 1.6692185401916504\n",
      "Step: 1175, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 1.7434439659118652\n",
      "Step: 1176, Loss: 0.9160804748535156, Accuracy: 1.0, Computation time: 1.6123614311218262\n",
      "Step: 1177, Loss: 0.9159811735153198, Accuracy: 1.0, Computation time: 1.6559038162231445\n",
      "Step: 1178, Loss: 0.9166086912155151, Accuracy: 1.0, Computation time: 1.414860486984253\n",
      "Step: 1179, Loss: 0.9159877896308899, Accuracy: 1.0, Computation time: 1.7691755294799805\n",
      "Step: 1180, Loss: 0.918275773525238, Accuracy: 1.0, Computation time: 2.0328803062438965\n",
      "Step: 1181, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 1.6400630474090576\n",
      "Step: 1182, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 1.617851972579956\n",
      "Step: 1183, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.7589397430419922\n",
      "Step: 1184, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.7211024761199951\n",
      "Step: 1185, Loss: 0.9288861751556396, Accuracy: 0.96875, Computation time: 2.4551427364349365\n",
      "Step: 1186, Loss: 0.915974497795105, Accuracy: 1.0, Computation time: 1.93703031539917\n",
      "Step: 1187, Loss: 0.915969967842102, Accuracy: 1.0, Computation time: 1.6782865524291992\n",
      "Step: 1188, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.5973153114318848\n",
      "Step: 1189, Loss: 0.9372029900550842, Accuracy: 0.9642857313156128, Computation time: 1.7860639095306396\n",
      "Step: 1190, Loss: 0.9159916639328003, Accuracy: 1.0, Computation time: 1.6725757122039795\n",
      "Step: 1191, Loss: 0.9379153251647949, Accuracy: 0.9772727489471436, Computation time: 1.5076525211334229\n",
      "Step: 1192, Loss: 0.9164488315582275, Accuracy: 1.0, Computation time: 1.7506203651428223\n",
      "Step: 1193, Loss: 0.94064861536026, Accuracy: 0.9642857313156128, Computation time: 2.258841037750244\n",
      "Step: 1194, Loss: 0.9234020709991455, Accuracy: 1.0, Computation time: 2.220865488052368\n",
      "Step: 1195, Loss: 0.9171815514564514, Accuracy: 1.0, Computation time: 2.192420244216919\n",
      "Step: 1196, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.7174136638641357\n",
      "Step: 1197, Loss: 0.9160048961639404, Accuracy: 1.0, Computation time: 2.051316261291504\n",
      "Step: 1198, Loss: 0.9159549474716187, Accuracy: 1.0, Computation time: 1.513824462890625\n",
      "Step: 1199, Loss: 0.9160937666893005, Accuracy: 1.0, Computation time: 1.9841773509979248\n",
      "Step: 1200, Loss: 0.9160680174827576, Accuracy: 1.0, Computation time: 1.8443644046783447\n",
      "Step: 1201, Loss: 0.9162180423736572, Accuracy: 1.0, Computation time: 2.033679723739624\n",
      "Step: 1202, Loss: 0.9161500334739685, Accuracy: 1.0, Computation time: 2.134040355682373\n",
      "Step: 1203, Loss: 0.916093647480011, Accuracy: 1.0, Computation time: 1.9630916118621826\n",
      "Step: 1204, Loss: 0.9161291718482971, Accuracy: 1.0, Computation time: 2.0981814861297607\n",
      "Step: 1205, Loss: 0.9161575436592102, Accuracy: 1.0, Computation time: 2.499415636062622\n",
      "Step: 1206, Loss: 0.9377560615539551, Accuracy: 0.9750000238418579, Computation time: 1.7520852088928223\n",
      "Step: 1207, Loss: 0.9163848757743835, Accuracy: 1.0, Computation time: 2.066711187362671\n",
      "Step: 1208, Loss: 0.9164589643478394, Accuracy: 1.0, Computation time: 2.2831172943115234\n",
      "Step: 1209, Loss: 0.9163937568664551, Accuracy: 1.0, Computation time: 2.10528302192688\n",
      "Step: 1210, Loss: 0.916098415851593, Accuracy: 1.0, Computation time: 1.855842113494873\n",
      "Step: 1211, Loss: 0.9176045656204224, Accuracy: 1.0, Computation time: 2.071077585220337\n",
      "Step: 1212, Loss: 0.9161012172698975, Accuracy: 1.0, Computation time: 1.7686281204223633\n",
      "Step: 1213, Loss: 0.9159737825393677, Accuracy: 1.0, Computation time: 2.017091989517212\n",
      "Step: 1214, Loss: 0.9205942749977112, Accuracy: 1.0, Computation time: 2.0850279331207275\n",
      "Step: 1215, Loss: 0.9377002716064453, Accuracy: 0.9583333730697632, Computation time: 1.9465959072113037\n",
      "Step: 1216, Loss: 0.9386341571807861, Accuracy: 0.9791666865348816, Computation time: 2.2546911239624023\n",
      "Step: 1217, Loss: 0.9159674048423767, Accuracy: 1.0, Computation time: 1.9983866214752197\n",
      "Step: 1218, Loss: 0.9161282777786255, Accuracy: 1.0, Computation time: 2.137923240661621\n",
      "Step: 1219, Loss: 0.9160969853401184, Accuracy: 1.0, Computation time: 2.076721668243408\n",
      "Step: 1220, Loss: 0.933225154876709, Accuracy: 0.9722222089767456, Computation time: 2.0628504753112793\n",
      "Step: 1221, Loss: 0.9180141091346741, Accuracy: 1.0, Computation time: 1.8965990543365479\n",
      "Step: 1222, Loss: 0.9203839898109436, Accuracy: 1.0, Computation time: 1.9471030235290527\n",
      "Step: 1223, Loss: 0.936123788356781, Accuracy: 0.96875, Computation time: 2.017056941986084\n",
      "Step: 1224, Loss: 0.9161461591720581, Accuracy: 1.0, Computation time: 1.6974503993988037\n",
      "Step: 1225, Loss: 0.9166048169136047, Accuracy: 1.0, Computation time: 2.006775379180908\n",
      "Step: 1226, Loss: 0.9179251194000244, Accuracy: 1.0, Computation time: 2.089982271194458\n",
      "Step: 1227, Loss: 0.9258018732070923, Accuracy: 0.9807692766189575, Computation time: 1.6557433605194092\n",
      "Step: 1228, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.5756042003631592\n",
      "Step: 1229, Loss: 0.9377652406692505, Accuracy: 0.9772727489471436, Computation time: 1.6902663707733154\n",
      "Step: 1230, Loss: 0.9183631539344788, Accuracy: 1.0, Computation time: 1.8625271320343018\n",
      "Step: 1231, Loss: 0.9163155555725098, Accuracy: 1.0, Computation time: 1.6398088932037354\n",
      "Step: 1232, Loss: 0.9167942404747009, Accuracy: 1.0, Computation time: 1.6837174892425537\n",
      "Step: 1233, Loss: 0.916687548160553, Accuracy: 1.0, Computation time: 1.5680079460144043\n",
      "Step: 1234, Loss: 0.9226458072662354, Accuracy: 1.0, Computation time: 2.0256121158599854\n",
      "Step: 1235, Loss: 0.9170802235603333, Accuracy: 1.0, Computation time: 2.0862271785736084\n",
      "Step: 1236, Loss: 0.9390290379524231, Accuracy: 0.9791666865348816, Computation time: 1.6121973991394043\n",
      "Step: 1237, Loss: 0.9172788858413696, Accuracy: 1.0, Computation time: 2.1240429878234863\n",
      "Step: 1238, Loss: 0.9186123013496399, Accuracy: 1.0, Computation time: 1.8916096687316895\n",
      "Step: 1239, Loss: 0.9164716601371765, Accuracy: 1.0, Computation time: 1.7724363803863525\n",
      "Step: 1240, Loss: 0.9188844561576843, Accuracy: 1.0, Computation time: 2.0862059593200684\n",
      "Step: 1241, Loss: 0.9163051843643188, Accuracy: 1.0, Computation time: 1.841642141342163\n",
      "Step: 1242, Loss: 0.9161232709884644, Accuracy: 1.0, Computation time: 2.0858712196350098\n",
      "Step: 1243, Loss: 0.9377016425132751, Accuracy: 0.9642857313156128, Computation time: 2.010256290435791\n",
      "Step: 1244, Loss: 0.9162770509719849, Accuracy: 1.0, Computation time: 1.908787488937378\n",
      "Step: 1245, Loss: 0.9378591775894165, Accuracy: 0.9791666865348816, Computation time: 2.187692642211914\n",
      "Step: 1246, Loss: 0.9162001013755798, Accuracy: 1.0, Computation time: 1.9716691970825195\n",
      "Step: 1247, Loss: 0.9161702990531921, Accuracy: 1.0, Computation time: 2.019800901412964\n",
      "Step: 1248, Loss: 0.9162124395370483, Accuracy: 1.0, Computation time: 1.6894843578338623\n",
      "Step: 1249, Loss: 0.9161978960037231, Accuracy: 1.0, Computation time: 1.7825393676757812\n",
      "Step: 1250, Loss: 0.9159835577011108, Accuracy: 1.0, Computation time: 1.8925766944885254\n",
      "Step: 1251, Loss: 0.9160000085830688, Accuracy: 1.0, Computation time: 1.569331407546997\n",
      "########################\n",
      "Test loss: 1.1339788436889648, Test Accuracy_epoch9: 0.7113656997680664\n",
      "########################\n",
      "Step: 1252, Loss: 0.9160112738609314, Accuracy: 1.0, Computation time: 1.9722223281860352\n",
      "Step: 1253, Loss: 0.9161252379417419, Accuracy: 1.0, Computation time: 2.1121299266815186\n",
      "Step: 1254, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 1.7020182609558105\n",
      "Step: 1255, Loss: 0.9229901432991028, Accuracy: 1.0, Computation time: 1.605595588684082\n",
      "Step: 1256, Loss: 0.9162350296974182, Accuracy: 1.0, Computation time: 2.2004480361938477\n",
      "Step: 1257, Loss: 0.9159670472145081, Accuracy: 1.0, Computation time: 2.6919608116149902\n",
      "Step: 1258, Loss: 0.9159914255142212, Accuracy: 1.0, Computation time: 1.7455682754516602\n",
      "Step: 1259, Loss: 0.9240648746490479, Accuracy: 1.0, Computation time: 2.6132848262786865\n",
      "Step: 1260, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.706587791442871\n",
      "Step: 1261, Loss: 0.9159654378890991, Accuracy: 1.0, Computation time: 1.9166181087493896\n",
      "Step: 1262, Loss: 0.9159972071647644, Accuracy: 1.0, Computation time: 1.8938581943511963\n",
      "Step: 1263, Loss: 0.9177078604698181, Accuracy: 1.0, Computation time: 1.6693589687347412\n",
      "Step: 1264, Loss: 0.9160462617874146, Accuracy: 1.0, Computation time: 1.6522443294525146\n",
      "Step: 1265, Loss: 0.9159750938415527, Accuracy: 1.0, Computation time: 1.7330248355865479\n",
      "Step: 1266, Loss: 0.9373949766159058, Accuracy: 0.9642857313156128, Computation time: 2.0727217197418213\n",
      "Step: 1267, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 1.6979618072509766\n",
      "Step: 1268, Loss: 0.9159740805625916, Accuracy: 1.0, Computation time: 1.783104658126831\n",
      "Step: 1269, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.803210973739624\n",
      "Step: 1270, Loss: 0.9159597754478455, Accuracy: 1.0, Computation time: 1.5854322910308838\n",
      "Step: 1271, Loss: 0.9159553050994873, Accuracy: 1.0, Computation time: 2.083096742630005\n",
      "Step: 1272, Loss: 0.9167400598526001, Accuracy: 1.0, Computation time: 1.7931022644042969\n",
      "Step: 1273, Loss: 0.9160200953483582, Accuracy: 1.0, Computation time: 1.889972448348999\n",
      "Step: 1274, Loss: 0.9160721302032471, Accuracy: 1.0, Computation time: 1.5873565673828125\n",
      "Step: 1275, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.650315761566162\n",
      "Step: 1276, Loss: 0.9376264810562134, Accuracy: 0.96875, Computation time: 2.226759433746338\n",
      "Step: 1277, Loss: 0.937732994556427, Accuracy: 0.9722222089767456, Computation time: 1.8906660079956055\n",
      "Step: 1278, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.663203477859497\n",
      "Step: 1279, Loss: 0.9213159680366516, Accuracy: 1.0, Computation time: 2.003011465072632\n",
      "Step: 1280, Loss: 0.9159720540046692, Accuracy: 1.0, Computation time: 1.9846951961517334\n",
      "Step: 1281, Loss: 0.9160985350608826, Accuracy: 1.0, Computation time: 1.681246042251587\n",
      "Step: 1282, Loss: 0.9160953760147095, Accuracy: 1.0, Computation time: 1.8688578605651855\n",
      "Step: 1283, Loss: 0.9160417914390564, Accuracy: 1.0, Computation time: 2.40602445602417\n",
      "Step: 1284, Loss: 0.9161208271980286, Accuracy: 1.0, Computation time: 1.8787813186645508\n",
      "Step: 1285, Loss: 0.9276379346847534, Accuracy: 0.9642857313156128, Computation time: 1.7372281551361084\n",
      "Step: 1286, Loss: 0.9160586595535278, Accuracy: 1.0, Computation time: 1.593562126159668\n",
      "Step: 1287, Loss: 0.9161807298660278, Accuracy: 1.0, Computation time: 1.706883430480957\n",
      "Step: 1288, Loss: 0.9160668849945068, Accuracy: 1.0, Computation time: 1.9844133853912354\n",
      "Step: 1289, Loss: 0.9162276983261108, Accuracy: 1.0, Computation time: 1.3735322952270508\n",
      "Step: 1290, Loss: 0.9361041784286499, Accuracy: 0.9868420958518982, Computation time: 2.057511329650879\n",
      "Step: 1291, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 1.5484163761138916\n",
      "Step: 1292, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 2.02122163772583\n",
      "Step: 1293, Loss: 0.91603684425354, Accuracy: 1.0, Computation time: 1.7967238426208496\n",
      "Step: 1294, Loss: 0.9160356521606445, Accuracy: 1.0, Computation time: 1.5483129024505615\n",
      "Step: 1295, Loss: 0.9347285032272339, Accuracy: 0.9722222089767456, Computation time: 1.8272037506103516\n",
      "Step: 1296, Loss: 0.9268522262573242, Accuracy: 0.9722222089767456, Computation time: 1.8397881984710693\n",
      "Step: 1297, Loss: 0.916183352470398, Accuracy: 1.0, Computation time: 1.7381069660186768\n",
      "Step: 1298, Loss: 0.918878436088562, Accuracy: 1.0, Computation time: 2.1338207721710205\n",
      "Step: 1299, Loss: 0.9183247089385986, Accuracy: 1.0, Computation time: 2.154712677001953\n",
      "Step: 1300, Loss: 0.9160045385360718, Accuracy: 1.0, Computation time: 2.277153730392456\n",
      "Step: 1301, Loss: 0.9160740971565247, Accuracy: 1.0, Computation time: 1.6505658626556396\n",
      "Step: 1302, Loss: 0.9160420298576355, Accuracy: 1.0, Computation time: 1.6723215579986572\n",
      "Step: 1303, Loss: 0.9161732792854309, Accuracy: 1.0, Computation time: 2.2059876918792725\n",
      "Step: 1304, Loss: 0.9163445234298706, Accuracy: 1.0, Computation time: 1.4399256706237793\n",
      "Step: 1305, Loss: 0.9379713535308838, Accuracy: 0.9791666865348816, Computation time: 1.6392755508422852\n",
      "Step: 1306, Loss: 0.9163260459899902, Accuracy: 1.0, Computation time: 2.1270439624786377\n",
      "Step: 1307, Loss: 0.9163575172424316, Accuracy: 1.0, Computation time: 1.6054136753082275\n",
      "Step: 1308, Loss: 0.9317823648452759, Accuracy: 0.9772727489471436, Computation time: 2.1244354248046875\n",
      "Step: 1309, Loss: 0.9159889817237854, Accuracy: 1.0, Computation time: 1.6169614791870117\n",
      "Step: 1310, Loss: 0.9163982272148132, Accuracy: 1.0, Computation time: 1.8183417320251465\n",
      "Step: 1311, Loss: 0.9159917831420898, Accuracy: 1.0, Computation time: 1.7689471244812012\n",
      "Step: 1312, Loss: 0.9160173535346985, Accuracy: 1.0, Computation time: 1.697793960571289\n",
      "Step: 1313, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 1.8319885730743408\n",
      "Step: 1314, Loss: 0.9161040782928467, Accuracy: 1.0, Computation time: 1.6159121990203857\n",
      "Step: 1315, Loss: 0.9163084030151367, Accuracy: 1.0, Computation time: 1.6931846141815186\n",
      "Step: 1316, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.6486213207244873\n",
      "Step: 1317, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 2.1621434688568115\n",
      "Step: 1318, Loss: 0.9160200953483582, Accuracy: 1.0, Computation time: 2.230092763900757\n",
      "Step: 1319, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.5809214115142822\n",
      "Step: 1320, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.823720932006836\n",
      "Step: 1321, Loss: 0.9161685705184937, Accuracy: 1.0, Computation time: 1.608459234237671\n",
      "Step: 1322, Loss: 0.9166654348373413, Accuracy: 1.0, Computation time: 1.31132173538208\n",
      "Step: 1323, Loss: 0.9234422445297241, Accuracy: 1.0, Computation time: 1.8754022121429443\n",
      "Step: 1324, Loss: 0.9376295804977417, Accuracy: 0.96875, Computation time: 1.9789700508117676\n",
      "Step: 1325, Loss: 0.9160382151603699, Accuracy: 1.0, Computation time: 2.1644253730773926\n",
      "Step: 1326, Loss: 0.9161733388900757, Accuracy: 1.0, Computation time: 1.6148412227630615\n",
      "Step: 1327, Loss: 0.9160473346710205, Accuracy: 1.0, Computation time: 1.5102076530456543\n",
      "Step: 1328, Loss: 0.918886125087738, Accuracy: 1.0, Computation time: 2.0831189155578613\n",
      "Step: 1329, Loss: 0.9161286950111389, Accuracy: 1.0, Computation time: 1.6797027587890625\n",
      "Step: 1330, Loss: 0.9163761138916016, Accuracy: 1.0, Computation time: 1.971245288848877\n",
      "Step: 1331, Loss: 0.9211670756340027, Accuracy: 1.0, Computation time: 1.8627684116363525\n",
      "Step: 1332, Loss: 0.9366917014122009, Accuracy: 0.9583333730697632, Computation time: 2.13431715965271\n",
      "Step: 1333, Loss: 0.91606205701828, Accuracy: 1.0, Computation time: 1.5585684776306152\n",
      "Step: 1334, Loss: 0.9172354936599731, Accuracy: 1.0, Computation time: 1.8616087436676025\n",
      "Step: 1335, Loss: 0.916039764881134, Accuracy: 1.0, Computation time: 2.148296594619751\n",
      "Step: 1336, Loss: 0.9159748554229736, Accuracy: 1.0, Computation time: 1.6596357822418213\n",
      "Step: 1337, Loss: 0.9160805344581604, Accuracy: 1.0, Computation time: 1.484010934829712\n",
      "Step: 1338, Loss: 0.9383864998817444, Accuracy: 0.949999988079071, Computation time: 2.1161742210388184\n",
      "Step: 1339, Loss: 0.916106104850769, Accuracy: 1.0, Computation time: 1.752453088760376\n",
      "Step: 1340, Loss: 0.915961742401123, Accuracy: 1.0, Computation time: 1.8430626392364502\n",
      "Step: 1341, Loss: 0.9174916744232178, Accuracy: 1.0, Computation time: 1.6485302448272705\n",
      "Step: 1342, Loss: 0.9170470833778381, Accuracy: 1.0, Computation time: 1.8440296649932861\n",
      "Step: 1343, Loss: 0.915947437286377, Accuracy: 1.0, Computation time: 1.7203328609466553\n",
      "Step: 1344, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.9420759677886963\n",
      "Step: 1345, Loss: 0.9160314202308655, Accuracy: 1.0, Computation time: 1.865889072418213\n",
      "Step: 1346, Loss: 0.9159833788871765, Accuracy: 1.0, Computation time: 2.3673007488250732\n",
      "Step: 1347, Loss: 0.9375526309013367, Accuracy: 0.9642857313156128, Computation time: 1.5415616035461426\n",
      "Step: 1348, Loss: 0.9285605549812317, Accuracy: 0.9821428656578064, Computation time: 1.7054517269134521\n",
      "Step: 1349, Loss: 0.9376227259635925, Accuracy: 0.96875, Computation time: 1.636207103729248\n",
      "Step: 1350, Loss: 0.9160106778144836, Accuracy: 1.0, Computation time: 1.7729990482330322\n",
      "Step: 1351, Loss: 0.9377536773681641, Accuracy: 0.9791666865348816, Computation time: 1.703751802444458\n",
      "Step: 1352, Loss: 0.9160175919532776, Accuracy: 1.0, Computation time: 2.1322712898254395\n",
      "Step: 1353, Loss: 0.91609787940979, Accuracy: 1.0, Computation time: 1.9406280517578125\n",
      "Step: 1354, Loss: 0.9167601466178894, Accuracy: 1.0, Computation time: 1.8668928146362305\n",
      "Step: 1355, Loss: 0.9165850281715393, Accuracy: 1.0, Computation time: 1.8439834117889404\n",
      "Step: 1356, Loss: 0.9159947037696838, Accuracy: 1.0, Computation time: 2.046898603439331\n",
      "Step: 1357, Loss: 0.9161408543586731, Accuracy: 1.0, Computation time: 1.5915074348449707\n",
      "Step: 1358, Loss: 0.9160330891609192, Accuracy: 1.0, Computation time: 1.8774244785308838\n",
      "Step: 1359, Loss: 0.9355331659317017, Accuracy: 0.9583333730697632, Computation time: 1.8296563625335693\n",
      "Step: 1360, Loss: 0.9316537380218506, Accuracy: 0.9583333730697632, Computation time: 2.0324113368988037\n",
      "Step: 1361, Loss: 0.9163783192634583, Accuracy: 1.0, Computation time: 2.4024300575256348\n",
      "Step: 1362, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 1.7217564582824707\n",
      "Step: 1363, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 2.0247292518615723\n",
      "Step: 1364, Loss: 0.9162839651107788, Accuracy: 1.0, Computation time: 1.9061100482940674\n",
      "Step: 1365, Loss: 0.915962278842926, Accuracy: 1.0, Computation time: 1.9410760402679443\n",
      "Step: 1366, Loss: 0.9363551735877991, Accuracy: 0.9722222089767456, Computation time: 1.734635829925537\n",
      "Step: 1367, Loss: 0.9159666299819946, Accuracy: 1.0, Computation time: 1.7710187435150146\n",
      "Step: 1368, Loss: 0.915909469127655, Accuracy: 1.0, Computation time: 1.6961336135864258\n",
      "Step: 1369, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.4724173545837402\n",
      "Step: 1370, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 1.6028225421905518\n",
      "Step: 1371, Loss: 0.9161813855171204, Accuracy: 1.0, Computation time: 1.9423038959503174\n",
      "Step: 1372, Loss: 0.9167921543121338, Accuracy: 1.0, Computation time: 2.2012832164764404\n",
      "Step: 1373, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 2.1042370796203613\n",
      "Step: 1374, Loss: 0.9439049363136292, Accuracy: 0.96875, Computation time: 1.8538742065429688\n",
      "Step: 1375, Loss: 0.9377706050872803, Accuracy: 0.9821428656578064, Computation time: 1.8438551425933838\n",
      "Step: 1376, Loss: 0.9327749013900757, Accuracy: 0.96875, Computation time: 1.83439040184021\n",
      "Step: 1377, Loss: 0.9163197875022888, Accuracy: 1.0, Computation time: 1.7681121826171875\n",
      "Step: 1378, Loss: 0.9160135388374329, Accuracy: 1.0, Computation time: 1.681053638458252\n",
      "Step: 1379, Loss: 0.9374343156814575, Accuracy: 0.9750000238418579, Computation time: 1.759876012802124\n",
      "Step: 1380, Loss: 0.9163506031036377, Accuracy: 1.0, Computation time: 1.6722564697265625\n",
      "Step: 1381, Loss: 0.9160813689231873, Accuracy: 1.0, Computation time: 1.4773094654083252\n",
      "Step: 1382, Loss: 0.9160759449005127, Accuracy: 1.0, Computation time: 1.485429286956787\n",
      "Step: 1383, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 2.0567688941955566\n",
      "Step: 1384, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.9363899230957031\n",
      "Step: 1385, Loss: 0.9377501010894775, Accuracy: 0.96875, Computation time: 1.800330638885498\n",
      "Step: 1386, Loss: 0.9160513877868652, Accuracy: 1.0, Computation time: 2.0172841548919678\n",
      "Step: 1387, Loss: 0.9193479418754578, Accuracy: 1.0, Computation time: 1.8324425220489502\n",
      "Step: 1388, Loss: 0.9221490025520325, Accuracy: 1.0, Computation time: 2.1023077964782715\n",
      "Step: 1389, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.8897864818572998\n",
      "Step: 1390, Loss: 0.9376949667930603, Accuracy: 0.96875, Computation time: 1.674720287322998\n",
      "########################\n",
      "Test loss: 1.1163455247879028, Test Accuracy_epoch10: 0.7276653051376343\n",
      "########################\n",
      "Step: 1391, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 1.5598185062408447\n",
      "Step: 1392, Loss: 0.9204393625259399, Accuracy: nan, Computation time: 2.3024404048919678\n",
      "Step: 1393, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 1.6087586879730225\n",
      "Step: 1394, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 1.598430871963501\n",
      "Step: 1395, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 2.099297523498535\n",
      "Step: 1396, Loss: 0.9161055088043213, Accuracy: 1.0, Computation time: 1.9136619567871094\n",
      "Step: 1397, Loss: 0.9159836769104004, Accuracy: 1.0, Computation time: 1.8582851886749268\n",
      "Step: 1398, Loss: 0.9159654378890991, Accuracy: 1.0, Computation time: 1.8938486576080322\n",
      "Step: 1399, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 1.7629978656768799\n",
      "Step: 1400, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.9960873126983643\n",
      "Step: 1401, Loss: 0.9159359335899353, Accuracy: 1.0, Computation time: 1.9105901718139648\n",
      "Step: 1402, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.7829899787902832\n",
      "Step: 1403, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 2.3385941982269287\n",
      "Step: 1404, Loss: 0.9160655736923218, Accuracy: 1.0, Computation time: 1.6278417110443115\n",
      "Step: 1405, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 2.1166603565216064\n",
      "Step: 1406, Loss: 0.9176496267318726, Accuracy: 1.0, Computation time: 2.2513210773468018\n",
      "Step: 1407, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.815596103668213\n",
      "Step: 1408, Loss: 0.9160902500152588, Accuracy: 1.0, Computation time: 2.1119887828826904\n",
      "Step: 1409, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 1.99092435836792\n",
      "Step: 1410, Loss: 0.9159425497055054, Accuracy: 1.0, Computation time: 1.9340453147888184\n",
      "Step: 1411, Loss: 0.915984034538269, Accuracy: 1.0, Computation time: 2.192898988723755\n",
      "Step: 1412, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 2.2392661571502686\n",
      "Step: 1413, Loss: 0.9159297347068787, Accuracy: 1.0, Computation time: 1.8445522785186768\n",
      "Step: 1414, Loss: 0.937532365322113, Accuracy: 0.9772727489471436, Computation time: 2.2160470485687256\n",
      "Step: 1415, Loss: 0.9159726500511169, Accuracy: 1.0, Computation time: 1.8012645244598389\n",
      "Step: 1416, Loss: 0.9372932314872742, Accuracy: 0.9807692766189575, Computation time: 2.1338613033294678\n",
      "Step: 1417, Loss: 0.9171441197395325, Accuracy: 1.0, Computation time: 2.1315691471099854\n",
      "Step: 1418, Loss: 0.9166558980941772, Accuracy: 1.0, Computation time: 1.738424301147461\n",
      "Step: 1419, Loss: 0.9179211258888245, Accuracy: 1.0, Computation time: 1.9316728115081787\n",
      "Step: 1420, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 1.9755816459655762\n",
      "Step: 1421, Loss: 0.9166911244392395, Accuracy: 1.0, Computation time: 2.1731836795806885\n",
      "Step: 1422, Loss: 0.9159139394760132, Accuracy: 1.0, Computation time: 1.924680233001709\n",
      "Step: 1423, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.587155818939209\n",
      "Step: 1424, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 2.0001180171966553\n",
      "Step: 1425, Loss: 0.9202882051467896, Accuracy: 1.0, Computation time: 1.9895820617675781\n",
      "Step: 1426, Loss: 0.9171436429023743, Accuracy: 1.0, Computation time: 2.1468889713287354\n",
      "Step: 1427, Loss: 0.9160110950469971, Accuracy: 1.0, Computation time: 2.071580410003662\n",
      "Step: 1428, Loss: 0.9159897565841675, Accuracy: 1.0, Computation time: 1.9875528812408447\n",
      "Step: 1429, Loss: 0.9159365296363831, Accuracy: 1.0, Computation time: 1.8864421844482422\n",
      "Step: 1430, Loss: 0.9159713983535767, Accuracy: 1.0, Computation time: 1.5853440761566162\n",
      "Step: 1431, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 2.1478519439697266\n",
      "Step: 1432, Loss: 0.9377102255821228, Accuracy: 0.9833333492279053, Computation time: 2.132448196411133\n",
      "Step: 1433, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 2.073154926300049\n",
      "Step: 1434, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.9033567905426025\n",
      "Step: 1435, Loss: 0.9370657801628113, Accuracy: 0.9642857313156128, Computation time: 2.269230842590332\n",
      "Step: 1436, Loss: 0.9363358020782471, Accuracy: 0.9772727489471436, Computation time: 1.8274471759796143\n",
      "Step: 1437, Loss: 0.9162342548370361, Accuracy: 1.0, Computation time: 1.8865644931793213\n",
      "Step: 1438, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 2.945305109024048\n",
      "Step: 1439, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 1.7805261611938477\n",
      "Step: 1440, Loss: 0.916284441947937, Accuracy: 1.0, Computation time: 2.625417470932007\n",
      "Step: 1441, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.8694374561309814\n",
      "Step: 1442, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 1.6559317111968994\n",
      "Step: 1443, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 1.644090175628662\n",
      "Step: 1444, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.9518938064575195\n",
      "Step: 1445, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 1.8480188846588135\n",
      "Step: 1446, Loss: 0.9169097542762756, Accuracy: 1.0, Computation time: 1.9514918327331543\n",
      "Step: 1447, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 2.1972103118896484\n",
      "Step: 1448, Loss: 0.9162390828132629, Accuracy: 1.0, Computation time: 1.6098079681396484\n",
      "Step: 1449, Loss: 0.937620997428894, Accuracy: 0.9642857313156128, Computation time: 1.9161274433135986\n",
      "Step: 1450, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.8411211967468262\n",
      "Step: 1451, Loss: 0.9308820962905884, Accuracy: 0.96875, Computation time: 2.1150901317596436\n",
      "Step: 1452, Loss: 0.9175320863723755, Accuracy: 1.0, Computation time: 1.9622867107391357\n",
      "Step: 1453, Loss: 0.9378319382667542, Accuracy: 0.949999988079071, Computation time: 2.1902005672454834\n",
      "Step: 1454, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 2.26743483543396\n",
      "Step: 1455, Loss: 0.9159841537475586, Accuracy: 1.0, Computation time: 1.7696864604949951\n",
      "Step: 1456, Loss: 0.9160401821136475, Accuracy: 1.0, Computation time: 1.5785930156707764\n",
      "Step: 1457, Loss: 0.9160149097442627, Accuracy: 1.0, Computation time: 1.8867805004119873\n",
      "Step: 1458, Loss: 0.9172912836074829, Accuracy: 1.0, Computation time: 1.964848518371582\n",
      "Step: 1459, Loss: 0.9159658551216125, Accuracy: 1.0, Computation time: 1.8433258533477783\n",
      "Step: 1460, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.6864697933197021\n",
      "Step: 1461, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 2.9457483291625977\n",
      "Step: 1462, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 1.7201333045959473\n",
      "Step: 1463, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 1.9396276473999023\n",
      "Step: 1464, Loss: 0.9486796855926514, Accuracy: 0.9272727370262146, Computation time: 2.0827534198760986\n",
      "Step: 1465, Loss: 0.9367306232452393, Accuracy: 0.96875, Computation time: 2.2051808834075928\n",
      "Step: 1466, Loss: 0.9376838803291321, Accuracy: 0.9722222089767456, Computation time: 1.9121417999267578\n",
      "Step: 1467, Loss: 0.9367241859436035, Accuracy: 0.9722222089767456, Computation time: 2.0120463371276855\n",
      "Step: 1468, Loss: 0.9177742004394531, Accuracy: 1.0, Computation time: 2.6026487350463867\n",
      "Step: 1469, Loss: 0.9161009192466736, Accuracy: 1.0, Computation time: 2.187966823577881\n",
      "Step: 1470, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 2.0736875534057617\n",
      "Step: 1471, Loss: 0.9159853458404541, Accuracy: 1.0, Computation time: 1.8058924674987793\n",
      "Step: 1472, Loss: 0.9159507751464844, Accuracy: 1.0, Computation time: 1.917285680770874\n",
      "Step: 1473, Loss: 0.9182078838348389, Accuracy: 1.0, Computation time: 2.082012891769409\n",
      "Step: 1474, Loss: 0.9380204081535339, Accuracy: 0.9583333730697632, Computation time: 1.7789747714996338\n",
      "Step: 1475, Loss: 0.9159600734710693, Accuracy: 1.0, Computation time: 1.8846008777618408\n",
      "Step: 1476, Loss: 0.9375660419464111, Accuracy: 0.949999988079071, Computation time: 1.8950862884521484\n",
      "Step: 1477, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 1.8893234729766846\n",
      "Step: 1478, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.7890756130218506\n",
      "Step: 1479, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 2.148378849029541\n",
      "Step: 1480, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.9354815483093262\n",
      "Step: 1481, Loss: 0.9377136826515198, Accuracy: 0.9772727489471436, Computation time: 1.7435948848724365\n",
      "Step: 1482, Loss: 0.9159259796142578, Accuracy: 1.0, Computation time: 2.0559499263763428\n",
      "Step: 1483, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 1.9953889846801758\n",
      "Step: 1484, Loss: 0.9159729480743408, Accuracy: 1.0, Computation time: 1.915255069732666\n",
      "Step: 1485, Loss: 0.936682939529419, Accuracy: 0.9772727489471436, Computation time: 1.9805903434753418\n",
      "Step: 1486, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.8897857666015625\n",
      "Step: 1487, Loss: 0.9159395694732666, Accuracy: 1.0, Computation time: 1.6787760257720947\n",
      "Step: 1488, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.5743162631988525\n",
      "Step: 1489, Loss: 0.9161176085472107, Accuracy: 1.0, Computation time: 2.253791570663452\n",
      "Step: 1490, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.8771588802337646\n",
      "Step: 1491, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.7067234516143799\n",
      "Step: 1492, Loss: 0.9159372448921204, Accuracy: 1.0, Computation time: 1.7533655166625977\n",
      "Step: 1493, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 2.048444986343384\n",
      "Step: 1494, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 2.236581563949585\n",
      "Step: 1495, Loss: 0.9376352429389954, Accuracy: 0.9772727489471436, Computation time: 1.518550157546997\n",
      "Step: 1496, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.907041311264038\n",
      "Step: 1497, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.9084155559539795\n",
      "Step: 1498, Loss: 0.9160110354423523, Accuracy: 1.0, Computation time: 1.867173433303833\n",
      "Step: 1499, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 2.075251579284668\n",
      "Step: 1500, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 1.770235538482666\n",
      "Step: 1501, Loss: 0.9166982769966125, Accuracy: 1.0, Computation time: 2.359598159790039\n",
      "Step: 1502, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 2.513120651245117\n",
      "Step: 1503, Loss: 0.9252433180809021, Accuracy: 1.0, Computation time: 2.025845766067505\n",
      "Step: 1504, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 2.0123746395111084\n",
      "Step: 1505, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 2.452272891998291\n",
      "Step: 1506, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 1.9456698894500732\n",
      "Step: 1507, Loss: 0.9364334940910339, Accuracy: 0.9772727489471436, Computation time: 1.7594037055969238\n",
      "Step: 1508, Loss: 0.9159671068191528, Accuracy: 1.0, Computation time: 2.2018637657165527\n",
      "Step: 1509, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 2.0324745178222656\n",
      "Step: 1510, Loss: 0.9161175489425659, Accuracy: 1.0, Computation time: 1.7641372680664062\n",
      "Step: 1511, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 1.6719589233398438\n",
      "Step: 1512, Loss: 0.9168329834938049, Accuracy: 1.0, Computation time: 1.8819468021392822\n",
      "Step: 1513, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 2.2579269409179688\n",
      "Step: 1514, Loss: 0.9187275767326355, Accuracy: 1.0, Computation time: 2.1259782314300537\n",
      "Step: 1515, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.9932069778442383\n",
      "Step: 1516, Loss: 0.9376090168952942, Accuracy: 0.96875, Computation time: 2.246406078338623\n",
      "Step: 1517, Loss: 0.9175134897232056, Accuracy: 1.0, Computation time: 1.6943485736846924\n",
      "Step: 1518, Loss: 0.9160459041595459, Accuracy: 1.0, Computation time: 1.8313496112823486\n",
      "Step: 1519, Loss: 0.9160442352294922, Accuracy: 1.0, Computation time: 1.8856959342956543\n",
      "Step: 1520, Loss: 0.916048526763916, Accuracy: 1.0, Computation time: 2.056551933288574\n",
      "Step: 1521, Loss: 0.9161655306816101, Accuracy: 1.0, Computation time: 2.2844622135162354\n",
      "Step: 1522, Loss: 0.934008002281189, Accuracy: 0.9375, Computation time: 2.381577253341675\n",
      "Step: 1523, Loss: 0.9159573912620544, Accuracy: 1.0, Computation time: 1.7095813751220703\n",
      "Step: 1524, Loss: 0.9173497557640076, Accuracy: 1.0, Computation time: 2.287041425704956\n",
      "Step: 1525, Loss: 0.9159790873527527, Accuracy: 1.0, Computation time: 1.8858537673950195\n",
      "Step: 1526, Loss: 0.9367972612380981, Accuracy: 0.9750000238418579, Computation time: 2.390900135040283\n",
      "Step: 1527, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.9521148204803467\n",
      "Step: 1528, Loss: 0.9160459041595459, Accuracy: 1.0, Computation time: 1.7143800258636475\n",
      "Step: 1529, Loss: 0.9160280823707581, Accuracy: 1.0, Computation time: 1.6419854164123535\n",
      "########################\n",
      "Test loss: 1.1239911317825317, Test Accuracy_epoch11: 0.7275787591934204\n",
      "########################\n",
      "Step: 1530, Loss: 0.9166630506515503, Accuracy: 1.0, Computation time: 1.764085292816162\n",
      "Step: 1531, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.860469102859497\n",
      "Step: 1532, Loss: 0.9379326701164246, Accuracy: 0.9750000238418579, Computation time: 1.6475560665130615\n",
      "Step: 1533, Loss: 0.9159401655197144, Accuracy: 1.0, Computation time: 1.7292594909667969\n",
      "Step: 1534, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 1.7725417613983154\n",
      "Step: 1535, Loss: 0.915976881980896, Accuracy: 1.0, Computation time: 1.7302567958831787\n",
      "Step: 1536, Loss: 0.9159429669380188, Accuracy: 1.0, Computation time: 1.6354389190673828\n",
      "Step: 1537, Loss: 0.9178634285926819, Accuracy: 1.0, Computation time: 1.7467138767242432\n",
      "Step: 1538, Loss: 0.9159942269325256, Accuracy: 1.0, Computation time: 2.118422269821167\n",
      "Step: 1539, Loss: 0.9168252944946289, Accuracy: 1.0, Computation time: 1.7253963947296143\n",
      "Step: 1540, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.861692190170288\n",
      "Step: 1541, Loss: 0.9160240292549133, Accuracy: 1.0, Computation time: 2.0345866680145264\n",
      "Step: 1542, Loss: 0.9159544110298157, Accuracy: 1.0, Computation time: 1.955193042755127\n",
      "Step: 1543, Loss: 0.9378188252449036, Accuracy: 0.9772727489471436, Computation time: 2.135526180267334\n",
      "Step: 1544, Loss: 0.9334521293640137, Accuracy: 0.984375, Computation time: 2.4639017581939697\n",
      "Step: 1545, Loss: 0.9159278869628906, Accuracy: 1.0, Computation time: 1.9851462841033936\n",
      "Step: 1546, Loss: 0.9160784482955933, Accuracy: 1.0, Computation time: 2.544835090637207\n",
      "Step: 1547, Loss: 0.9159581661224365, Accuracy: 1.0, Computation time: 1.8554050922393799\n",
      "Step: 1548, Loss: 0.9378541111946106, Accuracy: 0.9772727489471436, Computation time: 1.820542335510254\n",
      "Step: 1549, Loss: 0.9160068035125732, Accuracy: 1.0, Computation time: 2.3653340339660645\n",
      "Step: 1550, Loss: 0.916140079498291, Accuracy: 1.0, Computation time: 2.1012377738952637\n",
      "Step: 1551, Loss: 0.9192207455635071, Accuracy: 1.0, Computation time: 2.137547731399536\n",
      "Step: 1552, Loss: 0.9353646039962769, Accuracy: 0.9166666865348816, Computation time: 2.5318360328674316\n",
      "Step: 1553, Loss: 0.9159553647041321, Accuracy: 1.0, Computation time: 1.8206446170806885\n",
      "Step: 1554, Loss: 0.9375089406967163, Accuracy: 0.9722222089767456, Computation time: 1.7601268291473389\n",
      "Step: 1555, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.9510338306427002\n",
      "Step: 1556, Loss: 0.9372491836547852, Accuracy: 0.96875, Computation time: 2.120684862136841\n",
      "Step: 1557, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 2.283989429473877\n",
      "Step: 1558, Loss: 0.9160247445106506, Accuracy: 1.0, Computation time: 2.0546982288360596\n",
      "Step: 1559, Loss: 0.9368546009063721, Accuracy: 0.9583333730697632, Computation time: 2.5948662757873535\n",
      "Step: 1560, Loss: 0.9159736037254333, Accuracy: 1.0, Computation time: 1.8852427005767822\n",
      "Step: 1561, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 2.1264874935150146\n",
      "Step: 1562, Loss: 0.9159294962882996, Accuracy: 1.0, Computation time: 2.2776739597320557\n",
      "Step: 1563, Loss: 0.9374813437461853, Accuracy: 0.9772727489471436, Computation time: 1.8097882270812988\n",
      "Step: 1564, Loss: 0.9570953249931335, Accuracy: 0.949999988079071, Computation time: 1.7747435569763184\n",
      "Step: 1565, Loss: 0.9442845582962036, Accuracy: 0.9642857313156128, Computation time: 2.4702658653259277\n",
      "Step: 1566, Loss: 0.9160370826721191, Accuracy: 1.0, Computation time: 1.7837998867034912\n",
      "Step: 1567, Loss: 0.9160647392272949, Accuracy: 1.0, Computation time: 1.8192458152770996\n",
      "Step: 1568, Loss: 0.9159976243972778, Accuracy: 1.0, Computation time: 1.748150110244751\n",
      "Step: 1569, Loss: 0.936721920967102, Accuracy: 0.9772727489471436, Computation time: 1.9622855186462402\n",
      "Step: 1570, Loss: 0.9159551858901978, Accuracy: 1.0, Computation time: 1.693692922592163\n",
      "Step: 1571, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.9614853858947754\n",
      "Step: 1572, Loss: 0.9375114440917969, Accuracy: 0.9642857313156128, Computation time: 1.9514141082763672\n",
      "Step: 1573, Loss: 0.9159653782844543, Accuracy: 1.0, Computation time: 1.9814279079437256\n",
      "Step: 1574, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.9332611560821533\n",
      "Step: 1575, Loss: 0.9160386323928833, Accuracy: 1.0, Computation time: 2.483874797821045\n",
      "Step: 1576, Loss: 0.9159626960754395, Accuracy: 1.0, Computation time: 1.8425860404968262\n",
      "Step: 1577, Loss: 0.9159075617790222, Accuracy: 1.0, Computation time: 1.9834108352661133\n",
      "Step: 1578, Loss: 0.9159426093101501, Accuracy: 1.0, Computation time: 1.938636064529419\n",
      "Step: 1579, Loss: 0.9160634875297546, Accuracy: 1.0, Computation time: 1.8737778663635254\n",
      "Step: 1580, Loss: 0.9159730672836304, Accuracy: 1.0, Computation time: 2.089984893798828\n",
      "Step: 1581, Loss: 0.9435192942619324, Accuracy: 0.9791666865348816, Computation time: 2.1812615394592285\n",
      "Step: 1582, Loss: 0.9159121513366699, Accuracy: 1.0, Computation time: 2.0404248237609863\n",
      "Step: 1583, Loss: 0.9165788292884827, Accuracy: 1.0, Computation time: 2.154242515563965\n",
      "Step: 1584, Loss: 0.9159648418426514, Accuracy: 1.0, Computation time: 1.9496088027954102\n",
      "Step: 1585, Loss: 0.9159968495368958, Accuracy: 1.0, Computation time: 1.6670806407928467\n",
      "Step: 1586, Loss: 0.9159749746322632, Accuracy: 1.0, Computation time: 1.9600071907043457\n",
      "Step: 1587, Loss: 0.9159877896308899, Accuracy: 1.0, Computation time: 1.6859848499298096\n",
      "Step: 1588, Loss: 0.916100263595581, Accuracy: 1.0, Computation time: 2.349268913269043\n",
      "Step: 1589, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 2.1172847747802734\n",
      "Step: 1590, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 2.0688538551330566\n",
      "Step: 1591, Loss: 0.9200263023376465, Accuracy: 1.0, Computation time: 2.237680196762085\n",
      "Step: 1592, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.8613848686218262\n",
      "Step: 1593, Loss: 0.9363203644752502, Accuracy: 0.9722222089767456, Computation time: 1.8629932403564453\n",
      "Step: 1594, Loss: 0.916079044342041, Accuracy: 1.0, Computation time: 2.420994281768799\n",
      "Step: 1595, Loss: 0.9368473291397095, Accuracy: 0.9807692766189575, Computation time: 2.1820409297943115\n",
      "Step: 1596, Loss: 0.9163231253623962, Accuracy: 1.0, Computation time: 1.6933510303497314\n",
      "Step: 1597, Loss: 0.9160919785499573, Accuracy: 1.0, Computation time: 1.679048776626587\n",
      "Step: 1598, Loss: 0.916027843952179, Accuracy: 1.0, Computation time: 1.9124252796173096\n",
      "Step: 1599, Loss: 0.9159729480743408, Accuracy: 1.0, Computation time: 2.119325876235962\n",
      "Step: 1600, Loss: 0.9161714315414429, Accuracy: 1.0, Computation time: 1.7789952754974365\n",
      "Step: 1601, Loss: 0.9256470799446106, Accuracy: 0.9583333730697632, Computation time: 2.422089099884033\n",
      "Step: 1602, Loss: 0.9161059260368347, Accuracy: 1.0, Computation time: 2.050074577331543\n",
      "Step: 1603, Loss: 0.9294765591621399, Accuracy: 0.9772727489471436, Computation time: 1.9853882789611816\n",
      "Step: 1604, Loss: 0.91593998670578, Accuracy: 1.0, Computation time: 1.8703765869140625\n",
      "Step: 1605, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 1.671389102935791\n",
      "Step: 1606, Loss: 0.9160226583480835, Accuracy: 1.0, Computation time: 2.1051621437072754\n",
      "Step: 1607, Loss: 0.9160890579223633, Accuracy: 1.0, Computation time: 2.3458127975463867\n",
      "Step: 1608, Loss: 0.9160163402557373, Accuracy: 1.0, Computation time: 2.3719098567962646\n",
      "Step: 1609, Loss: 0.9159935116767883, Accuracy: 1.0, Computation time: 1.6835124492645264\n",
      "Step: 1610, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 2.0056538581848145\n",
      "Step: 1611, Loss: 0.9174205660820007, Accuracy: 1.0, Computation time: 1.8504674434661865\n",
      "Step: 1612, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.649996042251587\n",
      "Step: 1613, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 2.210625410079956\n",
      "Step: 1614, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.7540309429168701\n",
      "Step: 1615, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 2.064077138900757\n",
      "Step: 1616, Loss: 0.9159456491470337, Accuracy: 1.0, Computation time: 1.8102688789367676\n",
      "Step: 1617, Loss: 0.9182243347167969, Accuracy: 1.0, Computation time: 1.9575309753417969\n",
      "Step: 1618, Loss: 0.9159904718399048, Accuracy: 1.0, Computation time: 1.9706854820251465\n",
      "Step: 1619, Loss: 0.9162302017211914, Accuracy: 1.0, Computation time: 1.8212943077087402\n",
      "Step: 1620, Loss: 0.9375278949737549, Accuracy: 0.9807692766189575, Computation time: 2.221315383911133\n",
      "Step: 1621, Loss: 0.9167033433914185, Accuracy: 1.0, Computation time: 1.8399732112884521\n",
      "Step: 1622, Loss: 0.9160646200180054, Accuracy: 1.0, Computation time: 1.993384838104248\n",
      "Step: 1623, Loss: 0.917790412902832, Accuracy: 1.0, Computation time: 1.93790864944458\n",
      "Step: 1624, Loss: 0.915992021560669, Accuracy: 1.0, Computation time: 1.9180169105529785\n",
      "Step: 1625, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 2.0445072650909424\n",
      "Step: 1626, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.704725980758667\n",
      "Step: 1627, Loss: 0.9375739097595215, Accuracy: 0.9791666865348816, Computation time: 2.3571770191192627\n",
      "Step: 1628, Loss: 0.9159199595451355, Accuracy: 1.0, Computation time: 1.7407009601593018\n",
      "Step: 1629, Loss: 0.9168046712875366, Accuracy: 1.0, Computation time: 1.9993913173675537\n",
      "Step: 1630, Loss: 0.9377021193504333, Accuracy: 0.9722222089767456, Computation time: 1.9252839088439941\n",
      "Step: 1631, Loss: 0.9342426657676697, Accuracy: 0.9772727489471436, Computation time: 2.115182399749756\n",
      "Step: 1632, Loss: 0.9357048869132996, Accuracy: 0.9750000238418579, Computation time: 1.6354773044586182\n",
      "Step: 1633, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 1.9611737728118896\n",
      "Step: 1634, Loss: 0.9166486263275146, Accuracy: 1.0, Computation time: 1.938673973083496\n",
      "Step: 1635, Loss: 0.916063666343689, Accuracy: 1.0, Computation time: 2.0395774841308594\n",
      "Step: 1636, Loss: 0.915989875793457, Accuracy: 1.0, Computation time: 1.8349969387054443\n",
      "Step: 1637, Loss: 0.9160683751106262, Accuracy: 1.0, Computation time: 2.054191827774048\n",
      "Step: 1638, Loss: 0.9249805808067322, Accuracy: 1.0, Computation time: 2.332202911376953\n",
      "Step: 1639, Loss: 0.916041910648346, Accuracy: 1.0, Computation time: 1.9680607318878174\n",
      "Step: 1640, Loss: 0.9159542322158813, Accuracy: 1.0, Computation time: 1.9476408958435059\n",
      "Step: 1641, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 1.8261353969573975\n",
      "Step: 1642, Loss: 0.9163234233856201, Accuracy: 1.0, Computation time: 1.9918804168701172\n",
      "Step: 1643, Loss: 0.916083037853241, Accuracy: 1.0, Computation time: 2.1013455390930176\n",
      "Step: 1644, Loss: 0.9374445676803589, Accuracy: 0.9583333730697632, Computation time: 1.9531795978546143\n",
      "Step: 1645, Loss: 0.9160414934158325, Accuracy: 1.0, Computation time: 2.20155930519104\n",
      "Step: 1646, Loss: 0.9159443974494934, Accuracy: 1.0, Computation time: 1.9189014434814453\n",
      "Step: 1647, Loss: 0.9159212708473206, Accuracy: 1.0, Computation time: 1.9076569080352783\n",
      "Step: 1648, Loss: 0.9161176085472107, Accuracy: 1.0, Computation time: 2.0023629665374756\n",
      "Step: 1649, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.879338026046753\n",
      "Step: 1650, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.8243727684020996\n",
      "Step: 1651, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 1.9251351356506348\n",
      "Step: 1652, Loss: 0.933624267578125, Accuracy: 0.9583333730697632, Computation time: 2.0405514240264893\n",
      "Step: 1653, Loss: 0.9160556793212891, Accuracy: 1.0, Computation time: 2.076300859451294\n",
      "Step: 1654, Loss: 0.9282320141792297, Accuracy: 0.9750000238418579, Computation time: 2.097203493118286\n",
      "Step: 1655, Loss: 0.9159738421440125, Accuracy: 1.0, Computation time: 1.7634940147399902\n",
      "Step: 1656, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.958303689956665\n",
      "Step: 1657, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.884807825088501\n",
      "Step: 1658, Loss: 0.9170056581497192, Accuracy: 1.0, Computation time: 1.806436538696289\n",
      "Step: 1659, Loss: 0.9159684181213379, Accuracy: 1.0, Computation time: 1.641331434249878\n",
      "Step: 1660, Loss: 0.9159736037254333, Accuracy: 1.0, Computation time: 1.875246286392212\n",
      "Step: 1661, Loss: 0.916132926940918, Accuracy: 1.0, Computation time: 1.755577564239502\n",
      "Step: 1662, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 1.7585952281951904\n",
      "Step: 1663, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 1.6808226108551025\n",
      "Step: 1664, Loss: 0.9372620582580566, Accuracy: 0.9772727489471436, Computation time: 1.7071855068206787\n",
      "Step: 1665, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 1.9599609375\n",
      "Step: 1666, Loss: 0.9160703420639038, Accuracy: 1.0, Computation time: 1.864732265472412\n",
      "Step: 1667, Loss: 0.937662661075592, Accuracy: 0.9772727489471436, Computation time: 1.6911041736602783\n",
      "Step: 1668, Loss: 0.9161544442176819, Accuracy: 1.0, Computation time: 2.3358235359191895\n",
      "########################\n",
      "Test loss: 1.1140942573547363, Test Accuracy_epoch12: 0.729571521282196\n",
      "########################\n",
      "Step: 1669, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 1.9086341857910156\n",
      "Step: 1670, Loss: 0.961154043674469, Accuracy: 0.9642857313156128, Computation time: 1.759728193283081\n",
      "Step: 1671, Loss: 0.9160891175270081, Accuracy: 1.0, Computation time: 1.8819997310638428\n",
      "Step: 1672, Loss: 0.9160104990005493, Accuracy: 1.0, Computation time: 1.973649501800537\n",
      "Step: 1673, Loss: 0.9293290376663208, Accuracy: 0.949999988079071, Computation time: 2.1277692317962646\n",
      "Step: 1674, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.9889781475067139\n",
      "Step: 1675, Loss: 0.9173570275306702, Accuracy: 1.0, Computation time: 1.7355520725250244\n",
      "Step: 1676, Loss: 0.9165843725204468, Accuracy: 1.0, Computation time: 1.8945426940917969\n",
      "Step: 1677, Loss: 0.9160455465316772, Accuracy: 1.0, Computation time: 1.7741212844848633\n",
      "Step: 1678, Loss: 0.9165197014808655, Accuracy: 1.0, Computation time: 2.0063955783843994\n",
      "Step: 1679, Loss: 0.9160179495811462, Accuracy: 1.0, Computation time: 1.756995439529419\n",
      "Step: 1680, Loss: 0.9596247673034668, Accuracy: 0.9270833730697632, Computation time: 2.2507338523864746\n",
      "Step: 1681, Loss: 0.9160028100013733, Accuracy: 1.0, Computation time: 1.7729661464691162\n",
      "Step: 1682, Loss: 0.9159354567527771, Accuracy: 1.0, Computation time: 1.743811845779419\n",
      "Step: 1683, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.640214443206787\n",
      "Step: 1684, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.567488193511963\n",
      "Step: 1685, Loss: 0.937561571598053, Accuracy: 0.9722222089767456, Computation time: 1.843522071838379\n",
      "Step: 1686, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 2.185962677001953\n",
      "Step: 1687, Loss: 0.9159400463104248, Accuracy: 1.0, Computation time: 1.7357547283172607\n",
      "Step: 1688, Loss: 0.9378370046615601, Accuracy: 0.9722222089767456, Computation time: 2.058647394180298\n",
      "Step: 1689, Loss: 0.918958306312561, Accuracy: 1.0, Computation time: 2.059856414794922\n",
      "Step: 1690, Loss: 0.9158942103385925, Accuracy: 1.0, Computation time: 2.3512632846832275\n",
      "Step: 1691, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 2.1921329498291016\n",
      "Step: 1692, Loss: 0.9161051511764526, Accuracy: 1.0, Computation time: 2.1574323177337646\n",
      "Step: 1693, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 2.3014042377471924\n",
      "Step: 1694, Loss: 0.9362474679946899, Accuracy: 0.9791666865348816, Computation time: 2.195415735244751\n",
      "Step: 1695, Loss: 0.9158821105957031, Accuracy: nan, Computation time: 1.8790218830108643\n",
      "Step: 1696, Loss: 0.9159318804740906, Accuracy: 1.0, Computation time: 1.782637357711792\n",
      "Step: 1697, Loss: 0.9171050786972046, Accuracy: 1.0, Computation time: 1.8119845390319824\n",
      "Step: 1698, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 2.202298164367676\n",
      "Step: 1699, Loss: 0.937655508518219, Accuracy: 0.9722222089767456, Computation time: 1.9460551738739014\n",
      "Step: 1700, Loss: 0.9371219873428345, Accuracy: 0.9772727489471436, Computation time: 1.9332301616668701\n",
      "Step: 1701, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 2.418370008468628\n",
      "Step: 1702, Loss: 0.9178233742713928, Accuracy: 1.0, Computation time: 1.7496347427368164\n",
      "Step: 1703, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 2.4096922874450684\n",
      "Step: 1704, Loss: 0.9181028604507446, Accuracy: 1.0, Computation time: 1.8924050331115723\n",
      "Step: 1705, Loss: 0.9159032106399536, Accuracy: 1.0, Computation time: 2.3419394493103027\n",
      "Step: 1706, Loss: 0.9473185539245605, Accuracy: 0.9437500238418579, Computation time: 2.1550328731536865\n",
      "Step: 1707, Loss: 0.9161525964736938, Accuracy: 1.0, Computation time: 1.6530652046203613\n",
      "Step: 1708, Loss: 0.9159757494926453, Accuracy: 1.0, Computation time: 1.8085718154907227\n",
      "Step: 1709, Loss: 0.9160153269767761, Accuracy: 1.0, Computation time: 1.718189001083374\n",
      "Step: 1710, Loss: 0.9160054326057434, Accuracy: 1.0, Computation time: 1.9592673778533936\n",
      "Step: 1711, Loss: 0.9160328507423401, Accuracy: 1.0, Computation time: 1.7267673015594482\n",
      "Step: 1712, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.9419691562652588\n",
      "Step: 1713, Loss: 0.9163002371788025, Accuracy: 1.0, Computation time: 2.1033549308776855\n",
      "Step: 1714, Loss: 0.9160183072090149, Accuracy: 1.0, Computation time: 2.0300469398498535\n",
      "Step: 1715, Loss: 0.9201081991195679, Accuracy: 1.0, Computation time: 2.314387083053589\n",
      "Step: 1716, Loss: 0.9159723520278931, Accuracy: 1.0, Computation time: 2.2416465282440186\n",
      "Step: 1717, Loss: 0.9160292148590088, Accuracy: 1.0, Computation time: 1.8901557922363281\n",
      "Step: 1718, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 1.9369549751281738\n",
      "Step: 1719, Loss: 0.9159823060035706, Accuracy: 1.0, Computation time: 1.823927402496338\n",
      "Step: 1720, Loss: 0.9172791242599487, Accuracy: 1.0, Computation time: 1.925184965133667\n",
      "Step: 1721, Loss: 0.9161698222160339, Accuracy: 1.0, Computation time: 1.6251235008239746\n",
      "Step: 1722, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.8227989673614502\n",
      "Step: 1723, Loss: 0.9164459109306335, Accuracy: 1.0, Computation time: 1.9904615879058838\n",
      "Step: 1724, Loss: 0.9159197807312012, Accuracy: 1.0, Computation time: 1.6809148788452148\n",
      "Step: 1725, Loss: 0.9159395694732666, Accuracy: 1.0, Computation time: 2.0941238403320312\n",
      "Step: 1726, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 1.7606101036071777\n",
      "Step: 1727, Loss: 0.9160171747207642, Accuracy: 1.0, Computation time: 1.9437451362609863\n",
      "Step: 1728, Loss: 0.9159292578697205, Accuracy: 1.0, Computation time: 1.783961534500122\n",
      "Step: 1729, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.7608819007873535\n",
      "Step: 1730, Loss: 0.9246197938919067, Accuracy: 1.0, Computation time: 1.872046709060669\n",
      "Step: 1731, Loss: 0.9378457069396973, Accuracy: 0.9772727489471436, Computation time: 2.0365309715270996\n",
      "Step: 1732, Loss: 0.9159401059150696, Accuracy: 1.0, Computation time: 2.248889207839966\n",
      "Step: 1733, Loss: 0.9160099625587463, Accuracy: 1.0, Computation time: 1.9676485061645508\n",
      "Step: 1734, Loss: 0.937773585319519, Accuracy: 0.9807692766189575, Computation time: 2.0970263481140137\n",
      "Step: 1735, Loss: 0.9161760807037354, Accuracy: 1.0, Computation time: 2.3403007984161377\n",
      "Step: 1736, Loss: 0.9167138934135437, Accuracy: 1.0, Computation time: 2.314697265625\n",
      "Step: 1737, Loss: 0.9163685441017151, Accuracy: 1.0, Computation time: 2.0056464672088623\n",
      "Step: 1738, Loss: 0.9338474273681641, Accuracy: 0.949999988079071, Computation time: 2.3070521354675293\n",
      "Step: 1739, Loss: 0.9160379767417908, Accuracy: 1.0, Computation time: 2.0040338039398193\n",
      "Step: 1740, Loss: 0.916049063205719, Accuracy: 1.0, Computation time: 1.893423318862915\n",
      "Step: 1741, Loss: 0.9162858724594116, Accuracy: 1.0, Computation time: 2.062513828277588\n",
      "Step: 1742, Loss: 0.916256308555603, Accuracy: 1.0, Computation time: 2.346343755722046\n",
      "Step: 1743, Loss: 0.9339429140090942, Accuracy: 0.9772727489471436, Computation time: 2.472614049911499\n",
      "Step: 1744, Loss: 0.9162262082099915, Accuracy: 1.0, Computation time: 2.4592952728271484\n",
      "Step: 1745, Loss: 0.9160080552101135, Accuracy: 1.0, Computation time: 2.3718159198760986\n",
      "Step: 1746, Loss: 0.9376056790351868, Accuracy: 0.9750000238418579, Computation time: 2.1304800510406494\n",
      "Step: 1747, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 2.380356788635254\n",
      "Step: 1748, Loss: 0.9159870147705078, Accuracy: 1.0, Computation time: 2.270742177963257\n",
      "Step: 1749, Loss: 0.9162408709526062, Accuracy: 1.0, Computation time: 2.2196385860443115\n",
      "Step: 1750, Loss: 0.9160707592964172, Accuracy: 1.0, Computation time: 2.5154924392700195\n",
      "Step: 1751, Loss: 0.9161040782928467, Accuracy: 1.0, Computation time: 2.4573004245758057\n",
      "Step: 1752, Loss: 0.9160778522491455, Accuracy: 1.0, Computation time: 2.212444543838501\n",
      "Step: 1753, Loss: 0.9170068502426147, Accuracy: 1.0, Computation time: 2.2845749855041504\n",
      "Step: 1754, Loss: 0.9160702228546143, Accuracy: 1.0, Computation time: 2.320075750350952\n",
      "Step: 1755, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 1.9811205863952637\n",
      "Step: 1756, Loss: 0.9159983396530151, Accuracy: 1.0, Computation time: 2.0756561756134033\n",
      "Step: 1757, Loss: 0.9162899851799011, Accuracy: 1.0, Computation time: 1.9070663452148438\n",
      "Step: 1758, Loss: 0.9364188313484192, Accuracy: 0.949999988079071, Computation time: 1.7808539867401123\n",
      "Step: 1759, Loss: 0.9162977337837219, Accuracy: 1.0, Computation time: 1.8791370391845703\n",
      "Step: 1760, Loss: 0.9284874796867371, Accuracy: 0.96875, Computation time: 2.7313129901885986\n",
      "Step: 1761, Loss: 0.9236174821853638, Accuracy: 1.0, Computation time: 2.5139193534851074\n",
      "Step: 1762, Loss: 0.9160875082015991, Accuracy: 1.0, Computation time: 2.3604912757873535\n",
      "Step: 1763, Loss: 0.9371529221534729, Accuracy: 0.984375, Computation time: 2.2998206615448\n",
      "Step: 1764, Loss: 0.9164457321166992, Accuracy: 1.0, Computation time: 2.119483232498169\n",
      "Step: 1765, Loss: 0.916408896446228, Accuracy: 1.0, Computation time: 2.024099588394165\n",
      "Step: 1766, Loss: 0.9161776304244995, Accuracy: 1.0, Computation time: 2.022001028060913\n",
      "Step: 1767, Loss: 0.9167320132255554, Accuracy: 1.0, Computation time: 2.1585583686828613\n",
      "Step: 1768, Loss: 0.9160909056663513, Accuracy: 1.0, Computation time: 2.4401888847351074\n",
      "Step: 1769, Loss: 0.9161278605461121, Accuracy: 1.0, Computation time: 2.145906925201416\n",
      "Step: 1770, Loss: 0.9161413908004761, Accuracy: 1.0, Computation time: 2.2101612091064453\n",
      "Step: 1771, Loss: 0.9364559650421143, Accuracy: 0.949999988079071, Computation time: 2.716507911682129\n",
      "Step: 1772, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.996969223022461\n",
      "Step: 1773, Loss: 0.9168534874916077, Accuracy: 1.0, Computation time: 2.16436505317688\n",
      "Step: 1774, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.8365716934204102\n",
      "Step: 1775, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.839313268661499\n",
      "Step: 1776, Loss: 0.915971577167511, Accuracy: 1.0, Computation time: 1.9524991512298584\n",
      "Step: 1777, Loss: 0.9160172343254089, Accuracy: 1.0, Computation time: 2.360597848892212\n",
      "Step: 1778, Loss: 0.9370167255401611, Accuracy: 0.949999988079071, Computation time: 1.7462701797485352\n",
      "Step: 1779, Loss: 0.9160199761390686, Accuracy: 1.0, Computation time: 2.4885246753692627\n",
      "Step: 1780, Loss: 0.9160021543502808, Accuracy: 1.0, Computation time: 2.027606248855591\n",
      "Step: 1781, Loss: 0.9159359335899353, Accuracy: 1.0, Computation time: 2.0941720008850098\n",
      "Step: 1782, Loss: 0.9159290194511414, Accuracy: 1.0, Computation time: 1.8690307140350342\n",
      "Step: 1783, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 2.0755393505096436\n",
      "Step: 1784, Loss: 0.9166479110717773, Accuracy: 1.0, Computation time: 2.1769933700561523\n",
      "Step: 1785, Loss: 0.9164643883705139, Accuracy: 1.0, Computation time: 1.7999131679534912\n",
      "Step: 1786, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 1.746800422668457\n",
      "Step: 1787, Loss: 0.9159888625144958, Accuracy: 1.0, Computation time: 2.1198620796203613\n",
      "Step: 1788, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 2.1717848777770996\n",
      "Step: 1789, Loss: 0.9391767978668213, Accuracy: 0.9821428656578064, Computation time: 2.727895498275757\n",
      "Step: 1790, Loss: 0.9159474968910217, Accuracy: 1.0, Computation time: 2.0533971786499023\n",
      "Step: 1791, Loss: 0.9159356355667114, Accuracy: 1.0, Computation time: 1.7847590446472168\n",
      "Step: 1792, Loss: 0.9159698486328125, Accuracy: 1.0, Computation time: 1.8459923267364502\n",
      "Step: 1793, Loss: 0.9159731864929199, Accuracy: 1.0, Computation time: 1.9635369777679443\n",
      "Step: 1794, Loss: 0.9160488247871399, Accuracy: 1.0, Computation time: 1.9502177238464355\n",
      "Step: 1795, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 2.1868743896484375\n",
      "Step: 1796, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 2.349846601486206\n",
      "Step: 1797, Loss: 0.9159025549888611, Accuracy: 1.0, Computation time: 2.04769229888916\n",
      "Step: 1798, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 1.8577897548675537\n",
      "Step: 1799, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 2.1305770874023438\n",
      "Step: 1800, Loss: 0.9376274347305298, Accuracy: 0.96875, Computation time: 2.6199100017547607\n",
      "Step: 1801, Loss: 0.9159888029098511, Accuracy: 1.0, Computation time: 1.7920207977294922\n",
      "Step: 1802, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.7651798725128174\n",
      "Step: 1803, Loss: 0.9376549124717712, Accuracy: 0.9821428656578064, Computation time: 1.72749662399292\n",
      "Step: 1804, Loss: 0.9169749021530151, Accuracy: 1.0, Computation time: 2.4871959686279297\n",
      "Step: 1805, Loss: 0.917491614818573, Accuracy: 1.0, Computation time: 1.8910880088806152\n",
      "Step: 1806, Loss: 0.9378308653831482, Accuracy: 0.9583333730697632, Computation time: 2.5109753608703613\n",
      "Step: 1807, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 2.033567428588867\n",
      "########################\n",
      "Test loss: 1.120189905166626, Test Accuracy_epoch13: 0.7206224203109741\n",
      "########################\n",
      "Step: 1808, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 2.2675771713256836\n",
      "Step: 1809, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 1.8266444206237793\n",
      "Step: 1810, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 1.9891531467437744\n",
      "Step: 1811, Loss: 0.9375758767127991, Accuracy: 0.9722222089767456, Computation time: 1.8997399806976318\n",
      "Step: 1812, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 2.0853805541992188\n",
      "Step: 1813, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 2.260326385498047\n",
      "Step: 1814, Loss: 0.9160809516906738, Accuracy: 1.0, Computation time: 2.0817880630493164\n",
      "Step: 1815, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 2.1037821769714355\n",
      "Step: 1816, Loss: 0.9186068773269653, Accuracy: 1.0, Computation time: 1.992621660232544\n",
      "Step: 1817, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.9449498653411865\n",
      "Step: 1818, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.9694428443908691\n",
      "Step: 1819, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.9409832954406738\n",
      "Step: 1820, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.931859016418457\n",
      "Step: 1821, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 1.8190577030181885\n",
      "Step: 1822, Loss: 0.9375475645065308, Accuracy: 0.9772727489471436, Computation time: 1.9157421588897705\n",
      "Step: 1823, Loss: 0.9298900961875916, Accuracy: 0.96875, Computation time: 3.0256454944610596\n",
      "Step: 1824, Loss: 0.9159613251686096, Accuracy: 1.0, Computation time: 2.056286334991455\n",
      "Step: 1825, Loss: 0.9159436225891113, Accuracy: 1.0, Computation time: 1.852508544921875\n",
      "Step: 1826, Loss: 0.9159302115440369, Accuracy: 1.0, Computation time: 1.9305243492126465\n",
      "Step: 1827, Loss: 0.9159877896308899, Accuracy: 1.0, Computation time: 1.9302763938903809\n",
      "Step: 1828, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 2.026555061340332\n",
      "Step: 1829, Loss: 0.9187712669372559, Accuracy: 1.0, Computation time: 2.1583404541015625\n",
      "Step: 1830, Loss: 0.9373190402984619, Accuracy: 0.984375, Computation time: 2.3443243503570557\n",
      "Step: 1831, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 1.8895986080169678\n",
      "Step: 1832, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 1.7555184364318848\n",
      "Step: 1833, Loss: 0.9161772131919861, Accuracy: 1.0, Computation time: 2.0132040977478027\n",
      "Step: 1834, Loss: 0.9159774780273438, Accuracy: 1.0, Computation time: 2.227430582046509\n",
      "Step: 1835, Loss: 0.9161931276321411, Accuracy: 1.0, Computation time: 1.9968760013580322\n",
      "Step: 1836, Loss: 0.9166412353515625, Accuracy: 1.0, Computation time: 2.1225733757019043\n",
      "Step: 1837, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 2.300814628601074\n",
      "Step: 1838, Loss: 0.9161996245384216, Accuracy: 1.0, Computation time: 2.382706642150879\n",
      "Step: 1839, Loss: 0.937326967716217, Accuracy: 0.9807692766189575, Computation time: 2.0085947513580322\n",
      "Step: 1840, Loss: 0.9383833408355713, Accuracy: 0.9807692766189575, Computation time: 1.9210548400878906\n",
      "Step: 1841, Loss: 0.9158987402915955, Accuracy: 1.0, Computation time: 1.7520694732666016\n",
      "Step: 1842, Loss: 0.9159258008003235, Accuracy: 1.0, Computation time: 1.7620363235473633\n",
      "Step: 1843, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 1.786238193511963\n",
      "Step: 1844, Loss: 0.9369667768478394, Accuracy: 0.9583333730697632, Computation time: 1.9968435764312744\n",
      "Step: 1845, Loss: 0.9163273572921753, Accuracy: 1.0, Computation time: 1.9134929180145264\n",
      "Step: 1846, Loss: 0.9373432993888855, Accuracy: 0.9642857313156128, Computation time: 1.9576869010925293\n",
      "Step: 1847, Loss: 0.9185139536857605, Accuracy: 1.0, Computation time: 2.1889991760253906\n",
      "Step: 1848, Loss: 0.9161540865898132, Accuracy: 1.0, Computation time: 2.1624057292938232\n",
      "Step: 1849, Loss: 0.9176545739173889, Accuracy: 1.0, Computation time: 2.180297374725342\n",
      "Step: 1850, Loss: 0.9161748290061951, Accuracy: 1.0, Computation time: 1.986879825592041\n",
      "Step: 1851, Loss: 0.9159759283065796, Accuracy: 1.0, Computation time: 1.9335088729858398\n",
      "Step: 1852, Loss: 0.9159566164016724, Accuracy: 1.0, Computation time: 1.8729283809661865\n",
      "Step: 1853, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 1.966310739517212\n",
      "Step: 1854, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.9015629291534424\n",
      "Step: 1855, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 2.4723575115203857\n",
      "Step: 1856, Loss: 0.9159804582595825, Accuracy: 1.0, Computation time: 1.7972171306610107\n",
      "Step: 1857, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.8266360759735107\n",
      "Step: 1858, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.694488763809204\n",
      "Step: 1859, Loss: 0.9172879457473755, Accuracy: 1.0, Computation time: 1.9540579319000244\n",
      "Step: 1860, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.798354148864746\n",
      "Step: 1861, Loss: 0.91599041223526, Accuracy: 1.0, Computation time: 1.8355975151062012\n",
      "Step: 1862, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.9412720203399658\n",
      "Step: 1863, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 2.1794254779815674\n",
      "Step: 1864, Loss: 0.9163110256195068, Accuracy: 1.0, Computation time: 1.6159858703613281\n",
      "Step: 1865, Loss: 0.9357149600982666, Accuracy: 0.9583333730697632, Computation time: 1.9427170753479004\n",
      "Step: 1866, Loss: 0.915952205657959, Accuracy: 1.0, Computation time: 1.8901352882385254\n",
      "Step: 1867, Loss: 0.915957510471344, Accuracy: 1.0, Computation time: 2.200778007507324\n",
      "Step: 1868, Loss: 0.9197023510932922, Accuracy: 1.0, Computation time: 2.063361406326294\n",
      "Step: 1869, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.8670694828033447\n",
      "Step: 1870, Loss: 0.9214792251586914, Accuracy: 1.0, Computation time: 1.7499158382415771\n",
      "Step: 1871, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 2.182781219482422\n",
      "Step: 1872, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 1.8448176383972168\n",
      "Step: 1873, Loss: 0.9366282820701599, Accuracy: 0.9772727489471436, Computation time: 1.7602589130401611\n",
      "Step: 1874, Loss: 0.9377549886703491, Accuracy: 0.9772727489471436, Computation time: 1.493682861328125\n",
      "Step: 1875, Loss: 0.9160556793212891, Accuracy: 1.0, Computation time: 1.9148988723754883\n",
      "Step: 1876, Loss: 0.9210253953933716, Accuracy: 1.0, Computation time: 2.4790871143341064\n",
      "Step: 1877, Loss: 0.9159581065177917, Accuracy: 1.0, Computation time: 1.8050146102905273\n",
      "Step: 1878, Loss: 0.9160435795783997, Accuracy: 1.0, Computation time: 2.0102243423461914\n",
      "Step: 1879, Loss: 0.9160131216049194, Accuracy: 1.0, Computation time: 1.7265534400939941\n",
      "Step: 1880, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 1.822437047958374\n",
      "Step: 1881, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.73189115524292\n",
      "Step: 1882, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 2.057528257369995\n",
      "Step: 1883, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.6690785884857178\n",
      "Step: 1884, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 1.7280967235565186\n",
      "Step: 1885, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 1.593534231185913\n",
      "Step: 1886, Loss: 0.9169732332229614, Accuracy: 1.0, Computation time: 1.9910759925842285\n",
      "Step: 1887, Loss: 0.9327723979949951, Accuracy: 0.9642857313156128, Computation time: 1.9607112407684326\n",
      "Step: 1888, Loss: 0.9223556518554688, Accuracy: 1.0, Computation time: 1.9894168376922607\n",
      "Step: 1889, Loss: 0.9160333275794983, Accuracy: 1.0, Computation time: 2.0924394130706787\n",
      "Step: 1890, Loss: 0.9160676002502441, Accuracy: 1.0, Computation time: 1.9962735176086426\n",
      "Step: 1891, Loss: 0.9160397052764893, Accuracy: 1.0, Computation time: 2.032378673553467\n",
      "Step: 1892, Loss: 0.9208539128303528, Accuracy: 1.0, Computation time: 2.131723403930664\n",
      "Step: 1893, Loss: 0.9161359667778015, Accuracy: 1.0, Computation time: 1.878844976425171\n",
      "Step: 1894, Loss: 0.9165257811546326, Accuracy: 1.0, Computation time: 1.9125573635101318\n",
      "Step: 1895, Loss: 0.9294171333312988, Accuracy: 0.9583333730697632, Computation time: 2.029174566268921\n",
      "Step: 1896, Loss: 0.9160301685333252, Accuracy: 1.0, Computation time: 2.1442418098449707\n",
      "Step: 1897, Loss: 0.9177513718605042, Accuracy: 1.0, Computation time: 2.1779894828796387\n",
      "Step: 1898, Loss: 0.9160221219062805, Accuracy: 1.0, Computation time: 2.135507106781006\n",
      "Step: 1899, Loss: 0.9161801934242249, Accuracy: 1.0, Computation time: 1.894801139831543\n",
      "Step: 1900, Loss: 0.9380171895027161, Accuracy: 0.9833333492279053, Computation time: 2.212143659591675\n",
      "Step: 1901, Loss: 0.9164113402366638, Accuracy: 1.0, Computation time: 2.179853677749634\n",
      "Step: 1902, Loss: 0.9162004590034485, Accuracy: 1.0, Computation time: 2.1469149589538574\n",
      "Step: 1903, Loss: 0.9162985682487488, Accuracy: 1.0, Computation time: 1.83591890335083\n",
      "Step: 1904, Loss: 0.9161242246627808, Accuracy: 1.0, Computation time: 2.284151792526245\n",
      "Step: 1905, Loss: 0.9159677624702454, Accuracy: 1.0, Computation time: 2.0792150497436523\n",
      "Step: 1906, Loss: 0.9159107804298401, Accuracy: 1.0, Computation time: 2.2272562980651855\n",
      "Step: 1907, Loss: 0.9349809288978577, Accuracy: 0.96875, Computation time: 2.2607574462890625\n",
      "Step: 1908, Loss: 0.9159280061721802, Accuracy: 1.0, Computation time: 2.343451499938965\n",
      "Step: 1909, Loss: 0.9159876108169556, Accuracy: 1.0, Computation time: 2.2747962474823\n",
      "Step: 1910, Loss: 0.9159927368164062, Accuracy: 1.0, Computation time: 2.382081985473633\n",
      "Step: 1911, Loss: 0.9164813160896301, Accuracy: 1.0, Computation time: 2.5453968048095703\n",
      "Step: 1912, Loss: 0.9161335825920105, Accuracy: 1.0, Computation time: 2.051816940307617\n",
      "Step: 1913, Loss: 0.9160479307174683, Accuracy: 1.0, Computation time: 2.23401141166687\n",
      "Step: 1914, Loss: 0.9161698222160339, Accuracy: 1.0, Computation time: 2.363985061645508\n",
      "Step: 1915, Loss: 0.9160265326499939, Accuracy: 1.0, Computation time: 2.191549777984619\n",
      "Step: 1916, Loss: 0.9160671830177307, Accuracy: 1.0, Computation time: 2.1177408695220947\n",
      "Step: 1917, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 2.3444740772247314\n",
      "Step: 1918, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 2.2443795204162598\n",
      "Step: 1919, Loss: 0.9159655570983887, Accuracy: 1.0, Computation time: 2.2403390407562256\n",
      "Step: 1920, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 2.429542064666748\n",
      "Step: 1921, Loss: 0.9159704446792603, Accuracy: 1.0, Computation time: 2.180941104888916\n",
      "Step: 1922, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 2.2213408946990967\n",
      "Step: 1923, Loss: 0.9375907182693481, Accuracy: 0.9791666865348816, Computation time: 2.0169527530670166\n",
      "Step: 1924, Loss: 0.9159448742866516, Accuracy: 1.0, Computation time: 2.2789251804351807\n",
      "Step: 1925, Loss: 0.9160062670707703, Accuracy: 1.0, Computation time: 2.41892671585083\n",
      "Step: 1926, Loss: 0.9372956156730652, Accuracy: 0.9750000238418579, Computation time: 2.4410438537597656\n",
      "Step: 1927, Loss: 0.915911078453064, Accuracy: 1.0, Computation time: 2.341157913208008\n",
      "Step: 1928, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 2.102613687515259\n",
      "Step: 1929, Loss: 0.9163249135017395, Accuracy: 1.0, Computation time: 2.142712116241455\n",
      "Step: 1930, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 2.302551746368408\n",
      "Step: 1931, Loss: 0.9372699856758118, Accuracy: 0.9166666865348816, Computation time: 2.37170672416687\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4f4a6-a2fd-48dc-8f95-96fd0da4ce14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
