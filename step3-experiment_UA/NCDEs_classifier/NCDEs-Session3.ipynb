{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc5991ff-c342-4e44-a05c-29cca1e333d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285f8084-4bff-4633-a09a-0a7d2e4f0769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session3_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ba399b-c715-4c15-8371-db993d3c194d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4380, 256, 768) (4380,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last2, wav2vec_last4, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last2,label_last4,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3265ce-cf55-499e-a76d-6fb03ae0c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad32087-4e4a-41ed-810a-02a3b5fbeef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f224b9-3b7d-49d0-9a90-623b73303003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            #zn = jnp.concatenate((coeffs_i, yn_array), axis=1)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0) \n",
    "            pred_var=prediction.var(axis=0)   \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)    \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085c4d10-76bc-45b5-9afd-19f26d786be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c124d863-9940-401f-b5f0-3d1cd6dd5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last3], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last3\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f03b52-1d57-4b6a-ac5c-6523f18ece05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ec4e880-a109-42f7-afd0-cef238d3d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd8d1b39-5c46-46fc-a878-9da6444fa8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4380,\n",
    "    dataset_test_size=1151,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=6456,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    import jax.numpy as jnp\n",
    "    from jax import jit\n",
    "    def calculate_confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "        true_labels = true_labels.astype(jnp.int32)\n",
    "        pred_labels = pred_labels.astype(jnp.int32)\n",
    "        conf_matrix = jnp.zeros((num_classes, num_classes), dtype=jnp.int32)\n",
    "        for t, p in zip(true_labels, pred_labels):\n",
    "            conf_matrix = conf_matrix.at[t, p].add(1)\n",
    "        return conf_matrix\n",
    "\n",
    "    @jit\n",
    "    def calculate_ua(conf_matrix):\n",
    "        class_accuracy = jnp.diag(conf_matrix) / jnp.sum(conf_matrix, axis=1)\n",
    "        UA = jnp.mean(class_accuracy)\n",
    "        return UA\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        conf_matrix = calculate_confusion_matrix(label_i, pred, num_classes=4)\n",
    "        UA = calculate_ua(conf_matrix)\n",
    "        return UA\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7603d6f6-5ba1-4461-8aed-633eed202332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.4650264978408813, Accuracy: 0.25, Computation time: 17.33047127723694\n",
      "Step: 1, Loss: 1.4444735050201416, Accuracy: 0.25, Computation time: 3.6154239177703857\n",
      "Step: 2, Loss: 1.3534343242645264, Accuracy: 0.25, Computation time: 2.5828425884246826\n",
      "Step: 3, Loss: 1.4085370302200317, Accuracy: 0.25, Computation time: 3.3259189128875732\n",
      "Step: 4, Loss: 1.3720816373825073, Accuracy: 0.25, Computation time: 3.6622674465179443\n",
      "Step: 5, Loss: 1.4347901344299316, Accuracy: 0.25, Computation time: 3.782003164291382\n",
      "Step: 6, Loss: 1.3885008096694946, Accuracy: 0.25, Computation time: 3.4703662395477295\n",
      "Step: 7, Loss: 1.3494653701782227, Accuracy: 0.25, Computation time: 3.2124505043029785\n",
      "Step: 8, Loss: 1.399582028388977, Accuracy: 0.25, Computation time: 2.434253215789795\n",
      "Step: 9, Loss: 1.3214848041534424, Accuracy: 0.25, Computation time: 3.507326364517212\n",
      "Step: 10, Loss: 1.2648165225982666, Accuracy: 0.4583333432674408, Computation time: 3.0512092113494873\n",
      "Step: 11, Loss: 1.2603825330734253, Accuracy: 0.75, Computation time: 2.798962116241455\n",
      "Step: 12, Loss: 1.283712387084961, Accuracy: 0.6499999761581421, Computation time: 2.981344699859619\n",
      "Step: 13, Loss: 1.1327128410339355, Accuracy: 0.75, Computation time: 3.1878535747528076\n",
      "Step: 14, Loss: 1.124901533126831, Accuracy: 0.75, Computation time: 2.6588616371154785\n",
      "Step: 15, Loss: 1.1888881921768188, Accuracy: 0.75, Computation time: 3.7901742458343506\n",
      "Step: 16, Loss: 1.2165570259094238, Accuracy: 0.75, Computation time: 2.748741865158081\n",
      "Step: 17, Loss: 1.178528904914856, Accuracy: 0.75, Computation time: 2.86869215965271\n",
      "Step: 18, Loss: 1.1446126699447632, Accuracy: 0.75, Computation time: 2.567993402481079\n",
      "Step: 19, Loss: 1.1342827081680298, Accuracy: 0.75, Computation time: 2.383422613143921\n",
      "Step: 20, Loss: 1.2843605279922485, Accuracy: 0.75, Computation time: 2.2509219646453857\n",
      "Step: 21, Loss: 1.1928380727767944, Accuracy: 0.75, Computation time: 2.300309419631958\n",
      "Step: 22, Loss: 1.039739966392517, Accuracy: 0.8973214626312256, Computation time: 2.3354334831237793\n",
      "Step: 23, Loss: 1.0694561004638672, Accuracy: 0.9285714626312256, Computation time: 2.231750011444092\n",
      "Step: 24, Loss: 1.0337163209915161, Accuracy: 0.9375, Computation time: 2.465303897857666\n",
      "Step: 25, Loss: 1.032461166381836, Accuracy: 0.9807692766189575, Computation time: 2.025564193725586\n",
      "Step: 26, Loss: 1.01956307888031, Accuracy: 0.9375, Computation time: 2.2433550357818604\n",
      "Step: 27, Loss: 1.0229570865631104, Accuracy: 0.9722222089767456, Computation time: 2.1647117137908936\n",
      "Step: 28, Loss: 0.9952089786529541, Accuracy: 0.9722222089767456, Computation time: 2.2348520755767822\n",
      "Step: 29, Loss: 0.9921262264251709, Accuracy: 1.0, Computation time: 2.1741127967834473\n",
      "Step: 30, Loss: 0.9681237936019897, Accuracy: 1.0, Computation time: 2.7952866554260254\n",
      "Step: 31, Loss: 0.9692116379737854, Accuracy: 0.9375, Computation time: 1.9496443271636963\n",
      "Step: 32, Loss: 0.9663519263267517, Accuracy: 1.0, Computation time: 2.423085927963257\n",
      "Step: 33, Loss: 0.9583398699760437, Accuracy: 1.0, Computation time: 2.171750068664551\n",
      "Step: 34, Loss: 0.9606963396072388, Accuracy: 0.9583333730697632, Computation time: 2.303232192993164\n",
      "Step: 35, Loss: 0.9519664645195007, Accuracy: 1.0, Computation time: 2.657085418701172\n",
      "Step: 36, Loss: 0.9276717901229858, Accuracy: 1.0, Computation time: 2.6542153358459473\n",
      "Step: 37, Loss: 0.950146496295929, Accuracy: 0.9791666865348816, Computation time: 2.0574071407318115\n",
      "Step: 38, Loss: 0.9708510637283325, Accuracy: 0.9791666865348816, Computation time: 2.550551176071167\n",
      "Step: 39, Loss: 0.93690025806427, Accuracy: 0.9583333730697632, Computation time: 1.9579782485961914\n",
      "Step: 40, Loss: 0.9517062902450562, Accuracy: 0.9807692766189575, Computation time: 2.6318914890289307\n",
      "Step: 41, Loss: 0.940852165222168, Accuracy: 0.96875, Computation time: 2.276183843612671\n",
      "Step: 42, Loss: 0.9393420815467834, Accuracy: 0.9642857313156128, Computation time: 1.860377311706543\n",
      "Step: 43, Loss: 0.9548004269599915, Accuracy: 0.9791666865348816, Computation time: 2.5522100925445557\n",
      "Step: 44, Loss: 0.9377526640892029, Accuracy: 1.0, Computation time: 2.084780693054199\n",
      "Step: 45, Loss: 0.9234865307807922, Accuracy: 1.0, Computation time: 1.7961444854736328\n",
      "Step: 46, Loss: 0.934005618095398, Accuracy: 1.0, Computation time: 2.1321730613708496\n",
      "Step: 47, Loss: 0.9858080744743347, Accuracy: 0.9305555820465088, Computation time: 1.6536898612976074\n",
      "Step: 48, Loss: 0.9262507557868958, Accuracy: 1.0, Computation time: 1.7169020175933838\n",
      "Step: 49, Loss: 0.9256375432014465, Accuracy: 1.0, Computation time: 1.88722825050354\n",
      "Step: 50, Loss: 0.9230958819389343, Accuracy: 1.0, Computation time: 1.7242183685302734\n",
      "Step: 51, Loss: 0.9284733533859253, Accuracy: 1.0, Computation time: 1.942216157913208\n",
      "Step: 52, Loss: 0.9344353675842285, Accuracy: 0.9791666865348816, Computation time: 2.1054060459136963\n",
      "Step: 53, Loss: 0.9224486351013184, Accuracy: 1.0, Computation time: 1.5395514965057373\n",
      "Step: 54, Loss: 0.9464914202690125, Accuracy: 0.9642857313156128, Computation time: 1.5940613746643066\n",
      "Step: 55, Loss: 0.9231790900230408, Accuracy: 1.0, Computation time: 2.1418049335479736\n",
      "Step: 56, Loss: 0.9217713475227356, Accuracy: 1.0, Computation time: 1.6608004570007324\n",
      "Step: 57, Loss: 0.9222496151924133, Accuracy: 1.0, Computation time: 2.1432700157165527\n",
      "Step: 58, Loss: 0.9238213896751404, Accuracy: 1.0, Computation time: 1.781555414199829\n",
      "Step: 59, Loss: 0.9203173518180847, Accuracy: 1.0, Computation time: 1.9945828914642334\n",
      "Step: 60, Loss: 0.9336869120597839, Accuracy: 0.9772727489471436, Computation time: 1.545037031173706\n",
      "Step: 61, Loss: 0.9210363030433655, Accuracy: 1.0, Computation time: 1.726020336151123\n",
      "Step: 62, Loss: 0.9253246784210205, Accuracy: 1.0, Computation time: 1.5686144828796387\n",
      "Step: 63, Loss: 0.9231930375099182, Accuracy: 1.0, Computation time: 2.038944721221924\n",
      "Step: 64, Loss: 0.9383116364479065, Accuracy: 0.9750000238418579, Computation time: 1.6042685508728027\n",
      "Step: 65, Loss: 0.945936381816864, Accuracy: 0.96875, Computation time: 1.666179895401001\n",
      "Step: 66, Loss: 0.9239232540130615, Accuracy: 1.0, Computation time: 1.6850106716156006\n",
      "Step: 67, Loss: 0.9339718818664551, Accuracy: 0.9722222089767456, Computation time: 2.0900747776031494\n",
      "Step: 68, Loss: 0.9226900935173035, Accuracy: 1.0, Computation time: 1.548027753829956\n",
      "Step: 69, Loss: 0.9208523035049438, Accuracy: 1.0, Computation time: 1.6348960399627686\n",
      "Step: 70, Loss: 0.9201398491859436, Accuracy: 1.0, Computation time: 1.4903104305267334\n",
      "Step: 71, Loss: 0.9183492064476013, Accuracy: 1.0, Computation time: 1.6611382961273193\n",
      "Step: 72, Loss: 0.921140193939209, Accuracy: 1.0, Computation time: 1.8214666843414307\n",
      "Step: 73, Loss: 0.929936408996582, Accuracy: 1.0, Computation time: 1.6236741542816162\n",
      "Step: 74, Loss: 0.9199911952018738, Accuracy: 1.0, Computation time: 1.5740468502044678\n",
      "Step: 75, Loss: 0.9348024725914001, Accuracy: 1.0, Computation time: 1.601224422454834\n",
      "Step: 76, Loss: 0.9338212013244629, Accuracy: 0.9791666865348816, Computation time: 1.3752219676971436\n",
      "Step: 77, Loss: 0.9186062812805176, Accuracy: 1.0, Computation time: 1.6126644611358643\n",
      "Step: 78, Loss: 0.9452622532844543, Accuracy: 0.949999988079071, Computation time: 1.6912496089935303\n",
      "Step: 79, Loss: 0.9431806206703186, Accuracy: 0.9750000238418579, Computation time: 1.5079586505889893\n",
      "Step: 80, Loss: 0.9178330898284912, Accuracy: 1.0, Computation time: 1.7868406772613525\n",
      "Step: 81, Loss: 0.923428475856781, Accuracy: 1.0, Computation time: 1.8634014129638672\n",
      "Step: 82, Loss: 0.9194775223731995, Accuracy: 1.0, Computation time: 2.059757947921753\n",
      "Step: 83, Loss: 0.9342575073242188, Accuracy: 0.9750000238418579, Computation time: 1.6407923698425293\n",
      "Step: 84, Loss: 0.9190300703048706, Accuracy: 1.0, Computation time: 2.2288076877593994\n",
      "Step: 85, Loss: 0.935408890247345, Accuracy: 0.9642857313156128, Computation time: 1.442136287689209\n",
      "Step: 86, Loss: 0.9257267713546753, Accuracy: 1.0, Computation time: 1.8077504634857178\n",
      "Step: 87, Loss: 0.9222462773323059, Accuracy: 1.0, Computation time: 1.4231719970703125\n",
      "Step: 88, Loss: 0.9242095947265625, Accuracy: 1.0, Computation time: 1.677309513092041\n",
      "Step: 89, Loss: 0.9445414543151855, Accuracy: 0.9807692766189575, Computation time: 1.7520532608032227\n",
      "Step: 90, Loss: 0.9183762073516846, Accuracy: 1.0, Computation time: 1.8566005229949951\n",
      "Step: 91, Loss: 0.9219694137573242, Accuracy: 1.0, Computation time: 1.7637245655059814\n",
      "Step: 92, Loss: 0.9177320003509521, Accuracy: 1.0, Computation time: 1.9172606468200684\n",
      "Step: 93, Loss: 0.9459611177444458, Accuracy: 0.9807692766189575, Computation time: 1.4636144638061523\n",
      "Step: 94, Loss: 0.9187465906143188, Accuracy: 1.0, Computation time: 2.172008991241455\n",
      "Step: 95, Loss: 0.9196746349334717, Accuracy: 1.0, Computation time: 1.5676252841949463\n",
      "Step: 96, Loss: 0.9188532829284668, Accuracy: 1.0, Computation time: 1.8353374004364014\n",
      "Step: 97, Loss: 0.9389756917953491, Accuracy: 1.0, Computation time: 1.769768238067627\n",
      "Step: 98, Loss: 0.9168903827667236, Accuracy: 1.0, Computation time: 1.835920810699463\n",
      "Step: 99, Loss: 0.9222631454467773, Accuracy: 1.0, Computation time: 1.9950029850006104\n",
      "Step: 100, Loss: 0.9185913801193237, Accuracy: 1.0, Computation time: 1.7413299083709717\n",
      "Step: 101, Loss: 0.9178069829940796, Accuracy: 1.0, Computation time: 1.5505836009979248\n",
      "Step: 102, Loss: 0.9347613453865051, Accuracy: 0.96875, Computation time: 1.658097505569458\n",
      "Step: 103, Loss: 0.9190301895141602, Accuracy: 1.0, Computation time: 1.8539478778839111\n",
      "Step: 104, Loss: 0.9420425891876221, Accuracy: 0.9833333492279053, Computation time: 1.7477350234985352\n",
      "Step: 105, Loss: 0.9194974303245544, Accuracy: 1.0, Computation time: 1.8197057247161865\n",
      "Step: 106, Loss: 0.9203428626060486, Accuracy: 1.0, Computation time: 1.747950553894043\n",
      "Step: 107, Loss: 0.922851026058197, Accuracy: 1.0, Computation time: 1.8596148490905762\n",
      "Step: 108, Loss: 0.9170453548431396, Accuracy: 1.0, Computation time: 2.2774429321289062\n",
      "Step: 109, Loss: 0.9378041625022888, Accuracy: 0.9791666865348816, Computation time: 1.8672966957092285\n",
      "Step: 110, Loss: 0.9276725649833679, Accuracy: 0.9772727489471436, Computation time: 1.9787590503692627\n",
      "Step: 111, Loss: 0.9170841574668884, Accuracy: 1.0, Computation time: 1.6384787559509277\n",
      "Step: 112, Loss: 0.9349251389503479, Accuracy: 0.9772727489471436, Computation time: 1.682619333267212\n",
      "Step: 113, Loss: 0.9176765084266663, Accuracy: 1.0, Computation time: 1.7123663425445557\n",
      "Step: 114, Loss: 0.9442988634109497, Accuracy: 0.9722222089767456, Computation time: 1.650000810623169\n",
      "Step: 115, Loss: 0.9349780082702637, Accuracy: 0.9642857313156128, Computation time: 1.8674981594085693\n",
      "Step: 116, Loss: 0.9522490501403809, Accuracy: 0.9772727489471436, Computation time: 1.8381524085998535\n",
      "Step: 117, Loss: 0.9261485934257507, Accuracy: 1.0, Computation time: 1.7463791370391846\n",
      "Step: 118, Loss: 0.919620156288147, Accuracy: 1.0, Computation time: 1.7477457523345947\n",
      "Step: 119, Loss: 0.9477319121360779, Accuracy: 0.9772727489471436, Computation time: 1.817023515701294\n",
      "Step: 120, Loss: 0.9277085661888123, Accuracy: 1.0, Computation time: 1.5416722297668457\n",
      "Step: 121, Loss: 0.9199527502059937, Accuracy: 1.0, Computation time: 1.6972384452819824\n",
      "Step: 122, Loss: 0.9173892736434937, Accuracy: 1.0, Computation time: 1.7153325080871582\n",
      "Step: 123, Loss: 1.0008047819137573, Accuracy: 0.908730149269104, Computation time: 1.8836750984191895\n",
      "Step: 124, Loss: 0.9279508590698242, Accuracy: 0.9750000238418579, Computation time: 1.742100477218628\n",
      "Step: 125, Loss: 0.9279679656028748, Accuracy: 1.0, Computation time: 1.8327789306640625\n",
      "Step: 126, Loss: 0.9203975796699524, Accuracy: 1.0, Computation time: 1.6548330783843994\n",
      "Step: 127, Loss: 0.9197142124176025, Accuracy: 1.0, Computation time: 1.8482708930969238\n",
      "Step: 128, Loss: 0.9423052668571472, Accuracy: 0.9807692766189575, Computation time: 1.8420164585113525\n",
      "Step: 129, Loss: 0.9194893836975098, Accuracy: 1.0, Computation time: 1.8581254482269287\n",
      "Step: 130, Loss: 0.9189023375511169, Accuracy: 1.0, Computation time: 2.0344247817993164\n",
      "Step: 131, Loss: 0.9380821585655212, Accuracy: 0.9807692766189575, Computation time: 2.033417224884033\n",
      "Step: 132, Loss: 0.9207885265350342, Accuracy: 1.0, Computation time: 1.9330687522888184\n",
      "Step: 133, Loss: 0.9204838275909424, Accuracy: 1.0, Computation time: 2.8491058349609375\n",
      "Step: 134, Loss: 0.9181435704231262, Accuracy: 1.0, Computation time: 1.8063156604766846\n",
      "Step: 135, Loss: 0.9186281561851501, Accuracy: 1.0, Computation time: 2.0467491149902344\n",
      "Step: 136, Loss: 0.9182106852531433, Accuracy: 1.0, Computation time: 2.518183708190918\n",
      "Step: 137, Loss: 0.9193639755249023, Accuracy: 1.0, Computation time: 1.8605427742004395\n",
      "Step: 138, Loss: 0.9181565642356873, Accuracy: 1.0, Computation time: 1.5575757026672363\n",
      "Step: 139, Loss: 0.9404995441436768, Accuracy: 0.9807692766189575, Computation time: 2.4357717037200928\n",
      "########################\n",
      "Test loss: 1.1152405738830566, Test Accuracy_epoch1: 0.7087719440460205\n",
      "########################\n",
      "Step: 140, Loss: 0.9201771020889282, Accuracy: 1.0, Computation time: 2.100771903991699\n",
      "Step: 141, Loss: 0.9178824424743652, Accuracy: 1.0, Computation time: 2.1592981815338135\n",
      "Step: 142, Loss: 0.916577160358429, Accuracy: 1.0, Computation time: 1.883378267288208\n",
      "Step: 143, Loss: 0.9252304434776306, Accuracy: 1.0, Computation time: 1.8515064716339111\n",
      "Step: 144, Loss: 0.9169383645057678, Accuracy: 1.0, Computation time: 1.8272175788879395\n",
      "Step: 145, Loss: 0.9235467910766602, Accuracy: 1.0, Computation time: 1.8543388843536377\n",
      "Step: 146, Loss: 0.9281951189041138, Accuracy: 0.9583333730697632, Computation time: 2.243593692779541\n",
      "Step: 147, Loss: 0.975372314453125, Accuracy: 0.9166666865348816, Computation time: 2.219088315963745\n",
      "Step: 148, Loss: 0.9261965155601501, Accuracy: 1.0, Computation time: 1.6699440479278564\n",
      "Step: 149, Loss: 0.9240412712097168, Accuracy: 1.0, Computation time: 1.8060851097106934\n",
      "Step: 150, Loss: 0.9208564162254333, Accuracy: 1.0, Computation time: 2.138272285461426\n",
      "Step: 151, Loss: 0.9324650168418884, Accuracy: 0.96875, Computation time: 2.0417752265930176\n",
      "Step: 152, Loss: 0.919926643371582, Accuracy: 1.0, Computation time: 2.136606454849243\n",
      "Step: 153, Loss: 0.9216981530189514, Accuracy: 1.0, Computation time: 2.0218100547790527\n",
      "Step: 154, Loss: 0.917192280292511, Accuracy: 1.0, Computation time: 1.9791574478149414\n",
      "Step: 155, Loss: 0.9172272682189941, Accuracy: 1.0, Computation time: 2.226395845413208\n",
      "Step: 156, Loss: 0.9403014183044434, Accuracy: 0.9772727489471436, Computation time: 1.7067909240722656\n",
      "Step: 157, Loss: 0.9179638028144836, Accuracy: 1.0, Computation time: 2.0133392810821533\n",
      "Step: 158, Loss: 0.9225298762321472, Accuracy: 1.0, Computation time: 1.7457554340362549\n",
      "Step: 159, Loss: 0.9539266228675842, Accuracy: 0.949999988079071, Computation time: 1.9007949829101562\n",
      "Step: 160, Loss: 0.9364722371101379, Accuracy: 0.96875, Computation time: 2.2481324672698975\n",
      "Step: 161, Loss: 0.938768744468689, Accuracy: 0.96875, Computation time: 1.9330503940582275\n",
      "Step: 162, Loss: 0.9422899484634399, Accuracy: 0.9375, Computation time: 2.1326582431793213\n",
      "Step: 163, Loss: 0.9242388010025024, Accuracy: 1.0, Computation time: 2.0268714427948\n",
      "Step: 164, Loss: 0.9221853613853455, Accuracy: 1.0, Computation time: 1.7941477298736572\n",
      "Step: 165, Loss: 0.9174342751502991, Accuracy: 1.0, Computation time: 2.1224515438079834\n",
      "Step: 166, Loss: 0.9196734428405762, Accuracy: 1.0, Computation time: 1.8333587646484375\n",
      "Step: 167, Loss: 0.9286354780197144, Accuracy: 0.9791666865348816, Computation time: 2.174018144607544\n",
      "Step: 168, Loss: 0.9311953783035278, Accuracy: 0.9833333492279053, Computation time: 2.1696503162384033\n",
      "Step: 169, Loss: 0.9223169088363647, Accuracy: 1.0, Computation time: 2.0120136737823486\n",
      "Step: 170, Loss: 0.9173282980918884, Accuracy: 1.0, Computation time: 1.7872819900512695\n",
      "Step: 171, Loss: 0.9168486595153809, Accuracy: 1.0, Computation time: 2.228945016860962\n",
      "Step: 172, Loss: 0.9200349450111389, Accuracy: 1.0, Computation time: 1.8148517608642578\n",
      "Step: 173, Loss: 0.9390038251876831, Accuracy: 0.9772727489471436, Computation time: 2.1804392337799072\n",
      "Step: 174, Loss: 0.9368253350257874, Accuracy: 0.949999988079071, Computation time: 1.7771174907684326\n",
      "Step: 175, Loss: 0.9174463748931885, Accuracy: 1.0, Computation time: 1.6804020404815674\n",
      "Step: 176, Loss: 0.9184707999229431, Accuracy: 1.0, Computation time: 1.8475868701934814\n",
      "Step: 177, Loss: 0.916832447052002, Accuracy: 1.0, Computation time: 2.2527682781219482\n",
      "Step: 178, Loss: 0.9205597043037415, Accuracy: 1.0, Computation time: 1.8473851680755615\n",
      "Step: 179, Loss: 0.9223036766052246, Accuracy: 1.0, Computation time: 2.1168882846832275\n",
      "Step: 180, Loss: 0.9178528785705566, Accuracy: 1.0, Computation time: 1.794623851776123\n",
      "Step: 181, Loss: 0.9381902813911438, Accuracy: 0.9722222089767456, Computation time: 1.8740806579589844\n",
      "Step: 182, Loss: 0.9248610138893127, Accuracy: 1.0, Computation time: 1.9479234218597412\n",
      "Step: 183, Loss: 0.9247192740440369, Accuracy: 1.0, Computation time: 1.8096261024475098\n",
      "Step: 184, Loss: 0.9176213145256042, Accuracy: 1.0, Computation time: 2.1154747009277344\n",
      "Step: 185, Loss: 0.9191077351570129, Accuracy: 1.0, Computation time: 2.3513710498809814\n",
      "Step: 186, Loss: 0.9466568231582642, Accuracy: 0.9434524178504944, Computation time: 1.977513313293457\n",
      "Step: 187, Loss: 0.9332579970359802, Accuracy: 0.96875, Computation time: 2.0018715858459473\n",
      "Step: 188, Loss: 0.9191463589668274, Accuracy: 1.0, Computation time: 1.9381654262542725\n",
      "Step: 189, Loss: 0.9455094337463379, Accuracy: 0.922619104385376, Computation time: 1.8756084442138672\n",
      "Step: 190, Loss: 0.9338037967681885, Accuracy: 0.9722222089767456, Computation time: 1.9411358833312988\n",
      "Step: 191, Loss: 0.9193335771560669, Accuracy: 1.0, Computation time: 2.011460065841675\n",
      "Step: 192, Loss: 0.9171900153160095, Accuracy: 1.0, Computation time: 1.7348246574401855\n",
      "Step: 193, Loss: 0.9280526041984558, Accuracy: 0.9722222089767456, Computation time: 1.7403504848480225\n",
      "Step: 194, Loss: 0.9181318283081055, Accuracy: 1.0, Computation time: 1.9747579097747803\n",
      "Step: 195, Loss: 0.9210850596427917, Accuracy: 1.0, Computation time: 1.987633466720581\n",
      "Step: 196, Loss: 0.9405365586280823, Accuracy: 0.9750000238418579, Computation time: 1.9391517639160156\n",
      "Step: 197, Loss: 0.9164955019950867, Accuracy: 1.0, Computation time: 1.9962475299835205\n",
      "Step: 198, Loss: 0.9180151224136353, Accuracy: 1.0, Computation time: 2.125657558441162\n",
      "Step: 199, Loss: 0.9174512624740601, Accuracy: 1.0, Computation time: 2.9220662117004395\n",
      "Step: 200, Loss: 0.9385483264923096, Accuracy: 0.9750000238418579, Computation time: 2.7337634563446045\n",
      "Step: 201, Loss: 0.9192240834236145, Accuracy: 1.0, Computation time: 2.863136053085327\n",
      "Step: 202, Loss: 0.9170105457305908, Accuracy: 1.0, Computation time: 2.9779951572418213\n",
      "Step: 203, Loss: 0.9170229434967041, Accuracy: 1.0, Computation time: 2.604382276535034\n",
      "Step: 204, Loss: 0.9193304181098938, Accuracy: 1.0, Computation time: 2.944708824157715\n",
      "Step: 205, Loss: 0.9166399240493774, Accuracy: 1.0, Computation time: 2.836361885070801\n",
      "Step: 206, Loss: 0.9165164232254028, Accuracy: 1.0, Computation time: 2.6280128955841064\n",
      "Step: 207, Loss: 0.9194045662879944, Accuracy: 1.0, Computation time: 2.8486592769622803\n",
      "Step: 208, Loss: 0.9200233817100525, Accuracy: 1.0, Computation time: 2.9310131072998047\n",
      "Step: 209, Loss: 0.9169566035270691, Accuracy: 1.0, Computation time: 2.7192656993865967\n",
      "Step: 210, Loss: 0.9163548946380615, Accuracy: 1.0, Computation time: 2.714144706726074\n",
      "Step: 211, Loss: 0.9195742607116699, Accuracy: 1.0, Computation time: 2.9956445693969727\n",
      "Step: 212, Loss: 0.9390910267829895, Accuracy: 0.9583333730697632, Computation time: 2.572979211807251\n",
      "Step: 213, Loss: 0.9199254512786865, Accuracy: 1.0, Computation time: 3.483186721801758\n",
      "Step: 214, Loss: 0.9201130867004395, Accuracy: 1.0, Computation time: 2.3928890228271484\n",
      "Step: 215, Loss: 0.9221247434616089, Accuracy: 1.0, Computation time: 2.6095516681671143\n",
      "Step: 216, Loss: 0.9509408473968506, Accuracy: 0.9166666865348816, Computation time: 2.800128221511841\n",
      "Step: 217, Loss: 0.9164858460426331, Accuracy: 1.0, Computation time: 2.73494815826416\n",
      "Step: 218, Loss: 0.9355694055557251, Accuracy: 0.96875, Computation time: 2.8979592323303223\n",
      "Step: 219, Loss: 0.9167701601982117, Accuracy: 1.0, Computation time: 2.493410110473633\n",
      "Step: 220, Loss: 0.9389316439628601, Accuracy: 0.9821428656578064, Computation time: 2.1109206676483154\n",
      "Step: 221, Loss: 0.9165378212928772, Accuracy: 1.0, Computation time: 1.880619764328003\n",
      "Step: 222, Loss: 0.9386501312255859, Accuracy: 0.9821428656578064, Computation time: 2.4811906814575195\n",
      "Step: 223, Loss: 0.9162362217903137, Accuracy: 1.0, Computation time: 1.9300200939178467\n",
      "Step: 224, Loss: 0.9166326522827148, Accuracy: 1.0, Computation time: 2.542485475540161\n",
      "Step: 225, Loss: 0.9165045619010925, Accuracy: 1.0, Computation time: 2.2661983966827393\n",
      "Step: 226, Loss: 0.9330734014511108, Accuracy: 0.9750000238418579, Computation time: 2.1966686248779297\n",
      "Step: 227, Loss: 0.9166763424873352, Accuracy: 1.0, Computation time: 2.4515788555145264\n",
      "Step: 228, Loss: 0.9232414364814758, Accuracy: 1.0, Computation time: 2.4738879203796387\n",
      "Step: 229, Loss: 0.9168470501899719, Accuracy: 1.0, Computation time: 1.8485124111175537\n",
      "Step: 230, Loss: 0.9162456393241882, Accuracy: 1.0, Computation time: 2.0456390380859375\n",
      "Step: 231, Loss: 0.9359859824180603, Accuracy: 0.9722222089767456, Computation time: 2.3505077362060547\n",
      "Step: 232, Loss: 0.9175370335578918, Accuracy: 1.0, Computation time: 2.050718069076538\n",
      "Step: 233, Loss: 0.9170116782188416, Accuracy: 1.0, Computation time: 2.229898691177368\n",
      "Step: 234, Loss: 0.9163410663604736, Accuracy: 1.0, Computation time: 2.148207902908325\n",
      "Step: 235, Loss: 0.9306837916374207, Accuracy: 0.9642857313156128, Computation time: 2.133270025253296\n",
      "Step: 236, Loss: 0.9683586955070496, Accuracy: 0.9159722328186035, Computation time: 2.0192465782165527\n",
      "Step: 237, Loss: 0.927873969078064, Accuracy: 0.96875, Computation time: 2.0703866481781006\n",
      "Step: 238, Loss: 0.9175061583518982, Accuracy: 1.0, Computation time: 1.7546732425689697\n",
      "Step: 239, Loss: 0.9173570871353149, Accuracy: 1.0, Computation time: 2.338632822036743\n",
      "Step: 240, Loss: 0.9175123572349548, Accuracy: 1.0, Computation time: 2.147258996963501\n",
      "Step: 241, Loss: 0.9177820086479187, Accuracy: 1.0, Computation time: 2.1871628761291504\n",
      "Step: 242, Loss: 0.916467547416687, Accuracy: 1.0, Computation time: 2.0596156120300293\n",
      "Step: 243, Loss: 0.9174308776855469, Accuracy: 1.0, Computation time: 2.651691436767578\n",
      "Step: 244, Loss: 0.916878879070282, Accuracy: 1.0, Computation time: 1.9512391090393066\n",
      "Step: 245, Loss: 0.9161803722381592, Accuracy: 1.0, Computation time: 2.0328714847564697\n",
      "Step: 246, Loss: 0.9190027117729187, Accuracy: 1.0, Computation time: 2.4487974643707275\n",
      "Step: 247, Loss: 0.9365302920341492, Accuracy: 0.9772727489471436, Computation time: 1.8077142238616943\n",
      "Step: 248, Loss: 0.9190355539321899, Accuracy: 1.0, Computation time: 1.8891520500183105\n",
      "Step: 249, Loss: 0.9164512157440186, Accuracy: 1.0, Computation time: 2.0720701217651367\n",
      "Step: 250, Loss: 0.9209463596343994, Accuracy: 1.0, Computation time: 2.0424418449401855\n",
      "Step: 251, Loss: 0.9180431962013245, Accuracy: 1.0, Computation time: 2.1259474754333496\n",
      "Step: 252, Loss: 0.9167072176933289, Accuracy: 1.0, Computation time: 1.8050305843353271\n",
      "Step: 253, Loss: 0.9162128567695618, Accuracy: 1.0, Computation time: 1.7586097717285156\n",
      "Step: 254, Loss: 0.9497789144515991, Accuracy: 0.9333333373069763, Computation time: 1.920691728591919\n",
      "Step: 255, Loss: 0.9381579756736755, Accuracy: 0.9583333730697632, Computation time: 1.8141953945159912\n",
      "Step: 256, Loss: 0.918371319770813, Accuracy: 1.0, Computation time: 1.9785192012786865\n",
      "Step: 257, Loss: 0.9181035161018372, Accuracy: 1.0, Computation time: 1.985417127609253\n",
      "Step: 258, Loss: 0.9176043272018433, Accuracy: 1.0, Computation time: 1.8469889163970947\n",
      "Step: 259, Loss: 0.916636049747467, Accuracy: 1.0, Computation time: 1.8246557712554932\n",
      "Step: 260, Loss: 0.933619499206543, Accuracy: 0.9642857313156128, Computation time: 1.8653347492218018\n",
      "Step: 261, Loss: 0.916780948638916, Accuracy: 1.0, Computation time: 1.7818126678466797\n",
      "Step: 262, Loss: 0.9270239472389221, Accuracy: 0.96875, Computation time: 1.6460647583007812\n",
      "Step: 263, Loss: 0.9164905548095703, Accuracy: 1.0, Computation time: 1.81498384475708\n",
      "Step: 264, Loss: 0.9161601066589355, Accuracy: 1.0, Computation time: 2.0348360538482666\n",
      "Step: 265, Loss: 0.9374370574951172, Accuracy: 0.949999988079071, Computation time: 1.9057660102844238\n",
      "Step: 266, Loss: 0.916492760181427, Accuracy: 1.0, Computation time: 1.7364919185638428\n",
      "Step: 267, Loss: 0.9163328409194946, Accuracy: 1.0, Computation time: 2.2603023052215576\n",
      "Step: 268, Loss: 0.9166696667671204, Accuracy: 1.0, Computation time: 2.0994315147399902\n",
      "Step: 269, Loss: 0.9356285929679871, Accuracy: 0.9807692766189575, Computation time: 1.7356443405151367\n",
      "Step: 270, Loss: 0.9174463152885437, Accuracy: 1.0, Computation time: 1.7020502090454102\n",
      "Step: 271, Loss: 0.9392083883285522, Accuracy: 0.9772727489471436, Computation time: 1.770714282989502\n",
      "Step: 272, Loss: 0.9173508882522583, Accuracy: 1.0, Computation time: 1.6609690189361572\n",
      "Step: 273, Loss: 0.916088342666626, Accuracy: 1.0, Computation time: 1.809558629989624\n",
      "Step: 274, Loss: 0.9169340133666992, Accuracy: 1.0, Computation time: 1.6750218868255615\n",
      "Step: 275, Loss: 0.9167698621749878, Accuracy: 1.0, Computation time: 1.4249417781829834\n",
      "Step: 276, Loss: 0.9181968569755554, Accuracy: 1.0, Computation time: 1.766228199005127\n",
      "Step: 277, Loss: 0.9163097739219666, Accuracy: 1.0, Computation time: 1.6992321014404297\n",
      "Step: 278, Loss: 0.9163210391998291, Accuracy: 1.0, Computation time: 1.6586287021636963\n",
      "########################\n",
      "Test loss: 1.115358829498291, Test Accuracy_epoch2: 0.7023465633392334\n",
      "########################\n",
      "Step: 279, Loss: 0.9169557690620422, Accuracy: 1.0, Computation time: 1.7356557846069336\n",
      "Step: 280, Loss: 0.918086051940918, Accuracy: 1.0, Computation time: 1.5207762718200684\n",
      "Step: 281, Loss: 0.9369608759880066, Accuracy: 0.9791666865348816, Computation time: 1.717268705368042\n",
      "Step: 282, Loss: 0.9166572690010071, Accuracy: 1.0, Computation time: 1.6302952766418457\n",
      "Step: 283, Loss: 0.9162160754203796, Accuracy: 1.0, Computation time: 1.962740421295166\n",
      "Step: 284, Loss: 0.9351602792739868, Accuracy: 0.9642857313156128, Computation time: 1.9559242725372314\n",
      "Step: 285, Loss: 0.9162858128547668, Accuracy: 1.0, Computation time: 1.6863946914672852\n",
      "Step: 286, Loss: 0.9167441725730896, Accuracy: 1.0, Computation time: 1.8524675369262695\n",
      "Step: 287, Loss: 0.9166970252990723, Accuracy: 1.0, Computation time: 1.8347194194793701\n",
      "Step: 288, Loss: 0.916831910610199, Accuracy: 1.0, Computation time: 1.8280763626098633\n",
      "Step: 289, Loss: 0.9170639514923096, Accuracy: 1.0, Computation time: 1.5795791149139404\n",
      "Step: 290, Loss: 0.9163129329681396, Accuracy: 1.0, Computation time: 1.6858153343200684\n",
      "Step: 291, Loss: 0.9164857864379883, Accuracy: 1.0, Computation time: 1.839836597442627\n",
      "Step: 292, Loss: 0.9165551662445068, Accuracy: 1.0, Computation time: 2.0192158222198486\n",
      "Step: 293, Loss: 0.9166256785392761, Accuracy: 1.0, Computation time: 2.044804334640503\n",
      "Step: 294, Loss: 0.916719913482666, Accuracy: 1.0, Computation time: 1.900918960571289\n",
      "Step: 295, Loss: 0.9171080589294434, Accuracy: 1.0, Computation time: 1.8389897346496582\n",
      "Step: 296, Loss: 0.9184037446975708, Accuracy: 1.0, Computation time: 1.721719741821289\n",
      "Step: 297, Loss: 0.9162780046463013, Accuracy: 1.0, Computation time: 1.7497944831848145\n",
      "Step: 298, Loss: 0.9207520484924316, Accuracy: 1.0, Computation time: 1.6711242198944092\n",
      "Step: 299, Loss: 0.9164974093437195, Accuracy: 1.0, Computation time: 1.8373310565948486\n",
      "Step: 300, Loss: 0.9162294864654541, Accuracy: 1.0, Computation time: 1.5578532218933105\n",
      "Step: 301, Loss: 0.9385478496551514, Accuracy: 0.9750000238418579, Computation time: 1.9902360439300537\n",
      "Step: 302, Loss: 0.9170753955841064, Accuracy: 1.0, Computation time: 1.8843944072723389\n",
      "Step: 303, Loss: 0.916319727897644, Accuracy: 1.0, Computation time: 1.8622784614562988\n",
      "Step: 304, Loss: 0.9161199331283569, Accuracy: 1.0, Computation time: 1.6572566032409668\n",
      "Step: 305, Loss: 0.9331062436103821, Accuracy: 1.0, Computation time: 2.3426713943481445\n",
      "Step: 306, Loss: 0.9186199903488159, Accuracy: 1.0, Computation time: 1.6068761348724365\n",
      "Step: 307, Loss: 0.938661515712738, Accuracy: 0.9791666865348816, Computation time: 2.320249319076538\n",
      "Step: 308, Loss: 0.9568796753883362, Accuracy: 0.949999988079071, Computation time: 1.8500683307647705\n",
      "Step: 309, Loss: 0.9377656579017639, Accuracy: 0.96875, Computation time: 1.9442341327667236\n",
      "Step: 310, Loss: 0.9164605140686035, Accuracy: 1.0, Computation time: 2.0071303844451904\n",
      "Step: 311, Loss: 0.916965126991272, Accuracy: 1.0, Computation time: 1.681929111480713\n",
      "Step: 312, Loss: 0.9172928929328918, Accuracy: 1.0, Computation time: 1.6216230392456055\n",
      "Step: 313, Loss: 0.9167851209640503, Accuracy: 1.0, Computation time: 1.9388339519500732\n",
      "Step: 314, Loss: 0.9197025299072266, Accuracy: 1.0, Computation time: 1.9336824417114258\n",
      "Step: 315, Loss: 0.9250770211219788, Accuracy: 1.0, Computation time: 1.8247861862182617\n",
      "Step: 316, Loss: 0.9364723563194275, Accuracy: 0.984375, Computation time: 2.0865681171417236\n",
      "Step: 317, Loss: 0.9733113050460815, Accuracy: 0.9295454621315002, Computation time: 2.109619379043579\n",
      "Step: 318, Loss: 0.9557021856307983, Accuracy: 0.9392857551574707, Computation time: 2.195082426071167\n",
      "Step: 319, Loss: 0.91685950756073, Accuracy: 1.0, Computation time: 1.863391399383545\n",
      "Step: 320, Loss: 0.9178392291069031, Accuracy: 1.0, Computation time: 1.9903945922851562\n",
      "Step: 321, Loss: 0.9168576002120972, Accuracy: 1.0, Computation time: 2.0312676429748535\n",
      "Step: 322, Loss: 0.9190037846565247, Accuracy: 1.0, Computation time: 1.8844623565673828\n",
      "Step: 323, Loss: 0.9251956939697266, Accuracy: 1.0, Computation time: 1.6160635948181152\n",
      "Step: 324, Loss: 0.9337106943130493, Accuracy: 0.9642857313156128, Computation time: 1.7004272937774658\n",
      "Step: 325, Loss: 0.9380125999450684, Accuracy: 0.9772727489471436, Computation time: 2.2427351474761963\n",
      "Step: 326, Loss: 0.9169184565544128, Accuracy: 1.0, Computation time: 2.2642061710357666\n",
      "Step: 327, Loss: 0.9348385334014893, Accuracy: 0.9772727489471436, Computation time: 1.9664223194122314\n",
      "Step: 328, Loss: 0.9161551594734192, Accuracy: 1.0, Computation time: 1.628854513168335\n",
      "Step: 329, Loss: 0.9167757034301758, Accuracy: 1.0, Computation time: 1.8622841835021973\n",
      "Step: 330, Loss: 0.9165451526641846, Accuracy: 1.0, Computation time: 1.8559463024139404\n",
      "Step: 331, Loss: 0.9161683917045593, Accuracy: nan, Computation time: 1.8358666896820068\n",
      "Step: 332, Loss: 0.9170473217964172, Accuracy: 1.0, Computation time: 1.8012945652008057\n",
      "Step: 333, Loss: 0.9163153767585754, Accuracy: 1.0, Computation time: 1.837519645690918\n",
      "Step: 334, Loss: 0.9175204038619995, Accuracy: 1.0, Computation time: 2.044372081756592\n",
      "Step: 335, Loss: 0.9706293344497681, Accuracy: 0.9318181872367859, Computation time: 1.9565939903259277\n",
      "Step: 336, Loss: 0.9160887598991394, Accuracy: 1.0, Computation time: 1.8783786296844482\n",
      "Step: 337, Loss: 0.9161534309387207, Accuracy: 1.0, Computation time: 1.8308184146881104\n",
      "Step: 338, Loss: 0.9179398417472839, Accuracy: 1.0, Computation time: 1.7499301433563232\n",
      "Step: 339, Loss: 0.9164122939109802, Accuracy: 1.0, Computation time: 2.1284263134002686\n",
      "Step: 340, Loss: 0.9182369112968445, Accuracy: 1.0, Computation time: 1.823983907699585\n",
      "Step: 341, Loss: 0.9291343092918396, Accuracy: 0.9722222089767456, Computation time: 1.712618350982666\n",
      "Step: 342, Loss: 0.9265015721321106, Accuracy: 0.9722222089767456, Computation time: 1.681041955947876\n",
      "Step: 343, Loss: 0.9175032377243042, Accuracy: 1.0, Computation time: 1.975726842880249\n",
      "Step: 344, Loss: 0.9165213704109192, Accuracy: 1.0, Computation time: 2.0910391807556152\n",
      "Step: 345, Loss: 0.9167555570602417, Accuracy: 1.0, Computation time: 1.6906523704528809\n",
      "Step: 346, Loss: 0.9205178618431091, Accuracy: 1.0, Computation time: 1.7170748710632324\n",
      "Step: 347, Loss: 0.9162203669548035, Accuracy: 1.0, Computation time: 1.9257993698120117\n",
      "Step: 348, Loss: 0.9174820780754089, Accuracy: 1.0, Computation time: 1.9614002704620361\n",
      "Step: 349, Loss: 0.9406872391700745, Accuracy: 0.9750000238418579, Computation time: 1.728935718536377\n",
      "Step: 350, Loss: 0.9240571856498718, Accuracy: 1.0, Computation time: 2.0152347087860107\n",
      "Step: 351, Loss: 0.9161331653594971, Accuracy: 1.0, Computation time: 2.401811361312866\n",
      "Step: 352, Loss: 0.9163060188293457, Accuracy: 1.0, Computation time: 1.9750547409057617\n",
      "Step: 353, Loss: 0.916870653629303, Accuracy: 1.0, Computation time: 1.776900291442871\n",
      "Step: 354, Loss: 0.9213065505027771, Accuracy: 1.0, Computation time: 1.6152937412261963\n",
      "Step: 355, Loss: 0.9331534504890442, Accuracy: 0.984375, Computation time: 2.067655563354492\n",
      "Step: 356, Loss: 0.9366777539253235, Accuracy: 0.949999988079071, Computation time: 2.3055450916290283\n",
      "Step: 357, Loss: 0.9192859530448914, Accuracy: 1.0, Computation time: 2.16756010055542\n",
      "Step: 358, Loss: 0.9165729284286499, Accuracy: 1.0, Computation time: 2.061147689819336\n",
      "Step: 359, Loss: 0.9291478991508484, Accuracy: 0.96875, Computation time: 2.074185609817505\n",
      "Step: 360, Loss: 0.9164382815361023, Accuracy: 1.0, Computation time: 2.1010684967041016\n",
      "Step: 361, Loss: 0.916357159614563, Accuracy: 1.0, Computation time: 2.622666358947754\n",
      "Step: 362, Loss: 0.9161298274993896, Accuracy: 1.0, Computation time: 2.160243272781372\n",
      "Step: 363, Loss: 0.9371091723442078, Accuracy: 0.9807692766189575, Computation time: 1.9351246356964111\n",
      "Step: 364, Loss: 0.9165869355201721, Accuracy: 1.0, Computation time: 1.9843690395355225\n",
      "Step: 365, Loss: 0.9186395406723022, Accuracy: 1.0, Computation time: 1.6481282711029053\n",
      "Step: 366, Loss: 0.9161554574966431, Accuracy: 1.0, Computation time: 2.2326853275299072\n",
      "Step: 367, Loss: 0.9170012474060059, Accuracy: 1.0, Computation time: 1.7221992015838623\n",
      "Step: 368, Loss: 0.9274408221244812, Accuracy: 0.9642857313156128, Computation time: 2.131688356399536\n",
      "Step: 369, Loss: 0.9214797616004944, Accuracy: 1.0, Computation time: 1.8672280311584473\n",
      "Step: 370, Loss: 0.9241428971290588, Accuracy: 1.0, Computation time: 1.8861353397369385\n",
      "Step: 371, Loss: 0.923250675201416, Accuracy: 1.0, Computation time: 2.131549119949341\n",
      "Step: 372, Loss: 0.9197655916213989, Accuracy: 1.0, Computation time: 2.066178560256958\n",
      "Step: 373, Loss: 0.9160381555557251, Accuracy: 1.0, Computation time: 1.9389021396636963\n",
      "Step: 374, Loss: 0.9427776336669922, Accuracy: 0.9772727489471436, Computation time: 1.7737152576446533\n",
      "Step: 375, Loss: 0.9166392087936401, Accuracy: 1.0, Computation time: 1.8380753993988037\n",
      "Step: 376, Loss: 0.9169852137565613, Accuracy: 1.0, Computation time: 1.9045228958129883\n",
      "Step: 377, Loss: 0.9172138571739197, Accuracy: 1.0, Computation time: 1.7656917572021484\n",
      "Step: 378, Loss: 0.9174981117248535, Accuracy: 1.0, Computation time: 2.6279687881469727\n",
      "Step: 379, Loss: 0.924883246421814, Accuracy: 1.0, Computation time: 1.7974603176116943\n",
      "Step: 380, Loss: 0.9395133256912231, Accuracy: 0.9583333730697632, Computation time: 2.0382025241851807\n",
      "Step: 381, Loss: 0.9194201231002808, Accuracy: 1.0, Computation time: 1.7004117965698242\n",
      "Step: 382, Loss: 0.9300180673599243, Accuracy: 0.9583333730697632, Computation time: 1.6307356357574463\n",
      "Step: 383, Loss: 0.9163869619369507, Accuracy: 1.0, Computation time: 1.6709651947021484\n",
      "Step: 384, Loss: 0.9164000749588013, Accuracy: 1.0, Computation time: 1.798386812210083\n",
      "Step: 385, Loss: 0.9361025094985962, Accuracy: 0.9583333730697632, Computation time: 1.9793269634246826\n",
      "Step: 386, Loss: 0.9161792993545532, Accuracy: 1.0, Computation time: 1.7054316997528076\n",
      "Step: 387, Loss: 0.9166622757911682, Accuracy: 1.0, Computation time: 2.160374641418457\n",
      "Step: 388, Loss: 0.9426466226577759, Accuracy: 0.9807692766189575, Computation time: 2.018411874771118\n",
      "Step: 389, Loss: 0.9336007833480835, Accuracy: 0.9642857313156128, Computation time: 2.0880353450775146\n",
      "Step: 390, Loss: 0.919927716255188, Accuracy: 1.0, Computation time: 2.2493364810943604\n",
      "Step: 391, Loss: 0.9198398590087891, Accuracy: 1.0, Computation time: 2.2040064334869385\n",
      "Step: 392, Loss: 0.9169947504997253, Accuracy: 1.0, Computation time: 2.1408157348632812\n",
      "Step: 393, Loss: 0.91831374168396, Accuracy: 1.0, Computation time: 1.975428581237793\n",
      "Step: 394, Loss: 0.9163358211517334, Accuracy: 1.0, Computation time: 2.0288381576538086\n",
      "Step: 395, Loss: 0.9174533486366272, Accuracy: 1.0, Computation time: 2.2522542476654053\n",
      "Step: 396, Loss: 0.9170188307762146, Accuracy: 1.0, Computation time: 2.1007351875305176\n",
      "Step: 397, Loss: 0.9202841520309448, Accuracy: 1.0, Computation time: 2.4321084022521973\n",
      "Step: 398, Loss: 0.9532738924026489, Accuracy: 0.9791666865348816, Computation time: 2.227323055267334\n",
      "Step: 399, Loss: 0.9181687831878662, Accuracy: 1.0, Computation time: 2.222970485687256\n",
      "Step: 400, Loss: 0.937968909740448, Accuracy: 0.9722222089767456, Computation time: 2.364060640335083\n",
      "Step: 401, Loss: 0.919848620891571, Accuracy: 1.0, Computation time: 2.267475128173828\n",
      "Step: 402, Loss: 0.9348444938659668, Accuracy: 0.9821428656578064, Computation time: 2.3139941692352295\n",
      "Step: 403, Loss: 0.9600877165794373, Accuracy: 0.9513888955116272, Computation time: 2.5058603286743164\n",
      "Step: 404, Loss: 0.9332700967788696, Accuracy: 0.984375, Computation time: 2.4783742427825928\n",
      "Step: 405, Loss: 0.9459666609764099, Accuracy: 0.9404761791229248, Computation time: 2.0221245288848877\n",
      "Step: 406, Loss: 0.9162092804908752, Accuracy: 1.0, Computation time: 1.7987782955169678\n",
      "Step: 407, Loss: 0.936145007610321, Accuracy: 0.9807692766189575, Computation time: 2.067596435546875\n",
      "Step: 408, Loss: 0.9229015111923218, Accuracy: 1.0, Computation time: 2.2499210834503174\n",
      "Step: 409, Loss: 0.9168895483016968, Accuracy: 1.0, Computation time: 1.885446310043335\n",
      "Step: 410, Loss: 0.9172600507736206, Accuracy: 1.0, Computation time: 1.8311290740966797\n",
      "Step: 411, Loss: 0.9297471642494202, Accuracy: 0.949999988079071, Computation time: 2.480179786682129\n",
      "Step: 412, Loss: 0.9428598284721375, Accuracy: 0.9791666865348816, Computation time: 2.026933431625366\n",
      "Step: 413, Loss: 0.937920331954956, Accuracy: 0.9583333730697632, Computation time: 2.443215847015381\n",
      "Step: 414, Loss: 0.9188793897628784, Accuracy: 1.0, Computation time: 2.1655983924865723\n",
      "Step: 415, Loss: 0.915995180606842, Accuracy: 1.0, Computation time: 2.2491068840026855\n",
      "Step: 416, Loss: 0.9379141926765442, Accuracy: 0.9583333730697632, Computation time: 2.39333438873291\n",
      "Step: 417, Loss: 0.9182030558586121, Accuracy: 1.0, Computation time: 2.2977077960968018\n",
      "########################\n",
      "Test loss: 1.1089709997177124, Test Accuracy_epoch3: 0.7229626178741455\n",
      "########################\n",
      "Step: 418, Loss: 0.9170569181442261, Accuracy: 1.0, Computation time: 3.2322287559509277\n",
      "Step: 419, Loss: 0.9176986813545227, Accuracy: 1.0, Computation time: 3.184713363647461\n",
      "Step: 420, Loss: 0.9632852077484131, Accuracy: 0.9500000476837158, Computation time: 2.309922695159912\n",
      "Step: 421, Loss: 0.9268236756324768, Accuracy: 0.9791666865348816, Computation time: 2.0388710498809814\n",
      "Step: 422, Loss: 0.9160696864128113, Accuracy: 1.0, Computation time: 1.8643639087677002\n",
      "Step: 423, Loss: 0.9163042902946472, Accuracy: 1.0, Computation time: 1.8513209819793701\n",
      "Step: 424, Loss: 0.9331750273704529, Accuracy: 0.9375, Computation time: 1.8739676475524902\n",
      "Step: 425, Loss: 0.9164934158325195, Accuracy: 1.0, Computation time: 1.87799072265625\n",
      "Step: 426, Loss: 0.9390349984169006, Accuracy: 0.9791666865348816, Computation time: 1.9636588096618652\n",
      "Step: 427, Loss: 0.9170605540275574, Accuracy: 1.0, Computation time: 1.6799001693725586\n",
      "Step: 428, Loss: 0.9161372184753418, Accuracy: 1.0, Computation time: 2.0003437995910645\n",
      "Step: 429, Loss: 0.9190022945404053, Accuracy: 1.0, Computation time: 1.9162955284118652\n",
      "Step: 430, Loss: 0.9388331770896912, Accuracy: 0.9750000238418579, Computation time: 1.7660245895385742\n",
      "Step: 431, Loss: 0.9168486595153809, Accuracy: 1.0, Computation time: 1.818760633468628\n",
      "Step: 432, Loss: 0.9366390109062195, Accuracy: 0.9642857313156128, Computation time: 1.885814905166626\n",
      "Step: 433, Loss: 0.9190927743911743, Accuracy: 1.0, Computation time: 1.8441448211669922\n",
      "Step: 434, Loss: 0.9320213794708252, Accuracy: 0.9750000238418579, Computation time: 2.1932332515716553\n",
      "Step: 435, Loss: 0.9169338345527649, Accuracy: 1.0, Computation time: 1.8354988098144531\n",
      "Step: 436, Loss: 0.9385550022125244, Accuracy: 0.9772727489471436, Computation time: 1.7045459747314453\n",
      "Step: 437, Loss: 0.9389057755470276, Accuracy: 0.949999988079071, Computation time: 2.1169400215148926\n",
      "Step: 438, Loss: 0.9167735576629639, Accuracy: 1.0, Computation time: 2.048632860183716\n",
      "Step: 439, Loss: 0.9165215492248535, Accuracy: 1.0, Computation time: 1.7182276248931885\n",
      "Step: 440, Loss: 0.9387497901916504, Accuracy: 0.9750000238418579, Computation time: 1.716749668121338\n",
      "Step: 441, Loss: 0.9309402704238892, Accuracy: 0.9807692766189575, Computation time: 2.0489275455474854\n",
      "Step: 442, Loss: 0.9271163940429688, Accuracy: 0.96875, Computation time: 1.612377643585205\n",
      "Step: 443, Loss: 0.9163530468940735, Accuracy: 1.0, Computation time: 1.9703469276428223\n",
      "Step: 444, Loss: 0.9165369272232056, Accuracy: 1.0, Computation time: 2.206641435623169\n",
      "Step: 445, Loss: 0.9168177247047424, Accuracy: 1.0, Computation time: 1.969977617263794\n",
      "Step: 446, Loss: 0.916191816329956, Accuracy: 1.0, Computation time: 1.6195721626281738\n",
      "Step: 447, Loss: 0.9215967059135437, Accuracy: 1.0, Computation time: 1.8409371376037598\n",
      "Step: 448, Loss: 0.9161904454231262, Accuracy: 1.0, Computation time: 1.5766510963439941\n",
      "Step: 449, Loss: 0.9169054627418518, Accuracy: 1.0, Computation time: 1.6288812160491943\n",
      "Step: 450, Loss: 0.9165960550308228, Accuracy: 1.0, Computation time: 1.6882085800170898\n",
      "Step: 451, Loss: 0.9334269165992737, Accuracy: 0.9772727489471436, Computation time: 1.7322320938110352\n",
      "Step: 452, Loss: 0.948271632194519, Accuracy: 0.9522727727890015, Computation time: 1.6002860069274902\n",
      "Step: 453, Loss: 0.9173509478569031, Accuracy: 1.0, Computation time: 1.6575467586517334\n",
      "Step: 454, Loss: 0.9164834022521973, Accuracy: 1.0, Computation time: 1.5830941200256348\n",
      "Step: 455, Loss: 0.9236339330673218, Accuracy: 1.0, Computation time: 1.9833931922912598\n",
      "Step: 456, Loss: 0.9226502180099487, Accuracy: 1.0, Computation time: 1.5504531860351562\n",
      "Step: 457, Loss: 0.9170665144920349, Accuracy: 1.0, Computation time: 1.5657424926757812\n",
      "Step: 458, Loss: 0.9202466011047363, Accuracy: 1.0, Computation time: 2.1104960441589355\n",
      "Step: 459, Loss: 0.916398286819458, Accuracy: 1.0, Computation time: 1.5001742839813232\n",
      "Step: 460, Loss: 0.9437707662582397, Accuracy: 0.9375, Computation time: 1.9130282402038574\n",
      "Step: 461, Loss: 0.9379740357398987, Accuracy: 0.96875, Computation time: 1.5582520961761475\n",
      "Step: 462, Loss: 0.916603147983551, Accuracy: 1.0, Computation time: 1.6159186363220215\n",
      "Step: 463, Loss: 0.9172261357307434, Accuracy: 1.0, Computation time: 1.907810926437378\n",
      "Step: 464, Loss: 0.9166449904441833, Accuracy: 1.0, Computation time: 1.9943673610687256\n",
      "Step: 465, Loss: 0.9380654692649841, Accuracy: 0.9807692766189575, Computation time: 1.755129098892212\n",
      "Step: 466, Loss: 0.9382484555244446, Accuracy: 0.9722222089767456, Computation time: 1.8721420764923096\n",
      "Step: 467, Loss: 0.9331435561180115, Accuracy: 0.96875, Computation time: 2.4621267318725586\n",
      "Step: 468, Loss: 0.9168944358825684, Accuracy: 1.0, Computation time: 2.0264971256256104\n",
      "Step: 469, Loss: 0.91666179895401, Accuracy: 1.0, Computation time: 1.873304843902588\n",
      "Step: 470, Loss: 0.9165838956832886, Accuracy: 1.0, Computation time: 1.6646854877471924\n",
      "Step: 471, Loss: 0.9161121845245361, Accuracy: 1.0, Computation time: 2.214317798614502\n",
      "Step: 472, Loss: 0.9350292682647705, Accuracy: 0.9772727489471436, Computation time: 1.673703670501709\n",
      "Step: 473, Loss: 0.937802255153656, Accuracy: 0.9750000238418579, Computation time: 1.5188112258911133\n",
      "Step: 474, Loss: 0.9167794585227966, Accuracy: 1.0, Computation time: 1.7179934978485107\n",
      "Step: 475, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 2.022989511489868\n",
      "Step: 476, Loss: 0.9168728590011597, Accuracy: 1.0, Computation time: 1.6513879299163818\n",
      "Step: 477, Loss: 0.9162644147872925, Accuracy: 1.0, Computation time: 1.797640085220337\n",
      "Step: 478, Loss: 0.9338482022285461, Accuracy: 0.9807692766189575, Computation time: 1.5678153038024902\n",
      "Step: 479, Loss: 0.9163083434104919, Accuracy: 1.0, Computation time: 1.5729801654815674\n",
      "Step: 480, Loss: 0.9315242767333984, Accuracy: 0.9750000238418579, Computation time: 1.950364112854004\n",
      "Step: 481, Loss: 0.9164974689483643, Accuracy: 1.0, Computation time: 1.9838390350341797\n",
      "Step: 482, Loss: 0.9471792578697205, Accuracy: 0.9495192766189575, Computation time: 2.1419827938079834\n",
      "Step: 483, Loss: 0.9370538592338562, Accuracy: 0.9583333730697632, Computation time: 1.450596809387207\n",
      "Step: 484, Loss: 0.9358106851577759, Accuracy: 0.9791666865348816, Computation time: 1.896432876586914\n",
      "Step: 485, Loss: 0.9164841175079346, Accuracy: 1.0, Computation time: 1.682711124420166\n",
      "Step: 486, Loss: 0.916637659072876, Accuracy: 1.0, Computation time: 1.741189956665039\n",
      "Step: 487, Loss: 0.9170517325401306, Accuracy: 1.0, Computation time: 1.7601242065429688\n",
      "Step: 488, Loss: 0.9163181781768799, Accuracy: 1.0, Computation time: 1.6715540885925293\n",
      "Step: 489, Loss: 0.9338763952255249, Accuracy: 0.9772727489471436, Computation time: 1.9442965984344482\n",
      "Step: 490, Loss: 0.921780526638031, Accuracy: 1.0, Computation time: 2.086538076400757\n",
      "Step: 491, Loss: 0.9162769317626953, Accuracy: 1.0, Computation time: 1.678290605545044\n",
      "Step: 492, Loss: 0.9160175919532776, Accuracy: 1.0, Computation time: 1.5586869716644287\n",
      "Step: 493, Loss: 0.9414607286453247, Accuracy: 0.9642857313156128, Computation time: 1.5433950424194336\n",
      "Step: 494, Loss: 0.9168069362640381, Accuracy: 1.0, Computation time: 2.0476765632629395\n",
      "Step: 495, Loss: 0.9215844869613647, Accuracy: 1.0, Computation time: 1.4841933250427246\n",
      "Step: 496, Loss: 0.9367720484733582, Accuracy: 0.9722222089767456, Computation time: 1.6933817863464355\n",
      "Step: 497, Loss: 0.9162556529045105, Accuracy: 1.0, Computation time: 2.1665892601013184\n",
      "Step: 498, Loss: 0.9162037372589111, Accuracy: 1.0, Computation time: 1.7031307220458984\n",
      "Step: 499, Loss: 0.9160003066062927, Accuracy: 1.0, Computation time: 1.881617546081543\n",
      "Step: 500, Loss: 0.9160507917404175, Accuracy: 1.0, Computation time: 1.6268765926361084\n",
      "Step: 501, Loss: 0.9163985252380371, Accuracy: 1.0, Computation time: 1.9462502002716064\n",
      "Step: 502, Loss: 0.9178434014320374, Accuracy: 1.0, Computation time: 1.8359763622283936\n",
      "Step: 503, Loss: 0.9373955726623535, Accuracy: 0.96875, Computation time: 1.899115800857544\n",
      "Step: 504, Loss: 0.9163626432418823, Accuracy: 1.0, Computation time: 2.134063720703125\n",
      "Step: 505, Loss: 0.9162848591804504, Accuracy: 1.0, Computation time: 2.628089666366577\n",
      "Step: 506, Loss: 0.9163172245025635, Accuracy: 1.0, Computation time: 1.9541172981262207\n",
      "Step: 507, Loss: 0.9165552854537964, Accuracy: 1.0, Computation time: 1.817342758178711\n",
      "Step: 508, Loss: 0.9173704981803894, Accuracy: 1.0, Computation time: 1.9110569953918457\n",
      "Step: 509, Loss: 0.91663658618927, Accuracy: 1.0, Computation time: 2.2563793659210205\n",
      "Step: 510, Loss: 0.9163795709609985, Accuracy: 1.0, Computation time: 2.1554441452026367\n",
      "Step: 511, Loss: 0.9160001873970032, Accuracy: 1.0, Computation time: 2.0668718814849854\n",
      "Step: 512, Loss: 0.9162809252738953, Accuracy: 1.0, Computation time: 2.056204319000244\n",
      "Step: 513, Loss: 0.9165568351745605, Accuracy: 1.0, Computation time: 1.640916347503662\n",
      "Step: 514, Loss: 0.9162010550498962, Accuracy: 1.0, Computation time: 2.327195882797241\n",
      "Step: 515, Loss: 0.9290520548820496, Accuracy: 0.9750000238418579, Computation time: 1.9450304508209229\n",
      "Step: 516, Loss: 0.916588544845581, Accuracy: 1.0, Computation time: 2.072598457336426\n",
      "Step: 517, Loss: 0.9388047456741333, Accuracy: 0.9861111044883728, Computation time: 2.1503398418426514\n",
      "Step: 518, Loss: 0.916709303855896, Accuracy: 1.0, Computation time: 1.9531564712524414\n",
      "Step: 519, Loss: 0.9162795543670654, Accuracy: 1.0, Computation time: 2.05202054977417\n",
      "Step: 520, Loss: 0.916460394859314, Accuracy: 1.0, Computation time: 1.8935105800628662\n",
      "Step: 521, Loss: 0.9174126386642456, Accuracy: 1.0, Computation time: 2.271867036819458\n",
      "Step: 522, Loss: 0.9206079244613647, Accuracy: 1.0, Computation time: 2.239370346069336\n",
      "Step: 523, Loss: 0.9162985682487488, Accuracy: 1.0, Computation time: 2.1359167098999023\n",
      "Step: 524, Loss: 0.9354512095451355, Accuracy: 0.9791666865348816, Computation time: 2.1252472400665283\n",
      "Step: 525, Loss: 0.9165246486663818, Accuracy: 1.0, Computation time: 1.990281343460083\n",
      "Step: 526, Loss: 0.9161440134048462, Accuracy: 1.0, Computation time: 1.9491829872131348\n",
      "Step: 527, Loss: 0.9169915914535522, Accuracy: 1.0, Computation time: 2.1549553871154785\n",
      "Step: 528, Loss: 0.9161518216133118, Accuracy: 1.0, Computation time: 2.7238218784332275\n",
      "Step: 529, Loss: 0.9183687567710876, Accuracy: 1.0, Computation time: 1.9967947006225586\n",
      "Step: 530, Loss: 0.9336457252502441, Accuracy: 0.9772727489471436, Computation time: 1.8716044425964355\n",
      "Step: 531, Loss: 0.9350320100784302, Accuracy: 0.9722222089767456, Computation time: 2.024646520614624\n",
      "Step: 532, Loss: 0.916054904460907, Accuracy: 1.0, Computation time: 2.4886574745178223\n",
      "Step: 533, Loss: 0.9165944457054138, Accuracy: 1.0, Computation time: 2.234762668609619\n",
      "Step: 534, Loss: 0.9161813855171204, Accuracy: 1.0, Computation time: 2.175011157989502\n",
      "Step: 535, Loss: 0.916961669921875, Accuracy: 1.0, Computation time: 2.165931224822998\n",
      "Step: 536, Loss: 0.9173430800437927, Accuracy: 1.0, Computation time: 2.6962997913360596\n",
      "Step: 537, Loss: 0.9281039237976074, Accuracy: 0.9791666865348816, Computation time: 2.3899755477905273\n",
      "Step: 538, Loss: 0.9380757808685303, Accuracy: 0.9807692766189575, Computation time: 2.0966637134552\n",
      "Step: 539, Loss: 0.9161953330039978, Accuracy: 1.0, Computation time: 2.427990674972534\n",
      "Step: 540, Loss: 0.9163267612457275, Accuracy: 1.0, Computation time: 2.0465176105499268\n",
      "Step: 541, Loss: 0.9161586761474609, Accuracy: 1.0, Computation time: 2.1837761402130127\n",
      "Step: 542, Loss: 0.9279608726501465, Accuracy: 0.9821428656578064, Computation time: 2.1457247734069824\n",
      "Step: 543, Loss: 0.917221188545227, Accuracy: 1.0, Computation time: 2.107555627822876\n",
      "Step: 544, Loss: 0.9310433864593506, Accuracy: 0.9791666865348816, Computation time: 2.06300687789917\n",
      "Step: 545, Loss: 0.9186630249023438, Accuracy: 1.0, Computation time: 2.3330800533294678\n",
      "Step: 546, Loss: 0.934851884841919, Accuracy: 0.9583333730697632, Computation time: 1.9952969551086426\n",
      "Step: 547, Loss: 0.9376466870307922, Accuracy: 0.96875, Computation time: 2.4742462635040283\n",
      "Step: 548, Loss: 0.9162337183952332, Accuracy: 1.0, Computation time: 1.9154934883117676\n",
      "Step: 549, Loss: 0.9164729118347168, Accuracy: 1.0, Computation time: 1.7519984245300293\n",
      "Step: 550, Loss: 0.9164881706237793, Accuracy: 1.0, Computation time: 1.92781400680542\n",
      "Step: 551, Loss: 0.9166482090950012, Accuracy: 1.0, Computation time: 1.787125587463379\n",
      "Step: 552, Loss: 0.9163763523101807, Accuracy: 1.0, Computation time: 1.9675424098968506\n",
      "Step: 553, Loss: 0.9163844585418701, Accuracy: 1.0, Computation time: 2.3146209716796875\n",
      "Step: 554, Loss: 0.9161351919174194, Accuracy: 1.0, Computation time: 2.5124688148498535\n",
      "Step: 555, Loss: 0.9166119694709778, Accuracy: 1.0, Computation time: 1.8119494915008545\n",
      "Step: 556, Loss: 0.916084349155426, Accuracy: 1.0, Computation time: 2.2331416606903076\n",
      "########################\n",
      "Test loss: 1.1224874258041382, Test Accuracy_epoch4: 0.6999936103820801\n",
      "########################\n",
      "Step: 557, Loss: 0.9164423942565918, Accuracy: 1.0, Computation time: 3.141084909439087\n",
      "Step: 558, Loss: 0.9177083969116211, Accuracy: 1.0, Computation time: 2.846461772918701\n",
      "Step: 559, Loss: 0.9171763062477112, Accuracy: 1.0, Computation time: 2.8641090393066406\n",
      "Step: 560, Loss: 0.9187123775482178, Accuracy: 1.0, Computation time: 2.8043854236602783\n",
      "Step: 561, Loss: 0.9399274587631226, Accuracy: 0.96875, Computation time: 2.2831783294677734\n",
      "Step: 562, Loss: 0.9171459674835205, Accuracy: 1.0, Computation time: 2.4240987300872803\n",
      "Step: 563, Loss: 0.9160991907119751, Accuracy: 1.0, Computation time: 2.4464337825775146\n",
      "Step: 564, Loss: 0.9387472867965698, Accuracy: 0.9833333492279053, Computation time: 1.9471724033355713\n",
      "Step: 565, Loss: 0.9324163794517517, Accuracy: 0.9583333730697632, Computation time: 2.0382182598114014\n",
      "Step: 566, Loss: 0.9162623882293701, Accuracy: 1.0, Computation time: 1.9187450408935547\n",
      "Step: 567, Loss: 0.9162728786468506, Accuracy: 1.0, Computation time: 1.8577399253845215\n",
      "Step: 568, Loss: 0.9162663221359253, Accuracy: 1.0, Computation time: 1.7386837005615234\n",
      "Step: 569, Loss: 0.9183300137519836, Accuracy: 1.0, Computation time: 1.8450210094451904\n",
      "Step: 570, Loss: 0.9483360648155212, Accuracy: 0.96875, Computation time: 1.7686569690704346\n",
      "Step: 571, Loss: 0.9170783758163452, Accuracy: 1.0, Computation time: 2.1203324794769287\n",
      "Step: 572, Loss: 0.9486503005027771, Accuracy: 0.9522727727890015, Computation time: 1.8315989971160889\n",
      "Step: 573, Loss: 0.9169234037399292, Accuracy: 1.0, Computation time: 2.24709415435791\n",
      "Step: 574, Loss: 0.9166650176048279, Accuracy: 1.0, Computation time: 2.312746047973633\n",
      "Step: 575, Loss: 0.9171909689903259, Accuracy: 1.0, Computation time: 2.301595449447632\n",
      "Step: 576, Loss: 0.9340565204620361, Accuracy: 0.96875, Computation time: 2.432077407836914\n",
      "Step: 577, Loss: 0.9361516833305359, Accuracy: 0.96875, Computation time: 2.1622061729431152\n",
      "Step: 578, Loss: 0.9382777810096741, Accuracy: 0.9642857313156128, Computation time: 2.568502902984619\n",
      "Step: 579, Loss: 0.9204764366149902, Accuracy: 1.0, Computation time: 1.9401395320892334\n",
      "Step: 580, Loss: 0.9167841076850891, Accuracy: 1.0, Computation time: 2.36190128326416\n",
      "Step: 581, Loss: 0.9172827005386353, Accuracy: 1.0, Computation time: 2.3241586685180664\n",
      "Step: 582, Loss: 0.9380831718444824, Accuracy: 0.9750000238418579, Computation time: 2.119333028793335\n",
      "Step: 583, Loss: 0.9165019392967224, Accuracy: 1.0, Computation time: 2.391220808029175\n",
      "Step: 584, Loss: 0.916410505771637, Accuracy: 1.0, Computation time: 2.4030566215515137\n",
      "Step: 585, Loss: 0.9510732889175415, Accuracy: 0.9583333730697632, Computation time: 2.3999977111816406\n",
      "Step: 586, Loss: 0.9162887930870056, Accuracy: 1.0, Computation time: 2.445363998413086\n",
      "Step: 587, Loss: 0.9168558120727539, Accuracy: 1.0, Computation time: 2.412782907485962\n",
      "Step: 588, Loss: 0.9388421177864075, Accuracy: 0.9772727489471436, Computation time: 2.139997720718384\n",
      "Step: 589, Loss: 0.9381442070007324, Accuracy: 0.9833333492279053, Computation time: 2.4387435913085938\n",
      "Step: 590, Loss: 0.9162271618843079, Accuracy: 1.0, Computation time: 2.551405906677246\n",
      "Step: 591, Loss: 0.9163859486579895, Accuracy: 1.0, Computation time: 2.1638762950897217\n",
      "Step: 592, Loss: 0.9382245540618896, Accuracy: 0.9807692766189575, Computation time: 2.3303613662719727\n",
      "Step: 593, Loss: 0.9565135836601257, Accuracy: 0.9450550079345703, Computation time: 2.686251640319824\n",
      "Step: 594, Loss: 0.9164072275161743, Accuracy: 1.0, Computation time: 2.1018006801605225\n",
      "Step: 595, Loss: 0.9168411493301392, Accuracy: 1.0, Computation time: 1.8580093383789062\n",
      "Step: 596, Loss: 0.9166136980056763, Accuracy: 1.0, Computation time: 2.021695613861084\n",
      "Step: 597, Loss: 0.9164682626724243, Accuracy: 1.0, Computation time: 2.2518253326416016\n",
      "Step: 598, Loss: 0.9168586134910583, Accuracy: 1.0, Computation time: 2.509190320968628\n",
      "Step: 599, Loss: 0.9181220531463623, Accuracy: 1.0, Computation time: 1.7907741069793701\n",
      "Step: 600, Loss: 0.9164050817489624, Accuracy: 1.0, Computation time: 1.768256664276123\n",
      "Step: 601, Loss: 0.9162295460700989, Accuracy: 1.0, Computation time: 2.182101249694824\n",
      "Step: 602, Loss: 0.9161309599876404, Accuracy: 1.0, Computation time: 2.002924680709839\n",
      "Step: 603, Loss: 0.91668701171875, Accuracy: 1.0, Computation time: 1.6904139518737793\n",
      "Step: 604, Loss: 0.9159854054450989, Accuracy: 1.0, Computation time: 2.986602306365967\n",
      "Step: 605, Loss: 0.9160804748535156, Accuracy: 1.0, Computation time: 2.5324337482452393\n",
      "Step: 606, Loss: 0.9161011576652527, Accuracy: 1.0, Computation time: 1.9994969367980957\n",
      "Step: 607, Loss: 0.9160137176513672, Accuracy: 1.0, Computation time: 1.6002938747406006\n",
      "Step: 608, Loss: 0.9166528582572937, Accuracy: 1.0, Computation time: 1.7179696559906006\n",
      "Step: 609, Loss: 0.9161036014556885, Accuracy: 1.0, Computation time: 1.5791540145874023\n",
      "Step: 610, Loss: 0.9163033962249756, Accuracy: 1.0, Computation time: 1.5397443771362305\n",
      "Step: 611, Loss: 0.9335632920265198, Accuracy: 0.9772727489471436, Computation time: 1.977318525314331\n",
      "Step: 612, Loss: 0.9269426465034485, Accuracy: 0.9791666865348816, Computation time: 2.153627634048462\n",
      "Step: 613, Loss: 0.9166942238807678, Accuracy: 1.0, Computation time: 1.6779193878173828\n",
      "Step: 614, Loss: 0.9355760216712952, Accuracy: 0.9833333492279053, Computation time: 1.9042165279388428\n",
      "Step: 615, Loss: 0.9164876341819763, Accuracy: 1.0, Computation time: 1.5985515117645264\n",
      "Step: 616, Loss: 0.9162084460258484, Accuracy: 1.0, Computation time: 1.7985458374023438\n",
      "Step: 617, Loss: 0.9419651627540588, Accuracy: 0.9807692766189575, Computation time: 1.4900875091552734\n",
      "Step: 618, Loss: 0.9165427088737488, Accuracy: 1.0, Computation time: 2.019996166229248\n",
      "Step: 619, Loss: 0.9163259267807007, Accuracy: 1.0, Computation time: 1.5326600074768066\n",
      "Step: 620, Loss: 0.9162780046463013, Accuracy: 1.0, Computation time: 1.7033085823059082\n",
      "Step: 621, Loss: 0.9540054202079773, Accuracy: 0.9450550079345703, Computation time: 2.2925095558166504\n",
      "Step: 622, Loss: 0.9162181615829468, Accuracy: 1.0, Computation time: 1.7543039321899414\n",
      "Step: 623, Loss: 0.9162437915802002, Accuracy: 1.0, Computation time: 1.7066476345062256\n",
      "Step: 624, Loss: 0.9163394570350647, Accuracy: 1.0, Computation time: 1.8180913925170898\n",
      "Step: 625, Loss: 0.9160525798797607, Accuracy: 1.0, Computation time: 1.6047284603118896\n",
      "Step: 626, Loss: 0.919259250164032, Accuracy: 1.0, Computation time: 1.6248633861541748\n",
      "Step: 627, Loss: 0.9176613092422485, Accuracy: 1.0, Computation time: 1.7853078842163086\n",
      "Step: 628, Loss: 0.9371373057365417, Accuracy: 0.9791666865348816, Computation time: 1.5368211269378662\n",
      "Step: 629, Loss: 0.9180036783218384, Accuracy: 1.0, Computation time: 1.9785447120666504\n",
      "Step: 630, Loss: 0.9401471614837646, Accuracy: 0.949999988079071, Computation time: 2.2943739891052246\n",
      "Step: 631, Loss: 0.956186830997467, Accuracy: 0.9772727489471436, Computation time: 1.9567899703979492\n",
      "Step: 632, Loss: 0.9164987802505493, Accuracy: 1.0, Computation time: 1.8990287780761719\n",
      "Step: 633, Loss: 0.9163550734519958, Accuracy: 1.0, Computation time: 1.712991714477539\n",
      "Step: 634, Loss: 0.9244529604911804, Accuracy: 1.0, Computation time: 1.8022534847259521\n",
      "Step: 635, Loss: 0.9165406227111816, Accuracy: 1.0, Computation time: 1.553008794784546\n",
      "Step: 636, Loss: 0.9272199273109436, Accuracy: 0.9791666865348816, Computation time: 1.994504690170288\n",
      "Step: 637, Loss: 0.9163538813591003, Accuracy: 1.0, Computation time: 1.8058319091796875\n",
      "Step: 638, Loss: 0.9159857034683228, Accuracy: 1.0, Computation time: 1.877936840057373\n",
      "Step: 639, Loss: 0.9179174900054932, Accuracy: 1.0, Computation time: 1.9994137287139893\n",
      "Step: 640, Loss: 0.9164614081382751, Accuracy: 1.0, Computation time: 1.815333604812622\n",
      "Step: 641, Loss: 0.9306902289390564, Accuracy: 0.9750000238418579, Computation time: 2.1245193481445312\n",
      "Step: 642, Loss: 0.9169179201126099, Accuracy: 1.0, Computation time: 2.148468255996704\n",
      "Step: 643, Loss: 0.9161505103111267, Accuracy: 1.0, Computation time: 1.7497320175170898\n",
      "Step: 644, Loss: 0.9159309267997742, Accuracy: 1.0, Computation time: 1.9438979625701904\n",
      "Step: 645, Loss: 0.9378066062927246, Accuracy: 0.9722222089767456, Computation time: 1.9298486709594727\n",
      "Step: 646, Loss: 0.9159744381904602, Accuracy: 1.0, Computation time: 2.0471510887145996\n",
      "Step: 647, Loss: 0.9159602522850037, Accuracy: 1.0, Computation time: 2.0803842544555664\n",
      "Step: 648, Loss: 0.916120707988739, Accuracy: 1.0, Computation time: 2.1886396408081055\n",
      "Step: 649, Loss: 0.9160172343254089, Accuracy: 1.0, Computation time: 1.9179160594940186\n",
      "Step: 650, Loss: 0.9397475123405457, Accuracy: 0.9583333730697632, Computation time: 2.302438735961914\n",
      "Step: 651, Loss: 0.9160763621330261, Accuracy: 1.0, Computation time: 1.825284719467163\n",
      "Step: 652, Loss: 0.9160750508308411, Accuracy: 1.0, Computation time: 1.9169952869415283\n",
      "Step: 653, Loss: 0.915963351726532, Accuracy: 1.0, Computation time: 1.9390935897827148\n",
      "Step: 654, Loss: 0.9168782830238342, Accuracy: 1.0, Computation time: 1.8972818851470947\n",
      "Step: 655, Loss: 0.9215705394744873, Accuracy: 1.0, Computation time: 1.9762966632843018\n",
      "Step: 656, Loss: 0.9160858392715454, Accuracy: 1.0, Computation time: 2.013481378555298\n",
      "Step: 657, Loss: 0.9160493016242981, Accuracy: 1.0, Computation time: 1.8207111358642578\n",
      "Step: 658, Loss: 0.945183277130127, Accuracy: 0.9722222089767456, Computation time: 2.1283977031707764\n",
      "Step: 659, Loss: 0.9165369272232056, Accuracy: 1.0, Computation time: 1.9873571395874023\n",
      "Step: 660, Loss: 0.9365861415863037, Accuracy: 0.9791666865348816, Computation time: 2.0379648208618164\n",
      "Step: 661, Loss: 0.9167752265930176, Accuracy: 1.0, Computation time: 2.0692760944366455\n",
      "Step: 662, Loss: 0.9375576376914978, Accuracy: 0.9642857313156128, Computation time: 2.0366241931915283\n",
      "Step: 663, Loss: 0.9164175987243652, Accuracy: 1.0, Computation time: 2.2338709831237793\n",
      "Step: 664, Loss: 0.9179089665412903, Accuracy: 1.0, Computation time: 2.0093255043029785\n",
      "Step: 665, Loss: 0.9162815809249878, Accuracy: 1.0, Computation time: 2.4567487239837646\n",
      "Step: 666, Loss: 0.9161051511764526, Accuracy: 1.0, Computation time: 2.1174509525299072\n",
      "Step: 667, Loss: 0.9162846803665161, Accuracy: 1.0, Computation time: 1.9832677841186523\n",
      "Step: 668, Loss: 0.9163801074028015, Accuracy: 1.0, Computation time: 2.103604793548584\n",
      "Step: 669, Loss: 0.9165472388267517, Accuracy: 1.0, Computation time: 2.001535415649414\n",
      "Step: 670, Loss: 0.9193084239959717, Accuracy: 1.0, Computation time: 2.156459093093872\n",
      "Step: 671, Loss: 0.9231433272361755, Accuracy: 1.0, Computation time: 2.0863606929779053\n",
      "Step: 672, Loss: 0.9378682374954224, Accuracy: 0.9750000238418579, Computation time: 2.295506000518799\n",
      "Step: 673, Loss: 0.9180264472961426, Accuracy: 1.0, Computation time: 1.8364861011505127\n",
      "Step: 674, Loss: 0.916634202003479, Accuracy: 1.0, Computation time: 1.8483102321624756\n",
      "Step: 675, Loss: 0.9386922121047974, Accuracy: 0.9833333492279053, Computation time: 1.913480520248413\n",
      "Step: 676, Loss: 0.9177242517471313, Accuracy: 1.0, Computation time: 1.9344027042388916\n",
      "Step: 677, Loss: 0.9369761347770691, Accuracy: 0.9821428656578064, Computation time: 1.8530805110931396\n",
      "Step: 678, Loss: 0.9347361922264099, Accuracy: 0.9642857313156128, Computation time: 1.9666099548339844\n",
      "Step: 679, Loss: 0.9162760972976685, Accuracy: 1.0, Computation time: 1.8833823204040527\n",
      "Step: 680, Loss: 0.9167325496673584, Accuracy: 1.0, Computation time: 2.0337986946105957\n",
      "Step: 681, Loss: 0.917648196220398, Accuracy: 1.0, Computation time: 1.6226189136505127\n",
      "Step: 682, Loss: 0.9173661470413208, Accuracy: 1.0, Computation time: 1.8511848449707031\n",
      "Step: 683, Loss: 0.917641282081604, Accuracy: 1.0, Computation time: 2.103654146194458\n",
      "Step: 684, Loss: 0.9163943529129028, Accuracy: 1.0, Computation time: 1.6795172691345215\n",
      "Step: 685, Loss: 0.9165045022964478, Accuracy: 1.0, Computation time: 2.3366031646728516\n",
      "Step: 686, Loss: 0.9165780544281006, Accuracy: 1.0, Computation time: 2.1007487773895264\n",
      "Step: 687, Loss: 0.933367908000946, Accuracy: 0.9750000238418579, Computation time: 2.274970531463623\n",
      "Step: 688, Loss: 0.9162775278091431, Accuracy: 1.0, Computation time: 2.1717827320098877\n",
      "Step: 689, Loss: 0.9162694811820984, Accuracy: 1.0, Computation time: 2.251722812652588\n",
      "Step: 690, Loss: 0.918699324131012, Accuracy: 1.0, Computation time: 1.9032177925109863\n",
      "Step: 691, Loss: 0.91629558801651, Accuracy: 1.0, Computation time: 1.6724562644958496\n",
      "Step: 692, Loss: 0.9173471927642822, Accuracy: 1.0, Computation time: 1.9097874164581299\n",
      "Step: 693, Loss: 0.9163137674331665, Accuracy: 1.0, Computation time: 2.2114381790161133\n",
      "Step: 694, Loss: 0.9305448532104492, Accuracy: 0.9750000238418579, Computation time: 2.2620184421539307\n",
      "Step: 695, Loss: 0.9161293506622314, Accuracy: 1.0, Computation time: 1.7770402431488037\n",
      "########################\n",
      "Test loss: 1.1244210004806519, Test Accuracy_epoch5: 0.6983435153961182\n",
      "########################\n",
      "Step: 696, Loss: 0.9163764715194702, Accuracy: 1.0, Computation time: 1.842728853225708\n",
      "Step: 697, Loss: 0.9172095060348511, Accuracy: 1.0, Computation time: 2.0460031032562256\n",
      "Step: 698, Loss: 0.9161470532417297, Accuracy: 1.0, Computation time: 1.628380537033081\n",
      "Step: 699, Loss: 0.9184690117835999, Accuracy: 1.0, Computation time: 1.627213478088379\n",
      "Step: 700, Loss: 0.9349586963653564, Accuracy: 0.9722222089767456, Computation time: 1.7350490093231201\n",
      "Step: 701, Loss: 0.9160192608833313, Accuracy: 1.0, Computation time: 1.8072216510772705\n",
      "Step: 702, Loss: 0.9350362420082092, Accuracy: 0.9722222089767456, Computation time: 1.4949321746826172\n",
      "Step: 703, Loss: 0.9161809682846069, Accuracy: 1.0, Computation time: 1.500593662261963\n",
      "Step: 704, Loss: 0.9162424802780151, Accuracy: 1.0, Computation time: 1.6749804019927979\n",
      "Step: 705, Loss: 0.9198510646820068, Accuracy: 1.0, Computation time: 1.9857392311096191\n",
      "Step: 706, Loss: 0.9167431592941284, Accuracy: 1.0, Computation time: 1.918942928314209\n",
      "Step: 707, Loss: 0.9166689515113831, Accuracy: 1.0, Computation time: 2.147082567214966\n",
      "Step: 708, Loss: 0.9223012924194336, Accuracy: 1.0, Computation time: 1.4970145225524902\n",
      "Step: 709, Loss: 0.9167228937149048, Accuracy: 1.0, Computation time: 1.7237048149108887\n",
      "Step: 710, Loss: 0.9377641677856445, Accuracy: 0.9791666865348816, Computation time: 1.5247657299041748\n",
      "Step: 711, Loss: 0.9407728910446167, Accuracy: 0.96875, Computation time: 1.7063524723052979\n",
      "Step: 712, Loss: 0.9591634273529053, Accuracy: 0.9270833730697632, Computation time: 2.040543794631958\n",
      "Step: 713, Loss: 0.916294276714325, Accuracy: 1.0, Computation time: 1.7529635429382324\n",
      "Step: 714, Loss: 0.9164292812347412, Accuracy: 1.0, Computation time: 1.6706809997558594\n",
      "Step: 715, Loss: 0.9163714647293091, Accuracy: 1.0, Computation time: 1.8448140621185303\n",
      "Step: 716, Loss: 0.9163351655006409, Accuracy: 1.0, Computation time: 2.0151264667510986\n",
      "Step: 717, Loss: 0.9165462851524353, Accuracy: 1.0, Computation time: 1.7304584980010986\n",
      "Step: 718, Loss: 0.916454553604126, Accuracy: 1.0, Computation time: 1.909578800201416\n",
      "Step: 719, Loss: 0.9163772463798523, Accuracy: 1.0, Computation time: 1.9648792743682861\n",
      "Step: 720, Loss: 0.9174375534057617, Accuracy: 1.0, Computation time: 2.1621756553649902\n",
      "Step: 721, Loss: 0.9163978695869446, Accuracy: 1.0, Computation time: 2.0546720027923584\n",
      "Step: 722, Loss: 0.9161502122879028, Accuracy: 1.0, Computation time: 2.2793073654174805\n",
      "Step: 723, Loss: 0.9779439568519592, Accuracy: 0.8617216348648071, Computation time: 1.9411489963531494\n",
      "Step: 724, Loss: 0.9160359501838684, Accuracy: 1.0, Computation time: 1.7906494140625\n",
      "Step: 725, Loss: 0.9183005690574646, Accuracy: 1.0, Computation time: 2.2221808433532715\n",
      "Step: 726, Loss: 0.9361037611961365, Accuracy: 0.9772727489471436, Computation time: 1.529855728149414\n",
      "Step: 727, Loss: 0.9160454869270325, Accuracy: 1.0, Computation time: 1.95139741897583\n",
      "Step: 728, Loss: 0.9161515235900879, Accuracy: 1.0, Computation time: 1.9797887802124023\n",
      "Step: 729, Loss: 0.9160866737365723, Accuracy: 1.0, Computation time: 1.902287244796753\n",
      "Step: 730, Loss: 0.9377549886703491, Accuracy: 0.9583333730697632, Computation time: 1.7787747383117676\n",
      "Step: 731, Loss: 0.9179701805114746, Accuracy: 1.0, Computation time: 2.076233148574829\n",
      "Step: 732, Loss: 0.9365453124046326, Accuracy: 0.96875, Computation time: 2.5255582332611084\n",
      "Step: 733, Loss: 0.9162203073501587, Accuracy: 1.0, Computation time: 1.764951229095459\n",
      "Step: 734, Loss: 0.935297429561615, Accuracy: 0.9375, Computation time: 1.7022285461425781\n",
      "Step: 735, Loss: 0.9164057970046997, Accuracy: 1.0, Computation time: 1.9088773727416992\n",
      "Step: 736, Loss: 0.916256844997406, Accuracy: 1.0, Computation time: 2.334845542907715\n",
      "Step: 737, Loss: 0.9169411063194275, Accuracy: 1.0, Computation time: 2.041872024536133\n",
      "Step: 738, Loss: 0.9159747362136841, Accuracy: 1.0, Computation time: 2.146811008453369\n",
      "Step: 739, Loss: 0.9163093566894531, Accuracy: 1.0, Computation time: 2.054149866104126\n",
      "Step: 740, Loss: 0.9160306453704834, Accuracy: 1.0, Computation time: 1.7912774085998535\n",
      "Step: 741, Loss: 0.9160832762718201, Accuracy: 1.0, Computation time: 1.877758264541626\n",
      "Step: 742, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 1.9215922355651855\n",
      "Step: 743, Loss: 0.9183058142662048, Accuracy: 1.0, Computation time: 1.8767657279968262\n",
      "Step: 744, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 2.079127550125122\n",
      "Step: 745, Loss: 0.9160333275794983, Accuracy: 1.0, Computation time: 1.7734477519989014\n",
      "Step: 746, Loss: 0.9168872833251953, Accuracy: 1.0, Computation time: 1.654736042022705\n",
      "Step: 747, Loss: 0.9159794449806213, Accuracy: 1.0, Computation time: 1.7200639247894287\n",
      "Step: 748, Loss: 0.9160575866699219, Accuracy: 1.0, Computation time: 1.8780388832092285\n",
      "Step: 749, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 1.7465746402740479\n",
      "Step: 750, Loss: 0.9172220230102539, Accuracy: 1.0, Computation time: 1.886211633682251\n",
      "Step: 751, Loss: 0.9161237478256226, Accuracy: 1.0, Computation time: 1.7583673000335693\n",
      "Step: 752, Loss: 0.9160357117652893, Accuracy: 1.0, Computation time: 1.7605206966400146\n",
      "Step: 753, Loss: 0.9568390250205994, Accuracy: 0.9409722089767456, Computation time: 1.9917168617248535\n",
      "Step: 754, Loss: 0.9159875512123108, Accuracy: 1.0, Computation time: 1.8245909214019775\n",
      "Step: 755, Loss: 0.9212422370910645, Accuracy: 1.0, Computation time: 1.9003753662109375\n",
      "Step: 756, Loss: 0.9162948131561279, Accuracy: 1.0, Computation time: 2.00771427154541\n",
      "Step: 757, Loss: 0.9163007736206055, Accuracy: 1.0, Computation time: 1.854499340057373\n",
      "Step: 758, Loss: 0.9159832000732422, Accuracy: 1.0, Computation time: 1.6477837562561035\n",
      "Step: 759, Loss: 0.9159744381904602, Accuracy: 1.0, Computation time: 1.7434911727905273\n",
      "Step: 760, Loss: 0.9160289764404297, Accuracy: 1.0, Computation time: 1.8296113014221191\n",
      "Step: 761, Loss: 0.918828547000885, Accuracy: 1.0, Computation time: 1.8413736820220947\n",
      "Step: 762, Loss: 0.9372753500938416, Accuracy: 0.9750000238418579, Computation time: 1.9151203632354736\n",
      "Step: 763, Loss: 0.9376500248908997, Accuracy: 0.9750000238418579, Computation time: 1.7468750476837158\n",
      "Step: 764, Loss: 0.9162289500236511, Accuracy: 1.0, Computation time: 1.6486830711364746\n",
      "Step: 765, Loss: 0.9311318397521973, Accuracy: 0.96875, Computation time: 1.9471914768218994\n",
      "Step: 766, Loss: 0.9375351667404175, Accuracy: 0.9852941036224365, Computation time: 1.9583330154418945\n",
      "Step: 767, Loss: 0.9190949201583862, Accuracy: 1.0, Computation time: 2.0603129863739014\n",
      "Step: 768, Loss: 0.9166218042373657, Accuracy: 1.0, Computation time: 2.1952216625213623\n",
      "Step: 769, Loss: 0.9369868040084839, Accuracy: 0.9791666865348816, Computation time: 1.8737144470214844\n",
      "Step: 770, Loss: 0.9376705884933472, Accuracy: 0.9791666865348816, Computation time: 1.814685344696045\n",
      "Step: 771, Loss: 0.9161476492881775, Accuracy: 1.0, Computation time: 1.832021951675415\n",
      "Step: 772, Loss: 0.9406403303146362, Accuracy: 0.9642857313156128, Computation time: 2.1499853134155273\n",
      "Step: 773, Loss: 0.9373903274536133, Accuracy: 0.9791666865348816, Computation time: 2.080573081970215\n",
      "Step: 774, Loss: 0.9359717965126038, Accuracy: 0.9772727489471436, Computation time: 2.0403923988342285\n",
      "Step: 775, Loss: 0.9161592125892639, Accuracy: 1.0, Computation time: 2.186012029647827\n",
      "Step: 776, Loss: 0.9161624908447266, Accuracy: 1.0, Computation time: 2.6199517250061035\n",
      "Step: 777, Loss: 0.9378315210342407, Accuracy: 0.9750000238418579, Computation time: 1.7399747371673584\n",
      "Step: 778, Loss: 0.9161533713340759, Accuracy: 1.0, Computation time: 2.1664962768554688\n",
      "Step: 779, Loss: 0.9371628761291504, Accuracy: 0.9750000238418579, Computation time: 1.9645352363586426\n",
      "Step: 780, Loss: 0.9160305261611938, Accuracy: 1.0, Computation time: 2.026465654373169\n",
      "Step: 781, Loss: 0.918368935585022, Accuracy: 1.0, Computation time: 2.190706491470337\n",
      "Step: 782, Loss: 0.9219747185707092, Accuracy: 1.0, Computation time: 2.3160641193389893\n",
      "Step: 783, Loss: 0.9160974025726318, Accuracy: 1.0, Computation time: 1.9023151397705078\n",
      "Step: 784, Loss: 0.9164759516716003, Accuracy: 1.0, Computation time: 2.1553006172180176\n",
      "Step: 785, Loss: 0.9364035129547119, Accuracy: 0.9807692766189575, Computation time: 2.13424015045166\n",
      "Step: 786, Loss: 0.9366743564605713, Accuracy: 0.9750000238418579, Computation time: 2.04022216796875\n",
      "Step: 787, Loss: 0.9162902235984802, Accuracy: 1.0, Computation time: 1.790863275527954\n",
      "Step: 788, Loss: 0.9376505613327026, Accuracy: 0.9722222089767456, Computation time: 2.070723056793213\n",
      "Step: 789, Loss: 0.916128396987915, Accuracy: 1.0, Computation time: 1.9796233177185059\n",
      "Step: 790, Loss: 0.9162080883979797, Accuracy: 1.0, Computation time: 1.7521297931671143\n",
      "Step: 791, Loss: 0.9251022338867188, Accuracy: 1.0, Computation time: 2.1630351543426514\n",
      "Step: 792, Loss: 0.9423276782035828, Accuracy: 0.9722222089767456, Computation time: 2.138629674911499\n",
      "Step: 793, Loss: 0.916279673576355, Accuracy: 1.0, Computation time: 1.9233627319335938\n",
      "Step: 794, Loss: 0.9192413687705994, Accuracy: 1.0, Computation time: 1.9573445320129395\n",
      "Step: 795, Loss: 0.9363799691200256, Accuracy: 0.9722222089767456, Computation time: 1.3742117881774902\n",
      "Step: 796, Loss: 0.9161636829376221, Accuracy: 1.0, Computation time: 1.9419505596160889\n",
      "Step: 797, Loss: 0.9255000948905945, Accuracy: 0.9722222089767456, Computation time: 2.4043567180633545\n",
      "Step: 798, Loss: 0.9315447807312012, Accuracy: 0.9833333492279053, Computation time: 1.8538379669189453\n",
      "Step: 799, Loss: 0.9163417816162109, Accuracy: 1.0, Computation time: 1.8463480472564697\n",
      "Step: 800, Loss: 0.9181599020957947, Accuracy: 1.0, Computation time: 2.3914906978607178\n",
      "Step: 801, Loss: 0.9162987470626831, Accuracy: 1.0, Computation time: 1.7242765426635742\n",
      "Step: 802, Loss: 0.920876681804657, Accuracy: 1.0, Computation time: 1.806122064590454\n",
      "Step: 803, Loss: 0.9388709664344788, Accuracy: 0.9722222089767456, Computation time: 1.9441659450531006\n",
      "Step: 804, Loss: 0.9165487885475159, Accuracy: 1.0, Computation time: 1.9336042404174805\n",
      "Step: 805, Loss: 0.9258471727371216, Accuracy: 1.0, Computation time: 1.7408583164215088\n",
      "Step: 806, Loss: 0.9234342575073242, Accuracy: 1.0, Computation time: 1.4328243732452393\n",
      "Step: 807, Loss: 0.936075747013092, Accuracy: 0.9583333730697632, Computation time: 2.380380392074585\n",
      "Step: 808, Loss: 0.9161497950553894, Accuracy: 1.0, Computation time: 1.7321395874023438\n",
      "Step: 809, Loss: 0.9164426326751709, Accuracy: 1.0, Computation time: 1.844597578048706\n",
      "Step: 810, Loss: 0.9166802167892456, Accuracy: 1.0, Computation time: 1.6206862926483154\n",
      "Step: 811, Loss: 0.9169634580612183, Accuracy: 1.0, Computation time: 1.3816542625427246\n",
      "Step: 812, Loss: 0.9178878664970398, Accuracy: 1.0, Computation time: 1.3950233459472656\n",
      "Step: 813, Loss: 0.9167420864105225, Accuracy: 1.0, Computation time: 1.4491243362426758\n",
      "Step: 814, Loss: 0.933717668056488, Accuracy: 0.984375, Computation time: 1.8574082851409912\n",
      "Step: 815, Loss: 0.9163995981216431, Accuracy: 1.0, Computation time: 1.627878189086914\n",
      "Step: 816, Loss: 0.9243716597557068, Accuracy: 1.0, Computation time: 1.7545173168182373\n",
      "Step: 817, Loss: 0.9167693853378296, Accuracy: 1.0, Computation time: 1.5280911922454834\n",
      "Step: 818, Loss: 0.9165585041046143, Accuracy: 1.0, Computation time: 1.8115506172180176\n",
      "Step: 819, Loss: 0.917109489440918, Accuracy: 1.0, Computation time: 1.7470347881317139\n",
      "Step: 820, Loss: 0.917410671710968, Accuracy: 1.0, Computation time: 1.7348194122314453\n",
      "Step: 821, Loss: 0.9168541431427002, Accuracy: 1.0, Computation time: 1.3973774909973145\n",
      "Step: 822, Loss: 0.9213711023330688, Accuracy: 1.0, Computation time: 1.8328402042388916\n",
      "Step: 823, Loss: 0.9167307615280151, Accuracy: 1.0, Computation time: 1.4903478622436523\n",
      "Step: 824, Loss: 0.9163089394569397, Accuracy: 1.0, Computation time: 1.6666226387023926\n",
      "Step: 825, Loss: 0.9161918759346008, Accuracy: 1.0, Computation time: 1.5921733379364014\n",
      "Step: 826, Loss: 0.9160254597663879, Accuracy: 1.0, Computation time: 1.6373419761657715\n",
      "Step: 827, Loss: 0.91611647605896, Accuracy: 1.0, Computation time: 1.86946439743042\n",
      "Step: 828, Loss: 0.9376001358032227, Accuracy: 0.9722222089767456, Computation time: 2.2033889293670654\n",
      "Step: 829, Loss: 0.9165143966674805, Accuracy: 1.0, Computation time: 1.80745530128479\n",
      "Step: 830, Loss: 0.9162615537643433, Accuracy: 1.0, Computation time: 1.976198434829712\n",
      "Step: 831, Loss: 0.9369316101074219, Accuracy: 0.9791666865348816, Computation time: 1.8629422187805176\n",
      "Step: 832, Loss: 0.922265887260437, Accuracy: 1.0, Computation time: 2.067966938018799\n",
      "Step: 833, Loss: 0.9163795709609985, Accuracy: 1.0, Computation time: 1.6102080345153809\n",
      "Step: 834, Loss: 0.9160363674163818, Accuracy: 1.0, Computation time: 1.6817679405212402\n",
      "########################\n",
      "Test loss: 1.1189875602722168, Test Accuracy_epoch6: 0.6997437477111816\n",
      "########################\n",
      "Step: 835, Loss: 0.9223560094833374, Accuracy: 1.0, Computation time: 1.857254981994629\n",
      "Step: 836, Loss: 0.9162098169326782, Accuracy: 1.0, Computation time: 1.6477842330932617\n",
      "Step: 837, Loss: 0.9381892085075378, Accuracy: 0.9791666865348816, Computation time: 2.1735072135925293\n",
      "Step: 838, Loss: 0.9163504242897034, Accuracy: 1.0, Computation time: 2.0099637508392334\n",
      "Step: 839, Loss: 0.9171497225761414, Accuracy: 1.0, Computation time: 2.3425424098968506\n",
      "Step: 840, Loss: 0.9162072539329529, Accuracy: 1.0, Computation time: 1.829533576965332\n",
      "Step: 841, Loss: 0.9162974953651428, Accuracy: 1.0, Computation time: 1.9759035110473633\n",
      "Step: 842, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 2.1090281009674072\n",
      "Step: 843, Loss: 0.9160613417625427, Accuracy: 1.0, Computation time: 1.988555908203125\n",
      "Step: 844, Loss: 0.9162042737007141, Accuracy: 1.0, Computation time: 2.2708730697631836\n",
      "Step: 845, Loss: 0.9341902732849121, Accuracy: 0.949999988079071, Computation time: 1.888599157333374\n",
      "Step: 846, Loss: 0.9226908087730408, Accuracy: 1.0, Computation time: 2.3388078212738037\n",
      "Step: 847, Loss: 0.9242352843284607, Accuracy: 1.0, Computation time: 2.489267587661743\n",
      "Step: 848, Loss: 0.9176112413406372, Accuracy: 1.0, Computation time: 2.5206003189086914\n",
      "Step: 849, Loss: 0.9185783863067627, Accuracy: 1.0, Computation time: 2.138798236846924\n",
      "Step: 850, Loss: 0.9178840517997742, Accuracy: 1.0, Computation time: 2.1992602348327637\n",
      "Step: 851, Loss: 0.9163414239883423, Accuracy: 1.0, Computation time: 2.002793073654175\n",
      "Step: 852, Loss: 0.9164085984230042, Accuracy: 1.0, Computation time: 2.1799516677856445\n",
      "Step: 853, Loss: 0.9161304831504822, Accuracy: 1.0, Computation time: 2.2619731426239014\n",
      "Step: 854, Loss: 0.9161906838417053, Accuracy: 1.0, Computation time: 2.164097547531128\n",
      "Step: 855, Loss: 0.9167014360427856, Accuracy: 1.0, Computation time: 2.687814235687256\n",
      "Step: 856, Loss: 0.9378663301467896, Accuracy: 0.9722222089767456, Computation time: 2.5939059257507324\n",
      "Step: 857, Loss: 0.9159217476844788, Accuracy: 1.0, Computation time: 2.992896556854248\n",
      "Step: 858, Loss: 0.9159509539604187, Accuracy: 1.0, Computation time: 2.223123550415039\n",
      "Step: 859, Loss: 0.9161645174026489, Accuracy: 1.0, Computation time: 2.5667121410369873\n",
      "Step: 860, Loss: 0.9161543846130371, Accuracy: 1.0, Computation time: 2.445932626724243\n",
      "Step: 861, Loss: 0.951979398727417, Accuracy: 0.949999988079071, Computation time: 3.5659098625183105\n",
      "Step: 862, Loss: 0.9160694479942322, Accuracy: 1.0, Computation time: 2.3154454231262207\n",
      "Step: 863, Loss: 0.9202028512954712, Accuracy: 1.0, Computation time: 2.5947177410125732\n",
      "Step: 864, Loss: 0.9308567643165588, Accuracy: 0.9642857313156128, Computation time: 2.498997449874878\n",
      "Step: 865, Loss: 0.9162730574607849, Accuracy: 1.0, Computation time: 1.9686272144317627\n",
      "Step: 866, Loss: 0.9162716269493103, Accuracy: 1.0, Computation time: 1.961369276046753\n",
      "Step: 867, Loss: 0.9162381291389465, Accuracy: 1.0, Computation time: 1.817962408065796\n",
      "Step: 868, Loss: 0.9180309772491455, Accuracy: 1.0, Computation time: 1.6083896160125732\n",
      "Step: 869, Loss: 0.9171034097671509, Accuracy: 1.0, Computation time: 2.0145649909973145\n",
      "Step: 870, Loss: 0.9162285923957825, Accuracy: 1.0, Computation time: 1.5779838562011719\n",
      "Step: 871, Loss: 0.9282432198524475, Accuracy: 0.9772727489471436, Computation time: 2.0132241249084473\n",
      "Step: 872, Loss: 0.9163522720336914, Accuracy: 1.0, Computation time: 2.2466001510620117\n",
      "Step: 873, Loss: 0.9160538911819458, Accuracy: 1.0, Computation time: 1.3638317584991455\n",
      "Step: 874, Loss: 0.9373483657836914, Accuracy: 0.96875, Computation time: 1.7190723419189453\n",
      "Step: 875, Loss: 0.9161651730537415, Accuracy: 1.0, Computation time: 1.6364352703094482\n",
      "Step: 876, Loss: 0.9185518026351929, Accuracy: 1.0, Computation time: 1.6345200538635254\n",
      "Step: 877, Loss: 0.916195273399353, Accuracy: 1.0, Computation time: 1.6114416122436523\n",
      "Step: 878, Loss: 0.9160856008529663, Accuracy: 1.0, Computation time: 2.4097535610198975\n",
      "Step: 879, Loss: 0.9162357449531555, Accuracy: 1.0, Computation time: 1.4337034225463867\n",
      "Step: 880, Loss: 0.9161142706871033, Accuracy: 1.0, Computation time: 1.7302014827728271\n",
      "Step: 881, Loss: 0.9159623980522156, Accuracy: 1.0, Computation time: 1.629140853881836\n",
      "Step: 882, Loss: 0.9162927269935608, Accuracy: 1.0, Computation time: 1.3472037315368652\n",
      "Step: 883, Loss: 0.9338537454605103, Accuracy: 0.9722222089767456, Computation time: 1.7301220893859863\n",
      "Step: 884, Loss: 0.9166668057441711, Accuracy: 1.0, Computation time: 1.870898962020874\n",
      "Step: 885, Loss: 0.917586088180542, Accuracy: 1.0, Computation time: 1.8214828968048096\n",
      "Step: 886, Loss: 0.9184620380401611, Accuracy: 1.0, Computation time: 1.6456270217895508\n",
      "Step: 887, Loss: 0.9176121354103088, Accuracy: 1.0, Computation time: 1.6863834857940674\n",
      "Step: 888, Loss: 0.9532495141029358, Accuracy: 0.9125000238418579, Computation time: 2.0013928413391113\n",
      "Step: 889, Loss: 0.918449878692627, Accuracy: 1.0, Computation time: 1.7181472778320312\n",
      "Step: 890, Loss: 0.9162276387214661, Accuracy: 1.0, Computation time: 1.7803921699523926\n",
      "Step: 891, Loss: 0.9164528250694275, Accuracy: 1.0, Computation time: 1.5265593528747559\n",
      "Step: 892, Loss: 0.9165199995040894, Accuracy: 1.0, Computation time: 1.7472779750823975\n",
      "Step: 893, Loss: 0.9167413115501404, Accuracy: 1.0, Computation time: 1.6920692920684814\n",
      "Step: 894, Loss: 0.9162862300872803, Accuracy: 1.0, Computation time: 1.6025991439819336\n",
      "Step: 895, Loss: 0.9378810524940491, Accuracy: 0.9791666865348816, Computation time: 1.7545232772827148\n",
      "Step: 896, Loss: 0.9160174131393433, Accuracy: 1.0, Computation time: 1.4659175872802734\n",
      "Step: 897, Loss: 0.9159693121910095, Accuracy: 1.0, Computation time: 1.55189847946167\n",
      "Step: 898, Loss: 0.9212846755981445, Accuracy: 1.0, Computation time: 1.5324430465698242\n",
      "Step: 899, Loss: 0.925801694393158, Accuracy: 0.9791666865348816, Computation time: 1.6198289394378662\n",
      "Step: 900, Loss: 0.9335567951202393, Accuracy: 0.9791666865348816, Computation time: 1.7948155403137207\n",
      "Step: 901, Loss: 0.9238871932029724, Accuracy: 1.0, Computation time: 2.0587425231933594\n",
      "Step: 902, Loss: 0.9266166090965271, Accuracy: 0.9642857313156128, Computation time: 1.4887893199920654\n",
      "Step: 903, Loss: 0.9164220094680786, Accuracy: 1.0, Computation time: 1.5485310554504395\n",
      "Step: 904, Loss: 0.9164702296257019, Accuracy: 1.0, Computation time: 2.0091257095336914\n",
      "Step: 905, Loss: 0.9190420508384705, Accuracy: 1.0, Computation time: 1.5411460399627686\n",
      "Step: 906, Loss: 0.9195900559425354, Accuracy: 1.0, Computation time: 1.7348394393920898\n",
      "Step: 907, Loss: 0.9166507720947266, Accuracy: 1.0, Computation time: 2.085754156112671\n",
      "Step: 908, Loss: 0.9166296720504761, Accuracy: 1.0, Computation time: 2.114210844039917\n",
      "Step: 909, Loss: 0.9383580088615417, Accuracy: 0.9750000238418579, Computation time: 2.1413230895996094\n",
      "Step: 910, Loss: 0.957220733165741, Accuracy: 0.9365079402923584, Computation time: 1.7289230823516846\n",
      "Step: 911, Loss: 0.9545844197273254, Accuracy: 0.9464285969734192, Computation time: 1.9035015106201172\n",
      "Step: 912, Loss: 0.9289029836654663, Accuracy: 0.9791666865348816, Computation time: 2.3001761436462402\n",
      "Step: 913, Loss: 0.9315819144248962, Accuracy: 0.9807692766189575, Computation time: 2.175084352493286\n",
      "Step: 914, Loss: 0.9389612674713135, Accuracy: 0.9772727489471436, Computation time: 2.174595594406128\n",
      "Step: 915, Loss: 0.9356784820556641, Accuracy: 0.984375, Computation time: 1.879889965057373\n",
      "Step: 916, Loss: 0.9392731189727783, Accuracy: 0.9750000238418579, Computation time: 2.5929415225982666\n",
      "Step: 917, Loss: 0.9181812405586243, Accuracy: 1.0, Computation time: 2.0356674194335938\n",
      "Step: 918, Loss: 0.9167270660400391, Accuracy: 1.0, Computation time: 2.076125144958496\n",
      "Step: 919, Loss: 0.9330963492393494, Accuracy: 0.949999988079071, Computation time: 1.7578222751617432\n",
      "Step: 920, Loss: 0.9376705884933472, Accuracy: 0.9642857313156128, Computation time: 1.4328796863555908\n",
      "Step: 921, Loss: 0.9279146790504456, Accuracy: 0.9807692766189575, Computation time: 1.8693180084228516\n",
      "Step: 922, Loss: 0.9270204305648804, Accuracy: 0.9642857313156128, Computation time: 1.901319980621338\n",
      "Step: 923, Loss: 0.9167622327804565, Accuracy: 1.0, Computation time: 1.7440645694732666\n",
      "Step: 924, Loss: 0.9422919750213623, Accuracy: 0.9791666865348816, Computation time: 1.6628766059875488\n",
      "Step: 925, Loss: 0.9173508882522583, Accuracy: 1.0, Computation time: 1.566617727279663\n",
      "Step: 926, Loss: 0.9384273290634155, Accuracy: 0.9750000238418579, Computation time: 1.628312110900879\n",
      "Step: 927, Loss: 0.9175998568534851, Accuracy: 1.0, Computation time: 1.5945816040039062\n",
      "Step: 928, Loss: 0.917104184627533, Accuracy: 1.0, Computation time: 1.742647647857666\n",
      "Step: 929, Loss: 0.9169412851333618, Accuracy: 1.0, Computation time: 2.04555344581604\n",
      "Step: 930, Loss: 0.9171522259712219, Accuracy: 1.0, Computation time: 1.7394475936889648\n",
      "Step: 931, Loss: 0.9162066578865051, Accuracy: 1.0, Computation time: 1.950953722000122\n",
      "Step: 932, Loss: 0.9165981411933899, Accuracy: 1.0, Computation time: 1.652951955795288\n",
      "Step: 933, Loss: 0.9181357026100159, Accuracy: 1.0, Computation time: 1.8122389316558838\n",
      "Step: 934, Loss: 0.931316077709198, Accuracy: 0.9791666865348816, Computation time: 2.170832633972168\n",
      "Step: 935, Loss: 0.9165759086608887, Accuracy: 1.0, Computation time: 1.8114118576049805\n",
      "Step: 936, Loss: 0.9178776144981384, Accuracy: 1.0, Computation time: 2.1319782733917236\n",
      "Step: 937, Loss: 0.938274621963501, Accuracy: 0.949999988079071, Computation time: 2.094517469406128\n",
      "Step: 938, Loss: 0.9175623655319214, Accuracy: 1.0, Computation time: 1.8545827865600586\n",
      "Step: 939, Loss: 0.9160901308059692, Accuracy: 1.0, Computation time: 1.9911539554595947\n",
      "Step: 940, Loss: 0.938125729560852, Accuracy: 0.9821428656578064, Computation time: 1.965646743774414\n",
      "Step: 941, Loss: 0.9161375164985657, Accuracy: 1.0, Computation time: 1.8366334438323975\n",
      "Step: 942, Loss: 0.9192888140678406, Accuracy: 1.0, Computation time: 1.9661643505096436\n",
      "Step: 943, Loss: 0.9209345579147339, Accuracy: 1.0, Computation time: 1.6995344161987305\n",
      "Step: 944, Loss: 0.9163496494293213, Accuracy: 1.0, Computation time: 1.6964104175567627\n",
      "Step: 945, Loss: 0.9162473678588867, Accuracy: 1.0, Computation time: 1.7095212936401367\n",
      "Step: 946, Loss: 0.9166530966758728, Accuracy: 1.0, Computation time: 2.0898385047912598\n",
      "Step: 947, Loss: 0.9347200393676758, Accuracy: 0.9772727489471436, Computation time: 1.7387917041778564\n",
      "Step: 948, Loss: 0.9375425577163696, Accuracy: 0.949999988079071, Computation time: 1.8281700611114502\n",
      "Step: 949, Loss: 0.9596525430679321, Accuracy: 0.9583333730697632, Computation time: 1.6231305599212646\n",
      "Step: 950, Loss: 0.939094066619873, Accuracy: 0.9791666865348816, Computation time: 1.8510406017303467\n",
      "Step: 951, Loss: 0.9160870313644409, Accuracy: 1.0, Computation time: 1.640709638595581\n",
      "Step: 952, Loss: 0.9163472056388855, Accuracy: 1.0, Computation time: 1.6981585025787354\n",
      "Step: 953, Loss: 0.9161583185195923, Accuracy: 1.0, Computation time: 1.4469082355499268\n",
      "Step: 954, Loss: 0.9443878531455994, Accuracy: 0.9333333969116211, Computation time: 1.724134922027588\n",
      "Step: 955, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 2.100234031677246\n",
      "Step: 956, Loss: 0.9167677164077759, Accuracy: 1.0, Computation time: 1.7880074977874756\n",
      "Step: 957, Loss: 0.9164387583732605, Accuracy: 1.0, Computation time: 1.7792999744415283\n",
      "Step: 958, Loss: 0.9161419868469238, Accuracy: 1.0, Computation time: 1.8163502216339111\n",
      "Step: 959, Loss: 0.9232850670814514, Accuracy: 1.0, Computation time: 2.232884168624878\n",
      "Step: 960, Loss: 0.9379968643188477, Accuracy: 0.9750000238418579, Computation time: 1.481025218963623\n",
      "Step: 961, Loss: 0.9162006378173828, Accuracy: 1.0, Computation time: 1.8419978618621826\n",
      "Step: 962, Loss: 0.9163184762001038, Accuracy: 1.0, Computation time: 1.8480372428894043\n",
      "Step: 963, Loss: 0.9160935878753662, Accuracy: 1.0, Computation time: 2.094114303588867\n",
      "Step: 964, Loss: 0.938116729259491, Accuracy: 0.9750000238418579, Computation time: 1.667867660522461\n",
      "Step: 965, Loss: 0.9161224365234375, Accuracy: 1.0, Computation time: 1.7201831340789795\n",
      "Step: 966, Loss: 0.9177049994468689, Accuracy: 1.0, Computation time: 2.116684675216675\n",
      "Step: 967, Loss: 0.9160082340240479, Accuracy: 1.0, Computation time: 1.4179363250732422\n",
      "Step: 968, Loss: 0.9288432598114014, Accuracy: 0.9821428656578064, Computation time: 2.1540350914001465\n",
      "Step: 969, Loss: 0.9161664247512817, Accuracy: 1.0, Computation time: 2.0018672943115234\n",
      "Step: 970, Loss: 0.9185618758201599, Accuracy: 1.0, Computation time: 2.027118682861328\n",
      "Step: 971, Loss: 0.9160256385803223, Accuracy: 1.0, Computation time: 1.657731056213379\n",
      "Step: 972, Loss: 0.9164310097694397, Accuracy: 1.0, Computation time: 1.4746038913726807\n",
      "Step: 973, Loss: 0.9371187090873718, Accuracy: 0.9772727489471436, Computation time: 1.6239848136901855\n",
      "########################\n",
      "Test loss: 1.1288139820098877, Test Accuracy_epoch7: 0.6879169940948486\n",
      "########################\n",
      "Step: 974, Loss: 0.9163088202476501, Accuracy: 1.0, Computation time: 1.580498456954956\n",
      "Step: 975, Loss: 0.9576564431190491, Accuracy: 0.949999988079071, Computation time: 1.9148337841033936\n",
      "Step: 976, Loss: 0.9165913462638855, Accuracy: 1.0, Computation time: 1.4565298557281494\n",
      "Step: 977, Loss: 0.9363088607788086, Accuracy: 0.949999988079071, Computation time: 1.5185272693634033\n",
      "Step: 978, Loss: 0.9377304315567017, Accuracy: 0.9642857313156128, Computation time: 1.5980157852172852\n",
      "Step: 979, Loss: 0.9366573095321655, Accuracy: 0.9750000238418579, Computation time: 1.8445312976837158\n",
      "Step: 980, Loss: 0.9160659313201904, Accuracy: 1.0, Computation time: 1.6746704578399658\n",
      "Step: 981, Loss: 0.9376217722892761, Accuracy: 0.9583333730697632, Computation time: 1.7623295783996582\n",
      "Step: 982, Loss: 0.9712743759155273, Accuracy: 0.8772727251052856, Computation time: 2.041440010070801\n",
      "Step: 983, Loss: 0.9161297678947449, Accuracy: 1.0, Computation time: 2.0000274181365967\n",
      "Step: 984, Loss: 0.9162112474441528, Accuracy: 1.0, Computation time: 1.6293575763702393\n",
      "Step: 985, Loss: 0.9161216020584106, Accuracy: 1.0, Computation time: 2.1553549766540527\n",
      "Step: 986, Loss: 0.9160535335540771, Accuracy: 1.0, Computation time: 1.901061773300171\n",
      "Step: 987, Loss: 0.9161269664764404, Accuracy: 1.0, Computation time: 1.6829137802124023\n",
      "Step: 988, Loss: 0.9342379570007324, Accuracy: 0.96875, Computation time: 2.5865986347198486\n",
      "Step: 989, Loss: 0.9160944223403931, Accuracy: 1.0, Computation time: 2.174582004547119\n",
      "Step: 990, Loss: 0.9159587621688843, Accuracy: 1.0, Computation time: 1.7957172393798828\n",
      "Step: 991, Loss: 0.9184567928314209, Accuracy: 1.0, Computation time: 1.6854660511016846\n",
      "Step: 992, Loss: 0.9159298539161682, Accuracy: 1.0, Computation time: 1.7841956615447998\n",
      "Step: 993, Loss: 0.916264533996582, Accuracy: 1.0, Computation time: 1.8330280780792236\n",
      "Step: 994, Loss: 0.9160976409912109, Accuracy: 1.0, Computation time: 2.395301103591919\n",
      "Step: 995, Loss: 0.916018009185791, Accuracy: 1.0, Computation time: 2.0959668159484863\n",
      "Step: 996, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.8193695545196533\n",
      "Step: 997, Loss: 0.9193047881126404, Accuracy: 1.0, Computation time: 1.758819580078125\n",
      "Step: 998, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 2.2147490978240967\n",
      "Step: 999, Loss: 0.9184846878051758, Accuracy: 1.0, Computation time: 1.838754415512085\n",
      "Step: 1000, Loss: 0.9162801504135132, Accuracy: 1.0, Computation time: 2.272311210632324\n",
      "Step: 1001, Loss: 0.9163069725036621, Accuracy: 1.0, Computation time: 2.3417720794677734\n",
      "Step: 1002, Loss: 0.917044460773468, Accuracy: 1.0, Computation time: 2.0594868659973145\n",
      "Step: 1003, Loss: 0.9160871505737305, Accuracy: 1.0, Computation time: 1.58347749710083\n",
      "Step: 1004, Loss: 0.9166984558105469, Accuracy: 1.0, Computation time: 2.224489212036133\n",
      "Step: 1005, Loss: 0.9375857710838318, Accuracy: 0.9583333730697632, Computation time: 2.0379891395568848\n",
      "Step: 1006, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 1.9052410125732422\n",
      "Step: 1007, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 2.288431167602539\n",
      "Step: 1008, Loss: 0.9159930944442749, Accuracy: 1.0, Computation time: 2.3330633640289307\n",
      "Step: 1009, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.9492530822753906\n",
      "Step: 1010, Loss: 0.9375003576278687, Accuracy: 0.9583333730697632, Computation time: 1.9488129615783691\n",
      "Step: 1011, Loss: 0.9285030364990234, Accuracy: 0.9791666865348816, Computation time: 2.1658358573913574\n",
      "Step: 1012, Loss: 0.9160767197608948, Accuracy: 1.0, Computation time: 2.2241151332855225\n",
      "Step: 1013, Loss: 0.916246235370636, Accuracy: 1.0, Computation time: 1.7215697765350342\n",
      "Step: 1014, Loss: 0.916462242603302, Accuracy: 1.0, Computation time: 2.088364362716675\n",
      "Step: 1015, Loss: 0.9384393692016602, Accuracy: 0.9772727489471436, Computation time: 1.9195506572723389\n",
      "Step: 1016, Loss: 0.9268240332603455, Accuracy: 0.9583333730697632, Computation time: 2.2739768028259277\n",
      "Step: 1017, Loss: 0.9162889122962952, Accuracy: 1.0, Computation time: 2.2108514308929443\n",
      "Step: 1018, Loss: 0.9165555834770203, Accuracy: 1.0, Computation time: 1.7987983226776123\n",
      "Step: 1019, Loss: 0.916379451751709, Accuracy: 1.0, Computation time: 2.175663709640503\n",
      "Step: 1020, Loss: 0.9163658022880554, Accuracy: 1.0, Computation time: 2.3941287994384766\n",
      "Step: 1021, Loss: 0.9163911938667297, Accuracy: 1.0, Computation time: 2.312929630279541\n",
      "Step: 1022, Loss: 0.9162099957466125, Accuracy: 1.0, Computation time: 2.3460073471069336\n",
      "Step: 1023, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 2.063633680343628\n",
      "Step: 1024, Loss: 0.916025698184967, Accuracy: 1.0, Computation time: 1.8044967651367188\n",
      "Step: 1025, Loss: 0.9163874387741089, Accuracy: 1.0, Computation time: 2.1294119358062744\n",
      "Step: 1026, Loss: 0.9392993450164795, Accuracy: 0.9772727489471436, Computation time: 2.266836643218994\n",
      "Step: 1027, Loss: 0.9221606254577637, Accuracy: 1.0, Computation time: 2.1946208477020264\n",
      "Step: 1028, Loss: 0.9163708686828613, Accuracy: 1.0, Computation time: 2.006559371948242\n",
      "Step: 1029, Loss: 0.9166014790534973, Accuracy: 1.0, Computation time: 1.877664566040039\n",
      "Step: 1030, Loss: 0.9166268110275269, Accuracy: 1.0, Computation time: 1.9279711246490479\n",
      "Step: 1031, Loss: 0.9575443863868713, Accuracy: 0.9375, Computation time: 1.9769785404205322\n",
      "Step: 1032, Loss: 0.9164473414421082, Accuracy: 1.0, Computation time: 2.230283737182617\n",
      "Step: 1033, Loss: 0.9254553318023682, Accuracy: 1.0, Computation time: 1.8834056854248047\n",
      "Step: 1034, Loss: 0.916323721408844, Accuracy: 1.0, Computation time: 1.8861033916473389\n",
      "Step: 1035, Loss: 0.916206955909729, Accuracy: 1.0, Computation time: 1.6513142585754395\n",
      "Step: 1036, Loss: 0.9163894653320312, Accuracy: 1.0, Computation time: 1.6245155334472656\n",
      "Step: 1037, Loss: 0.9162679314613342, Accuracy: 1.0, Computation time: 1.7307555675506592\n",
      "Step: 1038, Loss: 0.9164827466011047, Accuracy: 1.0, Computation time: 2.029895782470703\n",
      "Step: 1039, Loss: 0.9371228218078613, Accuracy: 0.9791666865348816, Computation time: 1.5464348793029785\n",
      "Step: 1040, Loss: 0.9160591959953308, Accuracy: 1.0, Computation time: 1.3877131938934326\n",
      "Step: 1041, Loss: 0.9161210060119629, Accuracy: 1.0, Computation time: 2.184177875518799\n",
      "Step: 1042, Loss: 0.9211795330047607, Accuracy: 1.0, Computation time: 1.8060503005981445\n",
      "Step: 1043, Loss: 0.916049599647522, Accuracy: 1.0, Computation time: 2.055673599243164\n",
      "Step: 1044, Loss: 0.9169806838035583, Accuracy: 1.0, Computation time: 1.6396527290344238\n",
      "Step: 1045, Loss: 0.9252654314041138, Accuracy: 1.0, Computation time: 1.8977563381195068\n",
      "Step: 1046, Loss: 0.9175280928611755, Accuracy: 1.0, Computation time: 1.5953500270843506\n",
      "Step: 1047, Loss: 0.9162445664405823, Accuracy: 1.0, Computation time: 1.5301566123962402\n",
      "Step: 1048, Loss: 0.9161664247512817, Accuracy: 1.0, Computation time: 1.5298657417297363\n",
      "Step: 1049, Loss: 0.9173102378845215, Accuracy: 1.0, Computation time: 1.7042450904846191\n",
      "Step: 1050, Loss: 0.9159820079803467, Accuracy: 1.0, Computation time: 1.5997002124786377\n",
      "Step: 1051, Loss: 0.9159795045852661, Accuracy: 1.0, Computation time: 1.6304936408996582\n",
      "Step: 1052, Loss: 0.9597307443618774, Accuracy: 0.9642857313156128, Computation time: 1.871567726135254\n",
      "Step: 1053, Loss: 0.9377146363258362, Accuracy: 0.984375, Computation time: 1.688223123550415\n",
      "Step: 1054, Loss: 0.9292682409286499, Accuracy: 0.984375, Computation time: 1.729175090789795\n",
      "Step: 1055, Loss: 0.9161204099655151, Accuracy: 1.0, Computation time: 1.4622571468353271\n",
      "Step: 1056, Loss: 0.9163767695426941, Accuracy: 1.0, Computation time: 1.7792117595672607\n",
      "Step: 1057, Loss: 0.9384199976921082, Accuracy: 0.9722222089767456, Computation time: 1.4418411254882812\n",
      "Step: 1058, Loss: 0.9162887334823608, Accuracy: 1.0, Computation time: 1.4812123775482178\n",
      "Step: 1059, Loss: 0.9358595013618469, Accuracy: 0.9583333730697632, Computation time: 1.6972534656524658\n",
      "Step: 1060, Loss: 0.9161160588264465, Accuracy: 1.0, Computation time: 1.9898183345794678\n",
      "Step: 1061, Loss: 0.9310441017150879, Accuracy: 0.9772727489471436, Computation time: 1.5578985214233398\n",
      "Step: 1062, Loss: 0.9184216260910034, Accuracy: 1.0, Computation time: 1.4160747528076172\n",
      "Step: 1063, Loss: 0.9161372780799866, Accuracy: 1.0, Computation time: 1.7612829208374023\n",
      "Step: 1064, Loss: 0.9159839153289795, Accuracy: 1.0, Computation time: 1.5515143871307373\n",
      "Step: 1065, Loss: 0.9373863339424133, Accuracy: 0.9807692766189575, Computation time: 1.387451410293579\n",
      "Step: 1066, Loss: 0.9164079427719116, Accuracy: 1.0, Computation time: 1.508021354675293\n",
      "Step: 1067, Loss: 0.9313794374465942, Accuracy: 0.9750000238418579, Computation time: 1.5484449863433838\n",
      "Step: 1068, Loss: 0.9164049029350281, Accuracy: 1.0, Computation time: 1.52586030960083\n",
      "Step: 1069, Loss: 0.9161252379417419, Accuracy: 1.0, Computation time: 1.3077561855316162\n",
      "Step: 1070, Loss: 0.9328733682632446, Accuracy: 0.9642857313156128, Computation time: 1.5793030261993408\n",
      "Step: 1071, Loss: 0.9373239278793335, Accuracy: 0.9166666865348816, Computation time: 1.4848179817199707\n",
      "Step: 1072, Loss: 0.916361927986145, Accuracy: 1.0, Computation time: 1.4734015464782715\n",
      "Step: 1073, Loss: 0.9161651134490967, Accuracy: 1.0, Computation time: 1.2744550704956055\n",
      "Step: 1074, Loss: 0.9163147807121277, Accuracy: 1.0, Computation time: 1.6020855903625488\n",
      "Step: 1075, Loss: 0.917816162109375, Accuracy: 1.0, Computation time: 1.6941585540771484\n",
      "Step: 1076, Loss: 0.948173463344574, Accuracy: 0.9522727727890015, Computation time: 1.6115260124206543\n",
      "Step: 1077, Loss: 0.9161224365234375, Accuracy: 1.0, Computation time: 1.4559881687164307\n",
      "Step: 1078, Loss: 0.9161449670791626, Accuracy: 1.0, Computation time: 1.9012646675109863\n",
      "Step: 1079, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 1.5027656555175781\n",
      "Step: 1080, Loss: 0.916170597076416, Accuracy: 1.0, Computation time: 1.8351056575775146\n",
      "Step: 1081, Loss: 0.916526734828949, Accuracy: 1.0, Computation time: 2.1358001232147217\n",
      "Step: 1082, Loss: 0.9167006015777588, Accuracy: 1.0, Computation time: 1.6100044250488281\n",
      "Step: 1083, Loss: 0.9372172355651855, Accuracy: 0.9772727489471436, Computation time: 1.7303667068481445\n",
      "Step: 1084, Loss: 0.9161950349807739, Accuracy: 1.0, Computation time: 1.5364937782287598\n",
      "Step: 1085, Loss: 0.9161821603775024, Accuracy: 1.0, Computation time: 1.8731188774108887\n",
      "Step: 1086, Loss: 0.9160493612289429, Accuracy: 1.0, Computation time: 1.823441743850708\n",
      "Step: 1087, Loss: 0.9159511923789978, Accuracy: 1.0, Computation time: 1.758826494216919\n",
      "Step: 1088, Loss: 0.9163276553153992, Accuracy: 1.0, Computation time: 1.7673261165618896\n",
      "Step: 1089, Loss: 0.9159104228019714, Accuracy: 1.0, Computation time: 1.8010742664337158\n",
      "Step: 1090, Loss: 0.9161900281906128, Accuracy: 1.0, Computation time: 1.9748609066009521\n",
      "Step: 1091, Loss: 0.9160127639770508, Accuracy: 1.0, Computation time: 1.639725923538208\n",
      "Step: 1092, Loss: 0.9355964660644531, Accuracy: 0.9791666865348816, Computation time: 2.419538736343384\n",
      "Step: 1093, Loss: 0.9161906838417053, Accuracy: 1.0, Computation time: 1.8504557609558105\n",
      "Step: 1094, Loss: 0.9160701632499695, Accuracy: 1.0, Computation time: 1.936917781829834\n",
      "Step: 1095, Loss: 0.9159740805625916, Accuracy: 1.0, Computation time: 1.911271095275879\n",
      "Step: 1096, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.9482009410858154\n",
      "Step: 1097, Loss: 0.9161339402198792, Accuracy: 1.0, Computation time: 1.975687026977539\n",
      "Step: 1098, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 2.192441701889038\n",
      "Step: 1099, Loss: 0.928430438041687, Accuracy: 0.9722222089767456, Computation time: 1.9101917743682861\n",
      "Step: 1100, Loss: 0.943699300289154, Accuracy: 0.949999988079071, Computation time: 2.268203020095825\n",
      "Step: 1101, Loss: 0.9379682540893555, Accuracy: 0.9750000238418579, Computation time: 1.9668610095977783\n",
      "Step: 1102, Loss: 0.9407422542572021, Accuracy: 0.9750000238418579, Computation time: 2.4821853637695312\n",
      "Step: 1103, Loss: 0.9297133684158325, Accuracy: 0.9852941036224365, Computation time: 1.7082531452178955\n",
      "Step: 1104, Loss: 0.9375735521316528, Accuracy: 0.9833333492279053, Computation time: 2.191828966140747\n",
      "Step: 1105, Loss: 0.9164448976516724, Accuracy: 1.0, Computation time: 1.7211501598358154\n",
      "Step: 1106, Loss: 0.9159612059593201, Accuracy: 1.0, Computation time: 2.059357166290283\n",
      "Step: 1107, Loss: 0.9193335175514221, Accuracy: 1.0, Computation time: 1.9064695835113525\n",
      "Step: 1108, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.864866018295288\n",
      "Step: 1109, Loss: 0.9160835146903992, Accuracy: 1.0, Computation time: 1.9390270709991455\n",
      "Step: 1110, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 1.7171893119812012\n",
      "Step: 1111, Loss: 0.9160836935043335, Accuracy: 1.0, Computation time: 1.7706916332244873\n",
      "Step: 1112, Loss: 0.9376479387283325, Accuracy: 0.9166666865348816, Computation time: 1.5755677223205566\n",
      "########################\n",
      "Test loss: 1.1224244832992554, Test Accuracy_epoch8: 0.6976180076599121\n",
      "########################\n",
      "Step: 1113, Loss: 0.9383065700531006, Accuracy: 0.9791666865348816, Computation time: 1.5372929573059082\n",
      "Step: 1114, Loss: 0.9159914255142212, Accuracy: 1.0, Computation time: 1.7128479480743408\n",
      "Step: 1115, Loss: 0.9357241988182068, Accuracy: 0.9375, Computation time: 1.7247998714447021\n",
      "Step: 1116, Loss: 0.9283959269523621, Accuracy: 0.9791666865348816, Computation time: 2.0159242153167725\n",
      "Step: 1117, Loss: 0.9164837598800659, Accuracy: 1.0, Computation time: 1.5757331848144531\n",
      "Step: 1118, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 2.147239923477173\n",
      "Step: 1119, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.9432945251464844\n",
      "Step: 1120, Loss: 0.9159516096115112, Accuracy: 1.0, Computation time: 2.0407450199127197\n",
      "Step: 1121, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.6225900650024414\n",
      "Step: 1122, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.4582886695861816\n",
      "Step: 1123, Loss: 0.9161518216133118, Accuracy: 1.0, Computation time: 1.9179892539978027\n",
      "Step: 1124, Loss: 0.9162383675575256, Accuracy: 1.0, Computation time: 2.053818702697754\n",
      "Step: 1125, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 1.5124895572662354\n",
      "Step: 1126, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 1.6993601322174072\n",
      "Step: 1127, Loss: 0.9159941673278809, Accuracy: 1.0, Computation time: 1.7298121452331543\n",
      "Step: 1128, Loss: 0.9166929125785828, Accuracy: 1.0, Computation time: 1.3962507247924805\n",
      "Step: 1129, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.3171937465667725\n",
      "Step: 1130, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 1.7532424926757812\n",
      "Step: 1131, Loss: 0.9378238320350647, Accuracy: 0.9583333730697632, Computation time: 1.6221063137054443\n",
      "Step: 1132, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.3777382373809814\n",
      "Step: 1133, Loss: 0.918793797492981, Accuracy: 1.0, Computation time: 1.6062602996826172\n",
      "Step: 1134, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 1.4334590435028076\n",
      "Step: 1135, Loss: 0.9160206913948059, Accuracy: 1.0, Computation time: 1.4498980045318604\n",
      "Step: 1136, Loss: 0.9164606332778931, Accuracy: 1.0, Computation time: 1.5669374465942383\n",
      "Step: 1137, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.5384712219238281\n",
      "Step: 1138, Loss: 0.9163848161697388, Accuracy: 1.0, Computation time: 1.6356620788574219\n",
      "Step: 1139, Loss: 0.9377158284187317, Accuracy: 0.949999988079071, Computation time: 1.5480618476867676\n",
      "Step: 1140, Loss: 0.9165140986442566, Accuracy: 1.0, Computation time: 1.5872178077697754\n",
      "Step: 1141, Loss: 0.9170383810997009, Accuracy: 1.0, Computation time: 1.4913876056671143\n",
      "Step: 1142, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.353851556777954\n",
      "Step: 1143, Loss: 0.9161633253097534, Accuracy: 1.0, Computation time: 1.506500482559204\n",
      "Step: 1144, Loss: 0.9161628484725952, Accuracy: 1.0, Computation time: 1.569713830947876\n",
      "Step: 1145, Loss: 0.9188502430915833, Accuracy: 1.0, Computation time: 1.5954222679138184\n",
      "Step: 1146, Loss: 0.9159467220306396, Accuracy: 1.0, Computation time: 1.707763671875\n",
      "Step: 1147, Loss: 0.9159668684005737, Accuracy: 1.0, Computation time: 1.760488510131836\n",
      "Step: 1148, Loss: 0.9163500070571899, Accuracy: 1.0, Computation time: 1.6575508117675781\n",
      "Step: 1149, Loss: 0.9168850779533386, Accuracy: 1.0, Computation time: 1.5481979846954346\n",
      "Step: 1150, Loss: 0.9162118434906006, Accuracy: 1.0, Computation time: 1.426424503326416\n",
      "Step: 1151, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.4667410850524902\n",
      "Step: 1152, Loss: 0.9452088475227356, Accuracy: 0.9642857313156128, Computation time: 1.848677158355713\n",
      "Step: 1153, Loss: 0.9182471632957458, Accuracy: 1.0, Computation time: 1.5254254341125488\n",
      "Step: 1154, Loss: 0.916078507900238, Accuracy: 1.0, Computation time: 1.8982713222503662\n",
      "Step: 1155, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.6766536235809326\n",
      "Step: 1156, Loss: 0.9166929125785828, Accuracy: 1.0, Computation time: 1.8664624691009521\n",
      "Step: 1157, Loss: 0.9163578748703003, Accuracy: 1.0, Computation time: 1.4883229732513428\n",
      "Step: 1158, Loss: 0.916533350944519, Accuracy: 1.0, Computation time: 1.5888862609863281\n",
      "Step: 1159, Loss: 0.9161086678504944, Accuracy: 1.0, Computation time: 1.6613092422485352\n",
      "Step: 1160, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 1.65065336227417\n",
      "Step: 1161, Loss: 0.9160454273223877, Accuracy: 1.0, Computation time: 1.6523315906524658\n",
      "Step: 1162, Loss: 0.9160101413726807, Accuracy: 1.0, Computation time: 1.7609589099884033\n",
      "Step: 1163, Loss: 0.9368413090705872, Accuracy: 0.9791666865348816, Computation time: 1.7350468635559082\n",
      "Step: 1164, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 2.1441714763641357\n",
      "Step: 1165, Loss: 0.9375696182250977, Accuracy: 0.96875, Computation time: 1.6912808418273926\n",
      "Step: 1166, Loss: 0.9161219000816345, Accuracy: 1.0, Computation time: 1.6806862354278564\n",
      "Step: 1167, Loss: 0.9159539937973022, Accuracy: 1.0, Computation time: 1.4043316841125488\n",
      "Step: 1168, Loss: 0.9342512488365173, Accuracy: 0.9750000238418579, Computation time: 2.1037917137145996\n",
      "Step: 1169, Loss: 0.9345069527626038, Accuracy: 0.9583333730697632, Computation time: 1.8906617164611816\n",
      "Step: 1170, Loss: 0.9160019159317017, Accuracy: 1.0, Computation time: 1.841569423675537\n",
      "Step: 1171, Loss: 0.9225735068321228, Accuracy: 1.0, Computation time: 1.6925115585327148\n",
      "Step: 1172, Loss: 0.9159925580024719, Accuracy: 1.0, Computation time: 1.6624798774719238\n",
      "Step: 1173, Loss: 0.9164184331893921, Accuracy: 1.0, Computation time: 1.7287406921386719\n",
      "Step: 1174, Loss: 0.9179379940032959, Accuracy: 1.0, Computation time: 1.6181433200836182\n",
      "Step: 1175, Loss: 0.9166713953018188, Accuracy: 1.0, Computation time: 1.5575194358825684\n",
      "Step: 1176, Loss: 0.9383506178855896, Accuracy: 0.9807692766189575, Computation time: 2.0812418460845947\n",
      "Step: 1177, Loss: 0.916174590587616, Accuracy: 1.0, Computation time: 1.7966601848602295\n",
      "Step: 1178, Loss: 0.9161167740821838, Accuracy: 1.0, Computation time: 1.8985295295715332\n",
      "Step: 1179, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.70176362991333\n",
      "Step: 1180, Loss: 0.9354387521743774, Accuracy: 0.9722222089767456, Computation time: 1.9478936195373535\n",
      "Step: 1181, Loss: 0.9356752038002014, Accuracy: 0.9807692766189575, Computation time: 2.3711094856262207\n",
      "Step: 1182, Loss: 0.9163823127746582, Accuracy: 1.0, Computation time: 2.1080737113952637\n",
      "Step: 1183, Loss: 0.9161615967750549, Accuracy: 1.0, Computation time: 2.226128101348877\n",
      "Step: 1184, Loss: 0.9160433411598206, Accuracy: 1.0, Computation time: 2.2411458492279053\n",
      "Step: 1185, Loss: 0.9377800226211548, Accuracy: 0.9583333730697632, Computation time: 2.0282607078552246\n",
      "Step: 1186, Loss: 0.9159680008888245, Accuracy: 1.0, Computation time: 1.6785626411437988\n",
      "Step: 1187, Loss: 0.916840672492981, Accuracy: 1.0, Computation time: 1.7012813091278076\n",
      "Step: 1188, Loss: 0.9159787893295288, Accuracy: 1.0, Computation time: 1.4782662391662598\n",
      "Step: 1189, Loss: 0.9381241202354431, Accuracy: 0.9821428656578064, Computation time: 2.00925350189209\n",
      "Step: 1190, Loss: 0.9236217141151428, Accuracy: 1.0, Computation time: 1.9784753322601318\n",
      "Step: 1191, Loss: 0.9198537468910217, Accuracy: 1.0, Computation time: 1.8250198364257812\n",
      "Step: 1192, Loss: 0.918867290019989, Accuracy: 1.0, Computation time: 1.7696220874786377\n",
      "Step: 1193, Loss: 0.9367939233779907, Accuracy: 0.9722222089767456, Computation time: 2.2573792934417725\n",
      "Step: 1194, Loss: 0.9248201251029968, Accuracy: 1.0, Computation time: 1.8455851078033447\n",
      "Step: 1195, Loss: 0.9167547225952148, Accuracy: 1.0, Computation time: 1.7191622257232666\n",
      "Step: 1196, Loss: 0.9381416440010071, Accuracy: 0.9772727489471436, Computation time: 1.8246617317199707\n",
      "Step: 1197, Loss: 0.9164303541183472, Accuracy: 1.0, Computation time: 1.431532382965088\n",
      "Step: 1198, Loss: 0.9163113236427307, Accuracy: 1.0, Computation time: 1.7293598651885986\n",
      "Step: 1199, Loss: 0.9160729646682739, Accuracy: 1.0, Computation time: 1.954329013824463\n",
      "Step: 1200, Loss: 0.9159797430038452, Accuracy: 1.0, Computation time: 1.4294931888580322\n",
      "Step: 1201, Loss: 0.9159902930259705, Accuracy: 1.0, Computation time: 1.5549416542053223\n",
      "Step: 1202, Loss: 0.9160961508750916, Accuracy: 1.0, Computation time: 1.8955810070037842\n",
      "Step: 1203, Loss: 0.9377784729003906, Accuracy: 0.9722222089767456, Computation time: 2.004197120666504\n",
      "Step: 1204, Loss: 0.9168200492858887, Accuracy: 1.0, Computation time: 1.6707255840301514\n",
      "Step: 1205, Loss: 0.9366134405136108, Accuracy: 0.96875, Computation time: 1.8245532512664795\n",
      "Step: 1206, Loss: 0.9165331125259399, Accuracy: 1.0, Computation time: 1.8908839225769043\n",
      "Step: 1207, Loss: 0.9160407781600952, Accuracy: 1.0, Computation time: 1.6495060920715332\n",
      "Step: 1208, Loss: 0.9518924951553345, Accuracy: 0.949999988079071, Computation time: 1.8543922901153564\n",
      "Step: 1209, Loss: 0.9330493807792664, Accuracy: 0.9791666865348816, Computation time: 2.174727201461792\n",
      "Step: 1210, Loss: 0.9165859222412109, Accuracy: 1.0, Computation time: 1.6783146858215332\n",
      "Step: 1211, Loss: 0.9256786108016968, Accuracy: 0.9772727489471436, Computation time: 1.6743438243865967\n",
      "Step: 1212, Loss: 0.9446906447410583, Accuracy: 0.9642857313156128, Computation time: 1.6644260883331299\n",
      "Step: 1213, Loss: 0.9165077209472656, Accuracy: 1.0, Computation time: 1.9716942310333252\n",
      "Step: 1214, Loss: 0.916814386844635, Accuracy: 1.0, Computation time: 1.917466640472412\n",
      "Step: 1215, Loss: 0.9601361155509949, Accuracy: 0.9464285969734192, Computation time: 1.1965036392211914\n",
      "Step: 1216, Loss: 0.923304557800293, Accuracy: 1.0, Computation time: 1.8309359550476074\n",
      "Step: 1217, Loss: 0.9378371238708496, Accuracy: 0.96875, Computation time: 1.5884792804718018\n",
      "Step: 1218, Loss: 0.9212955832481384, Accuracy: 1.0, Computation time: 1.4124550819396973\n",
      "Step: 1219, Loss: 0.9162212014198303, Accuracy: 1.0, Computation time: 1.292088270187378\n",
      "Step: 1220, Loss: 0.9164057374000549, Accuracy: 1.0, Computation time: 1.3932905197143555\n",
      "Step: 1221, Loss: 0.9163574576377869, Accuracy: 1.0, Computation time: 1.3575172424316406\n",
      "Step: 1222, Loss: 0.9160087704658508, Accuracy: 1.0, Computation time: 1.281179666519165\n",
      "Step: 1223, Loss: 0.9162445068359375, Accuracy: 1.0, Computation time: 1.3722312450408936\n",
      "Step: 1224, Loss: 0.9162347316741943, Accuracy: 1.0, Computation time: 1.3760871887207031\n",
      "Step: 1225, Loss: 0.9169591069221497, Accuracy: 1.0, Computation time: 1.3545184135437012\n",
      "Step: 1226, Loss: 0.9210214614868164, Accuracy: 1.0, Computation time: 1.3890836238861084\n",
      "Step: 1227, Loss: 0.9163105487823486, Accuracy: 1.0, Computation time: 1.4479038715362549\n",
      "Step: 1228, Loss: 0.9163349270820618, Accuracy: 1.0, Computation time: 1.7296826839447021\n",
      "Step: 1229, Loss: 0.916465163230896, Accuracy: 1.0, Computation time: 1.574925422668457\n",
      "Step: 1230, Loss: 0.9171932935714722, Accuracy: 1.0, Computation time: 1.6634562015533447\n",
      "Step: 1231, Loss: 0.9379363656044006, Accuracy: 0.9772727489471436, Computation time: 1.1502110958099365\n",
      "Step: 1232, Loss: 0.9161936044692993, Accuracy: 1.0, Computation time: 1.4373764991760254\n",
      "Step: 1233, Loss: 0.9161593914031982, Accuracy: 1.0, Computation time: 1.804215908050537\n",
      "Step: 1234, Loss: 0.9160940647125244, Accuracy: 1.0, Computation time: 1.4058163166046143\n",
      "Step: 1235, Loss: 0.9163421392440796, Accuracy: 1.0, Computation time: 1.2530579566955566\n",
      "Step: 1236, Loss: 0.9160352945327759, Accuracy: 1.0, Computation time: 1.2792479991912842\n",
      "Step: 1237, Loss: 0.9161503911018372, Accuracy: 1.0, Computation time: 1.284576654434204\n",
      "Step: 1238, Loss: 0.9161802530288696, Accuracy: 1.0, Computation time: 1.4912374019622803\n",
      "Step: 1239, Loss: 0.9161210656166077, Accuracy: 1.0, Computation time: 1.4821908473968506\n",
      "Step: 1240, Loss: 0.9176238775253296, Accuracy: 1.0, Computation time: 1.3861339092254639\n",
      "Step: 1241, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.470885992050171\n",
      "Step: 1242, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 1.4688050746917725\n",
      "Step: 1243, Loss: 0.9180947542190552, Accuracy: 1.0, Computation time: 1.3578126430511475\n",
      "Step: 1244, Loss: 0.916024923324585, Accuracy: 1.0, Computation time: 1.310168981552124\n",
      "Step: 1245, Loss: 0.9160178303718567, Accuracy: 1.0, Computation time: 1.2513220310211182\n",
      "Step: 1246, Loss: 0.9161372780799866, Accuracy: 1.0, Computation time: 1.7287023067474365\n",
      "Step: 1247, Loss: 0.9587948322296143, Accuracy: 0.9434524178504944, Computation time: 1.5757668018341064\n",
      "Step: 1248, Loss: 0.9186819791793823, Accuracy: 1.0, Computation time: 1.857712984085083\n",
      "Step: 1249, Loss: 0.9160004258155823, Accuracy: 1.0, Computation time: 1.5028748512268066\n",
      "Step: 1250, Loss: 0.9159591794013977, Accuracy: 1.0, Computation time: 1.7671666145324707\n",
      "Step: 1251, Loss: 0.9162128567695618, Accuracy: 1.0, Computation time: 1.6124274730682373\n",
      "########################\n",
      "Test loss: 1.127274751663208, Test Accuracy_epoch9: 0.6924210786819458\n",
      "########################\n",
      "Step: 1252, Loss: 0.9159709215164185, Accuracy: 1.0, Computation time: 1.8943226337432861\n",
      "Step: 1253, Loss: 0.9375495314598083, Accuracy: 0.9722222089767456, Computation time: 1.4692647457122803\n",
      "Step: 1254, Loss: 0.9297760128974915, Accuracy: 0.949999988079071, Computation time: 2.058431625366211\n",
      "Step: 1255, Loss: 0.9160062074661255, Accuracy: 1.0, Computation time: 1.6740682125091553\n",
      "Step: 1256, Loss: 0.9159818291664124, Accuracy: 1.0, Computation time: 1.9320542812347412\n",
      "Step: 1257, Loss: 0.9345024824142456, Accuracy: 0.9807692766189575, Computation time: 2.0351364612579346\n",
      "Step: 1258, Loss: 0.9160735011100769, Accuracy: 1.0, Computation time: 2.054859161376953\n",
      "Step: 1259, Loss: 0.9355627298355103, Accuracy: 0.9772727489471436, Computation time: 1.8659303188323975\n",
      "Step: 1260, Loss: 0.9162997007369995, Accuracy: 1.0, Computation time: 1.9510858058929443\n",
      "Step: 1261, Loss: 0.9236055016517639, Accuracy: 1.0, Computation time: 1.613476276397705\n",
      "Step: 1262, Loss: 0.9160619378089905, Accuracy: 1.0, Computation time: 1.454782247543335\n",
      "Step: 1263, Loss: 0.9161158204078674, Accuracy: 1.0, Computation time: 1.560659646987915\n",
      "Step: 1264, Loss: 0.9378595948219299, Accuracy: 0.9750000238418579, Computation time: 1.718641996383667\n",
      "Step: 1265, Loss: 0.9162992238998413, Accuracy: 1.0, Computation time: 1.8808231353759766\n",
      "Step: 1266, Loss: 0.9383350014686584, Accuracy: 0.9750000238418579, Computation time: 1.7858617305755615\n",
      "Step: 1267, Loss: 0.9381344318389893, Accuracy: 0.9807692766189575, Computation time: 1.680788278579712\n",
      "Step: 1268, Loss: 0.9162876605987549, Accuracy: 1.0, Computation time: 1.6573145389556885\n",
      "Step: 1269, Loss: 0.9373409152030945, Accuracy: 0.9722222089767456, Computation time: 1.5601396560668945\n",
      "Step: 1270, Loss: 0.9162271618843079, Accuracy: 1.0, Computation time: 1.4981906414031982\n",
      "Step: 1271, Loss: 0.9159919619560242, Accuracy: 1.0, Computation time: 1.8909087181091309\n",
      "Step: 1272, Loss: 0.9159433841705322, Accuracy: 1.0, Computation time: 1.7059783935546875\n",
      "Step: 1273, Loss: 0.9161731004714966, Accuracy: 1.0, Computation time: 1.4957304000854492\n",
      "Step: 1274, Loss: 0.9203436970710754, Accuracy: 1.0, Computation time: 1.8858683109283447\n",
      "Step: 1275, Loss: 0.9161680936813354, Accuracy: 1.0, Computation time: 1.7407152652740479\n",
      "Step: 1276, Loss: 0.9350321888923645, Accuracy: 0.9750000238418579, Computation time: 2.2753398418426514\n",
      "Step: 1277, Loss: 0.916845440864563, Accuracy: 1.0, Computation time: 1.5412919521331787\n",
      "Step: 1278, Loss: 0.9161275029182434, Accuracy: 1.0, Computation time: 1.5382983684539795\n",
      "Step: 1279, Loss: 0.916096568107605, Accuracy: 1.0, Computation time: 1.738373041152954\n",
      "Step: 1280, Loss: 0.9161810874938965, Accuracy: 1.0, Computation time: 1.8082544803619385\n",
      "Step: 1281, Loss: 0.9159688949584961, Accuracy: 1.0, Computation time: 1.4530634880065918\n",
      "Step: 1282, Loss: 0.915988028049469, Accuracy: 1.0, Computation time: 1.8012444972991943\n",
      "Step: 1283, Loss: 0.9159241914749146, Accuracy: 1.0, Computation time: 1.8659262657165527\n",
      "Step: 1284, Loss: 0.9319317936897278, Accuracy: 0.9583333730697632, Computation time: 2.063779830932617\n",
      "Step: 1285, Loss: 0.9161015748977661, Accuracy: 1.0, Computation time: 2.017317056655884\n",
      "Step: 1286, Loss: 0.9160104990005493, Accuracy: 1.0, Computation time: 1.939100742340088\n",
      "Step: 1287, Loss: 0.9159735441207886, Accuracy: 1.0, Computation time: 2.0256686210632324\n",
      "Step: 1288, Loss: 0.9160515666007996, Accuracy: 1.0, Computation time: 1.5705878734588623\n",
      "Step: 1289, Loss: 0.9161926507949829, Accuracy: 1.0, Computation time: 1.6594901084899902\n",
      "Step: 1290, Loss: 0.9160367846488953, Accuracy: 1.0, Computation time: 2.104294538497925\n",
      "Step: 1291, Loss: 0.9319682717323303, Accuracy: 0.9807692766189575, Computation time: 1.8855602741241455\n",
      "Step: 1292, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 1.831899881362915\n",
      "Step: 1293, Loss: 0.9226395487785339, Accuracy: 1.0, Computation time: 2.1538033485412598\n",
      "Step: 1294, Loss: 0.9161192178726196, Accuracy: 1.0, Computation time: 1.6819629669189453\n",
      "Step: 1295, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.6864848136901855\n",
      "Step: 1296, Loss: 0.9379568099975586, Accuracy: 0.9750000238418579, Computation time: 2.140873670578003\n",
      "Step: 1297, Loss: 0.9214478731155396, Accuracy: 1.0, Computation time: 2.359405994415283\n",
      "Step: 1298, Loss: 0.9162563681602478, Accuracy: 1.0, Computation time: 2.142174482345581\n",
      "Step: 1299, Loss: 0.9375483989715576, Accuracy: 0.9583333730697632, Computation time: 2.0979862213134766\n",
      "Step: 1300, Loss: 0.9398621916770935, Accuracy: 0.9583333730697632, Computation time: 1.7598888874053955\n",
      "Step: 1301, Loss: 0.9321327209472656, Accuracy: 0.96875, Computation time: 2.0275328159332275\n",
      "Step: 1302, Loss: 0.916395902633667, Accuracy: 1.0, Computation time: 1.9764645099639893\n",
      "Step: 1303, Loss: 0.9170030355453491, Accuracy: 1.0, Computation time: 1.8910284042358398\n",
      "Step: 1304, Loss: 0.9313731789588928, Accuracy: 0.9772727489471436, Computation time: 1.8955059051513672\n",
      "Step: 1305, Loss: 0.9226436614990234, Accuracy: 1.0, Computation time: 2.0145678520202637\n",
      "Step: 1306, Loss: 0.9159776568412781, Accuracy: 1.0, Computation time: 1.5495238304138184\n",
      "Step: 1307, Loss: 0.9160686135292053, Accuracy: 1.0, Computation time: 1.7493767738342285\n",
      "Step: 1308, Loss: 0.916151762008667, Accuracy: 1.0, Computation time: 1.7019033432006836\n",
      "Step: 1309, Loss: 0.9159739017486572, Accuracy: 1.0, Computation time: 1.5418801307678223\n",
      "Step: 1310, Loss: 0.9160377979278564, Accuracy: 1.0, Computation time: 1.588832139968872\n",
      "Step: 1311, Loss: 0.9160416722297668, Accuracy: 1.0, Computation time: 1.6713788509368896\n",
      "Step: 1312, Loss: 0.9159581065177917, Accuracy: 1.0, Computation time: 1.5191943645477295\n",
      "Step: 1313, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.7524776458740234\n",
      "Step: 1314, Loss: 0.9189901351928711, Accuracy: 1.0, Computation time: 1.739058017730713\n",
      "Step: 1315, Loss: 0.9159964323043823, Accuracy: 1.0, Computation time: 1.6727402210235596\n",
      "Step: 1316, Loss: 0.9376624226570129, Accuracy: 0.9722222089767456, Computation time: 1.757761001586914\n",
      "Step: 1317, Loss: 0.9162092208862305, Accuracy: 1.0, Computation time: 1.535991907119751\n",
      "Step: 1318, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 1.5133118629455566\n",
      "Step: 1319, Loss: 0.9175639152526855, Accuracy: 1.0, Computation time: 1.6246113777160645\n",
      "Step: 1320, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.4476075172424316\n",
      "Step: 1321, Loss: 0.9159622192382812, Accuracy: 1.0, Computation time: 1.8662292957305908\n",
      "Step: 1322, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 2.28385853767395\n",
      "Step: 1323, Loss: 0.9264953136444092, Accuracy: 0.9772727489471436, Computation time: 1.602687120437622\n",
      "Step: 1324, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 1.6516718864440918\n",
      "Step: 1325, Loss: 0.9589887857437134, Accuracy: 0.9285714626312256, Computation time: 1.3867824077606201\n",
      "Step: 1326, Loss: 0.9173686504364014, Accuracy: 1.0, Computation time: 1.5505149364471436\n",
      "Step: 1327, Loss: 0.9160971641540527, Accuracy: 1.0, Computation time: 1.554032802581787\n",
      "Step: 1328, Loss: 0.9160277843475342, Accuracy: 1.0, Computation time: 1.460805892944336\n",
      "Step: 1329, Loss: 0.937036395072937, Accuracy: 0.9750000238418579, Computation time: 1.8216652870178223\n",
      "Step: 1330, Loss: 0.9160118699073792, Accuracy: 1.0, Computation time: 1.5309813022613525\n",
      "Step: 1331, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.8839111328125\n",
      "Step: 1332, Loss: 0.9209445714950562, Accuracy: 1.0, Computation time: 1.7729477882385254\n",
      "Step: 1333, Loss: 0.9318888187408447, Accuracy: 0.9772727489471436, Computation time: 1.9043254852294922\n",
      "Step: 1334, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.3146545886993408\n",
      "Step: 1335, Loss: 0.9199279546737671, Accuracy: 1.0, Computation time: 1.7807040214538574\n",
      "Step: 1336, Loss: 0.9159191846847534, Accuracy: 1.0, Computation time: 1.6911587715148926\n",
      "Step: 1337, Loss: 0.9160474538803101, Accuracy: 1.0, Computation time: 1.9354650974273682\n",
      "Step: 1338, Loss: 0.9371088147163391, Accuracy: 0.9375, Computation time: 1.8555810451507568\n",
      "Step: 1339, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.9741017818450928\n",
      "Step: 1340, Loss: 0.9159023761749268, Accuracy: 1.0, Computation time: 1.6200270652770996\n",
      "Step: 1341, Loss: 0.9560153484344482, Accuracy: 0.9285714626312256, Computation time: 2.0766594409942627\n",
      "Step: 1342, Loss: 0.9159873723983765, Accuracy: 1.0, Computation time: 2.176753044128418\n",
      "Step: 1343, Loss: 0.9159961342811584, Accuracy: 1.0, Computation time: 1.9882526397705078\n",
      "Step: 1344, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.9069833755493164\n",
      "Step: 1345, Loss: 0.9165579676628113, Accuracy: 1.0, Computation time: 1.8553695678710938\n",
      "Step: 1346, Loss: 0.9376728534698486, Accuracy: 0.96875, Computation time: 2.0445728302001953\n",
      "Step: 1347, Loss: 0.9300175309181213, Accuracy: 0.9722222089767456, Computation time: 1.735689401626587\n",
      "Step: 1348, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 2.028964042663574\n",
      "Step: 1349, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 1.7579751014709473\n",
      "Step: 1350, Loss: 0.9503898620605469, Accuracy: 0.9356061220169067, Computation time: 1.8271291255950928\n",
      "Step: 1351, Loss: 0.9632553458213806, Accuracy: 0.9409722089767456, Computation time: 1.9576611518859863\n",
      "Step: 1352, Loss: 0.916160523891449, Accuracy: 1.0, Computation time: 2.1014904975891113\n",
      "Step: 1353, Loss: 0.9162608981132507, Accuracy: 1.0, Computation time: 1.9104206562042236\n",
      "Step: 1354, Loss: 0.9366642832756042, Accuracy: 0.9791666865348816, Computation time: 2.0909039974212646\n",
      "Step: 1355, Loss: 0.9162818789482117, Accuracy: 1.0, Computation time: 1.7241120338439941\n",
      "Step: 1356, Loss: 0.9161027073860168, Accuracy: 1.0, Computation time: 1.907106876373291\n",
      "Step: 1357, Loss: 0.9173239469528198, Accuracy: 1.0, Computation time: 1.8874616622924805\n",
      "Step: 1358, Loss: 0.9160302877426147, Accuracy: 1.0, Computation time: 1.7000181674957275\n",
      "Step: 1359, Loss: 0.9296675324440002, Accuracy: 0.9852941036224365, Computation time: 1.6444103717803955\n",
      "Step: 1360, Loss: 0.9163100123405457, Accuracy: 1.0, Computation time: 1.9456584453582764\n",
      "Step: 1361, Loss: 0.9160623550415039, Accuracy: 1.0, Computation time: 1.6075379848480225\n",
      "Step: 1362, Loss: 0.9393384456634521, Accuracy: 0.9583333730697632, Computation time: 1.4998295307159424\n",
      "Step: 1363, Loss: 0.916067361831665, Accuracy: 1.0, Computation time: 1.9036290645599365\n",
      "Step: 1364, Loss: 0.9305023550987244, Accuracy: 0.9642857313156128, Computation time: 1.4345135688781738\n",
      "Step: 1365, Loss: 0.9356720447540283, Accuracy: 0.9772727489471436, Computation time: 1.5105695724487305\n",
      "Step: 1366, Loss: 0.9164853692054749, Accuracy: 1.0, Computation time: 1.341820240020752\n",
      "Step: 1367, Loss: 0.91633141040802, Accuracy: 1.0, Computation time: 1.448927640914917\n",
      "Step: 1368, Loss: 0.91681969165802, Accuracy: 1.0, Computation time: 1.2538330554962158\n",
      "Step: 1369, Loss: 0.9164400100708008, Accuracy: 1.0, Computation time: 1.614229679107666\n",
      "Step: 1370, Loss: 0.9380468130111694, Accuracy: 0.9642857313156128, Computation time: 1.5377540588378906\n",
      "Step: 1371, Loss: 0.9441501498222351, Accuracy: 0.9750000238418579, Computation time: 1.6643102169036865\n",
      "Step: 1372, Loss: 0.9200620055198669, Accuracy: 1.0, Computation time: 1.936568021774292\n",
      "Step: 1373, Loss: 0.9161882400512695, Accuracy: 1.0, Computation time: 1.502323865890503\n",
      "Step: 1374, Loss: 0.9159912467002869, Accuracy: 1.0, Computation time: 1.3861114978790283\n",
      "Step: 1375, Loss: 0.9160051345825195, Accuracy: 1.0, Computation time: 1.3995904922485352\n",
      "Step: 1376, Loss: 0.9159829616546631, Accuracy: 1.0, Computation time: 1.802724838256836\n",
      "Step: 1377, Loss: 0.9159875512123108, Accuracy: 1.0, Computation time: 1.4741325378417969\n",
      "Step: 1378, Loss: 0.9220098853111267, Accuracy: 1.0, Computation time: 1.9481391906738281\n",
      "Step: 1379, Loss: 0.9162417650222778, Accuracy: 1.0, Computation time: 1.5534863471984863\n",
      "Step: 1380, Loss: 0.9163937568664551, Accuracy: 1.0, Computation time: 1.4309296607971191\n",
      "Step: 1381, Loss: 0.9380061030387878, Accuracy: 0.96875, Computation time: 1.3857033252716064\n",
      "Step: 1382, Loss: 0.9195486903190613, Accuracy: 1.0, Computation time: 1.837763786315918\n",
      "Step: 1383, Loss: 0.9163520336151123, Accuracy: 1.0, Computation time: 1.369039535522461\n",
      "Step: 1384, Loss: 0.921616792678833, Accuracy: 1.0, Computation time: 1.550910234451294\n",
      "Step: 1385, Loss: 0.91724693775177, Accuracy: 1.0, Computation time: 1.6129717826843262\n",
      "Step: 1386, Loss: 0.9165951609611511, Accuracy: 1.0, Computation time: 1.2493767738342285\n",
      "Step: 1387, Loss: 0.9163084626197815, Accuracy: 1.0, Computation time: 1.4433763027191162\n",
      "Step: 1388, Loss: 0.9165469408035278, Accuracy: 1.0, Computation time: 1.5155186653137207\n",
      "Step: 1389, Loss: 0.9248791337013245, Accuracy: 1.0, Computation time: 1.7935686111450195\n",
      "Step: 1390, Loss: 0.9184129238128662, Accuracy: 1.0, Computation time: 1.7080416679382324\n",
      "########################\n",
      "Test loss: 1.1200021505355835, Test Accuracy_epoch10: 0.7030503153800964\n",
      "########################\n",
      "Step: 1391, Loss: 0.9161850810050964, Accuracy: 1.0, Computation time: 1.4792022705078125\n",
      "Step: 1392, Loss: 0.916058361530304, Accuracy: 1.0, Computation time: 1.6255910396575928\n",
      "Step: 1393, Loss: 0.9159702062606812, Accuracy: 1.0, Computation time: 1.3158857822418213\n",
      "Step: 1394, Loss: 0.9167025089263916, Accuracy: 1.0, Computation time: 1.799039363861084\n",
      "Step: 1395, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.7641563415527344\n",
      "Step: 1396, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 1.5010061264038086\n",
      "Step: 1397, Loss: 0.9159383773803711, Accuracy: 1.0, Computation time: 1.3715455532073975\n",
      "Step: 1398, Loss: 0.91605144739151, Accuracy: 1.0, Computation time: 1.8527374267578125\n",
      "Step: 1399, Loss: 0.9409029483795166, Accuracy: 0.9722222089767456, Computation time: 2.019775152206421\n",
      "Step: 1400, Loss: 0.9268393516540527, Accuracy: 0.9750000238418579, Computation time: 1.4959540367126465\n",
      "Step: 1401, Loss: 0.9218594431877136, Accuracy: 1.0, Computation time: 1.7768380641937256\n",
      "Step: 1402, Loss: 0.9165438413619995, Accuracy: 1.0, Computation time: 1.5592024326324463\n",
      "Step: 1403, Loss: 0.9383704662322998, Accuracy: 0.9772727489471436, Computation time: 1.8435440063476562\n",
      "Step: 1404, Loss: 0.9185256361961365, Accuracy: 1.0, Computation time: 1.7997095584869385\n",
      "Step: 1405, Loss: 0.9181034564971924, Accuracy: 1.0, Computation time: 1.8634839057922363\n",
      "Step: 1406, Loss: 0.915935754776001, Accuracy: 1.0, Computation time: 1.7632594108581543\n",
      "Step: 1407, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 1.3384151458740234\n",
      "Step: 1408, Loss: 0.9159992933273315, Accuracy: 1.0, Computation time: 1.559335708618164\n",
      "Step: 1409, Loss: 0.9160263538360596, Accuracy: 1.0, Computation time: 1.4313774108886719\n",
      "Step: 1410, Loss: 0.9163114428520203, Accuracy: 1.0, Computation time: 1.5166475772857666\n",
      "Step: 1411, Loss: 0.9198169112205505, Accuracy: 1.0, Computation time: 1.6529991626739502\n",
      "Step: 1412, Loss: 0.9163077473640442, Accuracy: 1.0, Computation time: 1.5596961975097656\n",
      "Step: 1413, Loss: 0.9374706149101257, Accuracy: 0.9166666865348816, Computation time: 1.572477102279663\n",
      "Step: 1414, Loss: 0.9364281892776489, Accuracy: 0.9642857313156128, Computation time: 1.654104232788086\n",
      "Step: 1415, Loss: 0.9160504937171936, Accuracy: 1.0, Computation time: 1.7262628078460693\n",
      "Step: 1416, Loss: 0.9159789085388184, Accuracy: 1.0, Computation time: 1.2505872249603271\n",
      "Step: 1417, Loss: 0.9161760210990906, Accuracy: 1.0, Computation time: 1.752441167831421\n",
      "Step: 1418, Loss: 0.9610549807548523, Accuracy: 0.949999988079071, Computation time: 1.356199026107788\n",
      "Step: 1419, Loss: 0.9160432815551758, Accuracy: 1.0, Computation time: 1.6705350875854492\n",
      "Step: 1420, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.4010553359985352\n",
      "Step: 1421, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 1.6573326587677002\n",
      "Step: 1422, Loss: 0.9159886837005615, Accuracy: 1.0, Computation time: 1.9287464618682861\n",
      "Step: 1423, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 1.4857006072998047\n",
      "Step: 1424, Loss: 0.9160366654396057, Accuracy: 1.0, Computation time: 1.4072318077087402\n",
      "Step: 1425, Loss: 0.9160579442977905, Accuracy: 1.0, Computation time: 1.5330455303192139\n",
      "Step: 1426, Loss: 0.954187273979187, Accuracy: 0.8333333730697632, Computation time: 2.579829454421997\n",
      "Step: 1427, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.5084514617919922\n",
      "Step: 1428, Loss: 0.9180250763893127, Accuracy: 1.0, Computation time: 1.8630988597869873\n",
      "Step: 1429, Loss: 0.9371879696846008, Accuracy: 0.9642857313156128, Computation time: 1.7369942665100098\n",
      "Step: 1430, Loss: 0.9370368123054504, Accuracy: 0.9821428656578064, Computation time: 1.463677167892456\n",
      "Step: 1431, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 1.5972812175750732\n",
      "Step: 1432, Loss: 0.938483476638794, Accuracy: 0.9772727489471436, Computation time: 1.9227409362792969\n",
      "Step: 1433, Loss: 0.91604083776474, Accuracy: 1.0, Computation time: 1.5132098197937012\n",
      "Step: 1434, Loss: 0.9162131547927856, Accuracy: 1.0, Computation time: 1.4432616233825684\n",
      "Step: 1435, Loss: 0.9175007939338684, Accuracy: 1.0, Computation time: 1.4657697677612305\n",
      "Step: 1436, Loss: 0.9160884022712708, Accuracy: 1.0, Computation time: 1.9118881225585938\n",
      "Step: 1437, Loss: 0.9378172755241394, Accuracy: 0.9750000238418579, Computation time: 1.568070411682129\n",
      "Step: 1438, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 1.4902303218841553\n",
      "Step: 1439, Loss: 0.9347843527793884, Accuracy: 0.9722222089767456, Computation time: 1.7212042808532715\n",
      "Step: 1440, Loss: 0.921201229095459, Accuracy: 1.0, Computation time: 1.5705804824829102\n",
      "Step: 1441, Loss: 0.9351628422737122, Accuracy: 0.9722222089767456, Computation time: 1.610755443572998\n",
      "Step: 1442, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 1.6755387783050537\n",
      "Step: 1443, Loss: 0.9162556529045105, Accuracy: 1.0, Computation time: 1.8902883529663086\n",
      "Step: 1444, Loss: 0.9162764549255371, Accuracy: 1.0, Computation time: 1.4150738716125488\n",
      "Step: 1445, Loss: 0.9164943099021912, Accuracy: 1.0, Computation time: 1.7141695022583008\n",
      "Step: 1446, Loss: 0.9161413311958313, Accuracy: 1.0, Computation time: 1.9776687622070312\n",
      "Step: 1447, Loss: 0.941372275352478, Accuracy: 0.9722222089767456, Computation time: 2.5088932514190674\n",
      "Step: 1448, Loss: 0.9160239100456238, Accuracy: 1.0, Computation time: 1.5693883895874023\n",
      "Step: 1449, Loss: 0.9162575006484985, Accuracy: 1.0, Computation time: 1.5859763622283936\n",
      "Step: 1450, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 1.9145455360412598\n",
      "Step: 1451, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.5764117240905762\n",
      "Step: 1452, Loss: 0.9159814119338989, Accuracy: 1.0, Computation time: 1.7853915691375732\n",
      "Step: 1453, Loss: 0.915988564491272, Accuracy: 1.0, Computation time: 2.052070379257202\n",
      "Step: 1454, Loss: 0.929418683052063, Accuracy: 0.9642857313156128, Computation time: 2.5530781745910645\n",
      "Step: 1455, Loss: 0.9160229563713074, Accuracy: 1.0, Computation time: 2.08113169670105\n",
      "Step: 1456, Loss: 0.916312038898468, Accuracy: 1.0, Computation time: 1.506713628768921\n",
      "Step: 1457, Loss: 0.9160259366035461, Accuracy: 1.0, Computation time: 2.4753127098083496\n",
      "Step: 1458, Loss: 0.9168551564216614, Accuracy: 1.0, Computation time: 2.0506982803344727\n",
      "Step: 1459, Loss: 0.9159887433052063, Accuracy: 1.0, Computation time: 1.9807605743408203\n",
      "Step: 1460, Loss: 0.9159249663352966, Accuracy: 1.0, Computation time: 1.7291531562805176\n",
      "Step: 1461, Loss: 0.9168749451637268, Accuracy: 1.0, Computation time: 2.1043238639831543\n",
      "Step: 1462, Loss: 0.9163017868995667, Accuracy: 1.0, Computation time: 1.7127511501312256\n",
      "Step: 1463, Loss: 0.9161449074745178, Accuracy: 1.0, Computation time: 1.6374471187591553\n",
      "Step: 1464, Loss: 0.9160733222961426, Accuracy: 1.0, Computation time: 1.3242416381835938\n",
      "Step: 1465, Loss: 0.9160677790641785, Accuracy: 1.0, Computation time: 1.5387136936187744\n",
      "Step: 1466, Loss: 0.9170874357223511, Accuracy: 1.0, Computation time: 1.9103381633758545\n",
      "Step: 1467, Loss: 0.9160004258155823, Accuracy: 1.0, Computation time: 1.374812126159668\n",
      "Step: 1468, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.6031394004821777\n",
      "Step: 1469, Loss: 0.9373819828033447, Accuracy: 0.9791666865348816, Computation time: 1.427351713180542\n",
      "Step: 1470, Loss: 0.9163386225700378, Accuracy: 1.0, Computation time: 1.432837724685669\n",
      "Step: 1471, Loss: 0.9160228371620178, Accuracy: 1.0, Computation time: 1.6169390678405762\n",
      "Step: 1472, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.5837304592132568\n",
      "Step: 1473, Loss: 0.9375699162483215, Accuracy: 0.9750000238418579, Computation time: 1.5869183540344238\n",
      "Step: 1474, Loss: 0.9214547872543335, Accuracy: 1.0, Computation time: 1.797252893447876\n",
      "Step: 1475, Loss: 0.9372965693473816, Accuracy: 0.9642857313156128, Computation time: 1.974111557006836\n",
      "Step: 1476, Loss: 0.916111409664154, Accuracy: 1.0, Computation time: 1.8136308193206787\n",
      "Step: 1477, Loss: 0.937062680721283, Accuracy: 0.9722222089767456, Computation time: 1.8324010372161865\n",
      "Step: 1478, Loss: 0.9224029183387756, Accuracy: 1.0, Computation time: 1.8202242851257324\n",
      "Step: 1479, Loss: 0.9255927205085754, Accuracy: 1.0, Computation time: 2.293494939804077\n",
      "Step: 1480, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 2.0703978538513184\n",
      "Step: 1481, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 1.6258201599121094\n",
      "Step: 1482, Loss: 0.9159740209579468, Accuracy: 1.0, Computation time: 2.193519353866577\n",
      "Step: 1483, Loss: 0.916052520275116, Accuracy: 1.0, Computation time: 1.6934795379638672\n",
      "Step: 1484, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 2.063624858856201\n",
      "Step: 1485, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 2.0995826721191406\n",
      "Step: 1486, Loss: 0.9159979820251465, Accuracy: 1.0, Computation time: 1.9864368438720703\n",
      "Step: 1487, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 2.293560266494751\n",
      "Step: 1488, Loss: 0.9160012006759644, Accuracy: 1.0, Computation time: 2.034329891204834\n",
      "Step: 1489, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 2.134535074234009\n",
      "Step: 1490, Loss: 0.9374208450317383, Accuracy: 0.9642857313156128, Computation time: 1.7741115093231201\n",
      "Step: 1491, Loss: 0.95912104845047, Accuracy: 0.9545454978942871, Computation time: 2.0887045860290527\n",
      "Step: 1492, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 1.7494969367980957\n",
      "Step: 1493, Loss: 0.9273956418037415, Accuracy: 0.9807692766189575, Computation time: 2.1182405948638916\n",
      "Step: 1494, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.8237316608428955\n",
      "Step: 1495, Loss: 0.9159477949142456, Accuracy: 1.0, Computation time: 1.7537972927093506\n",
      "Step: 1496, Loss: 0.9160748720169067, Accuracy: 1.0, Computation time: 1.8716886043548584\n",
      "Step: 1497, Loss: 0.9369733929634094, Accuracy: 0.9791666865348816, Computation time: 2.3012189865112305\n",
      "Step: 1498, Loss: 0.9160135388374329, Accuracy: 1.0, Computation time: 1.9677226543426514\n",
      "Step: 1499, Loss: 0.9179438948631287, Accuracy: 1.0, Computation time: 2.1081249713897705\n",
      "Step: 1500, Loss: 0.95894455909729, Accuracy: 0.9615384340286255, Computation time: 2.014728546142578\n",
      "Step: 1501, Loss: 0.9174675941467285, Accuracy: 1.0, Computation time: 1.962632656097412\n",
      "Step: 1502, Loss: 0.9160388112068176, Accuracy: 1.0, Computation time: 2.142768144607544\n",
      "Step: 1503, Loss: 0.9168351888656616, Accuracy: 1.0, Computation time: 1.8451600074768066\n",
      "Step: 1504, Loss: 0.9160110950469971, Accuracy: 1.0, Computation time: 1.9737298488616943\n",
      "Step: 1505, Loss: 0.9360362887382507, Accuracy: 0.9750000238418579, Computation time: 1.8326506614685059\n",
      "Step: 1506, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 1.7239549160003662\n",
      "Step: 1507, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 2.1286818981170654\n",
      "Step: 1508, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.8432996273040771\n",
      "Step: 1509, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.709517478942871\n",
      "Step: 1510, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.6898083686828613\n",
      "Step: 1511, Loss: 0.9159690141677856, Accuracy: 1.0, Computation time: 1.9413528442382812\n",
      "Step: 1512, Loss: 0.9163483381271362, Accuracy: 1.0, Computation time: 1.8695826530456543\n",
      "Step: 1513, Loss: 0.9249246120452881, Accuracy: 1.0, Computation time: 1.7079861164093018\n",
      "Step: 1514, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.6920106410980225\n",
      "Step: 1515, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.8467648029327393\n",
      "Step: 1516, Loss: 0.9803122282028198, Accuracy: 0.925000011920929, Computation time: 2.0113425254821777\n",
      "Step: 1517, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 2.038052558898926\n",
      "Step: 1518, Loss: 0.9160431027412415, Accuracy: 1.0, Computation time: 1.886760950088501\n",
      "Step: 1519, Loss: 0.9340419769287109, Accuracy: 0.9821428656578064, Computation time: 2.051255702972412\n",
      "Step: 1520, Loss: 0.9160085320472717, Accuracy: 1.0, Computation time: 2.02703595161438\n",
      "Step: 1521, Loss: 0.9160891175270081, Accuracy: 1.0, Computation time: 1.6597766876220703\n",
      "Step: 1522, Loss: 0.9173688888549805, Accuracy: 1.0, Computation time: 1.716355800628662\n",
      "Step: 1523, Loss: 0.9236564636230469, Accuracy: 1.0, Computation time: 1.682910680770874\n",
      "Step: 1524, Loss: 0.9536041617393494, Accuracy: 0.9333333969116211, Computation time: 1.8208801746368408\n",
      "Step: 1525, Loss: 0.9375354647636414, Accuracy: 0.9807692766189575, Computation time: 1.4231626987457275\n",
      "Step: 1526, Loss: 0.9161011576652527, Accuracy: 1.0, Computation time: 1.6474204063415527\n",
      "Step: 1527, Loss: 0.9160580039024353, Accuracy: 1.0, Computation time: 1.4648656845092773\n",
      "Step: 1528, Loss: 0.9294953942298889, Accuracy: 0.9750000238418579, Computation time: 1.5607638359069824\n",
      "Step: 1529, Loss: 0.937929630279541, Accuracy: 0.9807692766189575, Computation time: 2.027005195617676\n",
      "########################\n",
      "Test loss: 1.1259573698043823, Test Accuracy_epoch11: 0.6940805315971375\n",
      "########################\n",
      "Step: 1530, Loss: 0.9168394804000854, Accuracy: 1.0, Computation time: 1.6067883968353271\n",
      "Step: 1531, Loss: 0.9162383675575256, Accuracy: 1.0, Computation time: 1.4200944900512695\n",
      "Step: 1532, Loss: 0.9163485169410706, Accuracy: 1.0, Computation time: 1.40828537940979\n",
      "Step: 1533, Loss: 0.9170629382133484, Accuracy: 1.0, Computation time: 1.452986240386963\n",
      "Step: 1534, Loss: 0.9164021015167236, Accuracy: 1.0, Computation time: 1.7946131229400635\n",
      "Step: 1535, Loss: 0.9171549677848816, Accuracy: 1.0, Computation time: 1.7812700271606445\n",
      "Step: 1536, Loss: 0.9161539077758789, Accuracy: 1.0, Computation time: 1.5905766487121582\n",
      "Step: 1537, Loss: 0.917784571647644, Accuracy: 1.0, Computation time: 2.021334648132324\n",
      "Step: 1538, Loss: 0.9160507917404175, Accuracy: 1.0, Computation time: 1.6750385761260986\n",
      "Step: 1539, Loss: 0.9582725763320923, Accuracy: 0.9375, Computation time: 1.7339928150177002\n",
      "Step: 1540, Loss: 0.9160467386245728, Accuracy: 1.0, Computation time: 1.7224445343017578\n",
      "Step: 1541, Loss: 0.9159987568855286, Accuracy: 1.0, Computation time: 1.6641132831573486\n",
      "Step: 1542, Loss: 0.9161660075187683, Accuracy: 1.0, Computation time: 2.002704620361328\n",
      "Step: 1543, Loss: 0.937854528427124, Accuracy: 0.9852941036224365, Computation time: 1.6897270679473877\n",
      "Step: 1544, Loss: 0.9161416292190552, Accuracy: 1.0, Computation time: 1.8312017917633057\n",
      "Step: 1545, Loss: 0.9161986112594604, Accuracy: 1.0, Computation time: 1.8113031387329102\n",
      "Step: 1546, Loss: 0.9160100817680359, Accuracy: 1.0, Computation time: 1.6305480003356934\n",
      "Step: 1547, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.5339992046356201\n",
      "Step: 1548, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 1.4904603958129883\n",
      "Step: 1549, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.3807787895202637\n",
      "Step: 1550, Loss: 0.9159707427024841, Accuracy: 1.0, Computation time: 1.6147642135620117\n",
      "Step: 1551, Loss: 0.9163406491279602, Accuracy: 1.0, Computation time: 1.6212859153747559\n",
      "Step: 1552, Loss: 0.9163346886634827, Accuracy: 1.0, Computation time: 1.749077320098877\n",
      "Step: 1553, Loss: 0.91599041223526, Accuracy: 1.0, Computation time: 1.7592206001281738\n",
      "Step: 1554, Loss: 0.9159455895423889, Accuracy: 1.0, Computation time: 1.7314338684082031\n",
      "Step: 1555, Loss: 0.9159278273582458, Accuracy: 1.0, Computation time: 1.6702039241790771\n",
      "Step: 1556, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.7432126998901367\n",
      "Step: 1557, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.6266546249389648\n",
      "Step: 1558, Loss: 0.9159619808197021, Accuracy: 1.0, Computation time: 1.8049900531768799\n",
      "Step: 1559, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.868431568145752\n",
      "Step: 1560, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.5624339580535889\n",
      "Step: 1561, Loss: 0.9159104228019714, Accuracy: 1.0, Computation time: 1.7802865505218506\n",
      "Step: 1562, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.9232478141784668\n",
      "Step: 1563, Loss: 0.9592856764793396, Accuracy: 0.9444444179534912, Computation time: 1.8916833400726318\n",
      "Step: 1564, Loss: 0.9159685373306274, Accuracy: 1.0, Computation time: 2.067753314971924\n",
      "Step: 1565, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.785588026046753\n",
      "Step: 1566, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.465254306793213\n",
      "Step: 1567, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.6711208820343018\n",
      "Step: 1568, Loss: 0.9375989437103271, Accuracy: 0.9772727489471436, Computation time: 1.4898173809051514\n",
      "Step: 1569, Loss: 0.9160183072090149, Accuracy: 1.0, Computation time: 1.7395110130310059\n",
      "Step: 1570, Loss: 0.9366888403892517, Accuracy: 0.9642857313156128, Computation time: 2.2899858951568604\n",
      "Step: 1571, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.7608006000518799\n",
      "Step: 1572, Loss: 0.9161312580108643, Accuracy: 1.0, Computation time: 1.8520593643188477\n",
      "Step: 1573, Loss: 0.9159643650054932, Accuracy: 1.0, Computation time: 1.8173878192901611\n",
      "Step: 1574, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.7890262603759766\n",
      "Step: 1575, Loss: 0.9457331299781799, Accuracy: 0.9722222089767456, Computation time: 2.083784818649292\n",
      "Step: 1576, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 1.8169126510620117\n",
      "Step: 1577, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.8734214305877686\n",
      "Step: 1578, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 2.242755174636841\n",
      "Step: 1579, Loss: 0.9163225889205933, Accuracy: 1.0, Computation time: 1.9045135974884033\n",
      "Step: 1580, Loss: 0.9372825026512146, Accuracy: 0.9642857313156128, Computation time: 2.1409144401550293\n",
      "Step: 1581, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.6281359195709229\n",
      "Step: 1582, Loss: 0.9160826802253723, Accuracy: 1.0, Computation time: 1.983579158782959\n",
      "Step: 1583, Loss: 0.958899199962616, Accuracy: 0.9615384340286255, Computation time: 1.9201161861419678\n",
      "Step: 1584, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.8876700401306152\n",
      "Step: 1585, Loss: 0.933048665523529, Accuracy: 0.9375, Computation time: 1.7077550888061523\n",
      "Step: 1586, Loss: 0.9171313047409058, Accuracy: 1.0, Computation time: 1.7523014545440674\n",
      "Step: 1587, Loss: 0.9159162044525146, Accuracy: nan, Computation time: 2.15885591506958\n",
      "Step: 1588, Loss: 0.9199193120002747, Accuracy: 1.0, Computation time: 1.8183457851409912\n",
      "Step: 1589, Loss: 0.9159280061721802, Accuracy: 1.0, Computation time: 1.8511433601379395\n",
      "Step: 1590, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 2.1852986812591553\n",
      "Step: 1591, Loss: 0.9253912568092346, Accuracy: 1.0, Computation time: 2.188178539276123\n",
      "Step: 1592, Loss: 0.916085958480835, Accuracy: 1.0, Computation time: 2.429353713989258\n",
      "Step: 1593, Loss: 0.9160304665565491, Accuracy: 1.0, Computation time: 1.9950189590454102\n",
      "Step: 1594, Loss: 0.9171478748321533, Accuracy: 1.0, Computation time: 2.053389549255371\n",
      "Step: 1595, Loss: 0.9163891673088074, Accuracy: 1.0, Computation time: 2.0510475635528564\n",
      "Step: 1596, Loss: 0.9159576892852783, Accuracy: 1.0, Computation time: 2.272306203842163\n",
      "Step: 1597, Loss: 0.9159544706344604, Accuracy: 1.0, Computation time: 2.431823492050171\n",
      "Step: 1598, Loss: 0.9365164637565613, Accuracy: 0.9642857313156128, Computation time: 2.0197439193725586\n",
      "Step: 1599, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 2.34712815284729\n",
      "Step: 1600, Loss: 0.9344847202301025, Accuracy: 0.9583333730697632, Computation time: 2.035658359527588\n",
      "Step: 1601, Loss: 0.9432984590530396, Accuracy: 0.9821428656578064, Computation time: 2.143848419189453\n",
      "Step: 1602, Loss: 0.9377095699310303, Accuracy: 0.9750000238418579, Computation time: 2.0521697998046875\n",
      "Step: 1603, Loss: 0.9163768291473389, Accuracy: 1.0, Computation time: 2.0996108055114746\n",
      "Step: 1604, Loss: 0.9377652406692505, Accuracy: 0.9833333492279053, Computation time: 2.3136017322540283\n",
      "Step: 1605, Loss: 0.916146993637085, Accuracy: 1.0, Computation time: 2.169031858444214\n",
      "Step: 1606, Loss: 0.9372588396072388, Accuracy: 0.9722222089767456, Computation time: 2.1070384979248047\n",
      "Step: 1607, Loss: 0.916355550289154, Accuracy: 1.0, Computation time: 2.3763043880462646\n",
      "Step: 1608, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.808880090713501\n",
      "Step: 1609, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 2.287409543991089\n",
      "Step: 1610, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.9744131565093994\n",
      "Step: 1611, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.586923599243164\n",
      "Step: 1612, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.5329608917236328\n",
      "Step: 1613, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 2.29060959815979\n",
      "Step: 1614, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.5622644424438477\n",
      "Step: 1615, Loss: 0.9566538333892822, Accuracy: 0.9666666984558105, Computation time: 1.6647062301635742\n",
      "Step: 1616, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.8371548652648926\n",
      "Step: 1617, Loss: 0.9377954602241516, Accuracy: 0.9821428656578064, Computation time: 1.7943010330200195\n",
      "Step: 1618, Loss: 0.9374943971633911, Accuracy: 0.9642857313156128, Computation time: 1.5071125030517578\n",
      "Step: 1619, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 1.916921615600586\n",
      "Step: 1620, Loss: 0.9171658158302307, Accuracy: 1.0, Computation time: 1.7104198932647705\n",
      "Step: 1621, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.5340468883514404\n",
      "Step: 1622, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.7036323547363281\n",
      "Step: 1623, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.7716710567474365\n",
      "Step: 1624, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.7105474472045898\n",
      "Step: 1625, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.8196945190429688\n",
      "Step: 1626, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 1.5457816123962402\n",
      "Step: 1627, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.6424295902252197\n",
      "Step: 1628, Loss: 0.9160390496253967, Accuracy: 1.0, Computation time: 1.4487595558166504\n",
      "Step: 1629, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 1.321812391281128\n",
      "Step: 1630, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.6009466648101807\n",
      "Step: 1631, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.420933485031128\n",
      "Step: 1632, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 1.6902735233306885\n",
      "Step: 1633, Loss: 0.9372029900550842, Accuracy: 0.96875, Computation time: 1.4910321235656738\n",
      "Step: 1634, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.6168005466461182\n",
      "Step: 1635, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.348292589187622\n",
      "Step: 1636, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.6756246089935303\n",
      "Step: 1637, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.5803430080413818\n",
      "Step: 1638, Loss: 0.9176405668258667, Accuracy: 1.0, Computation time: 2.308030128479004\n",
      "Step: 1639, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.659113883972168\n",
      "Step: 1640, Loss: 0.9159414172172546, Accuracy: nan, Computation time: 1.573038101196289\n",
      "Step: 1641, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.9806647300720215\n",
      "Step: 1642, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.8065049648284912\n",
      "Step: 1643, Loss: 0.9369267225265503, Accuracy: 0.9722222089767456, Computation time: 1.8608982563018799\n",
      "Step: 1644, Loss: 0.9160316586494446, Accuracy: 1.0, Computation time: 1.6102490425109863\n",
      "Step: 1645, Loss: 0.915909469127655, Accuracy: 1.0, Computation time: 1.6594617366790771\n",
      "Step: 1646, Loss: 0.9160342812538147, Accuracy: 1.0, Computation time: 1.422633409500122\n",
      "Step: 1647, Loss: 0.9180863499641418, Accuracy: 1.0, Computation time: 1.2986202239990234\n",
      "Step: 1648, Loss: 0.9160922765731812, Accuracy: 1.0, Computation time: 1.5167202949523926\n",
      "Step: 1649, Loss: 0.9160438179969788, Accuracy: 1.0, Computation time: 1.3454093933105469\n",
      "Step: 1650, Loss: 0.916085422039032, Accuracy: 1.0, Computation time: 2.057525634765625\n",
      "Step: 1651, Loss: 0.916018545627594, Accuracy: 1.0, Computation time: 1.348508596420288\n",
      "Step: 1652, Loss: 0.9159551858901978, Accuracy: 1.0, Computation time: 1.5779368877410889\n",
      "Step: 1653, Loss: 0.9159637093544006, Accuracy: 1.0, Computation time: 1.5878498554229736\n",
      "Step: 1654, Loss: 0.9212469458580017, Accuracy: 1.0, Computation time: 1.4754784107208252\n",
      "Step: 1655, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 1.8762025833129883\n",
      "Step: 1656, Loss: 0.916191816329956, Accuracy: 1.0, Computation time: 1.7258248329162598\n",
      "Step: 1657, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 1.5736145973205566\n",
      "Step: 1658, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 1.826889991760254\n",
      "Step: 1659, Loss: 0.9159634113311768, Accuracy: 1.0, Computation time: 1.461836338043213\n",
      "Step: 1660, Loss: 0.915978729724884, Accuracy: 1.0, Computation time: 1.5024313926696777\n",
      "Step: 1661, Loss: 0.9159946441650391, Accuracy: 1.0, Computation time: 1.684293270111084\n",
      "Step: 1662, Loss: 0.9346979856491089, Accuracy: 0.9583333730697632, Computation time: 2.013368606567383\n",
      "Step: 1663, Loss: 0.915993332862854, Accuracy: 1.0, Computation time: 1.561983585357666\n",
      "Step: 1664, Loss: 0.9342418909072876, Accuracy: 0.9722222089767456, Computation time: 1.8086230754852295\n",
      "Step: 1665, Loss: 0.9375267624855042, Accuracy: 0.9750000238418579, Computation time: 1.490361213684082\n",
      "Step: 1666, Loss: 0.9375843405723572, Accuracy: 0.9807692766189575, Computation time: 1.4067163467407227\n",
      "Step: 1667, Loss: 0.9187778830528259, Accuracy: 1.0, Computation time: 1.4564447402954102\n",
      "Step: 1668, Loss: 0.9360082149505615, Accuracy: 0.9750000238418579, Computation time: 1.7752783298492432\n",
      "########################\n",
      "Test loss: 1.1195441484451294, Test Accuracy_epoch12: 0.7037727236747742\n",
      "########################\n",
      "Step: 1669, Loss: 0.9160072803497314, Accuracy: 1.0, Computation time: 1.4359841346740723\n",
      "Step: 1670, Loss: 0.9161931276321411, Accuracy: 1.0, Computation time: 1.7139537334442139\n",
      "Step: 1671, Loss: 0.9161938428878784, Accuracy: 1.0, Computation time: 1.6092724800109863\n",
      "Step: 1672, Loss: 0.9159650802612305, Accuracy: 1.0, Computation time: 1.6915321350097656\n",
      "Step: 1673, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 1.5800907611846924\n",
      "Step: 1674, Loss: 0.9260196685791016, Accuracy: 0.9583333730697632, Computation time: 2.692194938659668\n",
      "Step: 1675, Loss: 0.9377145171165466, Accuracy: 0.9583333730697632, Computation time: 1.8765270709991455\n",
      "Step: 1676, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.760425090789795\n",
      "Step: 1677, Loss: 0.9235718846321106, Accuracy: 1.0, Computation time: 2.327284574508667\n",
      "Step: 1678, Loss: 0.9161859750747681, Accuracy: 1.0, Computation time: 2.075413942337036\n",
      "Step: 1679, Loss: 0.9377185106277466, Accuracy: 0.9772727489471436, Computation time: 1.6459789276123047\n",
      "Step: 1680, Loss: 0.916113018989563, Accuracy: 1.0, Computation time: 1.8719274997711182\n",
      "Step: 1681, Loss: 0.9179306030273438, Accuracy: 1.0, Computation time: 2.1776061058044434\n",
      "Step: 1682, Loss: 0.9162781834602356, Accuracy: 1.0, Computation time: 2.0811655521392822\n",
      "Step: 1683, Loss: 0.9389015436172485, Accuracy: 0.9772727489471436, Computation time: 2.0857927799224854\n",
      "Step: 1684, Loss: 0.9163520932197571, Accuracy: 1.0, Computation time: 2.2050862312316895\n",
      "Step: 1685, Loss: 0.9163318276405334, Accuracy: 1.0, Computation time: 2.1932027339935303\n",
      "Step: 1686, Loss: 0.916022002696991, Accuracy: 1.0, Computation time: 2.0658791065216064\n",
      "Step: 1687, Loss: 0.9159629344940186, Accuracy: 1.0, Computation time: 2.1063320636749268\n",
      "Step: 1688, Loss: 0.9164742827415466, Accuracy: 1.0, Computation time: 2.434997797012329\n",
      "Step: 1689, Loss: 0.9590839743614197, Accuracy: 0.9166666865348816, Computation time: 2.167797803878784\n",
      "Step: 1690, Loss: 0.9166508316993713, Accuracy: 1.0, Computation time: 2.3498635292053223\n",
      "Step: 1691, Loss: 0.936277449131012, Accuracy: 0.9750000238418579, Computation time: 2.088526725769043\n",
      "Step: 1692, Loss: 0.9159888625144958, Accuracy: 1.0, Computation time: 2.167030096054077\n",
      "Step: 1693, Loss: 0.9159391522407532, Accuracy: 1.0, Computation time: 2.012403726577759\n",
      "Step: 1694, Loss: 0.9159932136535645, Accuracy: 1.0, Computation time: 2.0391814708709717\n",
      "Step: 1695, Loss: 0.9165155291557312, Accuracy: 1.0, Computation time: 2.394392251968384\n",
      "Step: 1696, Loss: 0.9375944137573242, Accuracy: 0.949999988079071, Computation time: 1.7896406650543213\n",
      "Step: 1697, Loss: 0.9213546514511108, Accuracy: 1.0, Computation time: 2.1065945625305176\n",
      "Step: 1698, Loss: 0.9378393292427063, Accuracy: 0.96875, Computation time: 2.2467222213745117\n",
      "Step: 1699, Loss: 0.9320952892303467, Accuracy: 0.9821428656578064, Computation time: 2.2716691493988037\n",
      "Step: 1700, Loss: 0.9375421404838562, Accuracy: 0.9750000238418579, Computation time: 1.9479472637176514\n",
      "Step: 1701, Loss: 0.9161451458930969, Accuracy: 1.0, Computation time: 1.6317989826202393\n",
      "Step: 1702, Loss: 0.9161638617515564, Accuracy: 1.0, Computation time: 2.0025970935821533\n",
      "Step: 1703, Loss: 0.9162992835044861, Accuracy: 1.0, Computation time: 1.8603122234344482\n",
      "Step: 1704, Loss: 0.9177902340888977, Accuracy: 1.0, Computation time: 2.1789157390594482\n",
      "Step: 1705, Loss: 0.9163064956665039, Accuracy: 1.0, Computation time: 2.018749475479126\n",
      "Step: 1706, Loss: 0.9308071732521057, Accuracy: 0.9807692766189575, Computation time: 1.8305490016937256\n",
      "Step: 1707, Loss: 0.9161130785942078, Accuracy: 1.0, Computation time: 1.5755553245544434\n",
      "Step: 1708, Loss: 0.9162189960479736, Accuracy: 1.0, Computation time: 1.8480162620544434\n",
      "Step: 1709, Loss: 0.916146457195282, Accuracy: 1.0, Computation time: 1.6806836128234863\n",
      "Step: 1710, Loss: 0.9163171052932739, Accuracy: 1.0, Computation time: 1.8075006008148193\n",
      "Step: 1711, Loss: 0.9160832762718201, Accuracy: 1.0, Computation time: 1.699415683746338\n",
      "Step: 1712, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 1.7978003025054932\n",
      "Step: 1713, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.8518767356872559\n",
      "Step: 1714, Loss: 0.916151762008667, Accuracy: 1.0, Computation time: 1.858900547027588\n",
      "Step: 1715, Loss: 0.9322363138198853, Accuracy: 0.9772727489471436, Computation time: 1.622464656829834\n",
      "Step: 1716, Loss: 0.9160337448120117, Accuracy: 1.0, Computation time: 1.5190820693969727\n",
      "Step: 1717, Loss: 0.9159727692604065, Accuracy: 1.0, Computation time: 1.8580060005187988\n",
      "Step: 1718, Loss: 0.9162156581878662, Accuracy: 1.0, Computation time: 1.4955010414123535\n",
      "Step: 1719, Loss: 0.9160942435264587, Accuracy: 1.0, Computation time: 1.9949285984039307\n",
      "Step: 1720, Loss: 0.9160283803939819, Accuracy: 1.0, Computation time: 1.6041409969329834\n",
      "Step: 1721, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 1.6540367603302002\n",
      "Step: 1722, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.5785746574401855\n",
      "Step: 1723, Loss: 0.9375061988830566, Accuracy: 0.9750000238418579, Computation time: 1.4850983619689941\n",
      "Step: 1724, Loss: 0.9176191687583923, Accuracy: 1.0, Computation time: 1.6154754161834717\n",
      "Step: 1725, Loss: 0.9813591837882996, Accuracy: 0.90625, Computation time: 1.3458869457244873\n",
      "Step: 1726, Loss: 0.9312109351158142, Accuracy: 0.9807692766189575, Computation time: 1.8667137622833252\n",
      "Step: 1727, Loss: 0.9204815626144409, Accuracy: 1.0, Computation time: 1.5171597003936768\n",
      "Step: 1728, Loss: 0.9159651398658752, Accuracy: 1.0, Computation time: 1.6433022022247314\n",
      "Step: 1729, Loss: 0.916310727596283, Accuracy: 1.0, Computation time: 1.6779358386993408\n",
      "Step: 1730, Loss: 0.916046679019928, Accuracy: 1.0, Computation time: 1.5827131271362305\n",
      "Step: 1731, Loss: 0.9160747528076172, Accuracy: 1.0, Computation time: 1.5457088947296143\n",
      "Step: 1732, Loss: 0.9165876507759094, Accuracy: 1.0, Computation time: 1.2643382549285889\n",
      "Step: 1733, Loss: 0.935541570186615, Accuracy: 0.949999988079071, Computation time: 1.3243982791900635\n",
      "Step: 1734, Loss: 0.9167866110801697, Accuracy: 1.0, Computation time: 1.2873194217681885\n",
      "Step: 1735, Loss: 0.9162367582321167, Accuracy: 1.0, Computation time: 1.370337963104248\n",
      "Step: 1736, Loss: 0.9164409041404724, Accuracy: 1.0, Computation time: 1.2180864810943604\n",
      "Step: 1737, Loss: 0.9406420588493347, Accuracy: 0.96875, Computation time: 1.4383213520050049\n",
      "Step: 1738, Loss: 0.91627436876297, Accuracy: 1.0, Computation time: 1.5876691341400146\n",
      "Step: 1739, Loss: 0.9162883758544922, Accuracy: 1.0, Computation time: 1.5873541831970215\n",
      "Step: 1740, Loss: 0.9160515666007996, Accuracy: 1.0, Computation time: 1.2612009048461914\n",
      "Step: 1741, Loss: 0.9160863757133484, Accuracy: 1.0, Computation time: 1.3477802276611328\n",
      "Step: 1742, Loss: 0.9160358905792236, Accuracy: 1.0, Computation time: 1.5328385829925537\n",
      "Step: 1743, Loss: 0.9329976439476013, Accuracy: 0.9791666865348816, Computation time: 1.5776851177215576\n",
      "Step: 1744, Loss: 0.9159731864929199, Accuracy: 1.0, Computation time: 1.379492998123169\n",
      "Step: 1745, Loss: 0.9176692962646484, Accuracy: 1.0, Computation time: 1.8297717571258545\n",
      "Step: 1746, Loss: 0.9168484210968018, Accuracy: 1.0, Computation time: 1.62593412399292\n",
      "Step: 1747, Loss: 0.9159706234931946, Accuracy: 1.0, Computation time: 1.3104557991027832\n",
      "Step: 1748, Loss: 0.933201253414154, Accuracy: 0.9852941036224365, Computation time: 1.5034890174865723\n",
      "Step: 1749, Loss: 0.9379437565803528, Accuracy: 0.949999988079071, Computation time: 1.865509271621704\n",
      "Step: 1750, Loss: 0.9159659147262573, Accuracy: 1.0, Computation time: 1.4175052642822266\n",
      "Step: 1751, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 1.7636101245880127\n",
      "Step: 1752, Loss: 0.9159222841262817, Accuracy: 1.0, Computation time: 1.7666926383972168\n",
      "Step: 1753, Loss: 0.9159157872200012, Accuracy: 1.0, Computation time: 1.5397417545318604\n",
      "Step: 1754, Loss: 0.9185787439346313, Accuracy: 1.0, Computation time: 1.5304312705993652\n",
      "Step: 1755, Loss: 0.9369579553604126, Accuracy: 0.9750000238418579, Computation time: 1.421140193939209\n",
      "Step: 1756, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.4691133499145508\n",
      "Step: 1757, Loss: 0.9160155057907104, Accuracy: 1.0, Computation time: 1.8780558109283447\n",
      "Step: 1758, Loss: 0.9159577488899231, Accuracy: 1.0, Computation time: 1.5033025741577148\n",
      "Step: 1759, Loss: 0.9454421401023865, Accuracy: 0.9522727727890015, Computation time: 1.91209077835083\n",
      "Step: 1760, Loss: 0.916018009185791, Accuracy: 1.0, Computation time: 1.5320186614990234\n",
      "Step: 1761, Loss: 0.9159970283508301, Accuracy: 1.0, Computation time: 1.4977452754974365\n",
      "Step: 1762, Loss: 0.9709863066673279, Accuracy: 0.9147727489471436, Computation time: 2.047555446624756\n",
      "Step: 1763, Loss: 0.916153073310852, Accuracy: 1.0, Computation time: 1.3440680503845215\n",
      "Step: 1764, Loss: 0.9377235770225525, Accuracy: 0.96875, Computation time: 1.2667949199676514\n",
      "Step: 1765, Loss: 0.9376701712608337, Accuracy: 0.9750000238418579, Computation time: 1.660407543182373\n",
      "Step: 1766, Loss: 0.9202243089675903, Accuracy: 1.0, Computation time: 2.7435004711151123\n",
      "Step: 1767, Loss: 0.9166578054428101, Accuracy: 1.0, Computation time: 1.4420602321624756\n",
      "Step: 1768, Loss: 0.9480747580528259, Accuracy: 0.887499988079071, Computation time: 2.037834882736206\n",
      "Step: 1769, Loss: 0.9162809252738953, Accuracy: 1.0, Computation time: 1.2778475284576416\n",
      "Step: 1770, Loss: 0.9159970283508301, Accuracy: 1.0, Computation time: 1.4430639743804932\n",
      "Step: 1771, Loss: 0.937751054763794, Accuracy: 0.96875, Computation time: 1.6300880908966064\n",
      "Step: 1772, Loss: 0.9162299633026123, Accuracy: 1.0, Computation time: 1.4072630405426025\n",
      "Step: 1773, Loss: 0.9160953760147095, Accuracy: 1.0, Computation time: 1.6948285102844238\n",
      "Step: 1774, Loss: 0.9179277420043945, Accuracy: 1.0, Computation time: 1.748427391052246\n",
      "Step: 1775, Loss: 0.9246359467506409, Accuracy: 1.0, Computation time: 1.4600939750671387\n",
      "Step: 1776, Loss: 0.9161775708198547, Accuracy: 1.0, Computation time: 1.5520379543304443\n",
      "Step: 1777, Loss: 0.9161726236343384, Accuracy: 1.0, Computation time: 1.1975703239440918\n",
      "Step: 1778, Loss: 0.9398857951164246, Accuracy: 0.9583333730697632, Computation time: 1.8562703132629395\n",
      "Step: 1779, Loss: 0.9161707162857056, Accuracy: 1.0, Computation time: 1.5439784526824951\n",
      "Step: 1780, Loss: 0.9161475300788879, Accuracy: 1.0, Computation time: 1.6495578289031982\n",
      "Step: 1781, Loss: 0.916104793548584, Accuracy: 1.0, Computation time: 1.822300672531128\n",
      "Step: 1782, Loss: 0.9161463975906372, Accuracy: 1.0, Computation time: 1.8485167026519775\n",
      "Step: 1783, Loss: 0.916186511516571, Accuracy: 1.0, Computation time: 1.6965150833129883\n",
      "Step: 1784, Loss: 0.9160149097442627, Accuracy: nan, Computation time: 1.18593168258667\n",
      "Step: 1785, Loss: 0.9159749746322632, Accuracy: 1.0, Computation time: 1.2754628658294678\n",
      "Step: 1786, Loss: 0.9159730672836304, Accuracy: 1.0, Computation time: 1.5996129512786865\n",
      "Step: 1787, Loss: 0.9163527488708496, Accuracy: 1.0, Computation time: 1.4240565299987793\n",
      "Step: 1788, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 1.5807816982269287\n",
      "Step: 1789, Loss: 0.9159415364265442, Accuracy: 1.0, Computation time: 1.6373357772827148\n",
      "Step: 1790, Loss: 0.9160386919975281, Accuracy: 1.0, Computation time: 1.5159378051757812\n",
      "Step: 1791, Loss: 0.9286125302314758, Accuracy: 0.9750000238418579, Computation time: 1.841160535812378\n",
      "Step: 1792, Loss: 0.9163139462471008, Accuracy: 1.0, Computation time: 1.6998364925384521\n",
      "Step: 1793, Loss: 0.915959894657135, Accuracy: 1.0, Computation time: 1.349076271057129\n",
      "Step: 1794, Loss: 0.9160628914833069, Accuracy: 1.0, Computation time: 1.6325767040252686\n",
      "Step: 1795, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 1.4853737354278564\n",
      "Step: 1796, Loss: 0.9160203337669373, Accuracy: 1.0, Computation time: 1.5825762748718262\n",
      "Step: 1797, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 1.3799195289611816\n",
      "Step: 1798, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.9044837951660156\n",
      "Step: 1799, Loss: 0.9375717043876648, Accuracy: 0.9833333492279053, Computation time: 1.7365355491638184\n",
      "Step: 1800, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 1.7512292861938477\n",
      "Step: 1801, Loss: 0.9160386919975281, Accuracy: 1.0, Computation time: 1.8934237957000732\n",
      "Step: 1802, Loss: 0.9379942417144775, Accuracy: 0.949999988079071, Computation time: 1.4516937732696533\n",
      "Step: 1803, Loss: 0.9311726689338684, Accuracy: 0.9772727489471436, Computation time: 1.9519267082214355\n",
      "Step: 1804, Loss: 0.9161353707313538, Accuracy: 1.0, Computation time: 1.6240551471710205\n",
      "Step: 1805, Loss: 0.9355307817459106, Accuracy: 0.9375, Computation time: 1.322408676147461\n",
      "Step: 1806, Loss: 0.9374146461486816, Accuracy: 0.96875, Computation time: 1.7721829414367676\n",
      "Step: 1807, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 1.4375245571136475\n",
      "########################\n",
      "Test loss: 1.1192644834518433, Test Accuracy_epoch13: 0.7040717601776123\n",
      "########################\n",
      "Step: 1808, Loss: 0.9158939719200134, Accuracy: 1.0, Computation time: 1.606330394744873\n",
      "Step: 1809, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.6483638286590576\n",
      "Step: 1810, Loss: 0.9368060231208801, Accuracy: 0.9791666865348816, Computation time: 1.9549851417541504\n",
      "Step: 1811, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.6047680377960205\n",
      "Step: 1812, Loss: 0.9163255095481873, Accuracy: 1.0, Computation time: 2.1634860038757324\n",
      "Step: 1813, Loss: 0.917544424533844, Accuracy: 1.0, Computation time: 1.7750942707061768\n",
      "Step: 1814, Loss: 0.928395688533783, Accuracy: 0.9750000238418579, Computation time: 1.6794564723968506\n",
      "Step: 1815, Loss: 0.9160739183425903, Accuracy: 1.0, Computation time: 1.6790881156921387\n",
      "Step: 1816, Loss: 0.9374213814735413, Accuracy: nan, Computation time: 2.1346192359924316\n",
      "Step: 1817, Loss: 0.9376550316810608, Accuracy: 0.9642857313156128, Computation time: 1.5525097846984863\n",
      "Step: 1818, Loss: 0.934110701084137, Accuracy: 0.9807692766189575, Computation time: 1.878943681716919\n",
      "Step: 1819, Loss: 0.9159199595451355, Accuracy: 1.0, Computation time: 1.902449131011963\n",
      "Step: 1820, Loss: 0.9182131886482239, Accuracy: 1.0, Computation time: 2.4319067001342773\n",
      "Step: 1821, Loss: 0.9547416567802429, Accuracy: 0.970588207244873, Computation time: 2.248518228530884\n",
      "Step: 1822, Loss: 0.9160786867141724, Accuracy: 1.0, Computation time: 1.8539960384368896\n",
      "Step: 1823, Loss: 0.9371038675308228, Accuracy: 0.9772727489471436, Computation time: 1.9009578227996826\n",
      "Step: 1824, Loss: 0.9161238074302673, Accuracy: 1.0, Computation time: 1.5823767185211182\n",
      "Step: 1825, Loss: 0.9161539673805237, Accuracy: 1.0, Computation time: 1.7998712062835693\n",
      "Step: 1826, Loss: 0.916069746017456, Accuracy: 1.0, Computation time: 1.4478518962860107\n",
      "Step: 1827, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 1.9058144092559814\n",
      "Step: 1828, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.543351411819458\n",
      "Step: 1829, Loss: 0.9375874996185303, Accuracy: 0.9722222089767456, Computation time: 1.6643104553222656\n",
      "Step: 1830, Loss: 0.9160000085830688, Accuracy: 1.0, Computation time: 1.4380674362182617\n",
      "Step: 1831, Loss: 0.9160282611846924, Accuracy: 1.0, Computation time: 1.6637842655181885\n",
      "Step: 1832, Loss: 0.9421062469482422, Accuracy: 0.9642857313156128, Computation time: 2.0156357288360596\n",
      "Step: 1833, Loss: 0.9159768223762512, Accuracy: 1.0, Computation time: 1.2745044231414795\n",
      "Step: 1834, Loss: 0.9168180227279663, Accuracy: 1.0, Computation time: 1.5886595249176025\n",
      "Step: 1835, Loss: 0.9160161018371582, Accuracy: 1.0, Computation time: 1.4396381378173828\n",
      "Step: 1836, Loss: 0.9171429872512817, Accuracy: 1.0, Computation time: 1.485323429107666\n",
      "Step: 1837, Loss: 0.9160096645355225, Accuracy: 1.0, Computation time: 1.417027473449707\n",
      "Step: 1838, Loss: 0.9347983002662659, Accuracy: 0.96875, Computation time: 1.156381368637085\n",
      "Step: 1839, Loss: 0.9165613055229187, Accuracy: 1.0, Computation time: 1.557786226272583\n",
      "Step: 1840, Loss: 0.9158837199211121, Accuracy: 1.0, Computation time: 1.3744473457336426\n",
      "Step: 1841, Loss: 0.9369304180145264, Accuracy: 0.9791666865348816, Computation time: 1.323570728302002\n",
      "Step: 1842, Loss: 0.9180587530136108, Accuracy: 1.0, Computation time: 1.4517481327056885\n",
      "Step: 1843, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 1.3922464847564697\n",
      "Step: 1844, Loss: 0.9217419028282166, Accuracy: 1.0, Computation time: 1.650977373123169\n",
      "Step: 1845, Loss: 0.9160425662994385, Accuracy: 1.0, Computation time: 1.715911626815796\n",
      "Step: 1846, Loss: 0.9160223007202148, Accuracy: 1.0, Computation time: 1.3991062641143799\n",
      "Step: 1847, Loss: 0.9378054738044739, Accuracy: 0.9772727489471436, Computation time: 1.2999670505523682\n",
      "Step: 1848, Loss: 0.9160594344139099, Accuracy: 1.0, Computation time: 1.4651908874511719\n",
      "Step: 1849, Loss: 0.9160872101783752, Accuracy: 1.0, Computation time: 1.1931226253509521\n",
      "Step: 1850, Loss: 0.9333884716033936, Accuracy: 0.96875, Computation time: 2.0774216651916504\n",
      "Step: 1851, Loss: 0.9159538149833679, Accuracy: 1.0, Computation time: 1.4974091053009033\n",
      "Step: 1852, Loss: 0.9159445762634277, Accuracy: 1.0, Computation time: 1.2276527881622314\n",
      "Step: 1853, Loss: 0.9159466624259949, Accuracy: 1.0, Computation time: 1.5051112174987793\n",
      "Step: 1854, Loss: 0.9159534573554993, Accuracy: 1.0, Computation time: 1.1486613750457764\n",
      "Step: 1855, Loss: 0.9159671664237976, Accuracy: 1.0, Computation time: 1.3795719146728516\n",
      "Step: 1856, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.3374123573303223\n",
      "Step: 1857, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.4819700717926025\n",
      "Step: 1858, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.217932939529419\n",
      "Step: 1859, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.4314024448394775\n",
      "Step: 1860, Loss: 0.9174596667289734, Accuracy: 1.0, Computation time: 1.6064472198486328\n",
      "Step: 1861, Loss: 0.95920729637146, Accuracy: 0.9529914855957031, Computation time: 1.4719438552856445\n",
      "Step: 1862, Loss: 0.9159420132637024, Accuracy: 1.0, Computation time: 1.4138126373291016\n",
      "Step: 1863, Loss: 0.9159880876541138, Accuracy: 1.0, Computation time: 1.5888056755065918\n",
      "Step: 1864, Loss: 0.9159045815467834, Accuracy: 1.0, Computation time: 1.148230791091919\n",
      "Step: 1865, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.7073545455932617\n",
      "Step: 1866, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.31784987449646\n",
      "Step: 1867, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 1.1094279289245605\n",
      "Step: 1868, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.4355418682098389\n",
      "Step: 1869, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.6028196811676025\n",
      "Step: 1870, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.119600772857666\n",
      "Step: 1871, Loss: 0.9311437010765076, Accuracy: 0.9772727489471436, Computation time: 1.3540878295898438\n",
      "Step: 1872, Loss: 0.9171658158302307, Accuracy: 1.0, Computation time: 1.649015188217163\n",
      "Step: 1873, Loss: 0.9198400974273682, Accuracy: 1.0, Computation time: 1.5463225841522217\n",
      "Step: 1874, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.2716760635375977\n",
      "Step: 1875, Loss: 0.9159448146820068, Accuracy: 1.0, Computation time: 1.564267873764038\n",
      "Step: 1876, Loss: 0.9379805326461792, Accuracy: 0.9791666865348816, Computation time: 1.9515047073364258\n",
      "Step: 1877, Loss: 0.9161376357078552, Accuracy: 1.0, Computation time: 1.4820098876953125\n",
      "Step: 1878, Loss: 0.9160084128379822, Accuracy: 1.0, Computation time: 1.2847847938537598\n",
      "Step: 1879, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 1.2962327003479004\n",
      "Step: 1880, Loss: 0.9186721444129944, Accuracy: 1.0, Computation time: 2.1498751640319824\n",
      "Step: 1881, Loss: 0.9388046860694885, Accuracy: 0.9791666865348816, Computation time: 1.2105591297149658\n",
      "Step: 1882, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 1.495922565460205\n",
      "Step: 1883, Loss: 0.9374867081642151, Accuracy: 0.9772727489471436, Computation time: 1.4088342189788818\n",
      "Step: 1884, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.3500776290893555\n",
      "Step: 1885, Loss: 0.9310992956161499, Accuracy: 0.9807692766189575, Computation time: 1.8309545516967773\n",
      "Step: 1886, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.3758232593536377\n",
      "Step: 1887, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.2257509231567383\n",
      "Step: 1888, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 1.4297380447387695\n",
      "Step: 1889, Loss: 0.937745988368988, Accuracy: 0.9642857313156128, Computation time: 1.4929978847503662\n",
      "Step: 1890, Loss: 0.9160792231559753, Accuracy: 1.0, Computation time: 1.384979009628296\n",
      "Step: 1891, Loss: 0.9241342544555664, Accuracy: 1.0, Computation time: 1.5127146244049072\n",
      "Step: 1892, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 1.7048873901367188\n",
      "Step: 1893, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.7604894638061523\n",
      "Step: 1894, Loss: 0.9377644658088684, Accuracy: 0.9791666865348816, Computation time: 2.024413824081421\n",
      "Step: 1895, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 2.0181262493133545\n",
      "Step: 1896, Loss: 0.9159302115440369, Accuracy: 1.0, Computation time: 1.72233247756958\n",
      "Step: 1897, Loss: 0.9160423874855042, Accuracy: 1.0, Computation time: 1.8363442420959473\n",
      "Step: 1898, Loss: 0.916018545627594, Accuracy: 1.0, Computation time: 1.6447749137878418\n",
      "Step: 1899, Loss: 0.917907178401947, Accuracy: 1.0, Computation time: 1.7942867279052734\n",
      "Step: 1900, Loss: 0.9159950613975525, Accuracy: 1.0, Computation time: 1.6480567455291748\n",
      "Step: 1901, Loss: 0.934180736541748, Accuracy: 0.949999988079071, Computation time: 1.6785621643066406\n",
      "Step: 1902, Loss: 0.9297360777854919, Accuracy: 0.9375, Computation time: 1.941697120666504\n",
      "Step: 1903, Loss: 0.9164866209030151, Accuracy: 1.0, Computation time: 2.094299077987671\n",
      "Step: 1904, Loss: 0.9160794615745544, Accuracy: 1.0, Computation time: 1.8303940296173096\n",
      "Step: 1905, Loss: 0.9184032678604126, Accuracy: 1.0, Computation time: 2.421640157699585\n",
      "Step: 1906, Loss: 0.9200662970542908, Accuracy: 1.0, Computation time: 2.107806444168091\n",
      "Step: 1907, Loss: 0.915974497795105, Accuracy: 1.0, Computation time: 2.1349923610687256\n",
      "Step: 1908, Loss: 0.9365283846855164, Accuracy: 0.9583333730697632, Computation time: 1.7700927257537842\n",
      "Step: 1909, Loss: 0.9344688653945923, Accuracy: 0.9791666865348816, Computation time: 1.9138214588165283\n",
      "Step: 1910, Loss: 0.9163306355476379, Accuracy: 1.0, Computation time: 2.0435750484466553\n",
      "Step: 1911, Loss: 0.9161917567253113, Accuracy: 1.0, Computation time: 1.5844385623931885\n",
      "Step: 1912, Loss: 0.9164814949035645, Accuracy: 1.0, Computation time: 1.823523998260498\n",
      "Step: 1913, Loss: 0.9162777662277222, Accuracy: 1.0, Computation time: 1.6685044765472412\n",
      "Step: 1914, Loss: 0.9166744351387024, Accuracy: 1.0, Computation time: 1.489783525466919\n",
      "Step: 1915, Loss: 0.9160230755805969, Accuracy: 1.0, Computation time: 1.5111134052276611\n",
      "Step: 1916, Loss: 0.9159669876098633, Accuracy: 1.0, Computation time: 1.9794738292694092\n",
      "Step: 1917, Loss: 0.9208375215530396, Accuracy: 1.0, Computation time: 2.678093433380127\n",
      "Step: 1918, Loss: 0.9165533781051636, Accuracy: 1.0, Computation time: 2.2709648609161377\n",
      "Step: 1919, Loss: 0.9159488081932068, Accuracy: 1.0, Computation time: 1.4915053844451904\n",
      "Step: 1920, Loss: 0.9352329969406128, Accuracy: 0.9772727489471436, Computation time: 1.9537901878356934\n",
      "Step: 1921, Loss: 0.9160006046295166, Accuracy: 1.0, Computation time: 1.5631422996520996\n",
      "Step: 1922, Loss: 0.9376713633537292, Accuracy: 0.9722222089767456, Computation time: 1.2751429080963135\n",
      "Step: 1923, Loss: 0.9160229563713074, Accuracy: 1.0, Computation time: 1.4969615936279297\n",
      "Step: 1924, Loss: 0.9163347482681274, Accuracy: 1.0, Computation time: 1.6271138191223145\n",
      "Step: 1925, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.8190860748291016\n",
      "Step: 1926, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.4473886489868164\n",
      "Step: 1927, Loss: 0.9159795641899109, Accuracy: 1.0, Computation time: 1.6751246452331543\n",
      "Step: 1928, Loss: 0.9375900030136108, Accuracy: 0.9807692766189575, Computation time: 1.7622027397155762\n",
      "Step: 1929, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.3203909397125244\n",
      "Step: 1930, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.3392016887664795\n",
      "Step: 1931, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.3486101627349854\n",
      "Step: 1932, Loss: 0.9164372086524963, Accuracy: 1.0, Computation time: 1.6751704216003418\n",
      "Step: 1933, Loss: 0.9161246418952942, Accuracy: 1.0, Computation time: 1.3230178356170654\n",
      "Step: 1934, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 1.3295021057128906\n",
      "Step: 1935, Loss: 0.9161186218261719, Accuracy: 1.0, Computation time: 1.567220687866211\n",
      "Step: 1936, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.6272151470184326\n",
      "Step: 1937, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.7358167171478271\n",
      "Step: 1938, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.666959524154663\n",
      "Step: 1939, Loss: 0.9365354776382446, Accuracy: 0.9722222089767456, Computation time: 1.397550106048584\n",
      "Step: 1940, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.8302762508392334\n",
      "Step: 1941, Loss: 0.9160866141319275, Accuracy: 1.0, Computation time: 1.6793813705444336\n",
      "Step: 1942, Loss: 0.9170225858688354, Accuracy: 1.0, Computation time: 1.2860114574432373\n",
      "Step: 1943, Loss: 0.9159130454063416, Accuracy: 1.0, Computation time: 1.2298879623413086\n",
      "Step: 1944, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.4660122394561768\n",
      "Step: 1945, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.270608901977539\n",
      "Step: 1946, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.2770659923553467\n",
      "########################\n",
      "Test loss: 1.115564227104187, Test Accuracy_epoch14: 0.7091334462165833\n",
      "########################\n",
      "Step: 1947, Loss: 0.9374317526817322, Accuracy: 0.9722222089767456, Computation time: 1.6471123695373535\n",
      "Step: 1948, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.4323837757110596\n",
      "Step: 1949, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 1.4563679695129395\n",
      "Step: 1950, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 1.3121018409729004\n",
      "Step: 1951, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.5200023651123047\n",
      "Step: 1952, Loss: 0.9171434640884399, Accuracy: 1.0, Computation time: 1.2760138511657715\n",
      "Step: 1953, Loss: 0.924617350101471, Accuracy: 1.0, Computation time: 1.6705262660980225\n",
      "Step: 1954, Loss: 0.9160646200180054, Accuracy: 1.0, Computation time: 1.6638774871826172\n",
      "Step: 1955, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 1.5999078750610352\n",
      "Step: 1956, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.4855294227600098\n",
      "Step: 1957, Loss: 0.9304839968681335, Accuracy: 0.9583333730697632, Computation time: 1.6089990139007568\n",
      "Step: 1958, Loss: 0.9374656677246094, Accuracy: 0.9166666865348816, Computation time: 1.9036064147949219\n",
      "Step: 1959, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 1.3518307209014893\n",
      "Step: 1960, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 1.6430644989013672\n",
      "Step: 1961, Loss: 0.915931761264801, Accuracy: 1.0, Computation time: 1.4133360385894775\n",
      "Step: 1962, Loss: 0.9161421656608582, Accuracy: 1.0, Computation time: 1.345656156539917\n",
      "Step: 1963, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.5839824676513672\n",
      "Step: 1964, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 1.3104159832000732\n",
      "Step: 1965, Loss: 0.9377483129501343, Accuracy: 0.9722222089767456, Computation time: 1.9357171058654785\n",
      "Step: 1966, Loss: 0.9350597262382507, Accuracy: 0.9807692766189575, Computation time: 1.6336491107940674\n",
      "Step: 1967, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 2.1958999633789062\n",
      "Step: 1968, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.6653037071228027\n",
      "Step: 1969, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.6446499824523926\n",
      "Step: 1970, Loss: 0.9233337044715881, Accuracy: 1.0, Computation time: 2.0292952060699463\n",
      "Step: 1971, Loss: 0.9161236882209778, Accuracy: 1.0, Computation time: 1.9611988067626953\n",
      "Step: 1972, Loss: 0.9217169284820557, Accuracy: 1.0, Computation time: 1.57496976852417\n",
      "Step: 1973, Loss: 0.9162653684616089, Accuracy: 1.0, Computation time: 1.6605265140533447\n",
      "Step: 1974, Loss: 0.9280335307121277, Accuracy: 0.9772727489471436, Computation time: 1.6753394603729248\n",
      "Step: 1975, Loss: 0.9193570017814636, Accuracy: 1.0, Computation time: 1.9697864055633545\n",
      "Step: 1976, Loss: 0.9161202311515808, Accuracy: 1.0, Computation time: 1.97650146484375\n",
      "Step: 1977, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 1.3161673545837402\n",
      "Step: 1978, Loss: 0.9159347414970398, Accuracy: 1.0, Computation time: 1.1014714241027832\n",
      "Step: 1979, Loss: 0.9164080619812012, Accuracy: 1.0, Computation time: 1.400526523590088\n",
      "Step: 1980, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.5138781070709229\n",
      "Step: 1981, Loss: 0.933160662651062, Accuracy: 0.9583333730697632, Computation time: 1.9395370483398438\n",
      "Step: 1982, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.626246690750122\n",
      "Step: 1983, Loss: 0.916520357131958, Accuracy: 1.0, Computation time: 1.2934656143188477\n",
      "Step: 1984, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.6898808479309082\n",
      "Step: 1985, Loss: 0.9178394079208374, Accuracy: 1.0, Computation time: 1.4317197799682617\n",
      "Step: 1986, Loss: 0.9159064888954163, Accuracy: 1.0, Computation time: 1.4919123649597168\n",
      "Step: 1987, Loss: 0.9371078610420227, Accuracy: 0.949999988079071, Computation time: 1.7001450061798096\n",
      "Step: 1988, Loss: 0.938093900680542, Accuracy: 0.9722222089767456, Computation time: 1.3877158164978027\n",
      "Step: 1989, Loss: 0.9363294839859009, Accuracy: 0.96875, Computation time: 1.6638197898864746\n",
      "Step: 1990, Loss: 0.9160959720611572, Accuracy: 1.0, Computation time: 1.5620710849761963\n",
      "Step: 1991, Loss: 0.9372064471244812, Accuracy: 0.9722222089767456, Computation time: 1.6577811241149902\n",
      "Step: 1992, Loss: 0.9160453081130981, Accuracy: 1.0, Computation time: 1.486962080001831\n",
      "Step: 1993, Loss: 0.9162811636924744, Accuracy: 1.0, Computation time: 1.6580452919006348\n",
      "Step: 1994, Loss: 0.9376922845840454, Accuracy: 0.9772727489471436, Computation time: 1.5118918418884277\n",
      "Step: 1995, Loss: 0.916005551815033, Accuracy: 1.0, Computation time: 1.451878547668457\n",
      "Step: 1996, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 1.7048566341400146\n",
      "Step: 1997, Loss: 0.9199637174606323, Accuracy: 1.0, Computation time: 2.0128285884857178\n",
      "Step: 1998, Loss: 0.9158987402915955, Accuracy: 1.0, Computation time: 1.4640450477600098\n",
      "Step: 1999, Loss: 0.9160604476928711, Accuracy: 1.0, Computation time: 1.4349782466888428\n",
      "Step: 2000, Loss: 0.923053503036499, Accuracy: 1.0, Computation time: 1.8362963199615479\n",
      "Step: 2001, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.4425079822540283\n",
      "Step: 2002, Loss: 0.9159377217292786, Accuracy: 1.0, Computation time: 1.743469476699829\n",
      "Step: 2003, Loss: 0.9378800392150879, Accuracy: 0.9642857313156128, Computation time: 1.5254778861999512\n",
      "Step: 2004, Loss: 0.934191882610321, Accuracy: 0.96875, Computation time: 1.4269237518310547\n",
      "Step: 2005, Loss: 0.9320089221000671, Accuracy: 0.9821428656578064, Computation time: 1.6002192497253418\n",
      "Step: 2006, Loss: 0.9160491824150085, Accuracy: 1.0, Computation time: 1.6380374431610107\n",
      "Step: 2007, Loss: 0.9369699358940125, Accuracy: 0.9166666865348816, Computation time: 1.756072998046875\n",
      "Step: 2008, Loss: 0.9159749150276184, Accuracy: 1.0, Computation time: 1.3658173084259033\n",
      "Step: 2009, Loss: 0.9162369966506958, Accuracy: 1.0, Computation time: 1.5031230449676514\n",
      "Step: 2010, Loss: 0.9350638389587402, Accuracy: 0.984375, Computation time: 1.7585201263427734\n",
      "Step: 2011, Loss: 0.9583888053894043, Accuracy: 0.8958333730697632, Computation time: 1.583756923675537\n",
      "Step: 2012, Loss: 0.9162082076072693, Accuracy: 1.0, Computation time: 1.4093708992004395\n",
      "Step: 2013, Loss: 0.9159544706344604, Accuracy: 1.0, Computation time: 1.4544782638549805\n",
      "Step: 2014, Loss: 0.9160845279693604, Accuracy: 1.0, Computation time: 1.5183181762695312\n",
      "Step: 2015, Loss: 0.9373247027397156, Accuracy: 0.9791666865348816, Computation time: 1.6250026226043701\n",
      "Step: 2016, Loss: 0.9346204400062561, Accuracy: 0.9791666865348816, Computation time: 1.6606807708740234\n",
      "Step: 2017, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 1.2268505096435547\n",
      "Step: 2018, Loss: 0.9160149097442627, Accuracy: 1.0, Computation time: 1.51116943359375\n",
      "Step: 2019, Loss: 0.9161242842674255, Accuracy: 1.0, Computation time: 1.3489947319030762\n",
      "Step: 2020, Loss: 0.916008472442627, Accuracy: 1.0, Computation time: 1.2456321716308594\n",
      "Step: 2021, Loss: 0.9160223007202148, Accuracy: 1.0, Computation time: 1.9428892135620117\n",
      "Step: 2022, Loss: 0.9164696335792542, Accuracy: 1.0, Computation time: 1.6060206890106201\n",
      "Step: 2023, Loss: 0.9376975893974304, Accuracy: 0.96875, Computation time: 2.010629415512085\n",
      "Step: 2024, Loss: 0.9160614013671875, Accuracy: 1.0, Computation time: 1.4674718379974365\n",
      "Step: 2025, Loss: 0.9159997701644897, Accuracy: 1.0, Computation time: 1.5871784687042236\n",
      "Step: 2026, Loss: 0.9223151803016663, Accuracy: 1.0, Computation time: 1.9006047248840332\n",
      "Step: 2027, Loss: 0.9180877208709717, Accuracy: 1.0, Computation time: 1.6599080562591553\n",
      "Step: 2028, Loss: 0.937795877456665, Accuracy: 0.9791666865348816, Computation time: 1.6602263450622559\n",
      "Step: 2029, Loss: 0.9159801006317139, Accuracy: 1.0, Computation time: 1.3382816314697266\n",
      "Step: 2030, Loss: 0.9182332158088684, Accuracy: 1.0, Computation time: 1.5433902740478516\n",
      "Step: 2031, Loss: 0.9160881042480469, Accuracy: 1.0, Computation time: 1.4804294109344482\n",
      "Step: 2032, Loss: 0.9161283373832703, Accuracy: 1.0, Computation time: 1.8680024147033691\n",
      "Step: 2033, Loss: 0.916714608669281, Accuracy: 1.0, Computation time: 1.757430076599121\n",
      "Step: 2034, Loss: 0.937825620174408, Accuracy: 0.949999988079071, Computation time: 1.8281581401824951\n",
      "Step: 2035, Loss: 0.9160491824150085, Accuracy: 1.0, Computation time: 1.4805948734283447\n",
      "Step: 2036, Loss: 0.9209609627723694, Accuracy: 1.0, Computation time: 2.1689953804016113\n",
      "Step: 2037, Loss: 0.9163807034492493, Accuracy: 1.0, Computation time: 1.7361927032470703\n",
      "Step: 2038, Loss: 0.9382469058036804, Accuracy: 0.9821428656578064, Computation time: 1.8295114040374756\n",
      "Step: 2039, Loss: 0.9160003662109375, Accuracy: 1.0, Computation time: 2.8237547874450684\n",
      "Step: 2040, Loss: 0.9373548626899719, Accuracy: 0.9791666865348816, Computation time: 1.9397470951080322\n",
      "Step: 2041, Loss: 0.9159368872642517, Accuracy: 1.0, Computation time: 1.30731201171875\n",
      "Step: 2042, Loss: 0.9159234166145325, Accuracy: 1.0, Computation time: 1.659630537033081\n",
      "Step: 2043, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.8571805953979492\n",
      "Step: 2044, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.665257215499878\n",
      "Step: 2045, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.662658452987671\n",
      "Step: 2046, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 1.591407060623169\n",
      "Step: 2047, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.5001640319824219\n",
      "Step: 2048, Loss: 0.9178913831710815, Accuracy: 1.0, Computation time: 2.1564948558807373\n",
      "Step: 2049, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.6355328559875488\n",
      "Step: 2050, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.7594618797302246\n",
      "Step: 2051, Loss: 0.9595546126365662, Accuracy: 0.9472222328186035, Computation time: 1.756669282913208\n",
      "Step: 2052, Loss: 0.9371764659881592, Accuracy: 0.9821428656578064, Computation time: 1.6958210468292236\n",
      "Step: 2053, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.5367414951324463\n",
      "Step: 2054, Loss: 0.9159664511680603, Accuracy: 1.0, Computation time: 1.9144442081451416\n",
      "Step: 2055, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.9605634212493896\n",
      "Step: 2056, Loss: 0.9160231947898865, Accuracy: 1.0, Computation time: 2.0421693325042725\n",
      "Step: 2057, Loss: 0.9158521890640259, Accuracy: 1.0, Computation time: 1.7231371402740479\n",
      "Step: 2058, Loss: 0.9346626400947571, Accuracy: 0.9750000238418579, Computation time: 1.7433815002441406\n",
      "Step: 2059, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.8141093254089355\n",
      "Step: 2060, Loss: 0.9339233636856079, Accuracy: 0.9750000238418579, Computation time: 1.800044059753418\n",
      "Step: 2061, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.640753984451294\n",
      "Step: 2062, Loss: 0.9159379005432129, Accuracy: 1.0, Computation time: 1.7973883152008057\n",
      "Step: 2063, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.8994393348693848\n",
      "Step: 2064, Loss: 0.9163265228271484, Accuracy: 1.0, Computation time: 1.717491626739502\n",
      "Step: 2065, Loss: 0.915906548500061, Accuracy: 1.0, Computation time: 1.6295156478881836\n",
      "Step: 2066, Loss: 0.9162273406982422, Accuracy: 1.0, Computation time: 2.206268310546875\n",
      "Step: 2067, Loss: 0.9163330793380737, Accuracy: 1.0, Computation time: 1.5779354572296143\n",
      "Step: 2068, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.5625104904174805\n",
      "Step: 2069, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.7041151523590088\n",
      "Step: 2070, Loss: 0.918319821357727, Accuracy: 1.0, Computation time: 1.7556452751159668\n",
      "Step: 2071, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.2714447975158691\n",
      "Step: 2072, Loss: 0.9160346388816833, Accuracy: 1.0, Computation time: 1.5475904941558838\n",
      "Step: 2073, Loss: 0.936345636844635, Accuracy: 0.949999988079071, Computation time: 1.3880805969238281\n",
      "Step: 2074, Loss: 0.9800259470939636, Accuracy: 0.9142857193946838, Computation time: 1.6689014434814453\n",
      "Step: 2075, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 1.6651442050933838\n",
      "Step: 2076, Loss: 0.9160158634185791, Accuracy: 1.0, Computation time: 1.980224609375\n",
      "Step: 2077, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.4311373233795166\n",
      "Step: 2078, Loss: 0.9158753156661987, Accuracy: 1.0, Computation time: 1.41337251663208\n",
      "Step: 2079, Loss: 0.9164859056472778, Accuracy: 1.0, Computation time: 1.8774006366729736\n",
      "Step: 2080, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.4911656379699707\n",
      "Step: 2081, Loss: 0.9158948063850403, Accuracy: 1.0, Computation time: 1.581312894821167\n",
      "Step: 2082, Loss: 0.936653196811676, Accuracy: 0.9642857313156128, Computation time: 1.3933587074279785\n",
      "Step: 2083, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.5109105110168457\n",
      "Step: 2084, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.424910306930542\n",
      "Step: 2085, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.435727596282959\n",
      "########################\n",
      "Test loss: 1.1219651699066162, Test Accuracy_epoch15: 0.6989741325378418\n",
      "########################\n",
      "Step: 2086, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.654618740081787\n",
      "Step: 2087, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.3644979000091553\n",
      "Step: 2088, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 1.6707055568695068\n",
      "Step: 2089, Loss: 0.9164592623710632, Accuracy: 1.0, Computation time: 1.5970244407653809\n",
      "Step: 2090, Loss: 0.9160340428352356, Accuracy: 1.0, Computation time: 1.425429105758667\n",
      "Step: 2091, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.542253017425537\n",
      "Step: 2092, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 1.3651509284973145\n",
      "Step: 2093, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.501509189605713\n",
      "Step: 2094, Loss: 0.9159687757492065, Accuracy: 1.0, Computation time: 1.5678999423980713\n",
      "Step: 2095, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 1.3615658283233643\n",
      "Step: 2096, Loss: 0.937586784362793, Accuracy: 0.9722222089767456, Computation time: 1.8314142227172852\n",
      "Step: 2097, Loss: 0.9338221549987793, Accuracy: 0.9791666865348816, Computation time: 1.8752970695495605\n",
      "Step: 2098, Loss: 0.9220737218856812, Accuracy: 1.0, Computation time: 1.9898719787597656\n",
      "Step: 2099, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 1.5807750225067139\n",
      "Step: 2100, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.3593151569366455\n",
      "Step: 2101, Loss: 0.9178619980812073, Accuracy: 1.0, Computation time: 1.840050220489502\n",
      "Step: 2102, Loss: 0.9160616397857666, Accuracy: 1.0, Computation time: 1.5546362400054932\n",
      "Step: 2103, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 1.2831945419311523\n",
      "Step: 2104, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.2869682312011719\n",
      "Step: 2105, Loss: 0.9158928394317627, Accuracy: 1.0, Computation time: 1.5128028392791748\n",
      "Step: 2106, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.671617031097412\n",
      "Step: 2107, Loss: 0.9164099097251892, Accuracy: 1.0, Computation time: 1.4480538368225098\n",
      "Step: 2108, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.6745731830596924\n",
      "Step: 2109, Loss: 0.9160640239715576, Accuracy: 1.0, Computation time: 1.513503074645996\n",
      "Step: 2110, Loss: 0.9612904787063599, Accuracy: 0.9444444179534912, Computation time: 1.7561240196228027\n",
      "Step: 2111, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 1.5665109157562256\n",
      "Step: 2112, Loss: 0.91797935962677, Accuracy: 1.0, Computation time: 1.7461695671081543\n",
      "Step: 2113, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.3349778652191162\n",
      "Step: 2114, Loss: 0.9159941673278809, Accuracy: 1.0, Computation time: 1.4313914775848389\n",
      "Step: 2115, Loss: 0.9160418510437012, Accuracy: 1.0, Computation time: 1.6018545627593994\n",
      "Step: 2116, Loss: 0.9378848671913147, Accuracy: 0.9791666865348816, Computation time: 1.4256911277770996\n",
      "Step: 2117, Loss: 0.9159335494041443, Accuracy: 1.0, Computation time: 1.7691988945007324\n",
      "Step: 2118, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 1.8379533290863037\n",
      "Step: 2119, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.3565285205841064\n",
      "Step: 2120, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.3323800563812256\n",
      "Step: 2121, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.2328739166259766\n",
      "Step: 2122, Loss: 0.9374575018882751, Accuracy: 0.9583333730697632, Computation time: 1.518207311630249\n",
      "Step: 2123, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.3921051025390625\n",
      "Step: 2124, Loss: 0.928799569606781, Accuracy: 0.9833333492279053, Computation time: 1.8778088092803955\n",
      "Step: 2125, Loss: 0.9559359550476074, Accuracy: 0.84375, Computation time: 1.7024562358856201\n",
      "Step: 2126, Loss: 0.9380478858947754, Accuracy: 0.9750000238418579, Computation time: 1.772763967514038\n",
      "Step: 2127, Loss: 0.9160816669464111, Accuracy: 1.0, Computation time: 1.7149853706359863\n",
      "Step: 2128, Loss: 0.9162986874580383, Accuracy: 1.0, Computation time: 1.5903151035308838\n",
      "Step: 2129, Loss: 0.9161345958709717, Accuracy: 1.0, Computation time: 1.3554978370666504\n",
      "Step: 2130, Loss: 0.9208548665046692, Accuracy: 1.0, Computation time: 1.6729369163513184\n",
      "Step: 2131, Loss: 0.9161378145217896, Accuracy: 1.0, Computation time: 1.503856897354126\n",
      "Step: 2132, Loss: 0.916607677936554, Accuracy: 1.0, Computation time: 1.3324997425079346\n",
      "Step: 2133, Loss: 0.9160211086273193, Accuracy: 1.0, Computation time: 1.3749632835388184\n",
      "Step: 2134, Loss: 0.9201220273971558, Accuracy: 1.0, Computation time: 1.6401569843292236\n",
      "Step: 2135, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.6321096420288086\n",
      "Step: 2136, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.2379190921783447\n",
      "Step: 2137, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.2860321998596191\n",
      "Step: 2138, Loss: 0.9359891414642334, Accuracy: 0.96875, Computation time: 1.7381782531738281\n",
      "Step: 2139, Loss: 0.9159791469573975, Accuracy: 1.0, Computation time: 1.39882493019104\n",
      "Step: 2140, Loss: 0.9159901738166809, Accuracy: 1.0, Computation time: 1.9248771667480469\n",
      "Step: 2141, Loss: 0.9159422516822815, Accuracy: 1.0, Computation time: 1.3131232261657715\n",
      "Step: 2142, Loss: 0.9311633110046387, Accuracy: 0.9642857313156128, Computation time: 2.171464681625366\n",
      "Step: 2143, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.6531760692596436\n",
      "Step: 2144, Loss: 0.9161695837974548, Accuracy: 1.0, Computation time: 1.9433510303497314\n",
      "Step: 2145, Loss: 0.9178065061569214, Accuracy: 1.0, Computation time: 1.8907949924468994\n",
      "Step: 2146, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 2.0846633911132812\n",
      "Step: 2147, Loss: 0.9159888029098511, Accuracy: 1.0, Computation time: 1.332813024520874\n",
      "Step: 2148, Loss: 0.917711615562439, Accuracy: 1.0, Computation time: 1.4561405181884766\n",
      "Step: 2149, Loss: 0.9161105751991272, Accuracy: 1.0, Computation time: 1.7399415969848633\n",
      "Step: 2150, Loss: 0.9376973509788513, Accuracy: 0.9772727489471436, Computation time: 1.4155426025390625\n",
      "Step: 2151, Loss: 0.9160261154174805, Accuracy: 1.0, Computation time: 1.6613187789916992\n",
      "Step: 2152, Loss: 0.9160213470458984, Accuracy: 1.0, Computation time: 1.4693818092346191\n",
      "Step: 2153, Loss: 0.9160870909690857, Accuracy: 1.0, Computation time: 1.8076329231262207\n",
      "Step: 2154, Loss: 0.9375312924385071, Accuracy: 0.9772727489471436, Computation time: 1.7445642948150635\n",
      "Step: 2155, Loss: 0.9158950448036194, Accuracy: 1.0, Computation time: 1.3295016288757324\n",
      "Step: 2156, Loss: 0.9170987010002136, Accuracy: 1.0, Computation time: 1.4800424575805664\n",
      "Step: 2157, Loss: 0.9374936819076538, Accuracy: 0.875, Computation time: 1.4866478443145752\n",
      "Step: 2158, Loss: 0.9159509539604187, Accuracy: 1.0, Computation time: 1.5365900993347168\n",
      "Step: 2159, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.901261568069458\n",
      "Step: 2160, Loss: 0.9376744031906128, Accuracy: 0.9722222089767456, Computation time: 1.540027141571045\n",
      "Step: 2161, Loss: 0.9187473058700562, Accuracy: 1.0, Computation time: 1.8046491146087646\n",
      "Step: 2162, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.6228959560394287\n",
      "Step: 2163, Loss: 0.9159678816795349, Accuracy: 1.0, Computation time: 1.5955541133880615\n",
      "Step: 2164, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.6395015716552734\n",
      "Step: 2165, Loss: 0.9159682989120483, Accuracy: 1.0, Computation time: 1.6158034801483154\n",
      "Step: 2166, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.695451021194458\n",
      "Step: 2167, Loss: 0.9159256219863892, Accuracy: 1.0, Computation time: 1.5006351470947266\n",
      "Step: 2168, Loss: 0.9232675433158875, Accuracy: 1.0, Computation time: 2.0417861938476562\n",
      "Step: 2169, Loss: 0.9159159064292908, Accuracy: 1.0, Computation time: 2.077300786972046\n",
      "Step: 2170, Loss: 0.9380151033401489, Accuracy: 0.96875, Computation time: 1.4482474327087402\n",
      "Step: 2171, Loss: 0.9376736879348755, Accuracy: 0.9807692766189575, Computation time: 2.147211790084839\n",
      "Step: 2172, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.6872544288635254\n",
      "Step: 2173, Loss: 0.9160480499267578, Accuracy: 1.0, Computation time: 1.8232343196868896\n",
      "Step: 2174, Loss: 0.9159882068634033, Accuracy: 1.0, Computation time: 1.6741209030151367\n",
      "Step: 2175, Loss: 0.9376997351646423, Accuracy: 0.9583333730697632, Computation time: 2.0585453510284424\n",
      "Step: 2176, Loss: 0.9159889817237854, Accuracy: 1.0, Computation time: 1.7869975566864014\n",
      "Step: 2177, Loss: 0.9161121249198914, Accuracy: 1.0, Computation time: 1.6738414764404297\n",
      "Step: 2178, Loss: 0.9163108468055725, Accuracy: 1.0, Computation time: 1.7408897876739502\n",
      "Step: 2179, Loss: 0.9159582257270813, Accuracy: 1.0, Computation time: 1.6591086387634277\n",
      "Step: 2180, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 1.7417783737182617\n",
      "Step: 2181, Loss: 0.9159941077232361, Accuracy: 1.0, Computation time: 1.6138241291046143\n",
      "Step: 2182, Loss: 0.959040105342865, Accuracy: 0.9529914855957031, Computation time: 1.4377453327178955\n",
      "Step: 2183, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 1.5443687438964844\n",
      "Step: 2184, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 1.361166000366211\n",
      "Step: 2185, Loss: 0.9187502264976501, Accuracy: 1.0, Computation time: 2.7246928215026855\n",
      "Step: 2186, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 1.4593119621276855\n",
      "Step: 2187, Loss: 0.9159873723983765, Accuracy: 1.0, Computation time: 1.5215392112731934\n",
      "Step: 2188, Loss: 0.9160894751548767, Accuracy: 1.0, Computation time: 1.7611572742462158\n",
      "Step: 2189, Loss: 0.9256950616836548, Accuracy: 0.9750000238418579, Computation time: 2.327915906906128\n",
      "Step: 2190, Loss: 0.9322874546051025, Accuracy: 0.9772727489471436, Computation time: 2.070697069168091\n",
      "Step: 2191, Loss: 0.9159594178199768, Accuracy: 1.0, Computation time: 1.567565679550171\n",
      "Step: 2192, Loss: 0.9160793423652649, Accuracy: 1.0, Computation time: 1.4505057334899902\n",
      "Step: 2193, Loss: 0.9161238670349121, Accuracy: 1.0, Computation time: 1.3022160530090332\n",
      "Step: 2194, Loss: 0.9161271452903748, Accuracy: 1.0, Computation time: 1.5734031200408936\n",
      "Step: 2195, Loss: 0.9160614609718323, Accuracy: 1.0, Computation time: 1.3757026195526123\n",
      "Step: 2196, Loss: 0.9188306927680969, Accuracy: 1.0, Computation time: 1.4757041931152344\n",
      "Step: 2197, Loss: 0.9366719722747803, Accuracy: 0.96875, Computation time: 1.5000278949737549\n",
      "Step: 2198, Loss: 0.9160524606704712, Accuracy: 1.0, Computation time: 1.3314507007598877\n",
      "Step: 2199, Loss: 0.9160165190696716, Accuracy: 1.0, Computation time: 1.2203855514526367\n",
      "Step: 2200, Loss: 0.9159738421440125, Accuracy: 1.0, Computation time: 1.202624797821045\n",
      "Step: 2201, Loss: 0.9159368872642517, Accuracy: 1.0, Computation time: 1.3524372577667236\n",
      "Step: 2202, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.2646756172180176\n",
      "Step: 2203, Loss: 0.9161285758018494, Accuracy: 1.0, Computation time: 1.16453218460083\n",
      "Step: 2204, Loss: 0.9160043001174927, Accuracy: 1.0, Computation time: 1.6962330341339111\n",
      "Step: 2205, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.2707204818725586\n",
      "Step: 2206, Loss: 0.9376600384712219, Accuracy: 0.9772727489471436, Computation time: 1.8759026527404785\n",
      "Step: 2207, Loss: 0.9375699162483215, Accuracy: 0.9772727489471436, Computation time: 1.5790116786956787\n",
      "Step: 2208, Loss: 0.9159651398658752, Accuracy: 1.0, Computation time: 1.3346693515777588\n",
      "Step: 2209, Loss: 0.9161059856414795, Accuracy: 1.0, Computation time: 1.1655900478363037\n",
      "Step: 2210, Loss: 0.9159547686576843, Accuracy: 1.0, Computation time: 1.3673059940338135\n",
      "Step: 2211, Loss: 0.9159777760505676, Accuracy: 1.0, Computation time: 1.5214266777038574\n",
      "Step: 2212, Loss: 0.9253358840942383, Accuracy: 1.0, Computation time: 1.6659460067749023\n",
      "Step: 2213, Loss: 0.9170638918876648, Accuracy: 1.0, Computation time: 1.4028007984161377\n",
      "Step: 2214, Loss: 0.9160376191139221, Accuracy: 1.0, Computation time: 1.2807598114013672\n",
      "Step: 2215, Loss: 0.9572589993476868, Accuracy: 0.9513888955116272, Computation time: 2.0973918437957764\n",
      "Step: 2216, Loss: 0.9367914199829102, Accuracy: 0.9642857313156128, Computation time: 1.2125170230865479\n",
      "Step: 2217, Loss: 0.9182114601135254, Accuracy: 1.0, Computation time: 1.9039900302886963\n",
      "Step: 2218, Loss: 0.9376928210258484, Accuracy: 0.9791666865348816, Computation time: 1.2402055263519287\n",
      "Step: 2219, Loss: 0.9368783235549927, Accuracy: 0.9821428656578064, Computation time: 2.1878764629364014\n",
      "Step: 2220, Loss: 0.9380783438682556, Accuracy: 0.9750000238418579, Computation time: 1.8135929107666016\n",
      "Step: 2221, Loss: 0.9160729646682739, Accuracy: 1.0, Computation time: 1.2954440116882324\n",
      "Step: 2222, Loss: 0.9160449504852295, Accuracy: 1.0, Computation time: 1.2933385372161865\n",
      "Step: 2223, Loss: 0.9375948309898376, Accuracy: 0.9772727489471436, Computation time: 1.1786892414093018\n",
      "########################\n",
      "Test loss: 1.1304948329925537, Test Accuracy_epoch16: 0.6883684992790222\n",
      "########################\n",
      "Step: 2224, Loss: 0.9179263114929199, Accuracy: 1.0, Computation time: 1.4686527252197266\n",
      "Step: 2225, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 1.396228551864624\n",
      "Step: 2226, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 1.2678442001342773\n",
      "Step: 2227, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 1.3138196468353271\n",
      "Step: 2228, Loss: 0.9160267114639282, Accuracy: 1.0, Computation time: 1.749697208404541\n",
      "Step: 2229, Loss: 0.9159826636314392, Accuracy: 1.0, Computation time: 1.2244553565979004\n",
      "Step: 2230, Loss: 0.9266666769981384, Accuracy: 0.9642857313156128, Computation time: 1.324845790863037\n",
      "Step: 2231, Loss: 0.9320901036262512, Accuracy: 0.9791666865348816, Computation time: 1.6461906433105469\n",
      "Step: 2232, Loss: 0.9164432883262634, Accuracy: 1.0, Computation time: 1.1897618770599365\n",
      "Step: 2233, Loss: 0.9160050749778748, Accuracy: 1.0, Computation time: 1.4139769077301025\n",
      "Step: 2234, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 1.5291545391082764\n",
      "Step: 2235, Loss: 0.9161863327026367, Accuracy: 1.0, Computation time: 1.5689032077789307\n",
      "Step: 2236, Loss: 0.9159892201423645, Accuracy: 1.0, Computation time: 1.3023359775543213\n",
      "Step: 2237, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 1.5413949489593506\n",
      "Step: 2238, Loss: 0.9376344680786133, Accuracy: 0.9722222089767456, Computation time: 1.474733591079712\n",
      "Step: 2239, Loss: 0.9179044961929321, Accuracy: 1.0, Computation time: 1.5034031867980957\n",
      "Step: 2240, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.2319581508636475\n",
      "Step: 2241, Loss: 0.9406177997589111, Accuracy: 0.9642857313156128, Computation time: 1.4172682762145996\n",
      "Step: 2242, Loss: 0.9160090088844299, Accuracy: 1.0, Computation time: 1.3321936130523682\n",
      "Step: 2243, Loss: 0.9161544442176819, Accuracy: 1.0, Computation time: 1.458061933517456\n",
      "Step: 2244, Loss: 0.9160569906234741, Accuracy: 1.0, Computation time: 1.3417866230010986\n",
      "Step: 2245, Loss: 0.9161451458930969, Accuracy: 1.0, Computation time: 1.4765207767486572\n",
      "Step: 2246, Loss: 0.916147768497467, Accuracy: 1.0, Computation time: 1.4591701030731201\n",
      "Step: 2247, Loss: 0.9378944635391235, Accuracy: 0.9750000238418579, Computation time: 1.0783140659332275\n",
      "Step: 2248, Loss: 0.9160069823265076, Accuracy: 1.0, Computation time: 1.3226165771484375\n",
      "Step: 2249, Loss: 0.915958046913147, Accuracy: 1.0, Computation time: 1.1705067157745361\n",
      "Step: 2250, Loss: 0.916121780872345, Accuracy: 1.0, Computation time: 1.3565056324005127\n",
      "Step: 2251, Loss: 0.9388807415962219, Accuracy: 0.9821428656578064, Computation time: 1.8408496379852295\n",
      "Step: 2252, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 1.4154887199401855\n",
      "Step: 2253, Loss: 0.9372173547744751, Accuracy: 0.9791666865348816, Computation time: 1.3369481563568115\n",
      "Step: 2254, Loss: 0.9165067076683044, Accuracy: 1.0, Computation time: 1.278808832168579\n",
      "Step: 2255, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.2918498516082764\n",
      "Step: 2256, Loss: 0.9377890825271606, Accuracy: 0.949999988079071, Computation time: 1.5197038650512695\n",
      "Step: 2257, Loss: 0.9163753986358643, Accuracy: 1.0, Computation time: 1.996349573135376\n",
      "Step: 2258, Loss: 0.9402924180030823, Accuracy: 0.9791666865348816, Computation time: 1.4695079326629639\n",
      "Step: 2259, Loss: 0.9333934783935547, Accuracy: 0.9791666865348816, Computation time: 2.232874631881714\n",
      "Step: 2260, Loss: 0.915932834148407, Accuracy: 1.0, Computation time: 1.518932580947876\n",
      "Step: 2261, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.0922324657440186\n",
      "Step: 2262, Loss: 0.9163370132446289, Accuracy: 1.0, Computation time: 1.3216004371643066\n",
      "Step: 2263, Loss: 0.9160141348838806, Accuracy: 1.0, Computation time: 1.7048678398132324\n",
      "Step: 2264, Loss: 0.9160101413726807, Accuracy: 1.0, Computation time: 1.2175686359405518\n",
      "Step: 2265, Loss: 0.9376492500305176, Accuracy: 0.9772727489471436, Computation time: 1.6660869121551514\n",
      "Step: 2266, Loss: 0.9161432981491089, Accuracy: 1.0, Computation time: 1.3292758464813232\n",
      "Step: 2267, Loss: 0.9163432717323303, Accuracy: 1.0, Computation time: 2.099142551422119\n",
      "Step: 2268, Loss: 0.9159181714057922, Accuracy: 1.0, Computation time: 1.506892204284668\n",
      "Step: 2269, Loss: 0.9159073233604431, Accuracy: 1.0, Computation time: 1.3123104572296143\n",
      "Step: 2270, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 1.2606542110443115\n",
      "Step: 2271, Loss: 0.9160146117210388, Accuracy: 1.0, Computation time: 1.450556993484497\n",
      "Step: 2272, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 1.3274083137512207\n",
      "Step: 2273, Loss: 0.9322600960731506, Accuracy: 0.9583333730697632, Computation time: 2.303441286087036\n",
      "Step: 2274, Loss: 0.9330976009368896, Accuracy: 0.9722222089767456, Computation time: 1.4915814399719238\n",
      "Step: 2275, Loss: 0.9161970615386963, Accuracy: 1.0, Computation time: 1.4997847080230713\n",
      "Step: 2276, Loss: 0.9373347759246826, Accuracy: 0.949999988079071, Computation time: 1.484649658203125\n",
      "Step: 2277, Loss: 0.916488528251648, Accuracy: 1.0, Computation time: 1.7311985492706299\n",
      "Step: 2278, Loss: 0.9165029525756836, Accuracy: 1.0, Computation time: 1.6149499416351318\n",
      "Step: 2279, Loss: 0.9159986972808838, Accuracy: 1.0, Computation time: 1.9820592403411865\n",
      "Step: 2280, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 2.0623788833618164\n",
      "Step: 2281, Loss: 0.9159916639328003, Accuracy: 1.0, Computation time: 1.6705129146575928\n",
      "Step: 2282, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.8324971199035645\n",
      "Step: 2283, Loss: 0.9511721730232239, Accuracy: 0.9513888955116272, Computation time: 1.8867394924163818\n",
      "Step: 2284, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 1.4888722896575928\n",
      "Step: 2285, Loss: 0.9162624478340149, Accuracy: 1.0, Computation time: 1.6187069416046143\n",
      "Step: 2286, Loss: 0.9160685539245605, Accuracy: 1.0, Computation time: 1.8846230506896973\n",
      "Step: 2287, Loss: 0.916031539440155, Accuracy: 1.0, Computation time: 1.717592477798462\n",
      "Step: 2288, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 1.8665306568145752\n",
      "Step: 2289, Loss: 0.9161033630371094, Accuracy: 1.0, Computation time: 1.7157189846038818\n",
      "Step: 2290, Loss: 0.9161266088485718, Accuracy: 1.0, Computation time: 1.495197057723999\n",
      "Step: 2291, Loss: 0.9160277843475342, Accuracy: 1.0, Computation time: 1.7941157817840576\n",
      "Step: 2292, Loss: 0.9160990118980408, Accuracy: 1.0, Computation time: 1.5283033847808838\n",
      "Step: 2293, Loss: 0.9160791635513306, Accuracy: 1.0, Computation time: 1.9434099197387695\n",
      "Step: 2294, Loss: 0.9159471392631531, Accuracy: 1.0, Computation time: 1.6027464866638184\n",
      "Step: 2295, Loss: 0.937788188457489, Accuracy: 0.9750000238418579, Computation time: 1.5533421039581299\n",
      "Step: 2296, Loss: 0.9159421920776367, Accuracy: 1.0, Computation time: 2.453113317489624\n",
      "Step: 2297, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.7662177085876465\n",
      "Step: 2298, Loss: 0.945021390914917, Accuracy: 0.9750000238418579, Computation time: 2.905364990234375\n",
      "Step: 2299, Loss: 0.9159366488456726, Accuracy: 1.0, Computation time: 1.635082483291626\n",
      "Step: 2300, Loss: 0.9160309433937073, Accuracy: 1.0, Computation time: 1.875777006149292\n",
      "Step: 2301, Loss: 0.9376075267791748, Accuracy: 0.9722222089767456, Computation time: 1.3827276229858398\n",
      "Step: 2302, Loss: 0.9159601330757141, Accuracy: 1.0, Computation time: 1.9944632053375244\n",
      "Step: 2303, Loss: 0.9197520017623901, Accuracy: 1.0, Computation time: 2.0514886379241943\n",
      "Step: 2304, Loss: 0.9159832000732422, Accuracy: 1.0, Computation time: 1.6439218521118164\n",
      "Step: 2305, Loss: 0.9334526062011719, Accuracy: 0.96875, Computation time: 1.748619556427002\n",
      "Step: 2306, Loss: 0.9160975217819214, Accuracy: 1.0, Computation time: 1.7177071571350098\n",
      "Step: 2307, Loss: 0.9218665361404419, Accuracy: 1.0, Computation time: 1.655791997909546\n",
      "Step: 2308, Loss: 0.9377421736717224, Accuracy: 0.9772727489471436, Computation time: 1.2977285385131836\n",
      "Step: 2309, Loss: 0.9160298705101013, Accuracy: 1.0, Computation time: 1.3770859241485596\n",
      "Step: 2310, Loss: 0.9228300452232361, Accuracy: nan, Computation time: 1.462599754333496\n",
      "Step: 2311, Loss: 0.9376723766326904, Accuracy: 0.9833333492279053, Computation time: 1.3941853046417236\n",
      "Step: 2312, Loss: 0.9159991145133972, Accuracy: 1.0, Computation time: 1.3476929664611816\n",
      "Step: 2313, Loss: 0.9161763787269592, Accuracy: 1.0, Computation time: 1.5230374336242676\n",
      "Step: 2314, Loss: 0.937629759311676, Accuracy: 0.9583333730697632, Computation time: 1.7983016967773438\n",
      "Step: 2315, Loss: 0.916158139705658, Accuracy: 1.0, Computation time: 1.6995930671691895\n",
      "Step: 2316, Loss: 0.9366422295570374, Accuracy: 0.9722222089767456, Computation time: 1.584536075592041\n",
      "Step: 2317, Loss: 0.9160835146903992, Accuracy: 1.0, Computation time: 1.8769550323486328\n",
      "Step: 2318, Loss: 0.9309883713722229, Accuracy: 0.9722222089767456, Computation time: 1.731985092163086\n",
      "Step: 2319, Loss: 0.9159833192825317, Accuracy: 1.0, Computation time: 1.702906847000122\n",
      "Step: 2320, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 1.4617681503295898\n",
      "Step: 2321, Loss: 0.9159541726112366, Accuracy: 1.0, Computation time: 1.8964111804962158\n",
      "Step: 2322, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.7451586723327637\n",
      "Step: 2323, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 1.2861683368682861\n",
      "Step: 2324, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 1.4918293952941895\n",
      "Step: 2325, Loss: 0.9166831374168396, Accuracy: 1.0, Computation time: 1.552570104598999\n",
      "Step: 2326, Loss: 0.9342324137687683, Accuracy: 0.9722222089767456, Computation time: 1.6806681156158447\n",
      "Step: 2327, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 1.7643752098083496\n",
      "Step: 2328, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.5072999000549316\n",
      "Step: 2329, Loss: 0.9375322461128235, Accuracy: 0.96875, Computation time: 1.783057451248169\n",
      "Step: 2330, Loss: 0.9368289113044739, Accuracy: 0.9821428656578064, Computation time: 1.8509349822998047\n",
      "Step: 2331, Loss: 0.9351434707641602, Accuracy: 0.9772727489471436, Computation time: 2.181511878967285\n",
      "Step: 2332, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 1.3394229412078857\n",
      "Step: 2333, Loss: 0.9363568425178528, Accuracy: 0.9722222089767456, Computation time: 1.720916748046875\n",
      "Step: 2334, Loss: 0.915942907333374, Accuracy: 1.0, Computation time: 1.5040345191955566\n",
      "Step: 2335, Loss: 0.9160932302474976, Accuracy: 1.0, Computation time: 1.3132431507110596\n",
      "Step: 2336, Loss: 0.9159796237945557, Accuracy: 1.0, Computation time: 1.332040548324585\n",
      "Step: 2337, Loss: 0.9159922003746033, Accuracy: 1.0, Computation time: 1.2722618579864502\n",
      "Step: 2338, Loss: 0.9161950349807739, Accuracy: 1.0, Computation time: 1.3852746486663818\n",
      "Step: 2339, Loss: 0.9160463213920593, Accuracy: 1.0, Computation time: 1.502378225326538\n",
      "Step: 2340, Loss: 0.9227627515792847, Accuracy: 1.0, Computation time: 1.9446930885314941\n",
      "Step: 2341, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.6154961585998535\n",
      "Step: 2342, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 1.3515939712524414\n",
      "Step: 2343, Loss: 0.94110506772995, Accuracy: 0.9772727489471436, Computation time: 1.6060967445373535\n",
      "Step: 2344, Loss: 0.9238287806510925, Accuracy: 1.0, Computation time: 1.495995044708252\n",
      "Step: 2345, Loss: 0.9227760434150696, Accuracy: 1.0, Computation time: 1.5302305221557617\n",
      "Step: 2346, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 1.7061665058135986\n",
      "Step: 2347, Loss: 0.9159759879112244, Accuracy: 1.0, Computation time: 1.4649713039398193\n",
      "Step: 2348, Loss: 0.9367778301239014, Accuracy: 0.96875, Computation time: 2.1055476665496826\n",
      "Step: 2349, Loss: 0.9162916541099548, Accuracy: 1.0, Computation time: 1.6437194347381592\n",
      "Step: 2350, Loss: 0.9168744683265686, Accuracy: 1.0, Computation time: 1.865706205368042\n",
      "Step: 2351, Loss: 0.9162508249282837, Accuracy: 1.0, Computation time: 1.705094337463379\n",
      "Step: 2352, Loss: 0.9237764477729797, Accuracy: 1.0, Computation time: 1.7027373313903809\n",
      "Step: 2353, Loss: 0.9159782528877258, Accuracy: 1.0, Computation time: 1.7246215343475342\n",
      "Step: 2354, Loss: 0.9159682989120483, Accuracy: 1.0, Computation time: 1.605454683303833\n",
      "Step: 2355, Loss: 0.9159806966781616, Accuracy: 1.0, Computation time: 1.7763087749481201\n",
      "Step: 2356, Loss: 0.9159601330757141, Accuracy: 1.0, Computation time: 1.5123400688171387\n",
      "Step: 2357, Loss: 0.9235866069793701, Accuracy: 1.0, Computation time: 1.8648207187652588\n",
      "Step: 2358, Loss: 0.9160059690475464, Accuracy: nan, Computation time: 1.4659738540649414\n",
      "Step: 2359, Loss: 0.9379237294197083, Accuracy: 0.984375, Computation time: 2.054887294769287\n",
      "Step: 2360, Loss: 0.9161444306373596, Accuracy: 1.0, Computation time: 1.5506188869476318\n",
      "Step: 2361, Loss: 0.9278915524482727, Accuracy: 0.949999988079071, Computation time: 2.2163031101226807\n",
      "Step: 2362, Loss: 0.9160927534103394, Accuracy: 1.0, Computation time: 1.8591198921203613\n",
      "########################\n",
      "Test loss: 1.1183079481124878, Test Accuracy_epoch17: 0.7057716846466064\n",
      "########################\n",
      "Step: 2363, Loss: 0.9163886308670044, Accuracy: 1.0, Computation time: 2.0620410442352295\n",
      "Step: 2364, Loss: 0.9161738157272339, Accuracy: 1.0, Computation time: 1.776108980178833\n",
      "Step: 2365, Loss: 0.9161605834960938, Accuracy: 1.0, Computation time: 1.6811914443969727\n",
      "Step: 2366, Loss: 0.9159958362579346, Accuracy: 1.0, Computation time: 1.5706086158752441\n",
      "Step: 2367, Loss: 0.9169007539749146, Accuracy: 1.0, Computation time: 1.7682883739471436\n",
      "Step: 2368, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 1.8609046936035156\n",
      "Step: 2369, Loss: 0.9162760972976685, Accuracy: 1.0, Computation time: 1.7537376880645752\n",
      "Step: 2370, Loss: 0.9173627495765686, Accuracy: 1.0, Computation time: 2.2061362266540527\n",
      "Step: 2371, Loss: 0.9160031676292419, Accuracy: 1.0, Computation time: 1.881530523300171\n",
      "Step: 2372, Loss: 0.9382160305976868, Accuracy: 0.9821428656578064, Computation time: 2.0305497646331787\n",
      "Step: 2373, Loss: 0.9162086248397827, Accuracy: 1.0, Computation time: 1.7195663452148438\n",
      "Step: 2374, Loss: 0.9160321950912476, Accuracy: 1.0, Computation time: 1.736250638961792\n",
      "Step: 2375, Loss: 0.9160051941871643, Accuracy: 1.0, Computation time: 1.7189793586730957\n",
      "Step: 2376, Loss: 0.915979266166687, Accuracy: 1.0, Computation time: 1.8408799171447754\n",
      "Step: 2377, Loss: 0.9160183072090149, Accuracy: 1.0, Computation time: 1.8057458400726318\n",
      "Step: 2378, Loss: 0.9364389777183533, Accuracy: 0.9750000238418579, Computation time: 2.057779550552368\n",
      "Step: 2379, Loss: 0.9160163998603821, Accuracy: 1.0, Computation time: 1.5925168991088867\n",
      "Step: 2380, Loss: 0.9376392960548401, Accuracy: 0.9642857313156128, Computation time: 1.5187246799468994\n",
      "Step: 2381, Loss: 0.9161415100097656, Accuracy: 1.0, Computation time: 1.7944154739379883\n",
      "Step: 2382, Loss: 0.9161773920059204, Accuracy: 1.0, Computation time: 1.680511236190796\n",
      "Step: 2383, Loss: 0.9170058965682983, Accuracy: 1.0, Computation time: 2.4104230403900146\n",
      "Step: 2384, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 1.8251404762268066\n",
      "Step: 2385, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.813267707824707\n",
      "Step: 2386, Loss: 0.9160636067390442, Accuracy: 1.0, Computation time: 2.2213337421417236\n",
      "Step: 2387, Loss: 0.9160299897193909, Accuracy: 1.0, Computation time: 2.0898449420928955\n",
      "Step: 2388, Loss: 0.9372053146362305, Accuracy: 0.9750000238418579, Computation time: 2.32993483543396\n",
      "Step: 2389, Loss: 0.9159332513809204, Accuracy: 1.0, Computation time: 1.5933167934417725\n",
      "Step: 2390, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 1.9054827690124512\n",
      "Step: 2391, Loss: 0.925144612789154, Accuracy: 1.0, Computation time: 2.0078091621398926\n",
      "Step: 2392, Loss: 0.9376181364059448, Accuracy: 0.96875, Computation time: 1.731459617614746\n",
      "Step: 2393, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.5915992259979248\n",
      "Step: 2394, Loss: 0.9364427328109741, Accuracy: 0.9642857313156128, Computation time: 1.8281099796295166\n",
      "Step: 2395, Loss: 0.9160653352737427, Accuracy: 1.0, Computation time: 2.1698029041290283\n",
      "Step: 2396, Loss: 0.916472315788269, Accuracy: 1.0, Computation time: 2.2532613277435303\n",
      "Step: 2397, Loss: 0.9177186489105225, Accuracy: 1.0, Computation time: 1.9005560874938965\n",
      "Step: 2398, Loss: 0.937796950340271, Accuracy: 0.9791666865348816, Computation time: 2.459559917449951\n",
      "Step: 2399, Loss: 0.9159331321716309, Accuracy: 1.0, Computation time: 1.8561930656433105\n",
      "Step: 2400, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.790174961090088\n",
      "Step: 2401, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 1.7025649547576904\n",
      "Step: 2402, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.666412591934204\n",
      "Step: 2403, Loss: 0.9373533129692078, Accuracy: 0.9722222089767456, Computation time: 1.8397140502929688\n",
      "Step: 2404, Loss: 0.9373326301574707, Accuracy: 0.949999988079071, Computation time: 1.536956787109375\n",
      "Step: 2405, Loss: 0.9159277081489563, Accuracy: 1.0, Computation time: 2.0136868953704834\n",
      "Step: 2406, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 1.9041728973388672\n",
      "Step: 2407, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.5445635318756104\n",
      "Step: 2408, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.9314935207366943\n",
      "Step: 2409, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 1.8512258529663086\n",
      "Step: 2410, Loss: 0.9161422252655029, Accuracy: 1.0, Computation time: 1.9341764450073242\n",
      "Step: 2411, Loss: 0.9164454340934753, Accuracy: 1.0, Computation time: 1.6066102981567383\n",
      "Step: 2412, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.89821195602417\n",
      "Step: 2413, Loss: 0.9291892647743225, Accuracy: 0.9772727489471436, Computation time: 1.625124216079712\n",
      "Step: 2414, Loss: 0.9161641597747803, Accuracy: 1.0, Computation time: 1.5924530029296875\n",
      "Step: 2415, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.648390293121338\n",
      "Step: 2416, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.4194726943969727\n",
      "Step: 2417, Loss: 0.9454507827758789, Accuracy: 0.9791666865348816, Computation time: 2.1781861782073975\n",
      "Step: 2418, Loss: 0.9159791469573975, Accuracy: 1.0, Computation time: 1.5812878608703613\n",
      "Step: 2419, Loss: 0.9160257577896118, Accuracy: 1.0, Computation time: 1.4080619812011719\n",
      "Step: 2420, Loss: 0.935481607913971, Accuracy: 0.9642857313156128, Computation time: 1.5780837535858154\n",
      "Step: 2421, Loss: 0.9379164576530457, Accuracy: 0.9821428656578064, Computation time: 1.5355010032653809\n",
      "Step: 2422, Loss: 0.9166755080223083, Accuracy: 1.0, Computation time: 1.751028299331665\n",
      "Step: 2423, Loss: 0.9161319136619568, Accuracy: 1.0, Computation time: 1.3137495517730713\n",
      "Step: 2424, Loss: 0.9183635711669922, Accuracy: 1.0, Computation time: 2.077773332595825\n",
      "Step: 2425, Loss: 0.9376939535140991, Accuracy: 0.9791666865348816, Computation time: 1.3597192764282227\n",
      "Step: 2426, Loss: 0.927833080291748, Accuracy: 0.9807692766189575, Computation time: 1.54632568359375\n",
      "Step: 2427, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 1.4079782962799072\n",
      "Step: 2428, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.3569822311401367\n",
      "Step: 2429, Loss: 0.9198947548866272, Accuracy: 1.0, Computation time: 2.6390695571899414\n",
      "Step: 2430, Loss: 0.9422897696495056, Accuracy: 0.96875, Computation time: 2.0359091758728027\n",
      "Step: 2431, Loss: 0.9338208436965942, Accuracy: 0.9772727489471436, Computation time: 1.6189179420471191\n",
      "Step: 2432, Loss: 0.9160839319229126, Accuracy: 1.0, Computation time: 1.4870929718017578\n",
      "Step: 2433, Loss: 0.9541014432907104, Accuracy: 0.9444444179534912, Computation time: 1.8730859756469727\n",
      "Step: 2434, Loss: 0.9183035492897034, Accuracy: 1.0, Computation time: 1.5541408061981201\n",
      "Step: 2435, Loss: 0.9160553812980652, Accuracy: 1.0, Computation time: 1.9822578430175781\n",
      "Step: 2436, Loss: 0.9175612330436707, Accuracy: 1.0, Computation time: 1.547919750213623\n",
      "Step: 2437, Loss: 0.9171147346496582, Accuracy: 1.0, Computation time: 1.68282151222229\n",
      "Step: 2438, Loss: 0.9160095453262329, Accuracy: 1.0, Computation time: 1.5488739013671875\n",
      "Step: 2439, Loss: 0.937687337398529, Accuracy: 0.9750000238418579, Computation time: 1.6757545471191406\n",
      "Step: 2440, Loss: 0.9159641861915588, Accuracy: 1.0, Computation time: 1.4519190788269043\n",
      "Step: 2441, Loss: 0.9172612428665161, Accuracy: 1.0, Computation time: 1.7221958637237549\n",
      "Step: 2442, Loss: 0.9160448312759399, Accuracy: 1.0, Computation time: 2.147981882095337\n",
      "Step: 2443, Loss: 0.9368817210197449, Accuracy: 0.9821428656578064, Computation time: 1.5556857585906982\n",
      "Step: 2444, Loss: 0.9159823060035706, Accuracy: 1.0, Computation time: 1.526789903640747\n",
      "Step: 2445, Loss: 0.9161905646324158, Accuracy: 1.0, Computation time: 1.729816198348999\n",
      "Step: 2446, Loss: 0.9160681366920471, Accuracy: 1.0, Computation time: 1.725722312927246\n",
      "Step: 2447, Loss: 0.9385490417480469, Accuracy: 0.9791666865348816, Computation time: 1.8224036693572998\n",
      "Step: 2448, Loss: 0.935547411441803, Accuracy: 0.9722222089767456, Computation time: 1.7237062454223633\n",
      "Step: 2449, Loss: 0.9190109968185425, Accuracy: 1.0, Computation time: 1.8444452285766602\n",
      "Step: 2450, Loss: 0.916075587272644, Accuracy: 1.0, Computation time: 1.5715954303741455\n",
      "Step: 2451, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.8739805221557617\n",
      "Step: 2452, Loss: 0.9305731058120728, Accuracy: 0.9750000238418579, Computation time: 2.1260101795196533\n",
      "Step: 2453, Loss: 0.9161030054092407, Accuracy: 1.0, Computation time: 1.4354920387268066\n",
      "Step: 2454, Loss: 0.9171432256698608, Accuracy: 1.0, Computation time: 1.9108881950378418\n",
      "Step: 2455, Loss: 0.9359349608421326, Accuracy: 0.9642857313156128, Computation time: 1.7382709980010986\n",
      "Step: 2456, Loss: 0.9171264171600342, Accuracy: 1.0, Computation time: 1.878133773803711\n",
      "Step: 2457, Loss: 0.916062593460083, Accuracy: 1.0, Computation time: 1.5615336894989014\n",
      "Step: 2458, Loss: 0.9305738210678101, Accuracy: 0.9642857313156128, Computation time: 1.979665994644165\n",
      "Step: 2459, Loss: 0.9162908792495728, Accuracy: 1.0, Computation time: 1.4978735446929932\n",
      "Step: 2460, Loss: 0.920756459236145, Accuracy: 1.0, Computation time: 1.8236348628997803\n",
      "Step: 2461, Loss: 0.9384125471115112, Accuracy: 0.9772727489471436, Computation time: 1.610445499420166\n",
      "Step: 2462, Loss: 0.9165663719177246, Accuracy: 1.0, Computation time: 1.6938457489013672\n",
      "Step: 2463, Loss: 0.9379212856292725, Accuracy: 0.9642857313156128, Computation time: 1.7017302513122559\n",
      "Step: 2464, Loss: 0.9375839829444885, Accuracy: 0.9807692766189575, Computation time: 2.0676515102386475\n",
      "Step: 2465, Loss: 0.9373936653137207, Accuracy: 0.96875, Computation time: 1.735640287399292\n",
      "Step: 2466, Loss: 0.9160579442977905, Accuracy: 1.0, Computation time: 1.5894510746002197\n",
      "Step: 2467, Loss: 0.9162234663963318, Accuracy: 1.0, Computation time: 1.8768999576568604\n",
      "Step: 2468, Loss: 0.9295186400413513, Accuracy: 0.9583333730697632, Computation time: 1.7312090396881104\n",
      "Step: 2469, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.672701358795166\n",
      "Step: 2470, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 1.9205024242401123\n",
      "Step: 2471, Loss: 0.9159095883369446, Accuracy: 1.0, Computation time: 1.4794623851776123\n",
      "Step: 2472, Loss: 0.919979453086853, Accuracy: 1.0, Computation time: 1.93137788772583\n",
      "Step: 2473, Loss: 0.9187950491905212, Accuracy: 1.0, Computation time: 1.686147928237915\n",
      "Step: 2474, Loss: 0.9163451194763184, Accuracy: 1.0, Computation time: 2.270627975463867\n",
      "Step: 2475, Loss: 0.9358899593353271, Accuracy: 0.9750000238418579, Computation time: 2.2717466354370117\n",
      "Step: 2476, Loss: 0.9160031676292419, Accuracy: 1.0, Computation time: 2.0235419273376465\n",
      "Step: 2477, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.6703081130981445\n",
      "Step: 2478, Loss: 0.9366022348403931, Accuracy: 0.9807692766189575, Computation time: 1.8007371425628662\n",
      "Step: 2479, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.843749761581421\n",
      "Step: 2480, Loss: 0.937106192111969, Accuracy: 0.949999988079071, Computation time: 1.8887453079223633\n",
      "Step: 2481, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 1.8600835800170898\n",
      "Step: 2482, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.6515448093414307\n",
      "Step: 2483, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 1.6954100131988525\n",
      "Step: 2484, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.8298015594482422\n",
      "Step: 2485, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 2.0315725803375244\n",
      "Step: 2486, Loss: 0.9159868955612183, Accuracy: 1.0, Computation time: 1.576880931854248\n",
      "Step: 2487, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.9406557083129883\n",
      "Step: 2488, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 1.8907625675201416\n",
      "Step: 2489, Loss: 0.9263822436332703, Accuracy: 0.9772727489471436, Computation time: 1.8795998096466064\n",
      "Step: 2490, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.7267286777496338\n",
      "Step: 2491, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 1.6365866661071777\n",
      "Step: 2492, Loss: 0.9161217212677002, Accuracy: 1.0, Computation time: 2.412296772003174\n",
      "Step: 2493, Loss: 0.9160540103912354, Accuracy: 1.0, Computation time: 1.5430700778961182\n",
      "Step: 2494, Loss: 0.9376615285873413, Accuracy: 0.9807692766189575, Computation time: 1.54160475730896\n",
      "Step: 2495, Loss: 0.9378332495689392, Accuracy: 0.9821428656578064, Computation time: 1.526820421218872\n",
      "Step: 2496, Loss: 0.9159830212593079, Accuracy: 1.0, Computation time: 1.543386697769165\n",
      "Step: 2497, Loss: 0.9159870147705078, Accuracy: 1.0, Computation time: 1.3816609382629395\n",
      "Step: 2498, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 1.496377944946289\n",
      "Step: 2499, Loss: 0.9158729910850525, Accuracy: 1.0, Computation time: 1.6147210597991943\n",
      "Step: 2500, Loss: 0.9158607125282288, Accuracy: 1.0, Computation time: 1.7306604385375977\n",
      "Step: 2501, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.5986623764038086\n",
      "########################\n",
      "Test loss: 1.1116394996643066, Test Accuracy_epoch18: 0.7118734121322632\n",
      "########################\n",
      "Step: 2502, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.6285758018493652\n",
      "Step: 2503, Loss: 0.9159677624702454, Accuracy: 1.0, Computation time: 1.6635420322418213\n",
      "Step: 2504, Loss: 0.9161217212677002, Accuracy: 1.0, Computation time: 2.360239267349243\n",
      "Step: 2505, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.684208631515503\n",
      "Step: 2506, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.7709131240844727\n",
      "Step: 2507, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.5907111167907715\n",
      "Step: 2508, Loss: 0.9165589809417725, Accuracy: nan, Computation time: 1.5763566493988037\n",
      "Step: 2509, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 2.178511142730713\n",
      "Step: 2510, Loss: 0.9159767627716064, Accuracy: 1.0, Computation time: 1.5976781845092773\n",
      "Step: 2511, Loss: 0.9249281883239746, Accuracy: 1.0, Computation time: 2.0363454818725586\n",
      "Step: 2512, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 2.0172922611236572\n",
      "Step: 2513, Loss: 0.9225313663482666, Accuracy: 1.0, Computation time: 1.7195944786071777\n",
      "Step: 2514, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.5593008995056152\n",
      "Step: 2515, Loss: 0.9374909996986389, Accuracy: 0.984375, Computation time: 1.694031000137329\n",
      "Step: 2516, Loss: 0.9292322397232056, Accuracy: 0.9722222089767456, Computation time: 2.547942638397217\n",
      "Step: 2517, Loss: 0.9161033034324646, Accuracy: 1.0, Computation time: 1.7505338191986084\n",
      "Step: 2518, Loss: 0.9164051413536072, Accuracy: 1.0, Computation time: 1.548142910003662\n",
      "Step: 2519, Loss: 0.9161280393600464, Accuracy: 1.0, Computation time: 1.5357301235198975\n",
      "Step: 2520, Loss: 0.9160704016685486, Accuracy: 1.0, Computation time: 1.4376351833343506\n",
      "Step: 2521, Loss: 0.9159812927246094, Accuracy: 1.0, Computation time: 1.646855115890503\n",
      "Step: 2522, Loss: 0.9375064373016357, Accuracy: 0.9807692766189575, Computation time: 1.8308231830596924\n",
      "Step: 2523, Loss: 0.9159626960754395, Accuracy: 1.0, Computation time: 1.4421467781066895\n",
      "Step: 2524, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 1.6445190906524658\n",
      "Step: 2525, Loss: 0.9159541726112366, Accuracy: 1.0, Computation time: 1.7850008010864258\n",
      "Step: 2526, Loss: 0.9370545148849487, Accuracy: 0.9791666865348816, Computation time: 2.1449697017669678\n",
      "Step: 2527, Loss: 0.9160692691802979, Accuracy: 1.0, Computation time: 1.9081940650939941\n",
      "Step: 2528, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.7944917678833008\n",
      "Step: 2529, Loss: 0.9166214466094971, Accuracy: 1.0, Computation time: 2.2743890285491943\n",
      "Step: 2530, Loss: 0.9159606099128723, Accuracy: 1.0, Computation time: 1.6879677772521973\n",
      "Step: 2531, Loss: 0.9159056544303894, Accuracy: 1.0, Computation time: 1.3963301181793213\n",
      "Step: 2532, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.399761438369751\n",
      "Step: 2533, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 1.578622817993164\n",
      "Step: 2534, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 1.7973337173461914\n",
      "Step: 2535, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 1.611212968826294\n",
      "Step: 2536, Loss: 0.9159799814224243, Accuracy: 1.0, Computation time: 1.7147436141967773\n",
      "Step: 2537, Loss: 0.9375814199447632, Accuracy: 0.9807692766189575, Computation time: 1.81181001663208\n",
      "Step: 2538, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.6118988990783691\n",
      "Step: 2539, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.6723332405090332\n",
      "Step: 2540, Loss: 0.9530413150787354, Accuracy: 0.9222221970558167, Computation time: 1.9170870780944824\n",
      "Step: 2541, Loss: 0.9159283638000488, Accuracy: 1.0, Computation time: 2.1214559078216553\n",
      "Step: 2542, Loss: 0.932829737663269, Accuracy: 0.9166666865348816, Computation time: 1.7646543979644775\n",
      "Step: 2543, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 2.0983970165252686\n",
      "Step: 2544, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.991136074066162\n",
      "Step: 2545, Loss: 0.959302544593811, Accuracy: 0.9494949579238892, Computation time: 1.598559856414795\n",
      "Step: 2546, Loss: 0.937553882598877, Accuracy: 0.96875, Computation time: 1.7796595096588135\n",
      "Step: 2547, Loss: 0.9196091890335083, Accuracy: 1.0, Computation time: 1.7365036010742188\n",
      "Step: 2548, Loss: 0.9582442045211792, Accuracy: 0.935606062412262, Computation time: 1.8306825160980225\n",
      "Step: 2549, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.4004795551300049\n",
      "Step: 2550, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.8797998428344727\n",
      "Step: 2551, Loss: 0.9368276596069336, Accuracy: 0.9722222089767456, Computation time: 1.8900573253631592\n",
      "Step: 2552, Loss: 0.916005551815033, Accuracy: 1.0, Computation time: 1.3962066173553467\n",
      "Step: 2553, Loss: 0.9161447286605835, Accuracy: 1.0, Computation time: 1.4837634563446045\n",
      "Step: 2554, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 1.855700969696045\n",
      "Step: 2555, Loss: 0.9167817831039429, Accuracy: 1.0, Computation time: 2.48439359664917\n",
      "Step: 2556, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.665849208831787\n",
      "Step: 2557, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.8274257183074951\n",
      "Step: 2558, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 1.6701710224151611\n",
      "Step: 2559, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.4690492153167725\n",
      "Step: 2560, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.67435622215271\n",
      "Step: 2561, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.767467737197876\n",
      "Step: 2562, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.826073169708252\n",
      "Step: 2563, Loss: 0.9188418388366699, Accuracy: 1.0, Computation time: 1.582228183746338\n",
      "Step: 2564, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 2.0535688400268555\n",
      "Step: 2565, Loss: 0.9375389218330383, Accuracy: 0.949999988079071, Computation time: 1.65952467918396\n",
      "Step: 2566, Loss: 0.91631680727005, Accuracy: 1.0, Computation time: 1.6806905269622803\n",
      "Step: 2567, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.8946630954742432\n",
      "Step: 2568, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.4598212242126465\n",
      "Step: 2569, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.5585565567016602\n",
      "Step: 2570, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.556077480316162\n",
      "Step: 2571, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.5098402500152588\n",
      "Step: 2572, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.6464221477508545\n",
      "Step: 2573, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.7855424880981445\n",
      "Step: 2574, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 1.5487778186798096\n",
      "Step: 2575, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.8617384433746338\n",
      "Step: 2576, Loss: 0.9801648855209351, Accuracy: 0.9500000476837158, Computation time: 1.2823007106781006\n",
      "Step: 2577, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 1.7567479610443115\n",
      "Step: 2578, Loss: 0.9189117550849915, Accuracy: 1.0, Computation time: 1.7116270065307617\n",
      "Step: 2579, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.5598411560058594\n",
      "Step: 2580, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.7772257328033447\n",
      "Step: 2581, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.8563849925994873\n",
      "Step: 2582, Loss: 0.9183908700942993, Accuracy: 1.0, Computation time: 1.4766480922698975\n",
      "Step: 2583, Loss: 0.9159813523292542, Accuracy: 1.0, Computation time: 1.7210757732391357\n",
      "Step: 2584, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 1.6582612991333008\n",
      "Step: 2585, Loss: 0.9159070253372192, Accuracy: 1.0, Computation time: 1.4063303470611572\n",
      "Step: 2586, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.6056926250457764\n",
      "Step: 2587, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.707892894744873\n",
      "Step: 2588, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.7554068565368652\n",
      "Step: 2589, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.7952601909637451\n",
      "Step: 2590, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.8340413570404053\n",
      "Step: 2591, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.5057053565979004\n",
      "Step: 2592, Loss: 0.9158458113670349, Accuracy: 1.0, Computation time: 1.5468924045562744\n",
      "Step: 2593, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.7085354328155518\n",
      "Step: 2594, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 1.7115604877471924\n",
      "Step: 2595, Loss: 0.9163918495178223, Accuracy: 1.0, Computation time: 1.7552378177642822\n",
      "Step: 2596, Loss: 0.9159673452377319, Accuracy: 1.0, Computation time: 1.6147141456604004\n",
      "Step: 2597, Loss: 0.9375668168067932, Accuracy: 0.9791666865348816, Computation time: 1.6962828636169434\n",
      "Step: 2598, Loss: 0.916190505027771, Accuracy: 1.0, Computation time: 1.721736192703247\n",
      "Step: 2599, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.4847288131713867\n",
      "Step: 2600, Loss: 0.915908694267273, Accuracy: 1.0, Computation time: 1.6515471935272217\n",
      "Step: 2601, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.665280818939209\n",
      "Step: 2602, Loss: 0.9158739447593689, Accuracy: 1.0, Computation time: 1.243328332901001\n",
      "Step: 2603, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.4929492473602295\n",
      "Step: 2604, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.6735239028930664\n",
      "Step: 2605, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.5496721267700195\n",
      "Step: 2606, Loss: 0.9371315836906433, Accuracy: 0.9583333730697632, Computation time: 1.4436702728271484\n",
      "Step: 2607, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 1.4771392345428467\n",
      "Step: 2608, Loss: 0.9159144163131714, Accuracy: 1.0, Computation time: 1.538773775100708\n",
      "Step: 2609, Loss: 0.9343119263648987, Accuracy: 0.9750000238418579, Computation time: 1.7825019359588623\n",
      "Step: 2610, Loss: 0.9171982407569885, Accuracy: 1.0, Computation time: 2.1619458198547363\n",
      "Step: 2611, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 1.6530399322509766\n",
      "Step: 2612, Loss: 0.9270378947257996, Accuracy: 0.9750000238418579, Computation time: 2.2996535301208496\n",
      "Step: 2613, Loss: 0.9159834980964661, Accuracy: 1.0, Computation time: 1.4401257038116455\n",
      "Step: 2614, Loss: 0.9159179329872131, Accuracy: 1.0, Computation time: 1.5933475494384766\n",
      "Step: 2615, Loss: 0.915974497795105, Accuracy: 1.0, Computation time: 1.5916194915771484\n",
      "Step: 2616, Loss: 0.9169705510139465, Accuracy: 1.0, Computation time: 1.6026341915130615\n",
      "Step: 2617, Loss: 0.9161732196807861, Accuracy: 1.0, Computation time: 1.747783899307251\n",
      "Step: 2618, Loss: 0.9236959218978882, Accuracy: 1.0, Computation time: 1.8039400577545166\n",
      "Step: 2619, Loss: 0.9159736633300781, Accuracy: 1.0, Computation time: 1.5683493614196777\n",
      "Step: 2620, Loss: 0.9272503852844238, Accuracy: 0.9807692766189575, Computation time: 2.1615922451019287\n",
      "Step: 2621, Loss: 0.9161702990531921, Accuracy: 1.0, Computation time: 1.6807358264923096\n",
      "Step: 2622, Loss: 0.9168874621391296, Accuracy: 1.0, Computation time: 1.7659213542938232\n",
      "Step: 2623, Loss: 0.9160135984420776, Accuracy: 1.0, Computation time: 1.4901041984558105\n",
      "Step: 2624, Loss: 0.9534084796905518, Accuracy: 0.8939394354820251, Computation time: 1.7823987007141113\n",
      "Step: 2625, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.5567936897277832\n",
      "Step: 2626, Loss: 0.9169122576713562, Accuracy: 1.0, Computation time: 1.5813758373260498\n",
      "Step: 2627, Loss: 0.9397779703140259, Accuracy: 0.9642857313156128, Computation time: 1.3222103118896484\n",
      "Step: 2628, Loss: 0.9174379706382751, Accuracy: 1.0, Computation time: 1.406679630279541\n",
      "Step: 2629, Loss: 0.9367404580116272, Accuracy: 0.9642857313156128, Computation time: 1.8360595703125\n",
      "Step: 2630, Loss: 0.9164072275161743, Accuracy: 1.0, Computation time: 1.9831862449645996\n",
      "Step: 2631, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 1.5949976444244385\n",
      "Step: 2632, Loss: 0.9583828449249268, Accuracy: 0.9494949579238892, Computation time: 1.5807702541351318\n",
      "Step: 2633, Loss: 0.9160906672477722, Accuracy: 1.0, Computation time: 2.229508638381958\n",
      "Step: 2634, Loss: 0.9197747111320496, Accuracy: 1.0, Computation time: 1.8566675186157227\n",
      "Step: 2635, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.6059291362762451\n",
      "Step: 2636, Loss: 0.9159437417984009, Accuracy: 1.0, Computation time: 1.9931340217590332\n",
      "Step: 2637, Loss: 0.9159769415855408, Accuracy: 1.0, Computation time: 1.7028815746307373\n",
      "Step: 2638, Loss: 0.9159263968467712, Accuracy: nan, Computation time: 1.6025035381317139\n",
      "Step: 2639, Loss: 0.915953516960144, Accuracy: 1.0, Computation time: 1.5251266956329346\n",
      "Step: 2640, Loss: 0.9159566164016724, Accuracy: 1.0, Computation time: 1.5248277187347412\n",
      "########################\n",
      "Test loss: 1.1096869707107544, Test Accuracy_epoch19: 0.7170071005821228\n",
      "########################\n",
      "Step: 2641, Loss: 0.9169453978538513, Accuracy: 1.0, Computation time: 1.636497974395752\n",
      "Step: 2642, Loss: 0.9299080967903137, Accuracy: 0.9722222089767456, Computation time: 2.7948970794677734\n",
      "Step: 2643, Loss: 0.9160444140434265, Accuracy: 1.0, Computation time: 1.5937201976776123\n",
      "Step: 2644, Loss: 0.9168500304222107, Accuracy: 1.0, Computation time: 1.9265425205230713\n",
      "Step: 2645, Loss: 0.9293608069419861, Accuracy: 0.9807692766189575, Computation time: 1.9552998542785645\n",
      "Step: 2646, Loss: 0.9179633855819702, Accuracy: 1.0, Computation time: 2.202427387237549\n",
      "Step: 2647, Loss: 0.938269317150116, Accuracy: 0.96875, Computation time: 1.7449591159820557\n",
      "Step: 2648, Loss: 0.9305877685546875, Accuracy: 0.96875, Computation time: 1.917834758758545\n",
      "Step: 2649, Loss: 0.9163801670074463, Accuracy: 1.0, Computation time: 1.8781774044036865\n",
      "Step: 2650, Loss: 0.9163675904273987, Accuracy: 1.0, Computation time: 1.6109263896942139\n",
      "Step: 2651, Loss: 0.9161633253097534, Accuracy: 1.0, Computation time: 1.605722427368164\n",
      "Step: 2652, Loss: 0.9163877964019775, Accuracy: 1.0, Computation time: 2.4703314304351807\n",
      "Step: 2653, Loss: 0.9367325305938721, Accuracy: 0.9791666865348816, Computation time: 1.8312907218933105\n",
      "Step: 2654, Loss: 0.9162155985832214, Accuracy: 1.0, Computation time: 1.983149528503418\n",
      "Step: 2655, Loss: 0.9380584955215454, Accuracy: 0.9722222089767456, Computation time: 2.4505667686462402\n",
      "Step: 2656, Loss: 0.9377564787864685, Accuracy: 0.9750000238418579, Computation time: 2.2450802326202393\n",
      "Step: 2657, Loss: 0.9163450598716736, Accuracy: 1.0, Computation time: 1.882265567779541\n",
      "Step: 2658, Loss: 0.9161946773529053, Accuracy: 1.0, Computation time: 1.9582102298736572\n",
      "Step: 2659, Loss: 0.9162725210189819, Accuracy: 1.0, Computation time: 2.063077211380005\n",
      "Step: 2660, Loss: 0.9160374999046326, Accuracy: 1.0, Computation time: 1.9267382621765137\n",
      "Step: 2661, Loss: 0.9373438358306885, Accuracy: 0.9166666865348816, Computation time: 1.791686773300171\n",
      "Step: 2662, Loss: 0.9166749715805054, Accuracy: 1.0, Computation time: 2.364225387573242\n",
      "Step: 2663, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 2.0798888206481934\n",
      "Step: 2664, Loss: 0.9170230627059937, Accuracy: 1.0, Computation time: 1.652029037475586\n",
      "Step: 2665, Loss: 0.9363026022911072, Accuracy: 0.9821428656578064, Computation time: 1.7041263580322266\n",
      "Step: 2666, Loss: 0.9161322712898254, Accuracy: 1.0, Computation time: 2.2192180156707764\n",
      "Step: 2667, Loss: 0.9167654514312744, Accuracy: 1.0, Computation time: 1.8336474895477295\n",
      "Step: 2668, Loss: 0.9339057803153992, Accuracy: 0.96875, Computation time: 1.571882963180542\n",
      "Step: 2669, Loss: 0.9163825511932373, Accuracy: 1.0, Computation time: 1.4641108512878418\n",
      "Step: 2670, Loss: 0.9161382913589478, Accuracy: 1.0, Computation time: 1.7462122440338135\n",
      "Step: 2671, Loss: 0.9391520023345947, Accuracy: 0.9833333492279053, Computation time: 1.8989217281341553\n",
      "Step: 2672, Loss: 0.9160177111625671, Accuracy: 1.0, Computation time: 1.7652664184570312\n",
      "Step: 2673, Loss: 0.9159974455833435, Accuracy: 1.0, Computation time: 1.8325080871582031\n",
      "Step: 2674, Loss: 0.9348430037498474, Accuracy: 0.9791666865348816, Computation time: 1.745392084121704\n",
      "Step: 2675, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 1.4964661598205566\n",
      "Step: 2676, Loss: 0.9160298705101013, Accuracy: 1.0, Computation time: 1.4928247928619385\n",
      "Step: 2677, Loss: 0.916923999786377, Accuracy: 1.0, Computation time: 1.600862741470337\n",
      "Step: 2678, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.1128511428833008\n",
      "Step: 2679, Loss: 0.9160199165344238, Accuracy: 1.0, Computation time: 1.3489830493927002\n",
      "Step: 2680, Loss: 0.9367594718933105, Accuracy: 0.9772727489471436, Computation time: 1.609422206878662\n",
      "Step: 2681, Loss: 0.9159859418869019, Accuracy: 1.0, Computation time: 1.416412591934204\n",
      "Step: 2682, Loss: 0.9159583449363708, Accuracy: 1.0, Computation time: 1.1206591129302979\n",
      "Step: 2683, Loss: 0.9423524141311646, Accuracy: 0.9807692766189575, Computation time: 1.6403717994689941\n",
      "Step: 2684, Loss: 0.9159056544303894, Accuracy: 1.0, Computation time: 1.2839288711547852\n",
      "Step: 2685, Loss: 0.9514073729515076, Accuracy: 0.8916667103767395, Computation time: 1.5323598384857178\n",
      "Step: 2686, Loss: 0.9175162315368652, Accuracy: 1.0, Computation time: 1.4768633842468262\n",
      "Step: 2687, Loss: 0.9275007843971252, Accuracy: 0.9583333730697632, Computation time: 1.4644546508789062\n",
      "Step: 2688, Loss: 0.9159069657325745, Accuracy: 1.0, Computation time: 1.2837815284729004\n",
      "Step: 2689, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.5964264869689941\n",
      "Step: 2690, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.2372353076934814\n",
      "Step: 2691, Loss: 0.9376695156097412, Accuracy: 0.9791666865348816, Computation time: 1.256026029586792\n",
      "Step: 2692, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.4819896221160889\n",
      "Step: 2693, Loss: 0.9222725629806519, Accuracy: 1.0, Computation time: 1.741445779800415\n",
      "Step: 2694, Loss: 0.9251160621643066, Accuracy: 1.0, Computation time: 1.5760924816131592\n",
      "Step: 2695, Loss: 0.9160347580909729, Accuracy: nan, Computation time: 1.5744144916534424\n",
      "Step: 2696, Loss: 0.9233307242393494, Accuracy: 1.0, Computation time: 1.569366693496704\n",
      "Step: 2697, Loss: 0.9160807728767395, Accuracy: 1.0, Computation time: 1.523496389389038\n",
      "Step: 2698, Loss: 0.9161114692687988, Accuracy: 1.0, Computation time: 1.4910249710083008\n",
      "Step: 2699, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 1.2670037746429443\n",
      "Step: 2700, Loss: 0.9176245927810669, Accuracy: 1.0, Computation time: 1.5103483200073242\n",
      "Step: 2701, Loss: 0.9376950860023499, Accuracy: 0.9750000238418579, Computation time: 1.849614143371582\n",
      "Step: 2702, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.24318265914917\n",
      "Step: 2703, Loss: 0.9173704385757446, Accuracy: 1.0, Computation time: 1.8206026554107666\n",
      "Step: 2704, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.1855778694152832\n",
      "Step: 2705, Loss: 0.9159348607063293, Accuracy: 1.0, Computation time: 1.491502046585083\n",
      "Step: 2706, Loss: 0.9371297359466553, Accuracy: 0.9772727489471436, Computation time: 1.6196186542510986\n",
      "Step: 2707, Loss: 0.9377073645591736, Accuracy: 0.9772727489471436, Computation time: 1.3809738159179688\n",
      "Step: 2708, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 1.5938799381256104\n",
      "Step: 2709, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 1.784618854522705\n",
      "Step: 2710, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.5999250411987305\n",
      "Step: 2711, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.7619585990905762\n",
      "Step: 2712, Loss: 0.9375313520431519, Accuracy: 0.9852941036224365, Computation time: 1.6217143535614014\n",
      "Step: 2713, Loss: 0.9375236630439758, Accuracy: nan, Computation time: 1.3631577491760254\n",
      "Step: 2714, Loss: 0.9163150787353516, Accuracy: 1.0, Computation time: 1.7052254676818848\n",
      "Step: 2715, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 1.8168203830718994\n",
      "Step: 2716, Loss: 0.9373640418052673, Accuracy: 0.9750000238418579, Computation time: 1.8213181495666504\n",
      "Step: 2717, Loss: 0.947209358215332, Accuracy: 0.95941561460495, Computation time: 2.0491530895233154\n",
      "Step: 2718, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.8132102489471436\n",
      "Step: 2719, Loss: 0.9159042239189148, Accuracy: 1.0, Computation time: 1.6398110389709473\n",
      "Step: 2720, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.590033769607544\n",
      "Step: 2721, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.9310340881347656\n",
      "Step: 2722, Loss: 0.9161394238471985, Accuracy: 1.0, Computation time: 1.6626577377319336\n",
      "Step: 2723, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 1.7679259777069092\n",
      "Step: 2724, Loss: 0.9164222478866577, Accuracy: 1.0, Computation time: 1.4498705863952637\n",
      "Step: 2725, Loss: 0.9159470796585083, Accuracy: 1.0, Computation time: 1.3547258377075195\n",
      "Step: 2726, Loss: 0.9159094095230103, Accuracy: 1.0, Computation time: 1.3677325248718262\n",
      "Step: 2727, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.7844836711883545\n",
      "Step: 2728, Loss: 0.9160490036010742, Accuracy: 1.0, Computation time: 1.388274908065796\n",
      "Step: 2729, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 1.5149033069610596\n",
      "Step: 2730, Loss: 0.9162806272506714, Accuracy: 1.0, Computation time: 1.8579027652740479\n",
      "Step: 2731, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 1.5304903984069824\n",
      "Step: 2732, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.8165724277496338\n",
      "Step: 2733, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.144672155380249\n",
      "Step: 2734, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 1.3668437004089355\n",
      "Step: 2735, Loss: 0.9159917831420898, Accuracy: 1.0, Computation time: 1.4051249027252197\n",
      "Step: 2736, Loss: 0.9172139167785645, Accuracy: 1.0, Computation time: 1.6332991123199463\n",
      "Step: 2737, Loss: 0.9373733401298523, Accuracy: 0.9166666865348816, Computation time: 1.7425832748413086\n",
      "Step: 2738, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.5425200462341309\n",
      "Step: 2739, Loss: 0.9160632491111755, Accuracy: 1.0, Computation time: 1.6654655933380127\n",
      "Step: 2740, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.617924451828003\n",
      "Step: 2741, Loss: 0.9351181387901306, Accuracy: 0.9821428656578064, Computation time: 1.8910636901855469\n",
      "Step: 2742, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.4039943218231201\n",
      "Step: 2743, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.3360633850097656\n",
      "Step: 2744, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 1.691490888595581\n",
      "Step: 2745, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.4686927795410156\n",
      "Step: 2746, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.530137062072754\n",
      "Step: 2747, Loss: 0.915976345539093, Accuracy: 1.0, Computation time: 1.746586799621582\n",
      "Step: 2748, Loss: 0.9168665409088135, Accuracy: 1.0, Computation time: 1.5379033088684082\n",
      "Step: 2749, Loss: 0.9211007356643677, Accuracy: 1.0, Computation time: 1.4526257514953613\n",
      "Step: 2750, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.6241397857666016\n",
      "Step: 2751, Loss: 0.9161314964294434, Accuracy: 1.0, Computation time: 1.5575802326202393\n",
      "Step: 2752, Loss: 0.9162906408309937, Accuracy: 1.0, Computation time: 1.7641279697418213\n",
      "Step: 2753, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.7167072296142578\n",
      "Step: 2754, Loss: 0.9590819478034973, Accuracy: 0.859375, Computation time: 1.4027957916259766\n",
      "Step: 2755, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 1.4796051979064941\n",
      "Step: 2756, Loss: 0.926418662071228, Accuracy: 0.9750000238418579, Computation time: 2.1362950801849365\n",
      "Step: 2757, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.4538850784301758\n",
      "Step: 2758, Loss: 0.9161828756332397, Accuracy: 1.0, Computation time: 1.6003227233886719\n",
      "Step: 2759, Loss: 0.9374507665634155, Accuracy: 0.9772727489471436, Computation time: 1.4195938110351562\n",
      "Step: 2760, Loss: 0.9377062916755676, Accuracy: 0.9722222089767456, Computation time: 1.7259776592254639\n",
      "Step: 2761, Loss: 0.9376767873764038, Accuracy: 0.9772727489471436, Computation time: 1.4752159118652344\n",
      "Step: 2762, Loss: 0.9160103797912598, Accuracy: 1.0, Computation time: 1.5266947746276855\n",
      "Step: 2763, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.7083230018615723\n",
      "Step: 2764, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.3478717803955078\n",
      "Step: 2765, Loss: 0.9354101419448853, Accuracy: 0.9833333492279053, Computation time: 1.9967660903930664\n",
      "Step: 2766, Loss: 0.9160768389701843, Accuracy: 1.0, Computation time: 1.3538768291473389\n",
      "Step: 2767, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.5999677181243896\n",
      "Step: 2768, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 1.5385868549346924\n",
      "Step: 2769, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 1.6655001640319824\n",
      "Step: 2770, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.679757833480835\n",
      "Step: 2771, Loss: 0.9170031547546387, Accuracy: 1.0, Computation time: 1.5250754356384277\n",
      "Step: 2772, Loss: 0.9159207344055176, Accuracy: 1.0, Computation time: 1.9725654125213623\n",
      "Step: 2773, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.5246119499206543\n",
      "Step: 2774, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.6774590015411377\n",
      "Step: 2775, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.5588352680206299\n",
      "Step: 2776, Loss: 0.9384888410568237, Accuracy: 0.9821428656578064, Computation time: 1.7190465927124023\n",
      "Step: 2777, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.8105623722076416\n",
      "Step: 2778, Loss: 0.9376565217971802, Accuracy: 0.9772727489471436, Computation time: 1.877880573272705\n",
      "Step: 2779, Loss: 0.9342483282089233, Accuracy: 0.9791666865348816, Computation time: 2.473715305328369\n",
      "########################\n",
      "Test loss: 1.1144976615905762, Test Accuracy_epoch20: 0.7095007300376892\n",
      "########################\n",
      "Step: 2780, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 2.094326972961426\n",
      "Step: 2781, Loss: 0.9159132838249207, Accuracy: 1.0, Computation time: 1.7317421436309814\n",
      "Step: 2782, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.6580965518951416\n",
      "Step: 2783, Loss: 0.9339919686317444, Accuracy: 0.9791666865348816, Computation time: 1.999809980392456\n",
      "Step: 2784, Loss: 0.9378334283828735, Accuracy: 0.9772727489471436, Computation time: 2.124218702316284\n",
      "Step: 2785, Loss: 0.91599041223526, Accuracy: 1.0, Computation time: 1.7321867942810059\n",
      "Step: 2786, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 1.8783152103424072\n",
      "Step: 2787, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 1.5959539413452148\n",
      "Step: 2788, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.6610491275787354\n",
      "Step: 2789, Loss: 0.9181477427482605, Accuracy: 1.0, Computation time: 1.8282859325408936\n",
      "Step: 2790, Loss: 0.936675488948822, Accuracy: 0.9750000238418579, Computation time: 1.708085060119629\n",
      "Step: 2791, Loss: 0.9192095398902893, Accuracy: 1.0, Computation time: 1.9615223407745361\n",
      "Step: 2792, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.4049808979034424\n",
      "Step: 2793, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.476196527481079\n",
      "Step: 2794, Loss: 0.9159253239631653, Accuracy: 1.0, Computation time: 1.4386110305786133\n",
      "Step: 2795, Loss: 0.9159876704216003, Accuracy: 1.0, Computation time: 1.7345435619354248\n",
      "Step: 2796, Loss: 0.9160259366035461, Accuracy: 1.0, Computation time: 1.4768056869506836\n",
      "Step: 2797, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.4140043258666992\n",
      "Step: 2798, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.4578688144683838\n",
      "Step: 2799, Loss: 0.9159280061721802, Accuracy: 1.0, Computation time: 1.4004731178283691\n",
      "Step: 2800, Loss: 0.9215539693832397, Accuracy: 1.0, Computation time: 1.4270095825195312\n",
      "Step: 2801, Loss: 0.9160041213035583, Accuracy: 1.0, Computation time: 1.4287621974945068\n",
      "Step: 2802, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.489412546157837\n",
      "Step: 2803, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.5337798595428467\n",
      "Step: 2804, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.3203094005584717\n",
      "Step: 2805, Loss: 0.9376860857009888, Accuracy: 0.984375, Computation time: 1.675999402999878\n",
      "Step: 2806, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.5526936054229736\n",
      "Step: 2807, Loss: 0.931888997554779, Accuracy: 0.9642857313156128, Computation time: 1.79221773147583\n",
      "Step: 2808, Loss: 0.9297254085540771, Accuracy: 0.9807692766189575, Computation time: 1.5981471538543701\n",
      "Step: 2809, Loss: 0.9159546494483948, Accuracy: 1.0, Computation time: 1.536806583404541\n",
      "Step: 2810, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 1.823836326599121\n",
      "Step: 2811, Loss: 0.9376234412193298, Accuracy: 0.9807692766189575, Computation time: 1.916050672531128\n",
      "Step: 2812, Loss: 0.9382022619247437, Accuracy: 0.9750000238418579, Computation time: 1.5615243911743164\n",
      "Step: 2813, Loss: 0.9159883856773376, Accuracy: 1.0, Computation time: 1.32957124710083\n",
      "Step: 2814, Loss: 0.9159859418869019, Accuracy: 1.0, Computation time: 1.4440200328826904\n",
      "Step: 2815, Loss: 0.9161534905433655, Accuracy: 1.0, Computation time: 1.2262320518493652\n",
      "Step: 2816, Loss: 0.9160608649253845, Accuracy: 1.0, Computation time: 1.5824203491210938\n",
      "Step: 2817, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.5658307075500488\n",
      "Step: 2818, Loss: 0.9159379601478577, Accuracy: 1.0, Computation time: 1.5980238914489746\n",
      "Step: 2819, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 1.4873554706573486\n",
      "Step: 2820, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.7166616916656494\n",
      "Step: 2821, Loss: 0.9361292719841003, Accuracy: 0.9722222089767456, Computation time: 1.638563632965088\n",
      "Step: 2822, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.4442830085754395\n",
      "Step: 2823, Loss: 0.9533630609512329, Accuracy: 0.9434524178504944, Computation time: 2.107595920562744\n",
      "Step: 2824, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.8745300769805908\n",
      "Step: 2825, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.7816948890686035\n",
      "Step: 2826, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.5077223777770996\n",
      "Step: 2827, Loss: 0.9375766515731812, Accuracy: 0.9821428656578064, Computation time: 1.8112618923187256\n",
      "Step: 2828, Loss: 0.9294828772544861, Accuracy: 0.9722222089767456, Computation time: 1.8008227348327637\n",
      "Step: 2829, Loss: 0.9160685539245605, Accuracy: 1.0, Computation time: 1.768017053604126\n",
      "Step: 2830, Loss: 0.9158980250358582, Accuracy: 1.0, Computation time: 1.6065011024475098\n",
      "Step: 2831, Loss: 0.9158872961997986, Accuracy: 1.0, Computation time: 1.4236712455749512\n",
      "Step: 2832, Loss: 0.9367098212242126, Accuracy: 0.9833333492279053, Computation time: 1.6594760417938232\n",
      "Step: 2833, Loss: 0.9184349775314331, Accuracy: 1.0, Computation time: 1.6282892227172852\n",
      "Step: 2834, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 1.944284439086914\n",
      "Step: 2835, Loss: 0.9418576955795288, Accuracy: 0.9722222089767456, Computation time: 2.286071300506592\n",
      "Step: 2836, Loss: 0.9368934631347656, Accuracy: 0.9722222089767456, Computation time: 1.448610782623291\n",
      "Step: 2837, Loss: 0.9252160787582397, Accuracy: 1.0, Computation time: 1.606111764907837\n",
      "Step: 2838, Loss: 0.9375279545783997, Accuracy: 0.9807692766189575, Computation time: 1.5980644226074219\n",
      "Step: 2839, Loss: 0.9161010384559631, Accuracy: 1.0, Computation time: 1.6425738334655762\n",
      "Step: 2840, Loss: 0.9166104197502136, Accuracy: 1.0, Computation time: 1.704418659210205\n",
      "Step: 2841, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.4983265399932861\n",
      "Step: 2842, Loss: 0.919893205165863, Accuracy: 1.0, Computation time: 1.4730679988861084\n",
      "Step: 2843, Loss: 0.9160367250442505, Accuracy: 1.0, Computation time: 1.575204610824585\n",
      "Step: 2844, Loss: 0.9162927865982056, Accuracy: 1.0, Computation time: 1.3602900505065918\n",
      "Step: 2845, Loss: 0.9160901308059692, Accuracy: 1.0, Computation time: 1.8854844570159912\n",
      "Step: 2846, Loss: 0.9180945158004761, Accuracy: 1.0, Computation time: 1.6751081943511963\n",
      "Step: 2847, Loss: 0.9159812331199646, Accuracy: 1.0, Computation time: 2.100310802459717\n",
      "Step: 2848, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 1.5670645236968994\n",
      "Step: 2849, Loss: 0.9207557439804077, Accuracy: 1.0, Computation time: 1.648108959197998\n",
      "Step: 2850, Loss: 0.9394788146018982, Accuracy: 0.96875, Computation time: 1.873169183731079\n",
      "Step: 2851, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.520857572555542\n",
      "Step: 2852, Loss: 0.9162810444831848, Accuracy: 1.0, Computation time: 1.4228131771087646\n",
      "Step: 2853, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 1.7073848247528076\n",
      "Step: 2854, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 1.5491387844085693\n",
      "Step: 2855, Loss: 0.9161360263824463, Accuracy: 1.0, Computation time: 1.372849464416504\n",
      "Step: 2856, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.5412664413452148\n",
      "Step: 2857, Loss: 0.9162607192993164, Accuracy: 1.0, Computation time: 1.5078232288360596\n",
      "Step: 2858, Loss: 0.9159125089645386, Accuracy: 1.0, Computation time: 1.9038341045379639\n",
      "Step: 2859, Loss: 0.9188272356987, Accuracy: 1.0, Computation time: 1.8478093147277832\n",
      "Step: 2860, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 2.213757276535034\n",
      "Step: 2861, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 2.037409543991089\n",
      "Step: 2862, Loss: 0.9159781336784363, Accuracy: 1.0, Computation time: 1.712646245956421\n",
      "Step: 2863, Loss: 0.9181279540061951, Accuracy: 1.0, Computation time: 1.8432488441467285\n",
      "Step: 2864, Loss: 0.9375748038291931, Accuracy: 0.9750000238418579, Computation time: 1.6739423274993896\n",
      "Step: 2865, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.9041271209716797\n",
      "Step: 2866, Loss: 0.9159332513809204, Accuracy: 1.0, Computation time: 2.0128655433654785\n",
      "Step: 2867, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 2.0990917682647705\n",
      "Step: 2868, Loss: 0.9376567006111145, Accuracy: 0.9375, Computation time: 1.800295114517212\n",
      "Step: 2869, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.9830100536346436\n",
      "Step: 2870, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 2.0621261596679688\n",
      "Step: 2871, Loss: 0.9159582853317261, Accuracy: 1.0, Computation time: 2.0603606700897217\n",
      "Step: 2872, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.8633923530578613\n",
      "Step: 2873, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.9741175174713135\n",
      "Step: 2874, Loss: 0.9160621166229248, Accuracy: 1.0, Computation time: 2.2015745639801025\n",
      "Step: 2875, Loss: 0.9159270524978638, Accuracy: 1.0, Computation time: 1.6510975360870361\n",
      "Step: 2876, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.3760323524475098\n",
      "Step: 2877, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.8692083358764648\n",
      "Step: 2878, Loss: 0.9159786701202393, Accuracy: 1.0, Computation time: 1.9394023418426514\n",
      "Step: 2879, Loss: 0.9664655923843384, Accuracy: 0.8958333730697632, Computation time: 1.716573715209961\n",
      "Step: 2880, Loss: 0.9376933574676514, Accuracy: 0.9583333730697632, Computation time: 1.852292537689209\n",
      "Step: 2881, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.675727128982544\n",
      "Step: 2882, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 2.1888811588287354\n",
      "Step: 2883, Loss: 0.9160522222518921, Accuracy: 1.0, Computation time: 2.009594678878784\n",
      "Step: 2884, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.8308649063110352\n",
      "Step: 2885, Loss: 0.9375996589660645, Accuracy: 0.96875, Computation time: 1.6949985027313232\n",
      "Step: 2886, Loss: 0.9159814715385437, Accuracy: 1.0, Computation time: 1.7262589931488037\n",
      "Step: 2887, Loss: 0.9162571430206299, Accuracy: 1.0, Computation time: 1.6627075672149658\n",
      "Step: 2888, Loss: 0.9159790873527527, Accuracy: 1.0, Computation time: 1.7272486686706543\n",
      "Step: 2889, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.6328859329223633\n",
      "Step: 2890, Loss: 0.915951132774353, Accuracy: 1.0, Computation time: 1.8267223834991455\n",
      "Step: 2891, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.471853494644165\n",
      "Step: 2892, Loss: 0.9158667922019958, Accuracy: 1.0, Computation time: 1.480414628982544\n",
      "Step: 2893, Loss: 0.9159842729568481, Accuracy: 1.0, Computation time: 1.7390000820159912\n",
      "Step: 2894, Loss: 0.9160517454147339, Accuracy: 1.0, Computation time: 1.6309335231781006\n",
      "Step: 2895, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 1.4709222316741943\n",
      "Step: 2896, Loss: 0.9159997701644897, Accuracy: 1.0, Computation time: 2.402742385864258\n",
      "Step: 2897, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 1.544301986694336\n",
      "Step: 2898, Loss: 0.9318219423294067, Accuracy: 0.9642857313156128, Computation time: 1.7133378982543945\n",
      "Step: 2899, Loss: 0.9159528613090515, Accuracy: 1.0, Computation time: 1.4976987838745117\n",
      "Step: 2900, Loss: 0.9159889221191406, Accuracy: 1.0, Computation time: 1.9865846633911133\n",
      "Step: 2901, Loss: 0.9159515500068665, Accuracy: 1.0, Computation time: 1.0629174709320068\n",
      "Step: 2902, Loss: 0.9165114164352417, Accuracy: 1.0, Computation time: 1.4813790321350098\n",
      "Step: 2903, Loss: 0.9243510365486145, Accuracy: 1.0, Computation time: 1.7661418914794922\n",
      "Step: 2904, Loss: 0.9162783026695251, Accuracy: 1.0, Computation time: 1.7703912258148193\n",
      "Step: 2905, Loss: 0.9159442782402039, Accuracy: 1.0, Computation time: 1.220137596130371\n",
      "Step: 2906, Loss: 0.915952205657959, Accuracy: 1.0, Computation time: 1.110187292098999\n",
      "Step: 2907, Loss: 0.9372875690460205, Accuracy: 0.9772727489471436, Computation time: 1.2341015338897705\n",
      "Step: 2908, Loss: 0.9202404022216797, Accuracy: 1.0, Computation time: 1.4144530296325684\n",
      "Step: 2909, Loss: 0.9159079790115356, Accuracy: 1.0, Computation time: 1.4647917747497559\n",
      "Step: 2910, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 1.4362857341766357\n",
      "Step: 2911, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.567664623260498\n",
      "Step: 2912, Loss: 0.9166077971458435, Accuracy: 1.0, Computation time: 1.5997252464294434\n",
      "Step: 2913, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.403468370437622\n",
      "Step: 2914, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.057542085647583\n",
      "Step: 2915, Loss: 0.9159102439880371, Accuracy: 1.0, Computation time: 1.3123531341552734\n",
      "Step: 2916, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 1.5536177158355713\n",
      "Step: 2917, Loss: 0.9377474188804626, Accuracy: 0.9722222089767456, Computation time: 1.0068976879119873\n",
      "Step: 2918, Loss: 0.9168829917907715, Accuracy: 1.0, Computation time: 1.6928417682647705\n",
      "########################\n",
      "Test loss: 1.1200921535491943, Test Accuracy_epoch21: 0.7019357681274414\n",
      "########################\n",
      "Step: 2919, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.520181655883789\n",
      "Step: 2920, Loss: 0.9376328587532043, Accuracy: 0.9583333730697632, Computation time: 1.5023295879364014\n",
      "Step: 2921, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.3572256565093994\n",
      "Step: 2922, Loss: 0.9375020265579224, Accuracy: 0.9791666865348816, Computation time: 1.4355051517486572\n",
      "Step: 2923, Loss: 0.915947437286377, Accuracy: 1.0, Computation time: 1.1516640186309814\n",
      "Step: 2924, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.207155704498291\n",
      "Step: 2925, Loss: 0.9574757218360901, Accuracy: 0.9375, Computation time: 2.2689034938812256\n",
      "Step: 2926, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 1.2505221366882324\n",
      "Step: 2927, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.5362095832824707\n",
      "Step: 2928, Loss: 0.9160640835762024, Accuracy: 1.0, Computation time: 1.4140727519989014\n",
      "Step: 2929, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.6985361576080322\n",
      "Step: 2930, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.4600236415863037\n",
      "Step: 2931, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.3893365859985352\n",
      "Step: 2932, Loss: 0.9169135689735413, Accuracy: 1.0, Computation time: 1.6554124355316162\n",
      "Step: 2933, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.562335729598999\n",
      "Step: 2934, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.3747494220733643\n",
      "Step: 2935, Loss: 0.9377021193504333, Accuracy: 0.9750000238418579, Computation time: 1.8779942989349365\n",
      "Step: 2936, Loss: 0.9365250468254089, Accuracy: 0.96875, Computation time: 1.4823887348175049\n",
      "Step: 2937, Loss: 0.9178642630577087, Accuracy: 1.0, Computation time: 1.4448673725128174\n",
      "Step: 2938, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 1.7712652683258057\n",
      "Step: 2939, Loss: 0.9159169793128967, Accuracy: 1.0, Computation time: 1.3624730110168457\n",
      "Step: 2940, Loss: 0.91591477394104, Accuracy: 1.0, Computation time: 1.4726977348327637\n",
      "Step: 2941, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 2.21527361869812\n",
      "Step: 2942, Loss: 0.9238995909690857, Accuracy: 1.0, Computation time: 1.8482890129089355\n",
      "Step: 2943, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.5732064247131348\n",
      "Step: 2944, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.6513283252716064\n",
      "Step: 2945, Loss: 0.9159641265869141, Accuracy: 1.0, Computation time: 1.702899694442749\n",
      "Step: 2946, Loss: 0.9159747958183289, Accuracy: 1.0, Computation time: 1.4316127300262451\n",
      "Step: 2947, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.5428857803344727\n",
      "Step: 2948, Loss: 0.9288855195045471, Accuracy: 0.9750000238418579, Computation time: 1.56022047996521\n",
      "Step: 2949, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 1.8265678882598877\n",
      "Step: 2950, Loss: 0.9364266991615295, Accuracy: 0.984375, Computation time: 2.0623791217803955\n",
      "Step: 2951, Loss: 0.9371489882469177, Accuracy: 0.9166666865348816, Computation time: 1.622767448425293\n",
      "Step: 2952, Loss: 0.91645747423172, Accuracy: 1.0, Computation time: 1.6090855598449707\n",
      "Step: 2953, Loss: 0.9160557985305786, Accuracy: 1.0, Computation time: 1.8932044506072998\n",
      "Step: 2954, Loss: 0.9227288961410522, Accuracy: 1.0, Computation time: 2.0257508754730225\n",
      "Step: 2955, Loss: 0.9160863161087036, Accuracy: 1.0, Computation time: 1.8014416694641113\n",
      "Step: 2956, Loss: 0.937764048576355, Accuracy: 0.9642857313156128, Computation time: 1.5707948207855225\n",
      "Step: 2957, Loss: 0.9161341786384583, Accuracy: 1.0, Computation time: 1.8224303722381592\n",
      "Step: 2958, Loss: 0.9186834692955017, Accuracy: 1.0, Computation time: 2.092068672180176\n",
      "Step: 2959, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 2.0513644218444824\n",
      "Step: 2960, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 1.8992242813110352\n",
      "Step: 2961, Loss: 0.9382997751235962, Accuracy: 0.9807692766189575, Computation time: 1.9540257453918457\n",
      "Step: 2962, Loss: 0.937543511390686, Accuracy: 0.9807692766189575, Computation time: 1.6690418720245361\n",
      "Step: 2963, Loss: 0.9594934582710266, Accuracy: 0.9545454978942871, Computation time: 2.0962061882019043\n",
      "Step: 2964, Loss: 0.916061282157898, Accuracy: 1.0, Computation time: 1.6687326431274414\n",
      "Step: 2965, Loss: 0.9161756634712219, Accuracy: 1.0, Computation time: 1.8374056816101074\n",
      "Step: 2966, Loss: 0.916044294834137, Accuracy: 1.0, Computation time: 1.7872843742370605\n",
      "Step: 2967, Loss: 0.9337027072906494, Accuracy: 0.9750000238418579, Computation time: 2.0458056926727295\n",
      "Step: 2968, Loss: 0.9159637093544006, Accuracy: 1.0, Computation time: 1.6084818840026855\n",
      "Step: 2969, Loss: 0.9375581741333008, Accuracy: 0.9583333730697632, Computation time: 1.7732579708099365\n",
      "Step: 2970, Loss: 0.9159436821937561, Accuracy: 1.0, Computation time: 1.4992005825042725\n",
      "Step: 2971, Loss: 0.9163200855255127, Accuracy: 1.0, Computation time: 2.2842555046081543\n",
      "Step: 2972, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.6434080600738525\n",
      "Step: 2973, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.697113275527954\n",
      "Step: 2974, Loss: 0.9276456832885742, Accuracy: 0.9807692766189575, Computation time: 1.97171950340271\n",
      "Step: 2975, Loss: 0.9159641861915588, Accuracy: 1.0, Computation time: 1.8004894256591797\n",
      "Step: 2976, Loss: 0.9159970283508301, Accuracy: 1.0, Computation time: 1.2302618026733398\n",
      "Step: 2977, Loss: 0.9163433313369751, Accuracy: 1.0, Computation time: 1.686415672302246\n",
      "Step: 2978, Loss: 0.9322019219398499, Accuracy: 0.9750000238418579, Computation time: 1.8909149169921875\n",
      "Step: 2979, Loss: 0.9162192344665527, Accuracy: 1.0, Computation time: 1.6503221988677979\n",
      "Step: 2980, Loss: 0.9161584377288818, Accuracy: 1.0, Computation time: 1.5620417594909668\n",
      "Step: 2981, Loss: 0.9365636706352234, Accuracy: 0.9750000238418579, Computation time: 1.9045231342315674\n",
      "Step: 2982, Loss: 0.9160076975822449, Accuracy: 1.0, Computation time: 1.6015996932983398\n",
      "Step: 2983, Loss: 0.9345174431800842, Accuracy: 0.949999988079071, Computation time: 1.802344560623169\n",
      "Step: 2984, Loss: 0.9159467220306396, Accuracy: 1.0, Computation time: 1.5916862487792969\n",
      "Step: 2985, Loss: 0.9159703850746155, Accuracy: 1.0, Computation time: 1.6837623119354248\n",
      "Step: 2986, Loss: 0.9164654612541199, Accuracy: 1.0, Computation time: 1.7943603992462158\n",
      "Step: 2987, Loss: 0.9160169363021851, Accuracy: 1.0, Computation time: 1.98203444480896\n",
      "Step: 2988, Loss: 0.9161484837532043, Accuracy: 1.0, Computation time: 1.9368391036987305\n",
      "Step: 2989, Loss: 0.916581928730011, Accuracy: 1.0, Computation time: 1.8599069118499756\n",
      "Step: 2990, Loss: 0.9160323739051819, Accuracy: 1.0, Computation time: 2.0527443885803223\n",
      "Step: 2991, Loss: 0.9344570636749268, Accuracy: 0.9833333492279053, Computation time: 1.9171066284179688\n",
      "Step: 2992, Loss: 0.9159521460533142, Accuracy: 1.0, Computation time: 2.0045485496520996\n",
      "Step: 2993, Loss: 0.9598434567451477, Accuracy: 0.9444444179534912, Computation time: 1.6364691257476807\n",
      "Step: 2994, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.8234601020812988\n",
      "Step: 2995, Loss: 0.9159851670265198, Accuracy: 1.0, Computation time: 1.7617230415344238\n",
      "Step: 2996, Loss: 0.9218265414237976, Accuracy: 1.0, Computation time: 1.5850005149841309\n",
      "Step: 2997, Loss: 0.9380024075508118, Accuracy: 0.949999988079071, Computation time: 1.6398341655731201\n",
      "Step: 2998, Loss: 0.9159724116325378, Accuracy: 1.0, Computation time: 1.7825725078582764\n",
      "Step: 2999, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 1.8718256950378418\n",
      "Step: 3000, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 1.4629664421081543\n",
      "Step: 3001, Loss: 0.9578536748886108, Accuracy: 0.9522727727890015, Computation time: 1.661508560180664\n",
      "Step: 3002, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 2.052483081817627\n",
      "Step: 3003, Loss: 0.9159110188484192, Accuracy: 1.0, Computation time: 1.5183346271514893\n",
      "Step: 3004, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.5064151287078857\n",
      "Step: 3005, Loss: 0.9266605377197266, Accuracy: 0.9750000238418579, Computation time: 1.7112529277801514\n",
      "Step: 3006, Loss: 0.9195083975791931, Accuracy: 1.0, Computation time: 1.888932228088379\n",
      "Step: 3007, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 1.4658725261688232\n",
      "Step: 3008, Loss: 0.9159711599349976, Accuracy: 1.0, Computation time: 1.4275329113006592\n",
      "Step: 3009, Loss: 0.9163161516189575, Accuracy: 1.0, Computation time: 1.5980255603790283\n",
      "Step: 3010, Loss: 0.9160096049308777, Accuracy: 1.0, Computation time: 1.4573297500610352\n",
      "Step: 3011, Loss: 0.9160318374633789, Accuracy: 1.0, Computation time: 1.3508081436157227\n",
      "Step: 3012, Loss: 0.9159474968910217, Accuracy: 1.0, Computation time: 1.5640559196472168\n",
      "Step: 3013, Loss: 0.9162847399711609, Accuracy: 1.0, Computation time: 1.6996049880981445\n",
      "Step: 3014, Loss: 0.9159483909606934, Accuracy: 1.0, Computation time: 1.8847079277038574\n",
      "Step: 3015, Loss: 0.9162101149559021, Accuracy: 1.0, Computation time: 1.4105050563812256\n",
      "Step: 3016, Loss: 0.9162803888320923, Accuracy: 1.0, Computation time: 1.6264910697937012\n",
      "Step: 3017, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 1.3684887886047363\n",
      "Step: 3018, Loss: 0.9196389317512512, Accuracy: 1.0, Computation time: 1.851923942565918\n",
      "Step: 3019, Loss: 0.9160130620002747, Accuracy: 1.0, Computation time: 1.7034039497375488\n",
      "Step: 3020, Loss: 0.9332464933395386, Accuracy: 0.9833333492279053, Computation time: 2.1998260021209717\n",
      "Step: 3021, Loss: 0.9376537799835205, Accuracy: 0.9722222089767456, Computation time: 1.5515177249908447\n",
      "Step: 3022, Loss: 0.9161479473114014, Accuracy: 1.0, Computation time: 1.8169398307800293\n",
      "Step: 3023, Loss: 0.9374196529388428, Accuracy: 0.96875, Computation time: 1.5116989612579346\n",
      "Step: 3024, Loss: 0.9159446954727173, Accuracy: 1.0, Computation time: 1.4595935344696045\n",
      "Step: 3025, Loss: 0.9377562403678894, Accuracy: 0.984375, Computation time: 1.7831299304962158\n",
      "Step: 3026, Loss: 0.9159506559371948, Accuracy: 1.0, Computation time: 1.9518213272094727\n",
      "Step: 3027, Loss: 0.9160382747650146, Accuracy: 1.0, Computation time: 1.4884486198425293\n",
      "Step: 3028, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.660663366317749\n",
      "Step: 3029, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.5487847328186035\n",
      "Step: 3030, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 1.8072185516357422\n",
      "Step: 3031, Loss: 0.9159408807754517, Accuracy: 1.0, Computation time: 1.7041585445404053\n",
      "Step: 3032, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.4310073852539062\n",
      "Step: 3033, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 1.6618986129760742\n",
      "Step: 3034, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.4916930198669434\n",
      "Step: 3035, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.5889055728912354\n",
      "Step: 3036, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.5310215950012207\n",
      "Step: 3037, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.5729131698608398\n",
      "Step: 3038, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.550180435180664\n",
      "Step: 3039, Loss: 0.9267392754554749, Accuracy: 0.9166666865348816, Computation time: 1.6722936630249023\n",
      "Step: 3040, Loss: 0.9251644015312195, Accuracy: 1.0, Computation time: 1.5167417526245117\n",
      "Step: 3041, Loss: 0.9162093997001648, Accuracy: 1.0, Computation time: 1.521219253540039\n",
      "Step: 3042, Loss: 0.9159455895423889, Accuracy: 1.0, Computation time: 1.6280252933502197\n",
      "Step: 3043, Loss: 0.9160045981407166, Accuracy: 1.0, Computation time: 1.427776575088501\n",
      "Step: 3044, Loss: 0.9160400032997131, Accuracy: 1.0, Computation time: 1.318589210510254\n",
      "Step: 3045, Loss: 0.9160414338111877, Accuracy: 1.0, Computation time: 1.430058479309082\n",
      "Step: 3046, Loss: 0.916042685508728, Accuracy: 1.0, Computation time: 1.8000226020812988\n",
      "Step: 3047, Loss: 0.9161762595176697, Accuracy: 1.0, Computation time: 1.6765847206115723\n",
      "Step: 3048, Loss: 0.9159579873085022, Accuracy: 1.0, Computation time: 1.5113070011138916\n",
      "Step: 3049, Loss: 0.9373431205749512, Accuracy: 0.949999988079071, Computation time: 1.8156702518463135\n",
      "Step: 3050, Loss: 0.9164503812789917, Accuracy: 1.0, Computation time: 1.6863486766815186\n",
      "Step: 3051, Loss: 0.9163013100624084, Accuracy: 1.0, Computation time: 1.3930330276489258\n",
      "Step: 3052, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.461172103881836\n",
      "Step: 3053, Loss: 0.926817774772644, Accuracy: 0.9722222089767456, Computation time: 1.7427425384521484\n",
      "Step: 3054, Loss: 0.9159854650497437, Accuracy: 1.0, Computation time: 1.5663673877716064\n",
      "Step: 3055, Loss: 0.9160282611846924, Accuracy: 1.0, Computation time: 1.466827630996704\n",
      "Step: 3056, Loss: 0.9161669611930847, Accuracy: 1.0, Computation time: 1.4318225383758545\n",
      "Step: 3057, Loss: 0.9204457402229309, Accuracy: 1.0, Computation time: 1.4368128776550293\n",
      "########################\n",
      "Test loss: 1.120667815208435, Test Accuracy_epoch22: 0.6935628652572632\n",
      "########################\n",
      "Step: 3058, Loss: 0.9159842729568481, Accuracy: 1.0, Computation time: 1.3120741844177246\n",
      "Step: 3059, Loss: 0.9161081314086914, Accuracy: 1.0, Computation time: 1.3323733806610107\n",
      "Step: 3060, Loss: 0.9670515060424805, Accuracy: 0.9615384340286255, Computation time: 1.3782212734222412\n",
      "Step: 3061, Loss: 0.9194877743721008, Accuracy: 1.0, Computation time: 1.5843307971954346\n",
      "Step: 3062, Loss: 0.9162554740905762, Accuracy: 1.0, Computation time: 1.2933323383331299\n",
      "Step: 3063, Loss: 0.9165142774581909, Accuracy: 1.0, Computation time: 1.4899232387542725\n",
      "Step: 3064, Loss: 0.9160757660865784, Accuracy: 1.0, Computation time: 1.4869720935821533\n",
      "Step: 3065, Loss: 0.9164423942565918, Accuracy: 1.0, Computation time: 1.2481787204742432\n",
      "Step: 3066, Loss: 0.9166879653930664, Accuracy: 1.0, Computation time: 1.3454406261444092\n",
      "Step: 3067, Loss: 0.9385939836502075, Accuracy: 0.9375, Computation time: 1.4162719249725342\n",
      "Step: 3068, Loss: 0.9163963198661804, Accuracy: 1.0, Computation time: 1.2151329517364502\n",
      "Step: 3069, Loss: 0.9162623286247253, Accuracy: 1.0, Computation time: 1.3241021633148193\n",
      "Step: 3070, Loss: 0.9563836455345154, Accuracy: 0.9375, Computation time: 1.2532422542572021\n",
      "Step: 3071, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.2529232501983643\n",
      "Step: 3072, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.2909443378448486\n",
      "Step: 3073, Loss: 0.9175831079483032, Accuracy: 1.0, Computation time: 1.4243416786193848\n",
      "Step: 3074, Loss: 0.9162774085998535, Accuracy: 1.0, Computation time: 1.1658508777618408\n",
      "Step: 3075, Loss: 0.9161562323570251, Accuracy: 1.0, Computation time: 1.355180025100708\n",
      "Step: 3076, Loss: 0.9161831140518188, Accuracy: 1.0, Computation time: 1.5023856163024902\n",
      "Step: 3077, Loss: 0.9579038023948669, Accuracy: 0.9599359035491943, Computation time: 1.4748256206512451\n",
      "Step: 3078, Loss: 0.9161238670349121, Accuracy: 1.0, Computation time: 1.3617115020751953\n",
      "Step: 3079, Loss: 0.9179115295410156, Accuracy: 1.0, Computation time: 1.119640827178955\n",
      "Step: 3080, Loss: 0.9159741997718811, Accuracy: 1.0, Computation time: 1.4759430885314941\n",
      "Step: 3081, Loss: 0.9159192442893982, Accuracy: 1.0, Computation time: 1.4804980754852295\n",
      "Step: 3082, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.3060979843139648\n",
      "Step: 3083, Loss: 0.9374883770942688, Accuracy: 0.9166666865348816, Computation time: 1.3167846202850342\n",
      "Step: 3084, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.2527134418487549\n",
      "Step: 3085, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 1.3003771305084229\n",
      "Step: 3086, Loss: 0.9168395400047302, Accuracy: 1.0, Computation time: 1.208054542541504\n",
      "Step: 3087, Loss: 0.9159756898880005, Accuracy: 1.0, Computation time: 1.3827199935913086\n",
      "Step: 3088, Loss: 0.9160527586936951, Accuracy: 1.0, Computation time: 1.8208024501800537\n",
      "Step: 3089, Loss: 0.9159887433052063, Accuracy: 1.0, Computation time: 1.6033906936645508\n",
      "Step: 3090, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.3333821296691895\n",
      "Step: 3091, Loss: 0.9160079956054688, Accuracy: 1.0, Computation time: 1.520792007446289\n",
      "Step: 3092, Loss: 0.9332674741744995, Accuracy: 0.9750000238418579, Computation time: 1.7788305282592773\n",
      "Step: 3093, Loss: 0.9165400266647339, Accuracy: 1.0, Computation time: 2.0883898735046387\n",
      "Step: 3094, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.4929449558258057\n",
      "Step: 3095, Loss: 0.9238517880439758, Accuracy: 1.0, Computation time: 1.7296395301818848\n",
      "Step: 3096, Loss: 0.918259859085083, Accuracy: 1.0, Computation time: 1.2496156692504883\n",
      "Step: 3097, Loss: 0.916243314743042, Accuracy: 1.0, Computation time: 1.6317391395568848\n",
      "Step: 3098, Loss: 0.9161041378974915, Accuracy: 1.0, Computation time: 1.5771105289459229\n",
      "Step: 3099, Loss: 0.937587320804596, Accuracy: 0.9791666865348816, Computation time: 1.5030553340911865\n",
      "Step: 3100, Loss: 0.9161167144775391, Accuracy: 1.0, Computation time: 1.3752567768096924\n",
      "Step: 3101, Loss: 0.9176440834999084, Accuracy: 1.0, Computation time: 1.492384910583496\n",
      "Step: 3102, Loss: 0.9159549474716187, Accuracy: 1.0, Computation time: 1.455367088317871\n",
      "Step: 3103, Loss: 0.9178420901298523, Accuracy: 1.0, Computation time: 1.3219780921936035\n",
      "Step: 3104, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.563493251800537\n",
      "Step: 3105, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 1.3921329975128174\n",
      "Step: 3106, Loss: 0.9376343488693237, Accuracy: 0.9772727489471436, Computation time: 1.4046297073364258\n",
      "Step: 3107, Loss: 0.9238766431808472, Accuracy: 1.0, Computation time: 1.8633537292480469\n",
      "Step: 3108, Loss: 0.9564316272735596, Accuracy: 0.96875, Computation time: 2.26375150680542\n",
      "Step: 3109, Loss: 0.9160658717155457, Accuracy: 1.0, Computation time: 1.10499906539917\n",
      "Step: 3110, Loss: 0.9160131216049194, Accuracy: 1.0, Computation time: 1.4779117107391357\n",
      "Step: 3111, Loss: 0.9483261704444885, Accuracy: 0.949999988079071, Computation time: 1.622185230255127\n",
      "Step: 3112, Loss: 0.9384431838989258, Accuracy: 0.9852941036224365, Computation time: 1.5264239311218262\n",
      "Step: 3113, Loss: 0.9160751700401306, Accuracy: 1.0, Computation time: 1.3808393478393555\n",
      "Step: 3114, Loss: 0.9169155955314636, Accuracy: 1.0, Computation time: 1.719933271408081\n",
      "Step: 3115, Loss: 0.9159157872200012, Accuracy: 1.0, Computation time: 1.267319679260254\n",
      "Step: 3116, Loss: 0.9188234806060791, Accuracy: 1.0, Computation time: 1.4203481674194336\n",
      "Step: 3117, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 1.1361496448516846\n",
      "Step: 3118, Loss: 0.9160416126251221, Accuracy: 1.0, Computation time: 2.1871328353881836\n",
      "Step: 3119, Loss: 0.9187273979187012, Accuracy: 1.0, Computation time: 1.6767396926879883\n",
      "Step: 3120, Loss: 0.9160000085830688, Accuracy: 1.0, Computation time: 1.411409616470337\n",
      "Step: 3121, Loss: 0.9159724712371826, Accuracy: 1.0, Computation time: 1.2796387672424316\n",
      "Step: 3122, Loss: 0.9379006624221802, Accuracy: 0.9750000238418579, Computation time: 1.320671796798706\n",
      "Step: 3123, Loss: 0.915963888168335, Accuracy: 1.0, Computation time: 1.5847828388214111\n",
      "Step: 3124, Loss: 0.936187744140625, Accuracy: 0.9750000238418579, Computation time: 1.5887155532836914\n",
      "Step: 3125, Loss: 0.9159854054450989, Accuracy: 1.0, Computation time: 1.4289638996124268\n",
      "Step: 3126, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 1.568878173828125\n",
      "Step: 3127, Loss: 0.9364018440246582, Accuracy: 0.9750000238418579, Computation time: 1.2385151386260986\n",
      "Step: 3128, Loss: 0.9329943656921387, Accuracy: 0.9583333730697632, Computation time: 1.422851800918579\n",
      "Step: 3129, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.3046224117279053\n",
      "Step: 3130, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.1490797996520996\n",
      "Step: 3131, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 1.2182278633117676\n",
      "Step: 3132, Loss: 0.9373143911361694, Accuracy: 0.9807692766189575, Computation time: 1.335423231124878\n",
      "Step: 3133, Loss: 0.9160987734794617, Accuracy: 1.0, Computation time: 1.3474864959716797\n",
      "Step: 3134, Loss: 0.9163047075271606, Accuracy: 1.0, Computation time: 1.4359490871429443\n",
      "Step: 3135, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 1.221153736114502\n",
      "Step: 3136, Loss: 0.9175440073013306, Accuracy: 1.0, Computation time: 1.405585527420044\n",
      "Step: 3137, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.219141960144043\n",
      "Step: 3138, Loss: 0.9159286618232727, Accuracy: 1.0, Computation time: 1.2221698760986328\n",
      "Step: 3139, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.241701602935791\n",
      "Step: 3140, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.0941705703735352\n",
      "Step: 3141, Loss: 0.9159728288650513, Accuracy: 1.0, Computation time: 1.796086072921753\n",
      "Step: 3142, Loss: 0.9167453050613403, Accuracy: 1.0, Computation time: 1.5062618255615234\n",
      "Step: 3143, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.1386380195617676\n",
      "Step: 3144, Loss: 0.9159621000289917, Accuracy: 1.0, Computation time: 1.583702564239502\n",
      "Step: 3145, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.5030572414398193\n",
      "Step: 3146, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.386162519454956\n",
      "Step: 3147, Loss: 0.9158954620361328, Accuracy: 1.0, Computation time: 1.4439795017242432\n",
      "Step: 3148, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.5807056427001953\n",
      "Step: 3149, Loss: 0.9160522818565369, Accuracy: 1.0, Computation time: 1.3603394031524658\n",
      "Step: 3150, Loss: 0.9174972772598267, Accuracy: 1.0, Computation time: 1.2339365482330322\n",
      "Step: 3151, Loss: 0.9158971905708313, Accuracy: 1.0, Computation time: 1.5505576133728027\n",
      "Step: 3152, Loss: 0.9360228180885315, Accuracy: 0.9642857313156128, Computation time: 1.6785993576049805\n",
      "Step: 3153, Loss: 0.937701404094696, Accuracy: 0.9791666865348816, Computation time: 1.360494613647461\n",
      "Step: 3154, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.339212417602539\n",
      "Step: 3155, Loss: 0.9159801602363586, Accuracy: 1.0, Computation time: 1.480903148651123\n",
      "Step: 3156, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.2275340557098389\n",
      "Step: 3157, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.4201796054840088\n",
      "Step: 3158, Loss: 0.9369704723358154, Accuracy: 0.9375, Computation time: 1.414184808731079\n",
      "Step: 3159, Loss: 0.9265034198760986, Accuracy: 1.0, Computation time: 1.7038383483886719\n",
      "Step: 3160, Loss: 0.9159576296806335, Accuracy: 1.0, Computation time: 1.7552001476287842\n",
      "Step: 3161, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 1.4737069606781006\n",
      "Step: 3162, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.3146777153015137\n",
      "Step: 3163, Loss: 0.9587383270263672, Accuracy: 0.9333333969116211, Computation time: 1.3887498378753662\n",
      "Step: 3164, Loss: 0.9163013696670532, Accuracy: 1.0, Computation time: 1.340754508972168\n",
      "Step: 3165, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.7972559928894043\n",
      "Step: 3166, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 1.5446600914001465\n",
      "Step: 3167, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.5887019634246826\n",
      "Step: 3168, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 1.5583293437957764\n",
      "Step: 3169, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.6874399185180664\n",
      "Step: 3170, Loss: 0.9269764423370361, Accuracy: 0.9772727489471436, Computation time: 1.6596887111663818\n",
      "Step: 3171, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.4855763912200928\n",
      "Step: 3172, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 1.2764604091644287\n",
      "Step: 3173, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.5858347415924072\n",
      "Step: 3174, Loss: 0.9159969687461853, Accuracy: 1.0, Computation time: 1.5984222888946533\n",
      "Step: 3175, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 1.5058085918426514\n",
      "Step: 3176, Loss: 0.9159517884254456, Accuracy: 1.0, Computation time: 1.6469814777374268\n",
      "Step: 3177, Loss: 0.9159460663795471, Accuracy: 1.0, Computation time: 1.5341181755065918\n",
      "Step: 3178, Loss: 0.9378805756568909, Accuracy: 0.9375, Computation time: 1.3635804653167725\n",
      "Step: 3179, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 1.7598631381988525\n",
      "Step: 3180, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 1.4371602535247803\n",
      "Step: 3181, Loss: 0.9160223603248596, Accuracy: 1.0, Computation time: 1.5067107677459717\n",
      "Step: 3182, Loss: 0.9310413002967834, Accuracy: 0.9772727489471436, Computation time: 1.5158519744873047\n",
      "Step: 3183, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 1.6466293334960938\n",
      "Step: 3184, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.4308674335479736\n",
      "Step: 3185, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 1.4059503078460693\n",
      "Step: 3186, Loss: 0.9160482287406921, Accuracy: 1.0, Computation time: 1.5694637298583984\n",
      "Step: 3187, Loss: 0.9161077737808228, Accuracy: 1.0, Computation time: 1.6541547775268555\n",
      "Step: 3188, Loss: 0.9162086248397827, Accuracy: 1.0, Computation time: 1.7813985347747803\n",
      "Step: 3189, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.7310247421264648\n",
      "Step: 3190, Loss: 0.915936291217804, Accuracy: 1.0, Computation time: 1.8626773357391357\n",
      "Step: 3191, Loss: 0.9376526474952698, Accuracy: 0.96875, Computation time: 1.590045690536499\n",
      "Step: 3192, Loss: 0.9375559091567993, Accuracy: 0.9642857313156128, Computation time: 1.6252198219299316\n",
      "Step: 3193, Loss: 0.9159525036811829, Accuracy: 1.0, Computation time: 1.7863349914550781\n",
      "Step: 3194, Loss: 0.9162164926528931, Accuracy: 1.0, Computation time: 1.8030364513397217\n",
      "Step: 3195, Loss: 0.9160298109054565, Accuracy: 1.0, Computation time: 1.7164671421051025\n",
      "Step: 3196, Loss: 0.9281099438667297, Accuracy: 0.9722222089767456, Computation time: 1.6075100898742676\n",
      "########################\n",
      "Test loss: 1.1182855367660522, Test Accuracy_epoch23: 0.7017409205436707\n",
      "########################\n",
      "Step: 3197, Loss: 0.9163831472396851, Accuracy: 1.0, Computation time: 1.8529481887817383\n",
      "Step: 3198, Loss: 0.9166393876075745, Accuracy: 1.0, Computation time: 1.983816146850586\n",
      "Step: 3199, Loss: 0.9367213249206543, Accuracy: 0.9750000238418579, Computation time: 1.9164695739746094\n",
      "Step: 3200, Loss: 0.916271448135376, Accuracy: 1.0, Computation time: 1.8969883918762207\n",
      "Step: 3201, Loss: 0.9379581212997437, Accuracy: 0.9722222089767456, Computation time: 1.730086326599121\n",
      "Step: 3202, Loss: 0.9166461229324341, Accuracy: 1.0, Computation time: 1.8021326065063477\n",
      "Step: 3203, Loss: 0.9161626696586609, Accuracy: 1.0, Computation time: 1.8261878490447998\n",
      "Step: 3204, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.781663417816162\n",
      "Step: 3205, Loss: 0.9373226165771484, Accuracy: 0.9642857313156128, Computation time: 2.0670437812805176\n",
      "Step: 3206, Loss: 0.9436833262443542, Accuracy: 0.9583333730697632, Computation time: 2.3731508255004883\n",
      "Step: 3207, Loss: 0.9163516759872437, Accuracy: 1.0, Computation time: 2.2167272567749023\n",
      "Step: 3208, Loss: 0.9161725044250488, Accuracy: 1.0, Computation time: 1.6396026611328125\n",
      "Step: 3209, Loss: 0.9161174893379211, Accuracy: 1.0, Computation time: 1.526845932006836\n",
      "Step: 3210, Loss: 0.9160417318344116, Accuracy: 1.0, Computation time: 2.0095834732055664\n",
      "Step: 3211, Loss: 0.9159538149833679, Accuracy: 1.0, Computation time: 1.8670129776000977\n",
      "Step: 3212, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.7351362705230713\n",
      "Step: 3213, Loss: 0.936224639415741, Accuracy: 0.9583333730697632, Computation time: 1.6402499675750732\n",
      "Step: 3214, Loss: 0.9166859984397888, Accuracy: 1.0, Computation time: 1.840226173400879\n",
      "Step: 3215, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 1.7333130836486816\n",
      "Step: 3216, Loss: 0.9160462021827698, Accuracy: 1.0, Computation time: 1.7565937042236328\n",
      "Step: 3217, Loss: 0.916010856628418, Accuracy: 1.0, Computation time: 1.553870677947998\n",
      "Step: 3218, Loss: 0.9161324501037598, Accuracy: 1.0, Computation time: 1.5432024002075195\n",
      "Step: 3219, Loss: 0.9159340858459473, Accuracy: 1.0, Computation time: 1.5258870124816895\n",
      "Step: 3220, Loss: 0.9165285229682922, Accuracy: 1.0, Computation time: 1.7793984413146973\n",
      "Step: 3221, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.514585018157959\n",
      "Step: 3222, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 1.6313040256500244\n",
      "Step: 3223, Loss: 0.9159178137779236, Accuracy: 1.0, Computation time: 1.3930556774139404\n",
      "Step: 3224, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.452974796295166\n",
      "Step: 3225, Loss: 0.9365410804748535, Accuracy: 0.9750000238418579, Computation time: 1.4509761333465576\n",
      "Step: 3226, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.3801240921020508\n",
      "Step: 3227, Loss: 0.9161595106124878, Accuracy: 1.0, Computation time: 1.3953509330749512\n",
      "Step: 3228, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.5272254943847656\n",
      "Step: 3229, Loss: 0.9159481525421143, Accuracy: 1.0, Computation time: 1.6140804290771484\n",
      "Step: 3230, Loss: 0.9376552700996399, Accuracy: 0.96875, Computation time: 1.164698600769043\n",
      "Step: 3231, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.4281482696533203\n",
      "Step: 3232, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.5196716785430908\n",
      "Step: 3233, Loss: 0.9375693798065186, Accuracy: 0.96875, Computation time: 1.4214658737182617\n",
      "Step: 3234, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.2505028247833252\n",
      "Step: 3235, Loss: 0.9160820245742798, Accuracy: 1.0, Computation time: 1.5540308952331543\n",
      "Step: 3236, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.4052255153656006\n",
      "Step: 3237, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.6086933612823486\n",
      "Step: 3238, Loss: 0.937516987323761, Accuracy: 0.9722222089767456, Computation time: 1.7985188961029053\n",
      "Step: 3239, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.3796069622039795\n",
      "Step: 3240, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 1.3398692607879639\n",
      "Step: 3241, Loss: 0.9159253239631653, Accuracy: 1.0, Computation time: 1.682478427886963\n",
      "Step: 3242, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.707320213317871\n",
      "Step: 3243, Loss: 0.9251152873039246, Accuracy: 1.0, Computation time: 1.7484028339385986\n",
      "Step: 3244, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.6889936923980713\n",
      "Step: 3245, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.4838957786560059\n",
      "Step: 3246, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.469238519668579\n",
      "Step: 3247, Loss: 0.9165492057800293, Accuracy: 1.0, Computation time: 2.343418836593628\n",
      "Step: 3248, Loss: 0.9561043381690979, Accuracy: 0.9494949579238892, Computation time: 2.080203056335449\n",
      "Step: 3249, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.6179172992706299\n",
      "Step: 3250, Loss: 0.9376417398452759, Accuracy: 0.9772727489471436, Computation time: 1.640679121017456\n",
      "Step: 3251, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.3921458721160889\n",
      "Step: 3252, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.6472270488739014\n",
      "Step: 3253, Loss: 0.9584718346595764, Accuracy: 0.9545454978942871, Computation time: 1.737335205078125\n",
      "Step: 3254, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.6871230602264404\n",
      "Step: 3255, Loss: 0.9265894293785095, Accuracy: 0.9583333730697632, Computation time: 1.6196379661560059\n",
      "Step: 3256, Loss: 0.9158980250358582, Accuracy: 1.0, Computation time: 1.744727611541748\n",
      "Step: 3257, Loss: 0.9160735607147217, Accuracy: 1.0, Computation time: 1.8225712776184082\n",
      "Step: 3258, Loss: 0.9163049459457397, Accuracy: 1.0, Computation time: 2.0091559886932373\n",
      "Step: 3259, Loss: 0.9161349534988403, Accuracy: 1.0, Computation time: 1.7768824100494385\n",
      "Step: 3260, Loss: 0.9159989953041077, Accuracy: 1.0, Computation time: 1.6643435955047607\n",
      "Step: 3261, Loss: 0.9160063862800598, Accuracy: 1.0, Computation time: 1.7945573329925537\n",
      "Step: 3262, Loss: 0.9377724528312683, Accuracy: 0.9722222089767456, Computation time: 1.7225837707519531\n",
      "Step: 3263, Loss: 0.916109561920166, Accuracy: 1.0, Computation time: 1.9096715450286865\n",
      "Step: 3264, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 2.1770594120025635\n",
      "Step: 3265, Loss: 0.9159324765205383, Accuracy: 1.0, Computation time: 1.5786468982696533\n",
      "Step: 3266, Loss: 0.93532794713974, Accuracy: 0.9807692766189575, Computation time: 1.5601985454559326\n",
      "Step: 3267, Loss: 0.9160251021385193, Accuracy: 1.0, Computation time: 2.013073682785034\n",
      "Step: 3268, Loss: 0.9159332513809204, Accuracy: 1.0, Computation time: 1.651181697845459\n",
      "Step: 3269, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 1.872511863708496\n",
      "Step: 3270, Loss: 0.9159179329872131, Accuracy: 1.0, Computation time: 1.7378270626068115\n",
      "Step: 3271, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.6014750003814697\n",
      "Step: 3272, Loss: 0.9484461545944214, Accuracy: 0.9522727727890015, Computation time: 2.442545175552368\n",
      "Step: 3273, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.7746448516845703\n",
      "Step: 3274, Loss: 0.9537036418914795, Accuracy: 0.9583333730697632, Computation time: 2.336693525314331\n",
      "Step: 3275, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 1.4328184127807617\n",
      "Step: 3276, Loss: 0.9375007748603821, Accuracy: 0.9722222089767456, Computation time: 1.5210161209106445\n",
      "Step: 3277, Loss: 0.9159526228904724, Accuracy: 1.0, Computation time: 1.587599277496338\n",
      "Step: 3278, Loss: 0.9159761667251587, Accuracy: 1.0, Computation time: 1.5389647483825684\n",
      "Step: 3279, Loss: 0.9160557389259338, Accuracy: 1.0, Computation time: 1.4145495891571045\n",
      "Step: 3280, Loss: 0.9162208437919617, Accuracy: 1.0, Computation time: 1.3458914756774902\n",
      "Step: 3281, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.3515470027923584\n",
      "Step: 3282, Loss: 0.9163978695869446, Accuracy: 1.0, Computation time: 1.0827717781066895\n",
      "Step: 3283, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.4803552627563477\n",
      "Step: 3284, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.4760987758636475\n",
      "Step: 3285, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.5664887428283691\n",
      "Step: 3286, Loss: 0.9161685705184937, Accuracy: 1.0, Computation time: 1.551208257675171\n",
      "Step: 3287, Loss: 0.9161770939826965, Accuracy: 1.0, Computation time: 1.5280630588531494\n",
      "Step: 3288, Loss: 0.9375250339508057, Accuracy: 0.9807692766189575, Computation time: 1.4520106315612793\n",
      "Step: 3289, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 1.2429554462432861\n",
      "Step: 3290, Loss: 0.9377712607383728, Accuracy: 0.9722222089767456, Computation time: 1.2713630199432373\n",
      "Step: 3291, Loss: 0.9171879291534424, Accuracy: 1.0, Computation time: 1.6558945178985596\n",
      "Step: 3292, Loss: 0.9375774264335632, Accuracy: 0.9642857313156128, Computation time: 1.5953843593597412\n",
      "Step: 3293, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.117006778717041\n",
      "Step: 3294, Loss: 0.9376608729362488, Accuracy: 0.9791666865348816, Computation time: 1.6034324169158936\n",
      "Step: 3295, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.2026870250701904\n",
      "Step: 3296, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.1779823303222656\n",
      "Step: 3297, Loss: 0.916008472442627, Accuracy: 1.0, Computation time: 1.3980395793914795\n",
      "Step: 3298, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 1.2126617431640625\n",
      "Step: 3299, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.2398719787597656\n",
      "Step: 3300, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.1359302997589111\n",
      "Step: 3301, Loss: 0.9375087022781372, Accuracy: 0.9166666865348816, Computation time: 1.4640982151031494\n",
      "Step: 3302, Loss: 0.9161860942840576, Accuracy: 1.0, Computation time: 1.3780343532562256\n",
      "Step: 3303, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 0.994959831237793\n",
      "Step: 3304, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.288848876953125\n",
      "Step: 3305, Loss: 0.9159274101257324, Accuracy: 1.0, Computation time: 1.5751116275787354\n",
      "Step: 3306, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.5707077980041504\n",
      "Step: 3307, Loss: 0.9375609755516052, Accuracy: 0.9722222089767456, Computation time: 1.092777967453003\n",
      "Step: 3308, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.2972924709320068\n",
      "Step: 3309, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.5425026416778564\n",
      "Step: 3310, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.2800688743591309\n",
      "Step: 3311, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 1.2258834838867188\n",
      "Step: 3312, Loss: 0.9318511486053467, Accuracy: 0.9807692766189575, Computation time: 1.3296232223510742\n",
      "Step: 3313, Loss: 0.9353137612342834, Accuracy: 0.949999988079071, Computation time: 1.3100645542144775\n",
      "Step: 3314, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.0815520286560059\n",
      "Step: 3315, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.104311227798462\n",
      "Step: 3316, Loss: 0.9375481009483337, Accuracy: 0.9750000238418579, Computation time: 1.0959405899047852\n",
      "Step: 3317, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.1823358535766602\n",
      "Step: 3318, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.420851707458496\n",
      "Step: 3319, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.3510591983795166\n",
      "Step: 3320, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.3144474029541016\n",
      "Step: 3321, Loss: 0.9276931285858154, Accuracy: 0.9750000238418579, Computation time: 1.454540491104126\n",
      "Step: 3322, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.3504750728607178\n",
      "Step: 3323, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 1.6521098613739014\n",
      "Step: 3324, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 1.2860455513000488\n",
      "Step: 3325, Loss: 0.9373502731323242, Accuracy: 0.9833333492279053, Computation time: 1.5289373397827148\n",
      "Step: 3326, Loss: 0.915949821472168, Accuracy: 1.0, Computation time: 1.1305785179138184\n",
      "Step: 3327, Loss: 0.9291930794715881, Accuracy: 0.9722222089767456, Computation time: 1.4259040355682373\n",
      "Step: 3328, Loss: 0.9160407185554504, Accuracy: 1.0, Computation time: 0.9855105876922607\n",
      "Step: 3329, Loss: 0.9160919785499573, Accuracy: 1.0, Computation time: 1.1676881313323975\n",
      "Step: 3330, Loss: 0.9159142374992371, Accuracy: 1.0, Computation time: 1.0495951175689697\n",
      "Step: 3331, Loss: 0.9186155200004578, Accuracy: 1.0, Computation time: 1.099168300628662\n",
      "Step: 3332, Loss: 0.9159435033798218, Accuracy: 1.0, Computation time: 1.4588325023651123\n",
      "Step: 3333, Loss: 0.9268415570259094, Accuracy: 0.9772727489471436, Computation time: 1.544867753982544\n",
      "Step: 3334, Loss: 0.9164790511131287, Accuracy: 1.0, Computation time: 1.6554334163665771\n",
      "Step: 3335, Loss: 0.9159451127052307, Accuracy: 1.0, Computation time: 1.3949894905090332\n",
      "########################\n",
      "Test loss: 1.117419719696045, Test Accuracy_epoch24: 0.7038335204124451\n",
      "########################\n",
      "Step: 3336, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.2333106994628906\n",
      "Step: 3337, Loss: 0.9161930680274963, Accuracy: 1.0, Computation time: 1.4337882995605469\n",
      "Step: 3338, Loss: 0.9159218668937683, Accuracy: 1.0, Computation time: 1.459033727645874\n",
      "Step: 3339, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.3157753944396973\n",
      "Step: 3340, Loss: 0.9160541296005249, Accuracy: 1.0, Computation time: 1.405435562133789\n",
      "Step: 3341, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.4868133068084717\n",
      "Step: 3342, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.2382197380065918\n",
      "Step: 3343, Loss: 0.9375752806663513, Accuracy: 0.9791666865348816, Computation time: 1.378617525100708\n",
      "Step: 3344, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.1673245429992676\n",
      "Step: 3345, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.8356142044067383\n",
      "Step: 3346, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.6318552494049072\n",
      "Step: 3347, Loss: 0.9261842370033264, Accuracy: 0.9807692766189575, Computation time: 1.7719788551330566\n",
      "Step: 3348, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 1.6670472621917725\n",
      "Step: 3349, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 1.3401639461517334\n",
      "Step: 3350, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 1.5274078845977783\n",
      "Step: 3351, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 1.2843477725982666\n",
      "Step: 3352, Loss: 0.9159541726112366, Accuracy: 1.0, Computation time: 1.4890220165252686\n",
      "Step: 3353, Loss: 0.9160208106040955, Accuracy: 1.0, Computation time: 1.5693585872650146\n",
      "Step: 3354, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.3657910823822021\n",
      "Step: 3355, Loss: 0.9374534487724304, Accuracy: 0.9807692766189575, Computation time: 1.6008970737457275\n",
      "Step: 3356, Loss: 0.9339650869369507, Accuracy: 0.9750000238418579, Computation time: 1.6126446723937988\n",
      "Step: 3357, Loss: 0.9161502718925476, Accuracy: 1.0, Computation time: 1.4169516563415527\n",
      "Step: 3358, Loss: 0.9159160256385803, Accuracy: 1.0, Computation time: 1.4049394130706787\n",
      "Step: 3359, Loss: 0.9208544492721558, Accuracy: 1.0, Computation time: 1.6402795314788818\n",
      "Step: 3360, Loss: 0.9281758069992065, Accuracy: 0.9772727489471436, Computation time: 1.53135347366333\n",
      "Step: 3361, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 1.5918188095092773\n",
      "Step: 3362, Loss: 0.937424898147583, Accuracy: 0.9722222089767456, Computation time: 1.3914434909820557\n",
      "Step: 3363, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 1.3186779022216797\n",
      "Step: 3364, Loss: 0.9159172177314758, Accuracy: 1.0, Computation time: 1.5078659057617188\n",
      "Step: 3365, Loss: 0.937545120716095, Accuracy: 0.9722222089767456, Computation time: 1.6998775005340576\n",
      "Step: 3366, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.3683509826660156\n",
      "Step: 3367, Loss: 0.9376105070114136, Accuracy: 0.96875, Computation time: 1.5405001640319824\n",
      "Step: 3368, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 1.2330436706542969\n",
      "Step: 3369, Loss: 0.9206136465072632, Accuracy: 1.0, Computation time: 1.7811336517333984\n",
      "Step: 3370, Loss: 0.9159038662910461, Accuracy: 1.0, Computation time: 1.1395807266235352\n",
      "Step: 3371, Loss: 0.9159889817237854, Accuracy: 1.0, Computation time: 1.3917193412780762\n",
      "Step: 3372, Loss: 0.916042149066925, Accuracy: 1.0, Computation time: 1.4653046131134033\n",
      "Step: 3373, Loss: 0.9159286618232727, Accuracy: 1.0, Computation time: 1.6383929252624512\n",
      "Step: 3374, Loss: 0.9373927712440491, Accuracy: 0.9807692766189575, Computation time: 1.2542963027954102\n",
      "Step: 3375, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.4919073581695557\n",
      "Step: 3376, Loss: 0.9182817935943604, Accuracy: 1.0, Computation time: 1.747230052947998\n",
      "Step: 3377, Loss: 0.9161328077316284, Accuracy: 1.0, Computation time: 1.3189265727996826\n",
      "Step: 3378, Loss: 0.9235826730728149, Accuracy: 1.0, Computation time: 1.568716287612915\n",
      "Step: 3379, Loss: 0.9159561991691589, Accuracy: 1.0, Computation time: 1.3362305164337158\n",
      "Step: 3380, Loss: 0.9175676107406616, Accuracy: 1.0, Computation time: 1.1993861198425293\n",
      "Step: 3381, Loss: 0.9409397840499878, Accuracy: 0.949999988079071, Computation time: 1.7946109771728516\n",
      "Step: 3382, Loss: 0.9347114562988281, Accuracy: 0.949999988079071, Computation time: 1.5996050834655762\n",
      "Step: 3383, Loss: 0.9161249995231628, Accuracy: 1.0, Computation time: 1.6843092441558838\n",
      "Step: 3384, Loss: 0.9161238074302673, Accuracy: 1.0, Computation time: 1.6868927478790283\n",
      "Step: 3385, Loss: 0.9160354733467102, Accuracy: 1.0, Computation time: 1.345839262008667\n",
      "Step: 3386, Loss: 0.916993260383606, Accuracy: 1.0, Computation time: 1.431328535079956\n",
      "Step: 3387, Loss: 0.9350863695144653, Accuracy: 0.9750000238418579, Computation time: 1.4355928897857666\n",
      "Step: 3388, Loss: 0.9173960089683533, Accuracy: 1.0, Computation time: 1.5039699077606201\n",
      "Step: 3389, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.465491533279419\n",
      "Step: 3390, Loss: 0.9159342050552368, Accuracy: 1.0, Computation time: 1.2489731311798096\n",
      "Step: 3391, Loss: 0.9160272479057312, Accuracy: 1.0, Computation time: 1.6878294944763184\n",
      "Step: 3392, Loss: 0.9160072803497314, Accuracy: 1.0, Computation time: 1.4523742198944092\n",
      "Step: 3393, Loss: 0.937665581703186, Accuracy: 0.9722222089767456, Computation time: 1.5449562072753906\n",
      "Step: 3394, Loss: 0.9160314798355103, Accuracy: 1.0, Computation time: 1.6408374309539795\n",
      "Step: 3395, Loss: 0.9159597158432007, Accuracy: 1.0, Computation time: 1.3991703987121582\n",
      "Step: 3396, Loss: 0.9162898063659668, Accuracy: 1.0, Computation time: 1.986091136932373\n",
      "Step: 3397, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.3373217582702637\n",
      "Step: 3398, Loss: 0.9373481273651123, Accuracy: 0.9722222089767456, Computation time: 1.9482946395874023\n",
      "Step: 3399, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.292980432510376\n",
      "Step: 3400, Loss: 0.9158950448036194, Accuracy: 1.0, Computation time: 1.2355151176452637\n",
      "Step: 3401, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 1.5580852031707764\n",
      "Step: 3402, Loss: 0.9161236882209778, Accuracy: 1.0, Computation time: 1.3371782302856445\n",
      "Step: 3403, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.69089674949646\n",
      "Step: 3404, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.8167166709899902\n",
      "Step: 3405, Loss: 0.9161495566368103, Accuracy: 1.0, Computation time: 1.2176291942596436\n",
      "Step: 3406, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.492788314819336\n",
      "Step: 3407, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.642829418182373\n",
      "Step: 3408, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.7028436660766602\n",
      "Step: 3409, Loss: 0.9374615550041199, Accuracy: 0.9166666865348816, Computation time: 1.7230172157287598\n",
      "Step: 3410, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.5080595016479492\n",
      "Step: 3411, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.5805630683898926\n",
      "Step: 3412, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.509779930114746\n",
      "Step: 3413, Loss: 0.9165056943893433, Accuracy: 1.0, Computation time: 1.6350016593933105\n",
      "Step: 3414, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.4369337558746338\n",
      "Step: 3415, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.5344929695129395\n",
      "Step: 3416, Loss: 0.9173740744590759, Accuracy: 1.0, Computation time: 1.362342119216919\n",
      "Step: 3417, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.9779009819030762\n",
      "Step: 3418, Loss: 0.9159227013587952, Accuracy: 1.0, Computation time: 1.4563612937927246\n",
      "Step: 3419, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.240182638168335\n",
      "Step: 3420, Loss: 0.9158858060836792, Accuracy: 1.0, Computation time: 1.479539394378662\n",
      "Step: 3421, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.2976620197296143\n",
      "Step: 3422, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.867161512374878\n",
      "Step: 3423, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.633296012878418\n",
      "Step: 3424, Loss: 0.9543271064758301, Accuracy: 0.9365079402923584, Computation time: 1.5300002098083496\n",
      "Step: 3425, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.7035388946533203\n",
      "Step: 3426, Loss: 0.9162318110466003, Accuracy: 1.0, Computation time: 1.497922658920288\n",
      "Step: 3427, Loss: 0.9371392130851746, Accuracy: 0.9583333730697632, Computation time: 1.565194845199585\n",
      "Step: 3428, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 1.301408052444458\n",
      "Step: 3429, Loss: 0.9158980250358582, Accuracy: 1.0, Computation time: 1.4342491626739502\n",
      "Step: 3430, Loss: 0.93182373046875, Accuracy: 0.9722222089767456, Computation time: 2.0001399517059326\n",
      "Step: 3431, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.6247100830078125\n",
      "Step: 3432, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 1.555187463760376\n",
      "Step: 3433, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.5813534259796143\n",
      "Step: 3434, Loss: 0.9160258769989014, Accuracy: 1.0, Computation time: 1.4932096004486084\n",
      "Step: 3435, Loss: 0.9159886837005615, Accuracy: 1.0, Computation time: 1.8006467819213867\n",
      "Step: 3436, Loss: 0.9160534739494324, Accuracy: 1.0, Computation time: 2.1719326972961426\n",
      "Step: 3437, Loss: 0.9161684513092041, Accuracy: 1.0, Computation time: 1.9922430515289307\n",
      "Step: 3438, Loss: 0.9159765243530273, Accuracy: 1.0, Computation time: 1.860111951828003\n",
      "Step: 3439, Loss: 0.9159289598464966, Accuracy: 1.0, Computation time: 1.9237432479858398\n",
      "Step: 3440, Loss: 0.9165840148925781, Accuracy: 1.0, Computation time: 2.170710563659668\n",
      "Step: 3441, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 2.076200246810913\n",
      "Step: 3442, Loss: 0.9249505400657654, Accuracy: 1.0, Computation time: 1.7647247314453125\n",
      "Step: 3443, Loss: 0.9159762263298035, Accuracy: 1.0, Computation time: 1.7540898323059082\n",
      "Step: 3444, Loss: 0.9160515666007996, Accuracy: 1.0, Computation time: 1.8466377258300781\n",
      "Step: 3445, Loss: 0.9356014132499695, Accuracy: 0.9807692766189575, Computation time: 1.8015706539154053\n",
      "Step: 3446, Loss: 0.9378089308738708, Accuracy: 0.9807692766189575, Computation time: 1.8619446754455566\n",
      "Step: 3447, Loss: 0.9161442518234253, Accuracy: 1.0, Computation time: 2.0698869228363037\n",
      "Step: 3448, Loss: 0.9479238390922546, Accuracy: 0.9444444179534912, Computation time: 2.0063822269439697\n",
      "Step: 3449, Loss: 0.9197571277618408, Accuracy: 1.0, Computation time: 1.4885451793670654\n",
      "Step: 3450, Loss: 0.9257548451423645, Accuracy: 0.949999988079071, Computation time: 1.5024657249450684\n",
      "Step: 3451, Loss: 0.916330873966217, Accuracy: 1.0, Computation time: 1.7698986530303955\n",
      "Step: 3452, Loss: 0.9165967702865601, Accuracy: 1.0, Computation time: 1.542689561843872\n",
      "Step: 3453, Loss: 0.9160356521606445, Accuracy: 1.0, Computation time: 1.7106070518493652\n",
      "Step: 3454, Loss: 0.9160072803497314, Accuracy: 1.0, Computation time: 1.2398920059204102\n",
      "Step: 3455, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 1.3001000881195068\n",
      "Step: 3456, Loss: 0.9160730242729187, Accuracy: nan, Computation time: 1.8153557777404785\n",
      "Step: 3457, Loss: 0.9161897897720337, Accuracy: 1.0, Computation time: 1.2488329410552979\n",
      "Step: 3458, Loss: 0.9342213869094849, Accuracy: 0.9772727489471436, Computation time: 1.9798808097839355\n",
      "Step: 3459, Loss: 0.9160304069519043, Accuracy: 1.0, Computation time: 1.4031875133514404\n",
      "Step: 3460, Loss: 0.9375633597373962, Accuracy: 0.9807692766189575, Computation time: 1.641874074935913\n",
      "Step: 3461, Loss: 0.9380596876144409, Accuracy: 0.96875, Computation time: 1.5092649459838867\n",
      "Step: 3462, Loss: 0.9346259236335754, Accuracy: 0.9772727489471436, Computation time: 1.7080059051513672\n",
      "Step: 3463, Loss: 0.9161781668663025, Accuracy: 1.0, Computation time: 1.6180229187011719\n",
      "Step: 3464, Loss: 0.9161311388015747, Accuracy: 1.0, Computation time: 2.3809916973114014\n",
      "Step: 3465, Loss: 0.9160864353179932, Accuracy: 1.0, Computation time: 1.318617343902588\n",
      "Step: 3466, Loss: 0.9160816669464111, Accuracy: 1.0, Computation time: 1.3237202167510986\n",
      "Step: 3467, Loss: 0.9160584211349487, Accuracy: 1.0, Computation time: 1.1496407985687256\n",
      "Step: 3468, Loss: 0.9159768223762512, Accuracy: 1.0, Computation time: 1.3527233600616455\n",
      "Step: 3469, Loss: 0.9159618616104126, Accuracy: 1.0, Computation time: 1.263375997543335\n",
      "Step: 3470, Loss: 0.9160563945770264, Accuracy: 1.0, Computation time: 1.1403586864471436\n",
      "Step: 3471, Loss: 0.9159489870071411, Accuracy: 1.0, Computation time: 1.101363182067871\n",
      "Step: 3472, Loss: 0.9171417355537415, Accuracy: 1.0, Computation time: 1.5744214057922363\n",
      "Step: 3473, Loss: 0.9162135124206543, Accuracy: 1.0, Computation time: 1.1947572231292725\n",
      "Step: 3474, Loss: 0.9159818291664124, Accuracy: 1.0, Computation time: 1.4606709480285645\n",
      "########################\n",
      "Test loss: 1.1268236637115479, Test Accuracy_epoch25: 0.6909010410308838\n",
      "########################\n",
      "Step: 3475, Loss: 0.9159713387489319, Accuracy: 1.0, Computation time: 1.6476097106933594\n",
      "Step: 3476, Loss: 0.9159725308418274, Accuracy: 1.0, Computation time: 1.6046340465545654\n",
      "Step: 3477, Loss: 0.9162408709526062, Accuracy: 1.0, Computation time: 1.4270882606506348\n",
      "Step: 3478, Loss: 0.9374768137931824, Accuracy: 0.96875, Computation time: 1.1208927631378174\n",
      "Step: 3479, Loss: 0.9160504937171936, Accuracy: 1.0, Computation time: 1.628915786743164\n",
      "Step: 3480, Loss: 0.916231632232666, Accuracy: 1.0, Computation time: 1.2324812412261963\n",
      "Step: 3481, Loss: 0.9375668168067932, Accuracy: 0.9722222089767456, Computation time: 1.2588081359863281\n",
      "Step: 3482, Loss: 0.9417115449905396, Accuracy: 0.9833333492279053, Computation time: 1.5323660373687744\n",
      "Step: 3483, Loss: 0.937708854675293, Accuracy: 0.9750000238418579, Computation time: 1.3252217769622803\n",
      "Step: 3484, Loss: 0.9374844431877136, Accuracy: 0.9583333730697632, Computation time: 1.4627995491027832\n",
      "Step: 3485, Loss: 0.9160197377204895, Accuracy: 1.0, Computation time: 1.2018787860870361\n",
      "Step: 3486, Loss: 0.9163718223571777, Accuracy: 1.0, Computation time: 2.0629725456237793\n",
      "Step: 3487, Loss: 0.9165756702423096, Accuracy: 1.0, Computation time: 1.5136616230010986\n",
      "Step: 3488, Loss: 0.9437304139137268, Accuracy: 0.9722222089767456, Computation time: 2.1381194591522217\n",
      "Step: 3489, Loss: 0.9159678816795349, Accuracy: 1.0, Computation time: 1.4981439113616943\n",
      "Step: 3490, Loss: 0.9161427617073059, Accuracy: 1.0, Computation time: 1.5140807628631592\n",
      "Step: 3491, Loss: 0.9163937568664551, Accuracy: 1.0, Computation time: 1.464085578918457\n",
      "Step: 3492, Loss: 0.916270911693573, Accuracy: 1.0, Computation time: 1.0659830570220947\n",
      "Step: 3493, Loss: 0.9368381500244141, Accuracy: 0.9807692766189575, Computation time: 1.9257159233093262\n",
      "Step: 3494, Loss: 0.9159784317016602, Accuracy: 1.0, Computation time: 1.2440130710601807\n",
      "Step: 3495, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.4428553581237793\n",
      "Step: 3496, Loss: 0.9331510066986084, Accuracy: 0.9807692766189575, Computation time: 1.586660623550415\n",
      "Step: 3497, Loss: 0.9241540431976318, Accuracy: 1.0, Computation time: 1.425553560256958\n",
      "Step: 3498, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.4343745708465576\n",
      "Step: 3499, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.597538709640503\n",
      "Step: 3500, Loss: 0.9162850975990295, Accuracy: 1.0, Computation time: 1.2729229927062988\n",
      "Step: 3501, Loss: 0.916021466255188, Accuracy: 1.0, Computation time: 1.2191884517669678\n",
      "Step: 3502, Loss: 0.9159684777259827, Accuracy: 1.0, Computation time: 1.4118320941925049\n",
      "Step: 3503, Loss: 0.9159788489341736, Accuracy: 1.0, Computation time: 1.2518270015716553\n",
      "Step: 3504, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.296583890914917\n",
      "Step: 3505, Loss: 0.9251294732093811, Accuracy: 1.0, Computation time: 2.3233437538146973\n",
      "Step: 3506, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.3064484596252441\n",
      "Step: 3507, Loss: 0.9380804896354675, Accuracy: 0.9833333492279053, Computation time: 1.5441617965698242\n",
      "Step: 3508, Loss: 0.9376896619796753, Accuracy: 0.9791666865348816, Computation time: 1.323305368423462\n",
      "Step: 3509, Loss: 0.9365891218185425, Accuracy: 0.96875, Computation time: 1.362473487854004\n",
      "Step: 3510, Loss: 0.9159610271453857, Accuracy: 1.0, Computation time: 1.401578664779663\n",
      "Step: 3511, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 1.5623555183410645\n",
      "Step: 3512, Loss: 0.9346855282783508, Accuracy: 0.9772727489471436, Computation time: 1.5335543155670166\n",
      "Step: 3513, Loss: 0.9374940991401672, Accuracy: 0.96875, Computation time: 1.3734941482543945\n",
      "Step: 3514, Loss: 0.9546238780021667, Accuracy: 0.9365079402923584, Computation time: 1.5252368450164795\n",
      "Step: 3515, Loss: 0.9376816749572754, Accuracy: 0.9807692766189575, Computation time: 1.5830790996551514\n",
      "Step: 3516, Loss: 0.9330196380615234, Accuracy: 0.96875, Computation time: 1.8735980987548828\n",
      "Step: 3517, Loss: 0.9160376191139221, Accuracy: 1.0, Computation time: 1.3390440940856934\n",
      "Step: 3518, Loss: 0.9161516427993774, Accuracy: 1.0, Computation time: 1.472503662109375\n",
      "Step: 3519, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 1.5417568683624268\n",
      "Step: 3520, Loss: 0.9160754680633545, Accuracy: 1.0, Computation time: 1.4655356407165527\n",
      "Step: 3521, Loss: 0.9160187244415283, Accuracy: 1.0, Computation time: 1.445894718170166\n",
      "Step: 3522, Loss: 0.9159771800041199, Accuracy: 1.0, Computation time: 1.2979507446289062\n",
      "Step: 3523, Loss: 0.9159635901451111, Accuracy: 1.0, Computation time: 1.3942334651947021\n",
      "Step: 3524, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 1.3102235794067383\n",
      "Step: 3525, Loss: 0.9160371422767639, Accuracy: 1.0, Computation time: 1.4523203372955322\n",
      "Step: 3526, Loss: 0.9587340354919434, Accuracy: 0.9615384340286255, Computation time: 1.2956085205078125\n",
      "Step: 3527, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.3780674934387207\n",
      "Step: 3528, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.4708287715911865\n",
      "Step: 3529, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.4649486541748047\n",
      "Step: 3530, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.0396051406860352\n",
      "Step: 3531, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.4308545589447021\n",
      "Step: 3532, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.222949504852295\n",
      "Step: 3533, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.4409105777740479\n",
      "Step: 3534, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.5892667770385742\n",
      "Step: 3535, Loss: 0.9200600981712341, Accuracy: 1.0, Computation time: 1.5230560302734375\n",
      "Step: 3536, Loss: 0.9376358985900879, Accuracy: 0.9750000238418579, Computation time: 1.6181466579437256\n",
      "Step: 3537, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.2186565399169922\n",
      "Step: 3538, Loss: 0.9170845150947571, Accuracy: 1.0, Computation time: 1.7683923244476318\n",
      "Step: 3539, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.2984073162078857\n",
      "Step: 3540, Loss: 0.9159099459648132, Accuracy: 1.0, Computation time: 1.1806106567382812\n",
      "Step: 3541, Loss: 0.9365529417991638, Accuracy: 0.9807692766189575, Computation time: 1.3460943698883057\n",
      "Step: 3542, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.1878514289855957\n",
      "Step: 3543, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.3684203624725342\n",
      "Step: 3544, Loss: 0.9377696514129639, Accuracy: 0.9583333730697632, Computation time: 1.7758750915527344\n",
      "Step: 3545, Loss: 0.9161133170127869, Accuracy: 1.0, Computation time: 1.4250621795654297\n",
      "Step: 3546, Loss: 0.9376123547554016, Accuracy: 0.96875, Computation time: 1.5602695941925049\n",
      "Step: 3547, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.4001171588897705\n",
      "Step: 3548, Loss: 0.9163216948509216, Accuracy: 1.0, Computation time: 1.3362343311309814\n",
      "Step: 3549, Loss: 0.9247292280197144, Accuracy: 1.0, Computation time: 2.0705745220184326\n",
      "Step: 3550, Loss: 0.9233967065811157, Accuracy: 1.0, Computation time: 1.7499744892120361\n",
      "Step: 3551, Loss: 0.9376843571662903, Accuracy: 0.9583333730697632, Computation time: 1.5925185680389404\n",
      "Step: 3552, Loss: 0.9160330891609192, Accuracy: 1.0, Computation time: 1.5513908863067627\n",
      "Step: 3553, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.5349876880645752\n",
      "Step: 3554, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.7251112461090088\n",
      "Step: 3555, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.762974500656128\n",
      "Step: 3556, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.3735935688018799\n",
      "Step: 3557, Loss: 0.9160557389259338, Accuracy: 1.0, Computation time: 1.6566822528839111\n",
      "Step: 3558, Loss: 0.9172635078430176, Accuracy: 1.0, Computation time: 1.767500400543213\n",
      "Step: 3559, Loss: 0.9159698486328125, Accuracy: 1.0, Computation time: 1.771108627319336\n",
      "Step: 3560, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 1.418588638305664\n",
      "Step: 3561, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.360924243927002\n",
      "Step: 3562, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.4516880512237549\n",
      "Step: 3563, Loss: 0.9161311984062195, Accuracy: 1.0, Computation time: 1.7194530963897705\n",
      "Step: 3564, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.3895301818847656\n",
      "Step: 3565, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.5231349468231201\n",
      "Step: 3566, Loss: 0.9175951480865479, Accuracy: 1.0, Computation time: 1.7286722660064697\n",
      "Step: 3567, Loss: 0.9376383423805237, Accuracy: 0.9807692766189575, Computation time: 1.3732099533081055\n",
      "Step: 3568, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.7552461624145508\n",
      "Step: 3569, Loss: 0.9158411622047424, Accuracy: 1.0, Computation time: 1.3478939533233643\n",
      "Step: 3570, Loss: 0.9372779130935669, Accuracy: 0.9750000238418579, Computation time: 1.4030308723449707\n",
      "Step: 3571, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 1.1977920532226562\n",
      "Step: 3572, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 1.4786121845245361\n",
      "Step: 3573, Loss: 0.9208647608757019, Accuracy: 1.0, Computation time: 1.3658428192138672\n",
      "Step: 3574, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.3199806213378906\n",
      "Step: 3575, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 1.3253188133239746\n",
      "Step: 3576, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.5719506740570068\n",
      "Step: 3577, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.757880687713623\n",
      "Step: 3578, Loss: 0.9338246583938599, Accuracy: 0.9642857313156128, Computation time: 2.63570499420166\n",
      "Step: 3579, Loss: 0.9390730261802673, Accuracy: 0.9821428656578064, Computation time: 1.6416959762573242\n",
      "Step: 3580, Loss: 0.9165146350860596, Accuracy: 1.0, Computation time: 1.6724445819854736\n",
      "Step: 3581, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.7948939800262451\n",
      "Step: 3582, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 1.606515884399414\n",
      "Step: 3583, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.7577104568481445\n",
      "Step: 3584, Loss: 0.9159514904022217, Accuracy: 1.0, Computation time: 1.7448315620422363\n",
      "Step: 3585, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 2.2434239387512207\n",
      "Step: 3586, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 1.9649581909179688\n",
      "Step: 3587, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.5888657569885254\n",
      "Step: 3588, Loss: 0.9171654582023621, Accuracy: 1.0, Computation time: 2.0588414669036865\n",
      "Step: 3589, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.7439439296722412\n",
      "Step: 3590, Loss: 0.9303762912750244, Accuracy: 0.9750000238418579, Computation time: 1.9870669841766357\n",
      "Step: 3591, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.8766560554504395\n",
      "Step: 3592, Loss: 0.9159787893295288, Accuracy: 1.0, Computation time: 2.0330770015716553\n",
      "Step: 3593, Loss: 0.919613242149353, Accuracy: 1.0, Computation time: 1.6927435398101807\n",
      "Step: 3594, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 1.3109076023101807\n",
      "Step: 3595, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 1.4463834762573242\n",
      "Step: 3596, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.6748239994049072\n",
      "Step: 3597, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.4261958599090576\n",
      "Step: 3598, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 1.6119723320007324\n",
      "Step: 3599, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.6637811660766602\n",
      "Step: 3600, Loss: 0.9167404174804688, Accuracy: 1.0, Computation time: 1.7094407081604004\n",
      "Step: 3601, Loss: 0.915997326374054, Accuracy: 1.0, Computation time: 1.1597249507904053\n",
      "Step: 3602, Loss: 0.9374510049819946, Accuracy: 0.9750000238418579, Computation time: 1.3069305419921875\n",
      "Step: 3603, Loss: 0.9252727031707764, Accuracy: 1.0, Computation time: 1.6112871170043945\n",
      "Step: 3604, Loss: 0.9159674048423767, Accuracy: 1.0, Computation time: 1.7601072788238525\n",
      "Step: 3605, Loss: 0.9160600900650024, Accuracy: 1.0, Computation time: 1.6825711727142334\n",
      "Step: 3606, Loss: 0.9159629344940186, Accuracy: 1.0, Computation time: 1.42189359664917\n",
      "Step: 3607, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 1.2790617942810059\n",
      "Step: 3608, Loss: 0.9159376621246338, Accuracy: 1.0, Computation time: 1.1801340579986572\n",
      "Step: 3609, Loss: 0.9400933980941772, Accuracy: 0.9375, Computation time: 1.4913139343261719\n",
      "Step: 3610, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.5809803009033203\n",
      "Step: 3611, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.4484751224517822\n",
      "Step: 3612, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 1.6036279201507568\n",
      "Step: 3613, Loss: 0.9375753402709961, Accuracy: 0.9750000238418579, Computation time: 1.8867335319519043\n",
      "########################\n",
      "Test loss: 1.1190309524536133, Test Accuracy_epoch26: 0.7035164833068848\n",
      "########################\n",
      "Step: 3614, Loss: 0.9159091711044312, Accuracy: 1.0, Computation time: 1.4212617874145508\n",
      "Step: 3615, Loss: 0.9376857280731201, Accuracy: 0.9750000238418579, Computation time: 1.4291009902954102\n",
      "Step: 3616, Loss: 0.9165230989456177, Accuracy: 1.0, Computation time: 1.8804590702056885\n",
      "Step: 3617, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.4469048976898193\n",
      "Step: 3618, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.6644389629364014\n",
      "Step: 3619, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.6704902648925781\n",
      "Step: 3620, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.8096117973327637\n",
      "Step: 3621, Loss: 0.9362736344337463, Accuracy: 0.9807692766189575, Computation time: 1.6266984939575195\n",
      "Step: 3622, Loss: 0.9230853319168091, Accuracy: 1.0, Computation time: 1.7566509246826172\n",
      "Step: 3623, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.3989524841308594\n",
      "Step: 3624, Loss: 0.9160004258155823, Accuracy: 1.0, Computation time: 1.2331321239471436\n",
      "Step: 3625, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 1.4764623641967773\n",
      "Step: 3626, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.5392074584960938\n",
      "Step: 3627, Loss: 0.9170769453048706, Accuracy: 1.0, Computation time: 1.667020559310913\n",
      "Step: 3628, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.5776383876800537\n",
      "Step: 3629, Loss: 0.9184776544570923, Accuracy: 1.0, Computation time: 1.8153178691864014\n",
      "Step: 3630, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.6811347007751465\n",
      "Step: 3631, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.6255626678466797\n",
      "Step: 3632, Loss: 0.915949285030365, Accuracy: 1.0, Computation time: 1.7070436477661133\n",
      "Step: 3633, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 1.39453125\n",
      "Step: 3634, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.3675575256347656\n",
      "Step: 3635, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.5688438415527344\n",
      "Step: 3636, Loss: 0.9158980846405029, Accuracy: 1.0, Computation time: 1.5740697383880615\n",
      "Step: 3637, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 1.2392544746398926\n",
      "Step: 3638, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.400141716003418\n",
      "Step: 3639, Loss: 0.9374507069587708, Accuracy: 0.9642857313156128, Computation time: 1.3796536922454834\n",
      "Step: 3640, Loss: 0.9370662569999695, Accuracy: 0.9750000238418579, Computation time: 1.496511459350586\n",
      "Step: 3641, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.216092824935913\n",
      "Step: 3642, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.349022388458252\n",
      "Step: 3643, Loss: 0.9375132322311401, Accuracy: 0.9722222089767456, Computation time: 1.60349440574646\n",
      "Step: 3644, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.7345004081726074\n",
      "Step: 3645, Loss: 0.9374584555625916, Accuracy: 0.9772727489471436, Computation time: 1.8257033824920654\n",
      "Step: 3646, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.3247511386871338\n",
      "Step: 3647, Loss: 0.9158689975738525, Accuracy: 1.0, Computation time: 1.727691411972046\n",
      "Step: 3648, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.5251307487487793\n",
      "Step: 3649, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.6664490699768066\n",
      "Step: 3650, Loss: 0.9374530911445618, Accuracy: 0.9750000238418579, Computation time: 1.37442946434021\n",
      "Step: 3651, Loss: 0.9375549554824829, Accuracy: 0.96875, Computation time: 1.832592248916626\n",
      "Step: 3652, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.5910260677337646\n",
      "Step: 3653, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.3827896118164062\n",
      "Step: 3654, Loss: 0.9159718155860901, Accuracy: 1.0, Computation time: 1.4667325019836426\n",
      "Step: 3655, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.4649055004119873\n",
      "Step: 3656, Loss: 0.9160131216049194, Accuracy: 1.0, Computation time: 1.611828327178955\n",
      "Step: 3657, Loss: 0.9159133434295654, Accuracy: 1.0, Computation time: 1.6103904247283936\n",
      "Step: 3658, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 1.5319159030914307\n",
      "Step: 3659, Loss: 0.9165284037590027, Accuracy: 1.0, Computation time: 1.7925493717193604\n",
      "Step: 3660, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.5753004550933838\n",
      "Step: 3661, Loss: 0.9225037097930908, Accuracy: 1.0, Computation time: 1.5282251834869385\n",
      "Step: 3662, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.6253609657287598\n",
      "Step: 3663, Loss: 0.9159719347953796, Accuracy: 1.0, Computation time: 1.4885883331298828\n",
      "Step: 3664, Loss: 0.918456494808197, Accuracy: 1.0, Computation time: 1.5861234664916992\n",
      "Step: 3665, Loss: 0.9169256687164307, Accuracy: 1.0, Computation time: 1.9247548580169678\n",
      "Step: 3666, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 1.542943000793457\n",
      "Step: 3667, Loss: 0.9159116744995117, Accuracy: 1.0, Computation time: 1.5242817401885986\n",
      "Step: 3668, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.3887982368469238\n",
      "Step: 3669, Loss: 0.9159620404243469, Accuracy: 1.0, Computation time: 1.6257781982421875\n",
      "Step: 3670, Loss: 0.9163292050361633, Accuracy: 1.0, Computation time: 2.147589921951294\n",
      "Step: 3671, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.5513834953308105\n",
      "Step: 3672, Loss: 0.9376341104507446, Accuracy: 0.949999988079071, Computation time: 1.5833401679992676\n",
      "Step: 3673, Loss: 0.9159061312675476, Accuracy: 1.0, Computation time: 1.6860973834991455\n",
      "Step: 3674, Loss: 0.9373379945755005, Accuracy: 0.9166666865348816, Computation time: 1.7117915153503418\n",
      "Step: 3675, Loss: 0.9374062418937683, Accuracy: 0.9583333730697632, Computation time: 1.865541696548462\n",
      "Step: 3676, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.618596076965332\n",
      "Step: 3677, Loss: 0.9159199595451355, Accuracy: 1.0, Computation time: 1.8550138473510742\n",
      "Step: 3678, Loss: 0.9375593066215515, Accuracy: 0.9722222089767456, Computation time: 1.9634532928466797\n",
      "Step: 3679, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 1.7215793132781982\n",
      "Step: 3680, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.72025465965271\n",
      "Step: 3681, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.8654096126556396\n",
      "Step: 3682, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.8557982444763184\n",
      "Step: 3683, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.5849995613098145\n",
      "Step: 3684, Loss: 0.9158819317817688, Accuracy: 1.0, Computation time: 1.6218445301055908\n",
      "Step: 3685, Loss: 0.9170057773590088, Accuracy: 1.0, Computation time: 1.6468136310577393\n",
      "Step: 3686, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.8280739784240723\n",
      "Step: 3687, Loss: 0.9159971475601196, Accuracy: 1.0, Computation time: 1.5460426807403564\n",
      "Step: 3688, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 1.8552272319793701\n",
      "Step: 3689, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.6356098651885986\n",
      "Step: 3690, Loss: 0.9159227013587952, Accuracy: 1.0, Computation time: 1.411475658416748\n",
      "Step: 3691, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 1.7224807739257812\n",
      "Step: 3692, Loss: 0.9159705638885498, Accuracy: 1.0, Computation time: 1.602884292602539\n",
      "Step: 3693, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.6186635494232178\n",
      "Step: 3694, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 2.031521797180176\n",
      "Step: 3695, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 1.9404714107513428\n",
      "Step: 3696, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.8791210651397705\n",
      "Step: 3697, Loss: 0.9376122951507568, Accuracy: 0.9772727489471436, Computation time: 1.8392963409423828\n",
      "Step: 3698, Loss: 0.9267137050628662, Accuracy: 0.9750000238418579, Computation time: 1.9389839172363281\n",
      "Step: 3699, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 2.00886869430542\n",
      "Step: 3700, Loss: 0.915949285030365, Accuracy: 1.0, Computation time: 1.8466529846191406\n",
      "Step: 3701, Loss: 0.9159867167472839, Accuracy: 1.0, Computation time: 2.0048716068267822\n",
      "Step: 3702, Loss: 0.9161379933357239, Accuracy: 1.0, Computation time: 1.9275991916656494\n",
      "Step: 3703, Loss: 0.9161064624786377, Accuracy: 1.0, Computation time: 1.8542428016662598\n",
      "Step: 3704, Loss: 0.9173759818077087, Accuracy: 1.0, Computation time: 1.7950685024261475\n",
      "Step: 3705, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 1.8877198696136475\n",
      "Step: 3706, Loss: 0.9265850186347961, Accuracy: 0.9821428656578064, Computation time: 2.3364341259002686\n",
      "Step: 3707, Loss: 0.9159947633743286, Accuracy: 1.0, Computation time: 1.831197738647461\n",
      "Step: 3708, Loss: 0.9162473082542419, Accuracy: 1.0, Computation time: 1.784769058227539\n",
      "Step: 3709, Loss: 0.916450560092926, Accuracy: 1.0, Computation time: 2.2086644172668457\n",
      "Step: 3710, Loss: 0.9169871211051941, Accuracy: 1.0, Computation time: 1.848339319229126\n",
      "Step: 3711, Loss: 0.9164600968360901, Accuracy: 1.0, Computation time: 1.9316964149475098\n",
      "Step: 3712, Loss: 0.9161501526832581, Accuracy: 1.0, Computation time: 1.7531471252441406\n",
      "Step: 3713, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.6537413597106934\n",
      "Step: 3714, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.7381665706634521\n",
      "Step: 3715, Loss: 0.9414214491844177, Accuracy: 0.9772727489471436, Computation time: 2.271265745162964\n",
      "Step: 3716, Loss: 0.9234604835510254, Accuracy: 1.0, Computation time: 1.6667582988739014\n",
      "Step: 3717, Loss: 0.9316379427909851, Accuracy: 0.96875, Computation time: 1.4055793285369873\n",
      "Step: 3718, Loss: 0.9160851240158081, Accuracy: 1.0, Computation time: 1.7292070388793945\n",
      "Step: 3719, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 1.4403603076934814\n",
      "Step: 3720, Loss: 0.9380025267601013, Accuracy: 0.9791666865348816, Computation time: 2.15305495262146\n",
      "Step: 3721, Loss: 0.9377261400222778, Accuracy: 0.9791666865348816, Computation time: 1.274153232574463\n",
      "Step: 3722, Loss: 0.9161847829818726, Accuracy: 1.0, Computation time: 1.4633703231811523\n",
      "Step: 3723, Loss: 0.9159411787986755, Accuracy: 1.0, Computation time: 1.7770235538482666\n",
      "Step: 3724, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 1.6517248153686523\n",
      "Step: 3725, Loss: 0.9159557223320007, Accuracy: 1.0, Computation time: 1.7019000053405762\n",
      "Step: 3726, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.3251686096191406\n",
      "Step: 3727, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 1.4872848987579346\n",
      "Step: 3728, Loss: 0.9164507389068604, Accuracy: 1.0, Computation time: 1.3306615352630615\n",
      "Step: 3729, Loss: 0.9159482717514038, Accuracy: 1.0, Computation time: 1.8849737644195557\n",
      "Step: 3730, Loss: 0.9160040020942688, Accuracy: 1.0, Computation time: 1.4951763153076172\n",
      "Step: 3731, Loss: 0.9160001277923584, Accuracy: 1.0, Computation time: 1.463508129119873\n",
      "Step: 3732, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.7359323501586914\n",
      "Step: 3733, Loss: 0.9159473180770874, Accuracy: 1.0, Computation time: 1.7453951835632324\n",
      "Step: 3734, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 1.5478737354278564\n",
      "Step: 3735, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.530918836593628\n",
      "Step: 3736, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.5954341888427734\n",
      "Step: 3737, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.7617239952087402\n",
      "Step: 3738, Loss: 0.915870189666748, Accuracy: 1.0, Computation time: 1.5022001266479492\n",
      "Step: 3739, Loss: 0.9376422166824341, Accuracy: 0.9821428656578064, Computation time: 1.4699478149414062\n",
      "Step: 3740, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 1.3339049816131592\n",
      "Step: 3741, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.4519145488739014\n",
      "Step: 3742, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.3761742115020752\n",
      "Step: 3743, Loss: 0.937542736530304, Accuracy: 0.9772727489471436, Computation time: 1.6055760383605957\n",
      "Step: 3744, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.3961730003356934\n",
      "Step: 3745, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.4301376342773438\n",
      "Step: 3746, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.388981819152832\n",
      "Step: 3747, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 1.4883639812469482\n",
      "Step: 3748, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 1.2193615436553955\n",
      "Step: 3749, Loss: 0.9375517964363098, Accuracy: 0.9772727489471436, Computation time: 1.7229764461517334\n",
      "Step: 3750, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.3735640048980713\n",
      "Step: 3751, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.3416247367858887\n",
      "Step: 3752, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.6136066913604736\n",
      "########################\n",
      "Test loss: 1.121187448501587, Test Accuracy_epoch27: 0.7001128196716309\n",
      "########################\n",
      "Step: 3753, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 1.3153059482574463\n",
      "Step: 3754, Loss: 0.9591885209083557, Accuracy: 0.9508928656578064, Computation time: 1.5598437786102295\n",
      "Step: 3755, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.5399141311645508\n",
      "Step: 3756, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.1912689208984375\n",
      "Step: 3757, Loss: 0.9372126460075378, Accuracy: 0.9375, Computation time: 1.028937816619873\n",
      "Step: 3758, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.3919053077697754\n",
      "Step: 3759, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.4169585704803467\n",
      "Step: 3760, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 1.2514863014221191\n",
      "Step: 3761, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 1.1480114459991455\n",
      "Step: 3762, Loss: 0.9160427451133728, Accuracy: 1.0, Computation time: 1.63112211227417\n",
      "Step: 3763, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.3918397426605225\n",
      "Step: 3764, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 1.427276849746704\n",
      "Step: 3765, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 1.4067070484161377\n",
      "Step: 3766, Loss: 0.9385590553283691, Accuracy: 0.9583333730697632, Computation time: 1.5325684547424316\n",
      "Step: 3767, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 1.6884384155273438\n",
      "Step: 3768, Loss: 0.9158441424369812, Accuracy: 1.0, Computation time: 1.5057644844055176\n",
      "Step: 3769, Loss: 0.9162521958351135, Accuracy: 1.0, Computation time: 1.5370917320251465\n",
      "Step: 3770, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 1.317652940750122\n",
      "Step: 3771, Loss: 0.9345996379852295, Accuracy: 0.9583333730697632, Computation time: 1.1743803024291992\n",
      "Step: 3772, Loss: 0.937631368637085, Accuracy: 0.96875, Computation time: 1.6865544319152832\n",
      "Step: 3773, Loss: 0.918919563293457, Accuracy: 1.0, Computation time: 1.8204760551452637\n",
      "Step: 3774, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 1.7905268669128418\n",
      "Step: 3775, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.238187551498413\n",
      "Step: 3776, Loss: 0.915901780128479, Accuracy: 1.0, Computation time: 1.5326571464538574\n",
      "Step: 3777, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 1.4649927616119385\n",
      "Step: 3778, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.5761258602142334\n",
      "Step: 3779, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.5363709926605225\n",
      "Step: 3780, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.642303228378296\n",
      "Step: 3781, Loss: 0.9375839233398438, Accuracy: 0.9750000238418579, Computation time: 1.670177698135376\n",
      "Step: 3782, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 1.6304597854614258\n",
      "Step: 3783, Loss: 0.9158419370651245, Accuracy: 1.0, Computation time: 1.7050492763519287\n",
      "Step: 3784, Loss: 0.9158452749252319, Accuracy: 1.0, Computation time: 1.512019395828247\n",
      "Step: 3785, Loss: 0.9375002980232239, Accuracy: 0.9852941036224365, Computation time: 1.7172026634216309\n",
      "Step: 3786, Loss: 0.9158775806427002, Accuracy: 1.0, Computation time: 1.4653613567352295\n",
      "Step: 3787, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.5827841758728027\n",
      "Step: 3788, Loss: 0.9158594608306885, Accuracy: 1.0, Computation time: 1.382080078125\n",
      "Step: 3789, Loss: 0.9168781042098999, Accuracy: 1.0, Computation time: 1.5629844665527344\n",
      "Step: 3790, Loss: 0.9158534407615662, Accuracy: 1.0, Computation time: 1.8882930278778076\n",
      "Step: 3791, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 1.4078612327575684\n",
      "Step: 3792, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.4098773002624512\n",
      "Step: 3793, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.24125337600708\n",
      "Step: 3794, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 1.410578966140747\n",
      "Step: 3795, Loss: 0.9159130454063416, Accuracy: 1.0, Computation time: 1.6064467430114746\n",
      "Step: 3796, Loss: 0.9178571701049805, Accuracy: 1.0, Computation time: 1.9450061321258545\n",
      "Step: 3797, Loss: 0.9159637093544006, Accuracy: 1.0, Computation time: 1.726182460784912\n",
      "Step: 3798, Loss: 0.9163964986801147, Accuracy: 1.0, Computation time: 1.661738395690918\n",
      "Step: 3799, Loss: 0.9595063924789429, Accuracy: 0.9365079402923584, Computation time: 1.4174859523773193\n",
      "Step: 3800, Loss: 0.9169338941574097, Accuracy: 1.0, Computation time: 2.384901285171509\n",
      "Step: 3801, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 1.8363168239593506\n",
      "Step: 3802, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 1.8819854259490967\n",
      "Step: 3803, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.620781421661377\n",
      "Step: 3804, Loss: 0.9161756038665771, Accuracy: 1.0, Computation time: 1.7611751556396484\n",
      "Step: 3805, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.6926958560943604\n",
      "Step: 3806, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.7336294651031494\n",
      "Step: 3807, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 1.81595778465271\n",
      "Step: 3808, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 2.1322176456451416\n",
      "Step: 3809, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.6212704181671143\n",
      "Step: 3810, Loss: 0.9374088048934937, Accuracy: 0.9722222089767456, Computation time: 1.6289048194885254\n",
      "Step: 3811, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.8804664611816406\n",
      "Step: 3812, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.7764108180999756\n",
      "Step: 3813, Loss: 0.9371270537376404, Accuracy: 0.9772727489471436, Computation time: 1.50004243850708\n",
      "Step: 3814, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.511366605758667\n",
      "Step: 3815, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 1.5706768035888672\n",
      "Step: 3816, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.3241584300994873\n",
      "Step: 3817, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 1.844498872756958\n",
      "Step: 3818, Loss: 0.9160080552101135, Accuracy: 1.0, Computation time: 1.4553532600402832\n",
      "Step: 3819, Loss: 0.917142391204834, Accuracy: 1.0, Computation time: 1.5793492794036865\n",
      "Step: 3820, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 1.5917420387268066\n",
      "Step: 3821, Loss: 0.9158481359481812, Accuracy: 1.0, Computation time: 1.6571295261383057\n",
      "Step: 3822, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.4800381660461426\n",
      "Step: 3823, Loss: 0.9159990549087524, Accuracy: 1.0, Computation time: 1.8237380981445312\n",
      "Step: 3824, Loss: 0.9161536693572998, Accuracy: 1.0, Computation time: 1.5484957695007324\n",
      "Step: 3825, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.4610521793365479\n",
      "Step: 3826, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.3434443473815918\n",
      "Step: 3827, Loss: 0.9590678215026855, Accuracy: 0.9444444179534912, Computation time: 1.7035841941833496\n",
      "Step: 3828, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 2.184309244155884\n",
      "Step: 3829, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.2427220344543457\n",
      "Step: 3830, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.7988722324371338\n",
      "Step: 3831, Loss: 0.9376838803291321, Accuracy: 0.96875, Computation time: 1.629096269607544\n",
      "Step: 3832, Loss: 0.9211042523384094, Accuracy: 1.0, Computation time: 2.229212522506714\n",
      "Step: 3833, Loss: 0.9159098863601685, Accuracy: 1.0, Computation time: 1.308384895324707\n",
      "Step: 3834, Loss: 0.9373548030853271, Accuracy: 0.96875, Computation time: 1.6479172706604004\n",
      "Step: 3835, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 1.542917013168335\n",
      "Step: 3836, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.41666841506958\n",
      "Step: 3837, Loss: 0.9165797829627991, Accuracy: 1.0, Computation time: 1.8916141986846924\n",
      "Step: 3838, Loss: 0.9160630106925964, Accuracy: 1.0, Computation time: 1.6255264282226562\n",
      "Step: 3839, Loss: 0.9378064274787903, Accuracy: 0.96875, Computation time: 1.9300823211669922\n",
      "Step: 3840, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 1.553459644317627\n",
      "Step: 3841, Loss: 0.9378774166107178, Accuracy: 0.96875, Computation time: 1.748000144958496\n",
      "Step: 3842, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 1.4644014835357666\n",
      "Step: 3843, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.4771625995635986\n",
      "Step: 3844, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 1.3083388805389404\n",
      "Step: 3845, Loss: 0.9159581661224365, Accuracy: 1.0, Computation time: 1.6394169330596924\n",
      "Step: 3846, Loss: 0.9596425294876099, Accuracy: 0.9270833730697632, Computation time: 1.6639957427978516\n",
      "Step: 3847, Loss: 0.9374812841415405, Accuracy: 0.9791666865348816, Computation time: 1.4618375301361084\n",
      "Step: 3848, Loss: 0.9160031080245972, Accuracy: 1.0, Computation time: 1.3834362030029297\n",
      "Step: 3849, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 1.1443743705749512\n",
      "Step: 3850, Loss: 0.9201072454452515, Accuracy: 1.0, Computation time: 1.3805336952209473\n",
      "Step: 3851, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.0944759845733643\n",
      "Step: 3852, Loss: 0.9164975881576538, Accuracy: 1.0, Computation time: 1.2674107551574707\n",
      "Step: 3853, Loss: 0.93753582239151, Accuracy: 0.96875, Computation time: 1.125002145767212\n",
      "Step: 3854, Loss: 0.9173693656921387, Accuracy: 1.0, Computation time: 1.2542462348937988\n",
      "Step: 3855, Loss: 0.9164917469024658, Accuracy: 1.0, Computation time: 1.2375454902648926\n",
      "Step: 3856, Loss: 0.9379445910453796, Accuracy: 0.9791666865348816, Computation time: 2.7214505672454834\n",
      "Step: 3857, Loss: 0.9159578084945679, Accuracy: 1.0, Computation time: 1.632108211517334\n",
      "Step: 3858, Loss: 0.9159659743309021, Accuracy: 1.0, Computation time: 1.5025217533111572\n",
      "Step: 3859, Loss: 0.9160239100456238, Accuracy: 1.0, Computation time: 1.275123119354248\n",
      "Step: 3860, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 1.454014778137207\n",
      "Step: 3861, Loss: 0.9348782300949097, Accuracy: 0.9750000238418579, Computation time: 2.7007808685302734\n",
      "Step: 3862, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 1.1939780712127686\n",
      "Step: 3863, Loss: 0.9161756634712219, Accuracy: 1.0, Computation time: 1.4148836135864258\n",
      "Step: 3864, Loss: 0.9160654544830322, Accuracy: 1.0, Computation time: 1.091881275177002\n",
      "Step: 3865, Loss: 0.9319137334823608, Accuracy: 0.9791666865348816, Computation time: 2.83634352684021\n",
      "Step: 3866, Loss: 0.916145384311676, Accuracy: 1.0, Computation time: 1.3727843761444092\n",
      "Step: 3867, Loss: 0.9384368658065796, Accuracy: 0.9833333492279053, Computation time: 2.4416911602020264\n",
      "Step: 3868, Loss: 0.9349095821380615, Accuracy: 0.9772727489471436, Computation time: 2.1313323974609375\n",
      "Step: 3869, Loss: 0.9162917137145996, Accuracy: 1.0, Computation time: 2.1090900897979736\n",
      "Step: 3870, Loss: 0.9163480401039124, Accuracy: 1.0, Computation time: 1.763427734375\n",
      "Step: 3871, Loss: 0.9163485169410706, Accuracy: 1.0, Computation time: 1.879291296005249\n",
      "Step: 3872, Loss: 0.9166163206100464, Accuracy: 1.0, Computation time: 1.9684958457946777\n",
      "Step: 3873, Loss: 0.920407772064209, Accuracy: 1.0, Computation time: 2.8206422328948975\n",
      "Step: 3874, Loss: 0.9273565411567688, Accuracy: 0.9583333730697632, Computation time: 2.5508902072906494\n",
      "Step: 3875, Loss: 0.9170069098472595, Accuracy: 1.0, Computation time: 2.027146100997925\n",
      "Step: 3876, Loss: 0.9169055223464966, Accuracy: 1.0, Computation time: 1.8955132961273193\n",
      "Step: 3877, Loss: 0.9182565212249756, Accuracy: 1.0, Computation time: 1.82017183303833\n",
      "Step: 3878, Loss: 0.9181911945343018, Accuracy: 1.0, Computation time: 1.5021677017211914\n",
      "Step: 3879, Loss: 0.9179949760437012, Accuracy: 1.0, Computation time: 1.8012099266052246\n",
      "Step: 3880, Loss: 0.9163591861724854, Accuracy: 1.0, Computation time: 1.5591230392456055\n",
      "Step: 3881, Loss: 0.9162003397941589, Accuracy: 1.0, Computation time: 1.812767744064331\n",
      "Step: 3882, Loss: 0.9167553782463074, Accuracy: 1.0, Computation time: 1.8338093757629395\n",
      "Step: 3883, Loss: 0.9159575700759888, Accuracy: 1.0, Computation time: 1.9745502471923828\n",
      "Step: 3884, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 2.087008237838745\n",
      "Step: 3885, Loss: 0.9352585077285767, Accuracy: 0.9750000238418579, Computation time: 1.92574143409729\n",
      "Step: 3886, Loss: 0.9167008996009827, Accuracy: 1.0, Computation time: 1.7979986667633057\n",
      "Step: 3887, Loss: 0.9163153171539307, Accuracy: 1.0, Computation time: 1.3042151927947998\n",
      "Step: 3888, Loss: 0.9363164901733398, Accuracy: 0.9791666865348816, Computation time: 1.8987035751342773\n",
      "Step: 3889, Loss: 0.9162387251853943, Accuracy: 1.0, Computation time: 1.3894908428192139\n",
      "Step: 3890, Loss: 0.9337980151176453, Accuracy: 0.9722222089767456, Computation time: 1.6790854930877686\n",
      "Step: 3891, Loss: 0.9305012822151184, Accuracy: 0.9791666865348816, Computation time: 1.5246424674987793\n",
      "########################\n",
      "Test loss: 1.1162368059158325, Test Accuracy_epoch28: 0.7066234350204468\n",
      "########################\n",
      "Step: 3892, Loss: 0.9168359041213989, Accuracy: 1.0, Computation time: 2.010324001312256\n",
      "Step: 3893, Loss: 0.9160711765289307, Accuracy: 1.0, Computation time: 1.104874610900879\n",
      "Step: 3894, Loss: 0.937406063079834, Accuracy: 0.9583333730697632, Computation time: 1.4857699871063232\n",
      "Step: 3895, Loss: 0.9163948893547058, Accuracy: 1.0, Computation time: 1.4508812427520752\n",
      "Step: 3896, Loss: 0.9163429737091064, Accuracy: 1.0, Computation time: 1.4926033020019531\n",
      "Step: 3897, Loss: 0.9167187213897705, Accuracy: 1.0, Computation time: 1.5515241622924805\n",
      "Step: 3898, Loss: 0.916041910648346, Accuracy: 1.0, Computation time: 1.4673340320587158\n",
      "Step: 3899, Loss: 0.9260736703872681, Accuracy: 1.0, Computation time: 1.432286262512207\n",
      "Step: 3900, Loss: 0.917626142501831, Accuracy: 1.0, Computation time: 1.217829942703247\n",
      "Step: 3901, Loss: 0.9162295460700989, Accuracy: 1.0, Computation time: 1.1905429363250732\n",
      "Step: 3902, Loss: 0.916225016117096, Accuracy: 1.0, Computation time: 1.1823089122772217\n",
      "Step: 3903, Loss: 0.9163306951522827, Accuracy: 1.0, Computation time: 1.672684669494629\n",
      "Step: 3904, Loss: 0.9164028167724609, Accuracy: 1.0, Computation time: 1.320951223373413\n",
      "Step: 3905, Loss: 0.9166467785835266, Accuracy: 1.0, Computation time: 1.4265124797821045\n",
      "Step: 3906, Loss: 0.9164025187492371, Accuracy: 1.0, Computation time: 1.1031968593597412\n",
      "Step: 3907, Loss: 0.9161398410797119, Accuracy: 1.0, Computation time: 1.275902271270752\n",
      "Step: 3908, Loss: 0.9159907698631287, Accuracy: 1.0, Computation time: 1.1217536926269531\n",
      "Step: 3909, Loss: 0.9375317096710205, Accuracy: 0.96875, Computation time: 1.3608500957489014\n",
      "Step: 3910, Loss: 0.9166454672813416, Accuracy: 1.0, Computation time: 1.3744604587554932\n",
      "Step: 3911, Loss: 0.9162582755088806, Accuracy: 1.0, Computation time: 1.461839199066162\n",
      "Step: 3912, Loss: 0.9160365462303162, Accuracy: 1.0, Computation time: 1.467792272567749\n",
      "Step: 3913, Loss: 0.9167057275772095, Accuracy: 1.0, Computation time: 1.492419958114624\n",
      "Step: 3914, Loss: 0.9331265091896057, Accuracy: 0.9583333730697632, Computation time: 1.5892066955566406\n",
      "Step: 3915, Loss: 0.9160193204879761, Accuracy: 1.0, Computation time: 1.068363904953003\n",
      "Step: 3916, Loss: 0.9160881638526917, Accuracy: 1.0, Computation time: 1.6210789680480957\n",
      "Step: 3917, Loss: 0.9161331057548523, Accuracy: 1.0, Computation time: 1.5641565322875977\n",
      "Step: 3918, Loss: 0.916147768497467, Accuracy: 1.0, Computation time: 1.4934306144714355\n",
      "Step: 3919, Loss: 0.9305495023727417, Accuracy: 0.9772727489471436, Computation time: 1.226938009262085\n",
      "Step: 3920, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.1371994018554688\n",
      "Step: 3921, Loss: 0.9375974535942078, Accuracy: 0.96875, Computation time: 1.4024701118469238\n",
      "Step: 3922, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.1282846927642822\n",
      "Step: 3923, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 1.2869434356689453\n",
      "Step: 3924, Loss: 0.9184502959251404, Accuracy: 1.0, Computation time: 1.3946261405944824\n",
      "Step: 3925, Loss: 0.9412764310836792, Accuracy: 0.9791666865348816, Computation time: 1.4049460887908936\n",
      "Step: 3926, Loss: 0.9159481525421143, Accuracy: 1.0, Computation time: 1.2720246315002441\n",
      "Step: 3927, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.2914090156555176\n",
      "Step: 3928, Loss: 0.9159973859786987, Accuracy: 1.0, Computation time: 1.3136699199676514\n",
      "Step: 3929, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 1.3667716979980469\n",
      "Step: 3930, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.3467764854431152\n",
      "Step: 3931, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.158493995666504\n",
      "Step: 3932, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.333885908126831\n",
      "Step: 3933, Loss: 0.9570791721343994, Accuracy: 0.918749988079071, Computation time: 1.411827802658081\n",
      "Step: 3934, Loss: 0.918962836265564, Accuracy: nan, Computation time: 1.346574068069458\n",
      "Step: 3935, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.5517218112945557\n",
      "Step: 3936, Loss: 0.9278546571731567, Accuracy: 0.984375, Computation time: 1.554978609085083\n",
      "Step: 3937, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.2488062381744385\n",
      "Step: 3938, Loss: 0.9228025078773499, Accuracy: 1.0, Computation time: 1.7295005321502686\n",
      "Step: 3939, Loss: 0.9161350727081299, Accuracy: 1.0, Computation time: 1.1294114589691162\n",
      "Step: 3940, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.1823005676269531\n",
      "Step: 3941, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 1.1679589748382568\n",
      "Step: 3942, Loss: 0.9159379005432129, Accuracy: 1.0, Computation time: 1.0926430225372314\n",
      "Step: 3943, Loss: 0.9159680604934692, Accuracy: 1.0, Computation time: 1.5767545700073242\n",
      "Step: 3944, Loss: 0.9161251187324524, Accuracy: 1.0, Computation time: 1.0712287425994873\n",
      "Step: 3945, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.2943227291107178\n",
      "Step: 3946, Loss: 0.9374619722366333, Accuracy: 0.984375, Computation time: 1.1835858821868896\n",
      "Step: 3947, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.105853796005249\n",
      "Step: 3948, Loss: 0.9176222085952759, Accuracy: 1.0, Computation time: 1.1625466346740723\n",
      "Step: 3949, Loss: 0.9260924458503723, Accuracy: 0.9807692766189575, Computation time: 1.8176283836364746\n",
      "Step: 3950, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.2974107265472412\n",
      "Step: 3951, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.0381546020507812\n",
      "Step: 3952, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 0.8086714744567871\n",
      "Step: 3953, Loss: 0.9164597392082214, Accuracy: 1.0, Computation time: 1.6883394718170166\n",
      "Step: 3954, Loss: 0.9180041551589966, Accuracy: 1.0, Computation time: 1.0679905414581299\n",
      "Step: 3955, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 1.3059329986572266\n",
      "Step: 3956, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.2817387580871582\n",
      "Step: 3957, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.1045434474945068\n",
      "Step: 3958, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 1.0926766395568848\n",
      "Step: 3959, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.231947898864746\n",
      "Step: 3960, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.1240224838256836\n",
      "Step: 3961, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.2129509449005127\n",
      "Step: 3962, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.3714401721954346\n",
      "Step: 3963, Loss: 0.9387387037277222, Accuracy: 0.9821428656578064, Computation time: 1.492676019668579\n",
      "Step: 3964, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.0476658344268799\n",
      "Step: 3965, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 1.4747953414916992\n",
      "Step: 3966, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.2325899600982666\n",
      "Step: 3967, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.3923537731170654\n",
      "Step: 3968, Loss: 0.9158522486686707, Accuracy: 1.0, Computation time: 1.0191926956176758\n",
      "Step: 3969, Loss: 0.9160672426223755, Accuracy: 1.0, Computation time: 1.327143907546997\n",
      "Step: 3970, Loss: 0.9158405661582947, Accuracy: 1.0, Computation time: 0.9925107955932617\n",
      "Step: 3971, Loss: 0.9376201629638672, Accuracy: 0.9750000238418579, Computation time: 1.1551287174224854\n",
      "Step: 3972, Loss: 0.9375482797622681, Accuracy: 0.96875, Computation time: 1.2831611633300781\n",
      "Step: 3973, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.7348999977111816\n",
      "Step: 3974, Loss: 0.9405239820480347, Accuracy: 0.9807692766189575, Computation time: 1.7870512008666992\n",
      "Step: 3975, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.0091567039489746\n",
      "Step: 3976, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 1.4005439281463623\n",
      "Step: 3977, Loss: 0.9376015067100525, Accuracy: 0.9791666865348816, Computation time: 1.1982426643371582\n",
      "Step: 3978, Loss: 0.9230524301528931, Accuracy: 1.0, Computation time: 1.3320024013519287\n",
      "Step: 3979, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.4683473110198975\n",
      "Step: 3980, Loss: 0.9376071095466614, Accuracy: 0.9791666865348816, Computation time: 1.4397556781768799\n",
      "Step: 3981, Loss: 0.9167877435684204, Accuracy: 1.0, Computation time: 1.7538843154907227\n",
      "Step: 3982, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.066511869430542\n",
      "Step: 3983, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 1.1437907218933105\n",
      "Step: 3984, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.2047622203826904\n",
      "Step: 3985, Loss: 0.9376778602600098, Accuracy: 0.9821428656578064, Computation time: 1.4464995861053467\n",
      "Step: 3986, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.3010928630828857\n",
      "Step: 3987, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.3379600048065186\n",
      "Step: 3988, Loss: 0.9588078260421753, Accuracy: 0.9291666746139526, Computation time: 1.269559621810913\n",
      "Step: 3989, Loss: 0.9167848229408264, Accuracy: 1.0, Computation time: 1.5614700317382812\n",
      "Step: 3990, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.3334858417510986\n",
      "Step: 3991, Loss: 0.9163981080055237, Accuracy: 1.0, Computation time: 1.1048438549041748\n",
      "Step: 3992, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.4718711376190186\n",
      "Step: 3993, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.3931248188018799\n",
      "Step: 3994, Loss: 0.9376537203788757, Accuracy: 0.949999988079071, Computation time: 1.1738979816436768\n",
      "Step: 3995, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.1701757907867432\n",
      "Step: 3996, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.2102022171020508\n",
      "Step: 3997, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 1.1986842155456543\n",
      "Step: 3998, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.1714036464691162\n",
      "Step: 3999, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.2270503044128418\n",
      "Step: 4000, Loss: 0.9159950017929077, Accuracy: 1.0, Computation time: 1.0248687267303467\n",
      "Step: 4001, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.2635316848754883\n",
      "Step: 4002, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.1510567665100098\n",
      "Step: 4003, Loss: 0.9161237478256226, Accuracy: 1.0, Computation time: 1.531325340270996\n",
      "Step: 4004, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.0204226970672607\n",
      "Step: 4005, Loss: 0.9167687296867371, Accuracy: 1.0, Computation time: 1.875558614730835\n",
      "Step: 4006, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 1.0976300239562988\n",
      "Step: 4007, Loss: 0.9164064526557922, Accuracy: 1.0, Computation time: 1.182297706604004\n",
      "Step: 4008, Loss: 0.919022262096405, Accuracy: 1.0, Computation time: 1.0490989685058594\n",
      "Step: 4009, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.203019380569458\n",
      "Step: 4010, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 1.285747766494751\n",
      "Step: 4011, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.4382336139678955\n",
      "Step: 4012, Loss: 0.9260216951370239, Accuracy: 0.96875, Computation time: 1.682192087173462\n",
      "Step: 4013, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.1925559043884277\n",
      "Step: 4014, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.0952603816986084\n",
      "Step: 4015, Loss: 0.9160904884338379, Accuracy: 1.0, Computation time: 1.6512069702148438\n",
      "Step: 4016, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.126570224761963\n",
      "Step: 4017, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 1.2887060642242432\n",
      "Step: 4018, Loss: 0.9159222841262817, Accuracy: 1.0, Computation time: 1.43695068359375\n",
      "Step: 4019, Loss: 0.9166569113731384, Accuracy: 1.0, Computation time: 1.4851138591766357\n",
      "Step: 4020, Loss: 0.9387199878692627, Accuracy: 0.9791666865348816, Computation time: 1.3367059230804443\n",
      "Step: 4021, Loss: 0.9159674048423767, Accuracy: 1.0, Computation time: 1.3628268241882324\n",
      "Step: 4022, Loss: 0.936631977558136, Accuracy: 0.949999988079071, Computation time: 1.6962575912475586\n",
      "Step: 4023, Loss: 0.9161412119865417, Accuracy: 1.0, Computation time: 1.4363305568695068\n",
      "Step: 4024, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.4130091667175293\n",
      "Step: 4025, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.1975114345550537\n",
      "Step: 4026, Loss: 0.9374148845672607, Accuracy: 0.9750000238418579, Computation time: 1.2824535369873047\n",
      "Step: 4027, Loss: 0.916358232498169, Accuracy: 1.0, Computation time: 1.2321233749389648\n",
      "Step: 4028, Loss: 0.9374150037765503, Accuracy: 0.949999988079071, Computation time: 1.341590166091919\n",
      "Step: 4029, Loss: 0.9375104904174805, Accuracy: 0.9807692766189575, Computation time: 1.1289005279541016\n",
      "Step: 4030, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.148329257965088\n",
      "########################\n",
      "Test loss: 1.1163508892059326, Test Accuracy_epoch29: 0.7067674398422241\n",
      "########################\n",
      "Step: 4031, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.2118861675262451\n",
      "Step: 4032, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.4809894561767578\n",
      "Step: 4033, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 1.2781987190246582\n",
      "Step: 4034, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.3698601722717285\n",
      "Step: 4035, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.284252405166626\n",
      "Step: 4036, Loss: 0.9374532103538513, Accuracy: 0.9642857313156128, Computation time: 1.2084739208221436\n",
      "Step: 4037, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.1947052478790283\n",
      "Step: 4038, Loss: 0.9215234518051147, Accuracy: 1.0, Computation time: 1.984384298324585\n",
      "Step: 4039, Loss: 0.9201096296310425, Accuracy: 1.0, Computation time: 1.2198734283447266\n",
      "Step: 4040, Loss: 0.9160842895507812, Accuracy: 1.0, Computation time: 1.161954402923584\n",
      "Step: 4041, Loss: 0.9159548282623291, Accuracy: 1.0, Computation time: 0.9437637329101562\n",
      "Step: 4042, Loss: 0.9159730672836304, Accuracy: 1.0, Computation time: 1.259347677230835\n",
      "Step: 4043, Loss: 0.9169968366622925, Accuracy: 1.0, Computation time: 1.624525785446167\n",
      "Step: 4044, Loss: 0.9375298023223877, Accuracy: 0.9772727489471436, Computation time: 1.46522855758667\n",
      "Step: 4045, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 1.2728326320648193\n",
      "Step: 4046, Loss: 0.91595059633255, Accuracy: 1.0, Computation time: 1.313316822052002\n",
      "Step: 4047, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.2394015789031982\n",
      "Step: 4048, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.1670799255371094\n",
      "Step: 4049, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.3168823719024658\n",
      "Step: 4050, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.7002840042114258\n",
      "Step: 4051, Loss: 0.9159305095672607, Accuracy: 1.0, Computation time: 1.4261353015899658\n",
      "Step: 4052, Loss: 0.9159380197525024, Accuracy: 1.0, Computation time: 1.2705299854278564\n",
      "Step: 4053, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 1.3061282634735107\n",
      "Step: 4054, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.2822701930999756\n",
      "Step: 4055, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.4723834991455078\n",
      "Step: 4056, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 1.6907992362976074\n",
      "Step: 4057, Loss: 0.9160908460617065, Accuracy: 1.0, Computation time: 1.1752562522888184\n",
      "Step: 4058, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.3875725269317627\n",
      "Step: 4059, Loss: 0.9189296364784241, Accuracy: 1.0, Computation time: 1.8026435375213623\n",
      "Step: 4060, Loss: 0.9373643398284912, Accuracy: 0.949999988079071, Computation time: 1.7705726623535156\n",
      "Step: 4061, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.1634657382965088\n",
      "Step: 4062, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.2609822750091553\n",
      "Step: 4063, Loss: 0.937609076499939, Accuracy: 0.9807692766189575, Computation time: 1.311185598373413\n",
      "Step: 4064, Loss: 0.9375450015068054, Accuracy: 0.9807692766189575, Computation time: 1.3243613243103027\n",
      "Step: 4065, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 1.0989651679992676\n",
      "Step: 4066, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.326746940612793\n",
      "Step: 4067, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 1.349386215209961\n",
      "Step: 4068, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.1869847774505615\n",
      "Step: 4069, Loss: 0.91610187292099, Accuracy: 1.0, Computation time: 1.3537192344665527\n",
      "Step: 4070, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.1144793033599854\n",
      "Step: 4071, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.0714850425720215\n",
      "Step: 4072, Loss: 0.9160616397857666, Accuracy: 1.0, Computation time: 1.2848422527313232\n",
      "Step: 4073, Loss: 0.9355722665786743, Accuracy: 0.9821428656578064, Computation time: 1.479682207107544\n",
      "Step: 4074, Loss: 0.9169186353683472, Accuracy: 1.0, Computation time: 1.4037394523620605\n",
      "Step: 4075, Loss: 0.9240536689758301, Accuracy: 1.0, Computation time: 1.929694652557373\n",
      "Step: 4076, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 1.1007194519042969\n",
      "Step: 4077, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.3252880573272705\n",
      "Step: 4078, Loss: 0.9375792145729065, Accuracy: 0.9722222089767456, Computation time: 1.0331590175628662\n",
      "Step: 4079, Loss: 0.9159132242202759, Accuracy: 1.0, Computation time: 1.2009758949279785\n",
      "Step: 4080, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.322801113128662\n",
      "Step: 4081, Loss: 0.9159404635429382, Accuracy: 1.0, Computation time: 1.4210381507873535\n",
      "Step: 4082, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.0550336837768555\n",
      "Step: 4083, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.2365763187408447\n",
      "Step: 4084, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.1264493465423584\n",
      "Step: 4085, Loss: 0.9359303116798401, Accuracy: 0.9722222089767456, Computation time: 1.4791367053985596\n",
      "Step: 4086, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.1633923053741455\n",
      "Step: 4087, Loss: 0.9169096946716309, Accuracy: 1.0, Computation time: 1.3405249118804932\n",
      "Step: 4088, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.094496250152588\n",
      "Step: 4089, Loss: 0.9159390330314636, Accuracy: 1.0, Computation time: 1.3419265747070312\n",
      "Step: 4090, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.1690893173217773\n",
      "Step: 4091, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.1514084339141846\n",
      "Step: 4092, Loss: 0.9373168349266052, Accuracy: 0.96875, Computation time: 1.2318568229675293\n",
      "Step: 4093, Loss: 0.9158616065979004, Accuracy: 1.0, Computation time: 1.219118356704712\n",
      "Step: 4094, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.1056053638458252\n",
      "Step: 4095, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.4732494354248047\n",
      "Step: 4096, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.1921401023864746\n",
      "Step: 4097, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.68056058883667\n",
      "Step: 4098, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 1.114969253540039\n",
      "Step: 4099, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.2763128280639648\n",
      "Step: 4100, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 1.4393081665039062\n",
      "Step: 4101, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.0924830436706543\n",
      "Step: 4102, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.258448600769043\n",
      "Step: 4103, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.1923530101776123\n",
      "Step: 4104, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.5419106483459473\n",
      "Step: 4105, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 1.2644782066345215\n",
      "Step: 4106, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 1.2446026802062988\n",
      "Step: 4107, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 1.1516170501708984\n",
      "Step: 4108, Loss: 0.9158521294593811, Accuracy: 1.0, Computation time: 1.0934853553771973\n",
      "Step: 4109, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 1.1338179111480713\n",
      "Step: 4110, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.1053223609924316\n",
      "Step: 4111, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.4208335876464844\n",
      "Step: 4112, Loss: 0.9177712798118591, Accuracy: 1.0, Computation time: 1.9219937324523926\n",
      "Step: 4113, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 0.9152209758758545\n",
      "Step: 4114, Loss: 0.9159989356994629, Accuracy: 1.0, Computation time: 1.6072092056274414\n",
      "Step: 4115, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.057403326034546\n",
      "Step: 4116, Loss: 0.9375432133674622, Accuracy: 0.9642857313156128, Computation time: 1.3957529067993164\n",
      "Step: 4117, Loss: 0.9375596046447754, Accuracy: 0.9791666865348816, Computation time: 1.5139644145965576\n",
      "Step: 4118, Loss: 0.9593710899353027, Accuracy: 0.9222222566604614, Computation time: 1.1057946681976318\n",
      "Step: 4119, Loss: 0.9372924566268921, Accuracy: 0.9583333730697632, Computation time: 1.3953239917755127\n",
      "Step: 4120, Loss: 0.915839672088623, Accuracy: 1.0, Computation time: 1.0317120552062988\n",
      "Step: 4121, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 1.1651549339294434\n",
      "Step: 4122, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.0807271003723145\n",
      "Step: 4123, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.2042012214660645\n",
      "Step: 4124, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.0478084087371826\n",
      "Step: 4125, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 1.147326946258545\n",
      "Step: 4126, Loss: 0.9178805947303772, Accuracy: 1.0, Computation time: 1.4999639987945557\n",
      "Step: 4127, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.4930224418640137\n",
      "Step: 4128, Loss: 0.9160257577896118, Accuracy: 1.0, Computation time: 1.4720771312713623\n",
      "Step: 4129, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.308668851852417\n",
      "Step: 4130, Loss: 0.9161835312843323, Accuracy: 1.0, Computation time: 1.8984897136688232\n",
      "Step: 4131, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.4553718566894531\n",
      "Step: 4132, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.368955373764038\n",
      "Step: 4133, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.2320687770843506\n",
      "Step: 4134, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 1.584087610244751\n",
      "Step: 4135, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.0944643020629883\n",
      "Step: 4136, Loss: 0.9350446462631226, Accuracy: 0.9791666865348816, Computation time: 1.5539088249206543\n",
      "Step: 4137, Loss: 0.9374213218688965, Accuracy: 0.9583333730697632, Computation time: 1.2729334831237793\n",
      "Step: 4138, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.40516996383667\n",
      "Step: 4139, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.3035228252410889\n",
      "Step: 4140, Loss: 0.9376721382141113, Accuracy: 0.96875, Computation time: 1.3515880107879639\n",
      "Step: 4141, Loss: 0.9159203171730042, Accuracy: 1.0, Computation time: 1.2088754177093506\n",
      "Step: 4142, Loss: 0.9371325969696045, Accuracy: 0.9583333730697632, Computation time: 1.6414248943328857\n",
      "Step: 4143, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.057607889175415\n",
      "Step: 4144, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 1.2849841117858887\n",
      "Step: 4145, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.1556200981140137\n",
      "Step: 4146, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.1255149841308594\n",
      "Step: 4147, Loss: 0.9161748290061951, Accuracy: 1.0, Computation time: 1.4862632751464844\n",
      "Step: 4148, Loss: 0.9172345399856567, Accuracy: 1.0, Computation time: 2.077866792678833\n",
      "Step: 4149, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.1104063987731934\n",
      "Step: 4150, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.025209903717041\n",
      "Step: 4151, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.3129475116729736\n",
      "Step: 4152, Loss: 0.915920615196228, Accuracy: 1.0, Computation time: 1.3085417747497559\n",
      "Step: 4153, Loss: 0.937661349773407, Accuracy: 0.9772727489471436, Computation time: 1.326892614364624\n",
      "Step: 4154, Loss: 0.9343868494033813, Accuracy: 0.984375, Computation time: 1.4801642894744873\n",
      "Step: 4155, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.0954492092132568\n",
      "Step: 4156, Loss: 0.9158620834350586, Accuracy: 1.0, Computation time: 0.9830260276794434\n",
      "Step: 4157, Loss: 0.9267756938934326, Accuracy: 0.9375, Computation time: 1.7928106784820557\n",
      "Step: 4158, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.4962873458862305\n",
      "Step: 4159, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.7691595554351807\n",
      "Step: 4160, Loss: 0.9164045453071594, Accuracy: 1.0, Computation time: 1.5861907005310059\n",
      "Step: 4161, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 1.009214162826538\n",
      "Step: 4162, Loss: 0.9159548878669739, Accuracy: 1.0, Computation time: 1.8921189308166504\n",
      "Step: 4163, Loss: 0.9159461259841919, Accuracy: 1.0, Computation time: 2.0153679847717285\n",
      "Step: 4164, Loss: 0.9159160852432251, Accuracy: 1.0, Computation time: 1.5842759609222412\n",
      "Step: 4165, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 1.2486085891723633\n",
      "Step: 4166, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.1711394786834717\n",
      "Step: 4167, Loss: 0.9201023578643799, Accuracy: 1.0, Computation time: 2.3161730766296387\n",
      "Step: 4168, Loss: 0.9159016013145447, Accuracy: 1.0, Computation time: 1.114260196685791\n",
      "Step: 4169, Loss: 0.9376577734947205, Accuracy: 0.9583333730697632, Computation time: 1.062558650970459\n",
      "########################\n",
      "Test loss: 1.1161198616027832, Test Accuracy_epoch30: 0.706860363483429\n",
      "########################\n",
      "Step: 4170, Loss: 0.9160118699073792, Accuracy: 1.0, Computation time: 1.3974406719207764\n",
      "Step: 4171, Loss: 0.9381504654884338, Accuracy: 0.9772727489471436, Computation time: 2.349918842315674\n",
      "Step: 4172, Loss: 0.9161540865898132, Accuracy: 1.0, Computation time: 1.2216989994049072\n",
      "Step: 4173, Loss: 0.9165124297142029, Accuracy: 1.0, Computation time: 1.466040849685669\n",
      "Step: 4174, Loss: 0.938083827495575, Accuracy: 0.949999988079071, Computation time: 1.7434661388397217\n",
      "Step: 4175, Loss: 0.9162405729293823, Accuracy: 1.0, Computation time: 1.0911874771118164\n",
      "Step: 4176, Loss: 0.9161615967750549, Accuracy: 1.0, Computation time: 0.9306163787841797\n",
      "Step: 4177, Loss: 0.9161414504051208, Accuracy: 1.0, Computation time: 1.753347396850586\n",
      "Step: 4178, Loss: 0.9161313772201538, Accuracy: 1.0, Computation time: 1.3726544380187988\n",
      "Step: 4179, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 1.5576121807098389\n",
      "Step: 4180, Loss: 0.9160380363464355, Accuracy: 1.0, Computation time: 1.391767978668213\n",
      "Step: 4181, Loss: 0.9159324169158936, Accuracy: 1.0, Computation time: 1.1197655200958252\n",
      "Step: 4182, Loss: 0.9159926176071167, Accuracy: 1.0, Computation time: 1.2572526931762695\n",
      "Step: 4183, Loss: 0.9162495136260986, Accuracy: 1.0, Computation time: 1.2169451713562012\n",
      "Step: 4184, Loss: 0.9379392862319946, Accuracy: 0.9821428656578064, Computation time: 2.5887696743011475\n",
      "Step: 4185, Loss: 0.9698783755302429, Accuracy: 0.8973214626312256, Computation time: 2.220857858657837\n",
      "Step: 4186, Loss: 0.9161384701728821, Accuracy: 1.0, Computation time: 1.2444984912872314\n",
      "Step: 4187, Loss: 0.9377044439315796, Accuracy: 0.9722222089767456, Computation time: 1.3209991455078125\n",
      "Step: 4188, Loss: 0.9159879684448242, Accuracy: 1.0, Computation time: 1.1017067432403564\n",
      "Step: 4189, Loss: 0.9160366058349609, Accuracy: 1.0, Computation time: 1.808243989944458\n",
      "Step: 4190, Loss: 0.9161151647567749, Accuracy: 1.0, Computation time: 1.834183692932129\n",
      "Step: 4191, Loss: 0.9364842772483826, Accuracy: 0.9772727489471436, Computation time: 1.901548147201538\n",
      "Step: 4192, Loss: 0.9161149263381958, Accuracy: 1.0, Computation time: 1.0840694904327393\n",
      "Step: 4193, Loss: 0.9158741235733032, Accuracy: nan, Computation time: 1.0688066482543945\n",
      "Step: 4194, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.3316659927368164\n",
      "Step: 4195, Loss: 0.9159859418869019, Accuracy: 1.0, Computation time: 1.4423339366912842\n",
      "Step: 4196, Loss: 0.916168212890625, Accuracy: 1.0, Computation time: 1.407400369644165\n",
      "Step: 4197, Loss: 0.9159300327301025, Accuracy: 1.0, Computation time: 1.046229362487793\n",
      "Step: 4198, Loss: 0.9184200763702393, Accuracy: 1.0, Computation time: 1.2493507862091064\n",
      "Step: 4199, Loss: 0.9159780740737915, Accuracy: 1.0, Computation time: 1.5086009502410889\n",
      "Step: 4200, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 1.3415248394012451\n",
      "Step: 4201, Loss: 0.9593295454978943, Accuracy: 0.9321428537368774, Computation time: 1.5211002826690674\n",
      "Step: 4202, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 1.3689985275268555\n",
      "Step: 4203, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.3104462623596191\n",
      "Step: 4204, Loss: 0.9159491658210754, Accuracy: 1.0, Computation time: 1.4675767421722412\n",
      "Step: 4205, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.6679208278656006\n",
      "Step: 4206, Loss: 0.9159528613090515, Accuracy: 1.0, Computation time: 1.4202489852905273\n",
      "Step: 4207, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 1.5747699737548828\n",
      "Step: 4208, Loss: 0.9161547422409058, Accuracy: 1.0, Computation time: 1.5301802158355713\n",
      "Step: 4209, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.828796625137329\n",
      "Step: 4210, Loss: 0.9163606762886047, Accuracy: 1.0, Computation time: 1.8789961338043213\n",
      "Step: 4211, Loss: 0.9392905831336975, Accuracy: 0.9833333492279053, Computation time: 1.9043498039245605\n",
      "Step: 4212, Loss: 0.9171244502067566, Accuracy: 1.0, Computation time: 1.3131306171417236\n",
      "Step: 4213, Loss: 0.9163967370986938, Accuracy: 1.0, Computation time: 1.508387804031372\n",
      "Step: 4214, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 1.2379460334777832\n",
      "Step: 4215, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 1.48298978805542\n",
      "Step: 4216, Loss: 0.9302065968513489, Accuracy: 0.9722222089767456, Computation time: 1.6131227016448975\n",
      "Step: 4217, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.5170574188232422\n",
      "Step: 4218, Loss: 0.9159106612205505, Accuracy: 1.0, Computation time: 1.3389596939086914\n",
      "Step: 4219, Loss: 0.9159671664237976, Accuracy: 1.0, Computation time: 1.4383594989776611\n",
      "Step: 4220, Loss: 0.9378542900085449, Accuracy: 0.9722222089767456, Computation time: 1.7570385932922363\n",
      "Step: 4221, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 1.0781123638153076\n",
      "Step: 4222, Loss: 0.9159809350967407, Accuracy: 1.0, Computation time: 1.3026041984558105\n",
      "Step: 4223, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 1.0783452987670898\n",
      "Step: 4224, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 1.304532527923584\n",
      "Step: 4225, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 1.2862591743469238\n",
      "Step: 4226, Loss: 0.9453655481338501, Accuracy: 0.9642857313156128, Computation time: 1.3416428565979004\n",
      "Step: 4227, Loss: 0.953799843788147, Accuracy: 0.922619104385376, Computation time: 1.530306339263916\n",
      "Step: 4228, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.5210657119750977\n",
      "Step: 4229, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.2622361183166504\n",
      "Step: 4230, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.2311367988586426\n",
      "Step: 4231, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 1.1169054508209229\n",
      "Step: 4232, Loss: 0.9158517122268677, Accuracy: 1.0, Computation time: 1.1285650730133057\n",
      "Step: 4233, Loss: 0.916094183921814, Accuracy: 1.0, Computation time: 1.0720009803771973\n",
      "Step: 4234, Loss: 0.9161639213562012, Accuracy: 1.0, Computation time: 1.7568378448486328\n",
      "Step: 4235, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.530709981918335\n",
      "Step: 4236, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.0461616516113281\n",
      "Step: 4237, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.3920793533325195\n",
      "Step: 4238, Loss: 0.9235308766365051, Accuracy: 1.0, Computation time: 1.1567966938018799\n",
      "Step: 4239, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.1206386089324951\n",
      "Step: 4240, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.2772676944732666\n",
      "Step: 4241, Loss: 0.9177321195602417, Accuracy: 1.0, Computation time: 1.847576379776001\n",
      "Step: 4242, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.3204987049102783\n",
      "Step: 4243, Loss: 0.9279047250747681, Accuracy: 0.9750000238418579, Computation time: 1.6643896102905273\n",
      "Step: 4244, Loss: 0.9159421324729919, Accuracy: 1.0, Computation time: 1.3813276290893555\n",
      "Step: 4245, Loss: 0.9160971641540527, Accuracy: 1.0, Computation time: 1.2860400676727295\n",
      "Step: 4246, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 1.5306086540222168\n",
      "Step: 4247, Loss: 0.9378055930137634, Accuracy: 0.9583333730697632, Computation time: 1.4292871952056885\n",
      "Step: 4248, Loss: 0.9172863364219666, Accuracy: 1.0, Computation time: 1.9547410011291504\n",
      "Step: 4249, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 1.6677238941192627\n",
      "Step: 4250, Loss: 0.9161235094070435, Accuracy: 1.0, Computation time: 1.9217958450317383\n",
      "Step: 4251, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 1.4999523162841797\n",
      "Step: 4252, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.2965269088745117\n",
      "Step: 4253, Loss: 0.9160781502723694, Accuracy: 1.0, Computation time: 1.6223511695861816\n",
      "Step: 4254, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 1.456406831741333\n",
      "Step: 4255, Loss: 0.9377857446670532, Accuracy: 0.9807692766189575, Computation time: 1.3400840759277344\n",
      "Step: 4256, Loss: 0.9377186298370361, Accuracy: 0.9750000238418579, Computation time: 1.3420605659484863\n",
      "Step: 4257, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.620100975036621\n",
      "Step: 4258, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.3295738697052002\n",
      "Step: 4259, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 1.5788590908050537\n",
      "Step: 4260, Loss: 0.9162051677703857, Accuracy: 1.0, Computation time: 1.4529500007629395\n",
      "Step: 4261, Loss: 0.9161349534988403, Accuracy: 1.0, Computation time: 1.3984384536743164\n",
      "Step: 4262, Loss: 0.9378719329833984, Accuracy: 0.9807692766189575, Computation time: 1.6971435546875\n",
      "Step: 4263, Loss: 0.9430214762687683, Accuracy: 0.9772727489471436, Computation time: 2.0303778648376465\n",
      "Step: 4264, Loss: 0.9172196984291077, Accuracy: 1.0, Computation time: 1.687471628189087\n",
      "Step: 4265, Loss: 0.9160312414169312, Accuracy: 1.0, Computation time: 1.644251823425293\n",
      "Step: 4266, Loss: 0.9160128831863403, Accuracy: 1.0, Computation time: 1.71708083152771\n",
      "Step: 4267, Loss: 0.937403678894043, Accuracy: 0.96875, Computation time: 1.746380090713501\n",
      "Step: 4268, Loss: 0.9161373376846313, Accuracy: 1.0, Computation time: 1.8567187786102295\n",
      "Step: 4269, Loss: 0.9175617694854736, Accuracy: 1.0, Computation time: 1.5861878395080566\n",
      "Step: 4270, Loss: 0.9161452651023865, Accuracy: 1.0, Computation time: 1.4359898567199707\n",
      "Step: 4271, Loss: 0.937694787979126, Accuracy: 0.9642857313156128, Computation time: 1.573009967803955\n",
      "Step: 4272, Loss: 0.91596519947052, Accuracy: 1.0, Computation time: 2.13763689994812\n",
      "Step: 4273, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 1.4904978275299072\n",
      "Step: 4274, Loss: 0.9159017205238342, Accuracy: nan, Computation time: 1.4449594020843506\n",
      "Step: 4275, Loss: 0.9160520434379578, Accuracy: 1.0, Computation time: 1.7645306587219238\n",
      "Step: 4276, Loss: 0.9160860776901245, Accuracy: 1.0, Computation time: 1.6739296913146973\n",
      "Step: 4277, Loss: 0.9159562587738037, Accuracy: 1.0, Computation time: 1.3355114459991455\n",
      "Step: 4278, Loss: 0.9160209894180298, Accuracy: 1.0, Computation time: 1.5473058223724365\n",
      "Step: 4279, Loss: 0.9159440398216248, Accuracy: 1.0, Computation time: 1.3100476264953613\n",
      "Step: 4280, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 1.5944128036499023\n",
      "Step: 4281, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.203315258026123\n",
      "Step: 4282, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.210400104522705\n",
      "Step: 4283, Loss: 0.9198952317237854, Accuracy: 1.0, Computation time: 1.6361403465270996\n",
      "Step: 4284, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 1.5646846294403076\n",
      "Step: 4285, Loss: 0.9161335229873657, Accuracy: 1.0, Computation time: 1.418487787246704\n",
      "Step: 4286, Loss: 0.9161057472229004, Accuracy: 1.0, Computation time: 1.602886438369751\n",
      "Step: 4287, Loss: 0.9160477519035339, Accuracy: 1.0, Computation time: 1.5627796649932861\n",
      "Step: 4288, Loss: 0.9159273505210876, Accuracy: 1.0, Computation time: 1.4962055683135986\n",
      "Step: 4289, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.1322956085205078\n",
      "Step: 4290, Loss: 0.9378501772880554, Accuracy: 0.9791666865348816, Computation time: 1.5597469806671143\n",
      "Step: 4291, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 1.5662226676940918\n",
      "Step: 4292, Loss: 0.9159634709358215, Accuracy: 1.0, Computation time: 1.708653211593628\n",
      "Step: 4293, Loss: 0.9159175753593445, Accuracy: 1.0, Computation time: 1.5477395057678223\n",
      "Step: 4294, Loss: 0.916034460067749, Accuracy: 1.0, Computation time: 1.6180384159088135\n",
      "Step: 4295, Loss: 0.9159471988677979, Accuracy: 1.0, Computation time: 1.6485264301300049\n",
      "Step: 4296, Loss: 0.9376870393753052, Accuracy: 0.9807692766189575, Computation time: 1.8200082778930664\n",
      "Step: 4297, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 1.7015163898468018\n",
      "Step: 4298, Loss: 0.9160269498825073, Accuracy: 1.0, Computation time: 1.5236737728118896\n",
      "Step: 4299, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.651484489440918\n",
      "Step: 4300, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 2.0345611572265625\n",
      "Step: 4301, Loss: 0.9163456559181213, Accuracy: 1.0, Computation time: 1.8995954990386963\n",
      "Step: 4302, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 1.7211766242980957\n",
      "Step: 4303, Loss: 0.9324400424957275, Accuracy: 0.9807692766189575, Computation time: 1.9592077732086182\n",
      "Step: 4304, Loss: 0.9158942699432373, Accuracy: 1.0, Computation time: 1.5673303604125977\n",
      "Step: 4305, Loss: 0.9356319308280945, Accuracy: 0.9750000238418579, Computation time: 2.0258495807647705\n",
      "Step: 4306, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 1.7766993045806885\n",
      "Step: 4307, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 1.417630910873413\n",
      "Step: 4308, Loss: 0.9159016013145447, Accuracy: 1.0, Computation time: 1.612391471862793\n",
      "########################\n",
      "Test loss: 1.1169639825820923, Test Accuracy_epoch31: 0.7069405317306519\n",
      "########################\n",
      "Step: 4309, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 1.5221598148345947\n",
      "Step: 4310, Loss: 0.959095299243927, Accuracy: 0.9142857193946838, Computation time: 1.7202904224395752\n",
      "Step: 4311, Loss: 0.9161545634269714, Accuracy: 1.0, Computation time: 1.5199122428894043\n",
      "Step: 4312, Loss: 0.9369391798973083, Accuracy: 0.96875, Computation time: 1.4356284141540527\n",
      "Step: 4313, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 1.3091390132904053\n",
      "Step: 4314, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.703948736190796\n",
      "Step: 4315, Loss: 0.9373118877410889, Accuracy: 0.9583333730697632, Computation time: 1.101931095123291\n",
      "Step: 4316, Loss: 0.9373517036437988, Accuracy: 0.9791666865348816, Computation time: 1.7543237209320068\n",
      "Step: 4317, Loss: 0.9158729314804077, Accuracy: 1.0, Computation time: 1.5238628387451172\n",
      "Step: 4318, Loss: 0.9343158006668091, Accuracy: 0.9772727489471436, Computation time: 2.3298885822296143\n",
      "Step: 4319, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 1.7566006183624268\n",
      "Step: 4320, Loss: 0.9166643619537354, Accuracy: 1.0, Computation time: 1.8155722618103027\n",
      "Step: 4321, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.4304075241088867\n",
      "Step: 4322, Loss: 0.9159350395202637, Accuracy: 1.0, Computation time: 1.9256651401519775\n",
      "Step: 4323, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.5806827545166016\n",
      "Step: 4324, Loss: 0.9376193284988403, Accuracy: 0.96875, Computation time: 1.3451895713806152\n",
      "Step: 4325, Loss: 0.9160735607147217, Accuracy: 1.0, Computation time: 1.982635736465454\n",
      "Step: 4326, Loss: 0.917447030544281, Accuracy: 1.0, Computation time: 2.0882363319396973\n",
      "Step: 4327, Loss: 0.9335719347000122, Accuracy: 0.9642857313156128, Computation time: 1.7828600406646729\n",
      "Step: 4328, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 1.6050629615783691\n",
      "Step: 4329, Loss: 0.9164655804634094, Accuracy: 1.0, Computation time: 1.6858837604522705\n",
      "Step: 4330, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 1.3957643508911133\n",
      "Step: 4331, Loss: 0.9192854166030884, Accuracy: 1.0, Computation time: 2.1914916038513184\n",
      "Step: 4332, Loss: 0.9160985946655273, Accuracy: 1.0, Computation time: 1.6980195045471191\n",
      "Step: 4333, Loss: 0.9160864949226379, Accuracy: 1.0, Computation time: 1.4015636444091797\n",
      "Step: 4334, Loss: 0.9162352085113525, Accuracy: 1.0, Computation time: 1.5782217979431152\n",
      "Step: 4335, Loss: 0.9159069657325745, Accuracy: 1.0, Computation time: 1.2414414882659912\n",
      "Step: 4336, Loss: 0.915963351726532, Accuracy: 1.0, Computation time: 1.3475134372711182\n",
      "Step: 4337, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.6121394634246826\n",
      "Step: 4338, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.301943063735962\n",
      "Step: 4339, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.518742561340332\n",
      "Step: 4340, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 1.4274563789367676\n",
      "Step: 4341, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.914842128753662\n",
      "Step: 4342, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.5144684314727783\n",
      "Step: 4343, Loss: 0.9159396886825562, Accuracy: 1.0, Computation time: 1.3724896907806396\n",
      "Step: 4344, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.4357340335845947\n",
      "Step: 4345, Loss: 0.9159972667694092, Accuracy: 1.0, Computation time: 1.3888306617736816\n",
      "Step: 4346, Loss: 0.9376397132873535, Accuracy: 0.9750000238418579, Computation time: 1.3254766464233398\n",
      "Step: 4347, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.4646310806274414\n",
      "Step: 4348, Loss: 0.9199357032775879, Accuracy: 1.0, Computation time: 1.4942598342895508\n",
      "Step: 4349, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.339367151260376\n",
      "Step: 4350, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.3370029926300049\n",
      "Step: 4351, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.6136794090270996\n",
      "Step: 4352, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.2746467590332031\n",
      "Step: 4353, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 1.1310808658599854\n",
      "Step: 4354, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.8799324035644531\n",
      "Step: 4355, Loss: 0.9161570072174072, Accuracy: 1.0, Computation time: 1.3633668422698975\n",
      "Step: 4356, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.2224533557891846\n",
      "Step: 4357, Loss: 0.9168468117713928, Accuracy: 1.0, Computation time: 1.3619837760925293\n",
      "Step: 4358, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.459212064743042\n",
      "Step: 4359, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.3784129619598389\n",
      "Step: 4360, Loss: 0.9375841617584229, Accuracy: nan, Computation time: 1.4993574619293213\n",
      "Step: 4361, Loss: 0.9161292910575867, Accuracy: 1.0, Computation time: 1.425642728805542\n",
      "Step: 4362, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.2422869205474854\n",
      "Step: 4363, Loss: 0.9315913319587708, Accuracy: 0.9642857313156128, Computation time: 2.13889479637146\n",
      "Step: 4364, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.227430820465088\n",
      "Step: 4365, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 1.420760154724121\n",
      "Step: 4366, Loss: 0.9374338388442993, Accuracy: 0.9722222089767456, Computation time: 1.3595943450927734\n",
      "Step: 4367, Loss: 0.9159273505210876, Accuracy: 1.0, Computation time: 1.3897860050201416\n",
      "Step: 4368, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 1.3399012088775635\n",
      "Step: 4369, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.2899351119995117\n",
      "Step: 4370, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 1.23518967628479\n",
      "Step: 4371, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.5095899105072021\n",
      "Step: 4372, Loss: 0.9158610105514526, Accuracy: 1.0, Computation time: 1.337130069732666\n",
      "Step: 4373, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 1.2186975479125977\n",
      "Step: 4374, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.1849701404571533\n",
      "Step: 4375, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.228754997253418\n",
      "Step: 4376, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.911926507949829\n",
      "Step: 4377, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.4328908920288086\n",
      "Step: 4378, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.276170015335083\n",
      "Step: 4379, Loss: 0.9365317225456238, Accuracy: 0.9807692766189575, Computation time: 1.824937343597412\n",
      "Step: 4380, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.3535840511322021\n",
      "Step: 4381, Loss: 0.9376721978187561, Accuracy: 0.9791666865348816, Computation time: 1.4172022342681885\n",
      "Step: 4382, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.429257869720459\n",
      "Step: 4383, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 1.5481929779052734\n",
      "Step: 4384, Loss: 0.9159018993377686, Accuracy: 1.0, Computation time: 1.3790061473846436\n",
      "Step: 4385, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 1.3817236423492432\n",
      "Step: 4386, Loss: 0.9361040592193604, Accuracy: 0.96875, Computation time: 1.2806057929992676\n",
      "Step: 4387, Loss: 0.9376124143600464, Accuracy: 0.9750000238418579, Computation time: 1.4921867847442627\n",
      "Step: 4388, Loss: 0.9356533288955688, Accuracy: 0.9821428656578064, Computation time: 1.6337859630584717\n",
      "Step: 4389, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.2495877742767334\n",
      "Step: 4390, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.2044463157653809\n",
      "Step: 4391, Loss: 0.9161094427108765, Accuracy: 1.0, Computation time: 1.6387965679168701\n",
      "Step: 4392, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 1.056335687637329\n",
      "Step: 4393, Loss: 0.9374309182167053, Accuracy: 0.9642857313156128, Computation time: 1.213282823562622\n",
      "Step: 4394, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 1.4606800079345703\n",
      "Step: 4395, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.1563072204589844\n",
      "Step: 4396, Loss: 0.9377375841140747, Accuracy: 0.9642857313156128, Computation time: 1.416682481765747\n",
      "Step: 4397, Loss: 0.9224827289581299, Accuracy: 1.0, Computation time: 1.6179707050323486\n",
      "Step: 4398, Loss: 0.9159315228462219, Accuracy: 1.0, Computation time: 1.9237449169158936\n",
      "Step: 4399, Loss: 0.9158935546875, Accuracy: 1.0, Computation time: 1.4406859874725342\n",
      "Step: 4400, Loss: 0.9289087653160095, Accuracy: 0.9772727489471436, Computation time: 2.1919174194335938\n",
      "Step: 4401, Loss: 0.9161497950553894, Accuracy: 1.0, Computation time: 1.6649971008300781\n",
      "Step: 4402, Loss: 0.9376291632652283, Accuracy: 0.9772727489471436, Computation time: 1.9585061073303223\n",
      "Step: 4403, Loss: 0.9160745739936829, Accuracy: 1.0, Computation time: 1.5947203636169434\n",
      "Step: 4404, Loss: 0.9161297082901001, Accuracy: 1.0, Computation time: 1.6091277599334717\n",
      "Step: 4405, Loss: 0.9367716908454895, Accuracy: 0.9772727489471436, Computation time: 1.7413835525512695\n",
      "Step: 4406, Loss: 0.915989100933075, Accuracy: 1.0, Computation time: 1.278435468673706\n",
      "Step: 4407, Loss: 0.9163293838500977, Accuracy: 1.0, Computation time: 2.142716646194458\n",
      "Step: 4408, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.8105380535125732\n",
      "Step: 4409, Loss: 0.9374726414680481, Accuracy: 0.9833333492279053, Computation time: 2.0019490718841553\n",
      "Step: 4410, Loss: 0.9159218668937683, Accuracy: 1.0, Computation time: 1.5511305332183838\n",
      "Step: 4411, Loss: 0.9159938097000122, Accuracy: 1.0, Computation time: 1.6499061584472656\n",
      "Step: 4412, Loss: 0.939373791217804, Accuracy: 0.9772727489471436, Computation time: 2.0599193572998047\n",
      "Step: 4413, Loss: 0.9261747598648071, Accuracy: 0.9722222089767456, Computation time: 2.0974903106689453\n",
      "Step: 4414, Loss: 0.9382458925247192, Accuracy: 0.9722222089767456, Computation time: 1.7723212242126465\n",
      "Step: 4415, Loss: 0.9169033169746399, Accuracy: 1.0, Computation time: 1.7616691589355469\n",
      "Step: 4416, Loss: 0.9162910580635071, Accuracy: 1.0, Computation time: 1.6275029182434082\n",
      "Step: 4417, Loss: 0.9172897338867188, Accuracy: 1.0, Computation time: 2.410879373550415\n",
      "Step: 4418, Loss: 0.9162964820861816, Accuracy: 1.0, Computation time: 1.2603325843811035\n",
      "Step: 4419, Loss: 0.9160364270210266, Accuracy: 1.0, Computation time: 1.411146879196167\n",
      "Step: 4420, Loss: 0.9159553647041321, Accuracy: 1.0, Computation time: 1.2362353801727295\n",
      "Step: 4421, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.2714595794677734\n",
      "Step: 4422, Loss: 0.91596919298172, Accuracy: 1.0, Computation time: 1.039794683456421\n",
      "Step: 4423, Loss: 0.91597980260849, Accuracy: 1.0, Computation time: 1.1538946628570557\n",
      "Step: 4424, Loss: 0.9405122399330139, Accuracy: 0.9750000238418579, Computation time: 1.6032366752624512\n",
      "Step: 4425, Loss: 0.9162685871124268, Accuracy: 1.0, Computation time: 1.2121515274047852\n",
      "Step: 4426, Loss: 0.9161152839660645, Accuracy: 1.0, Computation time: 1.361464262008667\n",
      "Step: 4427, Loss: 0.9160167574882507, Accuracy: 1.0, Computation time: 1.3853743076324463\n",
      "Step: 4428, Loss: 0.9160552024841309, Accuracy: 1.0, Computation time: 1.332362174987793\n",
      "Step: 4429, Loss: 0.9159558415412903, Accuracy: 1.0, Computation time: 1.3364367485046387\n",
      "Step: 4430, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 1.3117282390594482\n",
      "Step: 4431, Loss: 0.9250064492225647, Accuracy: 1.0, Computation time: 1.4523804187774658\n",
      "Step: 4432, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.3823556900024414\n",
      "Step: 4433, Loss: 0.9158894419670105, Accuracy: 1.0, Computation time: 0.982698917388916\n",
      "Step: 4434, Loss: 0.9159427881240845, Accuracy: 1.0, Computation time: 1.0380680561065674\n",
      "Step: 4435, Loss: 0.9159292578697205, Accuracy: 1.0, Computation time: 1.089099407196045\n",
      "Step: 4436, Loss: 0.9159058332443237, Accuracy: 1.0, Computation time: 0.9298205375671387\n",
      "Step: 4437, Loss: 0.9187625050544739, Accuracy: 1.0, Computation time: 1.0730047225952148\n",
      "Step: 4438, Loss: 0.9165345430374146, Accuracy: 1.0, Computation time: 1.3808207511901855\n",
      "Step: 4439, Loss: 0.9342632293701172, Accuracy: 0.9750000238418579, Computation time: 1.9244515895843506\n",
      "Step: 4440, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 1.6024196147918701\n",
      "Step: 4441, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 1.1800529956817627\n",
      "Step: 4442, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.3292741775512695\n",
      "Step: 4443, Loss: 0.9377303719520569, Accuracy: 0.9772727489471436, Computation time: 1.6564579010009766\n",
      "Step: 4444, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.0445358753204346\n",
      "Step: 4445, Loss: 0.9386915564537048, Accuracy: 0.9821428656578064, Computation time: 1.5535883903503418\n",
      "Step: 4446, Loss: 0.9189563393592834, Accuracy: 1.0, Computation time: 1.1542189121246338\n",
      "########################\n",
      "Test loss: 1.1216667890548706, Test Accuracy_epoch32: 0.697195827960968\n",
      "########################\n",
      "Step: 4447, Loss: 0.9375613331794739, Accuracy: 0.9750000238418579, Computation time: 1.1617772579193115\n",
      "Step: 4448, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.0229105949401855\n",
      "Step: 4449, Loss: 0.915935754776001, Accuracy: 1.0, Computation time: 0.9747409820556641\n",
      "Step: 4450, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.0948460102081299\n",
      "Step: 4451, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.0944244861602783\n",
      "Step: 4452, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.255110740661621\n",
      "Step: 4453, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.0510547161102295\n",
      "Step: 4454, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.098834753036499\n",
      "Step: 4455, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.1951725482940674\n",
      "Step: 4456, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 0.978081464767456\n",
      "Step: 4457, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 0.9484918117523193\n",
      "Step: 4458, Loss: 0.9361476898193359, Accuracy: 0.96875, Computation time: 2.0165042877197266\n",
      "Step: 4459, Loss: 0.9159528017044067, Accuracy: 1.0, Computation time: 1.0870890617370605\n",
      "Step: 4460, Loss: 0.9160546064376831, Accuracy: 1.0, Computation time: 1.2515807151794434\n",
      "Step: 4461, Loss: 0.9159777164459229, Accuracy: 1.0, Computation time: 1.103759527206421\n",
      "Step: 4462, Loss: 0.9592959880828857, Accuracy: 0.9196428656578064, Computation time: 0.9398071765899658\n",
      "Step: 4463, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.9545931816101074\n",
      "Step: 4464, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 0.9924514293670654\n",
      "Step: 4465, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 1.0432405471801758\n",
      "Step: 4466, Loss: 0.916117250919342, Accuracy: 1.0, Computation time: 1.5382373332977295\n",
      "Step: 4467, Loss: 0.9158577919006348, Accuracy: 1.0, Computation time: 0.9594745635986328\n",
      "Step: 4468, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 1.2060213088989258\n",
      "Step: 4469, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 1.4690983295440674\n",
      "Step: 4470, Loss: 0.9161341190338135, Accuracy: 1.0, Computation time: 1.1960210800170898\n",
      "Step: 4471, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 0.9011850357055664\n",
      "Step: 4472, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 1.1532478332519531\n",
      "Step: 4473, Loss: 0.9162348508834839, Accuracy: 1.0, Computation time: 1.0901920795440674\n",
      "Step: 4474, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 0.891970157623291\n",
      "Step: 4475, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 1.1362965106964111\n",
      "Step: 4476, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.1384332180023193\n",
      "Step: 4477, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 0.8359780311584473\n",
      "Step: 4478, Loss: 0.9375841617584229, Accuracy: 0.96875, Computation time: 1.0014736652374268\n",
      "Step: 4479, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 0.8819150924682617\n",
      "Step: 4480, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 0.9503250122070312\n",
      "Step: 4481, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.0975232124328613\n",
      "Step: 4482, Loss: 0.9374790787696838, Accuracy: 0.9750000238418579, Computation time: 1.3077867031097412\n",
      "Step: 4483, Loss: 0.9159093499183655, Accuracy: 1.0, Computation time: 1.4293577671051025\n",
      "Step: 4484, Loss: 0.9158384203910828, Accuracy: 1.0, Computation time: 0.9186716079711914\n",
      "Step: 4485, Loss: 0.9158363342285156, Accuracy: 1.0, Computation time: 0.8527078628540039\n",
      "Step: 4486, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.0401654243469238\n",
      "Step: 4487, Loss: 0.9376487731933594, Accuracy: 0.9772727489471436, Computation time: 1.5025427341461182\n",
      "Step: 4488, Loss: 0.9169692993164062, Accuracy: 1.0, Computation time: 1.3102939128875732\n",
      "Step: 4489, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.0470168590545654\n",
      "Step: 4490, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 1.040980339050293\n",
      "Step: 4491, Loss: 0.9161015152931213, Accuracy: 1.0, Computation time: 1.2296619415283203\n",
      "Step: 4492, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 0.9408159255981445\n",
      "Step: 4493, Loss: 0.9376586675643921, Accuracy: 0.9772727489471436, Computation time: 1.3993396759033203\n",
      "Step: 4494, Loss: 0.9160104990005493, Accuracy: 1.0, Computation time: 0.9110796451568604\n",
      "Step: 4495, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.0165088176727295\n",
      "Step: 4496, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 0.8378396034240723\n",
      "Step: 4497, Loss: 0.9373036623001099, Accuracy: 0.9821428656578064, Computation time: 0.93332839012146\n",
      "Step: 4498, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 0.9304471015930176\n",
      "Step: 4499, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.1830058097839355\n",
      "Step: 4500, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.1471095085144043\n",
      "Step: 4501, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 1.0676906108856201\n",
      "Step: 4502, Loss: 0.9158353805541992, Accuracy: 1.0, Computation time: 1.245304822921753\n",
      "Step: 4503, Loss: 0.9158380627632141, Accuracy: 1.0, Computation time: 0.9240670204162598\n",
      "Step: 4504, Loss: 0.9303077459335327, Accuracy: 0.949999988079071, Computation time: 1.401393175125122\n",
      "Step: 4505, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.48726487159729\n",
      "Step: 4506, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.0721535682678223\n",
      "Step: 4507, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 0.981680154800415\n",
      "Step: 4508, Loss: 0.9161288738250732, Accuracy: 1.0, Computation time: 1.358565092086792\n",
      "Step: 4509, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 1.1975014209747314\n",
      "Step: 4510, Loss: 0.9160398244857788, Accuracy: 1.0, Computation time: 1.062119960784912\n",
      "Step: 4511, Loss: 0.9375113248825073, Accuracy: 0.9772727489471436, Computation time: 1.4158873558044434\n",
      "Step: 4512, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.3137829303741455\n",
      "Step: 4513, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 0.9250240325927734\n",
      "Step: 4514, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 0.9330110549926758\n",
      "Step: 4515, Loss: 0.9158387184143066, Accuracy: 1.0, Computation time: 0.9595654010772705\n",
      "Step: 4516, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 0.9991436004638672\n",
      "Step: 4517, Loss: 0.9164203405380249, Accuracy: 1.0, Computation time: 1.0633721351623535\n",
      "Step: 4518, Loss: 0.9590484499931335, Accuracy: 0.9375, Computation time: 1.098475456237793\n",
      "Step: 4519, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.401829719543457\n",
      "Step: 4520, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 1.6247425079345703\n",
      "Step: 4521, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.6864073276519775\n",
      "Step: 4522, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.3668625354766846\n",
      "Step: 4523, Loss: 0.9161030650138855, Accuracy: 1.0, Computation time: 1.5920209884643555\n",
      "Step: 4524, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.5058410167694092\n",
      "Step: 4525, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 1.0887112617492676\n",
      "Step: 4526, Loss: 0.9162662029266357, Accuracy: 1.0, Computation time: 1.3728055953979492\n",
      "Step: 4527, Loss: 0.9372134208679199, Accuracy: 0.9750000238418579, Computation time: 1.667923927307129\n",
      "Step: 4528, Loss: 0.9158437252044678, Accuracy: 1.0, Computation time: 1.2811295986175537\n",
      "Step: 4529, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.4998681545257568\n",
      "Step: 4530, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.2447237968444824\n",
      "Step: 4531, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.3565962314605713\n",
      "Step: 4532, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 1.4580676555633545\n",
      "Step: 4533, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.140648365020752\n",
      "Step: 4534, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.3663029670715332\n",
      "Step: 4535, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.198854684829712\n",
      "Step: 4536, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.5995521545410156\n",
      "Step: 4537, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 1.3484327793121338\n",
      "Step: 4538, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.293750286102295\n",
      "Step: 4539, Loss: 0.9164615273475647, Accuracy: 1.0, Computation time: 1.6524055004119873\n",
      "Step: 4540, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.2547526359558105\n",
      "Step: 4541, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 1.327756404876709\n",
      "Step: 4542, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.3403041362762451\n",
      "Step: 4543, Loss: 0.9589866399765015, Accuracy: 0.9272727370262146, Computation time: 1.4352171421051025\n",
      "Step: 4544, Loss: 0.9164390563964844, Accuracy: 1.0, Computation time: 1.4342398643493652\n",
      "Step: 4545, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.5709688663482666\n",
      "Step: 4546, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.3000681400299072\n",
      "Step: 4547, Loss: 0.9158429503440857, Accuracy: nan, Computation time: 1.1980056762695312\n",
      "Step: 4548, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.264517068862915\n",
      "Step: 4549, Loss: 0.9164594411849976, Accuracy: 1.0, Computation time: 1.3922264575958252\n",
      "Step: 4550, Loss: 0.9162342548370361, Accuracy: 1.0, Computation time: 1.2126221656799316\n",
      "Step: 4551, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.2916691303253174\n",
      "Step: 4552, Loss: 0.9339823722839355, Accuracy: 0.9722222089767456, Computation time: 1.2170917987823486\n",
      "Step: 4553, Loss: 0.9395273923873901, Accuracy: 0.9722222089767456, Computation time: 1.4565024375915527\n",
      "Step: 4554, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.3588931560516357\n",
      "Step: 4555, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 1.0797317028045654\n",
      "Step: 4556, Loss: 0.9160005450248718, Accuracy: 1.0, Computation time: 1.1279890537261963\n",
      "Step: 4557, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 1.2419853210449219\n",
      "Step: 4558, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 0.9608407020568848\n",
      "Step: 4559, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.20772385597229\n",
      "Step: 4560, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.003504753112793\n",
      "Step: 4561, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.2642803192138672\n",
      "Step: 4562, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.632535696029663\n",
      "Step: 4563, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.0318830013275146\n",
      "Step: 4564, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.5070252418518066\n",
      "Step: 4565, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 1.0800127983093262\n",
      "Step: 4566, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 1.1194729804992676\n",
      "Step: 4567, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 1.5340752601623535\n",
      "Step: 4568, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 1.315209150314331\n",
      "Step: 4569, Loss: 0.9376035332679749, Accuracy: 0.9791666865348816, Computation time: 1.477640151977539\n",
      "Step: 4570, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 0.9295351505279541\n",
      "Step: 4571, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.0457236766815186\n",
      "Step: 4572, Loss: 0.9159417748451233, Accuracy: 1.0, Computation time: 1.172194004058838\n",
      "Step: 4573, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 1.1212637424468994\n",
      "Step: 4574, Loss: 0.9160581231117249, Accuracy: 1.0, Computation time: 1.60215425491333\n",
      "Step: 4575, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 1.122567892074585\n",
      "Step: 4576, Loss: 0.9231126308441162, Accuracy: 1.0, Computation time: 1.3520357608795166\n",
      "Step: 4577, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 0.9275312423706055\n",
      "Step: 4578, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.545304775238037\n",
      "Step: 4579, Loss: 0.9159278869628906, Accuracy: 1.0, Computation time: 0.9262239933013916\n",
      "Step: 4580, Loss: 0.9159860610961914, Accuracy: 1.0, Computation time: 1.3361570835113525\n",
      "Step: 4581, Loss: 0.9375026226043701, Accuracy: 0.9750000238418579, Computation time: 1.1834888458251953\n",
      "Step: 4582, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.172764778137207\n",
      "Step: 4583, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.3174934387207031\n",
      "Step: 4584, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 0.9142899513244629\n",
      "Step: 4585, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 0.9678630828857422\n",
      "########################\n",
      "Test loss: 1.1200940608978271, Test Accuracy_epoch33: 0.6997714042663574\n",
      "########################\n",
      "Step: 4586, Loss: 0.9368218779563904, Accuracy: 0.9642857313156128, Computation time: 1.2284069061279297\n",
      "Step: 4587, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.1200151443481445\n",
      "Step: 4588, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.8967885971069336\n",
      "Step: 4589, Loss: 0.9179182052612305, Accuracy: 1.0, Computation time: 1.4712989330291748\n",
      "Step: 4590, Loss: 0.9376925230026245, Accuracy: 0.9807692766189575, Computation time: 0.9505517482757568\n",
      "Step: 4591, Loss: 0.9376099705696106, Accuracy: 0.9750000238418579, Computation time: 0.9963126182556152\n",
      "Step: 4592, Loss: 0.9290226101875305, Accuracy: 0.984375, Computation time: 1.5327973365783691\n",
      "Step: 4593, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.0452547073364258\n",
      "Step: 4594, Loss: 0.9160324931144714, Accuracy: 1.0, Computation time: 1.1876885890960693\n",
      "Step: 4595, Loss: 0.9159830212593079, Accuracy: 1.0, Computation time: 1.0804650783538818\n",
      "Step: 4596, Loss: 0.9377938508987427, Accuracy: 0.9821428656578064, Computation time: 1.1565732955932617\n",
      "Step: 4597, Loss: 0.9159461259841919, Accuracy: 1.0, Computation time: 1.4621446132659912\n",
      "Step: 4598, Loss: 0.9273762702941895, Accuracy: 0.9722222089767456, Computation time: 1.5917026996612549\n",
      "Step: 4599, Loss: 0.9353654980659485, Accuracy: 0.9750000238418579, Computation time: 1.030538558959961\n",
      "Step: 4600, Loss: 0.9159775376319885, Accuracy: 1.0, Computation time: 1.0747525691986084\n",
      "Step: 4601, Loss: 0.9337894320487976, Accuracy: 0.9375, Computation time: 1.4785881042480469\n",
      "Step: 4602, Loss: 0.9376658797264099, Accuracy: 0.9583333730697632, Computation time: 1.1376044750213623\n",
      "Step: 4603, Loss: 0.9163388013839722, Accuracy: 1.0, Computation time: 1.3775136470794678\n",
      "Step: 4604, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.3437297344207764\n",
      "Step: 4605, Loss: 0.915972113609314, Accuracy: 1.0, Computation time: 1.0630254745483398\n",
      "Step: 4606, Loss: 0.9159350395202637, Accuracy: 1.0, Computation time: 1.3710980415344238\n",
      "Step: 4607, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.0313680171966553\n",
      "Step: 4608, Loss: 0.9324434399604797, Accuracy: 0.949999988079071, Computation time: 1.7395296096801758\n",
      "Step: 4609, Loss: 0.9159862995147705, Accuracy: 1.0, Computation time: 1.2009057998657227\n",
      "Step: 4610, Loss: 0.9160398840904236, Accuracy: 1.0, Computation time: 1.2056708335876465\n",
      "Step: 4611, Loss: 0.9160009026527405, Accuracy: 1.0, Computation time: 1.1008117198944092\n",
      "Step: 4612, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 1.0330097675323486\n",
      "Step: 4613, Loss: 0.915935754776001, Accuracy: 1.0, Computation time: 1.1623270511627197\n",
      "Step: 4614, Loss: 0.9160595536231995, Accuracy: 1.0, Computation time: 1.2315022945404053\n",
      "Step: 4615, Loss: 0.9160468578338623, Accuracy: 1.0, Computation time: 1.6111607551574707\n",
      "Step: 4616, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.1986842155456543\n",
      "Step: 4617, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 0.9855201244354248\n",
      "Step: 4618, Loss: 0.9376474618911743, Accuracy: 0.9722222089767456, Computation time: 1.5216567516326904\n",
      "Step: 4619, Loss: 0.9593018293380737, Accuracy: 0.9415584802627563, Computation time: 0.9683105945587158\n",
      "Step: 4620, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.1831226348876953\n",
      "Step: 4621, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.0679798126220703\n",
      "Step: 4622, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 0.9899072647094727\n",
      "Step: 4623, Loss: 0.916176974773407, Accuracy: 1.0, Computation time: 1.2971229553222656\n",
      "Step: 4624, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.9682693481445312\n",
      "Step: 4625, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.0043079853057861\n",
      "Step: 4626, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.0783665180206299\n",
      "Step: 4627, Loss: 0.9375672340393066, Accuracy: 0.9642857313156128, Computation time: 1.1080868244171143\n",
      "Step: 4628, Loss: 0.9173905849456787, Accuracy: 1.0, Computation time: 2.013197898864746\n",
      "Step: 4629, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 0.9878900051116943\n",
      "Step: 4630, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.039207935333252\n",
      "Step: 4631, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.0763659477233887\n",
      "Step: 4632, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 0.8510527610778809\n",
      "Step: 4633, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 0.8926482200622559\n",
      "Step: 4634, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.3692209720611572\n",
      "Step: 4635, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.1951916217803955\n",
      "Step: 4636, Loss: 0.9261865019798279, Accuracy: 0.96875, Computation time: 1.5506763458251953\n",
      "Step: 4637, Loss: 0.9375869631767273, Accuracy: 0.9821428656578064, Computation time: 1.2358345985412598\n",
      "Step: 4638, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.1459925174713135\n",
      "Step: 4639, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 1.023210048675537\n",
      "Step: 4640, Loss: 0.9372349381446838, Accuracy: 0.9821428656578064, Computation time: 1.0900263786315918\n",
      "Step: 4641, Loss: 0.9159924983978271, Accuracy: 1.0, Computation time: 1.8509037494659424\n",
      "Step: 4642, Loss: 0.9159292578697205, Accuracy: 1.0, Computation time: 1.134347915649414\n",
      "Step: 4643, Loss: 0.9375128149986267, Accuracy: 0.9772727489471436, Computation time: 1.3743689060211182\n",
      "Step: 4644, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 1.1059777736663818\n",
      "Step: 4645, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.1968083381652832\n",
      "Step: 4646, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.1097829341888428\n",
      "Step: 4647, Loss: 0.9381498694419861, Accuracy: 0.9750000238418579, Computation time: 1.5316641330718994\n",
      "Step: 4648, Loss: 0.9159846901893616, Accuracy: 1.0, Computation time: 1.0426547527313232\n",
      "Step: 4649, Loss: 0.9367356896400452, Accuracy: 0.9772727489471436, Computation time: 1.7423195838928223\n",
      "Step: 4650, Loss: 0.9160165190696716, Accuracy: 1.0, Computation time: 1.188080072402954\n",
      "Step: 4651, Loss: 0.9159905314445496, Accuracy: 1.0, Computation time: 0.9921560287475586\n",
      "Step: 4652, Loss: 0.9159725904464722, Accuracy: 1.0, Computation time: 1.1879780292510986\n",
      "Step: 4653, Loss: 0.9160000681877136, Accuracy: 1.0, Computation time: 1.5196843147277832\n",
      "Step: 4654, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.0792262554168701\n",
      "Step: 4655, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.1071112155914307\n",
      "Step: 4656, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.265848159790039\n",
      "Step: 4657, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 1.3082940578460693\n",
      "Step: 4658, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 1.3500032424926758\n",
      "Step: 4659, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 1.265620470046997\n",
      "Step: 4660, Loss: 0.9291197061538696, Accuracy: 0.9791666865348816, Computation time: 1.5606749057769775\n",
      "Step: 4661, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 1.0042667388916016\n",
      "Step: 4662, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.168597936630249\n",
      "Step: 4663, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 0.9151749610900879\n",
      "Step: 4664, Loss: 0.9375022649765015, Accuracy: 0.9791666865348816, Computation time: 1.4607212543487549\n",
      "Step: 4665, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 1.237349033355713\n",
      "Step: 4666, Loss: 0.9372881054878235, Accuracy: 0.9807692766189575, Computation time: 1.1861302852630615\n",
      "Step: 4667, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.0558133125305176\n",
      "Step: 4668, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 0.8789534568786621\n",
      "Step: 4669, Loss: 0.937477707862854, Accuracy: 0.9750000238418579, Computation time: 1.5559239387512207\n",
      "Step: 4670, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.3125989437103271\n",
      "Step: 4671, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.6163904666900635\n",
      "Step: 4672, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.1199493408203125\n",
      "Step: 4673, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.3370375633239746\n",
      "Step: 4674, Loss: 0.9360780715942383, Accuracy: 0.9807692766189575, Computation time: 1.2144594192504883\n",
      "Step: 4675, Loss: 0.9158638715744019, Accuracy: 1.0, Computation time: 1.4471547603607178\n",
      "Step: 4676, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 1.7947132587432861\n",
      "Step: 4677, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.2216517925262451\n",
      "Step: 4678, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 1.0184755325317383\n",
      "Step: 4679, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 1.6340243816375732\n",
      "Step: 4680, Loss: 0.9161523580551147, Accuracy: 1.0, Computation time: 1.371948003768921\n",
      "Step: 4681, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.1515202522277832\n",
      "Step: 4682, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.2068991661071777\n",
      "Step: 4683, Loss: 0.915849506855011, Accuracy: 1.0, Computation time: 1.5193212032318115\n",
      "Step: 4684, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 1.5509767532348633\n",
      "Step: 4685, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 0.9331467151641846\n",
      "Step: 4686, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.4064853191375732\n",
      "Step: 4687, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.4388139247894287\n",
      "Step: 4688, Loss: 0.9191601872444153, Accuracy: 1.0, Computation time: 1.1292266845703125\n",
      "Step: 4689, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 1.3101177215576172\n",
      "Step: 4690, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.0406346321105957\n",
      "Step: 4691, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.2572145462036133\n",
      "Step: 4692, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.0537331104278564\n",
      "Step: 4693, Loss: 0.9172957539558411, Accuracy: 1.0, Computation time: 1.630692958831787\n",
      "Step: 4694, Loss: 0.959148645401001, Accuracy: 0.9352940917015076, Computation time: 1.1501078605651855\n",
      "Step: 4695, Loss: 0.9194023609161377, Accuracy: 1.0, Computation time: 1.5914313793182373\n",
      "Step: 4696, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 1.6390507221221924\n",
      "Step: 4697, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 1.614013433456421\n",
      "Step: 4698, Loss: 0.9159367680549622, Accuracy: 1.0, Computation time: 1.1971392631530762\n",
      "Step: 4699, Loss: 0.9159547686576843, Accuracy: 1.0, Computation time: 1.163111686706543\n",
      "Step: 4700, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 1.3074150085449219\n",
      "Step: 4701, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 1.0754902362823486\n",
      "Step: 4702, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.340160846710205\n",
      "Step: 4703, Loss: 0.9375975728034973, Accuracy: 0.96875, Computation time: 1.4253926277160645\n",
      "Step: 4704, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.3716435432434082\n",
      "Step: 4705, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.3386409282684326\n",
      "Step: 4706, Loss: 0.9168516993522644, Accuracy: 1.0, Computation time: 1.530458927154541\n",
      "Step: 4707, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.5069196224212646\n",
      "Step: 4708, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.2335186004638672\n",
      "Step: 4709, Loss: 0.9166446924209595, Accuracy: 1.0, Computation time: 1.330958604812622\n",
      "Step: 4710, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.6791725158691406\n",
      "Step: 4711, Loss: 0.959206223487854, Accuracy: 0.9437500238418579, Computation time: 1.3832294940948486\n",
      "Step: 4712, Loss: 0.9159094095230103, Accuracy: 1.0, Computation time: 1.538193941116333\n",
      "Step: 4713, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.3112664222717285\n",
      "Step: 4714, Loss: 0.9375786781311035, Accuracy: 0.9375, Computation time: 1.2286429405212402\n",
      "Step: 4715, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.5109353065490723\n",
      "Step: 4716, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 1.1839797496795654\n",
      "Step: 4717, Loss: 0.9158543944358826, Accuracy: 1.0, Computation time: 1.5062921047210693\n",
      "Step: 4718, Loss: 0.9159844517707825, Accuracy: 1.0, Computation time: 1.2372055053710938\n",
      "Step: 4719, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.525932788848877\n",
      "Step: 4720, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.4255492687225342\n",
      "Step: 4721, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.095454454421997\n",
      "Step: 4722, Loss: 0.9164310097694397, Accuracy: 1.0, Computation time: 1.9555740356445312\n",
      "Step: 4723, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.710021734237671\n",
      "Step: 4724, Loss: 0.915878415107727, Accuracy: 1.0, Computation time: 0.9581174850463867\n",
      "########################\n",
      "Test loss: 1.1185816526412964, Test Accuracy_epoch34: 0.7029614448547363\n",
      "########################\n",
      "Step: 4725, Loss: 0.9374934434890747, Accuracy: 0.9750000238418579, Computation time: 1.1649692058563232\n",
      "Step: 4726, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.328230381011963\n",
      "Step: 4727, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.3841569423675537\n",
      "Step: 4728, Loss: 0.9158364534378052, Accuracy: 1.0, Computation time: 1.2955687046051025\n",
      "Step: 4729, Loss: 0.9372682571411133, Accuracy: 0.9791666865348816, Computation time: 2.0796666145324707\n",
      "Step: 4730, Loss: 0.9166009426116943, Accuracy: 1.0, Computation time: 1.2493867874145508\n",
      "Step: 4731, Loss: 0.9369326829910278, Accuracy: 0.96875, Computation time: 1.263162612915039\n",
      "Step: 4732, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 1.4242112636566162\n",
      "Step: 4733, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.1254088878631592\n",
      "Step: 4734, Loss: 0.937628984451294, Accuracy: 0.9722222089767456, Computation time: 1.459609031677246\n",
      "Step: 4735, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 1.4586741924285889\n",
      "Step: 4736, Loss: 0.9166505932807922, Accuracy: 1.0, Computation time: 1.1221773624420166\n",
      "Step: 4737, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.414233684539795\n",
      "Step: 4738, Loss: 0.9161180853843689, Accuracy: 1.0, Computation time: 1.5130329132080078\n",
      "Step: 4739, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.104950189590454\n",
      "Step: 4740, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.5153183937072754\n",
      "Step: 4741, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.1581733226776123\n",
      "Step: 4742, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.3484220504760742\n",
      "Step: 4743, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.1276791095733643\n",
      "Step: 4744, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.2937166690826416\n",
      "Step: 4745, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.5587406158447266\n",
      "Step: 4746, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.1531875133514404\n",
      "Step: 4747, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.6474285125732422\n",
      "Step: 4748, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 1.318824291229248\n",
      "Step: 4749, Loss: 0.9381766319274902, Accuracy: 0.9821428656578064, Computation time: 1.857081413269043\n",
      "Step: 4750, Loss: 0.9304161667823792, Accuracy: 0.9642857313156128, Computation time: 1.6337144374847412\n",
      "Step: 4751, Loss: 0.9192249178886414, Accuracy: 1.0, Computation time: 2.207209825515747\n",
      "Step: 4752, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.0446925163269043\n",
      "Step: 4753, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.2052156925201416\n",
      "Step: 4754, Loss: 0.9159935712814331, Accuracy: 1.0, Computation time: 1.5432395935058594\n",
      "Step: 4755, Loss: 0.9159333109855652, Accuracy: 1.0, Computation time: 1.1606109142303467\n",
      "Step: 4756, Loss: 0.9594267010688782, Accuracy: 0.9464285969734192, Computation time: 1.7141010761260986\n",
      "Step: 4757, Loss: 0.9160169959068298, Accuracy: 1.0, Computation time: 1.5034804344177246\n",
      "Step: 4758, Loss: 0.9353308081626892, Accuracy: 0.96875, Computation time: 1.9942858219146729\n",
      "Step: 4759, Loss: 0.9307982325553894, Accuracy: 0.9791666865348816, Computation time: 1.335439920425415\n",
      "Step: 4760, Loss: 0.9219778180122375, Accuracy: 1.0, Computation time: 1.5008883476257324\n",
      "Step: 4761, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 1.6273188591003418\n",
      "Step: 4762, Loss: 0.9160542488098145, Accuracy: 1.0, Computation time: 1.2358872890472412\n",
      "Step: 4763, Loss: 0.9162905812263489, Accuracy: 1.0, Computation time: 1.3616843223571777\n",
      "Step: 4764, Loss: 0.9161301851272583, Accuracy: 1.0, Computation time: 1.056220531463623\n",
      "Step: 4765, Loss: 0.940971851348877, Accuracy: 0.9772727489471436, Computation time: 1.637481689453125\n",
      "Step: 4766, Loss: 0.9160396456718445, Accuracy: 1.0, Computation time: 1.3183724880218506\n",
      "Step: 4767, Loss: 0.9159806966781616, Accuracy: 1.0, Computation time: 1.2780704498291016\n",
      "Step: 4768, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.0467491149902344\n",
      "Step: 4769, Loss: 0.9158995151519775, Accuracy: 1.0, Computation time: 1.3728291988372803\n",
      "Step: 4770, Loss: 0.9160877466201782, Accuracy: 1.0, Computation time: 1.4667682647705078\n",
      "Step: 4771, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.2041263580322266\n",
      "Step: 4772, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.1896100044250488\n",
      "Step: 4773, Loss: 0.915951132774353, Accuracy: 1.0, Computation time: 1.4653985500335693\n",
      "Step: 4774, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 0.9467694759368896\n",
      "Step: 4775, Loss: 0.9375816583633423, Accuracy: 0.9833333492279053, Computation time: 1.0160818099975586\n",
      "Step: 4776, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 1.690647840499878\n",
      "Step: 4777, Loss: 0.9379117488861084, Accuracy: 0.9722222089767456, Computation time: 1.3074729442596436\n",
      "Step: 4778, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 1.0134878158569336\n",
      "Step: 4779, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.3288345336914062\n",
      "Step: 4780, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 1.5386366844177246\n",
      "Step: 4781, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 1.0602607727050781\n",
      "Step: 4782, Loss: 0.9161259531974792, Accuracy: 1.0, Computation time: 1.5380773544311523\n",
      "Step: 4783, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.1085762977600098\n",
      "Step: 4784, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.0623986721038818\n",
      "Step: 4785, Loss: 0.9160445928573608, Accuracy: 1.0, Computation time: 1.1372487545013428\n",
      "Step: 4786, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.0817842483520508\n",
      "Step: 4787, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.287891149520874\n",
      "Step: 4788, Loss: 0.9173444509506226, Accuracy: 1.0, Computation time: 1.6822242736816406\n",
      "Step: 4789, Loss: 0.9368722438812256, Accuracy: 0.9821428656578064, Computation time: 1.5394654273986816\n",
      "Step: 4790, Loss: 0.9422628283500671, Accuracy: 0.9791666865348816, Computation time: 1.328887701034546\n",
      "Step: 4791, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.2585477828979492\n",
      "Step: 4792, Loss: 0.9391465187072754, Accuracy: 0.9807692766189575, Computation time: 1.9775218963623047\n",
      "Step: 4793, Loss: 0.9162189960479736, Accuracy: 1.0, Computation time: 1.206242561340332\n",
      "Step: 4794, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.241126537322998\n",
      "Step: 4795, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.2350749969482422\n",
      "Step: 4796, Loss: 0.9159045815467834, Accuracy: 1.0, Computation time: 1.078129768371582\n",
      "Step: 4797, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.1371183395385742\n",
      "Step: 4798, Loss: 0.9166337251663208, Accuracy: 1.0, Computation time: 1.5396511554718018\n",
      "Step: 4799, Loss: 0.937568187713623, Accuracy: 0.9791666865348816, Computation time: 1.393998384475708\n",
      "Step: 4800, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 1.539304494857788\n",
      "Step: 4801, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.296224594116211\n",
      "Step: 4802, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.3150007724761963\n",
      "Step: 4803, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.217153549194336\n",
      "Step: 4804, Loss: 0.915908694267273, Accuracy: 1.0, Computation time: 1.379131555557251\n",
      "Step: 4805, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.566751480102539\n",
      "Step: 4806, Loss: 0.9374530911445618, Accuracy: 0.9821428656578064, Computation time: 2.2344326972961426\n",
      "Step: 4807, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 1.336742877960205\n",
      "Step: 4808, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.3718798160552979\n",
      "Step: 4809, Loss: 0.9160460829734802, Accuracy: 1.0, Computation time: 1.429790735244751\n",
      "Step: 4810, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 1.3234586715698242\n",
      "Step: 4811, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 1.1859745979309082\n",
      "Step: 4812, Loss: 0.9588189721107483, Accuracy: 0.9615384340286255, Computation time: 1.8302934169769287\n",
      "Step: 4813, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.484696865081787\n",
      "Step: 4814, Loss: 0.9375270009040833, Accuracy: 0.96875, Computation time: 1.7080647945404053\n",
      "Step: 4815, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 1.0938365459442139\n",
      "Step: 4816, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.262946367263794\n",
      "Step: 4817, Loss: 0.9159265756607056, Accuracy: 1.0, Computation time: 1.2238185405731201\n",
      "Step: 4818, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 1.538374423980713\n",
      "Step: 4819, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.391404628753662\n",
      "Step: 4820, Loss: 0.9158821105957031, Accuracy: 1.0, Computation time: 1.2603728771209717\n",
      "Step: 4821, Loss: 0.9160895943641663, Accuracy: 1.0, Computation time: 1.3495049476623535\n",
      "Step: 4822, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 2.0019025802612305\n",
      "Step: 4823, Loss: 0.9274908304214478, Accuracy: 0.9791666865348816, Computation time: 1.687049150466919\n",
      "Step: 4824, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 1.324047565460205\n",
      "Step: 4825, Loss: 0.9376315474510193, Accuracy: 0.9750000238418579, Computation time: 1.3075213432312012\n",
      "Step: 4826, Loss: 0.9160304069519043, Accuracy: 1.0, Computation time: 1.2892124652862549\n",
      "Step: 4827, Loss: 0.9344808459281921, Accuracy: 0.9772727489471436, Computation time: 1.081608533859253\n",
      "Step: 4828, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.4566006660461426\n",
      "Step: 4829, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 1.0489258766174316\n",
      "Step: 4830, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.2830684185028076\n",
      "Step: 4831, Loss: 0.9158453941345215, Accuracy: 1.0, Computation time: 1.3076972961425781\n",
      "Step: 4832, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 1.129950761795044\n",
      "Step: 4833, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.9202725887298584\n",
      "Step: 4834, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 1.6651487350463867\n",
      "Step: 4835, Loss: 0.9159005284309387, Accuracy: 1.0, Computation time: 1.1524250507354736\n",
      "Step: 4836, Loss: 0.9372716546058655, Accuracy: 0.9750000238418579, Computation time: 1.6190767288208008\n",
      "Step: 4837, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 1.5167078971862793\n",
      "Step: 4838, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.448155164718628\n",
      "Step: 4839, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 1.4795591831207275\n",
      "Step: 4840, Loss: 0.916441798210144, Accuracy: 1.0, Computation time: 1.2585546970367432\n",
      "Step: 4841, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 1.327911138534546\n",
      "Step: 4842, Loss: 0.9592430591583252, Accuracy: 0.9557692408561707, Computation time: 1.1919164657592773\n",
      "Step: 4843, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.1142282485961914\n",
      "Step: 4844, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 1.4477193355560303\n",
      "Step: 4845, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.222074270248413\n",
      "Step: 4846, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 1.3966925144195557\n",
      "Step: 4847, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.3702919483184814\n",
      "Step: 4848, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.2132542133331299\n",
      "Step: 4849, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.2742574214935303\n",
      "Step: 4850, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.2556853294372559\n",
      "Step: 4851, Loss: 0.9374103546142578, Accuracy: 0.9722222089767456, Computation time: 1.447098731994629\n",
      "Step: 4852, Loss: 0.937415599822998, Accuracy: 0.9750000238418579, Computation time: 1.5155363082885742\n",
      "Step: 4853, Loss: 0.9158594012260437, Accuracy: 1.0, Computation time: 1.2614655494689941\n",
      "Step: 4854, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.4812657833099365\n",
      "Step: 4855, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 1.4511353969573975\n",
      "Step: 4856, Loss: 0.91591876745224, Accuracy: 1.0, Computation time: 1.204981803894043\n",
      "Step: 4857, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 1.1252021789550781\n",
      "Step: 4858, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 1.4508543014526367\n",
      "Step: 4859, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.9719717502593994\n",
      "Step: 4860, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 1.501103162765503\n",
      "Step: 4861, Loss: 0.9317270517349243, Accuracy: 0.9772727489471436, Computation time: 1.793764352798462\n",
      "Step: 4862, Loss: 0.9175287485122681, Accuracy: 1.0, Computation time: 1.3045217990875244\n",
      "Step: 4863, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.33036208152771\n",
      "########################\n",
      "Test loss: 1.119931936264038, Test Accuracy_epoch35: 0.701745867729187\n",
      "########################\n",
      "Step: 4864, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.6349313259124756\n",
      "Step: 4865, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.4380524158477783\n",
      "Step: 4866, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 1.2293837070465088\n",
      "Step: 4867, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.533928632736206\n",
      "Step: 4868, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 1.2643024921417236\n",
      "Step: 4869, Loss: 0.9158585667610168, Accuracy: nan, Computation time: 1.6740238666534424\n",
      "Step: 4870, Loss: 0.9374504089355469, Accuracy: 0.9166666865348816, Computation time: 1.1919834613800049\n",
      "Step: 4871, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.3326573371887207\n",
      "Step: 4872, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.4525439739227295\n",
      "Step: 4873, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.6418585777282715\n",
      "Step: 4874, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.195730209350586\n",
      "Step: 4875, Loss: 0.9375860691070557, Accuracy: 0.9583333730697632, Computation time: 1.646622657775879\n",
      "Step: 4876, Loss: 0.9373676180839539, Accuracy: 0.9807692766189575, Computation time: 2.0047686100006104\n",
      "Step: 4877, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.3795130252838135\n",
      "Step: 4878, Loss: 0.9160340428352356, Accuracy: 1.0, Computation time: 1.4134409427642822\n",
      "Step: 4879, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 1.315819501876831\n",
      "Step: 4880, Loss: 0.9375382661819458, Accuracy: 0.9375, Computation time: 1.3935649394989014\n",
      "Step: 4881, Loss: 0.9164987802505493, Accuracy: 1.0, Computation time: 1.9284112453460693\n",
      "Step: 4882, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.4711973667144775\n",
      "Step: 4883, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.485213279724121\n",
      "Step: 4884, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.9690635204315186\n",
      "Step: 4885, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.312628984451294\n",
      "Step: 4886, Loss: 0.9159075617790222, Accuracy: 1.0, Computation time: 2.1602375507354736\n",
      "Step: 4887, Loss: 0.9235380291938782, Accuracy: 1.0, Computation time: 2.1534013748168945\n",
      "Step: 4888, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 1.5603001117706299\n",
      "Step: 4889, Loss: 0.937552273273468, Accuracy: 0.9722222089767456, Computation time: 1.4449563026428223\n",
      "Step: 4890, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.7099080085754395\n",
      "Step: 4891, Loss: 0.9198571443557739, Accuracy: 1.0, Computation time: 1.4588940143585205\n",
      "Step: 4892, Loss: 0.9160255789756775, Accuracy: 1.0, Computation time: 1.4481172561645508\n",
      "Step: 4893, Loss: 0.9160019159317017, Accuracy: 1.0, Computation time: 1.9087553024291992\n",
      "Step: 4894, Loss: 0.9160159230232239, Accuracy: 1.0, Computation time: 1.492522954940796\n",
      "Step: 4895, Loss: 0.9378732442855835, Accuracy: 0.9772727489471436, Computation time: 1.6168458461761475\n",
      "Step: 4896, Loss: 0.9200279712677002, Accuracy: 1.0, Computation time: 1.5220081806182861\n",
      "Step: 4897, Loss: 0.9375670552253723, Accuracy: 0.9583333730697632, Computation time: 1.4970767498016357\n",
      "Step: 4898, Loss: 0.9377889633178711, Accuracy: 0.9791666865348816, Computation time: 1.496068000793457\n",
      "Step: 4899, Loss: 0.9376545548439026, Accuracy: 0.9772727489471436, Computation time: 1.4207541942596436\n",
      "Step: 4900, Loss: 0.9160205721855164, Accuracy: 1.0, Computation time: 1.7521090507507324\n",
      "Step: 4901, Loss: 0.9159535765647888, Accuracy: 1.0, Computation time: 1.7202682495117188\n",
      "Step: 4902, Loss: 0.9289937019348145, Accuracy: 0.9772727489471436, Computation time: 2.611008882522583\n",
      "Step: 4903, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 1.3720178604125977\n",
      "Step: 4904, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.3357148170471191\n",
      "Step: 4905, Loss: 0.9159784317016602, Accuracy: 1.0, Computation time: 1.0361976623535156\n",
      "Step: 4906, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.601332426071167\n",
      "Step: 4907, Loss: 0.9159543514251709, Accuracy: 1.0, Computation time: 1.2797012329101562\n",
      "Step: 4908, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.118382215499878\n",
      "Step: 4909, Loss: 0.9211991429328918, Accuracy: 1.0, Computation time: 1.2790718078613281\n",
      "Step: 4910, Loss: 0.9159467816352844, Accuracy: 1.0, Computation time: 1.1272172927856445\n",
      "Step: 4911, Loss: 0.9163562655448914, Accuracy: 1.0, Computation time: 1.4035274982452393\n",
      "Step: 4912, Loss: 0.9159212708473206, Accuracy: 1.0, Computation time: 0.8662698268890381\n",
      "Step: 4913, Loss: 0.9159969091415405, Accuracy: 1.0, Computation time: 0.8908743858337402\n",
      "Step: 4914, Loss: 0.9160270690917969, Accuracy: 1.0, Computation time: 1.1202092170715332\n",
      "Step: 4915, Loss: 0.9160439372062683, Accuracy: 1.0, Computation time: 0.8611011505126953\n",
      "Step: 4916, Loss: 0.9159881472587585, Accuracy: 1.0, Computation time: 0.9988508224487305\n",
      "Step: 4917, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 0.9698355197906494\n",
      "Step: 4918, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.4715652465820312\n",
      "Step: 4919, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.243687629699707\n",
      "Step: 4920, Loss: 0.9159140586853027, Accuracy: 1.0, Computation time: 1.1554288864135742\n",
      "Step: 4921, Loss: 0.9159539937973022, Accuracy: 1.0, Computation time: 1.0141708850860596\n",
      "Step: 4922, Loss: 0.9159377217292786, Accuracy: 1.0, Computation time: 0.8914451599121094\n",
      "Step: 4923, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 0.986987829208374\n",
      "Step: 4924, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 1.0102863311767578\n",
      "Step: 4925, Loss: 0.9159199595451355, Accuracy: 1.0, Computation time: 1.342822790145874\n",
      "Step: 4926, Loss: 0.9376795887947083, Accuracy: 0.9722222089767456, Computation time: 1.2422308921813965\n",
      "Step: 4927, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.0533487796783447\n",
      "Step: 4928, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.024796485900879\n",
      "Step: 4929, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 0.9469780921936035\n",
      "Step: 4930, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 0.9590060710906982\n",
      "Step: 4931, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 0.8486630916595459\n",
      "Step: 4932, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 2.0482869148254395\n",
      "Step: 4933, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.1251723766326904\n",
      "Step: 4934, Loss: 0.9213043451309204, Accuracy: 1.0, Computation time: 1.4363117218017578\n",
      "Step: 4935, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 1.0398273468017578\n",
      "Step: 4936, Loss: 0.9375677704811096, Accuracy: 0.9791666865348816, Computation time: 1.0264928340911865\n",
      "Step: 4937, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 1.3664212226867676\n",
      "Step: 4938, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 0.9664490222930908\n",
      "Step: 4939, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.6210198402404785\n",
      "Step: 4940, Loss: 0.9342705607414246, Accuracy: 0.9750000238418579, Computation time: 1.9498836994171143\n",
      "Step: 4941, Loss: 0.9159656167030334, Accuracy: 1.0, Computation time: 1.1382384300231934\n",
      "Step: 4942, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.0769844055175781\n",
      "Step: 4943, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 1.3611235618591309\n",
      "Step: 4944, Loss: 0.915985643863678, Accuracy: 1.0, Computation time: 1.2601027488708496\n",
      "Step: 4945, Loss: 0.9375039935112, Accuracy: 0.9772727489471436, Computation time: 1.2956881523132324\n",
      "Step: 4946, Loss: 0.9159705638885498, Accuracy: 1.0, Computation time: 1.1136400699615479\n",
      "Step: 4947, Loss: 0.9591256976127625, Accuracy: 0.9409722089767456, Computation time: 1.4900431632995605\n",
      "Step: 4948, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 1.1904284954071045\n",
      "Step: 4949, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 1.3516435623168945\n",
      "Step: 4950, Loss: 0.9161048531532288, Accuracy: 1.0, Computation time: 1.283719778060913\n",
      "Step: 4951, Loss: 0.9207389950752258, Accuracy: 1.0, Computation time: 1.4272418022155762\n",
      "Step: 4952, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.2715888023376465\n",
      "Step: 4953, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 1.448948860168457\n",
      "Step: 4954, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.4080729484558105\n",
      "Step: 4955, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 1.2846488952636719\n",
      "Step: 4956, Loss: 0.9159275889396667, Accuracy: 1.0, Computation time: 1.452319622039795\n",
      "Step: 4957, Loss: 0.9376035928726196, Accuracy: 0.9821428656578064, Computation time: 1.3337581157684326\n",
      "Step: 4958, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.355518102645874\n",
      "Step: 4959, Loss: 0.915953516960144, Accuracy: 1.0, Computation time: 1.5243310928344727\n",
      "Step: 4960, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.49725341796875\n",
      "Step: 4961, Loss: 0.9398658275604248, Accuracy: 0.949999988079071, Computation time: 1.9809808731079102\n",
      "Step: 4962, Loss: 0.9377809166908264, Accuracy: 0.9642857313156128, Computation time: 1.252303123474121\n",
      "Step: 4963, Loss: 0.9381285309791565, Accuracy: 0.9772727489471436, Computation time: 1.769761562347412\n",
      "Step: 4964, Loss: 0.9160448312759399, Accuracy: 1.0, Computation time: 1.673774242401123\n",
      "Step: 4965, Loss: 0.916089653968811, Accuracy: 1.0, Computation time: 1.3647873401641846\n",
      "Step: 4966, Loss: 0.9159880876541138, Accuracy: 1.0, Computation time: 1.7001540660858154\n",
      "Step: 4967, Loss: 0.9160788059234619, Accuracy: 1.0, Computation time: 1.535287857055664\n",
      "Step: 4968, Loss: 0.9309002757072449, Accuracy: 0.9750000238418579, Computation time: 1.7745604515075684\n",
      "Step: 4969, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 1.5525860786437988\n",
      "Step: 4970, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 1.2788121700286865\n",
      "Step: 4971, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 1.3426594734191895\n",
      "Step: 4972, Loss: 0.9365561604499817, Accuracy: 0.9833333492279053, Computation time: 1.4867775440216064\n",
      "Step: 4973, Loss: 0.9198428988456726, Accuracy: 1.0, Computation time: 1.9287998676300049\n",
      "Step: 4974, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.7687907218933105\n",
      "Step: 4975, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 1.2932238578796387\n",
      "Step: 4976, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.1687941551208496\n",
      "Step: 4977, Loss: 0.9159489274024963, Accuracy: 1.0, Computation time: 1.252657175064087\n",
      "Step: 4978, Loss: 0.9159591794013977, Accuracy: 1.0, Computation time: 1.2637319564819336\n",
      "Step: 4979, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 1.1105294227600098\n",
      "Step: 4980, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 1.2902262210845947\n",
      "Step: 4981, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.3552815914154053\n",
      "Step: 4982, Loss: 0.915917694568634, Accuracy: 1.0, Computation time: 1.186352014541626\n",
      "Step: 4983, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.4182310104370117\n",
      "Step: 4984, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.2590076923370361\n",
      "Step: 4985, Loss: 0.9159995317459106, Accuracy: 1.0, Computation time: 2.0135087966918945\n",
      "Step: 4986, Loss: 0.9596685171127319, Accuracy: 0.9365079402923584, Computation time: 1.9473085403442383\n",
      "Step: 4987, Loss: 0.9158847332000732, Accuracy: 1.0, Computation time: 1.3325068950653076\n",
      "Step: 4988, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 1.2177326679229736\n",
      "Step: 4989, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.154212474822998\n",
      "Step: 4990, Loss: 0.9365501999855042, Accuracy: 0.9791666865348816, Computation time: 1.2305572032928467\n",
      "Step: 4991, Loss: 0.9375598430633545, Accuracy: 0.9583333730697632, Computation time: 1.1055221557617188\n",
      "Step: 4992, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 1.0968821048736572\n",
      "Step: 4993, Loss: 0.9369530081748962, Accuracy: 0.9791666865348816, Computation time: 1.179532766342163\n",
      "Step: 4994, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.2131474018096924\n",
      "Step: 4995, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.3733649253845215\n",
      "Step: 4996, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 1.524949550628662\n",
      "Step: 4997, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.3534152507781982\n",
      "Step: 4998, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 1.0852339267730713\n",
      "Step: 4999, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.4318649768829346\n",
      "Step: 5000, Loss: 0.9240603446960449, Accuracy: 1.0, Computation time: 1.3921191692352295\n",
      "Step: 5001, Loss: 0.9375287890434265, Accuracy: 0.9807692766189575, Computation time: 1.5185394287109375\n",
      "Step: 5002, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 1.276862621307373\n",
      "########################\n",
      "Test loss: 1.1236804723739624, Test Accuracy_epoch36: 0.6928576231002808\n",
      "########################\n",
      "Step: 5003, Loss: 0.9159042239189148, Accuracy: 1.0, Computation time: 1.164189100265503\n",
      "Step: 5004, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.1171183586120605\n",
      "Step: 5005, Loss: 0.9159132242202759, Accuracy: 1.0, Computation time: 1.226212739944458\n",
      "Step: 5006, Loss: 0.9592092633247375, Accuracy: 0.9494949579238892, Computation time: 1.379410743713379\n",
      "Step: 5007, Loss: 0.9161384105682373, Accuracy: 1.0, Computation time: 1.2323317527770996\n",
      "Step: 5008, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.30293607711792\n",
      "Step: 5009, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 1.1485633850097656\n",
      "Step: 5010, Loss: 0.9195905327796936, Accuracy: 1.0, Computation time: 1.2235631942749023\n",
      "Step: 5011, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 1.3444559574127197\n",
      "Step: 5012, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 0.9452569484710693\n",
      "Step: 5013, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.226039171218872\n",
      "Step: 5014, Loss: 0.9158564805984497, Accuracy: 1.0, Computation time: 1.262488842010498\n",
      "Step: 5015, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.8663501739501953\n",
      "Step: 5016, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.5000674724578857\n",
      "Step: 5017, Loss: 0.9195559024810791, Accuracy: 1.0, Computation time: 2.2540767192840576\n",
      "Step: 5018, Loss: 0.945025622844696, Accuracy: 0.9821428656578064, Computation time: 1.7817599773406982\n",
      "Step: 5019, Loss: 0.9160531759262085, Accuracy: 1.0, Computation time: 1.5824921131134033\n",
      "Step: 5020, Loss: 0.9378344416618347, Accuracy: 0.9642857313156128, Computation time: 1.7114806175231934\n",
      "Step: 5021, Loss: 0.9163510203361511, Accuracy: 1.0, Computation time: 2.009908676147461\n",
      "Step: 5022, Loss: 0.9163122773170471, Accuracy: 1.0, Computation time: 1.886427879333496\n",
      "Step: 5023, Loss: 0.916012704372406, Accuracy: 1.0, Computation time: 1.8439950942993164\n",
      "Step: 5024, Loss: 0.9160460829734802, Accuracy: 1.0, Computation time: 2.039234161376953\n",
      "Step: 5025, Loss: 0.917342483997345, Accuracy: 1.0, Computation time: 1.8678090572357178\n",
      "Step: 5026, Loss: 0.9179738163948059, Accuracy: 1.0, Computation time: 1.3743228912353516\n",
      "Step: 5027, Loss: 0.9162951707839966, Accuracy: 1.0, Computation time: 1.7911715507507324\n",
      "Step: 5028, Loss: 0.9163573384284973, Accuracy: 1.0, Computation time: 2.0098764896392822\n",
      "Step: 5029, Loss: 0.9163057208061218, Accuracy: 1.0, Computation time: 1.8902623653411865\n",
      "Step: 5030, Loss: 0.9163229465484619, Accuracy: 1.0, Computation time: 1.6823937892913818\n",
      "Step: 5031, Loss: 0.9161367416381836, Accuracy: 1.0, Computation time: 1.847252368927002\n",
      "Step: 5032, Loss: 0.9159740805625916, Accuracy: 1.0, Computation time: 1.2810027599334717\n",
      "Step: 5033, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 1.7344694137573242\n",
      "Step: 5034, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 1.7394707202911377\n",
      "Step: 5035, Loss: 0.9159592390060425, Accuracy: 1.0, Computation time: 1.800588607788086\n",
      "Step: 5036, Loss: 0.9378396272659302, Accuracy: 0.9791666865348816, Computation time: 1.9079256057739258\n",
      "Step: 5037, Loss: 0.9160279035568237, Accuracy: 1.0, Computation time: 1.5600147247314453\n",
      "Step: 5038, Loss: 0.9160512685775757, Accuracy: 1.0, Computation time: 1.6597235202789307\n",
      "Step: 5039, Loss: 0.9372690320014954, Accuracy: 0.9772727489471436, Computation time: 1.6186041831970215\n",
      "Step: 5040, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.4213933944702148\n",
      "Step: 5041, Loss: 0.9802420139312744, Accuracy: 0.8785714507102966, Computation time: 2.1794519424438477\n",
      "Step: 5042, Loss: 0.9159414768218994, Accuracy: 1.0, Computation time: 1.6982920169830322\n",
      "Step: 5043, Loss: 0.9159691333770752, Accuracy: 1.0, Computation time: 1.7521543502807617\n",
      "Step: 5044, Loss: 0.9159227013587952, Accuracy: 1.0, Computation time: 1.624727487564087\n",
      "Step: 5045, Loss: 0.917538583278656, Accuracy: 1.0, Computation time: 1.4642703533172607\n",
      "Step: 5046, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 1.2231216430664062\n",
      "Step: 5047, Loss: 0.9507692456245422, Accuracy: 0.9522727727890015, Computation time: 1.548973560333252\n",
      "Step: 5048, Loss: 0.9203569293022156, Accuracy: 1.0, Computation time: 1.8424797058105469\n",
      "Step: 5049, Loss: 0.9163181781768799, Accuracy: 1.0, Computation time: 1.5270869731903076\n",
      "Step: 5050, Loss: 0.916292130947113, Accuracy: 1.0, Computation time: 1.3013787269592285\n",
      "Step: 5051, Loss: 0.9163897633552551, Accuracy: 1.0, Computation time: 1.704089879989624\n",
      "Step: 5052, Loss: 0.9161235094070435, Accuracy: 1.0, Computation time: 1.3519909381866455\n",
      "Step: 5053, Loss: 0.9397901296615601, Accuracy: 0.96875, Computation time: 1.695258378982544\n",
      "Step: 5054, Loss: 0.9374992251396179, Accuracy: 0.949999988079071, Computation time: 1.8165671825408936\n",
      "Step: 5055, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.6829333305358887\n",
      "Step: 5056, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 1.3134357929229736\n",
      "Step: 5057, Loss: 0.9388721585273743, Accuracy: 0.9722222089767456, Computation time: 2.3161420822143555\n",
      "Step: 5058, Loss: 0.9161072969436646, Accuracy: 1.0, Computation time: 1.6527507305145264\n",
      "Step: 5059, Loss: 0.9164113998413086, Accuracy: 1.0, Computation time: 1.3582098484039307\n",
      "Step: 5060, Loss: 0.9243578910827637, Accuracy: 1.0, Computation time: 1.883612871170044\n",
      "Step: 5061, Loss: 0.9161003828048706, Accuracy: 1.0, Computation time: 1.1212124824523926\n",
      "Step: 5062, Loss: 0.950550377368927, Accuracy: 0.9166666865348816, Computation time: 1.899437427520752\n",
      "Step: 5063, Loss: 0.9160877466201782, Accuracy: 1.0, Computation time: 1.0811352729797363\n",
      "Step: 5064, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.1564228534698486\n",
      "Step: 5065, Loss: 0.9373098015785217, Accuracy: 0.9722222089767456, Computation time: 1.3282051086425781\n",
      "Step: 5066, Loss: 0.9167726635932922, Accuracy: 1.0, Computation time: 1.7770001888275146\n",
      "Step: 5067, Loss: 0.9159337878227234, Accuracy: 1.0, Computation time: 1.118647575378418\n",
      "Step: 5068, Loss: 0.9160874485969543, Accuracy: 1.0, Computation time: 1.2022721767425537\n",
      "Step: 5069, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.1706604957580566\n",
      "Step: 5070, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 1.4580793380737305\n",
      "Step: 5071, Loss: 0.9159205555915833, Accuracy: 1.0, Computation time: 1.3518624305725098\n",
      "Step: 5072, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.6086151599884033\n",
      "Step: 5073, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.027043342590332\n",
      "Step: 5074, Loss: 0.915916383266449, Accuracy: 1.0, Computation time: 1.0216131210327148\n",
      "Step: 5075, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 1.554487943649292\n",
      "Step: 5076, Loss: 0.9376612305641174, Accuracy: 0.9642857313156128, Computation time: 1.1296281814575195\n",
      "Step: 5077, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 1.4452323913574219\n",
      "Step: 5078, Loss: 0.9172170758247375, Accuracy: 1.0, Computation time: 1.6312901973724365\n",
      "Step: 5079, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.0648341178894043\n",
      "Step: 5080, Loss: 0.916002631187439, Accuracy: 1.0, Computation time: 1.3750057220458984\n",
      "Step: 5081, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.3868720531463623\n",
      "Step: 5082, Loss: 0.9160966873168945, Accuracy: 1.0, Computation time: 1.4198355674743652\n",
      "Step: 5083, Loss: 0.9161815643310547, Accuracy: 1.0, Computation time: 1.1103510856628418\n",
      "Step: 5084, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.6623094081878662\n",
      "Step: 5085, Loss: 0.93759685754776, Accuracy: 0.9750000238418579, Computation time: 1.2893331050872803\n",
      "Step: 5086, Loss: 0.9375451803207397, Accuracy: 0.9821428656578064, Computation time: 1.3521451950073242\n",
      "Step: 5087, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.3763856887817383\n",
      "Step: 5088, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 1.1559772491455078\n",
      "Step: 5089, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.5759611129760742\n",
      "Step: 5090, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 1.1373801231384277\n",
      "Step: 5091, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.0951995849609375\n",
      "Step: 5092, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.6109142303466797\n",
      "Step: 5093, Loss: 0.9377352595329285, Accuracy: 0.9722222089767456, Computation time: 1.2599859237670898\n",
      "Step: 5094, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.382887601852417\n",
      "Step: 5095, Loss: 0.9184202551841736, Accuracy: 1.0, Computation time: 1.4857141971588135\n",
      "Step: 5096, Loss: 0.9158550500869751, Accuracy: 1.0, Computation time: 1.0181493759155273\n",
      "Step: 5097, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.2965583801269531\n",
      "Step: 5098, Loss: 0.9159248471260071, Accuracy: 1.0, Computation time: 1.6323323249816895\n",
      "Step: 5099, Loss: 0.9376716613769531, Accuracy: 0.9807692766189575, Computation time: 1.407235860824585\n",
      "Step: 5100, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 1.4770333766937256\n",
      "Step: 5101, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 1.098104476928711\n",
      "Step: 5102, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.2921454906463623\n",
      "Step: 5103, Loss: 0.9583174586296082, Accuracy: 0.9500000476837158, Computation time: 1.9828200340270996\n",
      "Step: 5104, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.5451605319976807\n",
      "Step: 5105, Loss: 0.9173164963722229, Accuracy: 1.0, Computation time: 1.568643569946289\n",
      "Step: 5106, Loss: 0.9200365543365479, Accuracy: 1.0, Computation time: 1.4576146602630615\n",
      "Step: 5107, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.3989920616149902\n",
      "Step: 5108, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.1564533710479736\n",
      "Step: 5109, Loss: 0.9158722758293152, Accuracy: 1.0, Computation time: 1.407745599746704\n",
      "Step: 5110, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 1.3750629425048828\n",
      "Step: 5111, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.217622995376587\n",
      "Step: 5112, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.769515037536621\n",
      "Step: 5113, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.445786714553833\n",
      "Step: 5114, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.6164510250091553\n",
      "Step: 5115, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.1149351596832275\n",
      "Step: 5116, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.1429510116577148\n",
      "Step: 5117, Loss: 0.9376095533370972, Accuracy: 0.9722222089767456, Computation time: 1.5528337955474854\n",
      "Step: 5118, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 1.421337604522705\n",
      "Step: 5119, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.2452483177185059\n",
      "Step: 5120, Loss: 0.9309441447257996, Accuracy: 0.9750000238418579, Computation time: 1.8045191764831543\n",
      "Step: 5121, Loss: 0.9373708963394165, Accuracy: 0.9791666865348816, Computation time: 1.7610435485839844\n",
      "Step: 5122, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.2456462383270264\n",
      "Step: 5123, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.1333484649658203\n",
      "Step: 5124, Loss: 0.9317750930786133, Accuracy: 0.9807692766189575, Computation time: 2.1452078819274902\n",
      "Step: 5125, Loss: 0.9159393310546875, Accuracy: 1.0, Computation time: 1.6781821250915527\n",
      "Step: 5126, Loss: 0.9159990549087524, Accuracy: 1.0, Computation time: 1.3552563190460205\n",
      "Step: 5127, Loss: 0.9159935712814331, Accuracy: 1.0, Computation time: 1.1838641166687012\n",
      "Step: 5128, Loss: 0.91595458984375, Accuracy: 1.0, Computation time: 0.833843469619751\n",
      "Step: 5129, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 0.9792687892913818\n",
      "Step: 5130, Loss: 0.9159026741981506, Accuracy: 1.0, Computation time: 1.1342823505401611\n",
      "Step: 5131, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 0.9806032180786133\n",
      "Step: 5132, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.4250168800354004\n",
      "Step: 5133, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.185354232788086\n",
      "Step: 5134, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.1697165966033936\n",
      "Step: 5135, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.5168354511260986\n",
      "Step: 5136, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 0.9373235702514648\n",
      "Step: 5137, Loss: 0.9351003170013428, Accuracy: 0.949999988079071, Computation time: 1.4092142581939697\n",
      "Step: 5138, Loss: 0.9373761415481567, Accuracy: 0.9642857313156128, Computation time: 0.9866940975189209\n",
      "Step: 5139, Loss: 0.9159117937088013, Accuracy: 1.0, Computation time: 1.0769233703613281\n",
      "Step: 5140, Loss: 0.9159139394760132, Accuracy: nan, Computation time: 0.8599917888641357\n",
      "Step: 5141, Loss: 0.9158914685249329, Accuracy: 1.0, Computation time: 1.3315699100494385\n",
      "########################\n",
      "Test loss: 1.1205376386642456, Test Accuracy_epoch37: 0.6970787048339844\n",
      "########################\n",
      "Step: 5142, Loss: 0.9191919565200806, Accuracy: 1.0, Computation time: 1.7509675025939941\n",
      "Step: 5143, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.2496240139007568\n",
      "Step: 5144, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.1400253772735596\n",
      "Step: 5145, Loss: 0.9354415535926819, Accuracy: 0.9807692766189575, Computation time: 1.344928503036499\n",
      "Step: 5146, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.075498342514038\n",
      "Step: 5147, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.156336784362793\n",
      "Step: 5148, Loss: 0.9159409999847412, Accuracy: 1.0, Computation time: 1.6982038021087646\n",
      "Step: 5149, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.6309940814971924\n",
      "Step: 5150, Loss: 0.9390829801559448, Accuracy: 0.9583333730697632, Computation time: 1.755666971206665\n",
      "Step: 5151, Loss: 0.9387511014938354, Accuracy: 0.9821428656578064, Computation time: 2.177333354949951\n",
      "Step: 5152, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.2237167358398438\n",
      "Step: 5153, Loss: 0.9377139806747437, Accuracy: 0.9772727489471436, Computation time: 1.397761344909668\n",
      "Step: 5154, Loss: 0.9305247068405151, Accuracy: 0.9642857313156128, Computation time: 1.9535465240478516\n",
      "Step: 5155, Loss: 0.9183967113494873, Accuracy: 1.0, Computation time: 1.4413516521453857\n",
      "Step: 5156, Loss: 0.9160426259040833, Accuracy: 1.0, Computation time: 1.2383739948272705\n",
      "Step: 5157, Loss: 0.9161654114723206, Accuracy: 1.0, Computation time: 1.6983325481414795\n",
      "Step: 5158, Loss: 0.9160855412483215, Accuracy: 1.0, Computation time: 1.2124543190002441\n",
      "Step: 5159, Loss: 0.9160201549530029, Accuracy: 1.0, Computation time: 1.4066600799560547\n",
      "Step: 5160, Loss: 0.9377350807189941, Accuracy: 0.96875, Computation time: 1.2365238666534424\n",
      "Step: 5161, Loss: 0.9161421656608582, Accuracy: 1.0, Computation time: 1.7255370616912842\n",
      "Step: 5162, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.3168766498565674\n",
      "Step: 5163, Loss: 0.9159826040267944, Accuracy: 1.0, Computation time: 1.2235031127929688\n",
      "Step: 5164, Loss: 0.9159945249557495, Accuracy: 1.0, Computation time: 1.1952409744262695\n",
      "Step: 5165, Loss: 0.9160045981407166, Accuracy: 1.0, Computation time: 1.5341763496398926\n",
      "Step: 5166, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 1.2094829082489014\n",
      "Step: 5167, Loss: 0.9160091280937195, Accuracy: 1.0, Computation time: 2.0856144428253174\n",
      "Step: 5168, Loss: 0.937688410282135, Accuracy: 0.9722222089767456, Computation time: 1.459977626800537\n",
      "Step: 5169, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 1.7377758026123047\n",
      "Step: 5170, Loss: 0.94013512134552, Accuracy: 0.9791666865348816, Computation time: 1.8198318481445312\n",
      "Step: 5171, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 1.4608161449432373\n",
      "Step: 5172, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 1.4941887855529785\n",
      "Step: 5173, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.7960174083709717\n",
      "Step: 5174, Loss: 0.9158995151519775, Accuracy: 1.0, Computation time: 1.5807476043701172\n",
      "Step: 5175, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 1.8870842456817627\n",
      "Step: 5176, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.6970937252044678\n",
      "Step: 5177, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 1.6096117496490479\n",
      "Step: 5178, Loss: 0.9158833026885986, Accuracy: 1.0, Computation time: 1.9587287902832031\n",
      "Step: 5179, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.5154290199279785\n",
      "Step: 5180, Loss: 0.9289997816085815, Accuracy: 0.96875, Computation time: 1.7724101543426514\n",
      "Step: 5181, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.657092571258545\n",
      "Step: 5182, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 1.2786033153533936\n",
      "Step: 5183, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.737391471862793\n",
      "Step: 5184, Loss: 0.9158936738967896, Accuracy: 1.0, Computation time: 1.6225554943084717\n",
      "Step: 5185, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 1.405750036239624\n",
      "Step: 5186, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.2460577487945557\n",
      "Step: 5187, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.4688947200775146\n",
      "Step: 5188, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 2.077077627182007\n",
      "Step: 5189, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 1.7274415493011475\n",
      "Step: 5190, Loss: 0.9163370132446289, Accuracy: 1.0, Computation time: 1.6041605472564697\n",
      "Step: 5191, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 1.4942407608032227\n",
      "Step: 5192, Loss: 0.9374510645866394, Accuracy: 0.9642857313156128, Computation time: 1.3698444366455078\n",
      "Step: 5193, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.0468831062316895\n",
      "Step: 5194, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.6519660949707031\n",
      "Step: 5195, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 1.6805920600891113\n",
      "Step: 5196, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 1.3792572021484375\n",
      "Step: 5197, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 1.573317289352417\n",
      "Step: 5198, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.5587739944458008\n",
      "Step: 5199, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 1.4735791683197021\n",
      "Step: 5200, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.4778978824615479\n",
      "Step: 5201, Loss: 0.9379696249961853, Accuracy: 0.9750000238418579, Computation time: 1.4124627113342285\n",
      "Step: 5202, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.237504482269287\n",
      "Step: 5203, Loss: 0.93751460313797, Accuracy: 0.949999988079071, Computation time: 1.3213832378387451\n",
      "Step: 5204, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 1.2208428382873535\n",
      "Step: 5205, Loss: 0.915951132774353, Accuracy: 1.0, Computation time: 1.3828723430633545\n",
      "Step: 5206, Loss: 0.9161111116409302, Accuracy: 1.0, Computation time: 1.63230299949646\n",
      "Step: 5207, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 1.8896584510803223\n",
      "Step: 5208, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.115248203277588\n",
      "Step: 5209, Loss: 0.9162991642951965, Accuracy: 1.0, Computation time: 1.2172887325286865\n",
      "Step: 5210, Loss: 0.9169778823852539, Accuracy: 1.0, Computation time: 1.5741393566131592\n",
      "Step: 5211, Loss: 0.9326645731925964, Accuracy: 0.9750000238418579, Computation time: 1.4666762351989746\n",
      "Step: 5212, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 1.6721899509429932\n",
      "Step: 5213, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 1.4966919422149658\n",
      "Step: 5214, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.4519195556640625\n",
      "Step: 5215, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.4427340030670166\n",
      "Step: 5216, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.6056244373321533\n",
      "Step: 5217, Loss: 0.933865487575531, Accuracy: 0.9821428656578064, Computation time: 1.529395580291748\n",
      "Step: 5218, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.5732612609863281\n",
      "Step: 5219, Loss: 0.9163504242897034, Accuracy: 1.0, Computation time: 1.6476433277130127\n",
      "Step: 5220, Loss: 0.9499708414077759, Accuracy: 0.9479166865348816, Computation time: 1.2917144298553467\n",
      "Step: 5221, Loss: 0.9159713983535767, Accuracy: 1.0, Computation time: 1.6129074096679688\n",
      "Step: 5222, Loss: 0.915993869304657, Accuracy: 1.0, Computation time: 1.5651555061340332\n",
      "Step: 5223, Loss: 0.9159913063049316, Accuracy: 1.0, Computation time: 1.4140334129333496\n",
      "Step: 5224, Loss: 0.9159791469573975, Accuracy: 1.0, Computation time: 1.5188283920288086\n",
      "Step: 5225, Loss: 0.9375868439674377, Accuracy: 0.9722222089767456, Computation time: 1.3065004348754883\n",
      "Step: 5226, Loss: 0.9295424222946167, Accuracy: 0.9833333492279053, Computation time: 1.838226079940796\n",
      "Step: 5227, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 1.2667300701141357\n",
      "Step: 5228, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 1.4065937995910645\n",
      "Step: 5229, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 1.463728666305542\n",
      "Step: 5230, Loss: 0.9159452319145203, Accuracy: 1.0, Computation time: 0.902799129486084\n",
      "Step: 5231, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 1.7521135807037354\n",
      "Step: 5232, Loss: 0.9159694314002991, Accuracy: 1.0, Computation time: 1.400911808013916\n",
      "Step: 5233, Loss: 0.9170694351196289, Accuracy: 1.0, Computation time: 1.598473310470581\n",
      "Step: 5234, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.856537103652954\n",
      "Step: 5235, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 1.3564891815185547\n",
      "Step: 5236, Loss: 0.9372186660766602, Accuracy: 0.9375, Computation time: 1.6483311653137207\n",
      "Step: 5237, Loss: 0.9168463945388794, Accuracy: 1.0, Computation time: 1.348360300064087\n",
      "Step: 5238, Loss: 0.916297435760498, Accuracy: 1.0, Computation time: 1.344132661819458\n",
      "Step: 5239, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 1.7497446537017822\n",
      "Step: 5240, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 1.2409067153930664\n",
      "Step: 5241, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.8576674461364746\n",
      "Step: 5242, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 1.8844056129455566\n",
      "Step: 5243, Loss: 0.9173338413238525, Accuracy: 1.0, Computation time: 1.6065003871917725\n",
      "Step: 5244, Loss: 0.9371771216392517, Accuracy: 0.9852941036224365, Computation time: 1.8235511779785156\n",
      "Step: 5245, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.9839739799499512\n",
      "Step: 5246, Loss: 0.9159094095230103, Accuracy: 1.0, Computation time: 1.183905839920044\n",
      "Step: 5247, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.4995875358581543\n",
      "Step: 5248, Loss: 0.9377158284187317, Accuracy: nan, Computation time: 1.6431806087493896\n",
      "Step: 5249, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.5612800121307373\n",
      "Step: 5250, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.194288730621338\n",
      "Step: 5251, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.8578879833221436\n",
      "Step: 5252, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.47877836227417\n",
      "Step: 5253, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.6818199157714844\n",
      "Step: 5254, Loss: 0.9375645518302917, Accuracy: 0.9772727489471436, Computation time: 1.5981154441833496\n",
      "Step: 5255, Loss: 0.9160439372062683, Accuracy: 1.0, Computation time: 1.3502123355865479\n",
      "Step: 5256, Loss: 0.9591718316078186, Accuracy: 0.9352940917015076, Computation time: 1.5542845726013184\n",
      "Step: 5257, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.4405899047851562\n",
      "Step: 5258, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.3829004764556885\n",
      "Step: 5259, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 1.3790290355682373\n",
      "Step: 5260, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.5558879375457764\n",
      "Step: 5261, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.4564783573150635\n",
      "Step: 5262, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.3643450736999512\n",
      "Step: 5263, Loss: 0.9374437928199768, Accuracy: 0.9833333492279053, Computation time: 1.1105403900146484\n",
      "Step: 5264, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.6011388301849365\n",
      "Step: 5265, Loss: 0.9374626874923706, Accuracy: 0.9642857313156128, Computation time: 1.5666923522949219\n",
      "Step: 5266, Loss: 0.9375591278076172, Accuracy: 0.9791666865348816, Computation time: 1.0193943977355957\n",
      "Step: 5267, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.2814314365386963\n",
      "Step: 5268, Loss: 0.9158512353897095, Accuracy: 1.0, Computation time: 1.5660791397094727\n",
      "Step: 5269, Loss: 0.9158374667167664, Accuracy: 1.0, Computation time: 1.6954843997955322\n",
      "Step: 5270, Loss: 0.9159960150718689, Accuracy: 1.0, Computation time: 1.4894280433654785\n",
      "Step: 5271, Loss: 0.9372904300689697, Accuracy: 0.9750000238418579, Computation time: 1.2937657833099365\n",
      "Step: 5272, Loss: 0.9375131726264954, Accuracy: 0.9807692766189575, Computation time: 1.3874034881591797\n",
      "Step: 5273, Loss: 0.9171872138977051, Accuracy: 1.0, Computation time: 1.466649055480957\n",
      "Step: 5274, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.8162956237792969\n",
      "Step: 5275, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 1.2738902568817139\n",
      "Step: 5276, Loss: 0.915840208530426, Accuracy: 1.0, Computation time: 1.5181620121002197\n",
      "Step: 5277, Loss: 0.9375576972961426, Accuracy: 0.9583333730697632, Computation time: 1.4587054252624512\n",
      "Step: 5278, Loss: 0.9375556707382202, Accuracy: 0.9583333730697632, Computation time: 1.2389721870422363\n",
      "Step: 5279, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.3617243766784668\n",
      "Step: 5280, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.135636806488037\n",
      "########################\n",
      "Test loss: 1.1186468601226807, Test Accuracy_epoch38: 0.7049360275268555\n",
      "########################\n",
      "Step: 5281, Loss: 0.9159090518951416, Accuracy: 1.0, Computation time: 1.5152101516723633\n",
      "Step: 5282, Loss: 0.9376100897789001, Accuracy: 0.9750000238418579, Computation time: 1.2828047275543213\n",
      "Step: 5283, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 1.264054536819458\n",
      "Step: 5284, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 1.3741557598114014\n",
      "Step: 5285, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 1.3522577285766602\n",
      "Step: 5286, Loss: 0.9364558458328247, Accuracy: 0.9821428656578064, Computation time: 1.955528974533081\n",
      "Step: 5287, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 1.6529507637023926\n",
      "Step: 5288, Loss: 0.9158520698547363, Accuracy: 1.0, Computation time: 1.4255852699279785\n",
      "Step: 5289, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 1.4907546043395996\n",
      "Step: 5290, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 1.462773323059082\n",
      "Step: 5291, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 1.3917748928070068\n",
      "Step: 5292, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 1.460036039352417\n",
      "Step: 5293, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 1.2417302131652832\n",
      "Step: 5294, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.3929059505462646\n",
      "Step: 5295, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 1.3072154521942139\n",
      "Step: 5296, Loss: 0.9158352613449097, Accuracy: 1.0, Computation time: 1.2857089042663574\n",
      "Step: 5297, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 1.4029295444488525\n",
      "Step: 5298, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.7886571884155273\n",
      "Step: 5299, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 1.3845455646514893\n",
      "Step: 5300, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 1.415893793106079\n",
      "Step: 5301, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.2131264209747314\n",
      "Step: 5302, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.5328037738800049\n",
      "Step: 5303, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.9330906867980957\n",
      "Step: 5304, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.4968914985656738\n",
      "Step: 5305, Loss: 0.9188370704650879, Accuracy: 1.0, Computation time: 1.623776912689209\n",
      "Step: 5306, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 1.2653298377990723\n",
      "Step: 5307, Loss: 0.9375095963478088, Accuracy: 0.9791666865348816, Computation time: 1.879497766494751\n",
      "Step: 5308, Loss: 0.9181718826293945, Accuracy: 1.0, Computation time: 1.9423210620880127\n",
      "Step: 5309, Loss: 0.9390333294868469, Accuracy: 0.9750000238418579, Computation time: 1.4941763877868652\n",
      "Step: 5310, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.3271665573120117\n",
      "Step: 5311, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 1.4288842678070068\n",
      "Step: 5312, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 1.21223783493042\n",
      "Step: 5313, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 1.3644843101501465\n",
      "Step: 5314, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.3256690502166748\n",
      "Step: 5315, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.3057677745819092\n",
      "Step: 5316, Loss: 0.9256867170333862, Accuracy: 0.9583333730697632, Computation time: 1.8394641876220703\n",
      "Step: 5317, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.2466096878051758\n",
      "Step: 5318, Loss: 0.9159384965896606, Accuracy: 1.0, Computation time: 1.468195915222168\n",
      "Step: 5319, Loss: 0.916012167930603, Accuracy: 1.0, Computation time: 1.152186632156372\n",
      "Step: 5320, Loss: 0.9169623851776123, Accuracy: 1.0, Computation time: 1.379593849182129\n",
      "Step: 5321, Loss: 0.9160643815994263, Accuracy: 1.0, Computation time: 1.4388680458068848\n",
      "Step: 5322, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 1.216458797454834\n",
      "Step: 5323, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 1.4065942764282227\n",
      "Step: 5324, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.319920301437378\n",
      "Step: 5325, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.3825469017028809\n",
      "Step: 5326, Loss: 0.9158539175987244, Accuracy: 1.0, Computation time: 1.3138551712036133\n",
      "Step: 5327, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.2908198833465576\n",
      "Step: 5328, Loss: 0.9238738417625427, Accuracy: 1.0, Computation time: 1.4504237174987793\n",
      "Step: 5329, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.290266752243042\n",
      "Step: 5330, Loss: 0.9158754348754883, Accuracy: 1.0, Computation time: 1.4121143817901611\n",
      "Step: 5331, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 1.0983290672302246\n",
      "Step: 5332, Loss: 0.9377278685569763, Accuracy: 0.9750000238418579, Computation time: 1.6483855247497559\n",
      "Step: 5333, Loss: 0.9159973859786987, Accuracy: 1.0, Computation time: 1.3218140602111816\n",
      "Step: 5334, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 0.8976755142211914\n",
      "Step: 5335, Loss: 0.9162026643753052, Accuracy: 1.0, Computation time: 1.5244247913360596\n",
      "Step: 5336, Loss: 0.9325090646743774, Accuracy: 0.9791666865348816, Computation time: 1.4618594646453857\n",
      "Step: 5337, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.3573787212371826\n",
      "Step: 5338, Loss: 0.9159079194068909, Accuracy: 1.0, Computation time: 0.9988744258880615\n",
      "Step: 5339, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.1361892223358154\n",
      "Step: 5340, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 1.22454833984375\n",
      "Step: 5341, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.4886796474456787\n",
      "Step: 5342, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.1452152729034424\n",
      "Step: 5343, Loss: 0.9982602000236511, Accuracy: 0.8939394354820251, Computation time: 2.1552388668060303\n",
      "Step: 5344, Loss: 0.9159107804298401, Accuracy: 1.0, Computation time: 1.3805387020111084\n",
      "Step: 5345, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 0.9595911502838135\n",
      "Step: 5346, Loss: 0.9159083962440491, Accuracy: 1.0, Computation time: 1.0379111766815186\n",
      "Step: 5347, Loss: 0.915923535823822, Accuracy: 1.0, Computation time: 1.199937343597412\n",
      "Step: 5348, Loss: 0.9162613749504089, Accuracy: 1.0, Computation time: 1.0349316596984863\n",
      "Step: 5349, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 0.9566609859466553\n",
      "Step: 5350, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 1.4365293979644775\n",
      "Step: 5351, Loss: 0.9159029126167297, Accuracy: 1.0, Computation time: 1.4960856437683105\n",
      "Step: 5352, Loss: 0.9165016412734985, Accuracy: 1.0, Computation time: 1.488922119140625\n",
      "Step: 5353, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.156935453414917\n",
      "Step: 5354, Loss: 0.9376479387283325, Accuracy: 0.9772727489471436, Computation time: 1.3928041458129883\n",
      "Step: 5355, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.3362290859222412\n",
      "Step: 5356, Loss: 0.9374321103096008, Accuracy: 0.9166666865348816, Computation time: 1.3260118961334229\n",
      "Step: 5357, Loss: 0.9160448908805847, Accuracy: 1.0, Computation time: 1.4332084655761719\n",
      "Step: 5358, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 1.5541110038757324\n",
      "Step: 5359, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.4304862022399902\n",
      "Step: 5360, Loss: 0.9159186482429504, Accuracy: 1.0, Computation time: 1.42010498046875\n",
      "Step: 5361, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.6472468376159668\n",
      "Step: 5362, Loss: 0.9158952236175537, Accuracy: 1.0, Computation time: 1.315152883529663\n",
      "Step: 5363, Loss: 0.9198923110961914, Accuracy: 1.0, Computation time: 1.9211695194244385\n",
      "Step: 5364, Loss: 0.915882408618927, Accuracy: 1.0, Computation time: 1.3743908405303955\n",
      "Step: 5365, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.1912450790405273\n",
      "Step: 5366, Loss: 0.9159435629844666, Accuracy: 1.0, Computation time: 1.2301263809204102\n",
      "Step: 5367, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 1.2146642208099365\n",
      "Step: 5368, Loss: 0.9376159906387329, Accuracy: 0.9583333730697632, Computation time: 1.4295542240142822\n",
      "Step: 5369, Loss: 0.9160408973693848, Accuracy: 1.0, Computation time: 1.2913639545440674\n",
      "Step: 5370, Loss: 0.9201323390007019, Accuracy: 1.0, Computation time: 1.712395429611206\n",
      "Step: 5371, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 1.4298832416534424\n",
      "Step: 5372, Loss: 0.9159326553344727, Accuracy: 1.0, Computation time: 1.2628004550933838\n",
      "Step: 5373, Loss: 0.9299214482307434, Accuracy: 0.9807692766189575, Computation time: 1.9015886783599854\n",
      "Step: 5374, Loss: 0.9160106182098389, Accuracy: 1.0, Computation time: 1.530238389968872\n",
      "Step: 5375, Loss: 0.9375350475311279, Accuracy: 0.9750000238418579, Computation time: 1.2807300090789795\n",
      "Step: 5376, Loss: 0.9377614855766296, Accuracy: 0.9722222089767456, Computation time: 1.3381502628326416\n",
      "Step: 5377, Loss: 0.9586109519004822, Accuracy: 0.9097222089767456, Computation time: 1.294001817703247\n",
      "Step: 5378, Loss: 0.9160110354423523, Accuracy: 1.0, Computation time: 1.391010046005249\n",
      "Step: 5379, Loss: 0.9159787893295288, Accuracy: 1.0, Computation time: 1.1746726036071777\n",
      "Step: 5380, Loss: 0.937577486038208, Accuracy: 0.96875, Computation time: 1.3915081024169922\n",
      "Step: 5381, Loss: 0.915917694568634, Accuracy: 1.0, Computation time: 1.16896390914917\n",
      "Step: 5382, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.2923693656921387\n",
      "Step: 5383, Loss: 0.915939450263977, Accuracy: 1.0, Computation time: 2.238741397857666\n",
      "Step: 5384, Loss: 0.9180704355239868, Accuracy: 1.0, Computation time: 1.7885074615478516\n",
      "Step: 5385, Loss: 0.9159291982650757, Accuracy: 1.0, Computation time: 1.1952483654022217\n",
      "Step: 5386, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 1.3403053283691406\n",
      "Step: 5387, Loss: 0.9375432729721069, Accuracy: 0.96875, Computation time: 1.3642196655273438\n",
      "Step: 5388, Loss: 0.9379279017448425, Accuracy: 0.9772727489471436, Computation time: 1.255357265472412\n",
      "Step: 5389, Loss: 0.9160974621772766, Accuracy: 1.0, Computation time: 1.1893737316131592\n",
      "Step: 5390, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.0865910053253174\n",
      "Step: 5391, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 1.393115520477295\n",
      "Step: 5392, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.0727014541625977\n",
      "Step: 5393, Loss: 0.91602623462677, Accuracy: 1.0, Computation time: 1.317690372467041\n",
      "Step: 5394, Loss: 0.9190908670425415, Accuracy: 1.0, Computation time: 1.424853801727295\n",
      "Step: 5395, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 1.2816481590270996\n",
      "Step: 5396, Loss: 0.9309004545211792, Accuracy: 0.9583333730697632, Computation time: 1.9227654933929443\n",
      "Step: 5397, Loss: 0.9164339303970337, Accuracy: 1.0, Computation time: 1.1673729419708252\n",
      "Step: 5398, Loss: 0.9159396290779114, Accuracy: 1.0, Computation time: 1.256474256515503\n",
      "Step: 5399, Loss: 0.9164080619812012, Accuracy: 1.0, Computation time: 1.1021122932434082\n",
      "Step: 5400, Loss: 0.9159818887710571, Accuracy: 1.0, Computation time: 1.1556322574615479\n",
      "Step: 5401, Loss: 0.9159641265869141, Accuracy: 1.0, Computation time: 1.1987807750701904\n",
      "Step: 5402, Loss: 0.9159876108169556, Accuracy: 1.0, Computation time: 0.9588606357574463\n",
      "Step: 5403, Loss: 0.9159747362136841, Accuracy: 1.0, Computation time: 1.100471019744873\n",
      "Step: 5404, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 1.577113389968872\n",
      "Step: 5405, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 1.0433058738708496\n",
      "Step: 5406, Loss: 0.9288087487220764, Accuracy: 0.9642857313156128, Computation time: 1.6935672760009766\n",
      "Step: 5407, Loss: 0.9378778338432312, Accuracy: 0.9722222089767456, Computation time: 1.0394439697265625\n",
      "Step: 5408, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 1.1601903438568115\n",
      "Step: 5409, Loss: 0.9159736633300781, Accuracy: 1.0, Computation time: 1.2612037658691406\n",
      "Step: 5410, Loss: 0.9362514019012451, Accuracy: 0.9772727489471436, Computation time: 1.193408727645874\n",
      "Step: 5411, Loss: 0.9171333312988281, Accuracy: 1.0, Computation time: 1.2585718631744385\n",
      "Step: 5412, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 1.1591434478759766\n",
      "Step: 5413, Loss: 0.9447158575057983, Accuracy: 0.9807692766189575, Computation time: 1.8354942798614502\n",
      "Step: 5414, Loss: 0.9159361720085144, Accuracy: 1.0, Computation time: 1.1577610969543457\n",
      "Step: 5415, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.2901275157928467\n",
      "Step: 5416, Loss: 0.9385875463485718, Accuracy: 0.9722222089767456, Computation time: 1.4764657020568848\n",
      "Step: 5417, Loss: 0.9235593676567078, Accuracy: 1.0, Computation time: 1.7235541343688965\n",
      "Step: 5418, Loss: 0.9160587191581726, Accuracy: 1.0, Computation time: 1.2402663230895996\n",
      "Step: 5419, Loss: 0.9161351323127747, Accuracy: 1.0, Computation time: 1.2934885025024414\n",
      "########################\n",
      "Test loss: 1.117801547050476, Test Accuracy_epoch39: 0.7053360939025879\n",
      "########################\n",
      "Step: 5420, Loss: 0.9160802364349365, Accuracy: 1.0, Computation time: 1.4020280838012695\n",
      "Step: 5421, Loss: 0.9160007238388062, Accuracy: 1.0, Computation time: 2.22268009185791\n",
      "Step: 5422, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.4329516887664795\n",
      "Step: 5423, Loss: 0.9377312660217285, Accuracy: 0.9750000238418579, Computation time: 1.646965742111206\n",
      "Step: 5424, Loss: 0.9327282309532166, Accuracy: 0.9807692766189575, Computation time: 1.280764102935791\n",
      "Step: 5425, Loss: 0.9162433743476868, Accuracy: 1.0, Computation time: 1.1904120445251465\n",
      "Step: 5426, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 1.5459034442901611\n",
      "Step: 5427, Loss: 0.9159530401229858, Accuracy: 1.0, Computation time: 1.4820351600646973\n",
      "Step: 5428, Loss: 0.9159850478172302, Accuracy: 1.0, Computation time: 1.201615333557129\n",
      "Step: 5429, Loss: 0.915985107421875, Accuracy: 1.0, Computation time: 1.3189465999603271\n",
      "Step: 5430, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 1.1691629886627197\n",
      "Step: 5431, Loss: 0.9592700600624084, Accuracy: 0.96875, Computation time: 1.462153673171997\n",
      "Step: 5432, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 1.3551228046417236\n",
      "Step: 5433, Loss: 0.9159932136535645, Accuracy: 1.0, Computation time: 1.4700899124145508\n",
      "Step: 5434, Loss: 0.9306370615959167, Accuracy: 0.9750000238418579, Computation time: 2.0027360916137695\n",
      "Step: 5435, Loss: 0.9159058332443237, Accuracy: 1.0, Computation time: 1.5476093292236328\n",
      "Step: 5436, Loss: 0.916004478931427, Accuracy: 1.0, Computation time: 1.3618128299713135\n",
      "Step: 5437, Loss: 0.9159502387046814, Accuracy: 1.0, Computation time: 1.2724902629852295\n",
      "Step: 5438, Loss: 0.916013777256012, Accuracy: 1.0, Computation time: 1.3254034519195557\n",
      "Step: 5439, Loss: 0.9160171151161194, Accuracy: 1.0, Computation time: 1.7122163772583008\n",
      "Step: 5440, Loss: 0.9378911256790161, Accuracy: 0.96875, Computation time: 1.457441806793213\n",
      "Step: 5441, Loss: 0.9379251003265381, Accuracy: 0.9791666865348816, Computation time: 1.5727624893188477\n",
      "Step: 5442, Loss: 0.9160550236701965, Accuracy: 1.0, Computation time: 1.4361941814422607\n",
      "Step: 5443, Loss: 0.9166882634162903, Accuracy: 1.0, Computation time: 1.497147560119629\n",
      "Step: 5444, Loss: 0.9160315990447998, Accuracy: 1.0, Computation time: 1.5802586078643799\n",
      "Step: 5445, Loss: 0.9590540528297424, Accuracy: 0.9444444179534912, Computation time: 1.3429937362670898\n",
      "Step: 5446, Loss: 0.9161269068717957, Accuracy: 1.0, Computation time: 1.8254635334014893\n",
      "Step: 5447, Loss: 0.9373853206634521, Accuracy: 0.96875, Computation time: 1.4332034587860107\n",
      "Step: 5448, Loss: 0.9379951357841492, Accuracy: 0.9807692766189575, Computation time: 2.846637725830078\n",
      "Step: 5449, Loss: 0.9596161246299744, Accuracy: 0.96875, Computation time: 1.6912298202514648\n",
      "Step: 5450, Loss: 0.916040301322937, Accuracy: 1.0, Computation time: 1.5257489681243896\n",
      "Step: 5451, Loss: 0.9166640043258667, Accuracy: 1.0, Computation time: 1.9186713695526123\n",
      "Step: 5452, Loss: 0.9377403259277344, Accuracy: 0.9722222089767456, Computation time: 1.6522421836853027\n",
      "Step: 5453, Loss: 0.9160202145576477, Accuracy: 1.0, Computation time: 1.2765405178070068\n",
      "Step: 5454, Loss: 0.9162424206733704, Accuracy: 1.0, Computation time: 1.6684062480926514\n",
      "Step: 5455, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.514124870300293\n",
      "Step: 5456, Loss: 0.915880024433136, Accuracy: 1.0, Computation time: 1.3923184871673584\n",
      "Step: 5457, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 1.5955889225006104\n",
      "Step: 5458, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.8969566822052002\n",
      "Step: 5459, Loss: 0.915952205657959, Accuracy: 1.0, Computation time: 1.695279836654663\n",
      "Step: 5460, Loss: 0.9161819815635681, Accuracy: 1.0, Computation time: 1.8510642051696777\n",
      "Step: 5461, Loss: 0.9159090518951416, Accuracy: nan, Computation time: 1.3904142379760742\n",
      "Step: 5462, Loss: 0.9163561463356018, Accuracy: 1.0, Computation time: 1.654067039489746\n",
      "Step: 5463, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 1.2494430541992188\n",
      "Step: 5464, Loss: 0.9159404635429382, Accuracy: 1.0, Computation time: 1.3786113262176514\n",
      "Step: 5465, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.8066940307617188\n",
      "Step: 5466, Loss: 0.916130781173706, Accuracy: 1.0, Computation time: 1.4777812957763672\n",
      "Step: 5467, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.5716121196746826\n",
      "Step: 5468, Loss: 0.9284987449645996, Accuracy: 0.9821428656578064, Computation time: 1.9569904804229736\n",
      "Step: 5469, Loss: 0.9159584045410156, Accuracy: 1.0, Computation time: 1.567892074584961\n",
      "Step: 5470, Loss: 0.9185460209846497, Accuracy: 1.0, Computation time: 1.855858325958252\n",
      "Step: 5471, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 1.0241725444793701\n",
      "Step: 5472, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.311460256576538\n",
      "Step: 5473, Loss: 0.9164647459983826, Accuracy: 1.0, Computation time: 1.6218695640563965\n",
      "Step: 5474, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 1.4706852436065674\n",
      "Step: 5475, Loss: 0.915908694267273, Accuracy: 1.0, Computation time: 1.6875455379486084\n",
      "Step: 5476, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.3936598300933838\n",
      "Step: 5477, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.47059965133667\n",
      "Step: 5478, Loss: 0.9165503978729248, Accuracy: 1.0, Computation time: 1.3526856899261475\n",
      "Step: 5479, Loss: 0.9160053730010986, Accuracy: 1.0, Computation time: 1.42154860496521\n",
      "Step: 5480, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.3126568794250488\n",
      "Step: 5481, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 1.3720557689666748\n",
      "Step: 5482, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 1.4774258136749268\n",
      "Step: 5483, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.3866195678710938\n",
      "Step: 5484, Loss: 0.9375451803207397, Accuracy: 0.9583333730697632, Computation time: 1.423090934753418\n",
      "Step: 5485, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.4842171669006348\n",
      "Step: 5486, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 1.111684799194336\n",
      "Step: 5487, Loss: 0.9366335868835449, Accuracy: 0.9722222089767456, Computation time: 1.3585641384124756\n",
      "Step: 5488, Loss: 0.938400149345398, Accuracy: 0.9791666865348816, Computation time: 1.1198070049285889\n",
      "Step: 5489, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.1049525737762451\n",
      "Step: 5490, Loss: 0.9162461757659912, Accuracy: 1.0, Computation time: 1.4410426616668701\n",
      "Step: 5491, Loss: 0.9159806370735168, Accuracy: 1.0, Computation time: 1.1977007389068604\n",
      "Step: 5492, Loss: 0.9159599542617798, Accuracy: 1.0, Computation time: 1.4234578609466553\n",
      "Step: 5493, Loss: 0.915925920009613, Accuracy: 1.0, Computation time: 1.1680212020874023\n",
      "Step: 5494, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.2153453826904297\n",
      "Step: 5495, Loss: 0.9374365210533142, Accuracy: 0.9642857313156128, Computation time: 1.106015682220459\n",
      "Step: 5496, Loss: 0.9160175323486328, Accuracy: 1.0, Computation time: 1.226970911026001\n",
      "Step: 5497, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 1.1456992626190186\n",
      "Step: 5498, Loss: 0.9200608134269714, Accuracy: 1.0, Computation time: 1.6432750225067139\n",
      "Step: 5499, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.2700576782226562\n",
      "Step: 5500, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.1056923866271973\n",
      "Step: 5501, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.2039098739624023\n",
      "Step: 5502, Loss: 0.9165442585945129, Accuracy: 1.0, Computation time: 1.0150854587554932\n",
      "Step: 5503, Loss: 0.9375861883163452, Accuracy: 0.9772727489471436, Computation time: 1.2200734615325928\n",
      "Step: 5504, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 1.0715289115905762\n",
      "Step: 5505, Loss: 0.9373831748962402, Accuracy: 0.9750000238418579, Computation time: 1.2625458240509033\n",
      "Step: 5506, Loss: 0.9159393906593323, Accuracy: 1.0, Computation time: 0.9508378505706787\n",
      "Step: 5507, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 1.09590744972229\n",
      "Step: 5508, Loss: 0.9177089929580688, Accuracy: 1.0, Computation time: 1.4088554382324219\n",
      "Step: 5509, Loss: 0.9159730076789856, Accuracy: 1.0, Computation time: 1.3467166423797607\n",
      "Step: 5510, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 0.9594159126281738\n",
      "Step: 5511, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 1.061856985092163\n",
      "Step: 5512, Loss: 0.9375240206718445, Accuracy: 0.9772727489471436, Computation time: 1.0633301734924316\n",
      "Step: 5513, Loss: 0.9175869226455688, Accuracy: 1.0, Computation time: 1.1015655994415283\n",
      "Step: 5514, Loss: 0.9159334301948547, Accuracy: 1.0, Computation time: 1.06782865524292\n",
      "Step: 5515, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.4448790550231934\n",
      "Step: 5516, Loss: 0.9376054406166077, Accuracy: 0.9583333730697632, Computation time: 1.3627088069915771\n",
      "Step: 5517, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.194692611694336\n",
      "Step: 5518, Loss: 0.9375750422477722, Accuracy: 0.9750000238418579, Computation time: 1.0706021785736084\n",
      "Step: 5519, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.0856726169586182\n",
      "Step: 5520, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 0.9813315868377686\n",
      "Step: 5521, Loss: 0.9315367341041565, Accuracy: 0.9583333730697632, Computation time: 1.1116855144500732\n",
      "Step: 5522, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.2964122295379639\n",
      "Step: 5523, Loss: 0.9158974885940552, Accuracy: 1.0, Computation time: 1.216113567352295\n",
      "Step: 5524, Loss: 0.915955662727356, Accuracy: 1.0, Computation time: 1.0888936519622803\n",
      "Step: 5525, Loss: 0.9159492254257202, Accuracy: nan, Computation time: 1.1453266143798828\n",
      "Step: 5526, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 1.2731812000274658\n",
      "Step: 5527, Loss: 0.9365101456642151, Accuracy: 0.9791666865348816, Computation time: 1.224731683731079\n",
      "Step: 5528, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.3465783596038818\n",
      "Step: 5529, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 1.670762062072754\n",
      "Step: 5530, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 1.0704872608184814\n",
      "Step: 5531, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.001020908355713\n",
      "Step: 5532, Loss: 0.916019082069397, Accuracy: 1.0, Computation time: 0.9701840877532959\n",
      "Step: 5533, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 1.178330659866333\n",
      "Step: 5534, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.0090978145599365\n",
      "Step: 5535, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 1.1282236576080322\n",
      "Step: 5536, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 0.9549806118011475\n",
      "Step: 5537, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 0.9648802280426025\n",
      "Step: 5538, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 1.439805269241333\n",
      "Step: 5539, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 1.2134099006652832\n",
      "Step: 5540, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.008653163909912\n",
      "Step: 5541, Loss: 0.915962278842926, Accuracy: 1.0, Computation time: 1.100987195968628\n",
      "Step: 5542, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.227802038192749\n",
      "Step: 5543, Loss: 0.915859580039978, Accuracy: 1.0, Computation time: 1.270493984222412\n",
      "Step: 5544, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 1.0356075763702393\n",
      "Step: 5545, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.5910518169403076\n",
      "Step: 5546, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 1.1187851428985596\n",
      "Step: 5547, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.0635600090026855\n",
      "Step: 5548, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 0.9498088359832764\n",
      "Step: 5549, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.0665309429168701\n",
      "Step: 5550, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.1050784587860107\n",
      "Step: 5551, Loss: 0.9158390164375305, Accuracy: 1.0, Computation time: 1.2247183322906494\n",
      "Step: 5552, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 0.9806444644927979\n",
      "Step: 5553, Loss: 0.9158434867858887, Accuracy: 1.0, Computation time: 1.2172760963439941\n",
      "Step: 5554, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 1.2698516845703125\n",
      "Step: 5555, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 1.2653279304504395\n",
      "Step: 5556, Loss: 0.9210176467895508, Accuracy: 1.0, Computation time: 1.1294019222259521\n",
      "Step: 5557, Loss: 0.937563419342041, Accuracy: 0.96875, Computation time: 1.0638856887817383\n",
      "Step: 5558, Loss: 0.9321045875549316, Accuracy: 0.9772727489471436, Computation time: 1.1900734901428223\n",
      "########################\n",
      "Test loss: 1.1198430061340332, Test Accuracy_epoch40: 0.7026102542877197\n",
      "########################\n",
      "Step: 5559, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 1.1563482284545898\n",
      "Step: 5560, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.2872653007507324\n",
      "Step: 5561, Loss: 0.9399540424346924, Accuracy: 0.9750000238418579, Computation time: 1.1563475131988525\n",
      "Step: 5562, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 0.9305436611175537\n",
      "Step: 5563, Loss: 0.9376482963562012, Accuracy: 0.9821428656578064, Computation time: 1.2126362323760986\n",
      "Step: 5564, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 1.145172357559204\n",
      "Step: 5565, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.2114012241363525\n",
      "Step: 5566, Loss: 0.9374939203262329, Accuracy: 0.9722222089767456, Computation time: 0.9777212142944336\n",
      "Step: 5567, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.0990335941314697\n",
      "Step: 5568, Loss: 0.9521589279174805, Accuracy: 0.9437500238418579, Computation time: 1.6174020767211914\n",
      "Step: 5569, Loss: 0.9159228205680847, Accuracy: 1.0, Computation time: 1.0011076927185059\n",
      "Step: 5570, Loss: 0.9160120487213135, Accuracy: 1.0, Computation time: 0.9916737079620361\n",
      "Step: 5571, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 1.4609730243682861\n",
      "Step: 5572, Loss: 0.9376800060272217, Accuracy: 0.9375, Computation time: 1.1593966484069824\n",
      "Step: 5573, Loss: 0.9375935792922974, Accuracy: 0.9750000238418579, Computation time: 1.1638894081115723\n",
      "Step: 5574, Loss: 0.9162644147872925, Accuracy: 1.0, Computation time: 1.55743408203125\n",
      "Step: 5575, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.0920202732086182\n",
      "Step: 5576, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 1.0770044326782227\n",
      "Step: 5577, Loss: 0.9158787131309509, Accuracy: 1.0, Computation time: 1.635519027709961\n",
      "Step: 5578, Loss: 0.9375765919685364, Accuracy: 0.9642857313156128, Computation time: 1.2850162982940674\n",
      "Step: 5579, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 1.165776252746582\n",
      "Step: 5580, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.4846115112304688\n",
      "Step: 5581, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 1.1303884983062744\n",
      "Step: 5582, Loss: 0.9371902346611023, Accuracy: 0.9722222089767456, Computation time: 1.2564916610717773\n",
      "Step: 5583, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 1.0731675624847412\n",
      "Step: 5584, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 1.3436636924743652\n",
      "Step: 5585, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 0.9459645748138428\n",
      "Step: 5586, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.241215705871582\n",
      "Step: 5587, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 1.397758960723877\n",
      "Step: 5588, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 1.1690089702606201\n",
      "Step: 5589, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.1980023384094238\n",
      "Step: 5590, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 1.2837772369384766\n",
      "Step: 5591, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.4884033203125\n",
      "Step: 5592, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 1.3199975490570068\n",
      "Step: 5593, Loss: 0.9183652400970459, Accuracy: 1.0, Computation time: 2.093641757965088\n",
      "Step: 5594, Loss: 0.9375713467597961, Accuracy: 0.9642857313156128, Computation time: 1.1140553951263428\n",
      "Step: 5595, Loss: 0.9158397316932678, Accuracy: 1.0, Computation time: 1.1686153411865234\n",
      "Step: 5596, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.1285748481750488\n",
      "Step: 5597, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 1.1582558155059814\n",
      "Step: 5598, Loss: 0.9161612391471863, Accuracy: 1.0, Computation time: 1.2771456241607666\n",
      "Step: 5599, Loss: 0.9158454537391663, Accuracy: 1.0, Computation time: 0.979478120803833\n",
      "Step: 5600, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.0465211868286133\n",
      "Step: 5601, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 1.0875895023345947\n",
      "Step: 5602, Loss: 0.9375265836715698, Accuracy: 0.9791666865348816, Computation time: 1.123304843902588\n",
      "Step: 5603, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.0243613719940186\n",
      "Step: 5604, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.3045876026153564\n",
      "Step: 5605, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.1484456062316895\n",
      "Step: 5606, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.1555042266845703\n",
      "Step: 5607, Loss: 0.9375771880149841, Accuracy: 0.9642857313156128, Computation time: 1.3906605243682861\n",
      "Step: 5608, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.9313399791717529\n",
      "Step: 5609, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 0.9889426231384277\n",
      "Step: 5610, Loss: 0.9216515421867371, Accuracy: 1.0, Computation time: 1.30464768409729\n",
      "Step: 5611, Loss: 0.9172536134719849, Accuracy: 1.0, Computation time: 1.1156103610992432\n",
      "Step: 5612, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.1102802753448486\n",
      "Step: 5613, Loss: 0.9539496302604675, Accuracy: 0.9444444179534912, Computation time: 1.3212862014770508\n",
      "Step: 5614, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.2297968864440918\n",
      "Step: 5615, Loss: 0.9159146547317505, Accuracy: 1.0, Computation time: 1.19356107711792\n",
      "Step: 5616, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 0.9729878902435303\n",
      "Step: 5617, Loss: 0.9194252490997314, Accuracy: 1.0, Computation time: 1.3448667526245117\n",
      "Step: 5618, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 1.0291943550109863\n",
      "Step: 5619, Loss: 0.9236183166503906, Accuracy: 1.0, Computation time: 1.165555715560913\n",
      "Step: 5620, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.4731192588806152\n",
      "Step: 5621, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 1.2893686294555664\n",
      "Step: 5622, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.0717730522155762\n",
      "Step: 5623, Loss: 0.9377008676528931, Accuracy: 0.9642857313156128, Computation time: 1.190072774887085\n",
      "Step: 5624, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.3553173542022705\n",
      "Step: 5625, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.167358160018921\n",
      "Step: 5626, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 1.2742819786071777\n",
      "Step: 5627, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.1674494743347168\n",
      "Step: 5628, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 1.2569875717163086\n",
      "Step: 5629, Loss: 0.9158639907836914, Accuracy: 1.0, Computation time: 1.6141884326934814\n",
      "Step: 5630, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.1239583492279053\n",
      "Step: 5631, Loss: 0.9375990629196167, Accuracy: 0.9821428656578064, Computation time: 1.0235271453857422\n",
      "Step: 5632, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.2131102085113525\n",
      "Step: 5633, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 1.2555139064788818\n",
      "Step: 5634, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.342050552368164\n",
      "Step: 5635, Loss: 0.9376330971717834, Accuracy: 0.9750000238418579, Computation time: 1.3386123180389404\n",
      "Step: 5636, Loss: 0.9164779782295227, Accuracy: 1.0, Computation time: 1.2801845073699951\n",
      "Step: 5637, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 1.0702919960021973\n",
      "Step: 5638, Loss: 0.9162188172340393, Accuracy: 1.0, Computation time: 1.346303939819336\n",
      "Step: 5639, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.1292598247528076\n",
      "Step: 5640, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.1685025691986084\n",
      "Step: 5641, Loss: 0.9320765137672424, Accuracy: 0.96875, Computation time: 1.862713098526001\n",
      "Step: 5642, Loss: 0.9159582853317261, Accuracy: 1.0, Computation time: 1.1021318435668945\n",
      "Step: 5643, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 1.0513231754302979\n",
      "Step: 5644, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.2373313903808594\n",
      "Step: 5645, Loss: 0.915965735912323, Accuracy: 1.0, Computation time: 1.5185375213623047\n",
      "Step: 5646, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 1.4226062297821045\n",
      "Step: 5647, Loss: 0.9377020001411438, Accuracy: 0.9750000238418579, Computation time: 1.2483253479003906\n",
      "Step: 5648, Loss: 0.9376134276390076, Accuracy: 0.949999988079071, Computation time: 1.0321707725524902\n",
      "Step: 5649, Loss: 0.9167381525039673, Accuracy: 1.0, Computation time: 1.2688329219818115\n",
      "Step: 5650, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.3161065578460693\n",
      "Step: 5651, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.1714556217193604\n",
      "Step: 5652, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 0.96793532371521\n",
      "Step: 5653, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.343001365661621\n",
      "Step: 5654, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 1.3847885131835938\n",
      "Step: 5655, Loss: 0.9376210570335388, Accuracy: 0.9583333730697632, Computation time: 1.515411138534546\n",
      "Step: 5656, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.12571382522583\n",
      "Step: 5657, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.0248034000396729\n",
      "Step: 5658, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.1395542621612549\n",
      "Step: 5659, Loss: 0.9167752265930176, Accuracy: 1.0, Computation time: 1.281102180480957\n",
      "Step: 5660, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.0765411853790283\n",
      "Step: 5661, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.276355266571045\n",
      "Step: 5662, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 1.071969985961914\n",
      "Step: 5663, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.0279674530029297\n",
      "Step: 5664, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 1.4235143661499023\n",
      "Step: 5665, Loss: 0.932769238948822, Accuracy: 0.9583333730697632, Computation time: 0.891066312789917\n",
      "Step: 5666, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.0183601379394531\n",
      "Step: 5667, Loss: 0.9297677874565125, Accuracy: 0.96875, Computation time: 1.7844481468200684\n",
      "Step: 5668, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 1.011467695236206\n",
      "Step: 5669, Loss: 0.9160754084587097, Accuracy: 1.0, Computation time: 1.4243288040161133\n",
      "Step: 5670, Loss: 0.9159879684448242, Accuracy: 1.0, Computation time: 1.5718505382537842\n",
      "Step: 5671, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 0.9373905658721924\n",
      "Step: 5672, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 1.285874843597412\n",
      "Step: 5673, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 1.0806386470794678\n",
      "Step: 5674, Loss: 0.9375577569007874, Accuracy: 0.9750000238418579, Computation time: 1.1394152641296387\n",
      "Step: 5675, Loss: 0.9373513460159302, Accuracy: 0.9583333730697632, Computation time: 1.4383313655853271\n",
      "Step: 5676, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 1.0835609436035156\n",
      "Step: 5677, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 1.4007351398468018\n",
      "Step: 5678, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 0.9171140193939209\n",
      "Step: 5679, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 1.0504801273345947\n",
      "Step: 5680, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 0.9196972846984863\n",
      "Step: 5681, Loss: 0.9374327063560486, Accuracy: 0.984375, Computation time: 1.2303593158721924\n",
      "Step: 5682, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 1.0568835735321045\n",
      "Step: 5683, Loss: 0.9159066677093506, Accuracy: 1.0, Computation time: 1.2655434608459473\n",
      "Step: 5684, Loss: 0.9593114852905273, Accuracy: 0.9285714626312256, Computation time: 1.206146478652954\n",
      "Step: 5685, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.170262336730957\n",
      "Step: 5686, Loss: 0.9159961938858032, Accuracy: 1.0, Computation time: 1.5328073501586914\n",
      "Step: 5687, Loss: 0.9158676862716675, Accuracy: 1.0, Computation time: 0.9771692752838135\n",
      "Step: 5688, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.050351858139038\n",
      "Step: 5689, Loss: 0.937764585018158, Accuracy: 0.9642857313156128, Computation time: 1.1875309944152832\n",
      "Step: 5690, Loss: 0.9158737063407898, Accuracy: 1.0, Computation time: 1.6418156623840332\n",
      "Step: 5691, Loss: 0.9376915693283081, Accuracy: 0.9833333492279053, Computation time: 1.5230929851531982\n",
      "Step: 5692, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 0.8938980102539062\n",
      "Step: 5693, Loss: 0.9163590669631958, Accuracy: 1.0, Computation time: 1.9837391376495361\n",
      "Step: 5694, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 0.9785270690917969\n",
      "Step: 5695, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 1.367363452911377\n",
      "Step: 5696, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 1.0277960300445557\n",
      "Step: 5697, Loss: 0.9159124493598938, Accuracy: 1.0, Computation time: 1.443638563156128\n",
      "########################\n",
      "Test loss: 1.1200883388519287, Test Accuracy_epoch41: 0.6995314359664917\n",
      "########################\n",
      "Step: 5698, Loss: 0.9367687702178955, Accuracy: 0.9750000238418579, Computation time: 1.3104057312011719\n",
      "Step: 5699, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.1684589385986328\n",
      "Step: 5700, Loss: 0.9180325269699097, Accuracy: 1.0, Computation time: 1.5574169158935547\n",
      "Step: 5701, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 0.9907855987548828\n",
      "Step: 5702, Loss: 0.9158961176872253, Accuracy: 1.0, Computation time: 0.991072416305542\n",
      "Step: 5703, Loss: 0.9190236926078796, Accuracy: 1.0, Computation time: 1.151369333267212\n",
      "Step: 5704, Loss: 0.9160359501838684, Accuracy: 1.0, Computation time: 1.1529624462127686\n",
      "Step: 5705, Loss: 0.9159442782402039, Accuracy: 1.0, Computation time: 1.452235460281372\n",
      "Step: 5706, Loss: 0.9159865379333496, Accuracy: 1.0, Computation time: 2.0664827823638916\n",
      "Step: 5707, Loss: 0.9376252889633179, Accuracy: 0.9833333492279053, Computation time: 1.171351432800293\n",
      "Step: 5708, Loss: 0.9159773588180542, Accuracy: 1.0, Computation time: 1.4005351066589355\n",
      "Step: 5709, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.5730922222137451\n",
      "Step: 5710, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.4956402778625488\n",
      "Step: 5711, Loss: 0.9159255027770996, Accuracy: 1.0, Computation time: 1.2012908458709717\n",
      "Step: 5712, Loss: 0.9160762429237366, Accuracy: 1.0, Computation time: 1.1908602714538574\n",
      "Step: 5713, Loss: 0.9160721898078918, Accuracy: 1.0, Computation time: 1.2343957424163818\n",
      "Step: 5714, Loss: 0.9159119129180908, Accuracy: 1.0, Computation time: 1.2298128604888916\n",
      "Step: 5715, Loss: 0.9376233220100403, Accuracy: 0.9821428656578064, Computation time: 1.3656086921691895\n",
      "Step: 5716, Loss: 0.9158511757850647, Accuracy: nan, Computation time: 1.317223072052002\n",
      "Step: 5717, Loss: 0.9368411302566528, Accuracy: 0.9807692766189575, Computation time: 1.635054588317871\n",
      "Step: 5718, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.2085163593292236\n",
      "Step: 5719, Loss: 0.915886640548706, Accuracy: 1.0, Computation time: 1.1227424144744873\n",
      "Step: 5720, Loss: 0.9161211252212524, Accuracy: 1.0, Computation time: 1.1731078624725342\n",
      "Step: 5721, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.0209410190582275\n",
      "Step: 5722, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 1.098834753036499\n",
      "Step: 5723, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 1.2060346603393555\n",
      "Step: 5724, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 1.1239287853240967\n",
      "Step: 5725, Loss: 0.9375263452529907, Accuracy: 0.96875, Computation time: 1.2748377323150635\n",
      "Step: 5726, Loss: 0.9158664345741272, Accuracy: 1.0, Computation time: 1.1261863708496094\n",
      "Step: 5727, Loss: 0.9161193370819092, Accuracy: 1.0, Computation time: 2.0814733505249023\n",
      "Step: 5728, Loss: 0.9162154197692871, Accuracy: 1.0, Computation time: 1.343315601348877\n",
      "Step: 5729, Loss: 0.9221007227897644, Accuracy: 1.0, Computation time: 1.5651555061340332\n",
      "Step: 5730, Loss: 0.9159576296806335, Accuracy: 1.0, Computation time: 1.791212558746338\n",
      "Step: 5731, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 1.546215534210205\n",
      "Step: 5732, Loss: 0.9161432385444641, Accuracy: 1.0, Computation time: 1.658750295639038\n",
      "Step: 5733, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 1.4509236812591553\n",
      "Step: 5734, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.5542726516723633\n",
      "Step: 5735, Loss: 0.9372538328170776, Accuracy: 0.9807692766189575, Computation time: 1.232609510421753\n",
      "Step: 5736, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.5152807235717773\n",
      "Step: 5737, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 1.528038501739502\n",
      "Step: 5738, Loss: 0.916004478931427, Accuracy: 1.0, Computation time: 1.9150915145874023\n",
      "Step: 5739, Loss: 0.91733318567276, Accuracy: 1.0, Computation time: 2.5360143184661865\n",
      "Step: 5740, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.5506985187530518\n",
      "Step: 5741, Loss: 0.9166210293769836, Accuracy: 1.0, Computation time: 2.02765154838562\n",
      "Step: 5742, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.7681782245635986\n",
      "Step: 5743, Loss: 0.9377846121788025, Accuracy: 0.9772727489471436, Computation time: 2.0824391841888428\n",
      "Step: 5744, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 1.4326748847961426\n",
      "Step: 5745, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 1.6337261199951172\n",
      "Step: 5746, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 2.1378352642059326\n",
      "Step: 5747, Loss: 0.9160894155502319, Accuracy: 1.0, Computation time: 2.148857355117798\n",
      "Step: 5748, Loss: 0.916059136390686, Accuracy: 1.0, Computation time: 1.775496006011963\n",
      "Step: 5749, Loss: 0.9162439703941345, Accuracy: 1.0, Computation time: 2.3213064670562744\n",
      "Step: 5750, Loss: 0.9164043068885803, Accuracy: 1.0, Computation time: 1.8005385398864746\n",
      "Step: 5751, Loss: 0.9167969822883606, Accuracy: 1.0, Computation time: 1.720008134841919\n",
      "Step: 5752, Loss: 0.9162756204605103, Accuracy: 1.0, Computation time: 2.2801177501678467\n",
      "Step: 5753, Loss: 0.9160088896751404, Accuracy: 1.0, Computation time: 1.625023603439331\n",
      "Step: 5754, Loss: 0.9170436859130859, Accuracy: 1.0, Computation time: 1.5528476238250732\n",
      "Step: 5755, Loss: 0.9159735441207886, Accuracy: 1.0, Computation time: 2.7182607650756836\n",
      "Step: 5756, Loss: 0.9374535083770752, Accuracy: 0.9722222089767456, Computation time: 2.277329921722412\n",
      "Step: 5757, Loss: 0.9160348773002625, Accuracy: 1.0, Computation time: 2.1305150985717773\n",
      "Step: 5758, Loss: 0.938133955001831, Accuracy: 0.9750000238418579, Computation time: 2.100318193435669\n",
      "Step: 5759, Loss: 0.9163156747817993, Accuracy: 1.0, Computation time: 1.745051622390747\n",
      "Step: 5760, Loss: 0.9174493551254272, Accuracy: 1.0, Computation time: 2.2376973628997803\n",
      "Step: 5761, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.5467302799224854\n",
      "Step: 5762, Loss: 0.9159408211708069, Accuracy: 1.0, Computation time: 1.81968092918396\n",
      "Step: 5763, Loss: 0.9160268306732178, Accuracy: 1.0, Computation time: 1.8785033226013184\n",
      "Step: 5764, Loss: 0.9160520434379578, Accuracy: 1.0, Computation time: 1.7116632461547852\n",
      "Step: 5765, Loss: 0.916102409362793, Accuracy: 1.0, Computation time: 2.4170801639556885\n",
      "Step: 5766, Loss: 0.9164343476295471, Accuracy: 1.0, Computation time: 2.0476326942443848\n",
      "Step: 5767, Loss: 0.9162099957466125, Accuracy: 1.0, Computation time: 2.369722604751587\n",
      "Step: 5768, Loss: 0.9317284822463989, Accuracy: 0.9642857313156128, Computation time: 3.348451852798462\n",
      "Step: 5769, Loss: 0.9159244894981384, Accuracy: 1.0, Computation time: 1.3524580001831055\n",
      "Step: 5770, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.1990482807159424\n",
      "Step: 5771, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 1.2309470176696777\n",
      "Step: 5772, Loss: 0.9159610271453857, Accuracy: 1.0, Computation time: 1.0064787864685059\n",
      "Step: 5773, Loss: 0.9159756898880005, Accuracy: 1.0, Computation time: 1.1696789264678955\n",
      "Step: 5774, Loss: 0.915972888469696, Accuracy: 1.0, Computation time: 1.068871259689331\n",
      "Step: 5775, Loss: 0.9160268306732178, Accuracy: 1.0, Computation time: 1.1178154945373535\n",
      "Step: 5776, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 1.1004033088684082\n",
      "Step: 5777, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 0.9669938087463379\n",
      "Step: 5778, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 1.1386802196502686\n",
      "Step: 5779, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 0.8697707653045654\n",
      "Step: 5780, Loss: 0.9319368600845337, Accuracy: 0.9750000238418579, Computation time: 0.9841151237487793\n",
      "Step: 5781, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 0.8149101734161377\n",
      "Step: 5782, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 0.8895690441131592\n",
      "Step: 5783, Loss: 0.9160409569740295, Accuracy: 1.0, Computation time: 1.874560832977295\n",
      "Step: 5784, Loss: 0.9159756898880005, Accuracy: 1.0, Computation time: 1.0382096767425537\n",
      "Step: 5785, Loss: 0.9160340428352356, Accuracy: 1.0, Computation time: 1.4509048461914062\n",
      "Step: 5786, Loss: 0.9159563779830933, Accuracy: 1.0, Computation time: 0.936474084854126\n",
      "Step: 5787, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 0.9408760070800781\n",
      "Step: 5788, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 0.902097225189209\n",
      "Step: 5789, Loss: 0.9158670902252197, Accuracy: 1.0, Computation time: 0.8702127933502197\n",
      "Step: 5790, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.1835923194885254\n",
      "Step: 5791, Loss: 0.9161293506622314, Accuracy: 1.0, Computation time: 1.0161681175231934\n",
      "Step: 5792, Loss: 0.9159638285636902, Accuracy: 1.0, Computation time: 1.000950813293457\n",
      "Step: 5793, Loss: 0.9159343838691711, Accuracy: 1.0, Computation time: 0.9036550521850586\n",
      "Step: 5794, Loss: 0.9173498153686523, Accuracy: 1.0, Computation time: 1.3604040145874023\n",
      "Step: 5795, Loss: 0.9373829364776611, Accuracy: 0.9583333730697632, Computation time: 1.2538502216339111\n",
      "Step: 5796, Loss: 0.9375813007354736, Accuracy: 0.949999988079071, Computation time: 0.964759111404419\n",
      "Step: 5797, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 0.883213996887207\n",
      "Step: 5798, Loss: 0.937606692314148, Accuracy: 0.9375, Computation time: 0.885019063949585\n",
      "Step: 5799, Loss: 0.9377829432487488, Accuracy: 0.96875, Computation time: 0.993873119354248\n",
      "Step: 5800, Loss: 0.9160309433937073, Accuracy: 1.0, Computation time: 1.5313990116119385\n",
      "Step: 5801, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 0.8881487846374512\n",
      "Step: 5802, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 0.9986789226531982\n",
      "Step: 5803, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.8334758281707764\n",
      "Step: 5804, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 0.8822836875915527\n",
      "Step: 5805, Loss: 0.9375301599502563, Accuracy: 0.9642857313156128, Computation time: 0.8666424751281738\n",
      "Step: 5806, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 0.9248743057250977\n",
      "Step: 5807, Loss: 0.9173414707183838, Accuracy: 1.0, Computation time: 1.2335774898529053\n",
      "Step: 5808, Loss: 0.9297534823417664, Accuracy: 0.96875, Computation time: 2.347080707550049\n",
      "Step: 5809, Loss: 0.9375723004341125, Accuracy: 0.9583333730697632, Computation time: 0.8574352264404297\n",
      "Step: 5810, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 0.8289432525634766\n",
      "Step: 5811, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 0.9772424697875977\n",
      "Step: 5812, Loss: 0.9159727692604065, Accuracy: 1.0, Computation time: 0.8600544929504395\n",
      "Step: 5813, Loss: 0.916615903377533, Accuracy: 1.0, Computation time: 1.105593204498291\n",
      "Step: 5814, Loss: 0.9189227223396301, Accuracy: 1.0, Computation time: 0.8900935649871826\n",
      "Step: 5815, Loss: 0.9377073049545288, Accuracy: 0.949999988079071, Computation time: 0.8943619728088379\n",
      "Step: 5816, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 0.8524887561798096\n",
      "Step: 5817, Loss: 0.9347810745239258, Accuracy: 0.9722222089767456, Computation time: 1.9646127223968506\n",
      "Step: 5818, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 0.9243032932281494\n",
      "Step: 5819, Loss: 0.9159742593765259, Accuracy: 1.0, Computation time: 1.0012547969818115\n",
      "Step: 5820, Loss: 0.9160013794898987, Accuracy: 1.0, Computation time: 0.9652307033538818\n",
      "Step: 5821, Loss: 0.9160186052322388, Accuracy: 1.0, Computation time: 0.8211667537689209\n",
      "Step: 5822, Loss: 0.9402961730957031, Accuracy: 0.9791666865348816, Computation time: 1.1295011043548584\n",
      "Step: 5823, Loss: 0.916002631187439, Accuracy: 1.0, Computation time: 1.0356438159942627\n",
      "Step: 5824, Loss: 0.9159669280052185, Accuracy: 1.0, Computation time: 0.8563790321350098\n",
      "Step: 5825, Loss: 0.9159829020500183, Accuracy: 1.0, Computation time: 1.0282843112945557\n",
      "Step: 5826, Loss: 0.9159952998161316, Accuracy: 1.0, Computation time: 0.8481254577636719\n",
      "Step: 5827, Loss: 0.9219656586647034, Accuracy: 1.0, Computation time: 1.4322469234466553\n",
      "Step: 5828, Loss: 0.9160208106040955, Accuracy: 1.0, Computation time: 1.1117393970489502\n",
      "Step: 5829, Loss: 0.9367743730545044, Accuracy: 0.9821428656578064, Computation time: 1.1948390007019043\n",
      "Step: 5830, Loss: 0.9160448908805847, Accuracy: 1.0, Computation time: 1.4714550971984863\n",
      "Step: 5831, Loss: 0.9161631464958191, Accuracy: 1.0, Computation time: 0.9527409076690674\n",
      "Step: 5832, Loss: 0.915992021560669, Accuracy: 1.0, Computation time: 0.9070932865142822\n",
      "Step: 5833, Loss: 0.9161111116409302, Accuracy: 1.0, Computation time: 0.933696985244751\n",
      "Step: 5834, Loss: 0.9192867279052734, Accuracy: 1.0, Computation time: 1.2685344219207764\n",
      "Step: 5835, Loss: 0.9456548690795898, Accuracy: 0.9722222089767456, Computation time: 1.003481149673462\n",
      "Step: 5836, Loss: 0.9159613847732544, Accuracy: 1.0, Computation time: 0.928739070892334\n",
      "########################\n",
      "Test loss: 1.1302951574325562, Test Accuracy_epoch42: 0.6875608563423157\n",
      "########################\n",
      "Step: 5837, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 0.8943207263946533\n",
      "Step: 5838, Loss: 0.9377374649047852, Accuracy: 0.984375, Computation time: 1.0279219150543213\n",
      "Step: 5839, Loss: 0.9545494318008423, Accuracy: 0.9437500238418579, Computation time: 1.3741402626037598\n",
      "Step: 5840, Loss: 0.9161614775657654, Accuracy: 1.0, Computation time: 1.067943811416626\n",
      "Step: 5841, Loss: 0.9160417914390564, Accuracy: 1.0, Computation time: 0.9597527980804443\n",
      "Step: 5842, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 0.9034740924835205\n",
      "Step: 5843, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 0.9740469455718994\n",
      "Step: 5844, Loss: 0.9377523064613342, Accuracy: 0.9772727489471436, Computation time: 1.0222620964050293\n",
      "Step: 5845, Loss: 0.9159485697746277, Accuracy: 1.0, Computation time: 1.0035083293914795\n",
      "Step: 5846, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.0165455341339111\n",
      "Step: 5847, Loss: 0.9379667639732361, Accuracy: 0.9642857313156128, Computation time: 1.7811172008514404\n",
      "Step: 5848, Loss: 0.9159948229789734, Accuracy: 1.0, Computation time: 1.126835823059082\n",
      "Step: 5849, Loss: 0.9372439384460449, Accuracy: 0.949999988079071, Computation time: 1.7553198337554932\n",
      "Step: 5850, Loss: 0.9198805093765259, Accuracy: 1.0, Computation time: 1.8504095077514648\n",
      "Step: 5851, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 0.9676997661590576\n",
      "Step: 5852, Loss: 0.9159639477729797, Accuracy: 1.0, Computation time: 0.9902007579803467\n",
      "Step: 5853, Loss: 0.9160324335098267, Accuracy: 1.0, Computation time: 1.1705386638641357\n",
      "Step: 5854, Loss: 0.9171125292778015, Accuracy: 1.0, Computation time: 1.9901623725891113\n",
      "Step: 5855, Loss: 0.9296280741691589, Accuracy: 0.9772727489471436, Computation time: 1.650763750076294\n",
      "Step: 5856, Loss: 0.9160751700401306, Accuracy: 1.0, Computation time: 1.2452683448791504\n",
      "Step: 5857, Loss: 0.9374241232872009, Accuracy: 0.9642857313156128, Computation time: 1.5951616764068604\n",
      "Step: 5858, Loss: 0.9162980318069458, Accuracy: 1.0, Computation time: 1.1240036487579346\n",
      "Step: 5859, Loss: 0.9162225127220154, Accuracy: 1.0, Computation time: 0.9882266521453857\n",
      "Step: 5860, Loss: 0.9161434173583984, Accuracy: 1.0, Computation time: 1.2158880233764648\n",
      "Step: 5861, Loss: 0.9585545063018799, Accuracy: 0.9545454978942871, Computation time: 1.528254508972168\n",
      "Step: 5862, Loss: 0.9160003662109375, Accuracy: 1.0, Computation time: 1.220045566558838\n",
      "Step: 5863, Loss: 0.9160370826721191, Accuracy: 1.0, Computation time: 1.1629667282104492\n",
      "Step: 5864, Loss: 0.9160306453704834, Accuracy: 1.0, Computation time: 1.139307975769043\n",
      "Step: 5865, Loss: 0.9160531759262085, Accuracy: 1.0, Computation time: 1.5478510856628418\n",
      "Step: 5866, Loss: 0.9162244200706482, Accuracy: 1.0, Computation time: 1.548499345779419\n",
      "Step: 5867, Loss: 0.9161471128463745, Accuracy: 1.0, Computation time: 1.3992972373962402\n",
      "Step: 5868, Loss: 0.916103184223175, Accuracy: 1.0, Computation time: 1.211721658706665\n",
      "Step: 5869, Loss: 0.9353418946266174, Accuracy: 0.96875, Computation time: 1.3228001594543457\n",
      "Step: 5870, Loss: 0.9159568548202515, Accuracy: 1.0, Computation time: 1.3724803924560547\n",
      "Step: 5871, Loss: 0.9160115718841553, Accuracy: 1.0, Computation time: 1.1764945983886719\n",
      "Step: 5872, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.0715322494506836\n",
      "Step: 5873, Loss: 0.9159294366836548, Accuracy: 1.0, Computation time: 1.0464153289794922\n",
      "Step: 5874, Loss: 0.9159443378448486, Accuracy: 1.0, Computation time: 1.1772215366363525\n",
      "Step: 5875, Loss: 0.9159706234931946, Accuracy: 1.0, Computation time: 1.3952689170837402\n",
      "Step: 5876, Loss: 0.916111946105957, Accuracy: 1.0, Computation time: 1.453362226486206\n",
      "Step: 5877, Loss: 0.937437891960144, Accuracy: 0.9750000238418579, Computation time: 1.851034164428711\n",
      "Step: 5878, Loss: 0.9182290434837341, Accuracy: 1.0, Computation time: 1.214942455291748\n",
      "Step: 5879, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 1.0945732593536377\n",
      "Step: 5880, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.1440367698669434\n",
      "Step: 5881, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.038299560546875\n",
      "Step: 5882, Loss: 0.915934681892395, Accuracy: 1.0, Computation time: 1.1976399421691895\n",
      "Step: 5883, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.2045695781707764\n",
      "Step: 5884, Loss: 0.9159387946128845, Accuracy: 1.0, Computation time: 0.9717724323272705\n",
      "Step: 5885, Loss: 0.9180205464363098, Accuracy: 1.0, Computation time: 1.2466697692871094\n",
      "Step: 5886, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 1.0682671070098877\n",
      "Step: 5887, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 1.4361732006072998\n",
      "Step: 5888, Loss: 0.9377480745315552, Accuracy: 0.9642857313156128, Computation time: 1.0712180137634277\n",
      "Step: 5889, Loss: 0.915900468826294, Accuracy: 1.0, Computation time: 0.9705905914306641\n",
      "Step: 5890, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 1.0702195167541504\n",
      "Step: 5891, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.0947368144989014\n",
      "Step: 5892, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.2841355800628662\n",
      "Step: 5893, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 1.153702735900879\n",
      "Step: 5894, Loss: 0.9420835375785828, Accuracy: 0.96875, Computation time: 1.868272066116333\n",
      "Step: 5895, Loss: 0.9375120997428894, Accuracy: 0.9642857313156128, Computation time: 1.5776946544647217\n",
      "Step: 5896, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.3253896236419678\n",
      "Step: 5897, Loss: 0.9160196781158447, Accuracy: 1.0, Computation time: 1.3432114124298096\n",
      "Step: 5898, Loss: 0.9159912467002869, Accuracy: 1.0, Computation time: 1.024721622467041\n",
      "Step: 5899, Loss: 0.9159223437309265, Accuracy: 1.0, Computation time: 1.1302769184112549\n",
      "Step: 5900, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.2150936126708984\n",
      "Step: 5901, Loss: 0.9160203337669373, Accuracy: 1.0, Computation time: 1.5676085948944092\n",
      "Step: 5902, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 1.0074856281280518\n",
      "Step: 5903, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 1.6744825839996338\n",
      "Step: 5904, Loss: 0.9186884164810181, Accuracy: 1.0, Computation time: 1.5118088722229004\n",
      "Step: 5905, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.912076711654663\n",
      "Step: 5906, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 1.2147622108459473\n",
      "Step: 5907, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 1.1197669506072998\n",
      "Step: 5908, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.428708791732788\n",
      "Step: 5909, Loss: 0.9181074500083923, Accuracy: 1.0, Computation time: 2.026282548904419\n",
      "Step: 5910, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.5006093978881836\n",
      "Step: 5911, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.9771606922149658\n",
      "Step: 5912, Loss: 0.9375466704368591, Accuracy: 0.9821428656578064, Computation time: 1.6636958122253418\n",
      "Step: 5913, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.8996844291687012\n",
      "Step: 5914, Loss: 0.9159027934074402, Accuracy: 1.0, Computation time: 1.1371076107025146\n",
      "Step: 5915, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 1.4325933456420898\n",
      "Step: 5916, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.5264523029327393\n",
      "Step: 5917, Loss: 0.9158797264099121, Accuracy: 1.0, Computation time: 1.911574363708496\n",
      "Step: 5918, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 1.4354708194732666\n",
      "Step: 5919, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 1.3577051162719727\n",
      "Step: 5920, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 1.7297918796539307\n",
      "Step: 5921, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.4652152061462402\n",
      "Step: 5922, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 1.562621831893921\n",
      "Step: 5923, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.8275246620178223\n",
      "Step: 5924, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 1.4336974620819092\n",
      "Step: 5925, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.6374971866607666\n",
      "Step: 5926, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 1.4872372150421143\n",
      "Step: 5927, Loss: 0.9163569808006287, Accuracy: 1.0, Computation time: 1.7095081806182861\n",
      "Step: 5928, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.5521833896636963\n",
      "Step: 5929, Loss: 0.9375872015953064, Accuracy: 0.9750000238418579, Computation time: 1.7407829761505127\n",
      "Step: 5930, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.873917579650879\n",
      "Step: 5931, Loss: 0.9162552356719971, Accuracy: 1.0, Computation time: 1.6288478374481201\n",
      "Step: 5932, Loss: 0.9158852696418762, Accuracy: 1.0, Computation time: 1.5247266292572021\n",
      "Step: 5933, Loss: 0.959201991558075, Accuracy: 0.9434524178504944, Computation time: 1.5268137454986572\n",
      "Step: 5934, Loss: 0.9375616908073425, Accuracy: 0.9722222089767456, Computation time: 1.5028142929077148\n",
      "Step: 5935, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 1.5836639404296875\n",
      "Step: 5936, Loss: 0.9159311056137085, Accuracy: 1.0, Computation time: 1.508798360824585\n",
      "Step: 5937, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 1.5126445293426514\n",
      "Step: 5938, Loss: 0.9158854484558105, Accuracy: 1.0, Computation time: 1.6966280937194824\n",
      "Step: 5939, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 1.7811803817749023\n",
      "Step: 5940, Loss: 0.9376026391983032, Accuracy: 0.984375, Computation time: 1.5615508556365967\n",
      "Step: 5941, Loss: 0.9600011706352234, Accuracy: 0.9545454978942871, Computation time: 1.941004991531372\n",
      "Step: 5942, Loss: 0.9365483522415161, Accuracy: 0.9791666865348816, Computation time: 1.5524303913116455\n",
      "Step: 5943, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 1.8021526336669922\n",
      "Step: 5944, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 1.5767874717712402\n",
      "Step: 5945, Loss: 0.9159809350967407, Accuracy: 1.0, Computation time: 1.8117504119873047\n",
      "Step: 5946, Loss: 0.9159864187240601, Accuracy: 1.0, Computation time: 1.8619301319122314\n",
      "Step: 5947, Loss: 0.9158991575241089, Accuracy: 1.0, Computation time: 1.4114267826080322\n",
      "Step: 5948, Loss: 0.916297197341919, Accuracy: 1.0, Computation time: 1.5795888900756836\n",
      "Step: 5949, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 1.3058083057403564\n",
      "Step: 5950, Loss: 0.9192261099815369, Accuracy: 1.0, Computation time: 1.682455062866211\n",
      "Step: 5951, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 1.5614609718322754\n",
      "Step: 5952, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.2697761058807373\n",
      "Step: 5953, Loss: 0.9576658010482788, Accuracy: 0.9545454978942871, Computation time: 1.597949504852295\n",
      "Step: 5954, Loss: 0.9159950017929077, Accuracy: 1.0, Computation time: 1.6058077812194824\n",
      "Step: 5955, Loss: 0.9159746766090393, Accuracy: 1.0, Computation time: 1.521843671798706\n",
      "Step: 5956, Loss: 0.9502567052841187, Accuracy: 0.9285714626312256, Computation time: 1.787412405014038\n",
      "Step: 5957, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 1.2153277397155762\n",
      "Step: 5958, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.1039767265319824\n",
      "Step: 5959, Loss: 0.9158833026885986, Accuracy: 1.0, Computation time: 1.163238286972046\n",
      "Step: 5960, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 1.1288034915924072\n",
      "Step: 5961, Loss: 0.9159045815467834, Accuracy: 1.0, Computation time: 1.1109673976898193\n",
      "Step: 5962, Loss: 0.9376680850982666, Accuracy: 0.949999988079071, Computation time: 1.1192054748535156\n",
      "Step: 5963, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.2319962978363037\n",
      "Step: 5964, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.301866054534912\n",
      "Step: 5965, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.1030681133270264\n",
      "Step: 5966, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.0153124332427979\n",
      "Step: 5967, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 0.9848518371582031\n",
      "Step: 5968, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.0308630466461182\n",
      "Step: 5969, Loss: 0.915850043296814, Accuracy: 1.0, Computation time: 1.026886224746704\n",
      "Step: 5970, Loss: 0.9373111724853516, Accuracy: 0.9583333730697632, Computation time: 1.440821886062622\n",
      "Step: 5971, Loss: 0.9375594854354858, Accuracy: 0.9750000238418579, Computation time: 0.9266157150268555\n",
      "Step: 5972, Loss: 0.936873197555542, Accuracy: 0.9722222089767456, Computation time: 1.133777379989624\n",
      "Step: 5973, Loss: 0.9375179409980774, Accuracy: 0.9583333730697632, Computation time: 1.1650471687316895\n",
      "Step: 5974, Loss: 0.9374904036521912, Accuracy: 0.9791666865348816, Computation time: 1.1879370212554932\n",
      "Step: 5975, Loss: 0.9375771880149841, Accuracy: 0.9583333730697632, Computation time: 1.0114772319793701\n",
      "########################\n",
      "Test loss: 1.1250420808792114, Test Accuracy_epoch43: 0.6967593431472778\n",
      "########################\n",
      "Step: 5976, Loss: 0.9535354375839233, Accuracy: 0.9494949579238892, Computation time: 2.0080292224884033\n",
      "Step: 5977, Loss: 0.9161363244056702, Accuracy: 1.0, Computation time: 1.3553123474121094\n",
      "Step: 5978, Loss: 0.9193213582038879, Accuracy: 1.0, Computation time: 1.3703742027282715\n",
      "Step: 5979, Loss: 0.9187047481536865, Accuracy: 1.0, Computation time: 1.0782177448272705\n",
      "Step: 5980, Loss: 0.9159225225448608, Accuracy: 1.0, Computation time: 1.2586028575897217\n",
      "Step: 5981, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.0004854202270508\n",
      "Step: 5982, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 0.9822597503662109\n",
      "Step: 5983, Loss: 0.9158976674079895, Accuracy: 1.0, Computation time: 0.9761466979980469\n",
      "Step: 5984, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 0.9598855972290039\n",
      "Step: 5985, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 1.0613586902618408\n",
      "Step: 5986, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 1.1367664337158203\n",
      "Step: 5987, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.3704111576080322\n",
      "Step: 5988, Loss: 0.9159045815467834, Accuracy: 1.0, Computation time: 1.2404234409332275\n",
      "Step: 5989, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.0479795932769775\n",
      "Step: 5990, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 1.0828461647033691\n",
      "Step: 5991, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.0727496147155762\n",
      "Step: 5992, Loss: 0.9162755608558655, Accuracy: 1.0, Computation time: 1.085761547088623\n",
      "Step: 5993, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 1.1082227230072021\n",
      "Step: 5994, Loss: 0.91590416431427, Accuracy: 1.0, Computation time: 1.409529447555542\n",
      "Step: 5995, Loss: 0.9188128709793091, Accuracy: 1.0, Computation time: 1.1625895500183105\n",
      "Step: 5996, Loss: 0.9374999403953552, Accuracy: 0.949999988079071, Computation time: 1.1276452541351318\n",
      "Step: 5997, Loss: 0.9159054756164551, Accuracy: 1.0, Computation time: 1.2630383968353271\n",
      "Step: 5998, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 1.1045238971710205\n",
      "Step: 5999, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.1964406967163086\n",
      "Step: 6000, Loss: 0.9377334713935852, Accuracy: 0.96875, Computation time: 1.1941301822662354\n",
      "Step: 6001, Loss: 0.9158704876899719, Accuracy: nan, Computation time: 1.3144261837005615\n",
      "Step: 6002, Loss: 0.9376122355461121, Accuracy: 0.9791666865348816, Computation time: 1.5842692852020264\n",
      "Step: 6003, Loss: 0.9173644781112671, Accuracy: 1.0, Computation time: 1.3135926723480225\n",
      "Step: 6004, Loss: 0.9160804748535156, Accuracy: 1.0, Computation time: 1.8350856304168701\n",
      "Step: 6005, Loss: 0.9159681797027588, Accuracy: 1.0, Computation time: 1.4047234058380127\n",
      "Step: 6006, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.6087462902069092\n",
      "Step: 6007, Loss: 0.9160242080688477, Accuracy: 1.0, Computation time: 1.4727976322174072\n",
      "Step: 6008, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 1.302405834197998\n",
      "Step: 6009, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.0889670848846436\n",
      "Step: 6010, Loss: 0.9158601760864258, Accuracy: 1.0, Computation time: 1.262620449066162\n",
      "Step: 6011, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.529393196105957\n",
      "Step: 6012, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 1.4399003982543945\n",
      "Step: 6013, Loss: 0.9158648252487183, Accuracy: 1.0, Computation time: 1.0451045036315918\n",
      "Step: 6014, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 1.2652783393859863\n",
      "Step: 6015, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 1.6307289600372314\n",
      "Step: 6016, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.267991542816162\n",
      "Step: 6017, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.407580852508545\n",
      "Step: 6018, Loss: 0.9159292578697205, Accuracy: 1.0, Computation time: 1.467186450958252\n",
      "Step: 6019, Loss: 0.9159632921218872, Accuracy: 1.0, Computation time: 1.9702777862548828\n",
      "Step: 6020, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.60353422164917\n",
      "Step: 6021, Loss: 0.9352599382400513, Accuracy: 0.9722222089767456, Computation time: 1.961791753768921\n",
      "Step: 6022, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.0588810443878174\n",
      "Step: 6023, Loss: 0.9159107804298401, Accuracy: 1.0, Computation time: 1.2029266357421875\n",
      "Step: 6024, Loss: 0.9377549886703491, Accuracy: 0.9583333730697632, Computation time: 1.3312878608703613\n",
      "Step: 6025, Loss: 0.9372146129608154, Accuracy: 0.9791666865348816, Computation time: 1.43644118309021\n",
      "Step: 6026, Loss: 0.9159789085388184, Accuracy: 1.0, Computation time: 1.4348959922790527\n",
      "Step: 6027, Loss: 0.916916012763977, Accuracy: 1.0, Computation time: 1.8360939025878906\n",
      "Step: 6028, Loss: 0.9161794185638428, Accuracy: 1.0, Computation time: 1.303023099899292\n",
      "Step: 6029, Loss: 0.9169463515281677, Accuracy: 1.0, Computation time: 1.3710613250732422\n",
      "Step: 6030, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.4340436458587646\n",
      "Step: 6031, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.1994044780731201\n",
      "Step: 6032, Loss: 0.9160152077674866, Accuracy: 1.0, Computation time: 1.3030989170074463\n",
      "Step: 6033, Loss: 0.9374423027038574, Accuracy: 0.96875, Computation time: 1.2671265602111816\n",
      "Step: 6034, Loss: 0.9160020351409912, Accuracy: 1.0, Computation time: 1.4830374717712402\n",
      "Step: 6035, Loss: 0.916131317615509, Accuracy: 1.0, Computation time: 1.3485968112945557\n",
      "Step: 6036, Loss: 0.9159936308860779, Accuracy: 1.0, Computation time: 1.5466468334197998\n",
      "Step: 6037, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.3750114440917969\n",
      "Step: 6038, Loss: 0.9375965595245361, Accuracy: 0.96875, Computation time: 1.2755444049835205\n",
      "Step: 6039, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 1.274845838546753\n",
      "Step: 6040, Loss: 0.9331282377243042, Accuracy: 0.9722222089767456, Computation time: 2.5131447315216064\n",
      "Step: 6041, Loss: 0.9198487401008606, Accuracy: 1.0, Computation time: 1.5074946880340576\n",
      "Step: 6042, Loss: 0.9160078763961792, Accuracy: 1.0, Computation time: 1.4466066360473633\n",
      "Step: 6043, Loss: 0.9164538979530334, Accuracy: 1.0, Computation time: 1.6363096237182617\n",
      "Step: 6044, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.3960087299346924\n",
      "Step: 6045, Loss: 0.9379721879959106, Accuracy: 0.96875, Computation time: 1.3896865844726562\n",
      "Step: 6046, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 1.3736085891723633\n",
      "Step: 6047, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 2.1938858032226562\n",
      "Step: 6048, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.4947407245635986\n",
      "Step: 6049, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 1.0367166996002197\n",
      "Step: 6050, Loss: 0.9359250664710999, Accuracy: 0.9833333492279053, Computation time: 1.7475402355194092\n",
      "Step: 6051, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 1.4310896396636963\n",
      "Step: 6052, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 0.9778928756713867\n",
      "Step: 6053, Loss: 0.9158725738525391, Accuracy: 1.0, Computation time: 1.1830949783325195\n",
      "Step: 6054, Loss: 0.9163227081298828, Accuracy: 1.0, Computation time: 1.6018991470336914\n",
      "Step: 6055, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 1.4413180351257324\n",
      "Step: 6056, Loss: 0.9158714413642883, Accuracy: 1.0, Computation time: 0.97705078125\n",
      "Step: 6057, Loss: 0.9158720374107361, Accuracy: 1.0, Computation time: 1.1703722476959229\n",
      "Step: 6058, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 1.0569968223571777\n",
      "Step: 6059, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 0.8572335243225098\n",
      "Step: 6060, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.5765104293823242\n",
      "Step: 6061, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 0.9832456111907959\n",
      "Step: 6062, Loss: 0.9375380873680115, Accuracy: 0.9791666865348816, Computation time: 1.1327972412109375\n",
      "Step: 6063, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 0.9054665565490723\n",
      "Step: 6064, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 1.1972239017486572\n",
      "Step: 6065, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 0.9299056529998779\n",
      "Step: 6066, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 1.6187150478363037\n",
      "Step: 6067, Loss: 0.916022777557373, Accuracy: 1.0, Computation time: 1.4952797889709473\n",
      "Step: 6068, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.351100206375122\n",
      "Step: 6069, Loss: 0.9158552289009094, Accuracy: 1.0, Computation time: 1.3288934230804443\n",
      "Step: 6070, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 1.0144529342651367\n",
      "Step: 6071, Loss: 0.9158379435539246, Accuracy: 1.0, Computation time: 1.085829257965088\n",
      "Step: 6072, Loss: 0.9158406257629395, Accuracy: 1.0, Computation time: 1.093094825744629\n",
      "Step: 6073, Loss: 0.93746417760849, Accuracy: 0.9821428656578064, Computation time: 1.0935311317443848\n",
      "Step: 6074, Loss: 0.9375234842300415, Accuracy: 0.96875, Computation time: 1.0686118602752686\n",
      "Step: 6075, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 1.1808509826660156\n",
      "Step: 6076, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 1.3897528648376465\n",
      "Step: 6077, Loss: 0.915977418422699, Accuracy: 1.0, Computation time: 1.0576937198638916\n",
      "Step: 6078, Loss: 0.9468623399734497, Accuracy: 0.8974359035491943, Computation time: 1.6536319255828857\n",
      "Step: 6079, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.1920835971832275\n",
      "Step: 6080, Loss: 0.9195668697357178, Accuracy: 1.0, Computation time: 1.971104621887207\n",
      "Step: 6081, Loss: 0.9159590005874634, Accuracy: 1.0, Computation time: 1.4219746589660645\n",
      "Step: 6082, Loss: 0.9376395344734192, Accuracy: 0.96875, Computation time: 1.054260492324829\n",
      "Step: 6083, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.3876512050628662\n",
      "Step: 6084, Loss: 0.9159696698188782, Accuracy: 1.0, Computation time: 1.053469181060791\n",
      "Step: 6085, Loss: 0.937724232673645, Accuracy: 0.96875, Computation time: 1.0810601711273193\n",
      "Step: 6086, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 1.0136768817901611\n",
      "Step: 6087, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 1.0737595558166504\n",
      "Step: 6088, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.2595205307006836\n",
      "Step: 6089, Loss: 0.9374983906745911, Accuracy: 0.9722222089767456, Computation time: 1.1819086074829102\n",
      "Step: 6090, Loss: 0.9159412980079651, Accuracy: 1.0, Computation time: 1.2019786834716797\n",
      "Step: 6091, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.1509368419647217\n",
      "Step: 6092, Loss: 0.9160104990005493, Accuracy: 1.0, Computation time: 0.9545588493347168\n",
      "Step: 6093, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 1.1808757781982422\n",
      "Step: 6094, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.8208496570587158\n",
      "Step: 6095, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 1.2510066032409668\n",
      "Step: 6096, Loss: 0.9374239444732666, Accuracy: 0.96875, Computation time: 1.2468299865722656\n",
      "Step: 6097, Loss: 0.9161291718482971, Accuracy: 1.0, Computation time: 1.1148700714111328\n",
      "Step: 6098, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 1.4616127014160156\n",
      "Step: 6099, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 1.1048154830932617\n",
      "Step: 6100, Loss: 0.933876633644104, Accuracy: 0.9772727489471436, Computation time: 1.4253981113433838\n",
      "Step: 6101, Loss: 0.9158727526664734, Accuracy: 1.0, Computation time: 0.8943464756011963\n",
      "Step: 6102, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 1.370985507965088\n",
      "Step: 6103, Loss: 0.9375102519989014, Accuracy: 0.9642857313156128, Computation time: 1.375981330871582\n",
      "Step: 6104, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 1.173614263534546\n",
      "Step: 6105, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 1.1675801277160645\n",
      "Step: 6106, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 0.911034345626831\n",
      "Step: 6107, Loss: 0.9162180423736572, Accuracy: 1.0, Computation time: 1.6967413425445557\n",
      "Step: 6108, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.3564651012420654\n",
      "Step: 6109, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 1.1346156597137451\n",
      "Step: 6110, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.9880514144897461\n",
      "Step: 6111, Loss: 0.9158853888511658, Accuracy: 1.0, Computation time: 1.0237410068511963\n",
      "Step: 6112, Loss: 0.9378894567489624, Accuracy: 0.9772727489471436, Computation time: 1.1453032493591309\n",
      "Step: 6113, Loss: 0.9201956391334534, Accuracy: 1.0, Computation time: 1.3202600479125977\n",
      "Step: 6114, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.5334665775299072\n",
      "########################\n",
      "Test loss: 1.1177284717559814, Test Accuracy_epoch44: 0.706548810005188\n",
      "########################\n",
      "Step: 6115, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 1.0868844985961914\n",
      "Step: 6116, Loss: 0.9159830212593079, Accuracy: 1.0, Computation time: 1.1838805675506592\n",
      "Step: 6117, Loss: 0.9348313808441162, Accuracy: 0.9821428656578064, Computation time: 1.4181675910949707\n",
      "Step: 6118, Loss: 0.9375221729278564, Accuracy: 0.9642857313156128, Computation time: 1.4880788326263428\n",
      "Step: 6119, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.2137465476989746\n",
      "Step: 6120, Loss: 0.916041910648346, Accuracy: 1.0, Computation time: 1.0953428745269775\n",
      "Step: 6121, Loss: 0.9159530401229858, Accuracy: 1.0, Computation time: 1.1189544200897217\n",
      "Step: 6122, Loss: 0.9159591197967529, Accuracy: 1.0, Computation time: 1.5630090236663818\n",
      "Step: 6123, Loss: 0.9159058332443237, Accuracy: 1.0, Computation time: 1.307710886001587\n",
      "Step: 6124, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.4171185493469238\n",
      "Step: 6125, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 1.0383689403533936\n",
      "Step: 6126, Loss: 0.915913462638855, Accuracy: 1.0, Computation time: 1.0413753986358643\n",
      "Step: 6127, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 1.0838558673858643\n",
      "Step: 6128, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.0991740226745605\n",
      "Step: 6129, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 1.0009863376617432\n",
      "Step: 6130, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.1076104640960693\n",
      "Step: 6131, Loss: 0.9158543348312378, Accuracy: 1.0, Computation time: 0.9765546321868896\n",
      "Step: 6132, Loss: 0.936464786529541, Accuracy: 0.949999988079071, Computation time: 1.3567605018615723\n",
      "Step: 6133, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.0781452655792236\n",
      "Step: 6134, Loss: 0.9269164204597473, Accuracy: 0.9642857313156128, Computation time: 1.638615369796753\n",
      "Step: 6135, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.2415149211883545\n",
      "Step: 6136, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.0177650451660156\n",
      "Step: 6137, Loss: 0.9375457763671875, Accuracy: 0.96875, Computation time: 1.514807939529419\n",
      "Step: 6138, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.1753778457641602\n",
      "Step: 6139, Loss: 0.916221022605896, Accuracy: 1.0, Computation time: 1.0845394134521484\n",
      "Step: 6140, Loss: 0.9366021156311035, Accuracy: 0.9791666865348816, Computation time: 1.1465673446655273\n",
      "Step: 6141, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.0846960544586182\n",
      "Step: 6142, Loss: 0.9214995503425598, Accuracy: 1.0, Computation time: 1.9449591636657715\n",
      "Step: 6143, Loss: 0.9159293174743652, Accuracy: 1.0, Computation time: 1.1621019840240479\n",
      "Step: 6144, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.3750674724578857\n",
      "Step: 6145, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.2873523235321045\n",
      "Step: 6146, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.2607650756835938\n",
      "Step: 6147, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 1.0908267498016357\n",
      "Step: 6148, Loss: 0.9159280061721802, Accuracy: 1.0, Computation time: 1.0719680786132812\n",
      "Step: 6149, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 1.27152419090271\n",
      "Step: 6150, Loss: 0.9159560799598694, Accuracy: 1.0, Computation time: 1.4060142040252686\n",
      "Step: 6151, Loss: 0.9158907532691956, Accuracy: 1.0, Computation time: 0.9191994667053223\n",
      "Step: 6152, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 1.0274696350097656\n",
      "Step: 6153, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.1234409809112549\n",
      "Step: 6154, Loss: 0.9371437430381775, Accuracy: 0.9807692766189575, Computation time: 1.1406002044677734\n",
      "Step: 6155, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 1.2552080154418945\n",
      "Step: 6156, Loss: 0.9333216547966003, Accuracy: 0.9791666865348816, Computation time: 1.0722918510437012\n",
      "Step: 6157, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.290271282196045\n",
      "Step: 6158, Loss: 0.9159041047096252, Accuracy: 1.0, Computation time: 1.015528917312622\n",
      "Step: 6159, Loss: 0.9374803900718689, Accuracy: 0.9807692766189575, Computation time: 1.3985929489135742\n",
      "Step: 6160, Loss: 0.9160541296005249, Accuracy: 1.0, Computation time: 1.1987721920013428\n",
      "Step: 6161, Loss: 0.9160585403442383, Accuracy: 1.0, Computation time: 1.1895389556884766\n",
      "Step: 6162, Loss: 0.9218295216560364, Accuracy: 1.0, Computation time: 1.2580881118774414\n",
      "Step: 6163, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 1.6456081867218018\n",
      "Step: 6164, Loss: 0.9169639945030212, Accuracy: 1.0, Computation time: 1.1743130683898926\n",
      "Step: 6165, Loss: 0.9160102009773254, Accuracy: 1.0, Computation time: 1.184082269668579\n",
      "Step: 6166, Loss: 0.9159610271453857, Accuracy: 1.0, Computation time: 1.3005306720733643\n",
      "Step: 6167, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 0.9972310066223145\n",
      "Step: 6168, Loss: 0.9161903858184814, Accuracy: 1.0, Computation time: 1.2798759937286377\n",
      "Step: 6169, Loss: 0.9161617755889893, Accuracy: 1.0, Computation time: 1.5542409420013428\n",
      "Step: 6170, Loss: 0.9159637689590454, Accuracy: 1.0, Computation time: 1.756739854812622\n",
      "Step: 6171, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.1736857891082764\n",
      "Step: 6172, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 1.6262202262878418\n",
      "Step: 6173, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.1242995262145996\n",
      "Step: 6174, Loss: 0.9159837365150452, Accuracy: 1.0, Computation time: 1.4665570259094238\n",
      "Step: 6175, Loss: 0.9159652590751648, Accuracy: 1.0, Computation time: 1.151129961013794\n",
      "Step: 6176, Loss: 0.9374657869338989, Accuracy: 0.9807692766189575, Computation time: 1.3418667316436768\n",
      "Step: 6177, Loss: 0.9366222023963928, Accuracy: 0.9642857313156128, Computation time: 1.8084959983825684\n",
      "Step: 6178, Loss: 0.9159426689147949, Accuracy: 1.0, Computation time: 1.332395315170288\n",
      "Step: 6179, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.348071575164795\n",
      "Step: 6180, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.4123022556304932\n",
      "Step: 6181, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.0404691696166992\n",
      "Step: 6182, Loss: 0.9375115036964417, Accuracy: 0.949999988079071, Computation time: 1.3727247714996338\n",
      "Step: 6183, Loss: 0.9159267544746399, Accuracy: 1.0, Computation time: 1.3826837539672852\n",
      "Step: 6184, Loss: 0.9374854564666748, Accuracy: 0.9750000238418579, Computation time: 1.2154603004455566\n",
      "Step: 6185, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 1.1406688690185547\n",
      "Step: 6186, Loss: 0.9375244975090027, Accuracy: 0.9375, Computation time: 1.4340224266052246\n",
      "Step: 6187, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.498044490814209\n",
      "Step: 6188, Loss: 0.9159466028213501, Accuracy: 1.0, Computation time: 1.4452826976776123\n",
      "Step: 6189, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 1.3151843547821045\n",
      "Step: 6190, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 1.2720980644226074\n",
      "Step: 6191, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 1.5232415199279785\n",
      "Step: 6192, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.8093640804290771\n",
      "Step: 6193, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 1.2203588485717773\n",
      "Step: 6194, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.3187124729156494\n",
      "Step: 6195, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.4751060009002686\n",
      "Step: 6196, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 1.504791498184204\n",
      "Step: 6197, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.2774114608764648\n",
      "Step: 6198, Loss: 0.9158761501312256, Accuracy: 1.0, Computation time: 1.6533982753753662\n",
      "Step: 6199, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.8879024982452393\n",
      "Step: 6200, Loss: 0.9379062652587891, Accuracy: 0.96875, Computation time: 2.01029896736145\n",
      "Step: 6201, Loss: 0.915839433670044, Accuracy: 1.0, Computation time: 1.3649225234985352\n",
      "Step: 6202, Loss: 0.9177431464195251, Accuracy: 1.0, Computation time: 1.9632110595703125\n",
      "Step: 6203, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 1.3323216438293457\n",
      "Step: 6204, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 1.566596269607544\n",
      "Step: 6205, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 1.384063720703125\n",
      "Step: 6206, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.351210355758667\n",
      "Step: 6207, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 1.338089942932129\n",
      "Step: 6208, Loss: 0.9374672174453735, Accuracy: 0.9772727489471436, Computation time: 1.5348050594329834\n",
      "Step: 6209, Loss: 0.918385922908783, Accuracy: 1.0, Computation time: 1.4806556701660156\n",
      "Step: 6210, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.6277201175689697\n",
      "Step: 6211, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 1.4663407802581787\n",
      "Step: 6212, Loss: 0.9376339316368103, Accuracy: 0.9750000238418579, Computation time: 1.4117381572723389\n",
      "Step: 6213, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 1.5609803199768066\n",
      "Step: 6214, Loss: 0.9377638697624207, Accuracy: 0.9722222089767456, Computation time: 1.5950984954833984\n",
      "Step: 6215, Loss: 0.9356721639633179, Accuracy: 0.9772727489471436, Computation time: 2.141511917114258\n",
      "Step: 6216, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.8715910911560059\n",
      "Step: 6217, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 2.0679309368133545\n",
      "Step: 6218, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.454709768295288\n",
      "Step: 6219, Loss: 0.9207327365875244, Accuracy: 1.0, Computation time: 1.7304737567901611\n",
      "Step: 6220, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 1.3633761405944824\n",
      "Step: 6221, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 1.5449364185333252\n",
      "Step: 6222, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.2904300689697266\n",
      "Step: 6223, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.6952664852142334\n",
      "Step: 6224, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 1.234443187713623\n",
      "Step: 6225, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.862476110458374\n",
      "Step: 6226, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.5089454650878906\n",
      "Step: 6227, Loss: 0.9158632755279541, Accuracy: 1.0, Computation time: 1.7632291316986084\n",
      "Step: 6228, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 1.56569504737854\n",
      "Step: 6229, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 1.58756685256958\n",
      "Step: 6230, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 1.6497118473052979\n",
      "Step: 6231, Loss: 0.9354720115661621, Accuracy: 0.9750000238418579, Computation time: 1.7921545505523682\n",
      "Step: 6232, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 1.5019218921661377\n",
      "Step: 6233, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 1.5281133651733398\n",
      "Step: 6234, Loss: 0.9591166973114014, Accuracy: 0.9415584802627563, Computation time: 1.4725043773651123\n",
      "Step: 6235, Loss: 0.9158749580383301, Accuracy: 1.0, Computation time: 1.9215233325958252\n",
      "Step: 6236, Loss: 0.920551598072052, Accuracy: 1.0, Computation time: 1.531548261642456\n",
      "Step: 6237, Loss: 0.937529444694519, Accuracy: 0.9722222089767456, Computation time: 1.4310400485992432\n",
      "Step: 6238, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 2.0165035724639893\n",
      "Step: 6239, Loss: 0.9159770607948303, Accuracy: 1.0, Computation time: 1.8927557468414307\n",
      "Step: 6240, Loss: 0.9159694314002991, Accuracy: 1.0, Computation time: 1.7031986713409424\n",
      "Step: 6241, Loss: 0.9376261830329895, Accuracy: 0.9807692766189575, Computation time: 2.036989688873291\n",
      "Step: 6242, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.643019199371338\n",
      "Step: 6243, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 1.5179481506347656\n",
      "Step: 6244, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 1.702854871749878\n",
      "Step: 6245, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.6342833042144775\n",
      "Step: 6246, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 1.6540336608886719\n",
      "Step: 6247, Loss: 0.9158863425254822, Accuracy: 1.0, Computation time: 1.899597406387329\n",
      "Step: 6248, Loss: 0.9376403093338013, Accuracy: 0.9791666865348816, Computation time: 1.5435864925384521\n",
      "Step: 6249, Loss: 0.9375661015510559, Accuracy: 0.9750000238418579, Computation time: 1.5302696228027344\n",
      "Step: 6250, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.7644758224487305\n",
      "Step: 6251, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.874316692352295\n",
      "Step: 6252, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 1.6050996780395508\n",
      "Step: 6253, Loss: 0.916917085647583, Accuracy: 1.0, Computation time: 1.4884674549102783\n",
      "########################\n",
      "Test loss: 1.1170027256011963, Test Accuracy_epoch45: 0.7063149213790894\n",
      "########################\n",
      "Step: 6254, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 1.410994291305542\n",
      "Step: 6255, Loss: 0.9159805178642273, Accuracy: 1.0, Computation time: 1.4001755714416504\n",
      "Step: 6256, Loss: 0.9159322381019592, Accuracy: 1.0, Computation time: 1.4557790756225586\n",
      "Step: 6257, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 1.2574522495269775\n",
      "Step: 6258, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.81477689743042\n",
      "Step: 6259, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.415177822113037\n",
      "Step: 6260, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 1.1395514011383057\n",
      "Step: 6261, Loss: 0.937495231628418, Accuracy: 0.9583333730697632, Computation time: 1.3359904289245605\n",
      "Step: 6262, Loss: 0.9162747263908386, Accuracy: 1.0, Computation time: 1.2842721939086914\n",
      "Step: 6263, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 1.5168635845184326\n",
      "Step: 6264, Loss: 0.9373096227645874, Accuracy: 0.9821428656578064, Computation time: 1.6390550136566162\n",
      "Step: 6265, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.5288031101226807\n",
      "Step: 6266, Loss: 0.9159750938415527, Accuracy: 1.0, Computation time: 1.660893440246582\n",
      "Step: 6267, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 1.6215465068817139\n",
      "Step: 6268, Loss: 0.9374792575836182, Accuracy: 0.9722222089767456, Computation time: 1.3433821201324463\n",
      "Step: 6269, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.6614179611206055\n",
      "Step: 6270, Loss: 0.9158857464790344, Accuracy: 1.0, Computation time: 1.5660450458526611\n",
      "Step: 6271, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.5782299041748047\n",
      "Step: 6272, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.6104097366333008\n",
      "Step: 6273, Loss: 0.9346674680709839, Accuracy: 0.9821428656578064, Computation time: 1.5562288761138916\n",
      "Step: 6274, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 1.3815627098083496\n",
      "Step: 6275, Loss: 0.9373449087142944, Accuracy: 0.949999988079071, Computation time: 1.6380183696746826\n",
      "Step: 6276, Loss: 0.9159273505210876, Accuracy: 1.0, Computation time: 1.7800312042236328\n",
      "Step: 6277, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 1.4571022987365723\n",
      "Step: 6278, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.346966028213501\n",
      "Step: 6279, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 1.8359589576721191\n",
      "Step: 6280, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.6550376415252686\n",
      "Step: 6281, Loss: 0.9374944567680359, Accuracy: 0.9807692766189575, Computation time: 1.5400183200836182\n",
      "Step: 6282, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.8033554553985596\n",
      "Step: 6283, Loss: 0.9158372282981873, Accuracy: 1.0, Computation time: 1.7595384120941162\n",
      "Step: 6284, Loss: 0.9158400297164917, Accuracy: 1.0, Computation time: 1.4414100646972656\n",
      "Step: 6285, Loss: 0.9158345460891724, Accuracy: 1.0, Computation time: 1.4968838691711426\n",
      "Step: 6286, Loss: 0.9247835874557495, Accuracy: 1.0, Computation time: 2.305663585662842\n",
      "Step: 6287, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.5403225421905518\n",
      "Step: 6288, Loss: 0.9375829100608826, Accuracy: 0.9722222089767456, Computation time: 1.5066933631896973\n",
      "Step: 6289, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 1.627336025238037\n",
      "Step: 6290, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 1.5732262134552002\n",
      "Step: 6291, Loss: 0.916196346282959, Accuracy: 1.0, Computation time: 1.8361220359802246\n",
      "Step: 6292, Loss: 0.9162764549255371, Accuracy: 1.0, Computation time: 1.5395400524139404\n",
      "Step: 6293, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 1.490703821182251\n",
      "Step: 6294, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.6700057983398438\n",
      "Step: 6295, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.4087738990783691\n",
      "Step: 6296, Loss: 0.916053056716919, Accuracy: 1.0, Computation time: 1.5122358798980713\n",
      "Step: 6297, Loss: 0.9159606695175171, Accuracy: 1.0, Computation time: 1.626509428024292\n",
      "Step: 6298, Loss: 0.9160786271095276, Accuracy: 1.0, Computation time: 1.6166560649871826\n",
      "Step: 6299, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 2.2417428493499756\n",
      "Step: 6300, Loss: 0.9204146862030029, Accuracy: 1.0, Computation time: 2.1802737712860107\n",
      "Step: 6301, Loss: 0.9159897565841675, Accuracy: 1.0, Computation time: 1.8910176753997803\n",
      "Step: 6302, Loss: 0.9159820079803467, Accuracy: 1.0, Computation time: 1.593299388885498\n",
      "Step: 6303, Loss: 0.9159725308418274, Accuracy: 1.0, Computation time: 1.3421130180358887\n",
      "Step: 6304, Loss: 0.9215708374977112, Accuracy: 1.0, Computation time: 1.719862937927246\n",
      "Step: 6305, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.486846685409546\n",
      "Step: 6306, Loss: 0.9169788956642151, Accuracy: 1.0, Computation time: 1.7088689804077148\n",
      "Step: 6307, Loss: 0.9159344434738159, Accuracy: 1.0, Computation time: 1.572965383529663\n",
      "Step: 6308, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.8730530738830566\n",
      "Step: 6309, Loss: 0.9158806204795837, Accuracy: 1.0, Computation time: 1.9623661041259766\n",
      "Step: 6310, Loss: 0.9373751878738403, Accuracy: 0.9642857313156128, Computation time: 1.5228381156921387\n",
      "Step: 6311, Loss: 0.9158560037612915, Accuracy: 1.0, Computation time: 1.2653782367706299\n",
      "Step: 6312, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.314173698425293\n",
      "Step: 6313, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.5353357791900635\n",
      "Step: 6314, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.610381841659546\n",
      "Step: 6315, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.6980586051940918\n",
      "Step: 6316, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 1.7482335567474365\n",
      "Step: 6317, Loss: 0.959248960018158, Accuracy: 0.9500000476837158, Computation time: 1.7096846103668213\n",
      "Step: 6318, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 1.571117877960205\n",
      "Step: 6319, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 1.4134554862976074\n",
      "Step: 6320, Loss: 0.9158499240875244, Accuracy: 1.0, Computation time: 1.315331220626831\n",
      "Step: 6321, Loss: 0.9158688187599182, Accuracy: 1.0, Computation time: 1.4361507892608643\n",
      "Step: 6322, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.228851556777954\n",
      "Step: 6323, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 1.3418052196502686\n",
      "Step: 6324, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.4791266918182373\n",
      "Step: 6325, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.4080815315246582\n",
      "Step: 6326, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.4139487743377686\n",
      "Step: 6327, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 1.5177652835845947\n",
      "Step: 6328, Loss: 0.9158452153205872, Accuracy: 1.0, Computation time: 1.7191522121429443\n",
      "Step: 6329, Loss: 0.9181836843490601, Accuracy: 1.0, Computation time: 1.709094762802124\n",
      "Step: 6330, Loss: 0.9158366918563843, Accuracy: 1.0, Computation time: 1.5300929546356201\n",
      "Step: 6331, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.8130645751953125\n",
      "Step: 6332, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 1.4630241394042969\n",
      "Step: 6333, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.3645355701446533\n",
      "Step: 6334, Loss: 0.9158875346183777, Accuracy: 1.0, Computation time: 1.3477978706359863\n",
      "Step: 6335, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.1061463356018066\n",
      "Step: 6336, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 1.358710527420044\n",
      "Step: 6337, Loss: 0.9384695887565613, Accuracy: 0.9722222089767456, Computation time: 1.3805429935455322\n",
      "Step: 6338, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.2489638328552246\n",
      "Step: 6339, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 1.320340871810913\n",
      "Step: 6340, Loss: 0.9375864863395691, Accuracy: 0.9722222089767456, Computation time: 1.501469612121582\n",
      "Step: 6341, Loss: 0.9681940078735352, Accuracy: 0.9058442115783691, Computation time: 1.2650792598724365\n",
      "Step: 6342, Loss: 0.9159063696861267, Accuracy: 1.0, Computation time: 1.463771104812622\n",
      "Step: 6343, Loss: 0.9159348011016846, Accuracy: 1.0, Computation time: 1.6551868915557861\n",
      "Step: 6344, Loss: 0.9377025365829468, Accuracy: 0.9772727489471436, Computation time: 1.5007891654968262\n",
      "Step: 6345, Loss: 0.9160860180854797, Accuracy: 1.0, Computation time: 1.6119554042816162\n",
      "Step: 6346, Loss: 0.9159640669822693, Accuracy: 1.0, Computation time: 1.3151767253875732\n",
      "Step: 6347, Loss: 0.9159455299377441, Accuracy: 1.0, Computation time: 1.1556284427642822\n",
      "Step: 6348, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 1.5284144878387451\n",
      "Step: 6349, Loss: 0.9158579111099243, Accuracy: 1.0, Computation time: 1.4321038722991943\n",
      "Step: 6350, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 1.7241880893707275\n",
      "Step: 6351, Loss: 0.9158750176429749, Accuracy: 1.0, Computation time: 1.0184400081634521\n",
      "Step: 6352, Loss: 0.937511682510376, Accuracy: 0.96875, Computation time: 1.2057578563690186\n",
      "Step: 6353, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 1.3206510543823242\n",
      "Step: 6354, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.4295244216918945\n",
      "Step: 6355, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 1.309168815612793\n",
      "Step: 6356, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 1.4458611011505127\n",
      "Step: 6357, Loss: 0.9376407265663147, Accuracy: 0.9642857313156128, Computation time: 1.0729243755340576\n",
      "Step: 6358, Loss: 0.9159090518951416, Accuracy: 1.0, Computation time: 1.1343441009521484\n",
      "Step: 6359, Loss: 0.9558995962142944, Accuracy: 0.9083333015441895, Computation time: 1.2392022609710693\n",
      "Step: 6360, Loss: 0.9166532158851624, Accuracy: 1.0, Computation time: 1.3916831016540527\n",
      "Step: 6361, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 1.1227116584777832\n",
      "Step: 6362, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.5405361652374268\n",
      "Step: 6363, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.1614859104156494\n",
      "Step: 6364, Loss: 0.9159437417984009, Accuracy: 1.0, Computation time: 1.1290254592895508\n",
      "Step: 6365, Loss: 0.9382068514823914, Accuracy: 0.9772727489471436, Computation time: 1.9175817966461182\n",
      "Step: 6366, Loss: 0.915948212146759, Accuracy: 1.0, Computation time: 1.1950693130493164\n",
      "Step: 6367, Loss: 0.9373660683631897, Accuracy: 0.9642857313156128, Computation time: 1.5778708457946777\n",
      "Step: 6368, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.2728734016418457\n",
      "Step: 6369, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 1.3096811771392822\n",
      "Step: 6370, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 1.3093860149383545\n",
      "Step: 6371, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.156522274017334\n",
      "Step: 6372, Loss: 0.9160431027412415, Accuracy: 1.0, Computation time: 1.659637212753296\n",
      "Step: 6373, Loss: 0.9159275889396667, Accuracy: 1.0, Computation time: 1.1885936260223389\n",
      "Step: 6374, Loss: 0.937515377998352, Accuracy: 0.949999988079071, Computation time: 1.5008580684661865\n",
      "Step: 6375, Loss: 0.9158862829208374, Accuracy: 1.0, Computation time: 1.100337028503418\n",
      "Step: 6376, Loss: 0.9375230073928833, Accuracy: 0.9750000238418579, Computation time: 1.5171148777008057\n",
      "Step: 6377, Loss: 0.9185096025466919, Accuracy: 1.0, Computation time: 1.4246563911437988\n",
      "Step: 6378, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 1.3869118690490723\n",
      "Step: 6379, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.3142809867858887\n",
      "Step: 6380, Loss: 0.9371330738067627, Accuracy: 0.9750000238418579, Computation time: 1.6213483810424805\n",
      "Step: 6381, Loss: 0.9292986989021301, Accuracy: 0.9791666865348816, Computation time: 1.545396089553833\n",
      "Step: 6382, Loss: 0.9160259366035461, Accuracy: 1.0, Computation time: 1.0740740299224854\n",
      "Step: 6383, Loss: 0.9159539937973022, Accuracy: 1.0, Computation time: 1.3099346160888672\n",
      "Step: 6384, Loss: 0.9159491062164307, Accuracy: 1.0, Computation time: 1.2653589248657227\n",
      "Step: 6385, Loss: 0.9160184264183044, Accuracy: 1.0, Computation time: 1.1279051303863525\n",
      "Step: 6386, Loss: 0.9294991493225098, Accuracy: 0.96875, Computation time: 1.8403658866882324\n",
      "Step: 6387, Loss: 0.9216668009757996, Accuracy: 1.0, Computation time: 1.8380978107452393\n",
      "Step: 6388, Loss: 0.9159696102142334, Accuracy: 1.0, Computation time: 1.309678316116333\n",
      "Step: 6389, Loss: 0.9159231185913086, Accuracy: 1.0, Computation time: 1.2433345317840576\n",
      "Step: 6390, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.4468321800231934\n",
      "Step: 6391, Loss: 0.9160105586051941, Accuracy: 1.0, Computation time: 1.500927209854126\n",
      "Step: 6392, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 1.4839849472045898\n",
      "########################\n",
      "Test loss: 1.1178122758865356, Test Accuracy_epoch46: 0.7055093050003052\n",
      "########################\n",
      "Step: 6393, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 1.1476821899414062\n",
      "Step: 6394, Loss: 0.9580715894699097, Accuracy: 0.9305555820465088, Computation time: 1.7481167316436768\n",
      "Step: 6395, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.1365203857421875\n",
      "Step: 6396, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.218351125717163\n",
      "Step: 6397, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 1.4087295532226562\n",
      "Step: 6398, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 1.9841256141662598\n",
      "Step: 6399, Loss: 0.9159141778945923, Accuracy: 1.0, Computation time: 1.6085665225982666\n",
      "Step: 6400, Loss: 0.9159008860588074, Accuracy: 1.0, Computation time: 1.2457261085510254\n",
      "Step: 6401, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.6292345523834229\n",
      "Step: 6402, Loss: 0.9159976840019226, Accuracy: 1.0, Computation time: 1.530545711517334\n",
      "Step: 6403, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.3371732234954834\n",
      "Step: 6404, Loss: 0.9374620914459229, Accuracy: 0.9750000238418579, Computation time: 1.3520581722259521\n",
      "Step: 6405, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.609743356704712\n",
      "Step: 6406, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 1.4089996814727783\n",
      "Step: 6407, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 1.6934278011322021\n",
      "Step: 6408, Loss: 0.9158909320831299, Accuracy: 1.0, Computation time: 1.6601366996765137\n",
      "Step: 6409, Loss: 0.9159225225448608, Accuracy: 1.0, Computation time: 1.523946762084961\n",
      "Step: 6410, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 1.4290187358856201\n",
      "Step: 6411, Loss: 0.9373870491981506, Accuracy: 0.9642857313156128, Computation time: 1.7367076873779297\n",
      "Step: 6412, Loss: 0.9375790953636169, Accuracy: 0.9807692766189575, Computation time: 1.9213826656341553\n",
      "Step: 6413, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.6440768241882324\n",
      "Step: 6414, Loss: 0.9376404881477356, Accuracy: 0.9772727489471436, Computation time: 1.7319679260253906\n",
      "Step: 6415, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.6491789817810059\n",
      "Step: 6416, Loss: 0.9279354810714722, Accuracy: 0.9642857313156128, Computation time: 1.5510823726654053\n",
      "Step: 6417, Loss: 0.9380742311477661, Accuracy: 0.9791666865348816, Computation time: 1.4599385261535645\n",
      "Step: 6418, Loss: 0.9159470200538635, Accuracy: 1.0, Computation time: 1.5852229595184326\n",
      "Step: 6419, Loss: 0.9159544706344604, Accuracy: 1.0, Computation time: 1.3747260570526123\n",
      "Step: 6420, Loss: 0.9159460067749023, Accuracy: 1.0, Computation time: 1.790769338607788\n",
      "Step: 6421, Loss: 0.9351702928543091, Accuracy: 0.9821428656578064, Computation time: 1.7370429039001465\n",
      "Step: 6422, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 1.6301381587982178\n",
      "Step: 6423, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 1.489975929260254\n",
      "Step: 6424, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.4656784534454346\n",
      "Step: 6425, Loss: 0.9376537203788757, Accuracy: 0.9772727489471436, Computation time: 1.4682409763336182\n",
      "Step: 6426, Loss: 0.9159072041511536, Accuracy: 1.0, Computation time: 1.7215557098388672\n",
      "Step: 6427, Loss: 0.9375360608100891, Accuracy: 0.9791666865348816, Computation time: 1.4243085384368896\n",
      "Step: 6428, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 1.372248649597168\n",
      "Step: 6429, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.3034415245056152\n",
      "Step: 6430, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 1.4040279388427734\n",
      "Step: 6431, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.409639835357666\n",
      "Step: 6432, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.3220345973968506\n",
      "Step: 6433, Loss: 0.9158448576927185, Accuracy: 1.0, Computation time: 1.3685531616210938\n",
      "Step: 6434, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 1.4933135509490967\n",
      "Step: 6435, Loss: 0.9159291982650757, Accuracy: 1.0, Computation time: 1.5151617527008057\n",
      "Step: 6436, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 2.1801376342773438\n",
      "Step: 6437, Loss: 0.9176439046859741, Accuracy: 1.0, Computation time: 2.347508668899536\n",
      "Step: 6438, Loss: 0.9158598780632019, Accuracy: 1.0, Computation time: 1.283475637435913\n",
      "Step: 6439, Loss: 0.9158719778060913, Accuracy: nan, Computation time: 1.3385770320892334\n",
      "Step: 6440, Loss: 0.9167563915252686, Accuracy: 1.0, Computation time: 1.5871222019195557\n",
      "Step: 6441, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 1.3903672695159912\n",
      "Step: 6442, Loss: 0.9215407371520996, Accuracy: 1.0, Computation time: 1.3842353820800781\n",
      "Step: 6443, Loss: 0.9158975481987, Accuracy: 1.0, Computation time: 1.5380785465240479\n",
      "Step: 6444, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 1.510077953338623\n",
      "Step: 6445, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 1.4831085205078125\n",
      "Step: 6446, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.5608341693878174\n",
      "Step: 6447, Loss: 0.9330694675445557, Accuracy: 0.96875, Computation time: 1.7553915977478027\n",
      "Step: 6448, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 1.4760570526123047\n",
      "Step: 6449, Loss: 0.9304071664810181, Accuracy: 0.9750000238418579, Computation time: 1.4839720726013184\n",
      "Step: 6450, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 1.824601411819458\n",
      "Step: 6451, Loss: 0.9373084306716919, Accuracy: nan, Computation time: 1.3981983661651611\n",
      "Step: 6452, Loss: 0.9163032174110413, Accuracy: 1.0, Computation time: 1.7150671482086182\n",
      "Step: 6453, Loss: 0.9377964735031128, Accuracy: 0.9772727489471436, Computation time: 1.8551106452941895\n",
      "Step: 6454, Loss: 0.9162880778312683, Accuracy: 1.0, Computation time: 1.5014903545379639\n",
      "Step: 6455, Loss: 0.9173159599304199, Accuracy: 1.0, Computation time: 1.3607969284057617\n",
      "Step: 6456, Loss: 0.9238511323928833, Accuracy: 1.0, Computation time: 2.0583667755126953\n",
      "Step: 6457, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.5379719734191895\n",
      "Step: 6458, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 1.3391311168670654\n",
      "Step: 6459, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 1.791452169418335\n",
      "Step: 6460, Loss: 0.9159981608390808, Accuracy: 1.0, Computation time: 1.0753192901611328\n",
      "Step: 6461, Loss: 0.916169285774231, Accuracy: 1.0, Computation time: 1.2590999603271484\n",
      "Step: 6462, Loss: 0.916139543056488, Accuracy: 1.0, Computation time: 1.4281723499298096\n",
      "Step: 6463, Loss: 0.9160362482070923, Accuracy: 1.0, Computation time: 1.2067618370056152\n",
      "Step: 6464, Loss: 0.9159634113311768, Accuracy: 1.0, Computation time: 1.228304147720337\n",
      "Step: 6465, Loss: 0.9169086217880249, Accuracy: 1.0, Computation time: 1.7152202129364014\n",
      "Step: 6466, Loss: 0.9159420728683472, Accuracy: 1.0, Computation time: 1.459416389465332\n",
      "Step: 6467, Loss: 0.9159452319145203, Accuracy: 1.0, Computation time: 1.3209784030914307\n",
      "Step: 6468, Loss: 0.9163606762886047, Accuracy: 1.0, Computation time: 1.360978364944458\n",
      "Step: 6469, Loss: 0.938054084777832, Accuracy: 0.9583333730697632, Computation time: 1.2863061428070068\n",
      "Step: 6470, Loss: 0.9160701632499695, Accuracy: 1.0, Computation time: 1.764272928237915\n",
      "Step: 6471, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 1.7754161357879639\n",
      "Step: 6472, Loss: 0.9374641180038452, Accuracy: 0.9833333492279053, Computation time: 1.6497933864593506\n",
      "Step: 6473, Loss: 0.9165399074554443, Accuracy: 1.0, Computation time: 1.7048559188842773\n",
      "Step: 6474, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 1.1528196334838867\n",
      "Step: 6475, Loss: 0.916123628616333, Accuracy: 1.0, Computation time: 1.206463098526001\n",
      "Step: 6476, Loss: 0.9375805854797363, Accuracy: 0.984375, Computation time: 1.054025411605835\n",
      "Step: 6477, Loss: 0.9159562587738037, Accuracy: 1.0, Computation time: 1.3614683151245117\n",
      "Step: 6478, Loss: 0.9163979887962341, Accuracy: 1.0, Computation time: 1.1986744403839111\n",
      "Step: 6479, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.2828433513641357\n",
      "Step: 6480, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 1.1484770774841309\n",
      "Step: 6481, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 1.178013801574707\n",
      "Step: 6482, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 1.0031585693359375\n",
      "Step: 6483, Loss: 0.9159116148948669, Accuracy: 1.0, Computation time: 1.6445677280426025\n",
      "Step: 6484, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.334719181060791\n",
      "Step: 6485, Loss: 0.9376556873321533, Accuracy: 0.984375, Computation time: 1.5464088916778564\n",
      "Step: 6486, Loss: 0.9375813007354736, Accuracy: 0.9722222089767456, Computation time: 1.1690332889556885\n",
      "Step: 6487, Loss: 0.9504494071006775, Accuracy: 0.9494949579238892, Computation time: 1.2933869361877441\n",
      "Step: 6488, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.3618533611297607\n",
      "Step: 6489, Loss: 0.9159597754478455, Accuracy: 1.0, Computation time: 1.314384937286377\n",
      "Step: 6490, Loss: 0.9160910844802856, Accuracy: 1.0, Computation time: 1.3078384399414062\n",
      "Step: 6491, Loss: 0.9164485335350037, Accuracy: 1.0, Computation time: 1.5428237915039062\n",
      "Step: 6492, Loss: 0.9166092276573181, Accuracy: 1.0, Computation time: 1.5275845527648926\n",
      "Step: 6493, Loss: 0.9376701712608337, Accuracy: 0.875, Computation time: 1.4101526737213135\n",
      "Step: 6494, Loss: 0.932524561882019, Accuracy: 0.9642857313156128, Computation time: 1.4050207138061523\n",
      "Step: 6495, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 1.2475173473358154\n",
      "Step: 6496, Loss: 0.9377475380897522, Accuracy: 0.9750000238418579, Computation time: 1.5848731994628906\n",
      "Step: 6497, Loss: 0.9167791604995728, Accuracy: 1.0, Computation time: 1.2820463180541992\n",
      "Step: 6498, Loss: 0.9167360067367554, Accuracy: 1.0, Computation time: 2.3145740032196045\n",
      "Step: 6499, Loss: 0.938226580619812, Accuracy: 0.9821428656578064, Computation time: 1.5477643013000488\n",
      "Step: 6500, Loss: 0.9164998531341553, Accuracy: 1.0, Computation time: 1.1799118518829346\n",
      "Step: 6501, Loss: 0.9159578084945679, Accuracy: 1.0, Computation time: 1.5789175033569336\n",
      "Step: 6502, Loss: 0.9377721548080444, Accuracy: 0.9642857313156128, Computation time: 1.469698429107666\n",
      "Step: 6503, Loss: 0.9159541726112366, Accuracy: 1.0, Computation time: 1.2178456783294678\n",
      "Step: 6504, Loss: 0.9179500341415405, Accuracy: 1.0, Computation time: 1.5764172077178955\n",
      "Step: 6505, Loss: 0.9161099791526794, Accuracy: 1.0, Computation time: 1.2818539142608643\n",
      "Step: 6506, Loss: 0.9160996675491333, Accuracy: 1.0, Computation time: 1.3237338066101074\n",
      "Step: 6507, Loss: 0.9161927103996277, Accuracy: 1.0, Computation time: 1.3659276962280273\n",
      "Step: 6508, Loss: 0.9161497950553894, Accuracy: 1.0, Computation time: 1.5528850555419922\n",
      "Step: 6509, Loss: 0.9159479141235352, Accuracy: 1.0, Computation time: 1.51596999168396\n",
      "Step: 6510, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 1.5977449417114258\n",
      "Step: 6511, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.7651772499084473\n",
      "Step: 6512, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.9560809135437012\n",
      "Step: 6513, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.3952734470367432\n",
      "Step: 6514, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.589287519454956\n",
      "Step: 6515, Loss: 0.9159348011016846, Accuracy: 1.0, Computation time: 1.5867648124694824\n",
      "Step: 6516, Loss: 0.9159455895423889, Accuracy: 1.0, Computation time: 1.6592645645141602\n",
      "Step: 6517, Loss: 0.9159916043281555, Accuracy: 1.0, Computation time: 1.7473564147949219\n",
      "Step: 6518, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.746797800064087\n",
      "Step: 6519, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 1.6214072704315186\n",
      "Step: 6520, Loss: 0.9166719317436218, Accuracy: 1.0, Computation time: 1.6630222797393799\n",
      "Step: 6521, Loss: 0.915966272354126, Accuracy: 1.0, Computation time: 1.574289083480835\n",
      "Step: 6522, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 1.8946681022644043\n",
      "Step: 6523, Loss: 0.9158377051353455, Accuracy: 1.0, Computation time: 1.7065160274505615\n",
      "Step: 6524, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.696112871170044\n",
      "Step: 6525, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.486964225769043\n",
      "Step: 6526, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 1.9181365966796875\n",
      "Step: 6527, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 2.1133244037628174\n",
      "Step: 6528, Loss: 0.936875581741333, Accuracy: 0.9821428656578064, Computation time: 1.6316065788269043\n",
      "Step: 6529, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 1.6760966777801514\n",
      "Step: 6530, Loss: 0.9162442088127136, Accuracy: 1.0, Computation time: 1.568615436553955\n",
      "Step: 6531, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 1.3402762413024902\n",
      "########################\n",
      "Test loss: 1.1205635070800781, Test Accuracy_epoch47: 0.6997009515762329\n",
      "########################\n",
      "Step: 6532, Loss: 0.9158369898796082, Accuracy: 1.0, Computation time: 1.8562655448913574\n",
      "Step: 6533, Loss: 0.9158381819725037, Accuracy: 1.0, Computation time: 1.9512388706207275\n",
      "Step: 6534, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.952486276626587\n",
      "Step: 6535, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 1.7804927825927734\n",
      "Step: 6536, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 2.1028168201446533\n",
      "Step: 6537, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 1.8434150218963623\n",
      "Step: 6538, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 1.7225658893585205\n",
      "Step: 6539, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.6607530117034912\n",
      "Step: 6540, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 2.325563669204712\n",
      "Step: 6541, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 1.9342772960662842\n",
      "Step: 6542, Loss: 0.9158321619033813, Accuracy: 1.0, Computation time: 1.7064967155456543\n",
      "Step: 6543, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.8524222373962402\n",
      "Step: 6544, Loss: 0.9310049414634705, Accuracy: 0.9722222089767456, Computation time: 2.537411689758301\n",
      "Step: 6545, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 1.6309795379638672\n",
      "Step: 6546, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.5847554206848145\n",
      "Step: 6547, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 1.3338623046875\n",
      "Step: 6548, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.2700870037078857\n",
      "Step: 6549, Loss: 0.9377054572105408, Accuracy: 0.9722222089767456, Computation time: 1.5806522369384766\n",
      "Step: 6550, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 1.3179233074188232\n",
      "Step: 6551, Loss: 0.9375280737876892, Accuracy: 0.9750000238418579, Computation time: 1.47403883934021\n",
      "Step: 6552, Loss: 0.9250736832618713, Accuracy: 1.0, Computation time: 2.217716932296753\n",
      "Step: 6553, Loss: 0.9374872446060181, Accuracy: 0.9791666865348816, Computation time: 1.3609516620635986\n",
      "Step: 6554, Loss: 0.9376290440559387, Accuracy: 0.9807692766189575, Computation time: 1.505225419998169\n",
      "Step: 6555, Loss: 0.9169882535934448, Accuracy: 1.0, Computation time: 1.7123138904571533\n",
      "Step: 6556, Loss: 0.9375647902488708, Accuracy: 0.96875, Computation time: 1.5754306316375732\n",
      "Step: 6557, Loss: 0.916519284248352, Accuracy: 1.0, Computation time: 1.565589427947998\n",
      "Step: 6558, Loss: 0.9162887930870056, Accuracy: 1.0, Computation time: 1.3705580234527588\n",
      "Step: 6559, Loss: 0.937934398651123, Accuracy: 0.9807692766189575, Computation time: 1.3008880615234375\n",
      "Step: 6560, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 1.0542850494384766\n",
      "Step: 6561, Loss: 0.916735053062439, Accuracy: 1.0, Computation time: 1.2689540386199951\n",
      "Step: 6562, Loss: 0.9183858036994934, Accuracy: 1.0, Computation time: 1.6030735969543457\n",
      "Step: 6563, Loss: 0.9377778172492981, Accuracy: 0.9722222089767456, Computation time: 1.171978235244751\n",
      "Step: 6564, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.1574130058288574\n",
      "Step: 6565, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 1.3971812725067139\n",
      "Step: 6566, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 1.0917177200317383\n",
      "Step: 6567, Loss: 0.9163727164268494, Accuracy: 1.0, Computation time: 1.2460458278656006\n",
      "Step: 6568, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 2.026946783065796\n",
      "Step: 6569, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 1.3651432991027832\n",
      "Step: 6570, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 1.0847651958465576\n",
      "Step: 6571, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.1177258491516113\n",
      "Step: 6572, Loss: 0.9161009192466736, Accuracy: 1.0, Computation time: 1.387282371520996\n",
      "Step: 6573, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 1.180833101272583\n",
      "Step: 6574, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 1.2438735961914062\n",
      "Step: 6575, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 1.117194652557373\n",
      "Step: 6576, Loss: 0.9159104228019714, Accuracy: 1.0, Computation time: 1.315392255783081\n",
      "Step: 6577, Loss: 0.9158933162689209, Accuracy: 1.0, Computation time: 1.4732255935668945\n",
      "Step: 6578, Loss: 0.9159517884254456, Accuracy: 1.0, Computation time: 1.4566116333007812\n",
      "Step: 6579, Loss: 0.9164521098136902, Accuracy: 1.0, Computation time: 1.2018041610717773\n",
      "Step: 6580, Loss: 0.916439414024353, Accuracy: 1.0, Computation time: 1.2433393001556396\n",
      "Step: 6581, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 0.9998390674591064\n",
      "Step: 6582, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.3929047584533691\n",
      "Step: 6583, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 1.0524437427520752\n",
      "Step: 6584, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 1.3031303882598877\n",
      "Step: 6585, Loss: 0.9376266002655029, Accuracy: 0.9750000238418579, Computation time: 0.997525691986084\n",
      "Step: 6586, Loss: 0.9376215934753418, Accuracy: 0.9642857313156128, Computation time: 0.8779430389404297\n",
      "Step: 6587, Loss: 0.9375206828117371, Accuracy: 0.9791666865348816, Computation time: 1.1441829204559326\n",
      "Step: 6588, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.0571768283843994\n",
      "Step: 6589, Loss: 0.9374549388885498, Accuracy: 0.9375, Computation time: 1.088789463043213\n",
      "Step: 6590, Loss: 0.937597393989563, Accuracy: 0.9642857313156128, Computation time: 1.2317137718200684\n",
      "Step: 6591, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.08803129196167\n",
      "Step: 6592, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.038788080215454\n",
      "Step: 6593, Loss: 0.9375784397125244, Accuracy: 0.9642857313156128, Computation time: 0.961714506149292\n",
      "Step: 6594, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.1069064140319824\n",
      "Step: 6595, Loss: 0.9162684082984924, Accuracy: 1.0, Computation time: 1.0843193531036377\n",
      "Step: 6596, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.9948186874389648\n",
      "Step: 6597, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 1.0932257175445557\n",
      "Step: 6598, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.3360176086425781\n",
      "Step: 6599, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 1.4116847515106201\n",
      "Step: 6600, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.3716003894805908\n",
      "Step: 6601, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.2945361137390137\n",
      "Step: 6602, Loss: 0.936296284198761, Accuracy: 0.9750000238418579, Computation time: 1.0607917308807373\n",
      "Step: 6603, Loss: 0.9161291718482971, Accuracy: 1.0, Computation time: 0.990729808807373\n",
      "Step: 6604, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.3185594081878662\n",
      "Step: 6605, Loss: 0.9209120869636536, Accuracy: 1.0, Computation time: 1.370490550994873\n",
      "Step: 6606, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.1605165004730225\n",
      "Step: 6607, Loss: 0.932723879814148, Accuracy: 0.9722222089767456, Computation time: 1.8915231227874756\n",
      "Step: 6608, Loss: 0.9160608649253845, Accuracy: 1.0, Computation time: 1.1030054092407227\n",
      "Step: 6609, Loss: 0.9161453247070312, Accuracy: 1.0, Computation time: 1.245323657989502\n",
      "Step: 6610, Loss: 0.9160381555557251, Accuracy: 1.0, Computation time: 1.1356449127197266\n",
      "Step: 6611, Loss: 0.9164230823516846, Accuracy: 1.0, Computation time: 0.9561889171600342\n",
      "Step: 6612, Loss: 0.9159737229347229, Accuracy: 1.0, Computation time: 1.073944330215454\n",
      "Step: 6613, Loss: 0.915927529335022, Accuracy: 1.0, Computation time: 1.1055500507354736\n",
      "Step: 6614, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 1.0384166240692139\n",
      "Step: 6615, Loss: 0.9413026571273804, Accuracy: 0.9642857313156128, Computation time: 1.3061163425445557\n",
      "Step: 6616, Loss: 0.9243600368499756, Accuracy: 1.0, Computation time: 1.4747841358184814\n",
      "Step: 6617, Loss: 0.9159768223762512, Accuracy: 1.0, Computation time: 1.1003518104553223\n",
      "Step: 6618, Loss: 0.9160007834434509, Accuracy: 1.0, Computation time: 1.3604795932769775\n",
      "Step: 6619, Loss: 0.9308863282203674, Accuracy: 0.9642857313156128, Computation time: 1.4259569644927979\n",
      "Step: 6620, Loss: 0.9377477765083313, Accuracy: 0.9583333730697632, Computation time: 1.1614906787872314\n",
      "Step: 6621, Loss: 0.9376109838485718, Accuracy: 0.9791666865348816, Computation time: 1.153662919998169\n",
      "Step: 6622, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 1.1355676651000977\n",
      "Step: 6623, Loss: 0.9159272909164429, Accuracy: nan, Computation time: 1.2697722911834717\n",
      "Step: 6624, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 1.1562504768371582\n",
      "Step: 6625, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 1.3438754081726074\n",
      "Step: 6626, Loss: 0.9159581661224365, Accuracy: 1.0, Computation time: 1.1662883758544922\n",
      "Step: 6627, Loss: 0.9159488677978516, Accuracy: 1.0, Computation time: 1.093048095703125\n",
      "Step: 6628, Loss: 0.9159759283065796, Accuracy: 1.0, Computation time: 1.2279584407806396\n",
      "Step: 6629, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 1.1493313312530518\n",
      "Step: 6630, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 1.386291742324829\n",
      "Step: 6631, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.4306633472442627\n",
      "Step: 6632, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 1.0812010765075684\n",
      "Step: 6633, Loss: 0.9373354911804199, Accuracy: 0.984375, Computation time: 1.187082052230835\n",
      "Step: 6634, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.2380805015563965\n",
      "Step: 6635, Loss: 0.9376052618026733, Accuracy: 0.9772727489471436, Computation time: 1.1830153465270996\n",
      "Step: 6636, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 1.0912513732910156\n",
      "Step: 6637, Loss: 0.916496753692627, Accuracy: 1.0, Computation time: 1.0187406539916992\n",
      "Step: 6638, Loss: 0.9374820590019226, Accuracy: 0.96875, Computation time: 1.1611402034759521\n",
      "Step: 6639, Loss: 0.9171282052993774, Accuracy: 1.0, Computation time: 1.619591236114502\n",
      "Step: 6640, Loss: 0.9370180368423462, Accuracy: 0.96875, Computation time: 1.119640827178955\n",
      "Step: 6641, Loss: 0.9159365296363831, Accuracy: 1.0, Computation time: 1.0769269466400146\n",
      "Step: 6642, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.2271945476531982\n",
      "Step: 6643, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.4466817378997803\n",
      "Step: 6644, Loss: 0.9160681962966919, Accuracy: 1.0, Computation time: 1.230485200881958\n",
      "Step: 6645, Loss: 0.9162726402282715, Accuracy: 1.0, Computation time: 1.203233242034912\n",
      "Step: 6646, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.1057426929473877\n",
      "Step: 6647, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 1.2151272296905518\n",
      "Step: 6648, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 1.1447646617889404\n",
      "Step: 6649, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 1.2950263023376465\n",
      "Step: 6650, Loss: 0.9161590337753296, Accuracy: 1.0, Computation time: 1.1256115436553955\n",
      "Step: 6651, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.2024953365325928\n",
      "Step: 6652, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 1.093432903289795\n",
      "Step: 6653, Loss: 0.9159330129623413, Accuracy: 1.0, Computation time: 1.0954453945159912\n",
      "Step: 6654, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.1776189804077148\n",
      "Step: 6655, Loss: 0.9158459305763245, Accuracy: 1.0, Computation time: 1.1706101894378662\n",
      "Step: 6656, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 1.1444191932678223\n",
      "Step: 6657, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.057305097579956\n",
      "Step: 6658, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 1.0998141765594482\n",
      "Step: 6659, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 1.0263195037841797\n",
      "Step: 6660, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 1.051121473312378\n",
      "Step: 6661, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 1.0852060317993164\n",
      "Step: 6662, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 1.193406581878662\n",
      "Step: 6663, Loss: 0.9158432483673096, Accuracy: 1.0, Computation time: 0.9687061309814453\n",
      "Step: 6664, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.2455081939697266\n",
      "Step: 6665, Loss: 0.9160295128822327, Accuracy: 1.0, Computation time: 1.1548240184783936\n",
      "Step: 6666, Loss: 0.9158450365066528, Accuracy: 1.0, Computation time: 1.0798003673553467\n",
      "Step: 6667, Loss: 0.9158411026000977, Accuracy: 1.0, Computation time: 1.0573163032531738\n",
      "Step: 6668, Loss: 0.9159563183784485, Accuracy: 1.0, Computation time: 1.2743396759033203\n",
      "Step: 6669, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 1.3787574768066406\n",
      "########################\n",
      "Test loss: 1.1149214506149292, Test Accuracy_epoch48: 0.7095714211463928\n",
      "########################\n",
      "Step: 6670, Loss: 0.9182459712028503, Accuracy: 1.0, Computation time: 1.6213338375091553\n",
      "Step: 6671, Loss: 0.937446653842926, Accuracy: 0.9642857313156128, Computation time: 1.2101662158966064\n",
      "Step: 6672, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.1476900577545166\n",
      "Step: 6673, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 1.4800777435302734\n",
      "Step: 6674, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 0.984321117401123\n",
      "Step: 6675, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 1.0567314624786377\n",
      "Step: 6676, Loss: 0.9277533292770386, Accuracy: 0.9642857313156128, Computation time: 1.234816312789917\n",
      "Step: 6677, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 1.053368330001831\n",
      "Step: 6678, Loss: 0.9374896287918091, Accuracy: 0.9750000238418579, Computation time: 1.1031875610351562\n",
      "Step: 6679, Loss: 0.9159345626831055, Accuracy: 1.0, Computation time: 1.075277328491211\n",
      "Step: 6680, Loss: 0.9159899950027466, Accuracy: 1.0, Computation time: 1.1807060241699219\n",
      "Step: 6681, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.1662840843200684\n",
      "Step: 6682, Loss: 0.9159423112869263, Accuracy: 1.0, Computation time: 1.1146924495697021\n",
      "Step: 6683, Loss: 0.9212568998336792, Accuracy: 1.0, Computation time: 1.6619577407836914\n",
      "Step: 6684, Loss: 0.9159401059150696, Accuracy: 1.0, Computation time: 1.0433635711669922\n",
      "Step: 6685, Loss: 0.9159975051879883, Accuracy: 1.0, Computation time: 0.9830961227416992\n",
      "Step: 6686, Loss: 0.937687337398529, Accuracy: 0.9750000238418579, Computation time: 0.9121403694152832\n",
      "Step: 6687, Loss: 0.9211176633834839, Accuracy: 1.0, Computation time: 1.0922207832336426\n",
      "Step: 6688, Loss: 0.9374175071716309, Accuracy: 0.9772727489471436, Computation time: 0.9250094890594482\n",
      "Step: 6689, Loss: 0.9159066081047058, Accuracy: 1.0, Computation time: 1.1512978076934814\n",
      "Step: 6690, Loss: 0.9159705638885498, Accuracy: 1.0, Computation time: 0.9051167964935303\n",
      "Step: 6691, Loss: 0.9159778356552124, Accuracy: 1.0, Computation time: 0.9778411388397217\n",
      "Step: 6692, Loss: 0.9159659743309021, Accuracy: 1.0, Computation time: 0.9657323360443115\n",
      "Step: 6693, Loss: 0.9159550070762634, Accuracy: 1.0, Computation time: 0.9609463214874268\n",
      "Step: 6694, Loss: 0.9159687161445618, Accuracy: 1.0, Computation time: 1.0061895847320557\n",
      "Step: 6695, Loss: 0.9158818125724792, Accuracy: 1.0, Computation time: 1.0334055423736572\n",
      "Step: 6696, Loss: 0.9160906076431274, Accuracy: 1.0, Computation time: 0.9396073818206787\n",
      "Step: 6697, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 0.9877686500549316\n",
      "Step: 6698, Loss: 0.9159384369850159, Accuracy: 1.0, Computation time: 1.092982292175293\n",
      "Step: 6699, Loss: 0.9160721898078918, Accuracy: 1.0, Computation time: 1.139709711074829\n",
      "Step: 6700, Loss: 0.9164607524871826, Accuracy: 1.0, Computation time: 0.9693493843078613\n",
      "Step: 6701, Loss: 0.9175059199333191, Accuracy: 1.0, Computation time: 1.0160675048828125\n",
      "Step: 6702, Loss: 0.9160138964653015, Accuracy: 1.0, Computation time: 1.1028268337249756\n",
      "Step: 6703, Loss: 0.9376690983772278, Accuracy: 0.9807692766189575, Computation time: 1.2694878578186035\n",
      "Step: 6704, Loss: 0.9376822113990784, Accuracy: 0.96875, Computation time: 0.8667869567871094\n",
      "Step: 6705, Loss: 0.9159552454948425, Accuracy: 1.0, Computation time: 0.9823153018951416\n",
      "Step: 6706, Loss: 0.9251615405082703, Accuracy: 1.0, Computation time: 1.24123215675354\n",
      "Step: 6707, Loss: 0.9159347414970398, Accuracy: 1.0, Computation time: 1.103731393814087\n",
      "Step: 6708, Loss: 0.9370408654212952, Accuracy: 0.9722222089767456, Computation time: 1.070427417755127\n",
      "Step: 6709, Loss: 0.9162772297859192, Accuracy: 1.0, Computation time: 0.9442720413208008\n",
      "Step: 6710, Loss: 0.9160309433937073, Accuracy: 1.0, Computation time: 0.9492747783660889\n",
      "Step: 6711, Loss: 0.9161786437034607, Accuracy: 1.0, Computation time: 0.9215526580810547\n",
      "Step: 6712, Loss: 0.9159904718399048, Accuracy: 1.0, Computation time: 1.366567850112915\n",
      "Step: 6713, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 0.8859603404998779\n",
      "Step: 6714, Loss: 0.9160076379776001, Accuracy: 1.0, Computation time: 0.9652059078216553\n",
      "Step: 6715, Loss: 0.9163526892662048, Accuracy: 1.0, Computation time: 1.6376047134399414\n",
      "Step: 6716, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.0491302013397217\n",
      "Step: 6717, Loss: 0.917495608329773, Accuracy: 1.0, Computation time: 0.9082052707672119\n",
      "Step: 6718, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 1.2204937934875488\n",
      "Step: 6719, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 0.9349946975708008\n",
      "Step: 6720, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 0.9818799495697021\n",
      "Step: 6721, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.057729959487915\n",
      "Step: 6722, Loss: 0.9378344416618347, Accuracy: 0.96875, Computation time: 1.1482527256011963\n",
      "Step: 6723, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 1.0727014541625977\n",
      "Step: 6724, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 0.9582552909851074\n",
      "Step: 6725, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 0.8784341812133789\n",
      "Step: 6726, Loss: 0.9375565052032471, Accuracy: 0.9821428656578064, Computation time: 0.9973511695861816\n",
      "Step: 6727, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 0.8881702423095703\n",
      "Step: 6728, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 1.0311353206634521\n",
      "Step: 6729, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 0.8752706050872803\n",
      "Step: 6730, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 1.1512916088104248\n",
      "Step: 6731, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 0.9309980869293213\n",
      "Step: 6732, Loss: 0.9161736965179443, Accuracy: 1.0, Computation time: 1.0027642250061035\n",
      "Step: 6733, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 1.0158939361572266\n",
      "Step: 6734, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 0.8557958602905273\n",
      "Step: 6735, Loss: 0.9377639889717102, Accuracy: 0.9750000238418579, Computation time: 1.2036988735198975\n",
      "Step: 6736, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 0.9649913311004639\n",
      "Step: 6737, Loss: 0.9160856604576111, Accuracy: 1.0, Computation time: 1.1247820854187012\n",
      "Step: 6738, Loss: 0.937593936920166, Accuracy: 0.9821428656578064, Computation time: 1.3662075996398926\n",
      "Step: 6739, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.6286823749542236\n",
      "Step: 6740, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 1.1632962226867676\n",
      "Step: 6741, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 0.9571585655212402\n",
      "Step: 6742, Loss: 0.937032163143158, Accuracy: 0.9791666865348816, Computation time: 0.9593634605407715\n",
      "Step: 6743, Loss: 0.917025625705719, Accuracy: 1.0, Computation time: 1.017103910446167\n",
      "Step: 6744, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 0.9643998146057129\n",
      "Step: 6745, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.167837142944336\n",
      "Step: 6746, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 1.1082191467285156\n",
      "Step: 6747, Loss: 0.9159742593765259, Accuracy: 1.0, Computation time: 1.0292952060699463\n",
      "Step: 6748, Loss: 0.9375272393226624, Accuracy: 0.9722222089767456, Computation time: 0.8232765197753906\n",
      "Step: 6749, Loss: 0.9375501275062561, Accuracy: 0.9722222089767456, Computation time: 1.0434215068817139\n",
      "Step: 6750, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 0.9152135848999023\n",
      "Step: 6751, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 0.9137606620788574\n",
      "Step: 6752, Loss: 0.9375113248825073, Accuracy: 0.9750000238418579, Computation time: 1.3116698265075684\n",
      "Step: 6753, Loss: 0.915857195854187, Accuracy: 1.0, Computation time: 0.8380904197692871\n",
      "Step: 6754, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 0.9595270156860352\n",
      "Step: 6755, Loss: 0.9164409041404724, Accuracy: 1.0, Computation time: 1.1854379177093506\n",
      "Step: 6756, Loss: 0.9375556111335754, Accuracy: 0.9583333730697632, Computation time: 0.948009729385376\n",
      "Step: 6757, Loss: 0.9158433079719543, Accuracy: 1.0, Computation time: 0.9411687850952148\n",
      "Step: 6758, Loss: 0.9158385396003723, Accuracy: 1.0, Computation time: 0.9814310073852539\n",
      "Step: 6759, Loss: 0.9158340096473694, Accuracy: 1.0, Computation time: 0.9478011131286621\n",
      "Step: 6760, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 1.2857317924499512\n",
      "Step: 6761, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 1.0268795490264893\n",
      "Step: 6762, Loss: 0.9195654988288879, Accuracy: 1.0, Computation time: 1.4577903747558594\n",
      "Step: 6763, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 0.9139523506164551\n",
      "Step: 6764, Loss: 0.9375709295272827, Accuracy: 0.9750000238418579, Computation time: 1.0364813804626465\n",
      "Step: 6765, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.1628813743591309\n",
      "Step: 6766, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 0.9202642440795898\n",
      "Step: 6767, Loss: 0.937620222568512, Accuracy: 0.9791666865348816, Computation time: 0.8979232311248779\n",
      "Step: 6768, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 0.9811301231384277\n",
      "Step: 6769, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 0.7779474258422852\n",
      "Step: 6770, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 1.0089263916015625\n",
      "Step: 6771, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 1.2959949970245361\n",
      "Step: 6772, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 0.9803972244262695\n",
      "Step: 6773, Loss: 0.9374752640724182, Accuracy: 0.9642857313156128, Computation time: 0.9241371154785156\n",
      "Step: 6774, Loss: 0.9482875466346741, Accuracy: 0.922619104385376, Computation time: 1.4312562942504883\n",
      "Step: 6775, Loss: 0.915910005569458, Accuracy: 1.0, Computation time: 1.6957802772521973\n",
      "Step: 6776, Loss: 0.9174363613128662, Accuracy: 1.0, Computation time: 0.8331568241119385\n",
      "Step: 6777, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 0.9570660591125488\n",
      "Step: 6778, Loss: 0.9159526228904724, Accuracy: 1.0, Computation time: 1.2909185886383057\n",
      "Step: 6779, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 1.0030004978179932\n",
      "Step: 6780, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 0.9541163444519043\n",
      "Step: 6781, Loss: 0.9414389133453369, Accuracy: 0.9722222089767456, Computation time: 1.2752923965454102\n",
      "Step: 6782, Loss: 0.9378215074539185, Accuracy: 0.96875, Computation time: 1.4923973083496094\n",
      "Step: 6783, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 0.9922864437103271\n",
      "Step: 6784, Loss: 0.9158977270126343, Accuracy: 1.0, Computation time: 0.9592382907867432\n",
      "Step: 6785, Loss: 0.9159089922904968, Accuracy: 1.0, Computation time: 0.9570302963256836\n",
      "Step: 6786, Loss: 0.9159427881240845, Accuracy: 1.0, Computation time: 1.1074774265289307\n",
      "Step: 6787, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.2071526050567627\n",
      "Step: 6788, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 1.44856858253479\n",
      "Step: 6789, Loss: 0.9167214632034302, Accuracy: 1.0, Computation time: 1.2273216247558594\n",
      "Step: 6790, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 0.9760510921478271\n",
      "Step: 6791, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 0.9357883930206299\n",
      "Step: 6792, Loss: 0.9158915877342224, Accuracy: 1.0, Computation time: 1.2512037754058838\n",
      "Step: 6793, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.1729049682617188\n",
      "Step: 6794, Loss: 0.9160184860229492, Accuracy: 1.0, Computation time: 1.1119003295898438\n",
      "Step: 6795, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 1.1720964908599854\n",
      "Step: 6796, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.2480878829956055\n",
      "Step: 6797, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 1.2187120914459229\n",
      "Step: 6798, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 1.327829122543335\n",
      "Step: 6799, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.0872607231140137\n",
      "Step: 6800, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.2631182670593262\n",
      "Step: 6801, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 0.9888949394226074\n",
      "Step: 6802, Loss: 0.9210909008979797, Accuracy: 1.0, Computation time: 1.6062111854553223\n",
      "Step: 6803, Loss: 0.9159017205238342, Accuracy: 1.0, Computation time: 0.984619140625\n",
      "Step: 6804, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 1.0250399112701416\n",
      "Step: 6805, Loss: 0.9163633584976196, Accuracy: 1.0, Computation time: 1.026228666305542\n",
      "Step: 6806, Loss: 0.916038990020752, Accuracy: 1.0, Computation time: 1.0092809200286865\n",
      "Step: 6807, Loss: 0.9376765489578247, Accuracy: 0.9791666865348816, Computation time: 0.9757885932922363\n",
      "Step: 6808, Loss: 0.9191362857818604, Accuracy: 1.0, Computation time: 1.3504235744476318\n",
      "########################\n",
      "Test loss: 1.1212170124053955, Test Accuracy_epoch49: 0.7020488977432251\n",
      "########################\n",
      "Step: 6809, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 1.4421513080596924\n",
      "Step: 6810, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 1.0437161922454834\n",
      "Step: 6811, Loss: 0.9573266506195068, Accuracy: 0.9285714626312256, Computation time: 1.3805606365203857\n",
      "Step: 6812, Loss: 0.9161373972892761, Accuracy: 1.0, Computation time: 1.3128561973571777\n",
      "Step: 6813, Loss: 0.9164482355117798, Accuracy: 1.0, Computation time: 0.8144962787628174\n",
      "Step: 6814, Loss: 0.9162859916687012, Accuracy: 1.0, Computation time: 1.2548918724060059\n",
      "Step: 6815, Loss: 0.9162209630012512, Accuracy: 1.0, Computation time: 1.1222877502441406\n",
      "Step: 6816, Loss: 0.9160251617431641, Accuracy: 1.0, Computation time: 1.2798902988433838\n",
      "Step: 6817, Loss: 0.9159936904907227, Accuracy: 1.0, Computation time: 1.2886426448822021\n",
      "Step: 6818, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 0.9878067970275879\n",
      "Step: 6819, Loss: 0.9159121513366699, Accuracy: 1.0, Computation time: 1.034843921661377\n",
      "Step: 6820, Loss: 0.9165825247764587, Accuracy: 1.0, Computation time: 1.0964031219482422\n",
      "Step: 6821, Loss: 0.915916383266449, Accuracy: 1.0, Computation time: 1.1652133464813232\n",
      "Step: 6822, Loss: 0.9160105586051941, Accuracy: 1.0, Computation time: 1.0429925918579102\n",
      "Step: 6823, Loss: 0.9393808841705322, Accuracy: 0.9722222089767456, Computation time: 0.9706745147705078\n",
      "Step: 6824, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.5548391342163086\n",
      "Step: 6825, Loss: 0.9159130454063416, Accuracy: 1.0, Computation time: 0.8954110145568848\n",
      "Step: 6826, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.1196329593658447\n",
      "Step: 6827, Loss: 0.916027307510376, Accuracy: 1.0, Computation time: 1.214214563369751\n",
      "Step: 6828, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 1.2686951160430908\n",
      "Step: 6829, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 0.8458676338195801\n",
      "Step: 6830, Loss: 0.9376405477523804, Accuracy: 0.9750000238418579, Computation time: 0.9550600051879883\n",
      "Step: 6831, Loss: 0.9375560283660889, Accuracy: 0.9750000238418579, Computation time: 1.283226728439331\n",
      "Step: 6832, Loss: 0.9216326475143433, Accuracy: 1.0, Computation time: 1.7115871906280518\n",
      "Step: 6833, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 0.9713988304138184\n",
      "Step: 6834, Loss: 0.9158967137336731, Accuracy: 1.0, Computation time: 0.8790674209594727\n",
      "Step: 6835, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 1.2731971740722656\n",
      "Step: 6836, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 1.5164210796356201\n",
      "Step: 6837, Loss: 0.9159575700759888, Accuracy: 1.0, Computation time: 1.4002671241760254\n",
      "Step: 6838, Loss: 0.9159342646598816, Accuracy: 1.0, Computation time: 1.1782286167144775\n",
      "Step: 6839, Loss: 0.9159016013145447, Accuracy: 1.0, Computation time: 1.1129670143127441\n",
      "Step: 6840, Loss: 0.9167726039886475, Accuracy: 1.0, Computation time: 1.37925386428833\n",
      "Step: 6841, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 0.9976086616516113\n",
      "Step: 6842, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 0.9945855140686035\n",
      "Step: 6843, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.088998794555664\n",
      "Step: 6844, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 1.3793017864227295\n",
      "Step: 6845, Loss: 0.9267500638961792, Accuracy: 0.9722222089767456, Computation time: 1.2309794425964355\n",
      "Step: 6846, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.224027156829834\n",
      "Step: 6847, Loss: 0.9376529455184937, Accuracy: 0.9583333730697632, Computation time: 1.3378214836120605\n",
      "Step: 6848, Loss: 0.9159181714057922, Accuracy: 1.0, Computation time: 1.126424789428711\n",
      "Step: 6849, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.0291266441345215\n",
      "Step: 6850, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 1.1125507354736328\n",
      "Step: 6851, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.182065725326538\n",
      "Step: 6852, Loss: 0.916782021522522, Accuracy: 1.0, Computation time: 1.2576804161071777\n",
      "Step: 6853, Loss: 0.9168461561203003, Accuracy: 1.0, Computation time: 2.126580238342285\n",
      "Step: 6854, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.091768503189087\n",
      "Step: 6855, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 1.0362763404846191\n",
      "Step: 6856, Loss: 0.9159232974052429, Accuracy: 1.0, Computation time: 0.9604263305664062\n",
      "Step: 6857, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.2418839931488037\n",
      "Step: 6858, Loss: 0.9159945845603943, Accuracy: 1.0, Computation time: 1.412710428237915\n",
      "Step: 6859, Loss: 0.9404645562171936, Accuracy: 0.9772727489471436, Computation time: 1.1443791389465332\n",
      "Step: 6860, Loss: 0.9160358905792236, Accuracy: 1.0, Computation time: 1.3842341899871826\n",
      "Step: 6861, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 1.3438172340393066\n",
      "Step: 6862, Loss: 0.9377443790435791, Accuracy: 0.9722222089767456, Computation time: 1.4295287132263184\n",
      "Step: 6863, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.041964054107666\n",
      "Step: 6864, Loss: 0.9364385604858398, Accuracy: 0.9750000238418579, Computation time: 1.4399354457855225\n",
      "Step: 6865, Loss: 0.9377342462539673, Accuracy: 0.9852941036224365, Computation time: 1.2150301933288574\n",
      "Step: 6866, Loss: 0.9386977553367615, Accuracy: 0.9722222089767456, Computation time: 1.3343181610107422\n",
      "Step: 6867, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.0687522888183594\n",
      "Step: 6868, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.0387063026428223\n",
      "Step: 6869, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.1605026721954346\n",
      "Step: 6870, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 0.9544954299926758\n",
      "Step: 6871, Loss: 0.9159203767776489, Accuracy: 1.0, Computation time: 1.0572052001953125\n",
      "Step: 6872, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.0970540046691895\n",
      "Step: 6873, Loss: 0.9373196363449097, Accuracy: 0.9791666865348816, Computation time: 1.118114948272705\n",
      "Step: 6874, Loss: 0.9202615022659302, Accuracy: 1.0, Computation time: 1.6703996658325195\n",
      "Step: 6875, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 1.0954711437225342\n",
      "Step: 6876, Loss: 0.9159175157546997, Accuracy: 1.0, Computation time: 1.009843111038208\n",
      "Step: 6877, Loss: 0.9159839153289795, Accuracy: 1.0, Computation time: 1.0853650569915771\n",
      "Step: 6878, Loss: 0.915962278842926, Accuracy: 1.0, Computation time: 1.2574841976165771\n",
      "Step: 6879, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 1.1658213138580322\n",
      "Step: 6880, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 1.0047967433929443\n",
      "Step: 6881, Loss: 0.915874183177948, Accuracy: 1.0, Computation time: 0.9876410961151123\n",
      "Step: 6882, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.250063419342041\n",
      "Step: 6883, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.0414445400238037\n",
      "Step: 6884, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.1636664867401123\n",
      "Step: 6885, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 1.1598920822143555\n",
      "Step: 6886, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.005544662475586\n",
      "Step: 6887, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.1067945957183838\n",
      "Step: 6888, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.2469251155853271\n",
      "Step: 6889, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.0673561096191406\n",
      "Step: 6890, Loss: 0.9591273665428162, Accuracy: 0.9583333730697632, Computation time: 0.9875037670135498\n",
      "Step: 6891, Loss: 0.9375312924385071, Accuracy: 0.9750000238418579, Computation time: 1.0194435119628906\n",
      "Step: 6892, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 1.0654520988464355\n",
      "Step: 6893, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.149954080581665\n",
      "Step: 6894, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.0042822360992432\n",
      "Step: 6895, Loss: 0.918735921382904, Accuracy: 1.0, Computation time: 1.2733139991760254\n",
      "Step: 6896, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.4722685813903809\n",
      "Step: 6897, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.2093493938446045\n",
      "Step: 6898, Loss: 0.9375133514404297, Accuracy: 0.9772727489471436, Computation time: 1.551154613494873\n",
      "Step: 6899, Loss: 0.9159303307533264, Accuracy: 1.0, Computation time: 0.8975248336791992\n",
      "Step: 6900, Loss: 0.9159809350967407, Accuracy: 1.0, Computation time: 1.2741947174072266\n",
      "Step: 6901, Loss: 0.9159098267555237, Accuracy: 1.0, Computation time: 1.4780631065368652\n",
      "Step: 6902, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 1.3893187046051025\n",
      "Step: 6903, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 1.5389676094055176\n",
      "Step: 6904, Loss: 0.9158543944358826, Accuracy: 1.0, Computation time: 1.615570306777954\n",
      "Step: 6905, Loss: 0.9375808835029602, Accuracy: 0.9750000238418579, Computation time: 0.9337496757507324\n",
      "Step: 6906, Loss: 0.923457682132721, Accuracy: 1.0, Computation time: 1.6814067363739014\n",
      "Step: 6907, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 0.9114158153533936\n",
      "Step: 6908, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.0153589248657227\n",
      "Step: 6909, Loss: 0.9160914421081543, Accuracy: 1.0, Computation time: 1.161759853363037\n",
      "Step: 6910, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.3211593627929688\n",
      "Step: 6911, Loss: 0.9159746766090393, Accuracy: 1.0, Computation time: 1.101243257522583\n",
      "Step: 6912, Loss: 0.9375749826431274, Accuracy: 0.9791666865348816, Computation time: 1.2192060947418213\n",
      "Step: 6913, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.1655430793762207\n",
      "Step: 6914, Loss: 0.9160134792327881, Accuracy: 1.0, Computation time: 1.0145177841186523\n",
      "Step: 6915, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.0354502201080322\n",
      "Step: 6916, Loss: 0.9159691333770752, Accuracy: 1.0, Computation time: 1.0914194583892822\n",
      "Step: 6917, Loss: 0.9159306883811951, Accuracy: 1.0, Computation time: 1.182124376296997\n",
      "Step: 6918, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.1907258033752441\n",
      "Step: 6919, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.5867791175842285\n",
      "Step: 6920, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 1.2090675830841064\n",
      "Step: 6921, Loss: 0.9161514639854431, Accuracy: 1.0, Computation time: 1.2956483364105225\n",
      "Step: 6922, Loss: 0.9369456768035889, Accuracy: 0.9750000238418579, Computation time: 1.7786433696746826\n",
      "Step: 6923, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 1.1932528018951416\n",
      "Step: 6924, Loss: 0.9374527335166931, Accuracy: 0.9772727489471436, Computation time: 1.3126513957977295\n",
      "Step: 6925, Loss: 0.9380530714988708, Accuracy: 0.9166666865348816, Computation time: 1.1436376571655273\n",
      "Step: 6926, Loss: 0.9160109162330627, Accuracy: 1.0, Computation time: 1.2025127410888672\n",
      "Step: 6927, Loss: 0.9381369948387146, Accuracy: 0.9807692766189575, Computation time: 1.9272124767303467\n",
      "Step: 6928, Loss: 0.9169645309448242, Accuracy: 1.0, Computation time: 1.7840752601623535\n",
      "Step: 6929, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.275527000427246\n",
      "Step: 6930, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 1.3026552200317383\n",
      "Step: 6931, Loss: 0.9171702861785889, Accuracy: 1.0, Computation time: 1.413959264755249\n",
      "Step: 6932, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 1.4522333145141602\n",
      "Step: 6933, Loss: 0.9251832962036133, Accuracy: 1.0, Computation time: 2.1984965801239014\n",
      "Step: 6934, Loss: 0.9235572218894958, Accuracy: 1.0, Computation time: 1.4618852138519287\n",
      "Step: 6935, Loss: 0.9160133600234985, Accuracy: 1.0, Computation time: 1.0745458602905273\n",
      "Step: 6936, Loss: 0.9160505533218384, Accuracy: 1.0, Computation time: 1.1200592517852783\n",
      "Step: 6937, Loss: 0.9160609841346741, Accuracy: 1.0, Computation time: 1.0401811599731445\n",
      "Step: 6938, Loss: 0.9159753918647766, Accuracy: 1.0, Computation time: 0.9990358352661133\n",
      "Step: 6939, Loss: 0.9165399670600891, Accuracy: 1.0, Computation time: 1.4476344585418701\n",
      "Step: 6940, Loss: 0.9161422848701477, Accuracy: 1.0, Computation time: 1.0967133045196533\n",
      "Step: 6941, Loss: 0.916644811630249, Accuracy: 1.0, Computation time: 1.2039415836334229\n",
      "Step: 6942, Loss: 0.916009247303009, Accuracy: 1.0, Computation time: 0.9400360584259033\n",
      "Step: 6943, Loss: 0.9159943461418152, Accuracy: 1.0, Computation time: 0.9112069606781006\n",
      "Step: 6944, Loss: 0.9159970879554749, Accuracy: 1.0, Computation time: 0.9878942966461182\n",
      "Step: 6945, Loss: 0.9160382747650146, Accuracy: 1.0, Computation time: 0.8244855403900146\n",
      "Step: 6946, Loss: 0.9159650802612305, Accuracy: 1.0, Computation time: 0.9606735706329346\n",
      "Step: 6947, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.0416195392608643\n",
      "########################\n",
      "Test loss: 1.128717303276062, Test Accuracy_epoch50: 0.6906843185424805\n",
      "########################\n",
      "Step: 6948, Loss: 0.930933952331543, Accuracy: 0.9807692766189575, Computation time: 1.0846660137176514\n",
      "Step: 6949, Loss: 0.9160566329956055, Accuracy: 1.0, Computation time: 1.2966842651367188\n",
      "Step: 6950, Loss: 0.9159550666809082, Accuracy: 1.0, Computation time: 1.0102474689483643\n",
      "Step: 6951, Loss: 0.9159905910491943, Accuracy: 1.0, Computation time: 1.3135311603546143\n",
      "Step: 6952, Loss: 0.9377007484436035, Accuracy: 0.9583333730697632, Computation time: 1.6362028121948242\n",
      "Step: 6953, Loss: 0.9159579277038574, Accuracy: 1.0, Computation time: 1.280168056488037\n",
      "Step: 6954, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.1960368156433105\n",
      "Step: 6955, Loss: 0.9159002900123596, Accuracy: 1.0, Computation time: 1.0702035427093506\n",
      "Step: 6956, Loss: 0.9376234412193298, Accuracy: 0.9750000238418579, Computation time: 0.9506900310516357\n",
      "Step: 6957, Loss: 0.9375424981117249, Accuracy: 0.9833333492279053, Computation time: 0.9273099899291992\n",
      "Step: 6958, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 0.9232778549194336\n",
      "Step: 6959, Loss: 0.9158978462219238, Accuracy: 1.0, Computation time: 0.8698375225067139\n",
      "Step: 6960, Loss: 0.9159379601478577, Accuracy: 1.0, Computation time: 1.0384495258331299\n",
      "Step: 6961, Loss: 0.9265381693840027, Accuracy: 0.9722222089767456, Computation time: 2.2149720191955566\n",
      "Step: 6962, Loss: 0.9160373210906982, Accuracy: 1.0, Computation time: 0.8089520931243896\n",
      "Step: 6963, Loss: 0.916670560836792, Accuracy: 1.0, Computation time: 0.862776517868042\n",
      "Step: 6964, Loss: 0.9168068170547485, Accuracy: 1.0, Computation time: 0.9283459186553955\n",
      "Step: 6965, Loss: 0.9378480911254883, Accuracy: 0.9722222089767456, Computation time: 1.354748249053955\n",
      "Step: 6966, Loss: 0.9351176023483276, Accuracy: 0.9750000238418579, Computation time: 1.0791492462158203\n",
      "Step: 6967, Loss: 0.9169080257415771, Accuracy: 1.0, Computation time: 1.0440094470977783\n",
      "Step: 6968, Loss: 0.9167892932891846, Accuracy: 1.0, Computation time: 1.2960658073425293\n",
      "Step: 6969, Loss: 0.918398916721344, Accuracy: 1.0, Computation time: 1.4448127746582031\n",
      "Step: 6970, Loss: 0.9170602560043335, Accuracy: 1.0, Computation time: 0.9118413925170898\n",
      "Step: 6971, Loss: 0.9162871241569519, Accuracy: 1.0, Computation time: 0.9388275146484375\n",
      "Step: 6972, Loss: 0.9162535667419434, Accuracy: 1.0, Computation time: 1.0466015338897705\n",
      "Step: 6973, Loss: 0.9162192344665527, Accuracy: 1.0, Computation time: 1.123957633972168\n",
      "Step: 6974, Loss: 0.9390840530395508, Accuracy: 0.9791666865348816, Computation time: 1.4110941886901855\n",
      "Step: 6975, Loss: 0.9174073934555054, Accuracy: 1.0, Computation time: 1.0135161876678467\n",
      "Step: 6976, Loss: 0.918031632900238, Accuracy: 1.0, Computation time: 1.0762839317321777\n",
      "Step: 6977, Loss: 0.9164199829101562, Accuracy: 1.0, Computation time: 1.174339771270752\n",
      "Step: 6978, Loss: 0.9162786602973938, Accuracy: 1.0, Computation time: 1.135127305984497\n",
      "Step: 6979, Loss: 0.9163880944252014, Accuracy: 1.0, Computation time: 0.9947299957275391\n",
      "Step: 6980, Loss: 0.9175903797149658, Accuracy: 1.0, Computation time: 1.3703386783599854\n",
      "Step: 6981, Loss: 0.9166489243507385, Accuracy: 1.0, Computation time: 0.9549188613891602\n",
      "Step: 6982, Loss: 0.9173359870910645, Accuracy: 1.0, Computation time: 1.1584148406982422\n",
      "Step: 6983, Loss: 0.9163936972618103, Accuracy: 1.0, Computation time: 0.973599910736084\n",
      "Step: 6984, Loss: 0.9165247678756714, Accuracy: 1.0, Computation time: 1.0261857509613037\n",
      "Step: 6985, Loss: 0.9166975021362305, Accuracy: 1.0, Computation time: 0.9890930652618408\n",
      "Step: 6986, Loss: 0.9159753322601318, Accuracy: 1.0, Computation time: 1.045079231262207\n",
      "Step: 6987, Loss: 0.9160374402999878, Accuracy: 1.0, Computation time: 0.922433614730835\n",
      "Step: 6988, Loss: 0.9162692427635193, Accuracy: 1.0, Computation time: 0.9275457859039307\n",
      "Step: 6989, Loss: 0.9162936806678772, Accuracy: 1.0, Computation time: 1.0233666896820068\n",
      "Step: 6990, Loss: 0.9162488579750061, Accuracy: 1.0, Computation time: 1.1301369667053223\n",
      "Step: 6991, Loss: 0.9168102145195007, Accuracy: 1.0, Computation time: 1.4432570934295654\n",
      "Step: 6992, Loss: 0.9187650680541992, Accuracy: 1.0, Computation time: 1.8391695022583008\n",
      "Step: 6993, Loss: 0.915930986404419, Accuracy: 1.0, Computation time: 1.8084545135498047\n",
      "Step: 6994, Loss: 0.9174765348434448, Accuracy: 1.0, Computation time: 1.0379207134246826\n",
      "Step: 6995, Loss: 0.9379765391349792, Accuracy: 0.9642857313156128, Computation time: 1.2055079936981201\n",
      "Step: 6996, Loss: 0.9379891157150269, Accuracy: 0.9791666865348816, Computation time: 1.289158821105957\n",
      "Step: 6997, Loss: 0.9272794127464294, Accuracy: 0.9821428656578064, Computation time: 1.6882522106170654\n",
      "Step: 6998, Loss: 0.9164662957191467, Accuracy: 1.0, Computation time: 1.2885816097259521\n",
      "Step: 6999, Loss: 0.9164333343505859, Accuracy: 1.0, Computation time: 1.1764602661132812\n",
      "Step: 7000, Loss: 0.916265606880188, Accuracy: 1.0, Computation time: 1.0150177478790283\n",
      "Step: 7001, Loss: 0.9161728620529175, Accuracy: 1.0, Computation time: 0.8486342430114746\n",
      "Step: 7002, Loss: 0.9161280393600464, Accuracy: 1.0, Computation time: 0.9989898204803467\n",
      "Step: 7003, Loss: 0.9161994457244873, Accuracy: 1.0, Computation time: 1.1310853958129883\n",
      "Step: 7004, Loss: 0.9161882996559143, Accuracy: 1.0, Computation time: 1.2458584308624268\n",
      "Step: 7005, Loss: 0.9160151481628418, Accuracy: 1.0, Computation time: 1.1821973323822021\n",
      "Step: 7006, Loss: 0.9374222755432129, Accuracy: 0.96875, Computation time: 1.2424983978271484\n",
      "Step: 7007, Loss: 0.9159719944000244, Accuracy: 1.0, Computation time: 1.4437015056610107\n",
      "Step: 7008, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 1.0653986930847168\n",
      "Step: 7009, Loss: 0.9161231517791748, Accuracy: 1.0, Computation time: 1.102463722229004\n",
      "Step: 7010, Loss: 0.9167969226837158, Accuracy: 1.0, Computation time: 0.926274299621582\n",
      "Step: 7011, Loss: 0.9160133004188538, Accuracy: 1.0, Computation time: 1.624682903289795\n",
      "Step: 7012, Loss: 0.9369703531265259, Accuracy: 0.9772727489471436, Computation time: 1.0321409702301025\n",
      "Step: 7013, Loss: 0.93742436170578, Accuracy: 0.9583333730697632, Computation time: 1.0598430633544922\n",
      "Step: 7014, Loss: 0.9161046147346497, Accuracy: 1.0, Computation time: 1.0310592651367188\n",
      "Step: 7015, Loss: 0.9367478489875793, Accuracy: 0.96875, Computation time: 1.0979626178741455\n",
      "Step: 7016, Loss: 0.9160048365592957, Accuracy: 1.0, Computation time: 1.504155158996582\n",
      "Step: 7017, Loss: 0.9424903392791748, Accuracy: 0.9791666865348816, Computation time: 0.9595615863800049\n",
      "Step: 7018, Loss: 0.9159354567527771, Accuracy: 1.0, Computation time: 1.1212449073791504\n",
      "Step: 7019, Loss: 0.9225504398345947, Accuracy: 1.0, Computation time: 1.0760223865509033\n",
      "Step: 7020, Loss: 0.9375100135803223, Accuracy: 0.9642857313156128, Computation time: 1.0862689018249512\n",
      "Step: 7021, Loss: 0.9159841537475586, Accuracy: 1.0, Computation time: 1.1236631870269775\n",
      "Step: 7022, Loss: 0.9159917831420898, Accuracy: 1.0, Computation time: 0.9597911834716797\n",
      "Step: 7023, Loss: 0.9373368620872498, Accuracy: 0.9772727489471436, Computation time: 1.215303897857666\n",
      "Step: 7024, Loss: 0.915934145450592, Accuracy: 1.0, Computation time: 1.2257237434387207\n",
      "Step: 7025, Loss: 0.9521162509918213, Accuracy: 0.9513888955116272, Computation time: 1.535522699356079\n",
      "Step: 7026, Loss: 0.9164087176322937, Accuracy: 1.0, Computation time: 1.310809850692749\n",
      "Step: 7027, Loss: 0.9176660180091858, Accuracy: 1.0, Computation time: 1.2534453868865967\n",
      "Step: 7028, Loss: 0.9159780740737915, Accuracy: 1.0, Computation time: 1.2320895195007324\n",
      "Step: 7029, Loss: 0.9160491228103638, Accuracy: 1.0, Computation time: 1.2738666534423828\n",
      "Step: 7030, Loss: 0.9378330707550049, Accuracy: 0.9583333730697632, Computation time: 1.2905776500701904\n",
      "Step: 7031, Loss: 0.9381064772605896, Accuracy: 0.9166666865348816, Computation time: 1.1531193256378174\n",
      "Step: 7032, Loss: 0.9396563172340393, Accuracy: 0.9807692766189575, Computation time: 1.2469143867492676\n",
      "Step: 7033, Loss: 0.937798261642456, Accuracy: 0.9642857313156128, Computation time: 1.1515100002288818\n",
      "Step: 7034, Loss: 0.9161655306816101, Accuracy: 1.0, Computation time: 1.1258254051208496\n",
      "Step: 7035, Loss: 0.937494158744812, Accuracy: 0.9722222089767456, Computation time: 1.0302011966705322\n",
      "Step: 7036, Loss: 0.9162624478340149, Accuracy: 1.0, Computation time: 1.081277847290039\n",
      "Step: 7037, Loss: 0.9388288855552673, Accuracy: 0.9750000238418579, Computation time: 1.0056138038635254\n",
      "Step: 7038, Loss: 0.9380517601966858, Accuracy: 0.9750000238418579, Computation time: 0.9930984973907471\n",
      "Step: 7039, Loss: 0.9162918925285339, Accuracy: 1.0, Computation time: 1.1844892501831055\n",
      "Step: 7040, Loss: 0.9161665439605713, Accuracy: 1.0, Computation time: 1.1862924098968506\n",
      "Step: 7041, Loss: 0.9161272048950195, Accuracy: 1.0, Computation time: 1.3162264823913574\n",
      "Step: 7042, Loss: 0.9174993634223938, Accuracy: 1.0, Computation time: 1.407057285308838\n",
      "Step: 7043, Loss: 0.9160549640655518, Accuracy: 1.0, Computation time: 1.3786015510559082\n",
      "Step: 7044, Loss: 0.9162003397941589, Accuracy: 1.0, Computation time: 1.2677431106567383\n",
      "Step: 7045, Loss: 0.9160864353179932, Accuracy: 1.0, Computation time: 1.2381563186645508\n",
      "Step: 7046, Loss: 0.9159162640571594, Accuracy: 1.0, Computation time: 1.019547939300537\n",
      "Step: 7047, Loss: 0.916049599647522, Accuracy: 1.0, Computation time: 1.1375532150268555\n",
      "Step: 7048, Loss: 0.9160172343254089, Accuracy: 1.0, Computation time: 1.1043684482574463\n",
      "Step: 7049, Loss: 0.9160081744194031, Accuracy: 1.0, Computation time: 1.4449996948242188\n",
      "Step: 7050, Loss: 0.9371709227561951, Accuracy: 0.9642857313156128, Computation time: 1.3775389194488525\n",
      "Step: 7051, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.1891958713531494\n",
      "Step: 7052, Loss: 0.9159837961196899, Accuracy: 1.0, Computation time: 1.2482051849365234\n",
      "Step: 7053, Loss: 0.9160065650939941, Accuracy: 1.0, Computation time: 1.240356206893921\n",
      "Step: 7054, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 1.631917953491211\n",
      "Step: 7055, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 1.335719347000122\n",
      "Step: 7056, Loss: 0.9160215854644775, Accuracy: 1.0, Computation time: 1.1466689109802246\n",
      "Step: 7057, Loss: 0.9158961176872253, Accuracy: 1.0, Computation time: 1.2331047058105469\n",
      "Step: 7058, Loss: 0.9356621503829956, Accuracy: 0.9772727489471436, Computation time: 1.3492298126220703\n",
      "Step: 7059, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.408599615097046\n",
      "Step: 7060, Loss: 0.915896475315094, Accuracy: 1.0, Computation time: 1.4076542854309082\n",
      "Step: 7061, Loss: 0.9159312844276428, Accuracy: 1.0, Computation time: 1.768979787826538\n",
      "Step: 7062, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.4054510593414307\n",
      "Step: 7063, Loss: 0.9159902930259705, Accuracy: 1.0, Computation time: 1.6226553916931152\n",
      "Step: 7064, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 1.5172405242919922\n",
      "Step: 7065, Loss: 0.9159388542175293, Accuracy: 1.0, Computation time: 1.6360533237457275\n",
      "Step: 7066, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 1.3259100914001465\n",
      "Step: 7067, Loss: 0.9160519242286682, Accuracy: 1.0, Computation time: 1.5416665077209473\n",
      "Step: 7068, Loss: 0.9209719896316528, Accuracy: 1.0, Computation time: 1.6535320281982422\n",
      "Step: 7069, Loss: 0.9161251187324524, Accuracy: 1.0, Computation time: 1.4497814178466797\n",
      "Step: 7070, Loss: 0.9160902500152588, Accuracy: 1.0, Computation time: 1.0776467323303223\n",
      "Step: 7071, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 1.294884443283081\n",
      "Step: 7072, Loss: 0.937573254108429, Accuracy: 0.9821428656578064, Computation time: 1.2408392429351807\n",
      "Step: 7073, Loss: 0.9159139394760132, Accuracy: 1.0, Computation time: 1.3625783920288086\n",
      "Step: 7074, Loss: 0.9377908110618591, Accuracy: 0.984375, Computation time: 1.2019340991973877\n",
      "Step: 7075, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.292961597442627\n",
      "Step: 7076, Loss: 0.916075587272644, Accuracy: 1.0, Computation time: 1.341853141784668\n",
      "Step: 7077, Loss: 0.9375136494636536, Accuracy: 0.9722222089767456, Computation time: 1.1214311122894287\n",
      "Step: 7078, Loss: 0.9161950349807739, Accuracy: 1.0, Computation time: 1.1977953910827637\n",
      "Step: 7079, Loss: 0.9246978163719177, Accuracy: 1.0, Computation time: 1.91994047164917\n",
      "Step: 7080, Loss: 0.9163954257965088, Accuracy: 1.0, Computation time: 0.9919335842132568\n",
      "Step: 7081, Loss: 0.9172629714012146, Accuracy: 1.0, Computation time: 1.188223123550415\n",
      "Step: 7082, Loss: 0.916853666305542, Accuracy: 1.0, Computation time: 1.1048462390899658\n",
      "Step: 7083, Loss: 0.9168753027915955, Accuracy: 1.0, Computation time: 1.1758055686950684\n",
      "Step: 7084, Loss: 0.9223195910453796, Accuracy: 1.0, Computation time: 1.164541482925415\n",
      "Step: 7085, Loss: 0.916638970375061, Accuracy: 1.0, Computation time: 1.1210408210754395\n",
      "Test loss: 1.1210715770721436, Test Accuracy: 0.7002078890800476\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55949333-b58c-4a23-929f-7f3853ce32e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
