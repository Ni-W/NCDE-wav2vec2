{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0b764e-7f27-4937-84a8-2685b59b6035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db38688c-7008-4226-8f48-7368fa2a60e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session5_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0726b8-b873-4961-9dd7-3d728d96b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4290, 256, 768) (4290,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last2, wav2vec_last3, wav2vec_last4),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last2,label_last3,label_last4))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842ff725-7d20-40ef-81fa-9ad9060fad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f67c00c-b891-4458-9f03-e90c8f58755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567dfde3-4965-4db3-bae4-d0bedd4f9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            #zn = jnp.concatenate((coeffs_i, yn_array), axis=1)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0) \n",
    "            pred_var=prediction.var(axis=0)   \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)    \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b8efc2-c9c1-460f-9a67-75b13effddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f140f2b7-4cb6-468a-b1bf-bbff9c6ab7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last5], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last5\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d87009-a3d4-462e-a758-764b4e22661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176fd739-3efc-467a-8697-c7b4129b7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed368de9-5253-4d81-b017-1b6fae1fb8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4290,\n",
    "    dataset_test_size=1241,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=5789,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    import jax.numpy as jnp\n",
    "    from jax import jit\n",
    "    def calculate_confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "        true_labels = true_labels.astype(jnp.int32)\n",
    "        pred_labels = pred_labels.astype(jnp.int32)\n",
    "        conf_matrix = jnp.zeros((num_classes, num_classes), dtype=jnp.int32)\n",
    "        for t, p in zip(true_labels, pred_labels):\n",
    "            conf_matrix = conf_matrix.at[t, p].add(1)\n",
    "        return conf_matrix\n",
    "\n",
    "    @jit\n",
    "    def calculate_ua(conf_matrix):\n",
    "        class_accuracy = jnp.diag(conf_matrix) / jnp.sum(conf_matrix, axis=1)\n",
    "        UA = jnp.mean(class_accuracy)\n",
    "        return UA\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        conf_matrix = calculate_confusion_matrix(label_i, pred, num_classes=4)\n",
    "        UA = calculate_ua(conf_matrix)\n",
    "        return UA\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "445f3424-d0de-448c-af51-58f3a638174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.4339078664779663, Accuracy: 0.25, Computation time: 12.975265741348267\n",
      "Step: 1, Loss: 1.40086829662323, Accuracy: 0.25, Computation time: 2.111938238143921\n",
      "Step: 2, Loss: 1.3940348625183105, Accuracy: 0.25, Computation time: 1.9906463623046875\n",
      "Step: 3, Loss: 1.392568588256836, Accuracy: 0.25, Computation time: 1.7745475769042969\n",
      "Step: 4, Loss: 1.3489582538604736, Accuracy: 0.25, Computation time: 1.9358937740325928\n",
      "Step: 5, Loss: 1.3850921392440796, Accuracy: 0.25, Computation time: 2.2329227924346924\n",
      "Step: 6, Loss: 1.3512823581695557, Accuracy: 0.25, Computation time: 1.865480661392212\n",
      "Step: 7, Loss: 1.4062696695327759, Accuracy: 0.25, Computation time: 2.133023500442505\n",
      "Step: 8, Loss: 1.3075374364852905, Accuracy: 0.25, Computation time: 2.1265368461608887\n",
      "Step: 9, Loss: 1.414945363998413, Accuracy: 0.25, Computation time: 1.764380931854248\n",
      "Step: 10, Loss: 1.307539701461792, Accuracy: 0.5, Computation time: 1.654832363128662\n",
      "Step: 11, Loss: 1.3080730438232422, Accuracy: 0.5, Computation time: 2.2918860912323\n",
      "Step: 12, Loss: 1.271375060081482, Accuracy: 0.4444444477558136, Computation time: 1.929532527923584\n",
      "Step: 13, Loss: 1.3400778770446777, Accuracy: 0.4583333432674408, Computation time: 1.7200076580047607\n",
      "Step: 14, Loss: 1.2742619514465332, Accuracy: 0.4375, Computation time: 2.026944398880005\n",
      "Step: 15, Loss: 1.2351901531219482, Accuracy: 0.44999998807907104, Computation time: 1.5549674034118652\n",
      "Step: 16, Loss: 1.2725982666015625, Accuracy: 0.4318181872367859, Computation time: 2.087292432785034\n",
      "Step: 17, Loss: 1.1458848714828491, Accuracy: 0.5, Computation time: 1.469301462173462\n",
      "Step: 18, Loss: 1.1471612453460693, Accuracy: 0.75, Computation time: 1.661916971206665\n",
      "Step: 19, Loss: 1.1196882724761963, Accuracy: 0.75, Computation time: 1.8849515914916992\n",
      "Step: 20, Loss: 1.1343718767166138, Accuracy: 0.699999988079071, Computation time: 1.6161632537841797\n",
      "Step: 21, Loss: 1.1039793491363525, Accuracy: 0.75, Computation time: 1.7944138050079346\n",
      "Step: 22, Loss: 1.124659776687622, Accuracy: 0.7272727489471436, Computation time: 1.8063037395477295\n",
      "Step: 23, Loss: 1.144309639930725, Accuracy: 0.75, Computation time: 1.9995307922363281\n",
      "Step: 24, Loss: 1.1367908716201782, Accuracy: 0.7222222089767456, Computation time: 1.7989745140075684\n",
      "Step: 25, Loss: 1.0078620910644531, Accuracy: 0.75, Computation time: 1.5552175045013428\n",
      "Step: 26, Loss: 1.1133588552474976, Accuracy: 0.75, Computation time: 1.7911100387573242\n",
      "Step: 27, Loss: 1.1031559705734253, Accuracy: 0.7222222089767456, Computation time: 1.36826491355896\n",
      "Step: 28, Loss: 1.014225959777832, Accuracy: 0.75, Computation time: 1.543945550918579\n",
      "Step: 29, Loss: 1.0512714385986328, Accuracy: 0.6875, Computation time: 1.5228736400604248\n",
      "Step: 30, Loss: 1.1086938381195068, Accuracy: 0.699999988079071, Computation time: 2.181337833404541\n",
      "Step: 31, Loss: 1.0355908870697021, Accuracy: 0.75, Computation time: 1.423344612121582\n",
      "Step: 32, Loss: 1.071235179901123, Accuracy: 0.7250000238418579, Computation time: 1.5472722053527832\n",
      "Step: 33, Loss: 1.0613847970962524, Accuracy: 0.7291666865348816, Computation time: 1.627183198928833\n",
      "Step: 34, Loss: 1.0528866052627563, Accuracy: 0.75, Computation time: 1.5282442569732666\n",
      "Step: 35, Loss: 1.1091176271438599, Accuracy: 0.75, Computation time: 1.4082305431365967\n",
      "Step: 36, Loss: 1.0866246223449707, Accuracy: 0.75, Computation time: 1.5473229885101318\n",
      "Step: 37, Loss: 1.0697901248931885, Accuracy: 0.75, Computation time: 1.4237887859344482\n",
      "Step: 38, Loss: 1.045990228652954, Accuracy: 0.75, Computation time: 1.4082279205322266\n",
      "Step: 39, Loss: 1.0660792589187622, Accuracy: 0.75, Computation time: 1.3893442153930664\n",
      "Step: 40, Loss: 1.103522777557373, Accuracy: 0.75, Computation time: 1.7999043464660645\n",
      "Step: 41, Loss: 1.099733591079712, Accuracy: 0.7142857313156128, Computation time: 1.3587472438812256\n",
      "Step: 42, Loss: 1.055158257484436, Accuracy: 0.75, Computation time: 1.2768449783325195\n",
      "Step: 43, Loss: 1.053812861442566, Accuracy: 0.75, Computation time: 1.2852249145507812\n",
      "Step: 44, Loss: 1.0367070436477661, Accuracy: 0.75, Computation time: 1.362396001815796\n",
      "Step: 45, Loss: 1.0302654504776, Accuracy: 0.75, Computation time: 1.4231624603271484\n",
      "Step: 46, Loss: 1.0938231945037842, Accuracy: 0.7142857313156128, Computation time: 1.0945549011230469\n",
      "Step: 47, Loss: 1.0507756471633911, Accuracy: 0.71875, Computation time: 1.1865077018737793\n",
      "Step: 48, Loss: 1.0261094570159912, Accuracy: 0.75, Computation time: 1.1301884651184082\n",
      "Step: 49, Loss: 1.1068006753921509, Accuracy: 0.7824675440788269, Computation time: 1.4851024150848389\n",
      "Step: 50, Loss: 1.0290135145187378, Accuracy: 0.9722222089767456, Computation time: 1.2151587009429932\n",
      "Step: 51, Loss: 0.996010959148407, Accuracy: 1.0, Computation time: 1.2249882221221924\n",
      "Step: 52, Loss: 0.9943966865539551, Accuracy: 1.0, Computation time: 1.1664752960205078\n",
      "Step: 53, Loss: 0.9691901803016663, Accuracy: 1.0, Computation time: 1.3964815139770508\n",
      "Step: 54, Loss: 0.9868971705436707, Accuracy: 1.0, Computation time: 1.0992777347564697\n",
      "Step: 55, Loss: 0.980469286441803, Accuracy: 1.0, Computation time: 1.3476104736328125\n",
      "Step: 56, Loss: 0.9957908391952515, Accuracy: 0.9722222089767456, Computation time: 1.206519365310669\n",
      "Step: 57, Loss: 0.9640187621116638, Accuracy: nan, Computation time: 1.2377195358276367\n",
      "Step: 58, Loss: 0.9678976535797119, Accuracy: 1.0, Computation time: 1.0740478038787842\n",
      "Step: 59, Loss: 0.984477162361145, Accuracy: 0.9494949579238892, Computation time: 1.1501502990722656\n",
      "Step: 60, Loss: 0.9588548541069031, Accuracy: 0.9772727489471436, Computation time: 1.1898162364959717\n",
      "Step: 61, Loss: 0.9566133618354797, Accuracy: 1.0, Computation time: 1.156386375427246\n",
      "Step: 62, Loss: 0.9440747499465942, Accuracy: 1.0, Computation time: 1.1209900379180908\n",
      "Step: 63, Loss: 0.9731515049934387, Accuracy: 0.9375, Computation time: 1.054265022277832\n",
      "Step: 64, Loss: 0.9388376474380493, Accuracy: 1.0, Computation time: 1.272362470626831\n",
      "Step: 65, Loss: 0.9318909049034119, Accuracy: 1.0, Computation time: 1.2420017719268799\n",
      "Step: 66, Loss: 0.9289492964744568, Accuracy: 1.0, Computation time: 1.4454641342163086\n",
      "Step: 67, Loss: 0.9268684387207031, Accuracy: 1.0, Computation time: 1.2602663040161133\n",
      "Step: 68, Loss: 0.9449557662010193, Accuracy: 0.9791666865348816, Computation time: 1.0914638042449951\n",
      "Step: 69, Loss: 0.9466722011566162, Accuracy: 0.9807692766189575, Computation time: 1.4346067905426025\n",
      "Step: 70, Loss: 0.9412509202957153, Accuracy: 0.9642857313156128, Computation time: 1.1717002391815186\n",
      "Step: 71, Loss: 0.9262090921401978, Accuracy: 1.0, Computation time: 1.4181849956512451\n",
      "Step: 72, Loss: 0.9285619258880615, Accuracy: 1.0, Computation time: 1.2176389694213867\n",
      "Step: 73, Loss: 0.9212203025817871, Accuracy: 1.0, Computation time: 1.3869073390960693\n",
      "Step: 74, Loss: 0.9191867709159851, Accuracy: 1.0, Computation time: 1.1222381591796875\n",
      "Step: 75, Loss: 0.9412345290184021, Accuracy: 0.9722222089767456, Computation time: 1.1662802696228027\n",
      "Step: 76, Loss: 0.9304753541946411, Accuracy: 1.0, Computation time: 1.2077734470367432\n",
      "Step: 77, Loss: 0.917881965637207, Accuracy: 1.0, Computation time: 1.1446995735168457\n",
      "Step: 78, Loss: 0.9378373026847839, Accuracy: 0.949999988079071, Computation time: 1.1796271800994873\n",
      "Step: 79, Loss: 0.9177060127258301, Accuracy: 1.0, Computation time: 1.0985674858093262\n",
      "Step: 80, Loss: 0.9539498090744019, Accuracy: 0.9833333492279053, Computation time: 1.1620285511016846\n",
      "Step: 81, Loss: 0.9185024499893188, Accuracy: 1.0, Computation time: 1.3465168476104736\n",
      "Step: 82, Loss: 0.9223663806915283, Accuracy: 1.0, Computation time: 1.4094698429107666\n",
      "Step: 83, Loss: 0.9196619987487793, Accuracy: 1.0, Computation time: 1.359391689300537\n",
      "Step: 84, Loss: 0.9182957410812378, Accuracy: 1.0, Computation time: 1.312427282333374\n",
      "Step: 85, Loss: 0.9220694899559021, Accuracy: 1.0, Computation time: 1.2866010665893555\n",
      "Step: 86, Loss: 0.9440790414810181, Accuracy: 0.9807692766189575, Computation time: 1.095479965209961\n",
      "Step: 87, Loss: 0.9198987483978271, Accuracy: 1.0, Computation time: 1.2999348640441895\n",
      "Step: 88, Loss: 0.9382659792900085, Accuracy: 0.9772727489471436, Computation time: 1.1620419025421143\n",
      "Step: 89, Loss: 0.9263741374015808, Accuracy: 1.0, Computation time: 1.12276029586792\n",
      "Step: 90, Loss: 0.9230858683586121, Accuracy: 1.0, Computation time: 1.790785312652588\n",
      "Step: 91, Loss: 0.918384313583374, Accuracy: 1.0, Computation time: 1.2099695205688477\n",
      "Step: 92, Loss: 0.9184553027153015, Accuracy: 1.0, Computation time: 1.4239561557769775\n",
      "Step: 93, Loss: 0.9175652265548706, Accuracy: 1.0, Computation time: 1.8505511283874512\n",
      "Step: 94, Loss: 0.9171434044837952, Accuracy: 1.0, Computation time: 1.7647645473480225\n",
      "Step: 95, Loss: 0.9179642796516418, Accuracy: 1.0, Computation time: 1.3403239250183105\n",
      "Step: 96, Loss: 0.9206568002700806, Accuracy: 1.0, Computation time: 1.5391974449157715\n",
      "Step: 97, Loss: 0.919350802898407, Accuracy: 1.0, Computation time: 1.2738900184631348\n",
      "Step: 98, Loss: 0.9311655759811401, Accuracy: 1.0, Computation time: 1.5495846271514893\n",
      "Step: 99, Loss: 0.9184572100639343, Accuracy: 1.0, Computation time: 1.4659206867218018\n",
      "Step: 100, Loss: 0.9192283153533936, Accuracy: 1.0, Computation time: 1.7154459953308105\n",
      "Step: 101, Loss: 0.9170858860015869, Accuracy: 1.0, Computation time: 1.2811801433563232\n",
      "Step: 102, Loss: 0.9163990020751953, Accuracy: 1.0, Computation time: 1.1032636165618896\n",
      "Step: 103, Loss: 0.9530942440032959, Accuracy: 0.9522727727890015, Computation time: 1.3575348854064941\n",
      "Step: 104, Loss: 0.9245269894599915, Accuracy: 1.0, Computation time: 1.4609293937683105\n",
      "Step: 105, Loss: 0.9188406467437744, Accuracy: 1.0, Computation time: 1.2232601642608643\n",
      "Step: 106, Loss: 0.9164693355560303, Accuracy: 1.0, Computation time: 1.5982475280761719\n",
      "Step: 107, Loss: 0.9167184829711914, Accuracy: 1.0, Computation time: 1.2117652893066406\n",
      "Step: 108, Loss: 0.9365475177764893, Accuracy: 0.9722222089767456, Computation time: 1.2355799674987793\n",
      "Step: 109, Loss: 0.9165291786193848, Accuracy: 1.0, Computation time: 0.9872081279754639\n",
      "Step: 110, Loss: 0.9167423248291016, Accuracy: 1.0, Computation time: 1.148712396621704\n",
      "Step: 111, Loss: 0.9284413456916809, Accuracy: 0.9807692766189575, Computation time: 2.017716646194458\n",
      "Step: 112, Loss: 0.9163774251937866, Accuracy: 1.0, Computation time: 1.414926528930664\n",
      "Step: 113, Loss: 0.9366622567176819, Accuracy: 0.9772727489471436, Computation time: 1.5770983695983887\n",
      "Step: 114, Loss: 0.9167620539665222, Accuracy: 1.0, Computation time: 1.3481099605560303\n",
      "Step: 115, Loss: 0.9328397512435913, Accuracy: 0.96875, Computation time: 1.3121778964996338\n",
      "Step: 116, Loss: 0.9556567668914795, Accuracy: 0.9375, Computation time: 1.2772152423858643\n",
      "Step: 117, Loss: 0.959073007106781, Accuracy: 0.9142857193946838, Computation time: 1.4641716480255127\n",
      "Step: 118, Loss: 0.9184560775756836, Accuracy: 1.0, Computation time: 1.6019058227539062\n",
      "Step: 119, Loss: 0.916989266872406, Accuracy: 1.0, Computation time: 1.4000799655914307\n",
      "Step: 120, Loss: 0.936524510383606, Accuracy: 0.9642857313156128, Computation time: 1.4612717628479004\n",
      "Step: 121, Loss: 0.9577910900115967, Accuracy: 0.9500000476837158, Computation time: 1.3402295112609863\n",
      "Step: 122, Loss: 0.9164531826972961, Accuracy: 1.0, Computation time: 1.540038824081421\n",
      "Step: 123, Loss: 0.917029619216919, Accuracy: 1.0, Computation time: 1.0687906742095947\n",
      "Step: 124, Loss: 0.9195578098297119, Accuracy: 1.0, Computation time: 1.1278464794158936\n",
      "Step: 125, Loss: 0.9166006445884705, Accuracy: 1.0, Computation time: 1.802612066268921\n",
      "Step: 126, Loss: 0.916754961013794, Accuracy: 1.0, Computation time: 1.2519967555999756\n",
      "Step: 127, Loss: 0.9164924025535583, Accuracy: 1.0, Computation time: 1.3897199630737305\n",
      "Step: 128, Loss: 0.9181168079376221, Accuracy: 1.0, Computation time: 1.0706486701965332\n",
      "Step: 129, Loss: 0.9369256496429443, Accuracy: 0.9772727489471436, Computation time: 1.3164362907409668\n",
      "Step: 130, Loss: 0.9343167543411255, Accuracy: 0.949999988079071, Computation time: 1.1590056419372559\n",
      "Step: 131, Loss: 0.9399276375770569, Accuracy: 0.9642857313156128, Computation time: 1.1931052207946777\n",
      "Step: 132, Loss: 0.9161959290504456, Accuracy: 1.0, Computation time: 1.446338415145874\n",
      "Step: 133, Loss: 0.9374527335166931, Accuracy: 0.9833333492279053, Computation time: 1.5538523197174072\n",
      "Step: 134, Loss: 0.9168691039085388, Accuracy: 1.0, Computation time: 2.046342372894287\n",
      "Step: 135, Loss: 0.9260460734367371, Accuracy: 1.0, Computation time: 1.2911522388458252\n",
      "Step: 136, Loss: 0.9163464903831482, Accuracy: 1.0, Computation time: 1.420820951461792\n",
      "Step: 137, Loss: 0.9201362729072571, Accuracy: 1.0, Computation time: 1.3858542442321777\n",
      "Step: 138, Loss: 0.9177495241165161, Accuracy: 1.0, Computation time: 1.199267864227295\n",
      "Step: 139, Loss: 0.9163730144500732, Accuracy: 1.0, Computation time: 1.3741521835327148\n",
      "########################\n",
      "Test loss: 1.1192601919174194, Test Accuracy_epoch1: 0.6922303438186646\n",
      "########################\n",
      "Step: 140, Loss: 0.9165725111961365, Accuracy: 1.0, Computation time: 1.7699835300445557\n",
      "Step: 141, Loss: 0.9164668917655945, Accuracy: 1.0, Computation time: 1.2566776275634766\n",
      "Step: 142, Loss: 0.91644287109375, Accuracy: 1.0, Computation time: 1.1752738952636719\n",
      "Step: 143, Loss: 0.9160420894622803, Accuracy: 1.0, Computation time: 1.068892002105713\n",
      "Step: 144, Loss: 0.945075273513794, Accuracy: 0.9722222089767456, Computation time: 1.4501192569732666\n",
      "Step: 145, Loss: 0.9201388955116272, Accuracy: 1.0, Computation time: 1.506302833557129\n",
      "Step: 146, Loss: 0.9162249565124512, Accuracy: 1.0, Computation time: 1.4711766242980957\n",
      "Step: 147, Loss: 0.9174919128417969, Accuracy: 1.0, Computation time: 1.3178277015686035\n",
      "Step: 148, Loss: 0.9184362888336182, Accuracy: 1.0, Computation time: 1.4234488010406494\n",
      "Step: 149, Loss: 0.9518668055534363, Accuracy: 0.922619104385376, Computation time: 1.7907297611236572\n",
      "Step: 150, Loss: 0.9367148876190186, Accuracy: 0.9750000238418579, Computation time: 1.1507925987243652\n",
      "Step: 151, Loss: 0.9167881011962891, Accuracy: 1.0, Computation time: 1.2929940223693848\n",
      "Step: 152, Loss: 0.9216536283493042, Accuracy: 1.0, Computation time: 1.4436736106872559\n",
      "Step: 153, Loss: 0.9552414417266846, Accuracy: 0.9642857313156128, Computation time: 1.387925386428833\n",
      "Step: 154, Loss: 0.9165151715278625, Accuracy: 1.0, Computation time: 1.2819194793701172\n",
      "Step: 155, Loss: 0.9162406921386719, Accuracy: 1.0, Computation time: 1.3219094276428223\n",
      "Step: 156, Loss: 0.9168605208396912, Accuracy: 1.0, Computation time: 1.1941986083984375\n",
      "Step: 157, Loss: 0.9181696772575378, Accuracy: 1.0, Computation time: 1.2333636283874512\n",
      "Step: 158, Loss: 0.9330567717552185, Accuracy: 0.96875, Computation time: 1.5742855072021484\n",
      "Step: 159, Loss: 0.9331977963447571, Accuracy: 0.9722222089767456, Computation time: 1.5524170398712158\n",
      "Step: 160, Loss: 0.9162365794181824, Accuracy: 1.0, Computation time: 1.2614290714263916\n",
      "Step: 161, Loss: 0.9167607426643372, Accuracy: 1.0, Computation time: 1.1352958679199219\n",
      "Step: 162, Loss: 0.9162848591804504, Accuracy: 1.0, Computation time: 1.0375771522521973\n",
      "Step: 163, Loss: 0.9589622616767883, Accuracy: 0.935606062412262, Computation time: 1.3580279350280762\n",
      "Step: 164, Loss: 0.9163684844970703, Accuracy: 1.0, Computation time: 1.3507771492004395\n",
      "Step: 165, Loss: 0.9162343740463257, Accuracy: 1.0, Computation time: 1.3852167129516602\n",
      "Step: 166, Loss: 0.9195553064346313, Accuracy: 1.0, Computation time: 1.3743679523468018\n",
      "Step: 167, Loss: 0.9397050142288208, Accuracy: 0.9750000238418579, Computation time: 1.511753797531128\n",
      "Step: 168, Loss: 0.9166592359542847, Accuracy: 1.0, Computation time: 1.2887167930603027\n",
      "Step: 169, Loss: 0.9171297550201416, Accuracy: 1.0, Computation time: 1.3793680667877197\n",
      "Step: 170, Loss: 0.9164348840713501, Accuracy: 1.0, Computation time: 1.3644042015075684\n",
      "Step: 171, Loss: 0.9388933181762695, Accuracy: 0.9583333730697632, Computation time: 1.4939661026000977\n",
      "Step: 172, Loss: 0.9170620441436768, Accuracy: 1.0, Computation time: 1.5981731414794922\n",
      "Step: 173, Loss: 0.9174032807350159, Accuracy: 1.0, Computation time: 1.6335198879241943\n",
      "Step: 174, Loss: 0.9399838447570801, Accuracy: 0.9821428656578064, Computation time: 1.5406904220581055\n",
      "Step: 175, Loss: 0.9400308132171631, Accuracy: 0.9642857313156128, Computation time: 1.5918192863464355\n",
      "Step: 176, Loss: 0.9162145256996155, Accuracy: 1.0, Computation time: 1.410707950592041\n",
      "Step: 177, Loss: 0.9165169596672058, Accuracy: 1.0, Computation time: 1.4300963878631592\n",
      "Step: 178, Loss: 0.9169787168502808, Accuracy: 1.0, Computation time: 1.3270983695983887\n",
      "Step: 179, Loss: 0.9161097407341003, Accuracy: 1.0, Computation time: 1.270843505859375\n",
      "Step: 180, Loss: 0.9162535667419434, Accuracy: 1.0, Computation time: 1.1194000244140625\n",
      "Step: 181, Loss: 0.9169890880584717, Accuracy: 1.0, Computation time: 1.2827143669128418\n",
      "Step: 182, Loss: 0.9303389191627502, Accuracy: 0.9807692766189575, Computation time: 1.388674259185791\n",
      "Step: 183, Loss: 0.9207478165626526, Accuracy: 1.0, Computation time: 1.5586566925048828\n",
      "Step: 184, Loss: 0.9184235334396362, Accuracy: 1.0, Computation time: 1.5550549030303955\n",
      "Step: 185, Loss: 0.9162679314613342, Accuracy: 1.0, Computation time: 1.311612844467163\n",
      "Step: 186, Loss: 0.9390230774879456, Accuracy: 0.96875, Computation time: 1.4973747730255127\n",
      "Step: 187, Loss: 0.91619473695755, Accuracy: 1.0, Computation time: 1.3014209270477295\n",
      "Step: 188, Loss: 0.916343092918396, Accuracy: 1.0, Computation time: 1.197993278503418\n",
      "Step: 189, Loss: 0.927431583404541, Accuracy: 1.0, Computation time: 1.5445070266723633\n",
      "Step: 190, Loss: 0.9306842088699341, Accuracy: 1.0, Computation time: 1.545619010925293\n",
      "Step: 191, Loss: 0.9195975661277771, Accuracy: 1.0, Computation time: 1.418466567993164\n",
      "Step: 192, Loss: 0.9163379669189453, Accuracy: 1.0, Computation time: 1.0176048278808594\n",
      "Step: 193, Loss: 0.9192991852760315, Accuracy: 1.0, Computation time: 1.5794317722320557\n",
      "Step: 194, Loss: 0.9455533027648926, Accuracy: 0.9642857313156128, Computation time: 1.0276587009429932\n",
      "Step: 195, Loss: 0.9178130626678467, Accuracy: 1.0, Computation time: 1.743870496749878\n",
      "Step: 196, Loss: 0.9169166684150696, Accuracy: 1.0, Computation time: 1.285202980041504\n",
      "Step: 197, Loss: 0.9161440134048462, Accuracy: 1.0, Computation time: 1.5168423652648926\n",
      "Step: 198, Loss: 0.9173365831375122, Accuracy: 1.0, Computation time: 1.121070146560669\n",
      "Step: 199, Loss: 0.916146993637085, Accuracy: 1.0, Computation time: 1.2745990753173828\n",
      "Step: 200, Loss: 0.9161462783813477, Accuracy: 1.0, Computation time: 1.4572770595550537\n",
      "Step: 201, Loss: 0.916978657245636, Accuracy: 1.0, Computation time: 1.2747459411621094\n",
      "Step: 202, Loss: 0.9166970252990723, Accuracy: 1.0, Computation time: 1.0280399322509766\n",
      "Step: 203, Loss: 0.9168217182159424, Accuracy: 1.0, Computation time: 1.4486291408538818\n",
      "Step: 204, Loss: 0.9168023467063904, Accuracy: 1.0, Computation time: 1.3391470909118652\n",
      "Step: 205, Loss: 0.9167742133140564, Accuracy: 1.0, Computation time: 1.4809863567352295\n",
      "Step: 206, Loss: 0.9164099097251892, Accuracy: 1.0, Computation time: 1.20841646194458\n",
      "Step: 207, Loss: 0.9160792827606201, Accuracy: 1.0, Computation time: 1.3634648323059082\n",
      "Step: 208, Loss: 0.9326068162918091, Accuracy: 0.9750000238418579, Computation time: 1.3507213592529297\n",
      "Step: 209, Loss: 0.9161120057106018, Accuracy: 1.0, Computation time: 1.4603631496429443\n",
      "Step: 210, Loss: 0.9352462887763977, Accuracy: 0.9772727489471436, Computation time: 1.5287556648254395\n",
      "Step: 211, Loss: 0.9355551600456238, Accuracy: 0.9807692766189575, Computation time: 1.7958288192749023\n",
      "Step: 212, Loss: 0.9236158132553101, Accuracy: 1.0, Computation time: 1.125807762145996\n",
      "Step: 213, Loss: 0.9165692925453186, Accuracy: 1.0, Computation time: 1.2029814720153809\n",
      "Step: 214, Loss: 0.9166255593299866, Accuracy: 1.0, Computation time: 1.2336909770965576\n",
      "Step: 215, Loss: 0.9161982536315918, Accuracy: 1.0, Computation time: 1.3793532848358154\n",
      "Step: 216, Loss: 0.9161471128463745, Accuracy: 1.0, Computation time: 1.5017058849334717\n",
      "Step: 217, Loss: 0.9164054989814758, Accuracy: 1.0, Computation time: 1.097996473312378\n",
      "Step: 218, Loss: 0.9176720976829529, Accuracy: 1.0, Computation time: 1.2462010383605957\n",
      "Step: 219, Loss: 0.9354067444801331, Accuracy: 0.9772727489471436, Computation time: 1.4411375522613525\n",
      "Step: 220, Loss: 0.9160207509994507, Accuracy: 1.0, Computation time: 1.3037614822387695\n",
      "Step: 221, Loss: 0.9163464307785034, Accuracy: 1.0, Computation time: 1.30582594871521\n",
      "Step: 222, Loss: 0.9162160158157349, Accuracy: 1.0, Computation time: 1.7220532894134521\n",
      "Step: 223, Loss: 0.9191938638687134, Accuracy: 1.0, Computation time: 1.2997071743011475\n",
      "Step: 224, Loss: 0.9437012672424316, Accuracy: 0.9375, Computation time: 1.6328742504119873\n",
      "Step: 225, Loss: 0.9162120223045349, Accuracy: 1.0, Computation time: 1.1209232807159424\n",
      "Step: 226, Loss: 0.9165393710136414, Accuracy: 1.0, Computation time: 1.030158281326294\n",
      "Step: 227, Loss: 0.9269347190856934, Accuracy: 0.9375, Computation time: 1.2891871929168701\n",
      "Step: 228, Loss: 0.9195399284362793, Accuracy: 1.0, Computation time: 1.0334680080413818\n",
      "Step: 229, Loss: 0.9229785203933716, Accuracy: 1.0, Computation time: 1.370415449142456\n",
      "Step: 230, Loss: 0.9261500835418701, Accuracy: 1.0, Computation time: 1.9953563213348389\n",
      "Step: 231, Loss: 0.9161965847015381, Accuracy: 1.0, Computation time: 1.1939971446990967\n",
      "Step: 232, Loss: 0.9164727926254272, Accuracy: 1.0, Computation time: 1.2513718605041504\n",
      "Step: 233, Loss: 0.9247121214866638, Accuracy: 1.0, Computation time: 1.904364824295044\n",
      "Step: 234, Loss: 0.9354481101036072, Accuracy: 0.9642857313156128, Computation time: 1.7476248741149902\n",
      "Step: 235, Loss: 0.9164377450942993, Accuracy: 1.0, Computation time: 1.1629829406738281\n",
      "Step: 236, Loss: 0.9177709817886353, Accuracy: 1.0, Computation time: 1.3305644989013672\n",
      "Step: 237, Loss: 0.9168758392333984, Accuracy: 1.0, Computation time: 1.3850743770599365\n",
      "Step: 238, Loss: 0.9166930913925171, Accuracy: 1.0, Computation time: 1.1144649982452393\n",
      "Step: 239, Loss: 0.9374954700469971, Accuracy: 0.9722222089767456, Computation time: 1.0978610515594482\n",
      "Step: 240, Loss: 0.9166914224624634, Accuracy: 1.0, Computation time: 1.1491215229034424\n",
      "Step: 241, Loss: 0.9163921475410461, Accuracy: 1.0, Computation time: 1.3871288299560547\n",
      "Step: 242, Loss: 0.9351046085357666, Accuracy: 0.9772727489471436, Computation time: 1.3649520874023438\n",
      "Step: 243, Loss: 0.9167325496673584, Accuracy: 1.0, Computation time: 1.5360493659973145\n",
      "Step: 244, Loss: 0.9516631364822388, Accuracy: 0.9444444179534912, Computation time: 1.6731328964233398\n",
      "Step: 245, Loss: 0.9353969693183899, Accuracy: 0.9750000238418579, Computation time: 1.4627861976623535\n",
      "Step: 246, Loss: 0.9163634181022644, Accuracy: 1.0, Computation time: 1.8135311603546143\n",
      "Step: 247, Loss: 0.9392635822296143, Accuracy: 0.9791666865348816, Computation time: 1.64327073097229\n",
      "Step: 248, Loss: 0.91961669921875, Accuracy: 1.0, Computation time: 1.0072083473205566\n",
      "Step: 249, Loss: 0.9166629314422607, Accuracy: 1.0, Computation time: 1.2241973876953125\n",
      "Step: 250, Loss: 0.9160827994346619, Accuracy: 1.0, Computation time: 1.4457406997680664\n",
      "Step: 251, Loss: 0.9372460842132568, Accuracy: 0.9807692766189575, Computation time: 1.5811221599578857\n",
      "Step: 252, Loss: 0.9463729858398438, Accuracy: 0.9722222089767456, Computation time: 1.554436206817627\n",
      "Step: 253, Loss: 0.9169139266014099, Accuracy: 1.0, Computation time: 1.3649656772613525\n",
      "Step: 254, Loss: 0.9166206121444702, Accuracy: 1.0, Computation time: 1.440363883972168\n",
      "Step: 255, Loss: 0.9161632657051086, Accuracy: 1.0, Computation time: 1.3890979290008545\n",
      "Step: 256, Loss: 0.9322580695152283, Accuracy: 0.9833333492279053, Computation time: 1.8578641414642334\n",
      "Step: 257, Loss: 0.9734113812446594, Accuracy: 0.8819444179534912, Computation time: 1.349297285079956\n",
      "Step: 258, Loss: 0.9165457487106323, Accuracy: 1.0, Computation time: 1.3808538913726807\n",
      "Step: 259, Loss: 0.9331557750701904, Accuracy: 0.9583333730697632, Computation time: 1.1214747428894043\n",
      "Step: 260, Loss: 0.9162150025367737, Accuracy: 1.0, Computation time: 1.5408399105072021\n",
      "Step: 261, Loss: 0.9377975463867188, Accuracy: 0.9750000238418579, Computation time: 1.3128612041473389\n",
      "Step: 262, Loss: 0.9169046878814697, Accuracy: 1.0, Computation time: 1.5952754020690918\n",
      "Step: 263, Loss: 0.9277870655059814, Accuracy: 0.9807692766189575, Computation time: 1.4073214530944824\n",
      "Step: 264, Loss: 0.9205067753791809, Accuracy: 1.0, Computation time: 1.2496552467346191\n",
      "Step: 265, Loss: 0.9452334046363831, Accuracy: 0.9833333492279053, Computation time: 1.240147590637207\n",
      "Step: 266, Loss: 0.9170241355895996, Accuracy: 1.0, Computation time: 1.5227923393249512\n",
      "Step: 267, Loss: 0.9355830550193787, Accuracy: 0.9583333730697632, Computation time: 1.35801100730896\n",
      "Step: 268, Loss: 0.9167168140411377, Accuracy: 1.0, Computation time: 1.2727594375610352\n",
      "Step: 269, Loss: 0.9160946607589722, Accuracy: 1.0, Computation time: 1.5575072765350342\n",
      "Step: 270, Loss: 0.9175576567649841, Accuracy: 1.0, Computation time: 1.479374647140503\n",
      "Step: 271, Loss: 0.9161747097969055, Accuracy: 1.0, Computation time: 1.3758728504180908\n",
      "Step: 272, Loss: 0.916385293006897, Accuracy: 1.0, Computation time: 1.4894740581512451\n",
      "Step: 273, Loss: 0.9337995052337646, Accuracy: 0.9722222089767456, Computation time: 1.5005946159362793\n",
      "Step: 274, Loss: 0.9162241220474243, Accuracy: 1.0, Computation time: 1.2343058586120605\n",
      "Step: 275, Loss: 0.9219860434532166, Accuracy: 1.0, Computation time: 1.7696921825408936\n",
      "Step: 276, Loss: 0.9163857102394104, Accuracy: 1.0, Computation time: 1.4386155605316162\n",
      "Step: 277, Loss: 0.91825932264328, Accuracy: 1.0, Computation time: 1.3865199089050293\n",
      "Step: 278, Loss: 0.9598894715309143, Accuracy: 0.9522727727890015, Computation time: 1.2972261905670166\n",
      "########################\n",
      "Test loss: 1.1215276718139648, Test Accuracy_epoch2: 0.6956424713134766\n",
      "########################\n",
      "Step: 279, Loss: 0.9164694547653198, Accuracy: 1.0, Computation time: 1.2282750606536865\n",
      "Step: 280, Loss: 0.9368137717247009, Accuracy: 0.9722222089767456, Computation time: 1.452373743057251\n",
      "Step: 281, Loss: 0.9161891937255859, Accuracy: 1.0, Computation time: 1.4331281185150146\n",
      "Step: 282, Loss: 0.9333310127258301, Accuracy: 0.96875, Computation time: 1.2572648525238037\n",
      "Step: 283, Loss: 0.916367769241333, Accuracy: 1.0, Computation time: 1.4320158958435059\n",
      "Step: 284, Loss: 0.9311330318450928, Accuracy: 0.9772727489471436, Computation time: 1.4551689624786377\n",
      "Step: 285, Loss: 0.9161036014556885, Accuracy: 1.0, Computation time: 1.1691060066223145\n",
      "Step: 286, Loss: 0.917853057384491, Accuracy: 1.0, Computation time: 1.6428651809692383\n",
      "Step: 287, Loss: 0.9164896011352539, Accuracy: 1.0, Computation time: 1.2898385524749756\n",
      "Step: 288, Loss: 0.9522240161895752, Accuracy: 0.9615384340286255, Computation time: 1.2322402000427246\n",
      "Step: 289, Loss: 0.9174043536186218, Accuracy: 1.0, Computation time: 1.6777377128601074\n",
      "Step: 290, Loss: 0.9383951425552368, Accuracy: 0.96875, Computation time: 1.595043659210205\n",
      "Step: 291, Loss: 0.9386301636695862, Accuracy: 0.9722222089767456, Computation time: 1.4975800514221191\n",
      "Step: 292, Loss: 0.9163253307342529, Accuracy: 1.0, Computation time: 1.0878455638885498\n",
      "Step: 293, Loss: 0.9161636829376221, Accuracy: 1.0, Computation time: 1.5025115013122559\n",
      "Step: 294, Loss: 0.9162188172340393, Accuracy: 1.0, Computation time: 1.3430140018463135\n",
      "Step: 295, Loss: 0.93617844581604, Accuracy: 0.9833333492279053, Computation time: 1.3445584774017334\n",
      "Step: 296, Loss: 0.9164924025535583, Accuracy: 1.0, Computation time: 1.5679144859313965\n",
      "Step: 297, Loss: 0.9163469672203064, Accuracy: 1.0, Computation time: 1.0426607131958008\n",
      "Step: 298, Loss: 0.9161313772201538, Accuracy: 1.0, Computation time: 1.0057942867279053\n",
      "Step: 299, Loss: 0.9164129495620728, Accuracy: 1.0, Computation time: 1.3087825775146484\n",
      "Step: 300, Loss: 0.9162358641624451, Accuracy: 1.0, Computation time: 1.2474987506866455\n",
      "Step: 301, Loss: 0.9249956011772156, Accuracy: 1.0, Computation time: 1.5349256992340088\n",
      "Step: 302, Loss: 0.9161466956138611, Accuracy: 1.0, Computation time: 1.0076255798339844\n",
      "Step: 303, Loss: 0.9541100859642029, Accuracy: 0.949999988079071, Computation time: 1.428807020187378\n",
      "Step: 304, Loss: 0.9172855615615845, Accuracy: 1.0, Computation time: 1.1323206424713135\n",
      "Step: 305, Loss: 0.9165661334991455, Accuracy: 1.0, Computation time: 1.453315258026123\n",
      "Step: 306, Loss: 0.9176689386367798, Accuracy: 1.0, Computation time: 1.2435319423675537\n",
      "Step: 307, Loss: 0.9167162775993347, Accuracy: 1.0, Computation time: 1.2745943069458008\n",
      "Step: 308, Loss: 0.9293439388275146, Accuracy: 0.984375, Computation time: 1.048485279083252\n",
      "Step: 309, Loss: 0.91620272397995, Accuracy: 1.0, Computation time: 1.045196533203125\n",
      "Step: 310, Loss: 0.9180083870887756, Accuracy: 1.0, Computation time: 1.2473440170288086\n",
      "Step: 311, Loss: 0.9164268374443054, Accuracy: 1.0, Computation time: 1.0661816596984863\n",
      "Step: 312, Loss: 0.9193817377090454, Accuracy: 1.0, Computation time: 1.2957329750061035\n",
      "Step: 313, Loss: 0.9171385169029236, Accuracy: 1.0, Computation time: 1.0281867980957031\n",
      "Step: 314, Loss: 0.9161849021911621, Accuracy: 1.0, Computation time: 1.3544976711273193\n",
      "Step: 315, Loss: 0.9167621731758118, Accuracy: 1.0, Computation time: 1.1086556911468506\n",
      "Step: 316, Loss: 0.9175621867179871, Accuracy: 1.0, Computation time: 1.4490764141082764\n",
      "Step: 317, Loss: 0.9164388179779053, Accuracy: 1.0, Computation time: 1.1849491596221924\n",
      "Step: 318, Loss: 0.9194322824478149, Accuracy: 1.0, Computation time: 1.495267629623413\n",
      "Step: 319, Loss: 0.92392498254776, Accuracy: 1.0, Computation time: 1.1144540309906006\n",
      "Step: 320, Loss: 0.9160445928573608, Accuracy: 1.0, Computation time: 1.0348694324493408\n",
      "Step: 321, Loss: 0.916252613067627, Accuracy: 1.0, Computation time: 1.5105738639831543\n",
      "Step: 322, Loss: 0.9359325170516968, Accuracy: 0.9750000238418579, Computation time: 1.1725778579711914\n",
      "Step: 323, Loss: 0.915947675704956, Accuracy: 1.0, Computation time: 1.189948320388794\n",
      "Step: 324, Loss: 0.9161008596420288, Accuracy: 1.0, Computation time: 1.167712926864624\n",
      "Step: 325, Loss: 0.9161295890808105, Accuracy: 1.0, Computation time: 1.4489037990570068\n",
      "Step: 326, Loss: 0.9229296445846558, Accuracy: 1.0, Computation time: 1.6034610271453857\n",
      "Step: 327, Loss: 0.9161643981933594, Accuracy: 1.0, Computation time: 1.029984951019287\n",
      "Step: 328, Loss: 0.9619649648666382, Accuracy: 0.949999988079071, Computation time: 1.1448118686676025\n",
      "Step: 329, Loss: 0.9167823791503906, Accuracy: 1.0, Computation time: 1.2665283679962158\n",
      "Step: 330, Loss: 0.942233145236969, Accuracy: 0.949999988079071, Computation time: 1.4216814041137695\n",
      "Step: 331, Loss: 0.9313257336616516, Accuracy: 1.0, Computation time: 3.012932538986206\n",
      "Step: 332, Loss: 0.9203934073448181, Accuracy: 1.0, Computation time: 1.1043651103973389\n",
      "Step: 333, Loss: 0.9166505336761475, Accuracy: 1.0, Computation time: 1.1031224727630615\n",
      "Step: 334, Loss: 0.9179419279098511, Accuracy: 1.0, Computation time: 1.3407585620880127\n",
      "Step: 335, Loss: 0.9167484045028687, Accuracy: 1.0, Computation time: 1.1948847770690918\n",
      "Step: 336, Loss: 0.9164760708808899, Accuracy: 1.0, Computation time: 1.1470208168029785\n",
      "Step: 337, Loss: 0.9280313849449158, Accuracy: 1.0, Computation time: 1.3087615966796875\n",
      "Step: 338, Loss: 0.9316604137420654, Accuracy: 0.9807692766189575, Computation time: 1.5152947902679443\n",
      "Step: 339, Loss: 0.9597636461257935, Accuracy: 0.9494949579238892, Computation time: 1.196305513381958\n",
      "Step: 340, Loss: 0.9339717626571655, Accuracy: 0.9642857313156128, Computation time: 1.6147916316986084\n",
      "Step: 341, Loss: 0.9190846085548401, Accuracy: 1.0, Computation time: 1.3189136981964111\n",
      "Step: 342, Loss: 0.938075840473175, Accuracy: 0.9722222089767456, Computation time: 1.3548507690429688\n",
      "Step: 343, Loss: 0.9161893129348755, Accuracy: 1.0, Computation time: 1.3151814937591553\n",
      "Step: 344, Loss: 0.9378398656845093, Accuracy: 0.949999988079071, Computation time: 1.1091094017028809\n",
      "Step: 345, Loss: 0.9164438843727112, Accuracy: 1.0, Computation time: 1.3123111724853516\n",
      "Step: 346, Loss: 0.9280670881271362, Accuracy: 0.9642857313156128, Computation time: 1.5996241569519043\n",
      "Step: 347, Loss: 0.9197728633880615, Accuracy: 1.0, Computation time: 1.813962459564209\n",
      "Step: 348, Loss: 0.9245860576629639, Accuracy: 1.0, Computation time: 1.4797251224517822\n",
      "Step: 349, Loss: 0.9186481833457947, Accuracy: 1.0, Computation time: 1.4835493564605713\n",
      "Step: 350, Loss: 0.9161482453346252, Accuracy: 1.0, Computation time: 1.2007129192352295\n",
      "Step: 351, Loss: 0.9163756370544434, Accuracy: 1.0, Computation time: 1.5517916679382324\n",
      "Step: 352, Loss: 0.9328042268753052, Accuracy: 0.96875, Computation time: 1.3450322151184082\n",
      "Step: 353, Loss: 0.9405916333198547, Accuracy: 0.96875, Computation time: 1.3848578929901123\n",
      "Step: 354, Loss: 0.9164674878120422, Accuracy: 1.0, Computation time: 1.4276432991027832\n",
      "Step: 355, Loss: 0.9179152250289917, Accuracy: 1.0, Computation time: 1.3219523429870605\n",
      "Step: 356, Loss: 0.9162831902503967, Accuracy: 1.0, Computation time: 1.2966077327728271\n",
      "Step: 357, Loss: 0.9161742329597473, Accuracy: 1.0, Computation time: 1.2868821620941162\n",
      "Step: 358, Loss: 0.9251672625541687, Accuracy: 1.0, Computation time: 1.3653430938720703\n",
      "Step: 359, Loss: 0.9352544546127319, Accuracy: 0.9750000238418579, Computation time: 1.1776971817016602\n",
      "Step: 360, Loss: 0.9667203426361084, Accuracy: 0.9166666865348816, Computation time: 2.095048427581787\n",
      "Step: 361, Loss: 0.9161094427108765, Accuracy: 1.0, Computation time: 1.4057426452636719\n",
      "Step: 362, Loss: 0.9162383675575256, Accuracy: 1.0, Computation time: 1.0440804958343506\n",
      "Step: 363, Loss: 0.9161956310272217, Accuracy: 1.0, Computation time: 1.469660997390747\n",
      "Step: 364, Loss: 0.9161670207977295, Accuracy: 1.0, Computation time: 1.4616751670837402\n",
      "Step: 365, Loss: 0.9163565039634705, Accuracy: 1.0, Computation time: 1.4998435974121094\n",
      "Step: 366, Loss: 0.9524502158164978, Accuracy: 0.9330357313156128, Computation time: 2.256375551223755\n",
      "Step: 367, Loss: 0.9179174304008484, Accuracy: 1.0, Computation time: 1.3245835304260254\n",
      "Step: 368, Loss: 0.9375688433647156, Accuracy: 0.9750000238418579, Computation time: 1.1938376426696777\n",
      "Step: 369, Loss: 0.9441888928413391, Accuracy: 0.9583333730697632, Computation time: 2.2005443572998047\n",
      "Step: 370, Loss: 0.916585385799408, Accuracy: 1.0, Computation time: 1.4574344158172607\n",
      "Step: 371, Loss: 0.9175494313240051, Accuracy: 1.0, Computation time: 1.2477569580078125\n",
      "Step: 372, Loss: 0.9162606596946716, Accuracy: 1.0, Computation time: 1.1451585292816162\n",
      "Step: 373, Loss: 0.9373959302902222, Accuracy: 0.9722222089767456, Computation time: 1.1994001865386963\n",
      "Step: 374, Loss: 0.9164661169052124, Accuracy: 1.0, Computation time: 1.287362813949585\n",
      "Step: 375, Loss: 0.9183170199394226, Accuracy: 1.0, Computation time: 1.2745604515075684\n",
      "Step: 376, Loss: 0.9166874289512634, Accuracy: 1.0, Computation time: 1.4923458099365234\n",
      "Step: 377, Loss: 0.9160115718841553, Accuracy: 1.0, Computation time: 1.1450235843658447\n",
      "Step: 378, Loss: 0.91609126329422, Accuracy: 1.0, Computation time: 1.2757325172424316\n",
      "Step: 379, Loss: 0.9161725640296936, Accuracy: 1.0, Computation time: 1.2375032901763916\n",
      "Step: 380, Loss: 0.9161640405654907, Accuracy: 1.0, Computation time: 1.0171456336975098\n",
      "Step: 381, Loss: 0.9160182476043701, Accuracy: 1.0, Computation time: 1.3956739902496338\n",
      "Step: 382, Loss: 0.9178187251091003, Accuracy: 1.0, Computation time: 1.3634002208709717\n",
      "Step: 383, Loss: 0.916068971157074, Accuracy: 1.0, Computation time: 1.147582769393921\n",
      "Step: 384, Loss: 0.9161133766174316, Accuracy: 1.0, Computation time: 1.326267957687378\n",
      "Step: 385, Loss: 0.9181004166603088, Accuracy: 1.0, Computation time: 1.3067760467529297\n",
      "Step: 386, Loss: 0.9160594344139099, Accuracy: 1.0, Computation time: 1.1047437191009521\n",
      "Step: 387, Loss: 0.9160953760147095, Accuracy: 1.0, Computation time: 1.4462666511535645\n",
      "Step: 388, Loss: 0.9368670582771301, Accuracy: 0.949999988079071, Computation time: 1.100724697113037\n",
      "Step: 389, Loss: 0.9363267421722412, Accuracy: 0.9642857313156128, Computation time: 1.6066250801086426\n",
      "Step: 390, Loss: 0.9161871671676636, Accuracy: 1.0, Computation time: 1.2046947479248047\n",
      "Step: 391, Loss: 0.9291177988052368, Accuracy: 0.96875, Computation time: 1.4576382637023926\n",
      "Step: 392, Loss: 0.9164257645606995, Accuracy: 1.0, Computation time: 1.2585043907165527\n",
      "Step: 393, Loss: 0.9162300825119019, Accuracy: 1.0, Computation time: 1.3864219188690186\n",
      "Step: 394, Loss: 0.9162001609802246, Accuracy: 1.0, Computation time: 1.25410795211792\n",
      "Step: 395, Loss: 0.9180542826652527, Accuracy: 1.0, Computation time: 1.5448894500732422\n",
      "Step: 396, Loss: 0.9162725210189819, Accuracy: 1.0, Computation time: 1.3113861083984375\n",
      "Step: 397, Loss: 0.9258412718772888, Accuracy: 1.0, Computation time: 1.3576035499572754\n",
      "Step: 398, Loss: 0.9168723821640015, Accuracy: 1.0, Computation time: 1.2189252376556396\n",
      "Step: 399, Loss: 0.924635112285614, Accuracy: 1.0, Computation time: 1.391488790512085\n",
      "Step: 400, Loss: 0.9164475202560425, Accuracy: 1.0, Computation time: 1.1930530071258545\n",
      "Step: 401, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.1406450271606445\n",
      "Step: 402, Loss: 0.918022096157074, Accuracy: 1.0, Computation time: 1.2912347316741943\n",
      "Step: 403, Loss: 0.9353064894676208, Accuracy: 0.9642857313156128, Computation time: 1.3742761611938477\n",
      "Step: 404, Loss: 0.9310700297355652, Accuracy: 0.9807692766189575, Computation time: 1.7059383392333984\n",
      "Step: 405, Loss: 0.9161266088485718, Accuracy: 1.0, Computation time: 1.654911994934082\n",
      "Step: 406, Loss: 0.9328791499137878, Accuracy: 0.9791666865348816, Computation time: 1.28822660446167\n",
      "Step: 407, Loss: 0.9393693208694458, Accuracy: 0.9791666865348816, Computation time: 1.1514036655426025\n",
      "Step: 408, Loss: 0.9392714500427246, Accuracy: 0.949999988079071, Computation time: 1.2367372512817383\n",
      "Step: 409, Loss: 0.9170287251472473, Accuracy: 1.0, Computation time: 1.262995958328247\n",
      "Step: 410, Loss: 0.9161302447319031, Accuracy: 1.0, Computation time: 1.3424835205078125\n",
      "Step: 411, Loss: 0.9163881540298462, Accuracy: 1.0, Computation time: 1.0180742740631104\n",
      "Step: 412, Loss: 0.9164562821388245, Accuracy: 1.0, Computation time: 1.219663381576538\n",
      "Step: 413, Loss: 0.937549889087677, Accuracy: 0.9807692766189575, Computation time: 1.2758910655975342\n",
      "Step: 414, Loss: 0.9374064803123474, Accuracy: 0.9791666865348816, Computation time: 1.4094891548156738\n",
      "Step: 415, Loss: 0.941122829914093, Accuracy: 0.9833333492279053, Computation time: 1.6958918571472168\n",
      "Step: 416, Loss: 0.9366348385810852, Accuracy: 0.9750000238418579, Computation time: 1.5077905654907227\n",
      "Step: 417, Loss: 0.916715145111084, Accuracy: 1.0, Computation time: 1.5598931312561035\n",
      "########################\n",
      "Test loss: 1.1220036745071411, Test Accuracy_epoch3: 0.6859647035598755\n",
      "########################\n",
      "Step: 418, Loss: 0.9377571940422058, Accuracy: 0.96875, Computation time: 1.699493646621704\n",
      "Step: 419, Loss: 0.9162660837173462, Accuracy: 1.0, Computation time: 1.0940163135528564\n",
      "Step: 420, Loss: 0.9160968661308289, Accuracy: 1.0, Computation time: 1.2659127712249756\n",
      "Step: 421, Loss: 0.9160648584365845, Accuracy: 1.0, Computation time: 1.3573410511016846\n",
      "Step: 422, Loss: 0.9161708950996399, Accuracy: 1.0, Computation time: 0.9462263584136963\n",
      "Step: 423, Loss: 0.9161232113838196, Accuracy: 1.0, Computation time: 1.0877153873443604\n",
      "Step: 424, Loss: 0.9588000774383545, Accuracy: 0.9529914855957031, Computation time: 1.1839921474456787\n",
      "Step: 425, Loss: 0.9165610074996948, Accuracy: 1.0, Computation time: 1.191652774810791\n",
      "Step: 426, Loss: 0.9170995950698853, Accuracy: 1.0, Computation time: 1.565274953842163\n",
      "Step: 427, Loss: 0.9159532785415649, Accuracy: 1.0, Computation time: 1.362762212753296\n",
      "Step: 428, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 1.2675368785858154\n",
      "Step: 429, Loss: 0.9164829850196838, Accuracy: 1.0, Computation time: 1.3207800388336182\n",
      "Step: 430, Loss: 0.9180594682693481, Accuracy: 1.0, Computation time: 1.5492238998413086\n",
      "Step: 431, Loss: 0.9183588624000549, Accuracy: 1.0, Computation time: 1.184901475906372\n",
      "Step: 432, Loss: 0.9164047837257385, Accuracy: 1.0, Computation time: 1.2945747375488281\n",
      "Step: 433, Loss: 0.9373230934143066, Accuracy: 0.9642857313156128, Computation time: 1.19252610206604\n",
      "Step: 434, Loss: 0.9159732460975647, Accuracy: 1.0, Computation time: 0.9876449108123779\n",
      "Step: 435, Loss: 0.9161204695701599, Accuracy: 1.0, Computation time: 1.4086101055145264\n",
      "Step: 436, Loss: 0.9359257221221924, Accuracy: 0.96875, Computation time: 1.1424734592437744\n",
      "Step: 437, Loss: 0.9159975647926331, Accuracy: 1.0, Computation time: 1.656818151473999\n",
      "Step: 438, Loss: 0.9160321950912476, Accuracy: 1.0, Computation time: 1.169226884841919\n",
      "Step: 439, Loss: 0.916017472743988, Accuracy: 1.0, Computation time: 1.0798885822296143\n",
      "Step: 440, Loss: 0.9161262512207031, Accuracy: 1.0, Computation time: 1.066908836364746\n",
      "Step: 441, Loss: 0.9171310663223267, Accuracy: 1.0, Computation time: 1.1534984111785889\n",
      "Step: 442, Loss: 0.9160162806510925, Accuracy: 1.0, Computation time: 1.4037225246429443\n",
      "Step: 443, Loss: 0.9176181554794312, Accuracy: 1.0, Computation time: 1.3172619342803955\n",
      "Step: 444, Loss: 0.9173257350921631, Accuracy: 1.0, Computation time: 1.1960744857788086\n",
      "Step: 445, Loss: 0.9371294379234314, Accuracy: 0.9722222089767456, Computation time: 1.1452350616455078\n",
      "Step: 446, Loss: 0.9160536527633667, Accuracy: 1.0, Computation time: 1.2571547031402588\n",
      "Step: 447, Loss: 0.9160361289978027, Accuracy: 1.0, Computation time: 1.3738312721252441\n",
      "Step: 448, Loss: 0.916144609451294, Accuracy: 1.0, Computation time: 1.3682096004486084\n",
      "Step: 449, Loss: 0.9160298705101013, Accuracy: 1.0, Computation time: 1.195342779159546\n",
      "Step: 450, Loss: 0.943683385848999, Accuracy: 0.96875, Computation time: 1.644721508026123\n",
      "Step: 451, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 1.41937255859375\n",
      "Step: 452, Loss: 0.9299710988998413, Accuracy: 0.9772727489471436, Computation time: 1.2973766326904297\n",
      "Step: 453, Loss: 0.9373174905776978, Accuracy: 0.9750000238418579, Computation time: 1.0268924236297607\n",
      "Step: 454, Loss: 0.9222145676612854, Accuracy: 1.0, Computation time: 1.895653247833252\n",
      "Step: 455, Loss: 0.9161224365234375, Accuracy: 1.0, Computation time: 1.2737400531768799\n",
      "Step: 456, Loss: 0.9378297924995422, Accuracy: 0.949999988079071, Computation time: 1.6849472522735596\n",
      "Step: 457, Loss: 0.9161856174468994, Accuracy: 1.0, Computation time: 1.1636309623718262\n",
      "Step: 458, Loss: 0.916022539138794, Accuracy: 1.0, Computation time: 1.0875754356384277\n",
      "Step: 459, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.1814217567443848\n",
      "Step: 460, Loss: 0.9166952967643738, Accuracy: 1.0, Computation time: 1.4356706142425537\n",
      "Step: 461, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 1.2512054443359375\n",
      "Step: 462, Loss: 0.9161049127578735, Accuracy: 1.0, Computation time: 1.1872732639312744\n",
      "Step: 463, Loss: 0.9282141923904419, Accuracy: 1.0, Computation time: 1.2483863830566406\n",
      "Step: 464, Loss: 0.9317795634269714, Accuracy: 0.9772727489471436, Computation time: 1.3664617538452148\n",
      "Step: 465, Loss: 0.9203038811683655, Accuracy: 1.0, Computation time: 1.2167699337005615\n",
      "Step: 466, Loss: 0.9160231351852417, Accuracy: 1.0, Computation time: 1.2374563217163086\n",
      "Step: 467, Loss: 0.9386755228042603, Accuracy: 0.96875, Computation time: 1.0760128498077393\n",
      "Step: 468, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 1.1371955871582031\n",
      "Step: 469, Loss: 0.9164296984672546, Accuracy: 1.0, Computation time: 1.3818261623382568\n",
      "Step: 470, Loss: 0.9368486404418945, Accuracy: 0.9807692766189575, Computation time: 1.6661350727081299\n",
      "Step: 471, Loss: 0.9175248742103577, Accuracy: 1.0, Computation time: 1.63985013961792\n",
      "Step: 472, Loss: 0.9353852272033691, Accuracy: 0.949999988079071, Computation time: 1.2508203983306885\n",
      "Step: 473, Loss: 0.9187289476394653, Accuracy: 1.0, Computation time: 1.2246928215026855\n",
      "Step: 474, Loss: 0.9160688519477844, Accuracy: 1.0, Computation time: 1.4595527648925781\n",
      "Step: 475, Loss: 0.9219083786010742, Accuracy: 1.0, Computation time: 1.4864988327026367\n",
      "Step: 476, Loss: 0.9160568118095398, Accuracy: nan, Computation time: 1.0520315170288086\n",
      "Step: 477, Loss: 0.9166415929794312, Accuracy: 1.0, Computation time: 1.4633569717407227\n",
      "Step: 478, Loss: 0.9162039160728455, Accuracy: 1.0, Computation time: 1.2163505554199219\n",
      "Step: 479, Loss: 0.9163467288017273, Accuracy: 1.0, Computation time: 1.1098949909210205\n",
      "Step: 480, Loss: 0.9166195392608643, Accuracy: 1.0, Computation time: 1.5817904472351074\n",
      "Step: 481, Loss: 0.9162595868110657, Accuracy: 1.0, Computation time: 1.1448874473571777\n",
      "Step: 482, Loss: 0.9169766306877136, Accuracy: 1.0, Computation time: 1.309600591659546\n",
      "Step: 483, Loss: 0.9224264025688171, Accuracy: 1.0, Computation time: 1.4788050651550293\n",
      "Step: 484, Loss: 0.9159969687461853, Accuracy: 1.0, Computation time: 1.2409839630126953\n",
      "Step: 485, Loss: 0.9164955615997314, Accuracy: 1.0, Computation time: 1.4818663597106934\n",
      "Step: 486, Loss: 0.916980504989624, Accuracy: 1.0, Computation time: 1.367661476135254\n",
      "Step: 487, Loss: 0.9169853329658508, Accuracy: 1.0, Computation time: 1.1931662559509277\n",
      "Step: 488, Loss: 0.92195725440979, Accuracy: 1.0, Computation time: 1.2907137870788574\n",
      "Step: 489, Loss: 0.9289416074752808, Accuracy: 1.0, Computation time: 1.829901933670044\n",
      "Step: 490, Loss: 0.9159768223762512, Accuracy: 1.0, Computation time: 1.0516412258148193\n",
      "Step: 491, Loss: 0.9160150289535522, Accuracy: 1.0, Computation time: 1.5738954544067383\n",
      "Step: 492, Loss: 0.9378433227539062, Accuracy: 0.9791666865348816, Computation time: 1.7728362083435059\n",
      "Step: 493, Loss: 0.9160550832748413, Accuracy: 1.0, Computation time: 1.109795331954956\n",
      "Step: 494, Loss: 0.9168307185173035, Accuracy: 1.0, Computation time: 1.3932294845581055\n",
      "Step: 495, Loss: 0.9162186980247498, Accuracy: 1.0, Computation time: 1.1609539985656738\n",
      "Step: 496, Loss: 0.9162576794624329, Accuracy: 1.0, Computation time: 1.1288490295410156\n",
      "Step: 497, Loss: 0.9162068367004395, Accuracy: 1.0, Computation time: 1.0375149250030518\n",
      "Step: 498, Loss: 0.9161142110824585, Accuracy: 1.0, Computation time: 1.1886911392211914\n",
      "Step: 499, Loss: 0.9160059094429016, Accuracy: 1.0, Computation time: 1.6259422302246094\n",
      "Step: 500, Loss: 0.9372650384902954, Accuracy: 0.9772727489471436, Computation time: 1.5075485706329346\n",
      "Step: 501, Loss: 0.9361350536346436, Accuracy: 0.96875, Computation time: 1.500739336013794\n",
      "Step: 502, Loss: 0.9378990530967712, Accuracy: 0.9807692766189575, Computation time: 1.557769775390625\n",
      "Step: 503, Loss: 0.9160566926002502, Accuracy: 1.0, Computation time: 1.1992082595825195\n",
      "Step: 504, Loss: 0.9166412353515625, Accuracy: 1.0, Computation time: 1.7280395030975342\n",
      "Step: 505, Loss: 0.9162680506706238, Accuracy: 1.0, Computation time: 1.2219948768615723\n",
      "Step: 506, Loss: 0.9160810112953186, Accuracy: 1.0, Computation time: 1.39296555519104\n",
      "Step: 507, Loss: 0.9160624146461487, Accuracy: 1.0, Computation time: 1.1554431915283203\n",
      "Step: 508, Loss: 0.9376662373542786, Accuracy: 0.9642857313156128, Computation time: 1.4108924865722656\n",
      "Step: 509, Loss: 0.9592042565345764, Accuracy: 0.9166666865348816, Computation time: 1.6319282054901123\n",
      "Step: 510, Loss: 0.9256705641746521, Accuracy: 1.0, Computation time: 1.6007075309753418\n",
      "Step: 511, Loss: 0.9277000427246094, Accuracy: 1.0, Computation time: 1.277428150177002\n",
      "Step: 512, Loss: 0.9161747097969055, Accuracy: 1.0, Computation time: 1.5289850234985352\n",
      "Step: 513, Loss: 0.916438639163971, Accuracy: 1.0, Computation time: 1.4805593490600586\n",
      "Step: 514, Loss: 0.9162967801094055, Accuracy: 1.0, Computation time: 1.1716349124908447\n",
      "Step: 515, Loss: 0.9170451760292053, Accuracy: 1.0, Computation time: 1.4621546268463135\n",
      "Step: 516, Loss: 0.9163053631782532, Accuracy: 1.0, Computation time: 1.1411888599395752\n",
      "Step: 517, Loss: 0.9165111780166626, Accuracy: 1.0, Computation time: 1.2120065689086914\n",
      "Step: 518, Loss: 0.9325864315032959, Accuracy: 0.9772727489471436, Computation time: 1.2069165706634521\n",
      "Step: 519, Loss: 0.9160863161087036, Accuracy: 1.0, Computation time: 1.137256383895874\n",
      "Step: 520, Loss: 0.9161630868911743, Accuracy: 1.0, Computation time: 1.55409574508667\n",
      "Step: 521, Loss: 0.916120707988739, Accuracy: 1.0, Computation time: 1.3361153602600098\n",
      "Step: 522, Loss: 0.916637122631073, Accuracy: 1.0, Computation time: 1.2613229751586914\n",
      "Step: 523, Loss: 0.9174875617027283, Accuracy: 1.0, Computation time: 1.617896556854248\n",
      "Step: 524, Loss: 0.9348756670951843, Accuracy: 0.9833333492279053, Computation time: 1.6479575634002686\n",
      "Step: 525, Loss: 0.9259710311889648, Accuracy: 0.9821428656578064, Computation time: 1.6420059204101562\n",
      "Step: 526, Loss: 0.9165943264961243, Accuracy: 1.0, Computation time: 1.2768528461456299\n",
      "Step: 527, Loss: 0.936274528503418, Accuracy: 0.9772727489471436, Computation time: 1.4641170501708984\n",
      "Step: 528, Loss: 0.9160979390144348, Accuracy: 1.0, Computation time: 1.0233945846557617\n",
      "Step: 529, Loss: 0.9180627465248108, Accuracy: 1.0, Computation time: 1.3482820987701416\n",
      "Step: 530, Loss: 0.9337764382362366, Accuracy: 0.9642857313156128, Computation time: 1.1382801532745361\n",
      "Step: 531, Loss: 0.9160133004188538, Accuracy: 1.0, Computation time: 1.1723618507385254\n",
      "Step: 532, Loss: 0.9167937636375427, Accuracy: 1.0, Computation time: 1.2497854232788086\n",
      "Step: 533, Loss: 0.9160187840461731, Accuracy: 1.0, Computation time: 1.8334355354309082\n",
      "Step: 534, Loss: 0.9376672506332397, Accuracy: 0.9722222089767456, Computation time: 1.2064459323883057\n",
      "Step: 535, Loss: 0.9324173927307129, Accuracy: 0.9642857313156128, Computation time: 1.034092664718628\n",
      "Step: 536, Loss: 0.9162810444831848, Accuracy: 1.0, Computation time: 1.214360237121582\n",
      "Step: 537, Loss: 0.9394406676292419, Accuracy: 0.9642857313156128, Computation time: 1.1414592266082764\n",
      "Step: 538, Loss: 0.9272084832191467, Accuracy: 0.9583333730697632, Computation time: 1.5643174648284912\n",
      "Step: 539, Loss: 0.917687177658081, Accuracy: 1.0, Computation time: 1.2870256900787354\n",
      "Step: 540, Loss: 0.9162182211875916, Accuracy: 1.0, Computation time: 1.0972709655761719\n",
      "Step: 541, Loss: 0.9381651282310486, Accuracy: 0.9583333730697632, Computation time: 1.0476491451263428\n",
      "Step: 542, Loss: 0.9161595106124878, Accuracy: 1.0, Computation time: 1.695793867111206\n",
      "Step: 543, Loss: 0.9376595616340637, Accuracy: 0.9772727489471436, Computation time: 1.35481858253479\n",
      "Step: 544, Loss: 0.9375197291374207, Accuracy: 0.9722222089767456, Computation time: 1.125396490097046\n",
      "Step: 545, Loss: 0.9499761462211609, Accuracy: 0.9520833492279053, Computation time: 1.6659164428710938\n",
      "Step: 546, Loss: 0.9238062500953674, Accuracy: 1.0, Computation time: 1.1967029571533203\n",
      "Step: 547, Loss: 0.9162366986274719, Accuracy: 1.0, Computation time: 1.3382174968719482\n",
      "Step: 548, Loss: 0.934174656867981, Accuracy: 0.9583333730697632, Computation time: 1.4502041339874268\n",
      "Step: 549, Loss: 0.9595839977264404, Accuracy: 0.9166666865348816, Computation time: 2.2821147441864014\n",
      "Step: 550, Loss: 0.927040159702301, Accuracy: 0.9722222089767456, Computation time: 1.3960094451904297\n",
      "Step: 551, Loss: 0.9194533824920654, Accuracy: 1.0, Computation time: 1.3028521537780762\n",
      "Step: 552, Loss: 0.9464649558067322, Accuracy: 0.9807692766189575, Computation time: 1.2541255950927734\n",
      "Step: 553, Loss: 0.9237355589866638, Accuracy: 1.0, Computation time: 1.4256224632263184\n",
      "Step: 554, Loss: 0.9163029193878174, Accuracy: 1.0, Computation time: 2.1733133792877197\n",
      "Step: 555, Loss: 0.9179708361625671, Accuracy: 1.0, Computation time: 1.2474420070648193\n",
      "Step: 556, Loss: 0.9162293672561646, Accuracy: 1.0, Computation time: 1.2123515605926514\n",
      "########################\n",
      "Test loss: 1.1231708526611328, Test Accuracy_epoch4: 0.6937196254730225\n",
      "########################\n",
      "Step: 557, Loss: 0.9162616729736328, Accuracy: 1.0, Computation time: 0.9992797374725342\n",
      "Step: 558, Loss: 0.9160862565040588, Accuracy: 1.0, Computation time: 0.9015200138092041\n",
      "Step: 559, Loss: 0.9165381193161011, Accuracy: 1.0, Computation time: 1.1898069381713867\n",
      "Step: 560, Loss: 0.9161734580993652, Accuracy: 1.0, Computation time: 1.1381332874298096\n",
      "Step: 561, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.2946791648864746\n",
      "Step: 562, Loss: 0.9160369634628296, Accuracy: 1.0, Computation time: 1.1135735511779785\n",
      "Step: 563, Loss: 0.9160845279693604, Accuracy: 1.0, Computation time: 1.6960697174072266\n",
      "Step: 564, Loss: 0.9375905990600586, Accuracy: 0.9772727489471436, Computation time: 1.1017179489135742\n",
      "Step: 565, Loss: 0.937861979007721, Accuracy: 0.9772727489471436, Computation time: 1.5447382926940918\n",
      "Step: 566, Loss: 0.9162783026695251, Accuracy: 1.0, Computation time: 1.2037479877471924\n",
      "Step: 567, Loss: 0.9340152144432068, Accuracy: 0.875, Computation time: 1.3156301975250244\n",
      "Step: 568, Loss: 0.9160482287406921, Accuracy: 1.0, Computation time: 1.1729254722595215\n",
      "Step: 569, Loss: 0.9160129427909851, Accuracy: 1.0, Computation time: 1.1352088451385498\n",
      "Step: 570, Loss: 0.9159525036811829, Accuracy: 1.0, Computation time: 1.0360569953918457\n",
      "Step: 571, Loss: 0.9367072582244873, Accuracy: 0.9772727489471436, Computation time: 1.5816123485565186\n",
      "Step: 572, Loss: 0.9196678400039673, Accuracy: 1.0, Computation time: 1.426792860031128\n",
      "Step: 573, Loss: 0.9189475178718567, Accuracy: 1.0, Computation time: 1.4489881992340088\n",
      "Step: 574, Loss: 0.9160481691360474, Accuracy: 1.0, Computation time: 1.1758089065551758\n",
      "Step: 575, Loss: 0.915989100933075, Accuracy: 1.0, Computation time: 1.0866062641143799\n",
      "Step: 576, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 1.1795086860656738\n",
      "Step: 577, Loss: 0.9169092774391174, Accuracy: 1.0, Computation time: 1.2911458015441895\n",
      "Step: 578, Loss: 0.916014552116394, Accuracy: 1.0, Computation time: 1.194061279296875\n",
      "Step: 579, Loss: 0.9377647638320923, Accuracy: 0.9821428656578064, Computation time: 1.5347480773925781\n",
      "Step: 580, Loss: 0.9369872808456421, Accuracy: 0.9722222089767456, Computation time: 1.2218551635742188\n",
      "Step: 581, Loss: 0.9160065054893494, Accuracy: 1.0, Computation time: 1.230891227722168\n",
      "Step: 582, Loss: 0.9175136089324951, Accuracy: 1.0, Computation time: 1.4679334163665771\n",
      "Step: 583, Loss: 0.9329490065574646, Accuracy: 0.9807692766189575, Computation time: 1.856445550918579\n",
      "Step: 584, Loss: 0.9164100885391235, Accuracy: 1.0, Computation time: 1.6482594013214111\n",
      "Step: 585, Loss: 0.9367424845695496, Accuracy: 0.9750000238418579, Computation time: 1.4182186126708984\n",
      "Step: 586, Loss: 0.9161410331726074, Accuracy: 1.0, Computation time: 1.0844402313232422\n",
      "Step: 587, Loss: 0.9165356755256653, Accuracy: 1.0, Computation time: 1.046466588973999\n",
      "Step: 588, Loss: 0.9162062406539917, Accuracy: 1.0, Computation time: 0.9609010219573975\n",
      "Step: 589, Loss: 0.9375532865524292, Accuracy: 0.96875, Computation time: 1.1675024032592773\n",
      "Step: 590, Loss: 0.9169362187385559, Accuracy: 1.0, Computation time: 1.3367033004760742\n",
      "Step: 591, Loss: 0.9207558035850525, Accuracy: 1.0, Computation time: 1.1590616703033447\n",
      "Step: 592, Loss: 0.9288208484649658, Accuracy: 0.96875, Computation time: 1.702148675918579\n",
      "Step: 593, Loss: 0.936915934085846, Accuracy: 0.9807692766189575, Computation time: 1.2574338912963867\n",
      "Step: 594, Loss: 0.9367836117744446, Accuracy: 0.9750000238418579, Computation time: 1.199876308441162\n",
      "Step: 595, Loss: 0.9164753556251526, Accuracy: 1.0, Computation time: 1.5641310214996338\n",
      "Step: 596, Loss: 0.916170597076416, Accuracy: 1.0, Computation time: 1.088512897491455\n",
      "Step: 597, Loss: 0.9359811544418335, Accuracy: 0.9722222089767456, Computation time: 1.7678956985473633\n",
      "Step: 598, Loss: 0.9377446174621582, Accuracy: 0.9722222089767456, Computation time: 1.1036550998687744\n",
      "Step: 599, Loss: 0.9162731766700745, Accuracy: 1.0, Computation time: 1.5423307418823242\n",
      "Step: 600, Loss: 0.916092574596405, Accuracy: 1.0, Computation time: 1.2656738758087158\n",
      "Step: 601, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 1.2617857456207275\n",
      "Step: 602, Loss: 0.9160423874855042, Accuracy: 1.0, Computation time: 1.0155370235443115\n",
      "Step: 603, Loss: 0.9159449934959412, Accuracy: 1.0, Computation time: 1.0578193664550781\n",
      "Step: 604, Loss: 0.9227673411369324, Accuracy: 1.0, Computation time: 1.1822764873504639\n",
      "Step: 605, Loss: 0.9159573912620544, Accuracy: 1.0, Computation time: 1.1926214694976807\n",
      "Step: 606, Loss: 0.9397343993186951, Accuracy: 0.9642857313156128, Computation time: 1.0848207473754883\n",
      "Step: 607, Loss: 0.9163668751716614, Accuracy: 1.0, Computation time: 1.3680615425109863\n",
      "Step: 608, Loss: 0.9160669445991516, Accuracy: 1.0, Computation time: 1.0837740898132324\n",
      "Step: 609, Loss: 0.9188874959945679, Accuracy: 1.0, Computation time: 1.4511117935180664\n",
      "Step: 610, Loss: 0.9159975647926331, Accuracy: 1.0, Computation time: 1.3199243545532227\n",
      "Step: 611, Loss: 0.9160378575325012, Accuracy: 1.0, Computation time: 1.0388751029968262\n",
      "Step: 612, Loss: 0.9577991366386414, Accuracy: 0.9375, Computation time: 1.1206233501434326\n",
      "Step: 613, Loss: 0.9258866906166077, Accuracy: 0.9807692766189575, Computation time: 1.0644311904907227\n",
      "Step: 614, Loss: 0.9160498976707458, Accuracy: 1.0, Computation time: 0.9394989013671875\n",
      "Step: 615, Loss: 0.9221033453941345, Accuracy: 1.0, Computation time: 1.2033226490020752\n",
      "Step: 616, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 1.207864761352539\n",
      "Step: 617, Loss: 0.9162769913673401, Accuracy: 1.0, Computation time: 1.470931053161621\n",
      "Step: 618, Loss: 0.9160974025726318, Accuracy: 1.0, Computation time: 1.268829345703125\n",
      "Step: 619, Loss: 0.9331669807434082, Accuracy: 0.9722222089767456, Computation time: 1.0859897136688232\n",
      "Step: 620, Loss: 0.9165439605712891, Accuracy: 1.0, Computation time: 1.0181732177734375\n",
      "Step: 621, Loss: 0.9369778037071228, Accuracy: 0.9833333492279053, Computation time: 1.3639214038848877\n",
      "Step: 622, Loss: 0.9364010691642761, Accuracy: 0.9166666865348816, Computation time: 1.8180572986602783\n",
      "Step: 623, Loss: 0.9160493612289429, Accuracy: 1.0, Computation time: 1.728180170059204\n",
      "Step: 624, Loss: 0.9160122871398926, Accuracy: 1.0, Computation time: 1.1587393283843994\n",
      "Step: 625, Loss: 0.9173641204833984, Accuracy: 1.0, Computation time: 1.5914454460144043\n",
      "Step: 626, Loss: 0.9174262285232544, Accuracy: 1.0, Computation time: 1.3536710739135742\n",
      "Step: 627, Loss: 0.9160712957382202, Accuracy: 1.0, Computation time: 1.175452470779419\n",
      "Step: 628, Loss: 0.9159586429595947, Accuracy: 1.0, Computation time: 1.1334238052368164\n",
      "Step: 629, Loss: 0.9179877042770386, Accuracy: 1.0, Computation time: 1.384312629699707\n",
      "Step: 630, Loss: 0.9180408716201782, Accuracy: 1.0, Computation time: 1.6333732604980469\n",
      "Step: 631, Loss: 0.916172444820404, Accuracy: 1.0, Computation time: 1.7219672203063965\n",
      "Step: 632, Loss: 0.9160789251327515, Accuracy: 1.0, Computation time: 1.312225341796875\n",
      "Step: 633, Loss: 0.9161074161529541, Accuracy: 1.0, Computation time: 1.2041935920715332\n",
      "Step: 634, Loss: 0.9160760641098022, Accuracy: 1.0, Computation time: 1.7048254013061523\n",
      "Step: 635, Loss: 0.9159454107284546, Accuracy: 1.0, Computation time: 1.3331482410430908\n",
      "Step: 636, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.1130402088165283\n",
      "Step: 637, Loss: 0.9162851572036743, Accuracy: 1.0, Computation time: 1.1057603359222412\n",
      "Step: 638, Loss: 0.9160280227661133, Accuracy: 1.0, Computation time: 1.0803241729736328\n",
      "Step: 639, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 1.0902831554412842\n",
      "Step: 640, Loss: 0.9160838723182678, Accuracy: 1.0, Computation time: 1.2423198223114014\n",
      "Step: 641, Loss: 0.9162614941596985, Accuracy: 1.0, Computation time: 1.3846068382263184\n",
      "Step: 642, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 1.1518242359161377\n",
      "Step: 643, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 1.3649370670318604\n",
      "Step: 644, Loss: 0.9159982800483704, Accuracy: 1.0, Computation time: 1.3146281242370605\n",
      "Step: 645, Loss: 0.9559704065322876, Accuracy: 0.9305555820465088, Computation time: 1.3325209617614746\n",
      "Step: 646, Loss: 0.9163498282432556, Accuracy: 1.0, Computation time: 1.59578275680542\n",
      "Step: 647, Loss: 0.9159237742424011, Accuracy: 1.0, Computation time: 1.2926652431488037\n",
      "Step: 648, Loss: 0.9374696612358093, Accuracy: 0.96875, Computation time: 1.4980721473693848\n",
      "Step: 649, Loss: 0.9162312746047974, Accuracy: 1.0, Computation time: 1.31669282913208\n",
      "Step: 650, Loss: 0.9165133833885193, Accuracy: 1.0, Computation time: 1.3220305442810059\n",
      "Step: 651, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 1.5503475666046143\n",
      "Step: 652, Loss: 0.915969729423523, Accuracy: 1.0, Computation time: 1.6829702854156494\n",
      "Step: 653, Loss: 0.9361854195594788, Accuracy: 0.9722222089767456, Computation time: 1.2709953784942627\n",
      "Step: 654, Loss: 0.9160197377204895, Accuracy: 1.0, Computation time: 1.4588377475738525\n",
      "Step: 655, Loss: 0.9186774492263794, Accuracy: 1.0, Computation time: 1.2796380519866943\n",
      "Step: 656, Loss: 0.9251194000244141, Accuracy: 1.0, Computation time: 1.4251117706298828\n",
      "Step: 657, Loss: 0.9160750508308411, Accuracy: 1.0, Computation time: 1.3601500988006592\n",
      "Step: 658, Loss: 0.9159239530563354, Accuracy: 1.0, Computation time: 1.2149512767791748\n",
      "Step: 659, Loss: 0.9343047738075256, Accuracy: 0.9583333730697632, Computation time: 1.476780891418457\n",
      "Step: 660, Loss: 0.9159287214279175, Accuracy: 1.0, Computation time: 1.096022605895996\n",
      "Step: 661, Loss: 0.9162620306015015, Accuracy: 1.0, Computation time: 1.3046536445617676\n",
      "Step: 662, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 1.2011876106262207\n",
      "Step: 663, Loss: 0.9348846673965454, Accuracy: 0.9583333730697632, Computation time: 0.9853858947753906\n",
      "Step: 664, Loss: 0.9162048101425171, Accuracy: 1.0, Computation time: 1.2163190841674805\n",
      "Step: 665, Loss: 0.9373573064804077, Accuracy: 0.9791666865348816, Computation time: 1.5788486003875732\n",
      "Step: 666, Loss: 0.9160240888595581, Accuracy: 1.0, Computation time: 1.3285224437713623\n",
      "Step: 667, Loss: 0.9197719097137451, Accuracy: 1.0, Computation time: 1.8365142345428467\n",
      "Step: 668, Loss: 0.9162142872810364, Accuracy: 1.0, Computation time: 1.4561491012573242\n",
      "Step: 669, Loss: 0.918839156627655, Accuracy: 1.0, Computation time: 1.516723871231079\n",
      "Step: 670, Loss: 0.9162969589233398, Accuracy: 1.0, Computation time: 1.2946834564208984\n",
      "Step: 671, Loss: 0.9160626530647278, Accuracy: 1.0, Computation time: 1.364210844039917\n",
      "Step: 672, Loss: 0.9161747694015503, Accuracy: 1.0, Computation time: 1.2902376651763916\n",
      "Step: 673, Loss: 0.9177200794219971, Accuracy: 1.0, Computation time: 1.4660446643829346\n",
      "Step: 674, Loss: 0.9160614013671875, Accuracy: 1.0, Computation time: 1.5050127506256104\n",
      "Step: 675, Loss: 0.9342121481895447, Accuracy: 0.9807692766189575, Computation time: 2.7450191974639893\n",
      "Step: 676, Loss: 0.9160301089286804, Accuracy: 1.0, Computation time: 1.2817494869232178\n",
      "Step: 677, Loss: 0.9160200953483582, Accuracy: 1.0, Computation time: 1.0883147716522217\n",
      "Step: 678, Loss: 0.9164750576019287, Accuracy: 1.0, Computation time: 1.2924633026123047\n",
      "Step: 679, Loss: 0.9591103792190552, Accuracy: 0.9356061220169067, Computation time: 1.9144988059997559\n",
      "Step: 680, Loss: 0.9161642789840698, Accuracy: 1.0, Computation time: 1.4977283477783203\n",
      "Step: 681, Loss: 0.9160763025283813, Accuracy: 1.0, Computation time: 1.1879565715789795\n",
      "Step: 682, Loss: 0.9161727428436279, Accuracy: 1.0, Computation time: 1.096320390701294\n",
      "Step: 683, Loss: 0.9167637228965759, Accuracy: 1.0, Computation time: 1.61445951461792\n",
      "Step: 684, Loss: 0.916029155254364, Accuracy: 1.0, Computation time: 1.1841833591461182\n",
      "Step: 685, Loss: 0.9160939455032349, Accuracy: 1.0, Computation time: 1.3766252994537354\n",
      "Step: 686, Loss: 0.9163193702697754, Accuracy: 1.0, Computation time: 1.539402961730957\n",
      "Step: 687, Loss: 0.933711051940918, Accuracy: 0.9772727489471436, Computation time: 1.2869601249694824\n",
      "Step: 688, Loss: 0.9160245060920715, Accuracy: 1.0, Computation time: 1.328284740447998\n",
      "Step: 689, Loss: 0.9161189794540405, Accuracy: 1.0, Computation time: 1.043837547302246\n",
      "Step: 690, Loss: 0.9160774946212769, Accuracy: 1.0, Computation time: 0.9080860614776611\n",
      "Step: 691, Loss: 0.9160841703414917, Accuracy: 1.0, Computation time: 1.159731149673462\n",
      "Step: 692, Loss: 0.9223243594169617, Accuracy: 1.0, Computation time: 1.4339945316314697\n",
      "Step: 693, Loss: 0.916006326675415, Accuracy: 1.0, Computation time: 1.2110280990600586\n",
      "Step: 694, Loss: 0.9374869465827942, Accuracy: 0.96875, Computation time: 1.1734843254089355\n",
      "Step: 695, Loss: 0.9159864187240601, Accuracy: 1.0, Computation time: 1.1036052703857422\n",
      "########################\n",
      "Test loss: 1.1318778991699219, Test Accuracy_epoch5: 0.6678720712661743\n",
      "########################\n",
      "Step: 696, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 1.0518410205841064\n",
      "Step: 697, Loss: 0.9604659080505371, Accuracy: 0.9437500238418579, Computation time: 1.830944299697876\n",
      "Step: 698, Loss: 0.9380641579627991, Accuracy: 0.9750000238418579, Computation time: 1.6118128299713135\n",
      "Step: 699, Loss: 0.9160140156745911, Accuracy: 1.0, Computation time: 1.0476338863372803\n",
      "Step: 700, Loss: 0.9376651644706726, Accuracy: 0.9722222089767456, Computation time: 1.1549980640411377\n",
      "Step: 701, Loss: 0.9161019921302795, Accuracy: 1.0, Computation time: 1.6688024997711182\n",
      "Step: 702, Loss: 0.9447839856147766, Accuracy: 0.9722222089767456, Computation time: 1.856360673904419\n",
      "Step: 703, Loss: 0.9159601926803589, Accuracy: 1.0, Computation time: 0.9988093376159668\n",
      "Step: 704, Loss: 0.9401246905326843, Accuracy: 0.9750000238418579, Computation time: 1.046471118927002\n",
      "Step: 705, Loss: 0.9162341952323914, Accuracy: 1.0, Computation time: 1.0171246528625488\n",
      "Step: 706, Loss: 0.9160398244857788, Accuracy: 1.0, Computation time: 1.024852991104126\n",
      "Step: 707, Loss: 0.9344950318336487, Accuracy: 0.9722222089767456, Computation time: 1.4339547157287598\n",
      "Step: 708, Loss: 0.9160389304161072, Accuracy: 1.0, Computation time: 0.9467947483062744\n",
      "Step: 709, Loss: 0.9335463643074036, Accuracy: 0.9833333492279053, Computation time: 1.6991972923278809\n",
      "Step: 710, Loss: 0.9481706619262695, Accuracy: 0.96875, Computation time: 1.126312255859375\n",
      "Step: 711, Loss: 0.9161819219589233, Accuracy: 1.0, Computation time: 0.9834938049316406\n",
      "Step: 712, Loss: 0.9161202311515808, Accuracy: 1.0, Computation time: 1.0270755290985107\n",
      "Step: 713, Loss: 0.9338518977165222, Accuracy: 0.9791666865348816, Computation time: 1.1683692932128906\n",
      "Step: 714, Loss: 0.9317513704299927, Accuracy: 0.9583333730697632, Computation time: 1.437431812286377\n",
      "Step: 715, Loss: 0.9160809516906738, Accuracy: 1.0, Computation time: 1.0946846008300781\n",
      "Step: 716, Loss: 0.9340317249298096, Accuracy: 0.96875, Computation time: 1.8098530769348145\n",
      "Step: 717, Loss: 0.9353592991828918, Accuracy: 0.9750000238418579, Computation time: 1.4861106872558594\n",
      "Step: 718, Loss: 0.9159696698188782, Accuracy: 1.0, Computation time: 1.107985019683838\n",
      "Step: 719, Loss: 0.9160041213035583, Accuracy: 1.0, Computation time: 1.223935842514038\n",
      "Step: 720, Loss: 0.9168370366096497, Accuracy: 1.0, Computation time: 1.635460376739502\n",
      "Step: 721, Loss: 0.9162412881851196, Accuracy: 1.0, Computation time: 1.3391344547271729\n",
      "Step: 722, Loss: 0.9161378145217896, Accuracy: 1.0, Computation time: 1.3625500202178955\n",
      "Step: 723, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 1.1537485122680664\n",
      "Step: 724, Loss: 0.9376499056816101, Accuracy: 0.9772727489471436, Computation time: 1.2902553081512451\n",
      "Step: 725, Loss: 0.9374197721481323, Accuracy: 0.9791666865348816, Computation time: 1.6005184650421143\n",
      "Step: 726, Loss: 0.9159647822380066, Accuracy: 1.0, Computation time: 1.229841947555542\n",
      "Step: 727, Loss: 0.9208249449729919, Accuracy: 1.0, Computation time: 1.8726716041564941\n",
      "Step: 728, Loss: 0.9178539514541626, Accuracy: 1.0, Computation time: 1.0395309925079346\n",
      "Step: 729, Loss: 0.9162003993988037, Accuracy: 1.0, Computation time: 1.0920226573944092\n",
      "Step: 730, Loss: 0.9160778522491455, Accuracy: 1.0, Computation time: 0.8662412166595459\n",
      "Step: 731, Loss: 0.9162167906761169, Accuracy: 1.0, Computation time: 1.0968191623687744\n",
      "Step: 732, Loss: 0.9317734837532043, Accuracy: 0.9821428656578064, Computation time: 1.31577730178833\n",
      "Step: 733, Loss: 0.9160081148147583, Accuracy: 1.0, Computation time: 1.1688923835754395\n",
      "Step: 734, Loss: 0.9163030385971069, Accuracy: 1.0, Computation time: 1.266247272491455\n",
      "Step: 735, Loss: 0.931282639503479, Accuracy: 0.9642857313156128, Computation time: 1.169567584991455\n",
      "Step: 736, Loss: 0.9165480136871338, Accuracy: 1.0, Computation time: 1.4856560230255127\n",
      "Step: 737, Loss: 0.916138231754303, Accuracy: 1.0, Computation time: 1.0997676849365234\n",
      "Step: 738, Loss: 0.9160670042037964, Accuracy: 1.0, Computation time: 0.9909672737121582\n",
      "Step: 739, Loss: 0.9273383617401123, Accuracy: 0.9772727489471436, Computation time: 1.4342377185821533\n",
      "Step: 740, Loss: 0.9164695739746094, Accuracy: 1.0, Computation time: 1.2843873500823975\n",
      "Step: 741, Loss: 0.9160342812538147, Accuracy: 1.0, Computation time: 1.0234107971191406\n",
      "Step: 742, Loss: 0.9161273837089539, Accuracy: 1.0, Computation time: 1.1000723838806152\n",
      "Step: 743, Loss: 0.9250428676605225, Accuracy: 1.0, Computation time: 1.3868725299835205\n",
      "Step: 744, Loss: 0.9355534911155701, Accuracy: 0.9791666865348816, Computation time: 1.4540936946868896\n",
      "Step: 745, Loss: 0.9162219166755676, Accuracy: 1.0, Computation time: 1.0261409282684326\n",
      "Step: 746, Loss: 0.9177729487419128, Accuracy: 1.0, Computation time: 1.0542850494384766\n",
      "Step: 747, Loss: 0.9178038835525513, Accuracy: 1.0, Computation time: 1.247635841369629\n",
      "Step: 748, Loss: 0.9159796237945557, Accuracy: 1.0, Computation time: 0.969926118850708\n",
      "Step: 749, Loss: 0.9161927700042725, Accuracy: 1.0, Computation time: 1.1286237239837646\n",
      "Step: 750, Loss: 0.9376407265663147, Accuracy: 0.9791666865348816, Computation time: 1.3892924785614014\n",
      "Step: 751, Loss: 0.9160714149475098, Accuracy: 1.0, Computation time: 0.8824541568756104\n",
      "Step: 752, Loss: 0.9376190900802612, Accuracy: 0.9642857313156128, Computation time: 1.115889072418213\n",
      "Step: 753, Loss: 0.92432701587677, Accuracy: 1.0, Computation time: 1.7776463031768799\n",
      "Step: 754, Loss: 0.9164974093437195, Accuracy: 1.0, Computation time: 1.1496608257293701\n",
      "Step: 755, Loss: 0.9195924997329712, Accuracy: 1.0, Computation time: 1.210829734802246\n",
      "Step: 756, Loss: 0.9241275787353516, Accuracy: 1.0, Computation time: 1.3217461109161377\n",
      "Step: 757, Loss: 0.937242865562439, Accuracy: 0.9750000238418579, Computation time: 1.1457574367523193\n",
      "Step: 758, Loss: 0.9159396886825562, Accuracy: 1.0, Computation time: 1.1868822574615479\n",
      "Step: 759, Loss: 0.923300564289093, Accuracy: 1.0, Computation time: 1.3841493129730225\n",
      "Step: 760, Loss: 0.9288013577461243, Accuracy: 0.9750000238418579, Computation time: 1.3735508918762207\n",
      "Step: 761, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 1.2421841621398926\n",
      "Step: 762, Loss: 0.9159308671951294, Accuracy: 1.0, Computation time: 1.2726325988769531\n",
      "Step: 763, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 0.8879399299621582\n",
      "Step: 764, Loss: 0.9349494576454163, Accuracy: 0.9807692766189575, Computation time: 1.5753867626190186\n",
      "Step: 765, Loss: 0.9167057275772095, Accuracy: 1.0, Computation time: 1.3427503108978271\n",
      "Step: 766, Loss: 0.915982186794281, Accuracy: 1.0, Computation time: 1.3009090423583984\n",
      "Step: 767, Loss: 0.934809148311615, Accuracy: 0.9772727489471436, Computation time: 2.092836618423462\n",
      "Step: 768, Loss: 0.9159557223320007, Accuracy: 1.0, Computation time: 1.0453040599822998\n",
      "Step: 769, Loss: 0.9163888096809387, Accuracy: 1.0, Computation time: 1.037881851196289\n",
      "Step: 770, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 0.9837207794189453\n",
      "Step: 771, Loss: 0.9179440140724182, Accuracy: 1.0, Computation time: 1.6539766788482666\n",
      "Step: 772, Loss: 0.9161342978477478, Accuracy: 1.0, Computation time: 1.1677532196044922\n",
      "Step: 773, Loss: 0.9376107454299927, Accuracy: 0.9772727489471436, Computation time: 1.037172555923462\n",
      "Step: 774, Loss: 0.9160242676734924, Accuracy: 1.0, Computation time: 1.3378915786743164\n",
      "Step: 775, Loss: 0.9199022054672241, Accuracy: 1.0, Computation time: 1.280059814453125\n",
      "Step: 776, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.1049933433532715\n",
      "Step: 777, Loss: 0.9164109826087952, Accuracy: 1.0, Computation time: 1.133819341659546\n",
      "Step: 778, Loss: 0.931944727897644, Accuracy: 0.9750000238418579, Computation time: 1.9770963191986084\n",
      "Step: 779, Loss: 0.9160065054893494, Accuracy: 1.0, Computation time: 1.489863634109497\n",
      "Step: 780, Loss: 0.9210475087165833, Accuracy: 1.0, Computation time: 1.4540679454803467\n",
      "Step: 781, Loss: 0.9160211682319641, Accuracy: 1.0, Computation time: 1.019714593887329\n",
      "Step: 782, Loss: 0.9160050749778748, Accuracy: 1.0, Computation time: 1.2266879081726074\n",
      "Step: 783, Loss: 0.9161278605461121, Accuracy: 1.0, Computation time: 1.1976032257080078\n",
      "Step: 784, Loss: 0.9162159562110901, Accuracy: 1.0, Computation time: 1.3449816703796387\n",
      "Step: 785, Loss: 0.9364535808563232, Accuracy: 0.9821428656578064, Computation time: 1.021620273590088\n",
      "Step: 786, Loss: 0.957791268825531, Accuracy: 0.9472222328186035, Computation time: 1.2219212055206299\n",
      "Step: 787, Loss: 0.9162670373916626, Accuracy: 1.0, Computation time: 1.2104847431182861\n",
      "Step: 788, Loss: 0.9163943529129028, Accuracy: 1.0, Computation time: 1.0032784938812256\n",
      "Step: 789, Loss: 0.9344125986099243, Accuracy: 0.9807692766189575, Computation time: 2.549248456954956\n",
      "Step: 790, Loss: 0.9160648584365845, Accuracy: 1.0, Computation time: 1.2266452312469482\n",
      "Step: 791, Loss: 0.9570598602294922, Accuracy: 0.9494949579238892, Computation time: 1.711533546447754\n",
      "Step: 792, Loss: 0.9537990093231201, Accuracy: 0.9444444179534912, Computation time: 1.9780900478363037\n",
      "Step: 793, Loss: 0.918414294719696, Accuracy: 1.0, Computation time: 1.1567034721374512\n",
      "Step: 794, Loss: 0.9160736799240112, Accuracy: 1.0, Computation time: 0.9696009159088135\n",
      "Step: 795, Loss: 0.916214108467102, Accuracy: 1.0, Computation time: 1.3023736476898193\n",
      "Step: 796, Loss: 0.9162331819534302, Accuracy: 1.0, Computation time: 1.2315945625305176\n",
      "Step: 797, Loss: 0.9161432385444641, Accuracy: 1.0, Computation time: 1.1372637748718262\n",
      "Step: 798, Loss: 0.9163297414779663, Accuracy: 1.0, Computation time: 1.010817527770996\n",
      "Step: 799, Loss: 0.9162300825119019, Accuracy: 1.0, Computation time: 1.243065595626831\n",
      "Step: 800, Loss: 0.9367849230766296, Accuracy: 0.9750000238418579, Computation time: 1.0876259803771973\n",
      "Step: 801, Loss: 0.9160356521606445, Accuracy: 1.0, Computation time: 1.2838432788848877\n",
      "Step: 802, Loss: 0.91619473695755, Accuracy: 1.0, Computation time: 1.4021823406219482\n",
      "Step: 803, Loss: 0.9337610602378845, Accuracy: 0.9750000238418579, Computation time: 1.4937794208526611\n",
      "Step: 804, Loss: 0.9169604778289795, Accuracy: 1.0, Computation time: 1.2976105213165283\n",
      "Step: 805, Loss: 0.9161844253540039, Accuracy: 1.0, Computation time: 2.1132876873016357\n",
      "Step: 806, Loss: 0.9160687327384949, Accuracy: 1.0, Computation time: 1.2797167301177979\n",
      "Step: 807, Loss: 0.9312354922294617, Accuracy: 0.9583333730697632, Computation time: 1.8419215679168701\n",
      "Step: 808, Loss: 0.9160853028297424, Accuracy: 1.0, Computation time: 1.0953662395477295\n",
      "Step: 809, Loss: 0.9166086912155151, Accuracy: 1.0, Computation time: 1.0365338325500488\n",
      "Step: 810, Loss: 0.9161308407783508, Accuracy: 1.0, Computation time: 1.0947701930999756\n",
      "Step: 811, Loss: 0.9318045377731323, Accuracy: 0.9642857313156128, Computation time: 1.45676589012146\n",
      "Step: 812, Loss: 0.9164626598358154, Accuracy: 1.0, Computation time: 1.454918384552002\n",
      "Step: 813, Loss: 0.9161331057548523, Accuracy: 1.0, Computation time: 1.4572052955627441\n",
      "Step: 814, Loss: 0.9161320328712463, Accuracy: 1.0, Computation time: 1.044198989868164\n",
      "Step: 815, Loss: 0.9558035135269165, Accuracy: 0.925000011920929, Computation time: 1.4609150886535645\n",
      "Step: 816, Loss: 0.9161712527275085, Accuracy: 1.0, Computation time: 1.1235332489013672\n",
      "Step: 817, Loss: 0.9166809916496277, Accuracy: 1.0, Computation time: 1.2457828521728516\n",
      "Step: 818, Loss: 0.9159667491912842, Accuracy: 1.0, Computation time: 0.9865403175354004\n",
      "Step: 819, Loss: 0.9162587523460388, Accuracy: 1.0, Computation time: 1.0182595252990723\n",
      "Step: 820, Loss: 0.9163216948509216, Accuracy: 1.0, Computation time: 1.1859772205352783\n",
      "Step: 821, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 1.0566003322601318\n",
      "Step: 822, Loss: 0.9159938097000122, Accuracy: 1.0, Computation time: 1.0909974575042725\n",
      "Step: 823, Loss: 0.9160314798355103, Accuracy: 1.0, Computation time: 0.9594802856445312\n",
      "Step: 824, Loss: 0.9159850478172302, Accuracy: 1.0, Computation time: 0.9568140506744385\n",
      "Step: 825, Loss: 0.9160938262939453, Accuracy: 1.0, Computation time: 1.2357347011566162\n",
      "Step: 826, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 0.9751753807067871\n",
      "Step: 827, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 0.9613635540008545\n",
      "Step: 828, Loss: 0.9377210736274719, Accuracy: 0.9750000238418579, Computation time: 1.2703840732574463\n",
      "Step: 829, Loss: 0.916126549243927, Accuracy: 1.0, Computation time: 1.2950656414031982\n",
      "Step: 830, Loss: 0.9354466795921326, Accuracy: 0.96875, Computation time: 1.194115161895752\n",
      "Step: 831, Loss: 0.91712886095047, Accuracy: 1.0, Computation time: 1.5423731803894043\n",
      "Step: 832, Loss: 0.9375530481338501, Accuracy: 0.9722222089767456, Computation time: 1.2047080993652344\n",
      "Step: 833, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 1.1835269927978516\n",
      "Step: 834, Loss: 0.9361213445663452, Accuracy: 0.9772727489471436, Computation time: 1.9898834228515625\n",
      "########################\n",
      "Test loss: 1.1232986450195312, Test Accuracy_epoch6: 0.6888967752456665\n",
      "########################\n",
      "Step: 835, Loss: 0.9205073714256287, Accuracy: 1.0, Computation time: 1.764317274093628\n",
      "Step: 836, Loss: 0.9164746403694153, Accuracy: 1.0, Computation time: 1.167226791381836\n",
      "Step: 837, Loss: 0.9588668346405029, Accuracy: 0.9409722089767456, Computation time: 1.3417863845825195\n",
      "Step: 838, Loss: 0.9159848690032959, Accuracy: 1.0, Computation time: 1.1602106094360352\n",
      "Step: 839, Loss: 0.9172798991203308, Accuracy: 1.0, Computation time: 1.3162364959716797\n",
      "Step: 840, Loss: 0.9159737825393677, Accuracy: 1.0, Computation time: 1.2024216651916504\n",
      "Step: 841, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 1.079977035522461\n",
      "Step: 842, Loss: 0.9366990923881531, Accuracy: 0.9750000238418579, Computation time: 1.6234874725341797\n",
      "Step: 843, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 0.9406793117523193\n",
      "Step: 844, Loss: 0.915947675704956, Accuracy: 1.0, Computation time: 1.0594511032104492\n",
      "Step: 845, Loss: 0.9159196019172668, Accuracy: 1.0, Computation time: 1.741339921951294\n",
      "Step: 846, Loss: 0.9159446358680725, Accuracy: 1.0, Computation time: 1.3539679050445557\n",
      "Step: 847, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 1.315911054611206\n",
      "Step: 848, Loss: 0.9206753373146057, Accuracy: 1.0, Computation time: 1.1399154663085938\n",
      "Step: 849, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 0.9678959846496582\n",
      "Step: 850, Loss: 0.9165328145027161, Accuracy: 1.0, Computation time: 1.5466649532318115\n",
      "Step: 851, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.3277595043182373\n",
      "Step: 852, Loss: 0.9160112738609314, Accuracy: 1.0, Computation time: 1.323173999786377\n",
      "Step: 853, Loss: 0.9394892454147339, Accuracy: 0.9583333730697632, Computation time: 1.3802218437194824\n",
      "Step: 854, Loss: 0.915983259677887, Accuracy: 1.0, Computation time: 1.0267043113708496\n",
      "Step: 855, Loss: 0.9160293340682983, Accuracy: 1.0, Computation time: 1.4206817150115967\n",
      "Step: 856, Loss: 0.9162025451660156, Accuracy: 1.0, Computation time: 1.070594072341919\n",
      "Step: 857, Loss: 0.9377051591873169, Accuracy: 0.9868420958518982, Computation time: 1.4194121360778809\n",
      "Step: 858, Loss: 0.9161003232002258, Accuracy: 1.0, Computation time: 1.1520183086395264\n",
      "Step: 859, Loss: 0.9326483011245728, Accuracy: 0.9722222089767456, Computation time: 1.2003495693206787\n",
      "Step: 860, Loss: 0.9159864783287048, Accuracy: 1.0, Computation time: 1.0750126838684082\n",
      "Step: 861, Loss: 0.9169707894325256, Accuracy: 1.0, Computation time: 0.9959886074066162\n",
      "Step: 862, Loss: 0.9441608190536499, Accuracy: 0.9791666865348816, Computation time: 1.4687435626983643\n",
      "Step: 863, Loss: 0.916096568107605, Accuracy: 1.0, Computation time: 1.0387520790100098\n",
      "Step: 864, Loss: 0.9161970615386963, Accuracy: 1.0, Computation time: 1.0838539600372314\n",
      "Step: 865, Loss: 0.9165517091751099, Accuracy: 1.0, Computation time: 1.116560459136963\n",
      "Step: 866, Loss: 0.939911961555481, Accuracy: 0.96875, Computation time: 1.1077392101287842\n",
      "Step: 867, Loss: 0.9161906838417053, Accuracy: 1.0, Computation time: 1.3030574321746826\n",
      "Step: 868, Loss: 0.9403171539306641, Accuracy: 0.9807692766189575, Computation time: 1.2163879871368408\n",
      "Step: 869, Loss: 0.9378927946090698, Accuracy: 0.949999988079071, Computation time: 1.1254959106445312\n",
      "Step: 870, Loss: 0.9160865545272827, Accuracy: 1.0, Computation time: 1.3994405269622803\n",
      "Step: 871, Loss: 0.9195031523704529, Accuracy: 1.0, Computation time: 1.2682056427001953\n",
      "Step: 872, Loss: 0.9160976409912109, Accuracy: 1.0, Computation time: 0.985414981842041\n",
      "Step: 873, Loss: 0.9395242929458618, Accuracy: 0.9166666865348816, Computation time: 1.2265453338623047\n",
      "Step: 874, Loss: 0.9161366820335388, Accuracy: 1.0, Computation time: 1.158919334411621\n",
      "Step: 875, Loss: 0.9160431027412415, Accuracy: 1.0, Computation time: 1.8264493942260742\n",
      "Step: 876, Loss: 0.916054904460907, Accuracy: 1.0, Computation time: 1.4136929512023926\n",
      "Step: 877, Loss: 0.9159374833106995, Accuracy: 1.0, Computation time: 1.2777979373931885\n",
      "Step: 878, Loss: 0.9164642691612244, Accuracy: 1.0, Computation time: 1.302889108657837\n",
      "Step: 879, Loss: 0.9245930910110474, Accuracy: 1.0, Computation time: 1.1550021171569824\n",
      "Step: 880, Loss: 0.9159276485443115, Accuracy: 1.0, Computation time: 1.2105631828308105\n",
      "Step: 881, Loss: 0.9159421920776367, Accuracy: 1.0, Computation time: 1.2993268966674805\n",
      "Step: 882, Loss: 0.9363017082214355, Accuracy: 0.9772727489471436, Computation time: 1.3718101978302002\n",
      "Step: 883, Loss: 0.9166483879089355, Accuracy: 1.0, Computation time: 1.371154546737671\n",
      "Step: 884, Loss: 0.9374758005142212, Accuracy: 0.9642857313156128, Computation time: 1.1013424396514893\n",
      "Step: 885, Loss: 0.937493085861206, Accuracy: 0.9642857313156128, Computation time: 1.1720151901245117\n",
      "Step: 886, Loss: 0.9255387187004089, Accuracy: 0.9642857313156128, Computation time: 1.3632493019104004\n",
      "Step: 887, Loss: 0.9162172079086304, Accuracy: 1.0, Computation time: 1.284834623336792\n",
      "Step: 888, Loss: 0.9218619465827942, Accuracy: 1.0, Computation time: 1.0932159423828125\n",
      "Step: 889, Loss: 0.9161983132362366, Accuracy: 1.0, Computation time: 1.3418445587158203\n",
      "Step: 890, Loss: 0.9160599708557129, Accuracy: 1.0, Computation time: 1.1342899799346924\n",
      "Step: 891, Loss: 0.9194541573524475, Accuracy: 1.0, Computation time: 1.9583923816680908\n",
      "Step: 892, Loss: 0.9167382717132568, Accuracy: 1.0, Computation time: 1.4619793891906738\n",
      "Step: 893, Loss: 0.9159798622131348, Accuracy: 1.0, Computation time: 1.5103528499603271\n",
      "Step: 894, Loss: 0.9540970325469971, Accuracy: 0.9555555582046509, Computation time: 1.6690776348114014\n",
      "Step: 895, Loss: 0.916146993637085, Accuracy: 1.0, Computation time: 1.2967164516448975\n",
      "Step: 896, Loss: 0.9161288142204285, Accuracy: 1.0, Computation time: 1.2217442989349365\n",
      "Step: 897, Loss: 0.926689624786377, Accuracy: 0.9722222089767456, Computation time: 1.4952151775360107\n",
      "Step: 898, Loss: 0.9168458580970764, Accuracy: 1.0, Computation time: 1.107576847076416\n",
      "Step: 899, Loss: 0.9166955947875977, Accuracy: 1.0, Computation time: 1.690453052520752\n",
      "Step: 900, Loss: 0.9161412715911865, Accuracy: 1.0, Computation time: 1.7998464107513428\n",
      "Step: 901, Loss: 0.9160546660423279, Accuracy: 1.0, Computation time: 1.444101095199585\n",
      "Step: 902, Loss: 0.9173280596733093, Accuracy: 1.0, Computation time: 1.4375016689300537\n",
      "Step: 903, Loss: 0.922354519367218, Accuracy: 1.0, Computation time: 1.8347234725952148\n",
      "Step: 904, Loss: 0.9160048961639404, Accuracy: 1.0, Computation time: 1.3257534503936768\n",
      "Step: 905, Loss: 0.9260512590408325, Accuracy: 0.9791666865348816, Computation time: 1.6201903820037842\n",
      "Step: 906, Loss: 0.9402536153793335, Accuracy: 0.9722222089767456, Computation time: 1.2648658752441406\n",
      "Step: 907, Loss: 0.9377962350845337, Accuracy: 0.9722222089767456, Computation time: 1.5136244297027588\n",
      "Step: 908, Loss: 0.9378618001937866, Accuracy: 0.9750000238418579, Computation time: 1.7275431156158447\n",
      "Step: 909, Loss: 0.9162564277648926, Accuracy: 1.0, Computation time: 1.4380967617034912\n",
      "Step: 910, Loss: 0.9376394748687744, Accuracy: 0.96875, Computation time: 1.2587919235229492\n",
      "Step: 911, Loss: 0.9243409633636475, Accuracy: 1.0, Computation time: 1.5515830516815186\n",
      "Step: 912, Loss: 0.9165278077125549, Accuracy: 1.0, Computation time: 1.2828540802001953\n",
      "Step: 913, Loss: 0.9167356491088867, Accuracy: 1.0, Computation time: 1.2996010780334473\n",
      "Step: 914, Loss: 0.9164610505104065, Accuracy: 1.0, Computation time: 1.2946031093597412\n",
      "Step: 915, Loss: 0.9163081645965576, Accuracy: 1.0, Computation time: 1.389704704284668\n",
      "Step: 916, Loss: 0.9162988066673279, Accuracy: 1.0, Computation time: 1.093503475189209\n",
      "Step: 917, Loss: 0.9361494779586792, Accuracy: 0.9722222089767456, Computation time: 1.7855219841003418\n",
      "Step: 918, Loss: 0.9159585237503052, Accuracy: 1.0, Computation time: 1.003718376159668\n",
      "Step: 919, Loss: 0.9160120487213135, Accuracy: 1.0, Computation time: 1.238126516342163\n",
      "Step: 920, Loss: 0.9166691303253174, Accuracy: 1.0, Computation time: 1.1980161666870117\n",
      "Step: 921, Loss: 0.9160426259040833, Accuracy: 1.0, Computation time: 1.1276121139526367\n",
      "Step: 922, Loss: 0.9160642623901367, Accuracy: 1.0, Computation time: 1.333233118057251\n",
      "Step: 923, Loss: 0.9372769594192505, Accuracy: 0.9583333730697632, Computation time: 1.1397557258605957\n",
      "Step: 924, Loss: 0.9375059604644775, Accuracy: 0.9750000238418579, Computation time: 1.2211802005767822\n",
      "Step: 925, Loss: 0.9499108791351318, Accuracy: 0.9375, Computation time: 1.5383546352386475\n",
      "Step: 926, Loss: 0.9164454936981201, Accuracy: 1.0, Computation time: 1.3212635517120361\n",
      "Step: 927, Loss: 0.9587196707725525, Accuracy: 0.949999988079071, Computation time: 1.5639753341674805\n",
      "Step: 928, Loss: 0.9175100326538086, Accuracy: 1.0, Computation time: 1.3927648067474365\n",
      "Step: 929, Loss: 0.9161124229431152, Accuracy: 1.0, Computation time: 1.0616474151611328\n",
      "Step: 930, Loss: 0.9160892367362976, Accuracy: 1.0, Computation time: 1.1860103607177734\n",
      "Step: 931, Loss: 0.916104793548584, Accuracy: 1.0, Computation time: 1.159907579421997\n",
      "Step: 932, Loss: 0.915999174118042, Accuracy: 1.0, Computation time: 1.0178205966949463\n",
      "Step: 933, Loss: 0.9296367168426514, Accuracy: 1.0, Computation time: 1.2743415832519531\n",
      "Step: 934, Loss: 0.9159764647483826, Accuracy: 1.0, Computation time: 1.1696245670318604\n",
      "Step: 935, Loss: 0.9159987568855286, Accuracy: 1.0, Computation time: 0.9562864303588867\n",
      "Step: 936, Loss: 0.9161180853843689, Accuracy: 1.0, Computation time: 1.2605316638946533\n",
      "Step: 937, Loss: 0.9161367416381836, Accuracy: 1.0, Computation time: 0.9837815761566162\n",
      "Step: 938, Loss: 0.9159489870071411, Accuracy: 1.0, Computation time: 1.0329186916351318\n",
      "Step: 939, Loss: 0.9375600218772888, Accuracy: 0.96875, Computation time: 1.4421288967132568\n",
      "Step: 940, Loss: 0.915995180606842, Accuracy: 1.0, Computation time: 0.9449253082275391\n",
      "Step: 941, Loss: 0.9161470532417297, Accuracy: 1.0, Computation time: 1.1164116859436035\n",
      "Step: 942, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 1.123136043548584\n",
      "Step: 943, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 1.0828332901000977\n",
      "Step: 944, Loss: 0.9215338826179504, Accuracy: 1.0, Computation time: 1.1364548206329346\n",
      "Step: 945, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.07285737991333\n",
      "Step: 946, Loss: 0.9376510381698608, Accuracy: 0.96875, Computation time: 1.048612356185913\n",
      "Step: 947, Loss: 0.9159179925918579, Accuracy: 1.0, Computation time: 0.8943800926208496\n",
      "Step: 948, Loss: 0.9162373542785645, Accuracy: 1.0, Computation time: 1.1402630805969238\n",
      "Step: 949, Loss: 0.9166097640991211, Accuracy: 1.0, Computation time: 1.2204673290252686\n",
      "Step: 950, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 1.0740141868591309\n",
      "Step: 951, Loss: 0.9252853989601135, Accuracy: 1.0, Computation time: 1.3486952781677246\n",
      "Step: 952, Loss: 0.9372118711471558, Accuracy: 0.96875, Computation time: 0.9363150596618652\n",
      "Step: 953, Loss: 0.9519564509391785, Accuracy: 0.9365079402923584, Computation time: 1.5629911422729492\n",
      "Step: 954, Loss: 0.9159501194953918, Accuracy: 1.0, Computation time: 1.247922658920288\n",
      "Step: 955, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 1.1914474964141846\n",
      "Step: 956, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 1.070683479309082\n",
      "Step: 957, Loss: 0.9369372129440308, Accuracy: 0.9821428656578064, Computation time: 1.2212820053100586\n",
      "Step: 958, Loss: 0.9168696403503418, Accuracy: 1.0, Computation time: 0.9941532611846924\n",
      "Step: 959, Loss: 0.9265448451042175, Accuracy: 0.96875, Computation time: 0.848280668258667\n",
      "Step: 960, Loss: 0.9332780241966248, Accuracy: 0.9821428656578064, Computation time: 0.918147087097168\n",
      "Step: 961, Loss: 0.9382817149162292, Accuracy: 0.9791666865348816, Computation time: 1.1662566661834717\n",
      "Step: 962, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 0.8997476100921631\n",
      "Step: 963, Loss: 0.9167742133140564, Accuracy: 1.0, Computation time: 0.8809289932250977\n",
      "Step: 964, Loss: 0.9162351489067078, Accuracy: 1.0, Computation time: 0.8793668746948242\n",
      "Step: 965, Loss: 0.9161746501922607, Accuracy: 1.0, Computation time: 1.0180654525756836\n",
      "Step: 966, Loss: 0.9202719926834106, Accuracy: 1.0, Computation time: 1.3428826332092285\n",
      "Step: 967, Loss: 0.9398667812347412, Accuracy: 0.9722222089767456, Computation time: 1.0930397510528564\n",
      "Step: 968, Loss: 0.9290785789489746, Accuracy: 0.9722222089767456, Computation time: 1.9021763801574707\n",
      "Step: 969, Loss: 0.9166650772094727, Accuracy: 1.0, Computation time: 1.0048668384552002\n",
      "Step: 970, Loss: 0.9161376953125, Accuracy: 1.0, Computation time: 1.1036851406097412\n",
      "Step: 971, Loss: 0.9185603260993958, Accuracy: 1.0, Computation time: 1.1618084907531738\n",
      "Step: 972, Loss: 0.9161880016326904, Accuracy: 1.0, Computation time: 0.9398279190063477\n",
      "Step: 973, Loss: 0.9163511395454407, Accuracy: 1.0, Computation time: 1.3990979194641113\n",
      "########################\n",
      "Test loss: 1.123966932296753, Test Accuracy_epoch7: 0.6810890436172485\n",
      "########################\n",
      "Step: 974, Loss: 0.9165366888046265, Accuracy: 1.0, Computation time: 1.2722806930541992\n",
      "Step: 975, Loss: 0.9361402988433838, Accuracy: 0.9642857313156128, Computation time: 1.2369017601013184\n",
      "Step: 976, Loss: 0.9334895014762878, Accuracy: 0.9722222089767456, Computation time: 1.2983500957489014\n",
      "Step: 977, Loss: 0.9161334037780762, Accuracy: 1.0, Computation time: 1.2872016429901123\n",
      "Step: 978, Loss: 0.9162628650665283, Accuracy: 1.0, Computation time: 1.4004576206207275\n",
      "Step: 979, Loss: 0.9329987168312073, Accuracy: 0.9772727489471436, Computation time: 1.1287496089935303\n",
      "Step: 980, Loss: 0.9379690289497375, Accuracy: 0.9642857313156128, Computation time: 1.1849985122680664\n",
      "Step: 981, Loss: 0.9161601066589355, Accuracy: 1.0, Computation time: 1.209061622619629\n",
      "Step: 982, Loss: 0.9159337282180786, Accuracy: 1.0, Computation time: 1.2252089977264404\n",
      "Step: 983, Loss: 0.9176778197288513, Accuracy: 1.0, Computation time: 1.3686182498931885\n",
      "Step: 984, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 1.0736539363861084\n",
      "Step: 985, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 1.240121841430664\n",
      "Step: 986, Loss: 0.9159672856330872, Accuracy: 1.0, Computation time: 1.4134998321533203\n",
      "Step: 987, Loss: 0.9160090088844299, Accuracy: 1.0, Computation time: 1.476632833480835\n",
      "Step: 988, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.2336714267730713\n",
      "Step: 989, Loss: 0.9354383945465088, Accuracy: 0.96875, Computation time: 1.1385278701782227\n",
      "Step: 990, Loss: 0.937279462814331, Accuracy: 0.9722222089767456, Computation time: 1.0843656063079834\n",
      "Step: 991, Loss: 0.9162339568138123, Accuracy: 1.0, Computation time: 1.3234920501708984\n",
      "Step: 992, Loss: 0.9159446358680725, Accuracy: 1.0, Computation time: 1.0066673755645752\n",
      "Step: 993, Loss: 0.9160757064819336, Accuracy: 1.0, Computation time: 1.1915996074676514\n",
      "Step: 994, Loss: 0.9159368872642517, Accuracy: 1.0, Computation time: 1.2387022972106934\n",
      "Step: 995, Loss: 0.9159967303276062, Accuracy: 1.0, Computation time: 1.1519653797149658\n",
      "Step: 996, Loss: 0.9279584884643555, Accuracy: 1.0, Computation time: 1.4238924980163574\n",
      "Step: 997, Loss: 0.9371211528778076, Accuracy: 0.9750000238418579, Computation time: 1.22359299659729\n",
      "Step: 998, Loss: 0.9167134165763855, Accuracy: 1.0, Computation time: 1.0970520973205566\n",
      "Step: 999, Loss: 0.9161218404769897, Accuracy: 1.0, Computation time: 1.1965863704681396\n",
      "Step: 1000, Loss: 0.915976345539093, Accuracy: 1.0, Computation time: 1.3600566387176514\n",
      "Step: 1001, Loss: 0.9161891937255859, Accuracy: 1.0, Computation time: 1.086289405822754\n",
      "Step: 1002, Loss: 0.9167641997337341, Accuracy: 1.0, Computation time: 1.0693233013153076\n",
      "Step: 1003, Loss: 0.916016697883606, Accuracy: 1.0, Computation time: 1.198166847229004\n",
      "Step: 1004, Loss: 0.9159474968910217, Accuracy: 1.0, Computation time: 1.1824193000793457\n",
      "Step: 1005, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 1.6841895580291748\n",
      "Step: 1006, Loss: 0.9160062670707703, Accuracy: 1.0, Computation time: 1.1381213665008545\n",
      "Step: 1007, Loss: 0.9335299730300903, Accuracy: 0.9791666865348816, Computation time: 1.4373393058776855\n",
      "Step: 1008, Loss: 0.937732458114624, Accuracy: 0.9772727489471436, Computation time: 1.087449312210083\n",
      "Step: 1009, Loss: 0.916023850440979, Accuracy: 1.0, Computation time: 1.0445291996002197\n",
      "Step: 1010, Loss: 0.9159495830535889, Accuracy: 1.0, Computation time: 0.8890683650970459\n",
      "Step: 1011, Loss: 0.9159419536590576, Accuracy: 1.0, Computation time: 0.9792189598083496\n",
      "Step: 1012, Loss: 0.9369833469390869, Accuracy: 0.9375, Computation time: 1.5931415557861328\n",
      "Step: 1013, Loss: 0.9259527325630188, Accuracy: 0.9642857313156128, Computation time: 1.3690969944000244\n",
      "Step: 1014, Loss: 0.9272102117538452, Accuracy: 0.9791666865348816, Computation time: 1.3165080547332764\n",
      "Step: 1015, Loss: 0.9163693785667419, Accuracy: 1.0, Computation time: 1.05043363571167\n",
      "Step: 1016, Loss: 0.9295356869697571, Accuracy: 0.9642857313156128, Computation time: 1.1231579780578613\n",
      "Step: 1017, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 1.2985913753509521\n",
      "Step: 1018, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 1.0364410877227783\n",
      "Step: 1019, Loss: 0.917579174041748, Accuracy: 1.0, Computation time: 1.0896849632263184\n",
      "Step: 1020, Loss: 0.9166877865791321, Accuracy: 1.0, Computation time: 1.4798355102539062\n",
      "Step: 1021, Loss: 0.9162798523902893, Accuracy: 1.0, Computation time: 1.0733625888824463\n",
      "Step: 1022, Loss: 0.958483874797821, Accuracy: 0.9375, Computation time: 1.11250901222229\n",
      "Step: 1023, Loss: 0.9367378354072571, Accuracy: 0.9791666865348816, Computation time: 1.2156317234039307\n",
      "Step: 1024, Loss: 0.916370153427124, Accuracy: 1.0, Computation time: 1.19663667678833\n",
      "Step: 1025, Loss: 0.9159659147262573, Accuracy: 1.0, Computation time: 1.0087165832519531\n",
      "Step: 1026, Loss: 0.9352017045021057, Accuracy: 0.9791666865348816, Computation time: 1.1559662818908691\n",
      "Step: 1027, Loss: 0.9191961288452148, Accuracy: 1.0, Computation time: 1.2368049621582031\n",
      "Step: 1028, Loss: 0.9172824025154114, Accuracy: 1.0, Computation time: 1.6664237976074219\n",
      "Step: 1029, Loss: 0.9159172773361206, Accuracy: 1.0, Computation time: 1.510066270828247\n",
      "Step: 1030, Loss: 0.9400649666786194, Accuracy: 0.9642857313156128, Computation time: 1.1099185943603516\n",
      "Step: 1031, Loss: 0.9166459441184998, Accuracy: 1.0, Computation time: 1.1647164821624756\n",
      "Step: 1032, Loss: 0.9160436987876892, Accuracy: 1.0, Computation time: 1.0768659114837646\n",
      "Step: 1033, Loss: 0.937624454498291, Accuracy: 0.9750000238418579, Computation time: 1.0828931331634521\n",
      "Step: 1034, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.0068061351776123\n",
      "Step: 1035, Loss: 0.9184771180152893, Accuracy: 1.0, Computation time: 1.1809680461883545\n",
      "Step: 1036, Loss: 0.9159709215164185, Accuracy: 1.0, Computation time: 1.3310620784759521\n",
      "Step: 1037, Loss: 0.9159483313560486, Accuracy: 1.0, Computation time: 1.2279999256134033\n",
      "Step: 1038, Loss: 0.9177226424217224, Accuracy: 1.0, Computation time: 1.3790900707244873\n",
      "Step: 1039, Loss: 0.9159473180770874, Accuracy: 1.0, Computation time: 1.4486520290374756\n",
      "Step: 1040, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 0.8651304244995117\n",
      "Step: 1041, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 1.097465991973877\n",
      "Step: 1042, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 0.9346163272857666\n",
      "Step: 1043, Loss: 0.9197940826416016, Accuracy: 1.0, Computation time: 1.0632247924804688\n",
      "Step: 1044, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 0.9834394454956055\n",
      "Step: 1045, Loss: 0.9159996509552002, Accuracy: 1.0, Computation time: 1.0663104057312012\n",
      "Step: 1046, Loss: 0.9160110354423523, Accuracy: 1.0, Computation time: 1.1163568496704102\n",
      "Step: 1047, Loss: 0.9160993695259094, Accuracy: 1.0, Computation time: 1.1325902938842773\n",
      "Step: 1048, Loss: 0.9339806437492371, Accuracy: 0.96875, Computation time: 1.7572112083435059\n",
      "Step: 1049, Loss: 0.9558085203170776, Accuracy: 0.9375, Computation time: 1.9479732513427734\n",
      "Step: 1050, Loss: 0.9160686135292053, Accuracy: 1.0, Computation time: 1.039764404296875\n",
      "Step: 1051, Loss: 0.916215717792511, Accuracy: 1.0, Computation time: 1.0999913215637207\n",
      "Step: 1052, Loss: 0.9163448214530945, Accuracy: 1.0, Computation time: 1.0539298057556152\n",
      "Step: 1053, Loss: 0.9169131517410278, Accuracy: 1.0, Computation time: 1.531541347503662\n",
      "Step: 1054, Loss: 0.938270092010498, Accuracy: 0.9791666865348816, Computation time: 1.9456963539123535\n",
      "Step: 1055, Loss: 0.9374311566352844, Accuracy: 0.9772727489471436, Computation time: 1.035217523574829\n",
      "Step: 1056, Loss: 0.9160155653953552, Accuracy: 1.0, Computation time: 1.0478081703186035\n",
      "Step: 1057, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 1.0505249500274658\n",
      "Step: 1058, Loss: 0.9160292744636536, Accuracy: 1.0, Computation time: 1.3231568336486816\n",
      "Step: 1059, Loss: 0.9159866571426392, Accuracy: 1.0, Computation time: 1.1228723526000977\n",
      "Step: 1060, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 1.100667953491211\n",
      "Step: 1061, Loss: 0.9160482883453369, Accuracy: 1.0, Computation time: 1.03641939163208\n",
      "Step: 1062, Loss: 0.9159864783287048, Accuracy: 1.0, Computation time: 1.2466387748718262\n",
      "Step: 1063, Loss: 0.9159295558929443, Accuracy: 1.0, Computation time: 1.31020188331604\n",
      "Step: 1064, Loss: 0.91620934009552, Accuracy: 1.0, Computation time: 1.1108152866363525\n",
      "Step: 1065, Loss: 0.9159619808197021, Accuracy: 1.0, Computation time: 1.2726545333862305\n",
      "Step: 1066, Loss: 0.9159996509552002, Accuracy: 1.0, Computation time: 1.3110473155975342\n",
      "Step: 1067, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 1.250669240951538\n",
      "Step: 1068, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 1.0894362926483154\n",
      "Step: 1069, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 0.975956916809082\n",
      "Step: 1070, Loss: 0.9530318379402161, Accuracy: 0.9083333015441895, Computation time: 1.0992493629455566\n",
      "Step: 1071, Loss: 0.9197980761528015, Accuracy: 1.0, Computation time: 1.2452623844146729\n",
      "Step: 1072, Loss: 0.9374867081642151, Accuracy: 0.9722222089767456, Computation time: 0.9983055591583252\n",
      "Step: 1073, Loss: 0.9163870811462402, Accuracy: 1.0, Computation time: 1.5676987171173096\n",
      "Step: 1074, Loss: 0.9160500764846802, Accuracy: 1.0, Computation time: 1.136357307434082\n",
      "Step: 1075, Loss: 0.9161605834960938, Accuracy: 1.0, Computation time: 0.9232759475708008\n",
      "Step: 1076, Loss: 0.916003406047821, Accuracy: 1.0, Computation time: 0.9806559085845947\n",
      "Step: 1077, Loss: 0.9159636497497559, Accuracy: 1.0, Computation time: 1.1821494102478027\n",
      "Step: 1078, Loss: 0.9587957262992859, Accuracy: 0.9434524178504944, Computation time: 1.2198638916015625\n",
      "Step: 1079, Loss: 0.9376425743103027, Accuracy: 0.96875, Computation time: 0.8758077621459961\n",
      "Step: 1080, Loss: 0.9375296831130981, Accuracy: 0.9821428656578064, Computation time: 1.075777292251587\n",
      "Step: 1081, Loss: 0.9188798069953918, Accuracy: 1.0, Computation time: 1.2744462490081787\n",
      "Step: 1082, Loss: 0.9160532355308533, Accuracy: 1.0, Computation time: 0.888214111328125\n",
      "Step: 1083, Loss: 0.9379620552062988, Accuracy: 0.9791666865348816, Computation time: 0.9925997257232666\n",
      "Step: 1084, Loss: 0.9191094636917114, Accuracy: nan, Computation time: 2.0770187377929688\n",
      "Step: 1085, Loss: 0.9162504076957703, Accuracy: 1.0, Computation time: 1.1734874248504639\n",
      "Step: 1086, Loss: 0.9403275847434998, Accuracy: 0.9750000238418579, Computation time: 1.1415998935699463\n",
      "Step: 1087, Loss: 0.9162696003913879, Accuracy: 1.0, Computation time: 1.3247168064117432\n",
      "Step: 1088, Loss: 0.9179685115814209, Accuracy: 1.0, Computation time: 1.409175157546997\n",
      "Step: 1089, Loss: 0.9159772992134094, Accuracy: 1.0, Computation time: 1.0427427291870117\n",
      "Step: 1090, Loss: 0.9159379601478577, Accuracy: 1.0, Computation time: 1.2865564823150635\n",
      "Step: 1091, Loss: 0.9364017248153687, Accuracy: 0.96875, Computation time: 1.378380298614502\n",
      "Step: 1092, Loss: 0.9159670472145081, Accuracy: 1.0, Computation time: 1.1003434658050537\n",
      "Step: 1093, Loss: 0.930095911026001, Accuracy: 0.9772727489471436, Computation time: 2.009223222732544\n",
      "Step: 1094, Loss: 0.9204767942428589, Accuracy: 1.0, Computation time: 0.9630606174468994\n",
      "Step: 1095, Loss: 0.9200716018676758, Accuracy: 1.0, Computation time: 1.5463242530822754\n",
      "Step: 1096, Loss: 0.9161708354949951, Accuracy: 1.0, Computation time: 1.2087976932525635\n",
      "Step: 1097, Loss: 0.9162856340408325, Accuracy: 1.0, Computation time: 1.180962085723877\n",
      "Step: 1098, Loss: 0.9163527488708496, Accuracy: 1.0, Computation time: 1.0209932327270508\n",
      "Step: 1099, Loss: 0.9373581409454346, Accuracy: 0.9750000238418579, Computation time: 1.1510539054870605\n",
      "Step: 1100, Loss: 0.9338791966438293, Accuracy: 0.9722222089767456, Computation time: 1.2339928150177002\n",
      "Step: 1101, Loss: 0.9256293773651123, Accuracy: 0.9821428656578064, Computation time: 1.3231451511383057\n",
      "Step: 1102, Loss: 0.9159549474716187, Accuracy: 1.0, Computation time: 1.1706938743591309\n",
      "Step: 1103, Loss: 0.9400844573974609, Accuracy: 0.949999988079071, Computation time: 1.2856128215789795\n",
      "Step: 1104, Loss: 0.9162706732749939, Accuracy: 1.0, Computation time: 1.2591965198516846\n",
      "Step: 1105, Loss: 0.9227789044380188, Accuracy: 1.0, Computation time: 1.603949785232544\n",
      "Step: 1106, Loss: 0.9480108022689819, Accuracy: 0.9791666865348816, Computation time: 2.70918345451355\n",
      "Step: 1107, Loss: 0.9578049182891846, Accuracy: 0.9392857551574707, Computation time: 1.5405473709106445\n",
      "Step: 1108, Loss: 0.9162828326225281, Accuracy: 1.0, Computation time: 1.0922293663024902\n",
      "Step: 1109, Loss: 0.9163687229156494, Accuracy: 1.0, Computation time: 1.1814570426940918\n",
      "Step: 1110, Loss: 0.9162665009498596, Accuracy: 1.0, Computation time: 1.2883224487304688\n",
      "Step: 1111, Loss: 0.9162042737007141, Accuracy: 1.0, Computation time: 1.3575398921966553\n",
      "Step: 1112, Loss: 0.916172444820404, Accuracy: 1.0, Computation time: 1.35569429397583\n",
      "########################\n",
      "Test loss: 1.126916527748108, Test Accuracy_epoch8: 0.6930617690086365\n",
      "########################\n",
      "Step: 1113, Loss: 0.9160807132720947, Accuracy: 1.0, Computation time: 1.246553659439087\n",
      "Step: 1114, Loss: 0.9161636233329773, Accuracy: 1.0, Computation time: 1.2298285961151123\n",
      "Step: 1115, Loss: 0.9164150953292847, Accuracy: 1.0, Computation time: 1.3902740478515625\n",
      "Step: 1116, Loss: 0.9160903096199036, Accuracy: 1.0, Computation time: 1.0330052375793457\n",
      "Step: 1117, Loss: 0.943809986114502, Accuracy: 0.9821428656578064, Computation time: 1.4525370597839355\n",
      "Step: 1118, Loss: 0.9164089560508728, Accuracy: 1.0, Computation time: 1.4360458850860596\n",
      "Step: 1119, Loss: 0.9161167144775391, Accuracy: 1.0, Computation time: 1.4203550815582275\n",
      "Step: 1120, Loss: 0.9162173271179199, Accuracy: 1.0, Computation time: 1.0345404148101807\n",
      "Step: 1121, Loss: 0.9162048697471619, Accuracy: 1.0, Computation time: 1.9063527584075928\n",
      "Step: 1122, Loss: 0.9320001006126404, Accuracy: 0.9772727489471436, Computation time: 1.3369815349578857\n",
      "Step: 1123, Loss: 0.9160432815551758, Accuracy: 1.0, Computation time: 1.0540742874145508\n",
      "Step: 1124, Loss: 0.9774169921875, Accuracy: 0.9217171669006348, Computation time: 1.350539207458496\n",
      "Step: 1125, Loss: 0.9160155653953552, Accuracy: 1.0, Computation time: 1.625962734222412\n",
      "Step: 1126, Loss: 0.9341993927955627, Accuracy: 0.9791666865348816, Computation time: 1.1960663795471191\n",
      "Step: 1127, Loss: 0.9278804659843445, Accuracy: 0.96875, Computation time: 1.5309727191925049\n",
      "Step: 1128, Loss: 0.9503209590911865, Accuracy: 0.9437500238418579, Computation time: 1.606384515762329\n",
      "Step: 1129, Loss: 0.9189431667327881, Accuracy: 1.0, Computation time: 1.6513118743896484\n",
      "Step: 1130, Loss: 0.9159610271453857, Accuracy: 1.0, Computation time: 1.0951428413391113\n",
      "Step: 1131, Loss: 0.9218943119049072, Accuracy: 1.0, Computation time: 1.2963833808898926\n",
      "Step: 1132, Loss: 0.9163885116577148, Accuracy: 1.0, Computation time: 1.2682127952575684\n",
      "Step: 1133, Loss: 0.9159485697746277, Accuracy: 1.0, Computation time: 0.992927074432373\n",
      "Step: 1134, Loss: 0.9159759879112244, Accuracy: 1.0, Computation time: 1.2522010803222656\n",
      "Step: 1135, Loss: 0.9166274666786194, Accuracy: 1.0, Computation time: 1.028580904006958\n",
      "Step: 1136, Loss: 0.9159261584281921, Accuracy: 1.0, Computation time: 1.2369089126586914\n",
      "Step: 1137, Loss: 0.9375364184379578, Accuracy: 0.9791666865348816, Computation time: 1.034754753112793\n",
      "Step: 1138, Loss: 0.9359737634658813, Accuracy: 0.9722222089767456, Computation time: 1.0653181076049805\n",
      "Step: 1139, Loss: 0.9371218681335449, Accuracy: 0.9821428656578064, Computation time: 1.097045660018921\n",
      "Step: 1140, Loss: 0.915946900844574, Accuracy: 1.0, Computation time: 1.146066665649414\n",
      "Step: 1141, Loss: 0.9159412980079651, Accuracy: 1.0, Computation time: 1.111327886581421\n",
      "Step: 1142, Loss: 0.9162479043006897, Accuracy: 1.0, Computation time: 1.620558738708496\n",
      "Step: 1143, Loss: 0.9161287546157837, Accuracy: 1.0, Computation time: 1.00752854347229\n",
      "Step: 1144, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 1.0404679775238037\n",
      "Step: 1145, Loss: 0.9164804816246033, Accuracy: 1.0, Computation time: 1.0798282623291016\n",
      "Step: 1146, Loss: 0.9374927282333374, Accuracy: 0.9833333492279053, Computation time: 1.1472909450531006\n",
      "Step: 1147, Loss: 0.9159798622131348, Accuracy: 1.0, Computation time: 0.8000340461730957\n",
      "Step: 1148, Loss: 0.9159662127494812, Accuracy: 1.0, Computation time: 1.195878267288208\n",
      "Step: 1149, Loss: 0.9374315738677979, Accuracy: 0.984375, Computation time: 0.9484298229217529\n",
      "Step: 1150, Loss: 0.916664719581604, Accuracy: 1.0, Computation time: 1.1583375930786133\n",
      "Step: 1151, Loss: 0.9386700391769409, Accuracy: 0.9791666865348816, Computation time: 1.2762160301208496\n",
      "Step: 1152, Loss: 0.9370984435081482, Accuracy: 0.9750000238418579, Computation time: 1.1698660850524902\n",
      "Step: 1153, Loss: 0.9333311915397644, Accuracy: 0.9722222089767456, Computation time: 0.9352545738220215\n",
      "Step: 1154, Loss: 0.9336762428283691, Accuracy: 0.9791666865348816, Computation time: 1.514331340789795\n",
      "Step: 1155, Loss: 0.9159976243972778, Accuracy: 1.0, Computation time: 0.9906613826751709\n",
      "Step: 1156, Loss: 0.9192339777946472, Accuracy: 1.0, Computation time: 1.3375906944274902\n",
      "Step: 1157, Loss: 0.9160544276237488, Accuracy: 1.0, Computation time: 0.8651254177093506\n",
      "Step: 1158, Loss: 0.9160494804382324, Accuracy: 1.0, Computation time: 0.9231958389282227\n",
      "Step: 1159, Loss: 0.9159751534461975, Accuracy: 1.0, Computation time: 1.3520641326904297\n",
      "Step: 1160, Loss: 0.9199203252792358, Accuracy: 1.0, Computation time: 1.120764970779419\n",
      "Step: 1161, Loss: 0.9343219995498657, Accuracy: 0.9772727489471436, Computation time: 1.2374293804168701\n",
      "Step: 1162, Loss: 0.9297243356704712, Accuracy: 0.9791666865348816, Computation time: 1.0943901538848877\n",
      "Step: 1163, Loss: 0.9160863161087036, Accuracy: 1.0, Computation time: 0.9394063949584961\n",
      "Step: 1164, Loss: 0.9160395264625549, Accuracy: 1.0, Computation time: 1.1590826511383057\n",
      "Step: 1165, Loss: 0.9161109924316406, Accuracy: 1.0, Computation time: 0.913428544998169\n",
      "Step: 1166, Loss: 0.9159656167030334, Accuracy: 1.0, Computation time: 1.1162526607513428\n",
      "Step: 1167, Loss: 0.9164021611213684, Accuracy: 1.0, Computation time: 1.0101933479309082\n",
      "Step: 1168, Loss: 0.9330115914344788, Accuracy: 0.9772727489471436, Computation time: 1.2938742637634277\n",
      "Step: 1169, Loss: 0.9174377918243408, Accuracy: 1.0, Computation time: 0.9648706912994385\n",
      "Step: 1170, Loss: 0.9171048402786255, Accuracy: 1.0, Computation time: 1.3144452571868896\n",
      "Step: 1171, Loss: 0.9177865386009216, Accuracy: 1.0, Computation time: 1.0158307552337646\n",
      "Step: 1172, Loss: 0.9160619974136353, Accuracy: 1.0, Computation time: 0.8485095500946045\n",
      "Step: 1173, Loss: 0.9160494804382324, Accuracy: 1.0, Computation time: 1.1402699947357178\n",
      "Step: 1174, Loss: 0.9160102605819702, Accuracy: 1.0, Computation time: 0.9664690494537354\n",
      "Step: 1175, Loss: 0.9159697890281677, Accuracy: 1.0, Computation time: 0.8778314590454102\n",
      "Step: 1176, Loss: 0.9160456657409668, Accuracy: 1.0, Computation time: 1.3675718307495117\n",
      "Step: 1177, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 0.9687049388885498\n",
      "Step: 1178, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 0.9157660007476807\n",
      "Step: 1179, Loss: 0.9159774780273438, Accuracy: 1.0, Computation time: 0.8764848709106445\n",
      "Step: 1180, Loss: 0.9159674644470215, Accuracy: 1.0, Computation time: 0.8647201061248779\n",
      "Step: 1181, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 1.3655378818511963\n",
      "Step: 1182, Loss: 0.9161704778671265, Accuracy: 1.0, Computation time: 0.8649604320526123\n",
      "Step: 1183, Loss: 0.915936291217804, Accuracy: 1.0, Computation time: 0.8980863094329834\n",
      "Step: 1184, Loss: 0.9159607291221619, Accuracy: 1.0, Computation time: 0.8778266906738281\n",
      "Step: 1185, Loss: 0.9180428385734558, Accuracy: 1.0, Computation time: 1.3694040775299072\n",
      "Step: 1186, Loss: 0.9168837070465088, Accuracy: 1.0, Computation time: 1.1583795547485352\n",
      "Step: 1187, Loss: 0.9376575946807861, Accuracy: 0.96875, Computation time: 1.1290719509124756\n",
      "Step: 1188, Loss: 0.9160833358764648, Accuracy: 1.0, Computation time: 0.8925585746765137\n",
      "Step: 1189, Loss: 0.9170823097229004, Accuracy: 1.0, Computation time: 1.028353214263916\n",
      "Step: 1190, Loss: 0.9164367318153381, Accuracy: 1.0, Computation time: 0.8097608089447021\n",
      "Step: 1191, Loss: 0.9328463077545166, Accuracy: 0.9772727489471436, Computation time: 1.0453600883483887\n",
      "Step: 1192, Loss: 0.9160798788070679, Accuracy: 1.0, Computation time: 0.9007689952850342\n",
      "Step: 1193, Loss: 0.9170486927032471, Accuracy: 1.0, Computation time: 1.4335482120513916\n",
      "Step: 1194, Loss: 0.9460898637771606, Accuracy: 0.984375, Computation time: 1.839223861694336\n",
      "Step: 1195, Loss: 0.9159282445907593, Accuracy: 1.0, Computation time: 0.7334389686584473\n",
      "Step: 1196, Loss: 0.9159658551216125, Accuracy: 1.0, Computation time: 0.8966109752655029\n",
      "Step: 1197, Loss: 0.9163419604301453, Accuracy: 1.0, Computation time: 1.0957446098327637\n",
      "Step: 1198, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 1.407470941543579\n",
      "Step: 1199, Loss: 0.9160059094429016, Accuracy: 1.0, Computation time: 0.8677139282226562\n",
      "Step: 1200, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 0.9540648460388184\n",
      "Step: 1201, Loss: 0.9362184405326843, Accuracy: 0.949999988079071, Computation time: 1.085803747177124\n",
      "Step: 1202, Loss: 0.9160217642784119, Accuracy: 1.0, Computation time: 1.0140104293823242\n",
      "Step: 1203, Loss: 0.9159799814224243, Accuracy: 1.0, Computation time: 0.835503101348877\n",
      "Step: 1204, Loss: 0.9162113070487976, Accuracy: 1.0, Computation time: 0.8610141277313232\n",
      "Step: 1205, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.1709482669830322\n",
      "Step: 1206, Loss: 0.9160199165344238, Accuracy: 1.0, Computation time: 0.9999716281890869\n",
      "Step: 1207, Loss: 0.9378992319107056, Accuracy: 0.9750000238418579, Computation time: 1.1569433212280273\n",
      "Step: 1208, Loss: 0.9397667646408081, Accuracy: 0.984375, Computation time: 1.1905591487884521\n",
      "Step: 1209, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.0594327449798584\n",
      "Step: 1210, Loss: 0.9159865379333496, Accuracy: 1.0, Computation time: 1.0325000286102295\n",
      "Step: 1211, Loss: 0.931647777557373, Accuracy: 0.9821428656578064, Computation time: 1.3264336585998535\n",
      "Step: 1212, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 0.869621753692627\n",
      "Step: 1213, Loss: 0.9159734845161438, Accuracy: 1.0, Computation time: 1.0448977947235107\n",
      "Step: 1214, Loss: 0.9159746170043945, Accuracy: 1.0, Computation time: 0.9798095226287842\n",
      "Step: 1215, Loss: 0.9572321176528931, Accuracy: 0.9583333730697632, Computation time: 1.1398048400878906\n",
      "Step: 1216, Loss: 0.9162882566452026, Accuracy: 1.0, Computation time: 1.29160475730896\n",
      "Step: 1217, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 0.8552842140197754\n",
      "Step: 1218, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 1.168198585510254\n",
      "Step: 1219, Loss: 0.9166347980499268, Accuracy: 1.0, Computation time: 1.1295084953308105\n",
      "Step: 1220, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 1.1267774105072021\n",
      "Step: 1221, Loss: 0.9176591634750366, Accuracy: 1.0, Computation time: 0.8670411109924316\n",
      "Step: 1222, Loss: 0.9159940481185913, Accuracy: 1.0, Computation time: 0.8602654933929443\n",
      "Step: 1223, Loss: 0.9162535071372986, Accuracy: 1.0, Computation time: 1.1120951175689697\n",
      "Step: 1224, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 0.8357219696044922\n",
      "Step: 1225, Loss: 0.9167018532752991, Accuracy: 1.0, Computation time: 1.5629987716674805\n",
      "Step: 1226, Loss: 0.9160608649253845, Accuracy: 1.0, Computation time: 0.8524787425994873\n",
      "Step: 1227, Loss: 0.9159016013145447, Accuracy: 1.0, Computation time: 1.136242151260376\n",
      "Step: 1228, Loss: 0.9252404570579529, Accuracy: 1.0, Computation time: 1.0410168170928955\n",
      "Step: 1229, Loss: 0.9370966553688049, Accuracy: 0.96875, Computation time: 1.3551571369171143\n",
      "Step: 1230, Loss: 0.9160087704658508, Accuracy: 1.0, Computation time: 0.9414753913879395\n",
      "Step: 1231, Loss: 0.9161909818649292, Accuracy: 1.0, Computation time: 0.9069392681121826\n",
      "Step: 1232, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.2146391868591309\n",
      "Step: 1233, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 0.8826637268066406\n",
      "Step: 1234, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 0.8928587436676025\n",
      "Step: 1235, Loss: 0.9371207356452942, Accuracy: 0.96875, Computation time: 1.3430771827697754\n",
      "Step: 1236, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 0.8794798851013184\n",
      "Step: 1237, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 1.230973482131958\n",
      "Step: 1238, Loss: 0.9270172715187073, Accuracy: 0.9583333730697632, Computation time: 1.4155142307281494\n",
      "Step: 1239, Loss: 0.9160254597663879, Accuracy: 1.0, Computation time: 1.0014619827270508\n",
      "Step: 1240, Loss: 0.9158871173858643, Accuracy: 1.0, Computation time: 1.0562100410461426\n",
      "Step: 1241, Loss: 0.9159615635871887, Accuracy: 1.0, Computation time: 0.8292381763458252\n",
      "Step: 1242, Loss: 0.9159116744995117, Accuracy: 1.0, Computation time: 1.0370123386383057\n",
      "Step: 1243, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 0.9500677585601807\n",
      "Step: 1244, Loss: 0.9217735528945923, Accuracy: 1.0, Computation time: 1.2090556621551514\n",
      "Step: 1245, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 1.04844069480896\n",
      "Step: 1246, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 0.7851264476776123\n",
      "Step: 1247, Loss: 0.9159506559371948, Accuracy: 1.0, Computation time: 1.1710710525512695\n",
      "Step: 1248, Loss: 0.9159319400787354, Accuracy: 1.0, Computation time: 1.0781097412109375\n",
      "Step: 1249, Loss: 0.9186513423919678, Accuracy: 1.0, Computation time: 1.2679901123046875\n",
      "Step: 1250, Loss: 0.9161650538444519, Accuracy: 1.0, Computation time: 0.9604804515838623\n",
      "Step: 1251, Loss: 0.9159756302833557, Accuracy: 1.0, Computation time: 0.9398632049560547\n",
      "########################\n",
      "Test loss: 1.1256887912750244, Test Accuracy_epoch9: 0.681606113910675\n",
      "########################\n",
      "Step: 1252, Loss: 0.9160649180412292, Accuracy: 1.0, Computation time: 0.9088051319122314\n",
      "Step: 1253, Loss: 0.916024386882782, Accuracy: 1.0, Computation time: 0.9626593589782715\n",
      "Step: 1254, Loss: 0.9159715175628662, Accuracy: 1.0, Computation time: 0.8809866905212402\n",
      "Step: 1255, Loss: 0.9168907403945923, Accuracy: 1.0, Computation time: 1.0416467189788818\n",
      "Step: 1256, Loss: 0.9160058498382568, Accuracy: 1.0, Computation time: 1.1298868656158447\n",
      "Step: 1257, Loss: 0.9268847703933716, Accuracy: 0.9821428656578064, Computation time: 1.3085362911224365\n",
      "Step: 1258, Loss: 0.9160112142562866, Accuracy: 1.0, Computation time: 1.1685338020324707\n",
      "Step: 1259, Loss: 0.9159579873085022, Accuracy: 1.0, Computation time: 1.0041577816009521\n",
      "Step: 1260, Loss: 0.9158989787101746, Accuracy: 1.0, Computation time: 1.3128035068511963\n",
      "Step: 1261, Loss: 0.9376553893089294, Accuracy: 0.9583333730697632, Computation time: 1.0465006828308105\n",
      "Step: 1262, Loss: 0.9167485237121582, Accuracy: 1.0, Computation time: 1.3539814949035645\n",
      "Step: 1263, Loss: 0.9170942902565002, Accuracy: 1.0, Computation time: 1.5008211135864258\n",
      "Step: 1264, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 0.8990075588226318\n",
      "Step: 1265, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 0.876924991607666\n",
      "Step: 1266, Loss: 0.9223700761795044, Accuracy: 1.0, Computation time: 1.3867390155792236\n",
      "Step: 1267, Loss: 0.9393875598907471, Accuracy: 0.96875, Computation time: 1.2244222164154053\n",
      "Step: 1268, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 0.9383189678192139\n",
      "Step: 1269, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.0064544677734375\n",
      "Step: 1270, Loss: 0.9383769631385803, Accuracy: 0.9791666865348816, Computation time: 1.2436020374298096\n",
      "Step: 1271, Loss: 0.9375143647193909, Accuracy: 0.96875, Computation time: 1.3040626049041748\n",
      "Step: 1272, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 0.8180351257324219\n",
      "Step: 1273, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 0.8648574352264404\n",
      "Step: 1274, Loss: 0.9382959008216858, Accuracy: 0.9750000238418579, Computation time: 1.08219313621521\n",
      "Step: 1275, Loss: 0.9363665580749512, Accuracy: 0.9807692766189575, Computation time: 1.1400296688079834\n",
      "Step: 1276, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.1529324054718018\n",
      "Step: 1277, Loss: 0.9165456295013428, Accuracy: 1.0, Computation time: 1.0835208892822266\n",
      "Step: 1278, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.1602225303649902\n",
      "Step: 1279, Loss: 0.9159480333328247, Accuracy: 1.0, Computation time: 0.9181346893310547\n",
      "Step: 1280, Loss: 0.9159703850746155, Accuracy: 1.0, Computation time: 0.910822868347168\n",
      "Step: 1281, Loss: 0.9160500168800354, Accuracy: 1.0, Computation time: 1.156672716140747\n",
      "Step: 1282, Loss: 0.9159339666366577, Accuracy: 1.0, Computation time: 0.8610274791717529\n",
      "Step: 1283, Loss: 0.9363322257995605, Accuracy: 0.9807692766189575, Computation time: 1.4420335292816162\n",
      "Step: 1284, Loss: 0.9346156716346741, Accuracy: 0.9791666865348816, Computation time: 1.434485673904419\n",
      "Step: 1285, Loss: 0.9159467220306396, Accuracy: 1.0, Computation time: 1.498812198638916\n",
      "Step: 1286, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.2730097770690918\n",
      "Step: 1287, Loss: 0.9375718832015991, Accuracy: 0.9807692766189575, Computation time: 1.189840316772461\n",
      "Step: 1288, Loss: 0.9160163402557373, Accuracy: 1.0, Computation time: 0.9938933849334717\n",
      "Step: 1289, Loss: 0.9369215965270996, Accuracy: 0.9772727489471436, Computation time: 1.2925562858581543\n",
      "Step: 1290, Loss: 0.9198995232582092, Accuracy: 1.0, Computation time: 1.8181509971618652\n",
      "Step: 1291, Loss: 0.9159250259399414, Accuracy: 1.0, Computation time: 1.1671547889709473\n",
      "Step: 1292, Loss: 0.9169050455093384, Accuracy: 1.0, Computation time: 1.169053316116333\n",
      "Step: 1293, Loss: 0.9375206232070923, Accuracy: 0.9772727489471436, Computation time: 1.2690470218658447\n",
      "Step: 1294, Loss: 0.9229097366333008, Accuracy: 1.0, Computation time: 1.0486083030700684\n",
      "Step: 1295, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 1.4866466522216797\n",
      "Step: 1296, Loss: 0.9381105303764343, Accuracy: 0.9772727489471436, Computation time: 0.9704875946044922\n",
      "Step: 1297, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 1.1597483158111572\n",
      "Step: 1298, Loss: 0.9159470796585083, Accuracy: 1.0, Computation time: 1.2597849369049072\n",
      "Step: 1299, Loss: 0.919670820236206, Accuracy: 1.0, Computation time: 1.262310266494751\n",
      "Step: 1300, Loss: 0.9162642955780029, Accuracy: 1.0, Computation time: 1.2911443710327148\n",
      "Step: 1301, Loss: 0.9160938262939453, Accuracy: 1.0, Computation time: 1.7091939449310303\n",
      "Step: 1302, Loss: 0.9159929156303406, Accuracy: 1.0, Computation time: 0.8501927852630615\n",
      "Step: 1303, Loss: 0.9480859637260437, Accuracy: 0.9464285969734192, Computation time: 1.3655900955200195\n",
      "Step: 1304, Loss: 0.9159537553787231, Accuracy: 1.0, Computation time: 1.3151500225067139\n",
      "Step: 1305, Loss: 0.9159777164459229, Accuracy: 1.0, Computation time: 1.3403429985046387\n",
      "Step: 1306, Loss: 0.9176293611526489, Accuracy: 1.0, Computation time: 1.3741743564605713\n",
      "Step: 1307, Loss: 0.9161096215248108, Accuracy: 1.0, Computation time: 1.453124761581421\n",
      "Step: 1308, Loss: 0.915996253490448, Accuracy: 1.0, Computation time: 1.1503019332885742\n",
      "Step: 1309, Loss: 0.9374297261238098, Accuracy: 0.9722222089767456, Computation time: 1.4771301746368408\n",
      "Step: 1310, Loss: 0.9159592986106873, Accuracy: 1.0, Computation time: 1.2694981098175049\n",
      "Step: 1311, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 1.0219552516937256\n",
      "Step: 1312, Loss: 0.9362746477127075, Accuracy: 0.9772727489471436, Computation time: 1.4645617008209229\n",
      "Step: 1313, Loss: 0.9160113334655762, Accuracy: 1.0, Computation time: 1.1107966899871826\n",
      "Step: 1314, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 1.368180274963379\n",
      "Step: 1315, Loss: 0.9159750938415527, Accuracy: 1.0, Computation time: 1.3643136024475098\n",
      "Step: 1316, Loss: 0.9159822463989258, Accuracy: 1.0, Computation time: 1.352501392364502\n",
      "Step: 1317, Loss: 0.9160071611404419, Accuracy: 1.0, Computation time: 1.0127251148223877\n",
      "Step: 1318, Loss: 0.9171001315116882, Accuracy: 1.0, Computation time: 1.1530284881591797\n",
      "Step: 1319, Loss: 0.9379151463508606, Accuracy: 0.9750000238418579, Computation time: 1.253333568572998\n",
      "Step: 1320, Loss: 0.9159606695175171, Accuracy: 1.0, Computation time: 1.1049845218658447\n",
      "Step: 1321, Loss: 0.9160250425338745, Accuracy: 1.0, Computation time: 1.2681288719177246\n",
      "Step: 1322, Loss: 0.9160471558570862, Accuracy: 1.0, Computation time: 1.1475954055786133\n",
      "Step: 1323, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.480642557144165\n",
      "Step: 1324, Loss: 0.9651191830635071, Accuracy: 0.9198718070983887, Computation time: 1.4812803268432617\n",
      "Step: 1325, Loss: 0.9160144925117493, Accuracy: 1.0, Computation time: 1.3316717147827148\n",
      "Step: 1326, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 1.0639944076538086\n",
      "Step: 1327, Loss: 0.9586787819862366, Accuracy: 0.925000011920929, Computation time: 1.654559850692749\n",
      "Step: 1328, Loss: 0.9217298030853271, Accuracy: 1.0, Computation time: 0.9989235401153564\n",
      "Step: 1329, Loss: 0.9452226161956787, Accuracy: 0.96875, Computation time: 2.408839464187622\n",
      "Step: 1330, Loss: 0.9246290922164917, Accuracy: 1.0, Computation time: 1.119945764541626\n",
      "Step: 1331, Loss: 0.9160133600234985, Accuracy: 1.0, Computation time: 0.9683418273925781\n",
      "Step: 1332, Loss: 0.9160934090614319, Accuracy: 1.0, Computation time: 1.0505878925323486\n",
      "Step: 1333, Loss: 0.916363537311554, Accuracy: 1.0, Computation time: 0.8494086265563965\n",
      "Step: 1334, Loss: 0.9160494208335876, Accuracy: 1.0, Computation time: 0.9657745361328125\n",
      "Step: 1335, Loss: 0.9161854386329651, Accuracy: 1.0, Computation time: 1.2074613571166992\n",
      "Step: 1336, Loss: 0.9160563349723816, Accuracy: 1.0, Computation time: 1.0210886001586914\n",
      "Step: 1337, Loss: 0.93587327003479, Accuracy: 0.9791666865348816, Computation time: 0.9775810241699219\n",
      "Step: 1338, Loss: 0.9310297966003418, Accuracy: 0.9583333730697632, Computation time: 1.1556413173675537\n",
      "Step: 1339, Loss: 0.9170905351638794, Accuracy: 1.0, Computation time: 0.8535566329956055\n",
      "Step: 1340, Loss: 0.9207457900047302, Accuracy: 1.0, Computation time: 0.9036364555358887\n",
      "Step: 1341, Loss: 0.932574450969696, Accuracy: 0.9772727489471436, Computation time: 0.9497487545013428\n",
      "Step: 1342, Loss: 0.9233017563819885, Accuracy: 1.0, Computation time: 1.439117431640625\n",
      "Step: 1343, Loss: 0.937437117099762, Accuracy: 0.9772727489471436, Computation time: 1.0743465423583984\n",
      "Step: 1344, Loss: 0.916356086730957, Accuracy: 1.0, Computation time: 1.1536669731140137\n",
      "Step: 1345, Loss: 0.9164334535598755, Accuracy: 1.0, Computation time: 1.4074723720550537\n",
      "Step: 1346, Loss: 0.9161631464958191, Accuracy: 1.0, Computation time: 1.3031854629516602\n",
      "Step: 1347, Loss: 0.9161940217018127, Accuracy: 1.0, Computation time: 0.8712334632873535\n",
      "Step: 1348, Loss: 0.9374408721923828, Accuracy: 0.949999988079071, Computation time: 1.9343986511230469\n",
      "Step: 1349, Loss: 0.9376102089881897, Accuracy: 0.9642857313156128, Computation time: 1.029210090637207\n",
      "Step: 1350, Loss: 0.9160621166229248, Accuracy: 1.0, Computation time: 1.168107509613037\n",
      "Step: 1351, Loss: 0.9160991311073303, Accuracy: 1.0, Computation time: 1.108238697052002\n",
      "Step: 1352, Loss: 0.9160286784172058, Accuracy: 1.0, Computation time: 0.9433624744415283\n",
      "Step: 1353, Loss: 0.9283910393714905, Accuracy: 0.9722222089767456, Computation time: 1.2971153259277344\n",
      "Step: 1354, Loss: 0.937811017036438, Accuracy: 0.9772727489471436, Computation time: 1.048954963684082\n",
      "Step: 1355, Loss: 0.915999174118042, Accuracy: 1.0, Computation time: 1.0680294036865234\n",
      "Step: 1356, Loss: 0.9162689447402954, Accuracy: 1.0, Computation time: 1.068328619003296\n",
      "Step: 1357, Loss: 0.9159576892852783, Accuracy: 1.0, Computation time: 1.172001600265503\n",
      "Step: 1358, Loss: 0.9171105623245239, Accuracy: 1.0, Computation time: 1.2705698013305664\n",
      "Step: 1359, Loss: 0.9184002876281738, Accuracy: 1.0, Computation time: 1.393812894821167\n",
      "Step: 1360, Loss: 0.9160928130149841, Accuracy: 1.0, Computation time: 1.003955602645874\n",
      "Step: 1361, Loss: 0.9590036869049072, Accuracy: 0.9666666984558105, Computation time: 1.2679331302642822\n",
      "Step: 1362, Loss: 0.9323930740356445, Accuracy: 0.9807692766189575, Computation time: 2.0457773208618164\n",
      "Step: 1363, Loss: 0.9248656630516052, Accuracy: 1.0, Computation time: 1.5295159816741943\n",
      "Step: 1364, Loss: 0.9165992736816406, Accuracy: 1.0, Computation time: 1.0558373928070068\n",
      "Step: 1365, Loss: 0.9169014096260071, Accuracy: 1.0, Computation time: 0.9680204391479492\n",
      "Step: 1366, Loss: 0.9167455434799194, Accuracy: 1.0, Computation time: 1.094024658203125\n",
      "Step: 1367, Loss: 0.917120099067688, Accuracy: 1.0, Computation time: 1.0819036960601807\n",
      "Step: 1368, Loss: 0.9375597238540649, Accuracy: 0.9750000238418579, Computation time: 1.230656623840332\n",
      "Step: 1369, Loss: 0.9196770191192627, Accuracy: 1.0, Computation time: 1.035409688949585\n",
      "Step: 1370, Loss: 0.9370625019073486, Accuracy: 0.9807692766189575, Computation time: 1.320023536682129\n",
      "Step: 1371, Loss: 0.9161440134048462, Accuracy: 1.0, Computation time: 1.080585241317749\n",
      "Step: 1372, Loss: 0.916090190410614, Accuracy: 1.0, Computation time: 1.0830605030059814\n",
      "Step: 1373, Loss: 0.9161247611045837, Accuracy: 1.0, Computation time: 1.230231523513794\n",
      "Step: 1374, Loss: 0.9161680936813354, Accuracy: 1.0, Computation time: 1.222008466720581\n",
      "Step: 1375, Loss: 0.9234554171562195, Accuracy: 1.0, Computation time: 1.1068623065948486\n",
      "Step: 1376, Loss: 0.9164192080497742, Accuracy: 1.0, Computation time: 1.1609723567962646\n",
      "Step: 1377, Loss: 0.9161216020584106, Accuracy: 1.0, Computation time: 1.064713954925537\n",
      "Step: 1378, Loss: 0.9165071845054626, Accuracy: 1.0, Computation time: 0.9972348213195801\n",
      "Step: 1379, Loss: 0.9161468148231506, Accuracy: 1.0, Computation time: 0.9708914756774902\n",
      "Step: 1380, Loss: 0.9166465997695923, Accuracy: 1.0, Computation time: 0.9588797092437744\n",
      "Step: 1381, Loss: 0.9374248385429382, Accuracy: 0.9750000238418579, Computation time: 0.9978494644165039\n",
      "Step: 1382, Loss: 0.9184452295303345, Accuracy: 1.0, Computation time: 1.8966386318206787\n",
      "Step: 1383, Loss: 0.9161864519119263, Accuracy: 1.0, Computation time: 1.1156418323516846\n",
      "Step: 1384, Loss: 0.916468620300293, Accuracy: 1.0, Computation time: 0.9405574798583984\n",
      "Step: 1385, Loss: 0.9173493385314941, Accuracy: 1.0, Computation time: 1.1025993824005127\n",
      "Step: 1386, Loss: 0.9218602180480957, Accuracy: 1.0, Computation time: 0.9257359504699707\n",
      "Step: 1387, Loss: 0.9381715655326843, Accuracy: 0.9750000238418579, Computation time: 0.9560811519622803\n",
      "Step: 1388, Loss: 0.9166772961616516, Accuracy: 1.0, Computation time: 1.136246681213379\n",
      "Step: 1389, Loss: 0.9292716979980469, Accuracy: 0.9375, Computation time: 1.4875612258911133\n",
      "Step: 1390, Loss: 0.916772723197937, Accuracy: 1.0, Computation time: 1.1923480033874512\n",
      "########################\n",
      "Test loss: 1.1288343667984009, Test Accuracy_epoch10: 0.6732513308525085\n",
      "########################\n",
      "Step: 1391, Loss: 0.9190405607223511, Accuracy: 1.0, Computation time: 1.1585381031036377\n",
      "Step: 1392, Loss: 0.9214751720428467, Accuracy: 1.0, Computation time: 0.9898326396942139\n",
      "Step: 1393, Loss: 0.9376292824745178, Accuracy: 0.9583333730697632, Computation time: 0.8972361087799072\n",
      "Step: 1394, Loss: 0.9217346906661987, Accuracy: 1.0, Computation time: 1.3450992107391357\n",
      "Step: 1395, Loss: 0.9233749508857727, Accuracy: 1.0, Computation time: 1.0339643955230713\n",
      "Step: 1396, Loss: 0.9187133312225342, Accuracy: 1.0, Computation time: 1.0207858085632324\n",
      "Step: 1397, Loss: 0.9188852906227112, Accuracy: 1.0, Computation time: 1.4888205528259277\n",
      "Step: 1398, Loss: 0.9169795513153076, Accuracy: 1.0, Computation time: 0.9220762252807617\n",
      "Step: 1399, Loss: 0.9385915398597717, Accuracy: 0.9583333730697632, Computation time: 1.0419492721557617\n",
      "Step: 1400, Loss: 0.9348357319831848, Accuracy: 0.949999988079071, Computation time: 1.0650100708007812\n",
      "Step: 1401, Loss: 0.91722172498703, Accuracy: 1.0, Computation time: 1.4269342422485352\n",
      "Step: 1402, Loss: 0.9411817789077759, Accuracy: 0.9722222089767456, Computation time: 1.3039393424987793\n",
      "Step: 1403, Loss: 0.9163079261779785, Accuracy: 1.0, Computation time: 0.9857003688812256\n",
      "Step: 1404, Loss: 0.9191160202026367, Accuracy: 1.0, Computation time: 1.3027656078338623\n",
      "Step: 1405, Loss: 0.9178816676139832, Accuracy: 1.0, Computation time: 0.903334379196167\n",
      "Step: 1406, Loss: 0.9369534254074097, Accuracy: 0.9807692766189575, Computation time: 1.3303072452545166\n",
      "Step: 1407, Loss: 0.9162358641624451, Accuracy: 1.0, Computation time: 0.9951984882354736\n",
      "Step: 1408, Loss: 0.9174621105194092, Accuracy: 1.0, Computation time: 0.995917797088623\n",
      "Step: 1409, Loss: 0.9160964488983154, Accuracy: 1.0, Computation time: 0.91068434715271\n",
      "Step: 1410, Loss: 0.9376977682113647, Accuracy: 0.96875, Computation time: 1.0707809925079346\n",
      "Step: 1411, Loss: 0.9347332715988159, Accuracy: 0.9583333730697632, Computation time: 1.5654387474060059\n",
      "Step: 1412, Loss: 0.9393298625946045, Accuracy: 0.9642857313156128, Computation time: 1.4908173084259033\n",
      "Step: 1413, Loss: 0.9351389408111572, Accuracy: 0.96875, Computation time: 0.8634355068206787\n",
      "Step: 1414, Loss: 0.9219791293144226, Accuracy: 1.0, Computation time: 1.2158818244934082\n",
      "Step: 1415, Loss: 0.9167553186416626, Accuracy: 1.0, Computation time: 1.2768306732177734\n",
      "Step: 1416, Loss: 0.9162257313728333, Accuracy: 1.0, Computation time: 0.9492316246032715\n",
      "Step: 1417, Loss: 0.9173574447631836, Accuracy: 1.0, Computation time: 1.057701587677002\n",
      "Step: 1418, Loss: 0.9220577478408813, Accuracy: 1.0, Computation time: 1.2783393859863281\n",
      "Step: 1419, Loss: 0.9161199927330017, Accuracy: 1.0, Computation time: 1.0456624031066895\n",
      "Step: 1420, Loss: 0.9166344404220581, Accuracy: 1.0, Computation time: 1.1022462844848633\n",
      "Step: 1421, Loss: 0.9160203337669373, Accuracy: 1.0, Computation time: 1.2158029079437256\n",
      "Step: 1422, Loss: 0.9184935092926025, Accuracy: 1.0, Computation time: 1.2213706970214844\n",
      "Step: 1423, Loss: 0.9163311123847961, Accuracy: 1.0, Computation time: 0.8575901985168457\n",
      "Step: 1424, Loss: 0.938035249710083, Accuracy: 0.9833333492279053, Computation time: 1.013451337814331\n",
      "Step: 1425, Loss: 0.9162002205848694, Accuracy: 1.0, Computation time: 0.9907171726226807\n",
      "Step: 1426, Loss: 0.9162249565124512, Accuracy: 1.0, Computation time: 1.1168460845947266\n",
      "Step: 1427, Loss: 0.9386125802993774, Accuracy: 0.9750000238418579, Computation time: 0.8090877532958984\n",
      "Step: 1428, Loss: 0.9296583533287048, Accuracy: 0.9772727489471436, Computation time: 1.564906358718872\n",
      "Step: 1429, Loss: 0.916049599647522, Accuracy: 1.0, Computation time: 1.2792720794677734\n",
      "Step: 1430, Loss: 0.9172562956809998, Accuracy: 1.0, Computation time: 0.9409739971160889\n",
      "Step: 1431, Loss: 0.9369975924491882, Accuracy: 0.9583333730697632, Computation time: 1.895925760269165\n",
      "Step: 1432, Loss: 0.9180262684822083, Accuracy: 1.0, Computation time: 1.3532631397247314\n",
      "Step: 1433, Loss: 0.9161396026611328, Accuracy: 1.0, Computation time: 1.1117660999298096\n",
      "Step: 1434, Loss: 0.9191304445266724, Accuracy: 1.0, Computation time: 0.9680840969085693\n",
      "Step: 1435, Loss: 0.9160047769546509, Accuracy: 1.0, Computation time: 0.9460194110870361\n",
      "Step: 1436, Loss: 0.9273759126663208, Accuracy: 0.9583333730697632, Computation time: 1.2569446563720703\n",
      "Step: 1437, Loss: 0.9373454451560974, Accuracy: 0.96875, Computation time: 1.0625009536743164\n",
      "Step: 1438, Loss: 0.9163801670074463, Accuracy: 1.0, Computation time: 1.1991217136383057\n",
      "Step: 1439, Loss: 0.916131317615509, Accuracy: 1.0, Computation time: 1.1616795063018799\n",
      "Step: 1440, Loss: 0.9160344004631042, Accuracy: 1.0, Computation time: 1.0126307010650635\n",
      "Step: 1441, Loss: 0.9160467982292175, Accuracy: 1.0, Computation time: 1.19096040725708\n",
      "Step: 1442, Loss: 0.9170467257499695, Accuracy: 1.0, Computation time: 0.9964427947998047\n",
      "Step: 1443, Loss: 0.9291613698005676, Accuracy: 0.9750000238418579, Computation time: 1.040670394897461\n",
      "Step: 1444, Loss: 0.9184301495552063, Accuracy: 1.0, Computation time: 1.0808959007263184\n",
      "Step: 1445, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 0.9341347217559814\n",
      "Step: 1446, Loss: 0.9163298010826111, Accuracy: 1.0, Computation time: 1.0170214176177979\n",
      "Step: 1447, Loss: 0.9160997867584229, Accuracy: 1.0, Computation time: 1.1954398155212402\n",
      "Step: 1448, Loss: 0.917055070400238, Accuracy: 1.0, Computation time: 0.9344003200531006\n",
      "Step: 1449, Loss: 0.9161916375160217, Accuracy: 1.0, Computation time: 1.1746351718902588\n",
      "Step: 1450, Loss: 0.9162993431091309, Accuracy: 1.0, Computation time: 1.1719765663146973\n",
      "Step: 1451, Loss: 0.9193646311759949, Accuracy: 1.0, Computation time: 0.9927859306335449\n",
      "Step: 1452, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 1.0401461124420166\n",
      "Step: 1453, Loss: 0.9180693626403809, Accuracy: 1.0, Computation time: 0.9964725971221924\n",
      "Step: 1454, Loss: 0.9159932732582092, Accuracy: 1.0, Computation time: 1.3738899230957031\n",
      "Step: 1455, Loss: 0.9159438610076904, Accuracy: 1.0, Computation time: 0.9754233360290527\n",
      "Step: 1456, Loss: 0.9161845445632935, Accuracy: 1.0, Computation time: 1.5016069412231445\n",
      "Step: 1457, Loss: 0.9163492321968079, Accuracy: 1.0, Computation time: 0.9342103004455566\n",
      "Step: 1458, Loss: 0.9246888160705566, Accuracy: 1.0, Computation time: 1.4170548915863037\n",
      "Step: 1459, Loss: 0.9159838557243347, Accuracy: 1.0, Computation time: 1.1612801551818848\n",
      "Step: 1460, Loss: 0.9167433977127075, Accuracy: 1.0, Computation time: 1.006779432296753\n",
      "Step: 1461, Loss: 0.9375513195991516, Accuracy: 0.9642857313156128, Computation time: 0.9910624027252197\n",
      "Step: 1462, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 1.007249355316162\n",
      "Step: 1463, Loss: 0.9159090518951416, Accuracy: 1.0, Computation time: 0.8852953910827637\n",
      "Step: 1464, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 0.9103162288665771\n",
      "Step: 1465, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 0.8635661602020264\n",
      "Step: 1466, Loss: 0.9159486889839172, Accuracy: 1.0, Computation time: 1.145691156387329\n",
      "Step: 1467, Loss: 0.9164468050003052, Accuracy: 1.0, Computation time: 0.8477761745452881\n",
      "Step: 1468, Loss: 0.9174779653549194, Accuracy: 1.0, Computation time: 1.4747214317321777\n",
      "Step: 1469, Loss: 0.9557761549949646, Accuracy: 0.9444444179534912, Computation time: 1.2858078479766846\n",
      "Step: 1470, Loss: 0.9159318208694458, Accuracy: 1.0, Computation time: 0.844473123550415\n",
      "Step: 1471, Loss: 0.9160523414611816, Accuracy: 1.0, Computation time: 0.8241415023803711\n",
      "Step: 1472, Loss: 0.9160702228546143, Accuracy: 1.0, Computation time: 0.9395015239715576\n",
      "Step: 1473, Loss: 0.9375532269477844, Accuracy: 0.9583333730697632, Computation time: 1.1185686588287354\n",
      "Step: 1474, Loss: 0.9160622954368591, Accuracy: 1.0, Computation time: 0.9124882221221924\n",
      "Step: 1475, Loss: 0.9159602522850037, Accuracy: 1.0, Computation time: 0.928931474685669\n",
      "Step: 1476, Loss: 0.9375760555267334, Accuracy: 0.96875, Computation time: 1.0200326442718506\n",
      "Step: 1477, Loss: 0.9167202115058899, Accuracy: 1.0, Computation time: 1.051581621170044\n",
      "Step: 1478, Loss: 0.9177817702293396, Accuracy: 1.0, Computation time: 0.9186384677886963\n",
      "Step: 1479, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 1.067763090133667\n",
      "Step: 1480, Loss: 0.9183070063591003, Accuracy: 1.0, Computation time: 1.0610871315002441\n",
      "Step: 1481, Loss: 0.9161304235458374, Accuracy: 1.0, Computation time: 0.9702019691467285\n",
      "Step: 1482, Loss: 0.9159340262413025, Accuracy: 1.0, Computation time: 1.3670682907104492\n",
      "Step: 1483, Loss: 0.9159446358680725, Accuracy: 1.0, Computation time: 0.8494858741760254\n",
      "Step: 1484, Loss: 0.9159534573554993, Accuracy: 1.0, Computation time: 0.8981552124023438\n",
      "Step: 1485, Loss: 0.9364116787910461, Accuracy: 0.9642857313156128, Computation time: 0.8366336822509766\n",
      "Step: 1486, Loss: 0.954127848148346, Accuracy: 0.9460227489471436, Computation time: 1.0039701461791992\n",
      "Step: 1487, Loss: 0.915932834148407, Accuracy: 1.0, Computation time: 0.9869105815887451\n",
      "Step: 1488, Loss: 0.9386000037193298, Accuracy: 0.949999988079071, Computation time: 0.937180757522583\n",
      "Step: 1489, Loss: 0.9161297082901001, Accuracy: 1.0, Computation time: 1.0768613815307617\n",
      "Step: 1490, Loss: 0.915989339351654, Accuracy: 1.0, Computation time: 1.618452548980713\n",
      "Step: 1491, Loss: 0.9381179809570312, Accuracy: 0.9642857313156128, Computation time: 1.2587015628814697\n",
      "Step: 1492, Loss: 0.9160312414169312, Accuracy: 1.0, Computation time: 1.0783843994140625\n",
      "Step: 1493, Loss: 0.9163569211959839, Accuracy: 1.0, Computation time: 0.9466965198516846\n",
      "Step: 1494, Loss: 0.9159893989562988, Accuracy: 1.0, Computation time: 0.8500804901123047\n",
      "Step: 1495, Loss: 0.917768657207489, Accuracy: 1.0, Computation time: 0.8795740604400635\n",
      "Step: 1496, Loss: 0.9159241914749146, Accuracy: 1.0, Computation time: 1.2962396144866943\n",
      "Step: 1497, Loss: 0.9168553948402405, Accuracy: 1.0, Computation time: 1.3260724544525146\n",
      "Step: 1498, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.0174858570098877\n",
      "Step: 1499, Loss: 0.9180331826210022, Accuracy: 1.0, Computation time: 1.1341989040374756\n",
      "Step: 1500, Loss: 0.9163631796836853, Accuracy: 1.0, Computation time: 0.9839482307434082\n",
      "Step: 1501, Loss: 0.9159546494483948, Accuracy: 1.0, Computation time: 1.1247789859771729\n",
      "Step: 1502, Loss: 0.9160436391830444, Accuracy: 1.0, Computation time: 0.881697416305542\n",
      "Step: 1503, Loss: 0.9160775542259216, Accuracy: 1.0, Computation time: 1.2560324668884277\n",
      "Step: 1504, Loss: 0.9159814119338989, Accuracy: 1.0, Computation time: 0.788710355758667\n",
      "Step: 1505, Loss: 0.9161899089813232, Accuracy: 1.0, Computation time: 1.1294152736663818\n",
      "Step: 1506, Loss: 0.9162327647209167, Accuracy: 1.0, Computation time: 1.4789073467254639\n",
      "Step: 1507, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 0.8312246799468994\n",
      "Step: 1508, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 0.867112398147583\n",
      "Step: 1509, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 1.2519943714141846\n",
      "Step: 1510, Loss: 0.9159072637557983, Accuracy: 1.0, Computation time: 0.9799740314483643\n",
      "Step: 1511, Loss: 0.9159612655639648, Accuracy: 1.0, Computation time: 1.5003788471221924\n",
      "Step: 1512, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 1.0720608234405518\n",
      "Step: 1513, Loss: 0.9377639293670654, Accuracy: 0.9750000238418579, Computation time: 1.2733385562896729\n",
      "Step: 1514, Loss: 0.9160052537918091, Accuracy: 1.0, Computation time: 1.1763911247253418\n",
      "Step: 1515, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 1.1026008129119873\n",
      "Step: 1516, Loss: 0.9385902881622314, Accuracy: 0.9750000238418579, Computation time: 1.2931342124938965\n",
      "Step: 1517, Loss: 0.9159733057022095, Accuracy: 1.0, Computation time: 1.018474817276001\n",
      "Step: 1518, Loss: 0.9272775650024414, Accuracy: 0.96875, Computation time: 1.2408277988433838\n",
      "Step: 1519, Loss: 0.9182797074317932, Accuracy: 1.0, Computation time: 1.1591427326202393\n",
      "Step: 1520, Loss: 0.9332150816917419, Accuracy: 0.9750000238418579, Computation time: 1.3107552528381348\n",
      "Step: 1521, Loss: 0.9160947799682617, Accuracy: 1.0, Computation time: 0.9133365154266357\n",
      "Step: 1522, Loss: 0.9464661478996277, Accuracy: 0.9807692766189575, Computation time: 1.67338228225708\n",
      "Step: 1523, Loss: 0.9168097972869873, Accuracy: 1.0, Computation time: 0.9118325710296631\n",
      "Step: 1524, Loss: 0.9184019565582275, Accuracy: 1.0, Computation time: 1.0719139575958252\n",
      "Step: 1525, Loss: 0.9239504337310791, Accuracy: 1.0, Computation time: 1.0486598014831543\n",
      "Step: 1526, Loss: 0.927236795425415, Accuracy: 1.0, Computation time: 1.029719591140747\n",
      "Step: 1527, Loss: 0.919863224029541, Accuracy: 1.0, Computation time: 0.8847494125366211\n",
      "Step: 1528, Loss: 0.9211180210113525, Accuracy: 1.0, Computation time: 0.8652141094207764\n",
      "Step: 1529, Loss: 0.9209791421890259, Accuracy: 1.0, Computation time: 1.0075864791870117\n",
      "########################\n",
      "Test loss: 1.1330556869506836, Test Accuracy_epoch11: 0.6943586468696594\n",
      "########################\n",
      "Step: 1530, Loss: 0.9174970388412476, Accuracy: 1.0, Computation time: 1.0919065475463867\n",
      "Step: 1531, Loss: 0.9210166335105896, Accuracy: 1.0, Computation time: 1.0245137214660645\n",
      "Step: 1532, Loss: 0.9195743203163147, Accuracy: 1.0, Computation time: 1.2633922100067139\n",
      "Step: 1533, Loss: 0.9556230902671814, Accuracy: 0.9545454978942871, Computation time: 1.067345142364502\n",
      "Step: 1534, Loss: 0.9471250176429749, Accuracy: 0.9750000238418579, Computation time: 1.2677545547485352\n",
      "Step: 1535, Loss: 0.9199879169464111, Accuracy: 1.0, Computation time: 1.290886402130127\n",
      "Step: 1536, Loss: 0.9214258790016174, Accuracy: 1.0, Computation time: 0.9915339946746826\n",
      "Step: 1537, Loss: 0.9186083078384399, Accuracy: 1.0, Computation time: 1.5124695301055908\n",
      "Step: 1538, Loss: 0.9396108984947205, Accuracy: 0.9791666865348816, Computation time: 1.131150722503662\n",
      "Step: 1539, Loss: 0.9588651061058044, Accuracy: 0.949999988079071, Computation time: 1.07381272315979\n",
      "Step: 1540, Loss: 0.9432135820388794, Accuracy: nan, Computation time: 1.3406705856323242\n",
      "Step: 1541, Loss: 0.9193022847175598, Accuracy: 1.0, Computation time: 1.207639217376709\n",
      "Step: 1542, Loss: 0.9171459078788757, Accuracy: 1.0, Computation time: 0.9216876029968262\n",
      "Step: 1543, Loss: 0.9383554458618164, Accuracy: 0.9833333492279053, Computation time: 1.0358176231384277\n",
      "Step: 1544, Loss: 0.9412434101104736, Accuracy: 0.9375, Computation time: 1.237525463104248\n",
      "Step: 1545, Loss: 0.9182788729667664, Accuracy: 1.0, Computation time: 0.9621913433074951\n",
      "Step: 1546, Loss: 0.919823169708252, Accuracy: 1.0, Computation time: 1.259408712387085\n",
      "Step: 1547, Loss: 0.9175275564193726, Accuracy: 1.0, Computation time: 1.3285794258117676\n",
      "Step: 1548, Loss: 0.9167589545249939, Accuracy: 1.0, Computation time: 1.494102954864502\n",
      "Step: 1549, Loss: 0.9175159335136414, Accuracy: 1.0, Computation time: 1.0840678215026855\n",
      "Step: 1550, Loss: 0.9167863726615906, Accuracy: 1.0, Computation time: 0.9826817512512207\n",
      "Step: 1551, Loss: 0.9162594676017761, Accuracy: 1.0, Computation time: 1.6488707065582275\n",
      "Step: 1552, Loss: 0.9379972219467163, Accuracy: 0.9791666865348816, Computation time: 1.375755786895752\n",
      "Step: 1553, Loss: 0.9165039658546448, Accuracy: 1.0, Computation time: 1.1624712944030762\n",
      "Step: 1554, Loss: 0.9171680212020874, Accuracy: 1.0, Computation time: 1.0149800777435303\n",
      "Step: 1555, Loss: 0.9168689250946045, Accuracy: 1.0, Computation time: 1.2028825283050537\n",
      "Step: 1556, Loss: 0.9377237558364868, Accuracy: 0.9722222089767456, Computation time: 0.9630417823791504\n",
      "Step: 1557, Loss: 0.9160406589508057, Accuracy: 1.0, Computation time: 0.9813253879547119\n",
      "Step: 1558, Loss: 0.9173085689544678, Accuracy: 1.0, Computation time: 0.9802238941192627\n",
      "Step: 1559, Loss: 0.9160096049308777, Accuracy: 1.0, Computation time: 0.8949065208435059\n",
      "Step: 1560, Loss: 0.9160206317901611, Accuracy: 1.0, Computation time: 0.9351756572723389\n",
      "Step: 1561, Loss: 0.9377526640892029, Accuracy: 0.9722222089767456, Computation time: 1.5163404941558838\n",
      "Step: 1562, Loss: 0.9175041317939758, Accuracy: 1.0, Computation time: 0.9799561500549316\n",
      "Step: 1563, Loss: 0.9160715341567993, Accuracy: 1.0, Computation time: 1.0210373401641846\n",
      "Step: 1564, Loss: 0.9173212647438049, Accuracy: 1.0, Computation time: 1.0499742031097412\n",
      "Step: 1565, Loss: 0.9167859554290771, Accuracy: 1.0, Computation time: 1.1180579662322998\n",
      "Step: 1566, Loss: 0.9376178979873657, Accuracy: 0.949999988079071, Computation time: 1.1247925758361816\n",
      "Step: 1567, Loss: 0.9205913543701172, Accuracy: 1.0, Computation time: 1.0606093406677246\n",
      "Step: 1568, Loss: 0.9386782050132751, Accuracy: 0.949999988079071, Computation time: 1.9213383197784424\n",
      "Step: 1569, Loss: 0.9160398840904236, Accuracy: 1.0, Computation time: 0.9985761642456055\n",
      "Step: 1570, Loss: 0.9161221385002136, Accuracy: 1.0, Computation time: 1.0011847019195557\n",
      "Step: 1571, Loss: 0.9160865545272827, Accuracy: 1.0, Computation time: 1.4293370246887207\n",
      "Step: 1572, Loss: 0.9162083864212036, Accuracy: 1.0, Computation time: 1.3008222579956055\n",
      "Step: 1573, Loss: 0.9162668585777283, Accuracy: 1.0, Computation time: 1.125293254852295\n",
      "Step: 1574, Loss: 0.9160009026527405, Accuracy: 1.0, Computation time: 0.9658505916595459\n",
      "Step: 1575, Loss: 0.9376877546310425, Accuracy: 0.9791666865348816, Computation time: 0.9921054840087891\n",
      "Step: 1576, Loss: 0.9202509522438049, Accuracy: 1.0, Computation time: 1.2864105701446533\n",
      "Step: 1577, Loss: 0.9160304069519043, Accuracy: 1.0, Computation time: 1.0831923484802246\n",
      "Step: 1578, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 1.524338722229004\n",
      "Step: 1579, Loss: 0.9161268472671509, Accuracy: 1.0, Computation time: 1.0913631916046143\n",
      "Step: 1580, Loss: 0.9359887838363647, Accuracy: 0.9807692766189575, Computation time: 1.2421066761016846\n",
      "Step: 1581, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 0.9205291271209717\n",
      "Step: 1582, Loss: 0.9159604907035828, Accuracy: 1.0, Computation time: 0.9738872051239014\n",
      "Step: 1583, Loss: 0.916228175163269, Accuracy: 1.0, Computation time: 1.1897780895233154\n",
      "Step: 1584, Loss: 0.9387151598930359, Accuracy: 0.9722222089767456, Computation time: 1.3778913021087646\n",
      "Step: 1585, Loss: 0.9374822974205017, Accuracy: 0.9772727489471436, Computation time: 1.2399630546569824\n",
      "Step: 1586, Loss: 0.917356550693512, Accuracy: 1.0, Computation time: 1.2757978439331055\n",
      "Step: 1587, Loss: 0.9159938097000122, Accuracy: 1.0, Computation time: 1.1181156635284424\n",
      "Step: 1588, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 0.9684774875640869\n",
      "Step: 1589, Loss: 0.9159314036369324, Accuracy: 1.0, Computation time: 0.9216597080230713\n",
      "Step: 1590, Loss: 0.918864369392395, Accuracy: 1.0, Computation time: 1.245171308517456\n",
      "Step: 1591, Loss: 0.9334424734115601, Accuracy: 0.9722222089767456, Computation time: 0.9949698448181152\n",
      "Step: 1592, Loss: 0.9160344004631042, Accuracy: 1.0, Computation time: 0.9897418022155762\n",
      "Step: 1593, Loss: 0.9371939301490784, Accuracy: 0.9750000238418579, Computation time: 0.9251072406768799\n",
      "Step: 1594, Loss: 0.9613742828369141, Accuracy: 0.9097222089767456, Computation time: 1.442917823791504\n",
      "Step: 1595, Loss: 0.9362422227859497, Accuracy: 0.9833333492279053, Computation time: 0.9667022228240967\n",
      "Step: 1596, Loss: 0.936296284198761, Accuracy: 0.9722222089767456, Computation time: 1.0664465427398682\n",
      "Step: 1597, Loss: 0.9162716269493103, Accuracy: 1.0, Computation time: 0.9933822154998779\n",
      "Step: 1598, Loss: 0.9162618517875671, Accuracy: 1.0, Computation time: 1.1580393314361572\n",
      "Step: 1599, Loss: 0.9160377383232117, Accuracy: 1.0, Computation time: 1.0158538818359375\n",
      "Step: 1600, Loss: 0.9160159826278687, Accuracy: 1.0, Computation time: 0.9841370582580566\n",
      "Step: 1601, Loss: 0.9191682934761047, Accuracy: 1.0, Computation time: 0.8990302085876465\n",
      "Step: 1602, Loss: 0.9381494522094727, Accuracy: 0.9722222089767456, Computation time: 1.0841612815856934\n",
      "Step: 1603, Loss: 0.9191060066223145, Accuracy: 1.0, Computation time: 1.0949618816375732\n",
      "Step: 1604, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 0.9154443740844727\n",
      "Step: 1605, Loss: 0.9185757637023926, Accuracy: 1.0, Computation time: 1.440913200378418\n",
      "Step: 1606, Loss: 0.9159423112869263, Accuracy: 1.0, Computation time: 1.0722284317016602\n",
      "Step: 1607, Loss: 0.9347260594367981, Accuracy: 0.9722222089767456, Computation time: 1.3560328483581543\n",
      "Step: 1608, Loss: 0.9159348607063293, Accuracy: 1.0, Computation time: 0.8669874668121338\n",
      "Step: 1609, Loss: 0.9374911785125732, Accuracy: 0.96875, Computation time: 1.4094195365905762\n",
      "Step: 1610, Loss: 0.9170951247215271, Accuracy: 1.0, Computation time: 1.1551177501678467\n",
      "Step: 1611, Loss: 0.9407054781913757, Accuracy: 0.9807692766189575, Computation time: 1.0113186836242676\n",
      "Step: 1612, Loss: 0.9250146150588989, Accuracy: 1.0, Computation time: 1.0728673934936523\n",
      "Step: 1613, Loss: 0.9159741997718811, Accuracy: 1.0, Computation time: 0.9785442352294922\n",
      "Step: 1614, Loss: 0.9160365462303162, Accuracy: 1.0, Computation time: 0.9959070682525635\n",
      "Step: 1615, Loss: 0.9160811901092529, Accuracy: 1.0, Computation time: 0.8999800682067871\n",
      "Step: 1616, Loss: 0.9162497520446777, Accuracy: 1.0, Computation time: 1.2342112064361572\n",
      "Step: 1617, Loss: 0.9160679578781128, Accuracy: 1.0, Computation time: 0.8936176300048828\n",
      "Step: 1618, Loss: 0.9162606596946716, Accuracy: 1.0, Computation time: 1.2462458610534668\n",
      "Step: 1619, Loss: 0.9159518480300903, Accuracy: 1.0, Computation time: 0.95334792137146\n",
      "Step: 1620, Loss: 0.941901445388794, Accuracy: 0.9750000238418579, Computation time: 1.5817651748657227\n",
      "Step: 1621, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 0.9776606559753418\n",
      "Step: 1622, Loss: 0.9220569133758545, Accuracy: 1.0, Computation time: 1.997227668762207\n",
      "Step: 1623, Loss: 0.9162532091140747, Accuracy: 1.0, Computation time: 1.3502731323242188\n",
      "Step: 1624, Loss: 0.9224070310592651, Accuracy: 1.0, Computation time: 1.366868495941162\n",
      "Step: 1625, Loss: 0.9334216713905334, Accuracy: 0.9722222089767456, Computation time: 1.5175628662109375\n",
      "Step: 1626, Loss: 0.9160733819007874, Accuracy: 1.0, Computation time: 0.8865623474121094\n",
      "Step: 1627, Loss: 0.916448712348938, Accuracy: 1.0, Computation time: 1.1616361141204834\n",
      "Step: 1628, Loss: 0.9162217378616333, Accuracy: 1.0, Computation time: 1.084305763244629\n",
      "Step: 1629, Loss: 0.9161056876182556, Accuracy: 1.0, Computation time: 0.9893801212310791\n",
      "Step: 1630, Loss: 0.9306501746177673, Accuracy: 0.96875, Computation time: 2.9289729595184326\n",
      "Step: 1631, Loss: 0.9375401139259338, Accuracy: 0.9772727489471436, Computation time: 1.1416702270507812\n",
      "Step: 1632, Loss: 0.9163596034049988, Accuracy: 1.0, Computation time: 1.0333161354064941\n",
      "Step: 1633, Loss: 0.9376906156539917, Accuracy: 0.9750000238418579, Computation time: 1.093156099319458\n",
      "Step: 1634, Loss: 0.937210202217102, Accuracy: 0.9791666865348816, Computation time: 1.024144172668457\n",
      "Step: 1635, Loss: 0.9383766055107117, Accuracy: 0.9821428656578064, Computation time: 1.0419800281524658\n",
      "Step: 1636, Loss: 0.9161062836647034, Accuracy: 1.0, Computation time: 0.9548335075378418\n",
      "Step: 1637, Loss: 0.9277680516242981, Accuracy: 0.96875, Computation time: 1.023942470550537\n",
      "Step: 1638, Loss: 0.9160672426223755, Accuracy: 1.0, Computation time: 0.9426183700561523\n",
      "Step: 1639, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 1.362417459487915\n",
      "Step: 1640, Loss: 0.9236032366752625, Accuracy: 1.0, Computation time: 1.1391115188598633\n",
      "Step: 1641, Loss: 0.9160251617431641, Accuracy: 1.0, Computation time: 1.2354621887207031\n",
      "Step: 1642, Loss: 0.9159464836120605, Accuracy: 1.0, Computation time: 0.8783481121063232\n",
      "Step: 1643, Loss: 0.9266841411590576, Accuracy: 0.9722222089767456, Computation time: 1.1053283214569092\n",
      "Step: 1644, Loss: 0.915975034236908, Accuracy: 1.0, Computation time: 0.9264962673187256\n",
      "Step: 1645, Loss: 0.9375783205032349, Accuracy: 0.9791666865348816, Computation time: 1.2949092388153076\n",
      "Step: 1646, Loss: 0.9592912793159485, Accuracy: 0.9437500238418579, Computation time: 0.977372407913208\n",
      "Step: 1647, Loss: 0.9160715937614441, Accuracy: 1.0, Computation time: 1.1678223609924316\n",
      "Step: 1648, Loss: 0.9306952953338623, Accuracy: 1.0, Computation time: 1.5269618034362793\n",
      "Step: 1649, Loss: 0.9195555448532104, Accuracy: 1.0, Computation time: 1.1243975162506104\n",
      "Step: 1650, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 0.9965319633483887\n",
      "Step: 1651, Loss: 0.9292702078819275, Accuracy: 0.96875, Computation time: 0.954409122467041\n",
      "Step: 1652, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 1.035855770111084\n",
      "Step: 1653, Loss: 0.9168104529380798, Accuracy: 1.0, Computation time: 1.0198359489440918\n",
      "Step: 1654, Loss: 0.9159445762634277, Accuracy: 1.0, Computation time: 1.1286289691925049\n",
      "Step: 1655, Loss: 0.9161629676818848, Accuracy: 1.0, Computation time: 0.9301121234893799\n",
      "Step: 1656, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.0947275161743164\n",
      "Step: 1657, Loss: 0.9232536554336548, Accuracy: 1.0, Computation time: 1.1385204792022705\n",
      "Step: 1658, Loss: 0.9159526228904724, Accuracy: 1.0, Computation time: 1.239814043045044\n",
      "Step: 1659, Loss: 0.9166610240936279, Accuracy: 1.0, Computation time: 1.170044183731079\n",
      "Step: 1660, Loss: 0.915969729423523, Accuracy: 1.0, Computation time: 0.9399418830871582\n",
      "Step: 1661, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.2157187461853027\n",
      "Step: 1662, Loss: 0.9159581065177917, Accuracy: 1.0, Computation time: 0.9399158954620361\n",
      "Step: 1663, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 1.101625919342041\n",
      "Step: 1664, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 0.8715364933013916\n",
      "Step: 1665, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 0.9961779117584229\n",
      "Step: 1666, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 0.8907997608184814\n",
      "Step: 1667, Loss: 0.9567108154296875, Accuracy: 0.9125000238418579, Computation time: 1.314138412475586\n",
      "Step: 1668, Loss: 0.9159120321273804, Accuracy: 1.0, Computation time: 1.665956735610962\n",
      "########################\n",
      "Test loss: 1.1226701736450195, Test Accuracy_epoch12: 0.6890698075294495\n",
      "########################\n",
      "Step: 1669, Loss: 0.9159964919090271, Accuracy: 1.0, Computation time: 0.9698240756988525\n",
      "Step: 1670, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 1.0503451824188232\n",
      "Step: 1671, Loss: 0.9158844947814941, Accuracy: 1.0, Computation time: 1.0144402980804443\n",
      "Step: 1672, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 0.9396007061004639\n",
      "Step: 1673, Loss: 0.9160624146461487, Accuracy: 1.0, Computation time: 0.9792153835296631\n",
      "Step: 1674, Loss: 0.9160157442092896, Accuracy: 1.0, Computation time: 0.8759121894836426\n",
      "Step: 1675, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 0.9802868366241455\n",
      "Step: 1676, Loss: 0.9311197400093079, Accuracy: 0.9791666865348816, Computation time: 1.0030272006988525\n",
      "Step: 1677, Loss: 0.9373950958251953, Accuracy: 0.9583333730697632, Computation time: 0.9049637317657471\n",
      "Step: 1678, Loss: 0.9160870909690857, Accuracy: 1.0, Computation time: 1.0690643787384033\n",
      "Step: 1679, Loss: 0.9375907778739929, Accuracy: 0.949999988079071, Computation time: 1.0324881076812744\n",
      "Step: 1680, Loss: 0.937013566493988, Accuracy: 0.9750000238418579, Computation time: 1.0059950351715088\n",
      "Step: 1681, Loss: 0.937553882598877, Accuracy: 0.9833333492279053, Computation time: 0.8969929218292236\n",
      "Step: 1682, Loss: 0.916854977607727, Accuracy: 1.0, Computation time: 1.0940773487091064\n",
      "Step: 1683, Loss: 0.915980875492096, Accuracy: 1.0, Computation time: 0.8603899478912354\n",
      "Step: 1684, Loss: 0.9162505865097046, Accuracy: 1.0, Computation time: 0.8630108833312988\n",
      "Step: 1685, Loss: 0.9161095023155212, Accuracy: 1.0, Computation time: 1.1250529289245605\n",
      "Step: 1686, Loss: 0.9375001192092896, Accuracy: 0.9583333730697632, Computation time: 1.0627388954162598\n",
      "Step: 1687, Loss: 0.9159583449363708, Accuracy: 1.0, Computation time: 0.8834686279296875\n",
      "Step: 1688, Loss: 0.915922999382019, Accuracy: 1.0, Computation time: 0.9536130428314209\n",
      "Step: 1689, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 0.9672093391418457\n",
      "Step: 1690, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 1.4899938106536865\n",
      "Step: 1691, Loss: 0.916018545627594, Accuracy: 1.0, Computation time: 0.9185292720794678\n",
      "Step: 1692, Loss: 0.9246202111244202, Accuracy: 1.0, Computation time: 1.4076001644134521\n",
      "Step: 1693, Loss: 0.916770875453949, Accuracy: 1.0, Computation time: 1.0139520168304443\n",
      "Step: 1694, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 1.0750656127929688\n",
      "Step: 1695, Loss: 0.9219530820846558, Accuracy: 1.0, Computation time: 0.9184010028839111\n",
      "Step: 1696, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 0.916445255279541\n",
      "Step: 1697, Loss: 0.9163541793823242, Accuracy: 1.0, Computation time: 1.0067532062530518\n",
      "Step: 1698, Loss: 0.9547320008277893, Accuracy: 0.949999988079071, Computation time: 1.6400833129882812\n",
      "Step: 1699, Loss: 0.9335035085678101, Accuracy: 0.9642857313156128, Computation time: 1.0007288455963135\n",
      "Step: 1700, Loss: 0.9162130951881409, Accuracy: 1.0, Computation time: 1.485304594039917\n",
      "Step: 1701, Loss: 0.9333622455596924, Accuracy: 0.9772727489471436, Computation time: 1.017815113067627\n",
      "Step: 1702, Loss: 0.9160286784172058, Accuracy: 1.0, Computation time: 0.9106531143188477\n",
      "Step: 1703, Loss: 0.9159855842590332, Accuracy: 1.0, Computation time: 0.8793885707855225\n",
      "Step: 1704, Loss: 0.937677264213562, Accuracy: 0.9642857313156128, Computation time: 1.1327266693115234\n",
      "Step: 1705, Loss: 0.9159777164459229, Accuracy: 1.0, Computation time: 1.0631589889526367\n",
      "Step: 1706, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 0.9136533737182617\n",
      "Step: 1707, Loss: 0.9159452319145203, Accuracy: 1.0, Computation time: 1.003204584121704\n",
      "Step: 1708, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 0.9598159790039062\n",
      "Step: 1709, Loss: 0.916168212890625, Accuracy: 1.0, Computation time: 0.9349346160888672\n",
      "Step: 1710, Loss: 0.9227191209793091, Accuracy: 1.0, Computation time: 1.2823302745819092\n",
      "Step: 1711, Loss: 0.9160915017127991, Accuracy: 1.0, Computation time: 1.0690560340881348\n",
      "Step: 1712, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 0.9444642066955566\n",
      "Step: 1713, Loss: 0.9363436102867126, Accuracy: 0.9807692766189575, Computation time: 1.826495885848999\n",
      "Step: 1714, Loss: 0.9160081744194031, Accuracy: 1.0, Computation time: 0.8874421119689941\n",
      "Step: 1715, Loss: 0.9368662238121033, Accuracy: 0.96875, Computation time: 1.0607728958129883\n",
      "Step: 1716, Loss: 0.9375689625740051, Accuracy: 0.9750000238418579, Computation time: 1.002634048461914\n",
      "Step: 1717, Loss: 0.9160488843917847, Accuracy: 1.0, Computation time: 0.9447963237762451\n",
      "Step: 1718, Loss: 0.9160250425338745, Accuracy: 1.0, Computation time: 1.0156066417694092\n",
      "Step: 1719, Loss: 0.9159547686576843, Accuracy: 1.0, Computation time: 1.043459177017212\n",
      "Step: 1720, Loss: 0.9159336686134338, Accuracy: 1.0, Computation time: 0.9302780628204346\n",
      "Step: 1721, Loss: 0.9456033706665039, Accuracy: 0.9722222089767456, Computation time: 1.2556185722351074\n",
      "Step: 1722, Loss: 0.9376575946807861, Accuracy: 0.9772727489471436, Computation time: 0.9573061466217041\n",
      "Step: 1723, Loss: 0.9280383586883545, Accuracy: 0.9750000238418579, Computation time: 1.06541109085083\n",
      "Step: 1724, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.3930702209472656\n",
      "Step: 1725, Loss: 0.9160757064819336, Accuracy: 1.0, Computation time: 1.1938974857330322\n",
      "Step: 1726, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 0.8587965965270996\n",
      "Step: 1727, Loss: 0.9161500334739685, Accuracy: 1.0, Computation time: 1.2386467456817627\n",
      "Step: 1728, Loss: 0.9366320371627808, Accuracy: 0.9807692766189575, Computation time: 0.9624457359313965\n",
      "Step: 1729, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 1.000391960144043\n",
      "Step: 1730, Loss: 0.9159342050552368, Accuracy: 1.0, Computation time: 1.0023787021636963\n",
      "Step: 1731, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 0.92453932762146\n",
      "Step: 1732, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 0.9170513153076172\n",
      "Step: 1733, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 1.0292043685913086\n",
      "Step: 1734, Loss: 0.9159659743309021, Accuracy: 1.0, Computation time: 1.1825892925262451\n",
      "Step: 1735, Loss: 0.916191041469574, Accuracy: 1.0, Computation time: 1.1919293403625488\n",
      "Step: 1736, Loss: 0.9170764088630676, Accuracy: 1.0, Computation time: 0.8634185791015625\n",
      "Step: 1737, Loss: 0.916346549987793, Accuracy: 1.0, Computation time: 0.9513928890228271\n",
      "Step: 1738, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 1.4628980159759521\n",
      "Step: 1739, Loss: 0.9304477572441101, Accuracy: 0.96875, Computation time: 0.8931853771209717\n",
      "Step: 1740, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 0.942000150680542\n",
      "Step: 1741, Loss: 0.9374605417251587, Accuracy: 0.9750000238418579, Computation time: 1.1454410552978516\n",
      "Step: 1742, Loss: 0.937086820602417, Accuracy: 0.96875, Computation time: 0.9334030151367188\n",
      "Step: 1743, Loss: 0.9160201549530029, Accuracy: 1.0, Computation time: 0.8949787616729736\n",
      "Step: 1744, Loss: 0.9159634709358215, Accuracy: 1.0, Computation time: 1.0353341102600098\n",
      "Step: 1745, Loss: 0.9361204504966736, Accuracy: 0.96875, Computation time: 1.3164927959442139\n",
      "Step: 1746, Loss: 0.9159762263298035, Accuracy: 1.0, Computation time: 1.1464905738830566\n",
      "Step: 1747, Loss: 0.9523102045059204, Accuracy: 0.949999988079071, Computation time: 1.4018738269805908\n",
      "Step: 1748, Loss: 0.9160208702087402, Accuracy: 1.0, Computation time: 1.0449519157409668\n",
      "Step: 1749, Loss: 0.9160616993904114, Accuracy: 1.0, Computation time: 0.9810848236083984\n",
      "Step: 1750, Loss: 0.9204310178756714, Accuracy: 1.0, Computation time: 1.018202781677246\n",
      "Step: 1751, Loss: 0.9160025715827942, Accuracy: 1.0, Computation time: 1.1120274066925049\n",
      "Step: 1752, Loss: 0.91596919298172, Accuracy: 1.0, Computation time: 0.9943430423736572\n",
      "Step: 1753, Loss: 0.9159929156303406, Accuracy: 1.0, Computation time: 1.1161763668060303\n",
      "Step: 1754, Loss: 0.9370465874671936, Accuracy: 0.949999988079071, Computation time: 1.3173789978027344\n",
      "Step: 1755, Loss: 0.9180505275726318, Accuracy: 1.0, Computation time: 1.046830654144287\n",
      "Step: 1756, Loss: 0.916088342666626, Accuracy: 1.0, Computation time: 1.0229592323303223\n",
      "Step: 1757, Loss: 0.9159772992134094, Accuracy: 1.0, Computation time: 0.8850741386413574\n",
      "Step: 1758, Loss: 0.9160005450248718, Accuracy: 1.0, Computation time: 0.9112756252288818\n",
      "Step: 1759, Loss: 0.9159466624259949, Accuracy: 1.0, Computation time: 0.9332869052886963\n",
      "Step: 1760, Loss: 0.9246922135353088, Accuracy: 1.0, Computation time: 1.199061632156372\n",
      "Step: 1761, Loss: 0.9165605902671814, Accuracy: 1.0, Computation time: 1.0113437175750732\n",
      "Step: 1762, Loss: 0.915928065776825, Accuracy: 1.0, Computation time: 0.9229960441589355\n",
      "Step: 1763, Loss: 0.9159290194511414, Accuracy: 1.0, Computation time: 0.8716614246368408\n",
      "Step: 1764, Loss: 0.9161455035209656, Accuracy: 1.0, Computation time: 1.0789375305175781\n",
      "Step: 1765, Loss: 0.9159571528434753, Accuracy: 1.0, Computation time: 0.9465827941894531\n",
      "Step: 1766, Loss: 0.9170858860015869, Accuracy: 1.0, Computation time: 1.2031593322753906\n",
      "Step: 1767, Loss: 0.9160040020942688, Accuracy: 1.0, Computation time: 1.1072039604187012\n",
      "Step: 1768, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 0.9580543041229248\n",
      "Step: 1769, Loss: 0.9204861521720886, Accuracy: 1.0, Computation time: 1.0896515846252441\n",
      "Step: 1770, Loss: 0.9167986512184143, Accuracy: 1.0, Computation time: 1.1780869960784912\n",
      "Step: 1771, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 0.8118801116943359\n",
      "Step: 1772, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 0.9904484748840332\n",
      "Step: 1773, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 0.8718717098236084\n",
      "Step: 1774, Loss: 0.9160240292549133, Accuracy: 1.0, Computation time: 1.0778326988220215\n",
      "Step: 1775, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 0.9317331314086914\n",
      "Step: 1776, Loss: 0.9159293174743652, Accuracy: 1.0, Computation time: 0.9383459091186523\n",
      "Step: 1777, Loss: 0.937617301940918, Accuracy: 0.9166666865348816, Computation time: 1.2105352878570557\n",
      "Step: 1778, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 0.8943591117858887\n",
      "Step: 1779, Loss: 0.9168761372566223, Accuracy: 1.0, Computation time: 0.9678339958190918\n",
      "Step: 1780, Loss: 0.9376873970031738, Accuracy: 0.9722222089767456, Computation time: 1.0134432315826416\n",
      "Step: 1781, Loss: 0.9158883690834045, Accuracy: 1.0, Computation time: 1.015204906463623\n",
      "Step: 1782, Loss: 0.916516900062561, Accuracy: 1.0, Computation time: 1.346015214920044\n",
      "Step: 1783, Loss: 0.9159511923789978, Accuracy: 1.0, Computation time: 0.8550472259521484\n",
      "Step: 1784, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 0.9425997734069824\n",
      "Step: 1785, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 0.907785177230835\n",
      "Step: 1786, Loss: 0.9262999892234802, Accuracy: 0.96875, Computation time: 0.8740460872650146\n",
      "Step: 1787, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 0.9522604942321777\n",
      "Step: 1788, Loss: 0.9587647318840027, Accuracy: 0.9392857551574707, Computation time: 1.220313549041748\n",
      "Step: 1789, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 0.9616372585296631\n",
      "Step: 1790, Loss: 0.9293785095214844, Accuracy: 0.9583333730697632, Computation time: 0.9973423480987549\n",
      "Step: 1791, Loss: 0.9159510135650635, Accuracy: 1.0, Computation time: 0.9278278350830078\n",
      "Step: 1792, Loss: 0.9159268736839294, Accuracy: 1.0, Computation time: 1.0264935493469238\n",
      "Step: 1793, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 0.9242055416107178\n",
      "Step: 1794, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 0.91129469871521\n",
      "Step: 1795, Loss: 0.9294145107269287, Accuracy: 0.9642857313156128, Computation time: 1.0548396110534668\n",
      "Step: 1796, Loss: 0.9322456121444702, Accuracy: 0.9750000238418579, Computation time: 1.482079029083252\n",
      "Step: 1797, Loss: 0.9167333245277405, Accuracy: 1.0, Computation time: 0.997462272644043\n",
      "Step: 1798, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 1.0118234157562256\n",
      "Step: 1799, Loss: 0.9164078235626221, Accuracy: 1.0, Computation time: 0.8773441314697266\n",
      "Step: 1800, Loss: 0.9159847497940063, Accuracy: 1.0, Computation time: 0.8448359966278076\n",
      "Step: 1801, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 0.7767653465270996\n",
      "Step: 1802, Loss: 0.9160187840461731, Accuracy: 1.0, Computation time: 0.8319005966186523\n",
      "Step: 1803, Loss: 0.917044997215271, Accuracy: 1.0, Computation time: 1.1331758499145508\n",
      "Step: 1804, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 0.9896066188812256\n",
      "Step: 1805, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 0.9065461158752441\n",
      "Step: 1806, Loss: 0.9371115565299988, Accuracy: 0.9722222089767456, Computation time: 0.9564175605773926\n",
      "Step: 1807, Loss: 0.9214346408843994, Accuracy: 1.0, Computation time: 1.1244041919708252\n",
      "########################\n",
      "Test loss: 1.121841311454773, Test Accuracy_epoch13: 0.692724347114563\n",
      "########################\n",
      "Step: 1808, Loss: 0.9364892244338989, Accuracy: 0.9772727489471436, Computation time: 1.3406298160552979\n",
      "Step: 1809, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.014206886291504\n",
      "Step: 1810, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 0.9874064922332764\n",
      "Step: 1811, Loss: 0.9159120917320251, Accuracy: 1.0, Computation time: 0.9594759941101074\n",
      "Step: 1812, Loss: 0.9375506639480591, Accuracy: 0.9772727489471436, Computation time: 0.8580260276794434\n",
      "Step: 1813, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.0375702381134033\n",
      "Step: 1814, Loss: 0.9302209615707397, Accuracy: 1.0, Computation time: 1.4876759052276611\n",
      "Step: 1815, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 1.0254461765289307\n",
      "Step: 1816, Loss: 0.9160548448562622, Accuracy: 1.0, Computation time: 0.915938138961792\n",
      "Step: 1817, Loss: 0.9159030318260193, Accuracy: 1.0, Computation time: 0.926807165145874\n",
      "Step: 1818, Loss: 0.9370942711830139, Accuracy: 0.9791666865348816, Computation time: 1.0068793296813965\n",
      "Step: 1819, Loss: 0.9190086722373962, Accuracy: 1.0, Computation time: 1.4358251094818115\n",
      "Step: 1820, Loss: 0.9161679148674011, Accuracy: 1.0, Computation time: 0.8982348442077637\n",
      "Step: 1821, Loss: 0.937583327293396, Accuracy: 0.9750000238418579, Computation time: 0.9287693500518799\n",
      "Step: 1822, Loss: 0.938011109828949, Accuracy: 0.9750000238418579, Computation time: 0.9251070022583008\n",
      "Step: 1823, Loss: 0.9159955382347107, Accuracy: 1.0, Computation time: 1.1770579814910889\n",
      "Step: 1824, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 0.9642670154571533\n",
      "Step: 1825, Loss: 0.9159523248672485, Accuracy: 1.0, Computation time: 0.8433637619018555\n",
      "Step: 1826, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 0.9171900749206543\n",
      "Step: 1827, Loss: 0.9166556596755981, Accuracy: 1.0, Computation time: 1.263885259628296\n",
      "Step: 1828, Loss: 0.9169883728027344, Accuracy: 1.0, Computation time: 1.0682427883148193\n",
      "Step: 1829, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 0.9531407356262207\n",
      "Step: 1830, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.1662476062774658\n",
      "Step: 1831, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 1.2491414546966553\n",
      "Step: 1832, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 0.9164974689483643\n",
      "Step: 1833, Loss: 0.915959894657135, Accuracy: 1.0, Computation time: 1.0919008255004883\n",
      "Step: 1834, Loss: 0.9215065836906433, Accuracy: 1.0, Computation time: 0.8808655738830566\n",
      "Step: 1835, Loss: 0.9378415942192078, Accuracy: 0.9642857313156128, Computation time: 0.9035243988037109\n",
      "Step: 1836, Loss: 0.9162092804908752, Accuracy: 1.0, Computation time: 0.908545732498169\n",
      "Step: 1837, Loss: 0.9199366569519043, Accuracy: 1.0, Computation time: 1.0887374877929688\n",
      "Step: 1838, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 0.9428396224975586\n",
      "Step: 1839, Loss: 0.9160048961639404, Accuracy: 1.0, Computation time: 1.0540144443511963\n",
      "Step: 1840, Loss: 0.9372451305389404, Accuracy: 0.9722222089767456, Computation time: 2.034188747406006\n",
      "Step: 1841, Loss: 0.9161827564239502, Accuracy: 1.0, Computation time: 1.1136550903320312\n",
      "Step: 1842, Loss: 0.9301954507827759, Accuracy: 0.9642857313156128, Computation time: 0.9191248416900635\n",
      "Step: 1843, Loss: 0.9166914820671082, Accuracy: 1.0, Computation time: 1.3395960330963135\n",
      "Step: 1844, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 0.8641595840454102\n",
      "Step: 1845, Loss: 0.9443209171295166, Accuracy: 0.9750000238418579, Computation time: 1.158884048461914\n",
      "Step: 1846, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 0.8978869915008545\n",
      "Step: 1847, Loss: 0.9165077209472656, Accuracy: 1.0, Computation time: 1.1762540340423584\n",
      "Step: 1848, Loss: 0.9202914237976074, Accuracy: 1.0, Computation time: 1.079308271408081\n",
      "Step: 1849, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 0.9218063354492188\n",
      "Step: 1850, Loss: 0.9159536361694336, Accuracy: 1.0, Computation time: 1.0294582843780518\n",
      "Step: 1851, Loss: 0.9159469604492188, Accuracy: 1.0, Computation time: 1.1635825634002686\n",
      "Step: 1852, Loss: 0.9373138546943665, Accuracy: 0.9772727489471436, Computation time: 0.9324197769165039\n",
      "Step: 1853, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 1.0176925659179688\n",
      "Step: 1854, Loss: 0.9380446672439575, Accuracy: 0.9807692766189575, Computation time: 1.278442621231079\n",
      "Step: 1855, Loss: 0.9160443544387817, Accuracy: 1.0, Computation time: 1.0033988952636719\n",
      "Step: 1856, Loss: 0.9360324144363403, Accuracy: 0.9772727489471436, Computation time: 1.3128948211669922\n",
      "Step: 1857, Loss: 0.9158830046653748, Accuracy: 1.0, Computation time: 0.9976675510406494\n",
      "Step: 1858, Loss: 0.9375038743019104, Accuracy: 0.9722222089767456, Computation time: 1.030637264251709\n",
      "Step: 1859, Loss: 0.9202886819839478, Accuracy: 1.0, Computation time: 1.0226588249206543\n",
      "Step: 1860, Loss: 0.9376236796379089, Accuracy: 0.9750000238418579, Computation time: 1.34440279006958\n",
      "Step: 1861, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 0.9492802619934082\n",
      "Step: 1862, Loss: 0.9376816153526306, Accuracy: 0.9807692766189575, Computation time: 1.9129977226257324\n",
      "Step: 1863, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.1077189445495605\n",
      "Step: 1864, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.0139155387878418\n",
      "Step: 1865, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 1.3790252208709717\n",
      "Step: 1866, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 0.9794800281524658\n",
      "Step: 1867, Loss: 0.9374760985374451, Accuracy: 0.9642857313156128, Computation time: 0.9654216766357422\n",
      "Step: 1868, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 1.112757921218872\n",
      "Step: 1869, Loss: 0.9377124905586243, Accuracy: 0.9722222089767456, Computation time: 1.1286671161651611\n",
      "Step: 1870, Loss: 0.9158738255500793, Accuracy: 1.0, Computation time: 0.9757275581359863\n",
      "Step: 1871, Loss: 0.9375602602958679, Accuracy: 0.9642857313156128, Computation time: 1.1274149417877197\n",
      "Step: 1872, Loss: 0.934588611125946, Accuracy: 0.9583333730697632, Computation time: 1.0527663230895996\n",
      "Step: 1873, Loss: 0.9169182777404785, Accuracy: 1.0, Computation time: 1.0431151390075684\n",
      "Step: 1874, Loss: 0.9161682724952698, Accuracy: 1.0, Computation time: 0.9082627296447754\n",
      "Step: 1875, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 0.9641904830932617\n",
      "Step: 1876, Loss: 0.9358772039413452, Accuracy: 0.9583333730697632, Computation time: 1.174710988998413\n",
      "Step: 1877, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 0.8302843570709229\n",
      "Step: 1878, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 0.9310600757598877\n",
      "Step: 1879, Loss: 0.9172901511192322, Accuracy: 1.0, Computation time: 1.2082505226135254\n",
      "Step: 1880, Loss: 0.9370009899139404, Accuracy: 0.9791666865348816, Computation time: 0.924708366394043\n",
      "Step: 1881, Loss: 0.9173183441162109, Accuracy: 1.0, Computation time: 0.8648586273193359\n",
      "Step: 1882, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 0.8707623481750488\n",
      "Step: 1883, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 0.8550314903259277\n",
      "Step: 1884, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 1.0137770175933838\n",
      "Step: 1885, Loss: 0.937508761882782, Accuracy: 0.9722222089767456, Computation time: 0.8598859310150146\n",
      "Step: 1886, Loss: 0.9158724546432495, Accuracy: 1.0, Computation time: 0.8660893440246582\n",
      "Step: 1887, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 0.9554064273834229\n",
      "Step: 1888, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.8563375473022461\n",
      "Step: 1889, Loss: 0.9165065884590149, Accuracy: 1.0, Computation time: 0.8255579471588135\n",
      "Step: 1890, Loss: 0.9178787469863892, Accuracy: 1.0, Computation time: 1.1247985363006592\n",
      "Step: 1891, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 0.8084006309509277\n",
      "Step: 1892, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.0211288928985596\n",
      "Step: 1893, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 0.9918806552886963\n",
      "Step: 1894, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 0.9565775394439697\n",
      "Step: 1895, Loss: 0.9159181714057922, Accuracy: 1.0, Computation time: 1.1297633647918701\n",
      "Step: 1896, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.1082134246826172\n",
      "Step: 1897, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 0.9941425323486328\n",
      "Step: 1898, Loss: 0.9160352349281311, Accuracy: 1.0, Computation time: 1.005364179611206\n",
      "Step: 1899, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 0.9599545001983643\n",
      "Step: 1900, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 1.0872035026550293\n",
      "Step: 1901, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 0.8963241577148438\n",
      "Step: 1902, Loss: 0.9393782019615173, Accuracy: 0.96875, Computation time: 1.0930101871490479\n",
      "Step: 1903, Loss: 0.9582358002662659, Accuracy: 0.9321428537368774, Computation time: 1.0820057392120361\n",
      "Step: 1904, Loss: 0.9165964722633362, Accuracy: 1.0, Computation time: 1.547013759613037\n",
      "Step: 1905, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.039844274520874\n",
      "Step: 1906, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 0.9331512451171875\n",
      "Step: 1907, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 1.1518769264221191\n",
      "Step: 1908, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.6451900005340576\n",
      "Step: 1909, Loss: 0.9158874154090881, Accuracy: 1.0, Computation time: 0.9281821250915527\n",
      "Step: 1910, Loss: 0.9159694910049438, Accuracy: 1.0, Computation time: 0.9886457920074463\n",
      "Step: 1911, Loss: 0.9171767830848694, Accuracy: 1.0, Computation time: 1.1270275115966797\n",
      "Step: 1912, Loss: 0.9169238209724426, Accuracy: 1.0, Computation time: 0.9777963161468506\n",
      "Step: 1913, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 0.9370250701904297\n",
      "Step: 1914, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.1831364631652832\n",
      "Step: 1915, Loss: 0.9200509190559387, Accuracy: 1.0, Computation time: 0.8953444957733154\n",
      "Step: 1916, Loss: 0.9160779118537903, Accuracy: 1.0, Computation time: 0.9490828514099121\n",
      "Step: 1917, Loss: 0.9159111380577087, Accuracy: 1.0, Computation time: 1.232527494430542\n",
      "Step: 1918, Loss: 0.9353048205375671, Accuracy: 0.9750000238418579, Computation time: 1.0051157474517822\n",
      "Step: 1919, Loss: 0.9158902764320374, Accuracy: 1.0, Computation time: 1.1481993198394775\n",
      "Step: 1920, Loss: 0.937404453754425, Accuracy: 0.9583333730697632, Computation time: 1.1183979511260986\n",
      "Step: 1921, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.3157739639282227\n",
      "Step: 1922, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 1.102959156036377\n",
      "Step: 1923, Loss: 0.9217923879623413, Accuracy: 1.0, Computation time: 1.4748592376708984\n",
      "Step: 1924, Loss: 0.9164246320724487, Accuracy: 1.0, Computation time: 1.2511460781097412\n",
      "Step: 1925, Loss: 0.9378127455711365, Accuracy: 0.96875, Computation time: 1.2396516799926758\n",
      "Step: 1926, Loss: 0.9375936388969421, Accuracy: 0.9791666865348816, Computation time: 1.059185266494751\n",
      "Step: 1927, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.0268199443817139\n",
      "Step: 1928, Loss: 0.9375119209289551, Accuracy: 0.9750000238418579, Computation time: 1.0321872234344482\n",
      "Step: 1929, Loss: 0.9159383773803711, Accuracy: 1.0, Computation time: 1.0097603797912598\n",
      "Step: 1930, Loss: 0.9186381101608276, Accuracy: 1.0, Computation time: 0.9515364170074463\n",
      "Step: 1931, Loss: 0.9359880089759827, Accuracy: 0.9722222089767456, Computation time: 0.9851119518280029\n",
      "Step: 1932, Loss: 0.9364268779754639, Accuracy: 0.9821428656578064, Computation time: 1.252305030822754\n",
      "Step: 1933, Loss: 0.916206419467926, Accuracy: 1.0, Computation time: 1.0544102191925049\n",
      "Step: 1934, Loss: 0.937423825263977, Accuracy: 0.9791666865348816, Computation time: 1.291447401046753\n",
      "Step: 1935, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 1.122605562210083\n",
      "Step: 1936, Loss: 0.9215927124023438, Accuracy: 1.0, Computation time: 1.3333771228790283\n",
      "Step: 1937, Loss: 0.9169663190841675, Accuracy: 1.0, Computation time: 1.2056975364685059\n",
      "Step: 1938, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 1.145035743713379\n",
      "Step: 1939, Loss: 0.9159422516822815, Accuracy: 1.0, Computation time: 0.9069650173187256\n",
      "Step: 1940, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 1.1423699855804443\n",
      "Step: 1941, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 0.8994028568267822\n",
      "Step: 1942, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 0.9508786201477051\n",
      "Step: 1943, Loss: 0.9373111128807068, Accuracy: 0.9642857313156128, Computation time: 0.908959150314331\n",
      "Step: 1944, Loss: 0.9158986210823059, Accuracy: 1.0, Computation time: 1.1071274280548096\n",
      "Step: 1945, Loss: 0.9159010052680969, Accuracy: 1.0, Computation time: 1.0053508281707764\n",
      "Step: 1946, Loss: 0.9159176349639893, Accuracy: 1.0, Computation time: 0.9656994342803955\n",
      "########################\n",
      "Test loss: 1.1243224143981934, Test Accuracy_epoch14: 0.6910829544067383\n",
      "########################\n",
      "Step: 1947, Loss: 0.9159346222877502, Accuracy: 1.0, Computation time: 0.9757051467895508\n",
      "Step: 1948, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 1.0077271461486816\n",
      "Step: 1949, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.0478081703186035\n",
      "Step: 1950, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 0.9553122520446777\n",
      "Step: 1951, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 0.9206714630126953\n",
      "Step: 1952, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 1.2406203746795654\n",
      "Step: 1953, Loss: 0.9189037084579468, Accuracy: 1.0, Computation time: 1.7291653156280518\n",
      "Step: 1954, Loss: 0.9159218668937683, Accuracy: 1.0, Computation time: 1.0253725051879883\n",
      "Step: 1955, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.8891856670379639\n",
      "Step: 1956, Loss: 0.9375683665275574, Accuracy: 0.9722222089767456, Computation time: 1.2511789798736572\n",
      "Step: 1957, Loss: 0.915888786315918, Accuracy: 1.0, Computation time: 0.8995013236999512\n",
      "Step: 1958, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 0.9312727451324463\n",
      "Step: 1959, Loss: 0.9389314651489258, Accuracy: 0.9750000238418579, Computation time: 1.3187434673309326\n",
      "Step: 1960, Loss: 0.9160059094429016, Accuracy: 1.0, Computation time: 1.4911272525787354\n",
      "Step: 1961, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 0.8800568580627441\n",
      "Step: 1962, Loss: 0.9375463724136353, Accuracy: 0.9642857313156128, Computation time: 1.1734838485717773\n",
      "Step: 1963, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.0532426834106445\n",
      "Step: 1964, Loss: 0.9161512851715088, Accuracy: 1.0, Computation time: 0.9393665790557861\n",
      "Step: 1965, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 0.9304099082946777\n",
      "Step: 1966, Loss: 0.9202742576599121, Accuracy: 1.0, Computation time: 0.9687681198120117\n",
      "Step: 1967, Loss: 0.9326300621032715, Accuracy: 0.9642857313156128, Computation time: 1.9794154167175293\n",
      "Step: 1968, Loss: 0.93761146068573, Accuracy: 0.984375, Computation time: 1.0864698886871338\n",
      "Step: 1969, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 1.15553879737854\n",
      "Step: 1970, Loss: 0.9379107356071472, Accuracy: 0.9821428656578064, Computation time: 1.1314113140106201\n",
      "Step: 1971, Loss: 0.9159612059593201, Accuracy: 1.0, Computation time: 1.0998220443725586\n",
      "Step: 1972, Loss: 0.9349912405014038, Accuracy: 0.9642857313156128, Computation time: 1.1140022277832031\n",
      "Step: 1973, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 1.0135276317596436\n",
      "Step: 1974, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 0.9399278163909912\n",
      "Step: 1975, Loss: 0.9159348607063293, Accuracy: 1.0, Computation time: 1.081840991973877\n",
      "Step: 1976, Loss: 0.915899932384491, Accuracy: 1.0, Computation time: 1.0435261726379395\n",
      "Step: 1977, Loss: 0.9158982634544373, Accuracy: 1.0, Computation time: 1.479902744293213\n",
      "Step: 1978, Loss: 0.9375176429748535, Accuracy: 0.9791666865348816, Computation time: 1.6868855953216553\n",
      "Step: 1979, Loss: 0.9579238891601562, Accuracy: 0.9166666865348816, Computation time: 1.307945966720581\n",
      "Step: 1980, Loss: 0.9158928394317627, Accuracy: 1.0, Computation time: 0.855276346206665\n",
      "Step: 1981, Loss: 0.9159084558486938, Accuracy: 1.0, Computation time: 0.8902251720428467\n",
      "Step: 1982, Loss: 0.9158893823623657, Accuracy: 1.0, Computation time: 0.8659524917602539\n",
      "Step: 1983, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 0.8706767559051514\n",
      "Step: 1984, Loss: 0.922197699546814, Accuracy: 1.0, Computation time: 1.167569637298584\n",
      "Step: 1985, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.0349864959716797\n",
      "Step: 1986, Loss: 0.9161010384559631, Accuracy: 1.0, Computation time: 1.0699183940887451\n",
      "Step: 1987, Loss: 0.9159736037254333, Accuracy: 1.0, Computation time: 1.0996592044830322\n",
      "Step: 1988, Loss: 0.9161854982376099, Accuracy: 1.0, Computation time: 1.0487849712371826\n",
      "Step: 1989, Loss: 0.934605062007904, Accuracy: 0.9583333730697632, Computation time: 1.0899949073791504\n",
      "Step: 1990, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.1430296897888184\n",
      "Step: 1991, Loss: 0.9419169425964355, Accuracy: 0.9772727489471436, Computation time: 1.5967381000518799\n",
      "Step: 1992, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 0.8856053352355957\n",
      "Step: 1993, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 0.9806511402130127\n",
      "Step: 1994, Loss: 0.9166402816772461, Accuracy: 1.0, Computation time: 1.1646263599395752\n",
      "Step: 1995, Loss: 0.9376023411750793, Accuracy: 0.96875, Computation time: 1.0371062755584717\n",
      "Step: 1996, Loss: 0.9159412384033203, Accuracy: 1.0, Computation time: 0.9692907333374023\n",
      "Step: 1997, Loss: 0.9191625118255615, Accuracy: 1.0, Computation time: 1.2814428806304932\n",
      "Step: 1998, Loss: 0.9161281585693359, Accuracy: 1.0, Computation time: 0.993535041809082\n",
      "Step: 1999, Loss: 0.9374093413352966, Accuracy: 0.9722222089767456, Computation time: 1.1060667037963867\n",
      "Step: 2000, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.1493995189666748\n",
      "Step: 2001, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 0.9283676147460938\n",
      "Step: 2002, Loss: 0.9353817105293274, Accuracy: 0.9791666865348816, Computation time: 1.0119819641113281\n",
      "Step: 2003, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 0.9405660629272461\n",
      "Step: 2004, Loss: 0.9159339070320129, Accuracy: 1.0, Computation time: 1.2505912780761719\n",
      "Step: 2005, Loss: 0.9167826175689697, Accuracy: 1.0, Computation time: 1.089151382446289\n",
      "Step: 2006, Loss: 0.9387229681015015, Accuracy: 0.9833333492279053, Computation time: 1.3203718662261963\n",
      "Step: 2007, Loss: 0.9166759252548218, Accuracy: 1.0, Computation time: 1.0917491912841797\n",
      "Step: 2008, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 0.8174078464508057\n",
      "Step: 2009, Loss: 0.9160762429237366, Accuracy: 1.0, Computation time: 1.2806482315063477\n",
      "Step: 2010, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 0.9387290477752686\n",
      "Step: 2011, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 0.9055802822113037\n",
      "Step: 2012, Loss: 0.9161052107810974, Accuracy: 1.0, Computation time: 0.8331167697906494\n",
      "Step: 2013, Loss: 0.9174280762672424, Accuracy: 1.0, Computation time: 1.1357674598693848\n",
      "Step: 2014, Loss: 0.92185378074646, Accuracy: 1.0, Computation time: 1.0240046977996826\n",
      "Step: 2015, Loss: 0.9163715839385986, Accuracy: 1.0, Computation time: 1.1298251152038574\n",
      "Step: 2016, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 0.8959662914276123\n",
      "Step: 2017, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 0.9966402053833008\n",
      "Step: 2018, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.0251874923706055\n",
      "Step: 2019, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 0.8091452121734619\n",
      "Step: 2020, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 0.8846602439880371\n",
      "Step: 2021, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 0.8643069267272949\n",
      "Step: 2022, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 0.979301929473877\n",
      "Step: 2023, Loss: 0.9375587105751038, Accuracy: 0.9583333730697632, Computation time: 1.0627284049987793\n",
      "Step: 2024, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 1.0409760475158691\n",
      "Step: 2025, Loss: 0.9182701110839844, Accuracy: 1.0, Computation time: 1.2004055976867676\n",
      "Step: 2026, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 1.0709879398345947\n",
      "Step: 2027, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 1.111074447631836\n",
      "Step: 2028, Loss: 0.9158903956413269, Accuracy: 1.0, Computation time: 0.9050314426422119\n",
      "Step: 2029, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 0.8299288749694824\n",
      "Step: 2030, Loss: 0.9376092553138733, Accuracy: 0.9772727489471436, Computation time: 1.0862555503845215\n",
      "Step: 2031, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 0.923635721206665\n",
      "Step: 2032, Loss: 0.9234760999679565, Accuracy: 1.0, Computation time: 1.149094581604004\n",
      "Step: 2033, Loss: 0.9168550968170166, Accuracy: 1.0, Computation time: 1.3117961883544922\n",
      "Step: 2034, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 0.8989148139953613\n",
      "Step: 2035, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 0.9554266929626465\n",
      "Step: 2036, Loss: 0.9164373874664307, Accuracy: 1.0, Computation time: 1.1411480903625488\n",
      "Step: 2037, Loss: 0.936704695224762, Accuracy: 0.96875, Computation time: 1.5923361778259277\n",
      "Step: 2038, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 0.8924450874328613\n",
      "Step: 2039, Loss: 0.9159415364265442, Accuracy: 1.0, Computation time: 1.127880334854126\n",
      "Step: 2040, Loss: 0.9376085996627808, Accuracy: 0.9750000238418579, Computation time: 0.9268312454223633\n",
      "Step: 2041, Loss: 0.9375566840171814, Accuracy: 0.9642857313156128, Computation time: 0.851370096206665\n",
      "Step: 2042, Loss: 0.9170988202095032, Accuracy: 1.0, Computation time: 1.6165502071380615\n",
      "Step: 2043, Loss: 0.9375209808349609, Accuracy: 0.9750000238418579, Computation time: 0.9329233169555664\n",
      "Step: 2044, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 0.8575758934020996\n",
      "Step: 2045, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.2719202041625977\n",
      "Step: 2046, Loss: 0.9374553561210632, Accuracy: 0.9722222089767456, Computation time: 1.2056632041931152\n",
      "Step: 2047, Loss: 0.9381656646728516, Accuracy: 0.9750000238418579, Computation time: 1.6957850456237793\n",
      "Step: 2048, Loss: 0.9161158204078674, Accuracy: 1.0, Computation time: 1.5493292808532715\n",
      "Step: 2049, Loss: 0.9158622622489929, Accuracy: 1.0, Computation time: 1.1392221450805664\n",
      "Step: 2050, Loss: 0.9375987648963928, Accuracy: 0.9722222089767456, Computation time: 1.2015273571014404\n",
      "Step: 2051, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 1.0121855735778809\n",
      "Step: 2052, Loss: 0.9331825375556946, Accuracy: 0.949999988079071, Computation time: 1.4547507762908936\n",
      "Step: 2053, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 0.897454023361206\n",
      "Step: 2054, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 1.005399465560913\n",
      "Step: 2055, Loss: 0.9186117649078369, Accuracy: 1.0, Computation time: 1.3418431282043457\n",
      "Step: 2056, Loss: 0.9159747362136841, Accuracy: 1.0, Computation time: 1.0457572937011719\n",
      "Step: 2057, Loss: 0.937260627746582, Accuracy: 0.96875, Computation time: 1.0081002712249756\n",
      "Step: 2058, Loss: 0.9374533891677856, Accuracy: 0.9750000238418579, Computation time: 0.9160115718841553\n",
      "Step: 2059, Loss: 0.91602623462677, Accuracy: 1.0, Computation time: 1.0620224475860596\n",
      "Step: 2060, Loss: 0.9159654378890991, Accuracy: 1.0, Computation time: 0.8559377193450928\n",
      "Step: 2061, Loss: 0.9377157688140869, Accuracy: 0.9722222089767456, Computation time: 0.8986341953277588\n",
      "Step: 2062, Loss: 0.9301751852035522, Accuracy: 0.949999988079071, Computation time: 1.7583353519439697\n",
      "Step: 2063, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 0.9020426273345947\n",
      "Step: 2064, Loss: 0.9158862233161926, Accuracy: 1.0, Computation time: 0.9987568855285645\n",
      "Step: 2065, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 1.325174331665039\n",
      "Step: 2066, Loss: 0.9282328486442566, Accuracy: 0.9772727489471436, Computation time: 1.0688636302947998\n",
      "Step: 2067, Loss: 0.9254739284515381, Accuracy: 1.0, Computation time: 1.1316463947296143\n",
      "Step: 2068, Loss: 0.9159056544303894, Accuracy: 1.0, Computation time: 1.0368449687957764\n",
      "Step: 2069, Loss: 0.9162993431091309, Accuracy: 1.0, Computation time: 1.0015621185302734\n",
      "Step: 2070, Loss: 0.9253011345863342, Accuracy: 1.0, Computation time: 1.7052052021026611\n",
      "Step: 2071, Loss: 0.9160165190696716, Accuracy: 1.0, Computation time: 1.1905324459075928\n",
      "Step: 2072, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 1.192228078842163\n",
      "Step: 2073, Loss: 0.9159461259841919, Accuracy: 1.0, Computation time: 0.9402978420257568\n",
      "Step: 2074, Loss: 0.9159642457962036, Accuracy: 1.0, Computation time: 1.7619988918304443\n",
      "Step: 2075, Loss: 0.9159976840019226, Accuracy: 1.0, Computation time: 0.8850588798522949\n",
      "Step: 2076, Loss: 0.9204675555229187, Accuracy: 1.0, Computation time: 1.1469173431396484\n",
      "Step: 2077, Loss: 0.9160053133964539, Accuracy: 1.0, Computation time: 0.9351890087127686\n",
      "Step: 2078, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 0.9314532279968262\n",
      "Step: 2079, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 0.9607295989990234\n",
      "Step: 2080, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 0.8882288932800293\n",
      "Step: 2081, Loss: 0.9162275195121765, Accuracy: 1.0, Computation time: 1.0298418998718262\n",
      "Step: 2082, Loss: 0.9158826470375061, Accuracy: 1.0, Computation time: 0.940608024597168\n",
      "Step: 2083, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.111525297164917\n",
      "Step: 2084, Loss: 0.9376124143600464, Accuracy: 0.96875, Computation time: 1.0328478813171387\n",
      "Step: 2085, Loss: 0.9159131646156311, Accuracy: 1.0, Computation time: 1.1360893249511719\n",
      "########################\n",
      "Test loss: 1.1212035417556763, Test Accuracy_epoch15: 0.6953168511390686\n",
      "########################\n",
      "Step: 2086, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 1.040543556213379\n",
      "Step: 2087, Loss: 0.9159762859344482, Accuracy: 1.0, Computation time: 1.2385649681091309\n",
      "Step: 2088, Loss: 0.9376916885375977, Accuracy: 0.9642857313156128, Computation time: 1.2086036205291748\n",
      "Step: 2089, Loss: 0.9375295639038086, Accuracy: 0.96875, Computation time: 1.1766993999481201\n",
      "Step: 2090, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.8396427631378174\n",
      "Step: 2091, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 0.9637184143066406\n",
      "Step: 2092, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.1189172267913818\n",
      "Step: 2093, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 1.242682933807373\n",
      "Step: 2094, Loss: 0.9159549474716187, Accuracy: 1.0, Computation time: 1.2018547058105469\n",
      "Step: 2095, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 1.0067455768585205\n",
      "Step: 2096, Loss: 0.9342550039291382, Accuracy: 0.9772727489471436, Computation time: 1.0043120384216309\n",
      "Step: 2097, Loss: 0.9799723625183105, Accuracy: 0.9295454621315002, Computation time: 1.1107239723205566\n",
      "Step: 2098, Loss: 0.9159244298934937, Accuracy: 1.0, Computation time: 1.1041502952575684\n",
      "Step: 2099, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 1.0690224170684814\n",
      "Step: 2100, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 0.9507498741149902\n",
      "Step: 2101, Loss: 0.915987491607666, Accuracy: 1.0, Computation time: 1.1155469417572021\n",
      "Step: 2102, Loss: 0.9162625670433044, Accuracy: 1.0, Computation time: 1.180330753326416\n",
      "Step: 2103, Loss: 0.9172965884208679, Accuracy: 1.0, Computation time: 1.2327959537506104\n",
      "Step: 2104, Loss: 0.9371610879898071, Accuracy: 0.9821428656578064, Computation time: 1.0957627296447754\n",
      "Step: 2105, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 1.1203010082244873\n",
      "Step: 2106, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.0425996780395508\n",
      "Step: 2107, Loss: 0.9353984594345093, Accuracy: 0.96875, Computation time: 1.087442398071289\n",
      "Step: 2108, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 0.9406180381774902\n",
      "Step: 2109, Loss: 0.9314446449279785, Accuracy: 0.9722222089767456, Computation time: 1.1068077087402344\n",
      "Step: 2110, Loss: 0.9159618020057678, Accuracy: 1.0, Computation time: 0.9041526317596436\n",
      "Step: 2111, Loss: 0.916012704372406, Accuracy: 1.0, Computation time: 1.0169978141784668\n",
      "Step: 2112, Loss: 0.9159923195838928, Accuracy: 1.0, Computation time: 1.1278669834136963\n",
      "Step: 2113, Loss: 0.9165202975273132, Accuracy: 1.0, Computation time: 1.2717411518096924\n",
      "Step: 2114, Loss: 0.9159231781959534, Accuracy: 1.0, Computation time: 0.8726766109466553\n",
      "Step: 2115, Loss: 0.9159250855445862, Accuracy: 1.0, Computation time: 0.9861941337585449\n",
      "Step: 2116, Loss: 0.9170296788215637, Accuracy: 1.0, Computation time: 1.1969172954559326\n",
      "Step: 2117, Loss: 0.9159435033798218, Accuracy: 1.0, Computation time: 0.919480562210083\n",
      "Step: 2118, Loss: 0.9159174561500549, Accuracy: 1.0, Computation time: 1.0109975337982178\n",
      "Step: 2119, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 0.9811632633209229\n",
      "Step: 2120, Loss: 0.9158779978752136, Accuracy: 1.0, Computation time: 1.0604052543640137\n",
      "Step: 2121, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 1.2158713340759277\n",
      "Step: 2122, Loss: 0.9592472314834595, Accuracy: 0.9545454978942871, Computation time: 0.8336868286132812\n",
      "Step: 2123, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 0.9667232036590576\n",
      "Step: 2124, Loss: 0.9360681772232056, Accuracy: 0.9642857313156128, Computation time: 1.00299072265625\n",
      "Step: 2125, Loss: 0.9165455102920532, Accuracy: 1.0, Computation time: 0.928225040435791\n",
      "Step: 2126, Loss: 0.9159747958183289, Accuracy: 1.0, Computation time: 1.0155508518218994\n",
      "Step: 2127, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.0564618110656738\n",
      "Step: 2128, Loss: 0.9222142100334167, Accuracy: 1.0, Computation time: 0.9036006927490234\n",
      "Step: 2129, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.3974857330322266\n",
      "Step: 2130, Loss: 0.915963888168335, Accuracy: 1.0, Computation time: 0.9461033344268799\n",
      "Step: 2131, Loss: 0.9369752407073975, Accuracy: 0.9772727489471436, Computation time: 1.1477560997009277\n",
      "Step: 2132, Loss: 0.9364749193191528, Accuracy: 0.96875, Computation time: 1.1551060676574707\n",
      "Step: 2133, Loss: 0.9160257577896118, Accuracy: 1.0, Computation time: 1.2045998573303223\n",
      "Step: 2134, Loss: 0.9160405397415161, Accuracy: 1.0, Computation time: 1.149010181427002\n",
      "Step: 2135, Loss: 0.9159879684448242, Accuracy: 1.0, Computation time: 0.9991726875305176\n",
      "Step: 2136, Loss: 0.9159730076789856, Accuracy: 1.0, Computation time: 0.8656466007232666\n",
      "Step: 2137, Loss: 0.9160106778144836, Accuracy: 1.0, Computation time: 1.1824414730072021\n",
      "Step: 2138, Loss: 0.9369213581085205, Accuracy: 0.9722222089767456, Computation time: 1.0089178085327148\n",
      "Step: 2139, Loss: 0.9159374237060547, Accuracy: 1.0, Computation time: 0.9367692470550537\n",
      "Step: 2140, Loss: 0.9159442186355591, Accuracy: 1.0, Computation time: 0.9375503063201904\n",
      "Step: 2141, Loss: 0.9159855246543884, Accuracy: 1.0, Computation time: 1.0505123138427734\n",
      "Step: 2142, Loss: 0.9163181781768799, Accuracy: 1.0, Computation time: 0.9986715316772461\n",
      "Step: 2143, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.0154905319213867\n",
      "Step: 2144, Loss: 0.9158989787101746, Accuracy: 1.0, Computation time: 0.9801151752471924\n",
      "Step: 2145, Loss: 0.9198812246322632, Accuracy: 1.0, Computation time: 1.2150731086730957\n",
      "Step: 2146, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 1.087822675704956\n",
      "Step: 2147, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 0.9225265979766846\n",
      "Step: 2148, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 1.0425055027008057\n",
      "Step: 2149, Loss: 0.9159011244773865, Accuracy: 1.0, Computation time: 0.9924485683441162\n",
      "Step: 2150, Loss: 0.915982723236084, Accuracy: 1.0, Computation time: 1.0205366611480713\n",
      "Step: 2151, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 0.9730823040008545\n",
      "Step: 2152, Loss: 0.9159196615219116, Accuracy: 1.0, Computation time: 1.0104522705078125\n",
      "Step: 2153, Loss: 0.923275887966156, Accuracy: 1.0, Computation time: 1.4625766277313232\n",
      "Step: 2154, Loss: 0.9375739097595215, Accuracy: 0.9807692766189575, Computation time: 1.0880448818206787\n",
      "Step: 2155, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 0.9099588394165039\n",
      "Step: 2156, Loss: 0.9159689545631409, Accuracy: 1.0, Computation time: 1.1369028091430664\n",
      "Step: 2157, Loss: 0.9375842213630676, Accuracy: 0.9722222089767456, Computation time: 0.9380300045013428\n",
      "Step: 2158, Loss: 0.9160847663879395, Accuracy: 1.0, Computation time: 0.9725997447967529\n",
      "Step: 2159, Loss: 0.9159575700759888, Accuracy: 1.0, Computation time: 1.0056803226470947\n",
      "Step: 2160, Loss: 0.916211724281311, Accuracy: 1.0, Computation time: 1.4506423473358154\n",
      "Step: 2161, Loss: 0.9195261001586914, Accuracy: 1.0, Computation time: 0.9796459674835205\n",
      "Step: 2162, Loss: 0.9379745125770569, Accuracy: 0.9583333730697632, Computation time: 0.9692668914794922\n",
      "Step: 2163, Loss: 0.9207305908203125, Accuracy: 1.0, Computation time: 1.9093868732452393\n",
      "Step: 2164, Loss: 0.9350957274436951, Accuracy: 0.9583333730697632, Computation time: 0.9386842250823975\n",
      "Step: 2165, Loss: 0.9163740873336792, Accuracy: 1.0, Computation time: 0.9344124794006348\n",
      "Step: 2166, Loss: 0.9309340119361877, Accuracy: 0.949999988079071, Computation time: 1.0309512615203857\n",
      "Step: 2167, Loss: 0.9160289168357849, Accuracy: 1.0, Computation time: 1.1405842304229736\n",
      "Step: 2168, Loss: 0.9318304061889648, Accuracy: 0.9772727489471436, Computation time: 0.9188461303710938\n",
      "Step: 2169, Loss: 0.937674880027771, Accuracy: 0.9750000238418579, Computation time: 1.025599718093872\n",
      "Step: 2170, Loss: 0.9162989854812622, Accuracy: 1.0, Computation time: 1.1325278282165527\n",
      "Step: 2171, Loss: 0.9165889024734497, Accuracy: 1.0, Computation time: 0.9444541931152344\n",
      "Step: 2172, Loss: 0.9161122441291809, Accuracy: 1.0, Computation time: 1.0344607830047607\n",
      "Step: 2173, Loss: 0.9160844087600708, Accuracy: 1.0, Computation time: 1.0829107761383057\n",
      "Step: 2174, Loss: 0.9160532355308533, Accuracy: 1.0, Computation time: 0.8711304664611816\n",
      "Step: 2175, Loss: 0.9373008012771606, Accuracy: 0.984375, Computation time: 0.9724342823028564\n",
      "Step: 2176, Loss: 0.9164538979530334, Accuracy: 1.0, Computation time: 1.2245676517486572\n",
      "Step: 2177, Loss: 0.9372320175170898, Accuracy: 0.949999988079071, Computation time: 1.1297297477722168\n",
      "Step: 2178, Loss: 0.9186915755271912, Accuracy: 1.0, Computation time: 1.0878393650054932\n",
      "Step: 2179, Loss: 0.918146550655365, Accuracy: 1.0, Computation time: 1.1499073505401611\n",
      "Step: 2180, Loss: 0.9159270524978638, Accuracy: 1.0, Computation time: 0.8648056983947754\n",
      "Step: 2181, Loss: 0.9160204529762268, Accuracy: 1.0, Computation time: 1.0341503620147705\n",
      "Step: 2182, Loss: 0.9160510897636414, Accuracy: 1.0, Computation time: 1.1194732189178467\n",
      "Step: 2183, Loss: 0.9161866903305054, Accuracy: 1.0, Computation time: 1.1002700328826904\n",
      "Step: 2184, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 0.9207847118377686\n",
      "Step: 2185, Loss: 0.9160277247428894, Accuracy: 1.0, Computation time: 1.1746654510498047\n",
      "Step: 2186, Loss: 0.9159744381904602, Accuracy: 1.0, Computation time: 1.066617488861084\n",
      "Step: 2187, Loss: 0.9464284777641296, Accuracy: 0.9807692766189575, Computation time: 1.2676339149475098\n",
      "Step: 2188, Loss: 0.9371436834335327, Accuracy: 0.9642857313156128, Computation time: 1.1615898609161377\n",
      "Step: 2189, Loss: 0.937620222568512, Accuracy: 0.949999988079071, Computation time: 0.940972089767456\n",
      "Step: 2190, Loss: 0.9193738698959351, Accuracy: 1.0, Computation time: 1.2291216850280762\n",
      "Step: 2191, Loss: 0.9159596562385559, Accuracy: 1.0, Computation time: 1.084254264831543\n",
      "Step: 2192, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 1.0862231254577637\n",
      "Step: 2193, Loss: 0.9271250367164612, Accuracy: 0.9791666865348816, Computation time: 1.2478845119476318\n",
      "Step: 2194, Loss: 0.9173286557197571, Accuracy: 1.0, Computation time: 1.401761531829834\n",
      "Step: 2195, Loss: 0.9160125851631165, Accuracy: 1.0, Computation time: 0.9409127235412598\n",
      "Step: 2196, Loss: 0.9163623452186584, Accuracy: 1.0, Computation time: 0.9952836036682129\n",
      "Step: 2197, Loss: 0.9160042405128479, Accuracy: 1.0, Computation time: 1.064357042312622\n",
      "Step: 2198, Loss: 0.9291781783103943, Accuracy: 0.9750000238418579, Computation time: 1.1605029106140137\n",
      "Step: 2199, Loss: 0.9158951044082642, Accuracy: 1.0, Computation time: 0.8559129238128662\n",
      "Step: 2200, Loss: 0.9165797233581543, Accuracy: 1.0, Computation time: 1.2468421459197998\n",
      "Step: 2201, Loss: 0.9160329699516296, Accuracy: 1.0, Computation time: 1.1280503273010254\n",
      "Step: 2202, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 0.9330580234527588\n",
      "Step: 2203, Loss: 0.9376236200332642, Accuracy: 0.9722222089767456, Computation time: 1.403364896774292\n",
      "Step: 2204, Loss: 0.9375824928283691, Accuracy: 0.9583333730697632, Computation time: 0.9867076873779297\n",
      "Step: 2205, Loss: 0.9159619212150574, Accuracy: 1.0, Computation time: 0.8976197242736816\n",
      "Step: 2206, Loss: 0.9589702486991882, Accuracy: 0.9666666984558105, Computation time: 1.0532994270324707\n",
      "Step: 2207, Loss: 0.9160078167915344, Accuracy: 1.0, Computation time: 1.0582537651062012\n",
      "Step: 2208, Loss: 0.9160124063491821, Accuracy: 1.0, Computation time: 0.8531737327575684\n",
      "Step: 2209, Loss: 0.9188203811645508, Accuracy: 1.0, Computation time: 1.0243065357208252\n",
      "Step: 2210, Loss: 0.9159547090530396, Accuracy: 1.0, Computation time: 1.0427396297454834\n",
      "Step: 2211, Loss: 0.9170494675636292, Accuracy: 1.0, Computation time: 1.3661103248596191\n",
      "Step: 2212, Loss: 0.9162717461585999, Accuracy: 1.0, Computation time: 0.8566045761108398\n",
      "Step: 2213, Loss: 0.9444227814674377, Accuracy: 0.96875, Computation time: 1.204956293106079\n",
      "Step: 2214, Loss: 0.9589772820472717, Accuracy: 0.9642857313156128, Computation time: 1.2817811965942383\n",
      "Step: 2215, Loss: 0.9320753812789917, Accuracy: 1.0, Computation time: 1.058312177658081\n",
      "Step: 2216, Loss: 0.9161512851715088, Accuracy: 1.0, Computation time: 1.26997971534729\n",
      "Step: 2217, Loss: 0.9160982370376587, Accuracy: 1.0, Computation time: 0.9820511341094971\n",
      "Step: 2218, Loss: 0.9164020419120789, Accuracy: 1.0, Computation time: 1.005253791809082\n",
      "Step: 2219, Loss: 0.9165089726448059, Accuracy: 1.0, Computation time: 1.2436285018920898\n",
      "Step: 2220, Loss: 0.9165046811103821, Accuracy: 1.0, Computation time: 1.2487702369689941\n",
      "Step: 2221, Loss: 0.9166516661643982, Accuracy: 1.0, Computation time: 1.178781509399414\n",
      "Step: 2222, Loss: 0.9380533695220947, Accuracy: 0.9791666865348816, Computation time: 1.5862987041473389\n",
      "Step: 2223, Loss: 0.9164207577705383, Accuracy: 1.0, Computation time: 1.0107500553131104\n",
      "########################\n",
      "Test loss: 1.1248995065689087, Test Accuracy_epoch16: 0.6940817832946777\n",
      "########################\n",
      "Step: 2224, Loss: 0.9160971641540527, Accuracy: 1.0, Computation time: 1.2841522693634033\n",
      "Step: 2225, Loss: 0.916357159614563, Accuracy: 1.0, Computation time: 1.2076849937438965\n",
      "Step: 2226, Loss: 0.9160352945327759, Accuracy: 1.0, Computation time: 1.2808973789215088\n",
      "Step: 2227, Loss: 0.9160614013671875, Accuracy: 1.0, Computation time: 0.9610867500305176\n",
      "Step: 2228, Loss: 0.9162729978561401, Accuracy: 1.0, Computation time: 1.1522042751312256\n",
      "Step: 2229, Loss: 0.9162088632583618, Accuracy: 1.0, Computation time: 0.9406323432922363\n",
      "Step: 2230, Loss: 0.9163113832473755, Accuracy: 1.0, Computation time: 1.086129903793335\n",
      "Step: 2231, Loss: 0.9162192940711975, Accuracy: 1.0, Computation time: 0.9509568214416504\n",
      "Step: 2232, Loss: 0.9163061380386353, Accuracy: 1.0, Computation time: 1.444814682006836\n",
      "Step: 2233, Loss: 0.9387534260749817, Accuracy: 0.9642857313156128, Computation time: 1.089087963104248\n",
      "Step: 2234, Loss: 0.9161922931671143, Accuracy: 1.0, Computation time: 1.1545398235321045\n",
      "Step: 2235, Loss: 0.9359235167503357, Accuracy: 0.9722222089767456, Computation time: 1.0950841903686523\n",
      "Step: 2236, Loss: 0.9160525798797607, Accuracy: 1.0, Computation time: 1.0663836002349854\n",
      "Step: 2237, Loss: 0.9162066578865051, Accuracy: 1.0, Computation time: 1.169935703277588\n",
      "Step: 2238, Loss: 0.9161770343780518, Accuracy: 1.0, Computation time: 1.4060893058776855\n",
      "Step: 2239, Loss: 0.9351434707641602, Accuracy: 0.9642857313156128, Computation time: 1.3869354724884033\n",
      "Step: 2240, Loss: 0.9161904454231262, Accuracy: 1.0, Computation time: 1.5814855098724365\n",
      "Step: 2241, Loss: 0.9162136316299438, Accuracy: 1.0, Computation time: 1.5511295795440674\n",
      "Step: 2242, Loss: 0.9160841703414917, Accuracy: 1.0, Computation time: 1.5952095985412598\n",
      "Step: 2243, Loss: 0.9162046909332275, Accuracy: 1.0, Computation time: 1.0988438129425049\n",
      "Step: 2244, Loss: 0.9160757660865784, Accuracy: 1.0, Computation time: 0.8023455142974854\n",
      "Step: 2245, Loss: 0.9353200793266296, Accuracy: 0.9772727489471436, Computation time: 1.0458652973175049\n",
      "Step: 2246, Loss: 0.9160777926445007, Accuracy: 1.0, Computation time: 1.1866753101348877\n",
      "Step: 2247, Loss: 0.9161434769630432, Accuracy: 1.0, Computation time: 1.0820872783660889\n",
      "Step: 2248, Loss: 0.9162547588348389, Accuracy: 1.0, Computation time: 0.884232759475708\n",
      "Step: 2249, Loss: 0.9161704778671265, Accuracy: 1.0, Computation time: 1.076756238937378\n",
      "Step: 2250, Loss: 0.9160563349723816, Accuracy: 1.0, Computation time: 0.9388234615325928\n",
      "Step: 2251, Loss: 0.9160405397415161, Accuracy: 1.0, Computation time: 0.8896939754486084\n",
      "Step: 2252, Loss: 0.9169216752052307, Accuracy: 1.0, Computation time: 1.1976292133331299\n",
      "Step: 2253, Loss: 0.916114866733551, Accuracy: 1.0, Computation time: 0.8915226459503174\n",
      "Step: 2254, Loss: 0.9204154014587402, Accuracy: 1.0, Computation time: 1.357328176498413\n",
      "Step: 2255, Loss: 0.9173429012298584, Accuracy: 1.0, Computation time: 1.1071677207946777\n",
      "Step: 2256, Loss: 0.916810929775238, Accuracy: 1.0, Computation time: 0.9221482276916504\n",
      "Step: 2257, Loss: 0.9191550016403198, Accuracy: 1.0, Computation time: 1.7034087181091309\n",
      "Step: 2258, Loss: 0.9161004424095154, Accuracy: 1.0, Computation time: 0.9787054061889648\n",
      "Step: 2259, Loss: 0.9163929224014282, Accuracy: 1.0, Computation time: 1.0229606628417969\n",
      "Step: 2260, Loss: 0.916095495223999, Accuracy: 1.0, Computation time: 0.983518123626709\n",
      "Step: 2261, Loss: 0.9369590878486633, Accuracy: 0.9772727489471436, Computation time: 0.934133768081665\n",
      "Step: 2262, Loss: 0.9160551428794861, Accuracy: 1.0, Computation time: 0.8834505081176758\n",
      "Step: 2263, Loss: 0.9161592721939087, Accuracy: 1.0, Computation time: 1.1086831092834473\n",
      "Step: 2264, Loss: 0.9160312414169312, Accuracy: 1.0, Computation time: 1.186840295791626\n",
      "Step: 2265, Loss: 0.9169358611106873, Accuracy: 1.0, Computation time: 1.1202144622802734\n",
      "Step: 2266, Loss: 0.9171100854873657, Accuracy: 1.0, Computation time: 1.1802775859832764\n",
      "Step: 2267, Loss: 0.9159730076789856, Accuracy: 1.0, Computation time: 0.9398033618927002\n",
      "Step: 2268, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 0.9283466339111328\n",
      "Step: 2269, Loss: 0.9166294932365417, Accuracy: 1.0, Computation time: 1.1165990829467773\n",
      "Step: 2270, Loss: 0.9380745887756348, Accuracy: 0.9583333730697632, Computation time: 1.2860314846038818\n",
      "Step: 2271, Loss: 0.9159606099128723, Accuracy: 1.0, Computation time: 1.221531867980957\n",
      "Step: 2272, Loss: 0.937811553478241, Accuracy: 0.9722222089767456, Computation time: 1.143249750137329\n",
      "Step: 2273, Loss: 0.9160737991333008, Accuracy: 1.0, Computation time: 0.9560034275054932\n",
      "Step: 2274, Loss: 0.9160011410713196, Accuracy: 1.0, Computation time: 1.0045385360717773\n",
      "Step: 2275, Loss: 0.9159640073776245, Accuracy: 1.0, Computation time: 0.9173731803894043\n",
      "Step: 2276, Loss: 0.9591980576515198, Accuracy: 0.9285714626312256, Computation time: 0.9502007961273193\n",
      "Step: 2277, Loss: 0.9161123633384705, Accuracy: 1.0, Computation time: 0.8701050281524658\n",
      "Step: 2278, Loss: 0.9162153601646423, Accuracy: 1.0, Computation time: 0.9796791076660156\n",
      "Step: 2279, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 0.9750738143920898\n",
      "Step: 2280, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 0.8553977012634277\n",
      "Step: 2281, Loss: 0.9159887433052063, Accuracy: 1.0, Computation time: 1.2377350330352783\n",
      "Step: 2282, Loss: 0.9160304665565491, Accuracy: 1.0, Computation time: 1.1698153018951416\n",
      "Step: 2283, Loss: 0.9161254167556763, Accuracy: 1.0, Computation time: 1.063722848892212\n",
      "Step: 2284, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 1.3372650146484375\n",
      "Step: 2285, Loss: 0.9159700870513916, Accuracy: 1.0, Computation time: 0.9885978698730469\n",
      "Step: 2286, Loss: 0.9162004590034485, Accuracy: 1.0, Computation time: 0.923274040222168\n",
      "Step: 2287, Loss: 0.9281613230705261, Accuracy: 0.96875, Computation time: 1.54185152053833\n",
      "Step: 2288, Loss: 0.9175453186035156, Accuracy: 1.0, Computation time: 0.9080064296722412\n",
      "Step: 2289, Loss: 0.9161047339439392, Accuracy: 1.0, Computation time: 0.9387669563293457\n",
      "Step: 2290, Loss: 0.9375601410865784, Accuracy: 0.96875, Computation time: 1.0652432441711426\n",
      "Step: 2291, Loss: 0.9159613847732544, Accuracy: 1.0, Computation time: 0.9693965911865234\n",
      "Step: 2292, Loss: 0.9198745489120483, Accuracy: 1.0, Computation time: 1.4699597358703613\n",
      "Step: 2293, Loss: 0.9308900833129883, Accuracy: 0.949999988079071, Computation time: 1.389068365097046\n",
      "Step: 2294, Loss: 0.9161515831947327, Accuracy: 1.0, Computation time: 1.0286874771118164\n",
      "Step: 2295, Loss: 0.916082501411438, Accuracy: 1.0, Computation time: 0.967301607131958\n",
      "Step: 2296, Loss: 0.9163509011268616, Accuracy: 1.0, Computation time: 0.9920117855072021\n",
      "Step: 2297, Loss: 0.9596129059791565, Accuracy: 0.898809552192688, Computation time: 0.8369126319885254\n",
      "Step: 2298, Loss: 0.916486918926239, Accuracy: 1.0, Computation time: 0.9819900989532471\n",
      "Step: 2299, Loss: 0.916278600692749, Accuracy: 1.0, Computation time: 0.9174516201019287\n",
      "Step: 2300, Loss: 0.9395790696144104, Accuracy: 0.949999988079071, Computation time: 1.279486894607544\n",
      "Step: 2301, Loss: 0.9160022735595703, Accuracy: 1.0, Computation time: 1.005115032196045\n",
      "Step: 2302, Loss: 0.9159553050994873, Accuracy: 1.0, Computation time: 0.85536789894104\n",
      "Step: 2303, Loss: 0.9161325693130493, Accuracy: 1.0, Computation time: 1.0209779739379883\n",
      "Step: 2304, Loss: 0.9160411953926086, Accuracy: 1.0, Computation time: 0.9031379222869873\n",
      "Step: 2305, Loss: 0.9160122871398926, Accuracy: 1.0, Computation time: 0.9437308311462402\n",
      "Step: 2306, Loss: 0.9161798357963562, Accuracy: 1.0, Computation time: 0.9909303188323975\n",
      "Step: 2307, Loss: 0.9447518587112427, Accuracy: 0.9750000238418579, Computation time: 1.0391013622283936\n",
      "Step: 2308, Loss: 0.9161331653594971, Accuracy: 1.0, Computation time: 0.866096019744873\n",
      "Step: 2309, Loss: 0.917259931564331, Accuracy: 1.0, Computation time: 1.2120606899261475\n",
      "Step: 2310, Loss: 0.9380121827125549, Accuracy: 0.9722222089767456, Computation time: 0.9277474880218506\n",
      "Step: 2311, Loss: 0.9164997339248657, Accuracy: 1.0, Computation time: 0.9011802673339844\n",
      "Step: 2312, Loss: 0.9162355661392212, Accuracy: 1.0, Computation time: 1.2706153392791748\n",
      "Step: 2313, Loss: 0.9161640405654907, Accuracy: 1.0, Computation time: 1.169503927230835\n",
      "Step: 2314, Loss: 0.9160413146018982, Accuracy: 1.0, Computation time: 0.8271465301513672\n",
      "Step: 2315, Loss: 0.9354264140129089, Accuracy: 0.9722222089767456, Computation time: 1.1113314628601074\n",
      "Step: 2316, Loss: 0.9160450100898743, Accuracy: 1.0, Computation time: 1.0919218063354492\n",
      "Step: 2317, Loss: 0.9160762429237366, Accuracy: 1.0, Computation time: 1.0355453491210938\n",
      "Step: 2318, Loss: 0.9161737561225891, Accuracy: 1.0, Computation time: 1.2354927062988281\n",
      "Step: 2319, Loss: 0.9161207675933838, Accuracy: 1.0, Computation time: 0.9299750328063965\n",
      "Step: 2320, Loss: 0.9394266605377197, Accuracy: 0.9583333730697632, Computation time: 1.2357840538024902\n",
      "Step: 2321, Loss: 0.9161065220832825, Accuracy: 1.0, Computation time: 1.4558851718902588\n",
      "Step: 2322, Loss: 0.9575964212417603, Accuracy: 0.9375, Computation time: 1.356696605682373\n",
      "Step: 2323, Loss: 0.916175127029419, Accuracy: 1.0, Computation time: 0.8481888771057129\n",
      "Step: 2324, Loss: 0.9160135388374329, Accuracy: 1.0, Computation time: 1.5055041313171387\n",
      "Step: 2325, Loss: 0.916102945804596, Accuracy: 1.0, Computation time: 1.1805930137634277\n",
      "Step: 2326, Loss: 0.9160588979721069, Accuracy: 1.0, Computation time: 1.0188117027282715\n",
      "Step: 2327, Loss: 0.9159879684448242, Accuracy: 1.0, Computation time: 0.901508092880249\n",
      "Step: 2328, Loss: 0.916026771068573, Accuracy: 1.0, Computation time: 0.9599072933197021\n",
      "Step: 2329, Loss: 0.9159605503082275, Accuracy: 1.0, Computation time: 0.9052317142486572\n",
      "Step: 2330, Loss: 0.9470885992050171, Accuracy: 0.9415584802627563, Computation time: 2.127609968185425\n",
      "Step: 2331, Loss: 0.9168156981468201, Accuracy: 1.0, Computation time: 1.1578614711761475\n",
      "Step: 2332, Loss: 0.9160999655723572, Accuracy: 1.0, Computation time: 1.4310722351074219\n",
      "Step: 2333, Loss: 0.916032075881958, Accuracy: 1.0, Computation time: 0.9816341400146484\n",
      "Step: 2334, Loss: 0.9171357154846191, Accuracy: 1.0, Computation time: 1.36466646194458\n",
      "Step: 2335, Loss: 0.9161884188652039, Accuracy: 1.0, Computation time: 1.0659289360046387\n",
      "Step: 2336, Loss: 0.9594089984893799, Accuracy: 0.9270833730697632, Computation time: 0.9475934505462646\n",
      "Step: 2337, Loss: 0.9360161423683167, Accuracy: 0.96875, Computation time: 1.299520492553711\n",
      "Step: 2338, Loss: 0.9161355495452881, Accuracy: 1.0, Computation time: 1.5837340354919434\n",
      "Step: 2339, Loss: 0.9160943627357483, Accuracy: 1.0, Computation time: 1.0343801975250244\n",
      "Step: 2340, Loss: 0.915996789932251, Accuracy: 1.0, Computation time: 0.9640378952026367\n",
      "Step: 2341, Loss: 0.917463481426239, Accuracy: 1.0, Computation time: 1.2210609912872314\n",
      "Step: 2342, Loss: 0.9163641929626465, Accuracy: 1.0, Computation time: 1.4015674591064453\n",
      "Step: 2343, Loss: 0.9219582080841064, Accuracy: 1.0, Computation time: 1.271148920059204\n",
      "Step: 2344, Loss: 0.9160705804824829, Accuracy: 1.0, Computation time: 1.0302550792694092\n",
      "Step: 2345, Loss: 0.9376561641693115, Accuracy: 0.9750000238418579, Computation time: 1.1614983081817627\n",
      "Step: 2346, Loss: 0.9159714579582214, Accuracy: 1.0, Computation time: 0.9492855072021484\n",
      "Step: 2347, Loss: 0.9160248041152954, Accuracy: 1.0, Computation time: 1.0238924026489258\n",
      "Step: 2348, Loss: 0.9163029789924622, Accuracy: 1.0, Computation time: 0.9457027912139893\n",
      "Step: 2349, Loss: 0.9389382600784302, Accuracy: 0.9750000238418579, Computation time: 1.0259547233581543\n",
      "Step: 2350, Loss: 0.9159559607505798, Accuracy: 1.0, Computation time: 0.9457006454467773\n",
      "Step: 2351, Loss: 0.9169157147407532, Accuracy: 1.0, Computation time: 1.282581090927124\n",
      "Step: 2352, Loss: 0.9160054326057434, Accuracy: 1.0, Computation time: 1.156463861465454\n",
      "Step: 2353, Loss: 0.915976881980896, Accuracy: 1.0, Computation time: 0.9307899475097656\n",
      "Step: 2354, Loss: 0.9159823656082153, Accuracy: 1.0, Computation time: 1.0900609493255615\n",
      "Step: 2355, Loss: 0.937510073184967, Accuracy: 0.9722222089767456, Computation time: 1.190380573272705\n",
      "Step: 2356, Loss: 0.9160465002059937, Accuracy: 1.0, Computation time: 1.1230621337890625\n",
      "Step: 2357, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 1.0343170166015625\n",
      "Step: 2358, Loss: 0.9159202575683594, Accuracy: 1.0, Computation time: 0.8467559814453125\n",
      "Step: 2359, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 0.8264992237091064\n",
      "Step: 2360, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 0.9672555923461914\n",
      "Step: 2361, Loss: 0.9159545302391052, Accuracy: 1.0, Computation time: 1.3362936973571777\n",
      "Step: 2362, Loss: 0.9374521374702454, Accuracy: 0.9750000238418579, Computation time: 1.0512328147888184\n",
      "########################\n",
      "Test loss: 1.1309373378753662, Test Accuracy_epoch17: 0.6762247085571289\n",
      "########################\n",
      "Step: 2363, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 1.023350477218628\n",
      "Step: 2364, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 0.7795894145965576\n",
      "Step: 2365, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 0.8701629638671875\n",
      "Step: 2366, Loss: 0.9310227036476135, Accuracy: 0.96875, Computation time: 0.9372007846832275\n",
      "Step: 2367, Loss: 0.936668872833252, Accuracy: 0.9583333730697632, Computation time: 1.1828904151916504\n",
      "Step: 2368, Loss: 0.9358763694763184, Accuracy: 0.949999988079071, Computation time: 1.3461148738861084\n",
      "Step: 2369, Loss: 0.9160519242286682, Accuracy: 1.0, Computation time: 1.0202343463897705\n",
      "Step: 2370, Loss: 0.9256170988082886, Accuracy: 0.9750000238418579, Computation time: 1.2896485328674316\n",
      "Step: 2371, Loss: 0.9159323573112488, Accuracy: 1.0, Computation time: 0.8822410106658936\n",
      "Step: 2372, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 0.8703806400299072\n",
      "Step: 2373, Loss: 0.9162673950195312, Accuracy: 1.0, Computation time: 0.990032434463501\n",
      "Step: 2374, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 0.8654088973999023\n",
      "Step: 2375, Loss: 0.9376459717750549, Accuracy: 0.9722222089767456, Computation time: 0.9498512744903564\n",
      "Step: 2376, Loss: 0.9374881982803345, Accuracy: 0.96875, Computation time: 0.8116865158081055\n",
      "Step: 2377, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 0.8974063396453857\n",
      "Step: 2378, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 0.9028537273406982\n",
      "Step: 2379, Loss: 0.9374122023582458, Accuracy: 0.9807692766189575, Computation time: 0.8065111637115479\n",
      "Step: 2380, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 1.0278630256652832\n",
      "Step: 2381, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.8121051788330078\n",
      "Step: 2382, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 1.046879768371582\n",
      "Step: 2383, Loss: 0.9162747859954834, Accuracy: 1.0, Computation time: 0.7981812953948975\n",
      "Step: 2384, Loss: 0.9163493514060974, Accuracy: 1.0, Computation time: 1.202688455581665\n",
      "Step: 2385, Loss: 0.9375319480895996, Accuracy: 0.9852941036224365, Computation time: 1.2669141292572021\n",
      "Step: 2386, Loss: 0.9159284830093384, Accuracy: 1.0, Computation time: 1.106804609298706\n",
      "Step: 2387, Loss: 0.9159485697746277, Accuracy: 1.0, Computation time: 1.0530998706817627\n",
      "Step: 2388, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.1704766750335693\n",
      "Step: 2389, Loss: 0.9159113168716431, Accuracy: 1.0, Computation time: 0.9886646270751953\n",
      "Step: 2390, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 0.9047901630401611\n",
      "Step: 2391, Loss: 0.9163848161697388, Accuracy: 1.0, Computation time: 1.0164110660552979\n",
      "Step: 2392, Loss: 0.9172455072402954, Accuracy: 1.0, Computation time: 0.9086520671844482\n",
      "Step: 2393, Loss: 0.9350022673606873, Accuracy: 0.9642857313156128, Computation time: 0.9889004230499268\n",
      "Step: 2394, Loss: 0.9163076281547546, Accuracy: 1.0, Computation time: 0.8715310096740723\n",
      "Step: 2395, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 0.986386775970459\n",
      "Step: 2396, Loss: 0.9160865545272827, Accuracy: 1.0, Computation time: 1.1081154346466064\n",
      "Step: 2397, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 0.9547209739685059\n",
      "Step: 2398, Loss: 0.9373801946640015, Accuracy: 0.9750000238418579, Computation time: 0.8161048889160156\n",
      "Step: 2399, Loss: 0.9598895907402039, Accuracy: 0.8500000238418579, Computation time: 1.0222134590148926\n",
      "Step: 2400, Loss: 0.9159365892410278, Accuracy: 1.0, Computation time: 0.9386420249938965\n",
      "Step: 2401, Loss: 0.9160866141319275, Accuracy: 1.0, Computation time: 1.0401592254638672\n",
      "Step: 2402, Loss: 0.9160301685333252, Accuracy: 1.0, Computation time: 0.8272755146026611\n",
      "Step: 2403, Loss: 0.9159573316574097, Accuracy: 1.0, Computation time: 0.9218504428863525\n",
      "Step: 2404, Loss: 0.9202951192855835, Accuracy: 1.0, Computation time: 1.214571475982666\n",
      "Step: 2405, Loss: 0.9162572622299194, Accuracy: 1.0, Computation time: 1.0111303329467773\n",
      "Step: 2406, Loss: 0.9164369106292725, Accuracy: 1.0, Computation time: 0.9640507698059082\n",
      "Step: 2407, Loss: 0.9159128069877625, Accuracy: 1.0, Computation time: 0.8840889930725098\n",
      "Step: 2408, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 0.9042749404907227\n",
      "Step: 2409, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 0.8761692047119141\n",
      "Step: 2410, Loss: 0.9159426093101501, Accuracy: 1.0, Computation time: 0.9047138690948486\n",
      "Step: 2411, Loss: 0.9159212112426758, Accuracy: 1.0, Computation time: 0.9130809307098389\n",
      "Step: 2412, Loss: 0.9159230589866638, Accuracy: 1.0, Computation time: 0.8634071350097656\n",
      "Step: 2413, Loss: 0.916007936000824, Accuracy: 1.0, Computation time: 1.1586623191833496\n",
      "Step: 2414, Loss: 0.9391153454780579, Accuracy: 0.9750000238418579, Computation time: 1.3763363361358643\n",
      "Step: 2415, Loss: 0.9289677739143372, Accuracy: 0.9583333730697632, Computation time: 0.9693071842193604\n",
      "Step: 2416, Loss: 0.9373268485069275, Accuracy: 0.9583333730697632, Computation time: 1.0666999816894531\n",
      "Step: 2417, Loss: 0.9160809516906738, Accuracy: 1.0, Computation time: 0.9798734188079834\n",
      "Step: 2418, Loss: 0.9167460203170776, Accuracy: 1.0, Computation time: 0.8921692371368408\n",
      "Step: 2419, Loss: 0.9167083501815796, Accuracy: 1.0, Computation time: 1.201627492904663\n",
      "Step: 2420, Loss: 0.9590890407562256, Accuracy: 0.9434524178504944, Computation time: 0.9333717823028564\n",
      "Step: 2421, Loss: 0.9164372682571411, Accuracy: 1.0, Computation time: 1.1120641231536865\n",
      "Step: 2422, Loss: 0.9160626530647278, Accuracy: 1.0, Computation time: 0.9727511405944824\n",
      "Step: 2423, Loss: 0.915995717048645, Accuracy: 1.0, Computation time: 1.249791145324707\n",
      "Step: 2424, Loss: 0.9375097155570984, Accuracy: 0.9791666865348816, Computation time: 1.2288665771484375\n",
      "Step: 2425, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 0.811408281326294\n",
      "Step: 2426, Loss: 0.9374119639396667, Accuracy: 0.9722222089767456, Computation time: 1.013277292251587\n",
      "Step: 2427, Loss: 0.9232210516929626, Accuracy: 1.0, Computation time: 0.9873499870300293\n",
      "Step: 2428, Loss: 0.9159706234931946, Accuracy: 1.0, Computation time: 1.052779197692871\n",
      "Step: 2429, Loss: 0.9163176417350769, Accuracy: 1.0, Computation time: 0.891531229019165\n",
      "Step: 2430, Loss: 0.9344891309738159, Accuracy: 0.9772727489471436, Computation time: 1.327606439590454\n",
      "Step: 2431, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 0.9455218315124512\n",
      "Step: 2432, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 0.8926715850830078\n",
      "Step: 2433, Loss: 0.9160170555114746, Accuracy: 1.0, Computation time: 0.9039425849914551\n",
      "Step: 2434, Loss: 0.9159760475158691, Accuracy: 1.0, Computation time: 1.0465049743652344\n",
      "Step: 2435, Loss: 0.9159294962882996, Accuracy: 1.0, Computation time: 1.2768878936767578\n",
      "Step: 2436, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 0.9105806350708008\n",
      "Step: 2437, Loss: 0.9163249731063843, Accuracy: 1.0, Computation time: 1.0207183361053467\n",
      "Step: 2438, Loss: 0.9163427352905273, Accuracy: 1.0, Computation time: 1.1855652332305908\n",
      "Step: 2439, Loss: 0.9201573729515076, Accuracy: 1.0, Computation time: 1.01395845413208\n",
      "Step: 2440, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 0.9927854537963867\n",
      "Step: 2441, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 1.0311119556427002\n",
      "Step: 2442, Loss: 0.9159318208694458, Accuracy: 1.0, Computation time: 1.1125237941741943\n",
      "Step: 2443, Loss: 0.9426693320274353, Accuracy: 0.9807692766189575, Computation time: 1.0306460857391357\n",
      "Step: 2444, Loss: 0.937546968460083, Accuracy: 0.9642857313156128, Computation time: 0.9472594261169434\n",
      "Step: 2445, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 0.8841626644134521\n",
      "Step: 2446, Loss: 0.9159551858901978, Accuracy: 1.0, Computation time: 0.9290287494659424\n",
      "Step: 2447, Loss: 0.9160253405570984, Accuracy: 1.0, Computation time: 0.9085428714752197\n",
      "Step: 2448, Loss: 0.9159649014472961, Accuracy: 1.0, Computation time: 0.8299229145050049\n",
      "Step: 2449, Loss: 0.9160180687904358, Accuracy: 1.0, Computation time: 1.2121005058288574\n",
      "Step: 2450, Loss: 0.915989100933075, Accuracy: 1.0, Computation time: 0.992631196975708\n",
      "Step: 2451, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 0.8740963935852051\n",
      "Step: 2452, Loss: 0.9354346990585327, Accuracy: 0.9791666865348816, Computation time: 1.1112053394317627\n",
      "Step: 2453, Loss: 0.9158859848976135, Accuracy: 1.0, Computation time: 1.0991697311401367\n",
      "Step: 2454, Loss: 0.9159210324287415, Accuracy: 1.0, Computation time: 1.1086876392364502\n",
      "Step: 2455, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 0.956953763961792\n",
      "Step: 2456, Loss: 0.9369971752166748, Accuracy: 0.96875, Computation time: 1.0057034492492676\n",
      "Step: 2457, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 0.9172732830047607\n",
      "Step: 2458, Loss: 0.9162039756774902, Accuracy: 1.0, Computation time: 1.0689520835876465\n",
      "Step: 2459, Loss: 0.9162274599075317, Accuracy: 1.0, Computation time: 1.1749136447906494\n",
      "Step: 2460, Loss: 0.9158959984779358, Accuracy: 1.0, Computation time: 0.9342131614685059\n",
      "Step: 2461, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 0.8905665874481201\n",
      "Step: 2462, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.1684107780456543\n",
      "Step: 2463, Loss: 0.916046142578125, Accuracy: 1.0, Computation time: 0.8169889450073242\n",
      "Step: 2464, Loss: 0.9375081658363342, Accuracy: 0.9772727489471436, Computation time: 0.852203369140625\n",
      "Step: 2465, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 0.9128332138061523\n",
      "Step: 2466, Loss: 0.9375296235084534, Accuracy: 0.9583333730697632, Computation time: 1.0538153648376465\n",
      "Step: 2467, Loss: 0.9159161448478699, Accuracy: 1.0, Computation time: 0.8796224594116211\n",
      "Step: 2468, Loss: 0.9158563613891602, Accuracy: 1.0, Computation time: 1.0682487487792969\n",
      "Step: 2469, Loss: 0.9531208276748657, Accuracy: 0.970588207244873, Computation time: 1.365788459777832\n",
      "Step: 2470, Loss: 0.9158809781074524, Accuracy: 1.0, Computation time: 0.9329133033752441\n",
      "Step: 2471, Loss: 0.9158869981765747, Accuracy: 1.0, Computation time: 0.9816160202026367\n",
      "Step: 2472, Loss: 0.9342738389968872, Accuracy: 0.9750000238418579, Computation time: 1.3829700946807861\n",
      "Step: 2473, Loss: 0.9161742925643921, Accuracy: 1.0, Computation time: 0.7868990898132324\n",
      "Step: 2474, Loss: 0.9159455895423889, Accuracy: 1.0, Computation time: 1.0312609672546387\n",
      "Step: 2475, Loss: 0.9373939633369446, Accuracy: 0.9791666865348816, Computation time: 0.989403486251831\n",
      "Step: 2476, Loss: 0.9379929900169373, Accuracy: 0.9807692766189575, Computation time: 0.9028613567352295\n",
      "Step: 2477, Loss: 0.9158902168273926, Accuracy: 1.0, Computation time: 0.944267988204956\n",
      "Step: 2478, Loss: 0.9158828854560852, Accuracy: 1.0, Computation time: 1.0267601013183594\n",
      "Step: 2479, Loss: 0.916618824005127, Accuracy: 1.0, Computation time: 1.0174682140350342\n",
      "Step: 2480, Loss: 0.9167516827583313, Accuracy: 1.0, Computation time: 0.969109058380127\n",
      "Step: 2481, Loss: 0.9163421392440796, Accuracy: 1.0, Computation time: 1.0599029064178467\n",
      "Step: 2482, Loss: 0.9191434979438782, Accuracy: 1.0, Computation time: 1.1804959774017334\n",
      "Step: 2483, Loss: 0.9158918261528015, Accuracy: 1.0, Computation time: 1.113365888595581\n",
      "Step: 2484, Loss: 0.9169903993606567, Accuracy: 1.0, Computation time: 1.035874605178833\n",
      "Step: 2485, Loss: 0.9162782430648804, Accuracy: 1.0, Computation time: 1.0583693981170654\n",
      "Step: 2486, Loss: 0.9161431193351746, Accuracy: 1.0, Computation time: 0.9420723915100098\n",
      "Step: 2487, Loss: 0.9159311056137085, Accuracy: 1.0, Computation time: 0.9404499530792236\n",
      "Step: 2488, Loss: 0.9164127707481384, Accuracy: 1.0, Computation time: 0.8130142688751221\n",
      "Step: 2489, Loss: 0.9159783720970154, Accuracy: 1.0, Computation time: 0.8480625152587891\n",
      "Step: 2490, Loss: 0.9172300696372986, Accuracy: 1.0, Computation time: 1.0592939853668213\n",
      "Step: 2491, Loss: 0.937251627445221, Accuracy: 0.9750000238418579, Computation time: 1.0178191661834717\n",
      "Step: 2492, Loss: 0.9376213550567627, Accuracy: 0.9375, Computation time: 0.7828497886657715\n",
      "Step: 2493, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 1.2383944988250732\n",
      "Step: 2494, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 0.8588309288024902\n",
      "Step: 2495, Loss: 0.9164774417877197, Accuracy: 1.0, Computation time: 1.0462696552276611\n",
      "Step: 2496, Loss: 0.9343504905700684, Accuracy: 0.9772727489471436, Computation time: 1.1010770797729492\n",
      "Step: 2497, Loss: 0.9159232378005981, Accuracy: 1.0, Computation time: 0.9168300628662109\n",
      "Step: 2498, Loss: 0.9159248471260071, Accuracy: 1.0, Computation time: 1.0317113399505615\n",
      "Step: 2499, Loss: 0.9161055088043213, Accuracy: 1.0, Computation time: 0.9325361251831055\n",
      "Step: 2500, Loss: 0.9379143118858337, Accuracy: 0.9750000238418579, Computation time: 1.127375841140747\n",
      "Step: 2501, Loss: 0.9159032702445984, Accuracy: 1.0, Computation time: 0.8692023754119873\n",
      "########################\n",
      "Test loss: 1.128395676612854, Test Accuracy_epoch18: 0.682065486907959\n",
      "########################\n",
      "Step: 2502, Loss: 0.9340103268623352, Accuracy: 0.9772727489471436, Computation time: 1.3380489349365234\n",
      "Step: 2503, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 0.9065637588500977\n",
      "Step: 2504, Loss: 0.916114330291748, Accuracy: 1.0, Computation time: 0.9376428127288818\n",
      "Step: 2505, Loss: 0.9252899289131165, Accuracy: 0.9833333492279053, Computation time: 1.187601089477539\n",
      "Step: 2506, Loss: 0.9375325441360474, Accuracy: 0.9807692766189575, Computation time: 0.902951717376709\n",
      "Step: 2507, Loss: 0.937804639339447, Accuracy: 0.9583333730697632, Computation time: 0.8832225799560547\n",
      "Step: 2508, Loss: 0.9183791875839233, Accuracy: 1.0, Computation time: 0.9536323547363281\n",
      "Step: 2509, Loss: 0.9159447550773621, Accuracy: 1.0, Computation time: 0.8000025749206543\n",
      "Step: 2510, Loss: 0.9375733733177185, Accuracy: 0.9722222089767456, Computation time: 0.9328665733337402\n",
      "Step: 2511, Loss: 0.9159500598907471, Accuracy: 1.0, Computation time: 0.7289445400238037\n",
      "Step: 2512, Loss: 0.9159319400787354, Accuracy: 1.0, Computation time: 1.2831237316131592\n",
      "Step: 2513, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 0.8861887454986572\n",
      "Step: 2514, Loss: 0.9159827828407288, Accuracy: 1.0, Computation time: 1.203540563583374\n",
      "Step: 2515, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 0.9161491394042969\n",
      "Step: 2516, Loss: 0.9159334897994995, Accuracy: 1.0, Computation time: 0.9156019687652588\n",
      "Step: 2517, Loss: 0.9167667031288147, Accuracy: 1.0, Computation time: 1.0571351051330566\n",
      "Step: 2518, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 1.0051383972167969\n",
      "Step: 2519, Loss: 0.9159512519836426, Accuracy: 1.0, Computation time: 0.8592529296875\n",
      "Step: 2520, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 0.9488914012908936\n",
      "Step: 2521, Loss: 0.924936056137085, Accuracy: 1.0, Computation time: 1.1706194877624512\n",
      "Step: 2522, Loss: 0.9183787107467651, Accuracy: 1.0, Computation time: 0.9931631088256836\n",
      "Step: 2523, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 0.8588736057281494\n",
      "Step: 2524, Loss: 0.9381353855133057, Accuracy: 0.9772727489471436, Computation time: 1.1137073040008545\n",
      "Step: 2525, Loss: 0.9159019589424133, Accuracy: 1.0, Computation time: 0.9204761981964111\n",
      "Step: 2526, Loss: 0.9159048199653625, Accuracy: 1.0, Computation time: 1.314743995666504\n",
      "Step: 2527, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 1.0346145629882812\n",
      "Step: 2528, Loss: 0.9159125089645386, Accuracy: 1.0, Computation time: 0.920792818069458\n",
      "Step: 2529, Loss: 0.9163203239440918, Accuracy: 1.0, Computation time: 1.2016615867614746\n",
      "Step: 2530, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 0.828021764755249\n",
      "Step: 2531, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 0.8783934116363525\n",
      "Step: 2532, Loss: 0.9159170985221863, Accuracy: 1.0, Computation time: 0.8377532958984375\n",
      "Step: 2533, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 0.8850283622741699\n",
      "Step: 2534, Loss: 0.9159002304077148, Accuracy: 1.0, Computation time: 0.8491592407226562\n",
      "Step: 2535, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 0.9306776523590088\n",
      "Step: 2536, Loss: 0.915891170501709, Accuracy: 1.0, Computation time: 1.0044565200805664\n",
      "Step: 2537, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.2577707767486572\n",
      "Step: 2538, Loss: 0.9160544872283936, Accuracy: 1.0, Computation time: 1.5464472770690918\n",
      "Step: 2539, Loss: 0.9373025298118591, Accuracy: 0.9791666865348816, Computation time: 0.9571163654327393\n",
      "Step: 2540, Loss: 0.915855884552002, Accuracy: 1.0, Computation time: 1.047631025314331\n",
      "Step: 2541, Loss: 0.9159899950027466, Accuracy: 1.0, Computation time: 0.8977437019348145\n",
      "Step: 2542, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 0.9792428016662598\n",
      "Step: 2543, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 0.8603725433349609\n",
      "Step: 2544, Loss: 0.9159772396087646, Accuracy: 1.0, Computation time: 0.952301025390625\n",
      "Step: 2545, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 0.8946793079376221\n",
      "Step: 2546, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 0.9521455764770508\n",
      "Step: 2547, Loss: 0.9163933396339417, Accuracy: 1.0, Computation time: 1.0777208805084229\n",
      "Step: 2548, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 0.8419179916381836\n",
      "Step: 2549, Loss: 0.9375133514404297, Accuracy: 0.949999988079071, Computation time: 0.9657392501831055\n",
      "Step: 2550, Loss: 0.9442557096481323, Accuracy: 0.96875, Computation time: 1.26932692527771\n",
      "Step: 2551, Loss: 0.9159610867500305, Accuracy: 1.0, Computation time: 0.9153151512145996\n",
      "Step: 2552, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.0465319156646729\n",
      "Step: 2553, Loss: 0.9158735275268555, Accuracy: 1.0, Computation time: 0.8296201229095459\n",
      "Step: 2554, Loss: 0.9159910678863525, Accuracy: 1.0, Computation time: 1.0704329013824463\n",
      "Step: 2555, Loss: 0.937579870223999, Accuracy: nan, Computation time: 0.8846549987792969\n",
      "Step: 2556, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 0.8935408592224121\n",
      "Step: 2557, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 0.9772608280181885\n",
      "Step: 2558, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 1.0508251190185547\n",
      "Step: 2559, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 1.0848863124847412\n",
      "Step: 2560, Loss: 0.9229906797409058, Accuracy: 1.0, Computation time: 1.1779978275299072\n",
      "Step: 2561, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 0.9505364894866943\n",
      "Step: 2562, Loss: 0.9159743785858154, Accuracy: 1.0, Computation time: 0.7693924903869629\n",
      "Step: 2563, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 1.1051814556121826\n",
      "Step: 2564, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 0.9030084609985352\n",
      "Step: 2565, Loss: 0.9247111082077026, Accuracy: 1.0, Computation time: 0.9722566604614258\n",
      "Step: 2566, Loss: 0.9161084294319153, Accuracy: 1.0, Computation time: 1.075368881225586\n",
      "Step: 2567, Loss: 0.9159002900123596, Accuracy: 1.0, Computation time: 1.0579640865325928\n",
      "Step: 2568, Loss: 0.9180825352668762, Accuracy: 1.0, Computation time: 1.062075138092041\n",
      "Step: 2569, Loss: 0.9327206015586853, Accuracy: 0.9852941036224365, Computation time: 1.3042001724243164\n",
      "Step: 2570, Loss: 0.9364804625511169, Accuracy: 0.96875, Computation time: 0.9552962779998779\n",
      "Step: 2571, Loss: 0.9158945679664612, Accuracy: 1.0, Computation time: 0.7574484348297119\n",
      "Step: 2572, Loss: 0.9167569875717163, Accuracy: 1.0, Computation time: 0.9356033802032471\n",
      "Step: 2573, Loss: 0.9339159727096558, Accuracy: 0.96875, Computation time: 0.90132737159729\n",
      "Step: 2574, Loss: 0.9359854459762573, Accuracy: 0.9750000238418579, Computation time: 1.136070966720581\n",
      "Step: 2575, Loss: 0.9161588549613953, Accuracy: 1.0, Computation time: 0.8216683864593506\n",
      "Step: 2576, Loss: 0.9161631464958191, Accuracy: 1.0, Computation time: 0.9521167278289795\n",
      "Step: 2577, Loss: 0.937592625617981, Accuracy: 0.9642857313156128, Computation time: 0.9418554306030273\n",
      "Step: 2578, Loss: 0.9160232543945312, Accuracy: 1.0, Computation time: 0.9206249713897705\n",
      "Step: 2579, Loss: 0.9161301255226135, Accuracy: 1.0, Computation time: 1.0427782535552979\n",
      "Step: 2580, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 0.9468064308166504\n",
      "Step: 2581, Loss: 0.9159346222877502, Accuracy: 1.0, Computation time: 0.8531181812286377\n",
      "Step: 2582, Loss: 0.93735671043396, Accuracy: 0.9772727489471436, Computation time: 0.9125878810882568\n",
      "Step: 2583, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 1.7337446212768555\n",
      "Step: 2584, Loss: 0.9158950448036194, Accuracy: 1.0, Computation time: 0.8130457401275635\n",
      "Step: 2585, Loss: 0.9370259046554565, Accuracy: 0.9772727489471436, Computation time: 0.960798978805542\n",
      "Step: 2586, Loss: 0.9159139394760132, Accuracy: 1.0, Computation time: 0.9302072525024414\n",
      "Step: 2587, Loss: 0.9376632571220398, Accuracy: 0.9772727489471436, Computation time: 0.9950289726257324\n",
      "Step: 2588, Loss: 0.9159791469573975, Accuracy: 1.0, Computation time: 0.9839117527008057\n",
      "Step: 2589, Loss: 0.9159355759620667, Accuracy: 1.0, Computation time: 0.9095416069030762\n",
      "Step: 2590, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.9641687870025635\n",
      "Step: 2591, Loss: 0.9159325361251831, Accuracy: 1.0, Computation time: 0.8155226707458496\n",
      "Step: 2592, Loss: 0.9162462949752808, Accuracy: 1.0, Computation time: 0.9174299240112305\n",
      "Step: 2593, Loss: 0.9336730241775513, Accuracy: 0.9772727489471436, Computation time: 1.7242796421051025\n",
      "Step: 2594, Loss: 0.915946900844574, Accuracy: 1.0, Computation time: 0.7764561176300049\n",
      "Step: 2595, Loss: 0.916013240814209, Accuracy: 1.0, Computation time: 0.921414852142334\n",
      "Step: 2596, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 0.9650487899780273\n",
      "Step: 2597, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 0.9175422191619873\n",
      "Step: 2598, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 1.0015769004821777\n",
      "Step: 2599, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 0.8535678386688232\n",
      "Step: 2600, Loss: 0.9159951210021973, Accuracy: 1.0, Computation time: 1.0141527652740479\n",
      "Step: 2601, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 0.8989994525909424\n",
      "Step: 2602, Loss: 0.9372643232345581, Accuracy: 0.96875, Computation time: 1.1221535205841064\n",
      "Step: 2603, Loss: 0.9373888969421387, Accuracy: 0.9722222089767456, Computation time: 0.8358304500579834\n",
      "Step: 2604, Loss: 0.9159500002861023, Accuracy: 1.0, Computation time: 0.9895997047424316\n",
      "Step: 2605, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 0.8684413433074951\n",
      "Step: 2606, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 0.9832868576049805\n",
      "Step: 2607, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 0.8717544078826904\n",
      "Step: 2608, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 0.97812819480896\n",
      "Step: 2609, Loss: 0.9159988164901733, Accuracy: 1.0, Computation time: 0.9774172306060791\n",
      "Step: 2610, Loss: 0.9237850904464722, Accuracy: 1.0, Computation time: 1.3340797424316406\n",
      "Step: 2611, Loss: 0.9190672039985657, Accuracy: 1.0, Computation time: 1.2670540809631348\n",
      "Step: 2612, Loss: 0.9376130700111389, Accuracy: 0.9642857313156128, Computation time: 1.0924642086029053\n",
      "Step: 2613, Loss: 0.9161561131477356, Accuracy: 1.0, Computation time: 1.1344475746154785\n",
      "Step: 2614, Loss: 0.9362234473228455, Accuracy: 0.9807692766189575, Computation time: 1.2332446575164795\n",
      "Step: 2615, Loss: 0.9159935712814331, Accuracy: 1.0, Computation time: 1.0905811786651611\n",
      "Step: 2616, Loss: 0.9160526990890503, Accuracy: 1.0, Computation time: 0.8811883926391602\n",
      "Step: 2617, Loss: 0.9239740967750549, Accuracy: 1.0, Computation time: 1.4854645729064941\n",
      "Step: 2618, Loss: 0.9228668212890625, Accuracy: 1.0, Computation time: 0.8779149055480957\n",
      "Step: 2619, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 1.2597675323486328\n",
      "Step: 2620, Loss: 0.9202596545219421, Accuracy: 1.0, Computation time: 1.1834547519683838\n",
      "Step: 2621, Loss: 0.9163326025009155, Accuracy: 1.0, Computation time: 1.2427148818969727\n",
      "Step: 2622, Loss: 0.9159452319145203, Accuracy: 1.0, Computation time: 0.9316420555114746\n",
      "Step: 2623, Loss: 0.9159126877784729, Accuracy: 1.0, Computation time: 0.9767999649047852\n",
      "Step: 2624, Loss: 0.9350287914276123, Accuracy: 0.9642857313156128, Computation time: 0.8120546340942383\n",
      "Step: 2625, Loss: 0.9375545978546143, Accuracy: 0.9722222089767456, Computation time: 1.0566737651824951\n",
      "Step: 2626, Loss: 0.916126012802124, Accuracy: 1.0, Computation time: 0.9491863250732422\n",
      "Step: 2627, Loss: 0.9374831914901733, Accuracy: 0.9750000238418579, Computation time: 1.1732780933380127\n",
      "Step: 2628, Loss: 0.9159923195838928, Accuracy: 1.0, Computation time: 0.7574875354766846\n",
      "Step: 2629, Loss: 0.9179076552391052, Accuracy: 1.0, Computation time: 0.9975674152374268\n",
      "Step: 2630, Loss: 0.9375059604644775, Accuracy: 0.96875, Computation time: 0.8275449275970459\n",
      "Step: 2631, Loss: 0.9272852540016174, Accuracy: 0.949999988079071, Computation time: 1.3335349559783936\n",
      "Step: 2632, Loss: 0.9159137010574341, Accuracy: 1.0, Computation time: 0.9395463466644287\n",
      "Step: 2633, Loss: 0.9159052968025208, Accuracy: 1.0, Computation time: 0.8300838470458984\n",
      "Step: 2634, Loss: 0.9353208541870117, Accuracy: 0.96875, Computation time: 0.9971270561218262\n",
      "Step: 2635, Loss: 0.9161610007286072, Accuracy: 1.0, Computation time: 0.9458470344543457\n",
      "Step: 2636, Loss: 0.9375270009040833, Accuracy: 0.9722222089767456, Computation time: 0.894960880279541\n",
      "Step: 2637, Loss: 0.9165617227554321, Accuracy: 1.0, Computation time: 0.9404330253601074\n",
      "Step: 2638, Loss: 0.9381544589996338, Accuracy: 0.9791666865348816, Computation time: 1.0427968502044678\n",
      "Step: 2639, Loss: 0.9159716367721558, Accuracy: 1.0, Computation time: 0.9886438846588135\n",
      "Step: 2640, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 0.9839324951171875\n",
      "########################\n",
      "Test loss: 1.129774570465088, Test Accuracy_epoch19: 0.6775590181350708\n",
      "########################\n",
      "Step: 2641, Loss: 0.9161968231201172, Accuracy: 1.0, Computation time: 1.078016757965088\n",
      "Step: 2642, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 0.8961706161499023\n",
      "Step: 2643, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 0.9079158306121826\n",
      "Step: 2644, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 0.8637056350708008\n",
      "Step: 2645, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.0244040489196777\n",
      "Step: 2646, Loss: 0.9165381789207458, Accuracy: 1.0, Computation time: 1.1600213050842285\n",
      "Step: 2647, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 0.9567859172821045\n",
      "Step: 2648, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 1.0647950172424316\n",
      "Step: 2649, Loss: 0.9190592765808105, Accuracy: 1.0, Computation time: 0.9337673187255859\n",
      "Step: 2650, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 1.1734106540679932\n",
      "Step: 2651, Loss: 0.9158940315246582, Accuracy: 1.0, Computation time: 0.895524263381958\n",
      "Step: 2652, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 0.8923616409301758\n",
      "Step: 2653, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 0.9657294750213623\n",
      "Step: 2654, Loss: 0.9186735153198242, Accuracy: 1.0, Computation time: 1.2912421226501465\n",
      "Step: 2655, Loss: 0.9360445737838745, Accuracy: 0.9642857313156128, Computation time: 1.276116132736206\n",
      "Step: 2656, Loss: 0.9160420298576355, Accuracy: 1.0, Computation time: 1.0360114574432373\n",
      "Step: 2657, Loss: 0.9159637689590454, Accuracy: 1.0, Computation time: 0.9566068649291992\n",
      "Step: 2658, Loss: 0.9159147143363953, Accuracy: 1.0, Computation time: 1.0346546173095703\n",
      "Step: 2659, Loss: 0.9374703168869019, Accuracy: 0.9722222089767456, Computation time: 0.9150888919830322\n",
      "Step: 2660, Loss: 0.9595625996589661, Accuracy: 0.9479166865348816, Computation time: 0.9519798755645752\n",
      "Step: 2661, Loss: 0.916081964969635, Accuracy: 1.0, Computation time: 1.237135648727417\n",
      "Step: 2662, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 0.8109512329101562\n",
      "Step: 2663, Loss: 0.9169763922691345, Accuracy: 1.0, Computation time: 0.8470406532287598\n",
      "Step: 2664, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 1.0009605884552002\n",
      "Step: 2665, Loss: 0.915868878364563, Accuracy: 1.0, Computation time: 0.9331996440887451\n",
      "Step: 2666, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 0.9444997310638428\n",
      "Step: 2667, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 0.8949120044708252\n",
      "Step: 2668, Loss: 0.9161702990531921, Accuracy: 1.0, Computation time: 0.9499342441558838\n",
      "Step: 2669, Loss: 0.9192860126495361, Accuracy: 1.0, Computation time: 1.0114028453826904\n",
      "Step: 2670, Loss: 0.9178864359855652, Accuracy: 1.0, Computation time: 0.8148996829986572\n",
      "Step: 2671, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.069715976715088\n",
      "Step: 2672, Loss: 0.9159690737724304, Accuracy: 1.0, Computation time: 0.8762509822845459\n",
      "Step: 2673, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 1.0983850955963135\n",
      "Step: 2674, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.0070245265960693\n",
      "Step: 2675, Loss: 0.9366650581359863, Accuracy: 0.949999988079071, Computation time: 0.9062621593475342\n",
      "Step: 2676, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 0.9788222312927246\n",
      "Step: 2677, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 0.9546840190887451\n",
      "Step: 2678, Loss: 0.9159757494926453, Accuracy: 1.0, Computation time: 0.8580596446990967\n",
      "Step: 2679, Loss: 0.9159422516822815, Accuracy: 1.0, Computation time: 0.8972868919372559\n",
      "Step: 2680, Loss: 0.915934145450592, Accuracy: 1.0, Computation time: 0.9617633819580078\n",
      "Step: 2681, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.8925833702087402\n",
      "Step: 2682, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 0.8806705474853516\n",
      "Step: 2683, Loss: 0.9158965349197388, Accuracy: 1.0, Computation time: 0.8957664966583252\n",
      "Step: 2684, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 0.8600940704345703\n",
      "Step: 2685, Loss: 0.915993869304657, Accuracy: 1.0, Computation time: 1.1617703437805176\n",
      "Step: 2686, Loss: 0.9159709811210632, Accuracy: 1.0, Computation time: 0.8378164768218994\n",
      "Step: 2687, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 0.8054146766662598\n",
      "Step: 2688, Loss: 0.9196409583091736, Accuracy: 1.0, Computation time: 1.0742557048797607\n",
      "Step: 2689, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 0.8587794303894043\n",
      "Step: 2690, Loss: 0.9177665710449219, Accuracy: 1.0, Computation time: 1.2565364837646484\n",
      "Step: 2691, Loss: 0.9160703420639038, Accuracy: 1.0, Computation time: 0.8826098442077637\n",
      "Step: 2692, Loss: 0.9159857034683228, Accuracy: 1.0, Computation time: 1.0203309059143066\n",
      "Step: 2693, Loss: 0.9159491062164307, Accuracy: 1.0, Computation time: 1.0235280990600586\n",
      "Step: 2694, Loss: 0.9159644842147827, Accuracy: 1.0, Computation time: 1.029848575592041\n",
      "Step: 2695, Loss: 0.9162548780441284, Accuracy: 1.0, Computation time: 0.9120032787322998\n",
      "Step: 2696, Loss: 0.9168198108673096, Accuracy: 1.0, Computation time: 1.1344480514526367\n",
      "Step: 2697, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 0.9916203022003174\n",
      "Step: 2698, Loss: 0.9374967217445374, Accuracy: 0.9750000238418579, Computation time: 0.9579229354858398\n",
      "Step: 2699, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 1.0135314464569092\n",
      "Step: 2700, Loss: 0.9365459084510803, Accuracy: 0.9642857313156128, Computation time: 1.0132033824920654\n",
      "Step: 2701, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 0.8631823062896729\n",
      "Step: 2702, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 0.8390212059020996\n",
      "Step: 2703, Loss: 0.9159277677536011, Accuracy: 1.0, Computation time: 0.7803421020507812\n",
      "Step: 2704, Loss: 0.9376181960105896, Accuracy: 0.9772727489471436, Computation time: 0.8738136291503906\n",
      "Step: 2705, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 0.940582275390625\n",
      "Step: 2706, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 0.8298499584197998\n",
      "Step: 2707, Loss: 0.9160508513450623, Accuracy: 1.0, Computation time: 0.895808219909668\n",
      "Step: 2708, Loss: 0.9376024603843689, Accuracy: 0.9807692766189575, Computation time: 0.8647603988647461\n",
      "Step: 2709, Loss: 0.9190847873687744, Accuracy: 1.0, Computation time: 0.9397289752960205\n",
      "Step: 2710, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.8694043159484863\n",
      "Step: 2711, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 0.9432897567749023\n",
      "Step: 2712, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 0.7378206253051758\n",
      "Step: 2713, Loss: 0.9591972827911377, Accuracy: 0.9147727489471436, Computation time: 0.7823355197906494\n",
      "Step: 2714, Loss: 0.9197060465812683, Accuracy: 1.0, Computation time: 1.2520930767059326\n",
      "Step: 2715, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 0.9527261257171631\n",
      "Step: 2716, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.5860011577606201\n",
      "Step: 2717, Loss: 0.9168790578842163, Accuracy: 1.0, Computation time: 1.0051426887512207\n",
      "Step: 2718, Loss: 0.916193425655365, Accuracy: 1.0, Computation time: 1.0092802047729492\n",
      "Step: 2719, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 0.8903460502624512\n",
      "Step: 2720, Loss: 0.9376623034477234, Accuracy: 0.96875, Computation time: 1.079547643661499\n",
      "Step: 2721, Loss: 0.9159614443778992, Accuracy: 1.0, Computation time: 1.3298437595367432\n",
      "Step: 2722, Loss: 0.9159127473831177, Accuracy: 1.0, Computation time: 0.9230360984802246\n",
      "Step: 2723, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 0.9227867126464844\n",
      "Step: 2724, Loss: 0.9158733487129211, Accuracy: 1.0, Computation time: 0.9011261463165283\n",
      "Step: 2725, Loss: 0.9375419020652771, Accuracy: 0.9642857313156128, Computation time: 0.9034435749053955\n",
      "Step: 2726, Loss: 0.9553364515304565, Accuracy: 0.9415584802627563, Computation time: 0.7722227573394775\n",
      "Step: 2727, Loss: 0.9159800410270691, Accuracy: 1.0, Computation time: 0.9527628421783447\n",
      "Step: 2728, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 0.8949956893920898\n",
      "Step: 2729, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 0.9672980308532715\n",
      "Step: 2730, Loss: 0.9159693121910095, Accuracy: 1.0, Computation time: 1.0564968585968018\n",
      "Step: 2731, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 1.036839246749878\n",
      "Step: 2732, Loss: 0.9159753322601318, Accuracy: 1.0, Computation time: 1.1282384395599365\n",
      "Step: 2733, Loss: 0.9595853686332703, Accuracy: 0.9495192766189575, Computation time: 1.7100343704223633\n",
      "Step: 2734, Loss: 0.9159421920776367, Accuracy: 1.0, Computation time: 0.9851930141448975\n",
      "Step: 2735, Loss: 0.9204165935516357, Accuracy: 1.0, Computation time: 1.3278741836547852\n",
      "Step: 2736, Loss: 0.9158544540405273, Accuracy: 1.0, Computation time: 0.8226492404937744\n",
      "Step: 2737, Loss: 0.9158878922462463, Accuracy: 1.0, Computation time: 0.9174208641052246\n",
      "Step: 2738, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 0.918729305267334\n",
      "Step: 2739, Loss: 0.9159190654754639, Accuracy: 1.0, Computation time: 0.8986721038818359\n",
      "Step: 2740, Loss: 0.9159069061279297, Accuracy: 1.0, Computation time: 0.8539271354675293\n",
      "Step: 2741, Loss: 0.9159207940101624, Accuracy: 1.0, Computation time: 0.8832104206085205\n",
      "Step: 2742, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 0.9414715766906738\n",
      "Step: 2743, Loss: 0.9375759959220886, Accuracy: 0.9750000238418579, Computation time: 1.0916624069213867\n",
      "Step: 2744, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.0350208282470703\n",
      "Step: 2745, Loss: 0.9159663915634155, Accuracy: 1.0, Computation time: 1.352250099182129\n",
      "Step: 2746, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.0035712718963623\n",
      "Step: 2747, Loss: 0.9159160852432251, Accuracy: 1.0, Computation time: 1.0441582202911377\n",
      "Step: 2748, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 1.0102388858795166\n",
      "Step: 2749, Loss: 0.9386470317840576, Accuracy: 0.96875, Computation time: 1.0239250659942627\n",
      "Step: 2750, Loss: 0.9381392002105713, Accuracy: 0.9375, Computation time: 1.3548839092254639\n",
      "Step: 2751, Loss: 0.9158576130867004, Accuracy: 1.0, Computation time: 1.0055136680603027\n",
      "Step: 2752, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 1.7008707523345947\n",
      "Step: 2753, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.7828519344329834\n",
      "Step: 2754, Loss: 0.9158493280410767, Accuracy: 1.0, Computation time: 0.9008910655975342\n",
      "Step: 2755, Loss: 0.9164046049118042, Accuracy: 1.0, Computation time: 0.9746766090393066\n",
      "Step: 2756, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 0.9848287105560303\n",
      "Step: 2757, Loss: 0.9375038743019104, Accuracy: 0.9583333730697632, Computation time: 0.9627935886383057\n",
      "Step: 2758, Loss: 0.9159315228462219, Accuracy: 1.0, Computation time: 1.0179214477539062\n",
      "Step: 2759, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 1.0689449310302734\n",
      "Step: 2760, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 1.0890696048736572\n",
      "Step: 2761, Loss: 0.9158619046211243, Accuracy: 1.0, Computation time: 0.8149185180664062\n",
      "Step: 2762, Loss: 0.9373835325241089, Accuracy: 0.9807692766189575, Computation time: 0.8889946937561035\n",
      "Step: 2763, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 1.1845941543579102\n",
      "Step: 2764, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 1.0117735862731934\n",
      "Step: 2765, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 0.8870947360992432\n",
      "Step: 2766, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.8592872619628906\n",
      "Step: 2767, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.068737268447876\n",
      "Step: 2768, Loss: 0.9159266352653503, Accuracy: 1.0, Computation time: 0.971027135848999\n",
      "Step: 2769, Loss: 0.9603429436683655, Accuracy: 0.922619104385376, Computation time: 1.1431519985198975\n",
      "Step: 2770, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.05472731590271\n",
      "Step: 2771, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 1.0866129398345947\n",
      "Step: 2772, Loss: 0.9206779599189758, Accuracy: 1.0, Computation time: 2.2312686443328857\n",
      "Step: 2773, Loss: 0.9159038662910461, Accuracy: 1.0, Computation time: 1.2721409797668457\n",
      "Step: 2774, Loss: 0.9373512864112854, Accuracy: 0.96875, Computation time: 1.070927381515503\n",
      "Step: 2775, Loss: 0.9250155687332153, Accuracy: 1.0, Computation time: 1.0177056789398193\n",
      "Step: 2776, Loss: 0.9159494042396545, Accuracy: 1.0, Computation time: 0.9381628036499023\n",
      "Step: 2777, Loss: 0.9374425411224365, Accuracy: 0.9750000238418579, Computation time: 1.0437285900115967\n",
      "Step: 2778, Loss: 0.9381044507026672, Accuracy: 0.9722222089767456, Computation time: 1.053487777709961\n",
      "Step: 2779, Loss: 0.9166553020477295, Accuracy: 1.0, Computation time: 1.056535005569458\n",
      "########################\n",
      "Test loss: 1.1309126615524292, Test Accuracy_epoch20: 0.6794313788414001\n",
      "########################\n",
      "Step: 2780, Loss: 0.9267892837524414, Accuracy: 0.9791666865348816, Computation time: 1.488374948501587\n",
      "Step: 2781, Loss: 0.9160529971122742, Accuracy: 1.0, Computation time: 0.8346853256225586\n",
      "Step: 2782, Loss: 0.9160757660865784, Accuracy: 1.0, Computation time: 1.0988125801086426\n",
      "Step: 2783, Loss: 0.937626838684082, Accuracy: 0.9722222089767456, Computation time: 1.1517326831817627\n",
      "Step: 2784, Loss: 0.9161816835403442, Accuracy: 1.0, Computation time: 0.9315676689147949\n",
      "Step: 2785, Loss: 0.9160942435264587, Accuracy: 1.0, Computation time: 1.0911438465118408\n",
      "Step: 2786, Loss: 0.9161096215248108, Accuracy: 1.0, Computation time: 0.8123748302459717\n",
      "Step: 2787, Loss: 0.9161201119422913, Accuracy: 1.0, Computation time: 1.1114585399627686\n",
      "Step: 2788, Loss: 0.9179898500442505, Accuracy: 1.0, Computation time: 1.0673911571502686\n",
      "Step: 2789, Loss: 0.9163305163383484, Accuracy: 1.0, Computation time: 0.8485147953033447\n",
      "Step: 2790, Loss: 0.9158913493156433, Accuracy: 1.0, Computation time: 0.8190059661865234\n",
      "Step: 2791, Loss: 0.9376798272132874, Accuracy: 0.9583333730697632, Computation time: 0.8896276950836182\n",
      "Step: 2792, Loss: 0.9371954202651978, Accuracy: 0.9583333730697632, Computation time: 1.1632366180419922\n",
      "Step: 2793, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 0.828761100769043\n",
      "Step: 2794, Loss: 0.9377784729003906, Accuracy: 0.949999988079071, Computation time: 1.0951473712921143\n",
      "Step: 2795, Loss: 0.9378722310066223, Accuracy: 0.949999988079071, Computation time: 1.046076774597168\n",
      "Step: 2796, Loss: 0.9163752198219299, Accuracy: 1.0, Computation time: 0.927788257598877\n",
      "Step: 2797, Loss: 0.9160099625587463, Accuracy: 1.0, Computation time: 0.8702802658081055\n",
      "Step: 2798, Loss: 0.9160159826278687, Accuracy: 1.0, Computation time: 0.8889498710632324\n",
      "Step: 2799, Loss: 0.9159848093986511, Accuracy: 1.0, Computation time: 1.0105345249176025\n",
      "Step: 2800, Loss: 0.915987491607666, Accuracy: 1.0, Computation time: 1.1033222675323486\n",
      "Step: 2801, Loss: 0.9158908724784851, Accuracy: 1.0, Computation time: 0.9666597843170166\n",
      "Step: 2802, Loss: 0.9184005856513977, Accuracy: 1.0, Computation time: 1.4349267482757568\n",
      "Step: 2803, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 0.9548511505126953\n",
      "Step: 2804, Loss: 0.9377040863037109, Accuracy: 0.96875, Computation time: 1.0160605907440186\n",
      "Step: 2805, Loss: 0.9159025549888611, Accuracy: 1.0, Computation time: 1.1135585308074951\n",
      "Step: 2806, Loss: 0.9623016715049744, Accuracy: 0.9077380895614624, Computation time: 1.1361331939697266\n",
      "Step: 2807, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 0.8990433216094971\n",
      "Step: 2808, Loss: 0.9159165024757385, Accuracy: 1.0, Computation time: 0.805584192276001\n",
      "Step: 2809, Loss: 0.9159643054008484, Accuracy: 1.0, Computation time: 0.9591064453125\n",
      "Step: 2810, Loss: 0.9374852180480957, Accuracy: 0.9642857313156128, Computation time: 1.1926090717315674\n",
      "Step: 2811, Loss: 0.9160289168357849, Accuracy: 1.0, Computation time: 1.1546902656555176\n",
      "Step: 2812, Loss: 0.9159510135650635, Accuracy: 1.0, Computation time: 0.9435346126556396\n",
      "Step: 2813, Loss: 0.9159674048423767, Accuracy: 1.0, Computation time: 0.871328592300415\n",
      "Step: 2814, Loss: 0.9159496426582336, Accuracy: 1.0, Computation time: 1.1976699829101562\n",
      "Step: 2815, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 0.8857455253601074\n",
      "Step: 2816, Loss: 0.9220433235168457, Accuracy: 1.0, Computation time: 0.8565242290496826\n",
      "Step: 2817, Loss: 0.9317721128463745, Accuracy: 0.9722222089767456, Computation time: 1.2182674407958984\n",
      "Step: 2818, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 1.002695083618164\n",
      "Step: 2819, Loss: 0.9405467510223389, Accuracy: 0.9791666865348816, Computation time: 1.0822670459747314\n",
      "Step: 2820, Loss: 0.9160028696060181, Accuracy: 1.0, Computation time: 0.9909377098083496\n",
      "Step: 2821, Loss: 0.9161261320114136, Accuracy: 1.0, Computation time: 1.024001121520996\n",
      "Step: 2822, Loss: 0.9159983396530151, Accuracy: 1.0, Computation time: 0.8430206775665283\n",
      "Step: 2823, Loss: 0.9161686301231384, Accuracy: 1.0, Computation time: 1.209420919418335\n",
      "Step: 2824, Loss: 0.9375939965248108, Accuracy: 0.9791666865348816, Computation time: 0.838921070098877\n",
      "Step: 2825, Loss: 0.9241247773170471, Accuracy: 1.0, Computation time: 1.0301563739776611\n",
      "Step: 2826, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.0060365200042725\n",
      "Step: 2827, Loss: 0.9378976821899414, Accuracy: 0.9791666865348816, Computation time: 1.1400270462036133\n",
      "Step: 2828, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 1.0574698448181152\n",
      "Step: 2829, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 1.160496711730957\n",
      "Step: 2830, Loss: 0.9160538911819458, Accuracy: 1.0, Computation time: 1.0229432582855225\n",
      "Step: 2831, Loss: 0.9166313409805298, Accuracy: 1.0, Computation time: 1.0921142101287842\n",
      "Step: 2832, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 0.8969118595123291\n",
      "Step: 2833, Loss: 0.9280277490615845, Accuracy: 0.9833333492279053, Computation time: 1.3183057308197021\n",
      "Step: 2834, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 1.1855747699737549\n",
      "Step: 2835, Loss: 0.9159082770347595, Accuracy: 1.0, Computation time: 0.8286209106445312\n",
      "Step: 2836, Loss: 0.9160168170928955, Accuracy: 1.0, Computation time: 1.0649492740631104\n",
      "Step: 2837, Loss: 0.917302131652832, Accuracy: 1.0, Computation time: 1.4062294960021973\n",
      "Step: 2838, Loss: 0.9347077012062073, Accuracy: 0.9642857313156128, Computation time: 1.1933321952819824\n",
      "Step: 2839, Loss: 0.915990948677063, Accuracy: 1.0, Computation time: 1.0263607501983643\n",
      "Step: 2840, Loss: 0.9188212156295776, Accuracy: 1.0, Computation time: 0.9695620536804199\n",
      "Step: 2841, Loss: 0.9161216616630554, Accuracy: 1.0, Computation time: 1.2146580219268799\n",
      "Step: 2842, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 0.9676334857940674\n",
      "Step: 2843, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.1848065853118896\n",
      "Step: 2844, Loss: 0.9161940217018127, Accuracy: 1.0, Computation time: 1.092797040939331\n",
      "Step: 2845, Loss: 0.9375693798065186, Accuracy: 0.9821428656578064, Computation time: 0.9913365840911865\n",
      "Step: 2846, Loss: 0.9160568714141846, Accuracy: 1.0, Computation time: 0.9437887668609619\n",
      "Step: 2847, Loss: 0.9225761294364929, Accuracy: 1.0, Computation time: 1.5128717422485352\n",
      "Step: 2848, Loss: 0.9375705122947693, Accuracy: 0.949999988079071, Computation time: 1.0611557960510254\n",
      "Step: 2849, Loss: 0.9163258671760559, Accuracy: 1.0, Computation time: 1.5040485858917236\n",
      "Step: 2850, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 1.0047924518585205\n",
      "Step: 2851, Loss: 0.9159439206123352, Accuracy: 1.0, Computation time: 1.4339373111724854\n",
      "Step: 2852, Loss: 0.9159645438194275, Accuracy: 1.0, Computation time: 1.0425879955291748\n",
      "Step: 2853, Loss: 0.915979266166687, Accuracy: 1.0, Computation time: 0.9045891761779785\n",
      "Step: 2854, Loss: 0.9159575700759888, Accuracy: 1.0, Computation time: 0.8639862537384033\n",
      "Step: 2855, Loss: 0.9159732460975647, Accuracy: 1.0, Computation time: 0.9154307842254639\n",
      "Step: 2856, Loss: 0.9375450015068054, Accuracy: 0.9821428656578064, Computation time: 1.1440060138702393\n",
      "Step: 2857, Loss: 0.9376083016395569, Accuracy: 0.9642857313156128, Computation time: 0.9836833477020264\n",
      "Step: 2858, Loss: 0.9164748191833496, Accuracy: 1.0, Computation time: 1.122734546661377\n",
      "Step: 2859, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.2901616096496582\n",
      "Step: 2860, Loss: 0.9376273155212402, Accuracy: 0.9722222089767456, Computation time: 1.1050541400909424\n",
      "Step: 2861, Loss: 0.9160606861114502, Accuracy: 1.0, Computation time: 0.9943466186523438\n",
      "Step: 2862, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 0.8452870845794678\n",
      "Step: 2863, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 1.0325617790222168\n",
      "Step: 2864, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 0.9160492420196533\n",
      "Step: 2865, Loss: 0.9160124063491821, Accuracy: 1.0, Computation time: 0.9859912395477295\n",
      "Step: 2866, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 1.0749313831329346\n",
      "Step: 2867, Loss: 0.9377317428588867, Accuracy: 0.96875, Computation time: 0.9821038246154785\n",
      "Step: 2868, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 1.0405523777008057\n",
      "Step: 2869, Loss: 0.9302576184272766, Accuracy: 0.96875, Computation time: 1.157883644104004\n",
      "Step: 2870, Loss: 0.9160453677177429, Accuracy: 1.0, Computation time: 0.9926512241363525\n",
      "Step: 2871, Loss: 0.9377465844154358, Accuracy: 0.9722222089767456, Computation time: 0.8756606578826904\n",
      "Step: 2872, Loss: 0.9375835657119751, Accuracy: 0.9772727489471436, Computation time: 1.127755880355835\n",
      "Step: 2873, Loss: 0.9159427285194397, Accuracy: 1.0, Computation time: 0.9064755439758301\n",
      "Step: 2874, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 0.7860233783721924\n",
      "Step: 2875, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 0.7813329696655273\n",
      "Step: 2876, Loss: 0.9200658798217773, Accuracy: 1.0, Computation time: 0.9059996604919434\n",
      "Step: 2877, Loss: 0.9161231517791748, Accuracy: 1.0, Computation time: 0.8298697471618652\n",
      "Step: 2878, Loss: 0.9159663319587708, Accuracy: 1.0, Computation time: 0.9369583129882812\n",
      "Step: 2879, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 0.865140438079834\n",
      "Step: 2880, Loss: 0.9159150719642639, Accuracy: 1.0, Computation time: 1.0684056282043457\n",
      "Step: 2881, Loss: 0.915984034538269, Accuracy: 1.0, Computation time: 1.2013185024261475\n",
      "Step: 2882, Loss: 0.9159656167030334, Accuracy: 1.0, Computation time: 0.9192109107971191\n",
      "Step: 2883, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 0.9715745449066162\n",
      "Step: 2884, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 0.9364378452301025\n",
      "Step: 2885, Loss: 0.937520444393158, Accuracy: 0.9722222089767456, Computation time: 0.9043669700622559\n",
      "Step: 2886, Loss: 0.9164046049118042, Accuracy: 1.0, Computation time: 1.1484816074371338\n",
      "Step: 2887, Loss: 0.9158956408500671, Accuracy: 1.0, Computation time: 1.1439995765686035\n",
      "Step: 2888, Loss: 0.9158897399902344, Accuracy: 1.0, Computation time: 1.1162536144256592\n",
      "Step: 2889, Loss: 0.9376710057258606, Accuracy: 0.9642857313156128, Computation time: 0.9372451305389404\n",
      "Step: 2890, Loss: 0.9163995981216431, Accuracy: 1.0, Computation time: 0.9147136211395264\n",
      "Step: 2891, Loss: 0.9160773158073425, Accuracy: 1.0, Computation time: 1.3387386798858643\n",
      "Step: 2892, Loss: 0.9374908208847046, Accuracy: 0.9722222089767456, Computation time: 0.8926515579223633\n",
      "Step: 2893, Loss: 0.9158824682235718, Accuracy: 1.0, Computation time: 1.0919404029846191\n",
      "Step: 2894, Loss: 0.9159359335899353, Accuracy: 1.0, Computation time: 1.4512174129486084\n",
      "Step: 2895, Loss: 0.9275175333023071, Accuracy: 0.9852941036224365, Computation time: 1.0411357879638672\n",
      "Step: 2896, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 0.8545536994934082\n",
      "Step: 2897, Loss: 0.9159445762634277, Accuracy: 1.0, Computation time: 0.9815480709075928\n",
      "Step: 2898, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 0.8428430557250977\n",
      "Step: 2899, Loss: 0.9158979058265686, Accuracy: 1.0, Computation time: 1.078402042388916\n",
      "Step: 2900, Loss: 0.9161193370819092, Accuracy: 1.0, Computation time: 0.826153039932251\n",
      "Step: 2901, Loss: 0.9192491769790649, Accuracy: 1.0, Computation time: 1.8821301460266113\n",
      "Step: 2902, Loss: 0.9592904448509216, Accuracy: 0.9444444179534912, Computation time: 0.8423981666564941\n",
      "Step: 2903, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 0.8771083354949951\n",
      "Step: 2904, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 0.8106229305267334\n",
      "Step: 2905, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.1747937202453613\n",
      "Step: 2906, Loss: 0.9158780574798584, Accuracy: 1.0, Computation time: 1.3626716136932373\n",
      "Step: 2907, Loss: 0.9159624576568604, Accuracy: 1.0, Computation time: 1.031815767288208\n",
      "Step: 2908, Loss: 0.9184466004371643, Accuracy: 1.0, Computation time: 1.5610265731811523\n",
      "Step: 2909, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 0.9969120025634766\n",
      "Step: 2910, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 0.8938639163970947\n",
      "Step: 2911, Loss: 0.915995180606842, Accuracy: 1.0, Computation time: 1.046971321105957\n",
      "Step: 2912, Loss: 0.9160661697387695, Accuracy: 1.0, Computation time: 0.9090347290039062\n",
      "Step: 2913, Loss: 0.9187158346176147, Accuracy: 1.0, Computation time: 1.2321665287017822\n",
      "Step: 2914, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 1.1008143424987793\n",
      "Step: 2915, Loss: 0.9391632080078125, Accuracy: 0.96875, Computation time: 1.2354040145874023\n",
      "Step: 2916, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 1.251521110534668\n",
      "Step: 2917, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 0.9982867240905762\n",
      "Step: 2918, Loss: 0.9161292314529419, Accuracy: 1.0, Computation time: 1.0895893573760986\n",
      "########################\n",
      "Test loss: 1.1308786869049072, Test Accuracy_epoch21: 0.679286003112793\n",
      "########################\n",
      "Step: 2919, Loss: 0.9376407265663147, Accuracy: 0.9642857313156128, Computation time: 1.0331213474273682\n",
      "Step: 2920, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 1.1376426219940186\n",
      "Step: 2921, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 1.0727152824401855\n",
      "Step: 2922, Loss: 0.9159074425697327, Accuracy: 1.0, Computation time: 0.9549431800842285\n",
      "Step: 2923, Loss: 0.9158915281295776, Accuracy: 1.0, Computation time: 1.0968594551086426\n",
      "Step: 2924, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.4008138179779053\n",
      "Step: 2925, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 1.3474843502044678\n",
      "Step: 2926, Loss: 0.9163101315498352, Accuracy: 1.0, Computation time: 1.169905424118042\n",
      "Step: 2927, Loss: 0.9158505797386169, Accuracy: 1.0, Computation time: 1.0032174587249756\n",
      "Step: 2928, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.09977126121521\n",
      "Step: 2929, Loss: 0.9375606775283813, Accuracy: 0.9807692766189575, Computation time: 1.1194953918457031\n",
      "Step: 2930, Loss: 0.9160574078559875, Accuracy: 1.0, Computation time: 1.22727370262146\n",
      "Step: 2931, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.0163648128509521\n",
      "Step: 2932, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.1562168598175049\n",
      "Step: 2933, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.0015671253204346\n",
      "Step: 2934, Loss: 0.9331667423248291, Accuracy: 0.9722222089767456, Computation time: 1.1206185817718506\n",
      "Step: 2935, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 0.9732637405395508\n",
      "Step: 2936, Loss: 0.9160184264183044, Accuracy: 1.0, Computation time: 0.9392027854919434\n",
      "Step: 2937, Loss: 0.9159843325614929, Accuracy: 1.0, Computation time: 1.3798625469207764\n",
      "Step: 2938, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.1437864303588867\n",
      "Step: 2939, Loss: 0.9159468412399292, Accuracy: 1.0, Computation time: 1.095597505569458\n",
      "Step: 2940, Loss: 0.9159549474716187, Accuracy: 1.0, Computation time: 1.148754358291626\n",
      "Step: 2941, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.8735878467559814\n",
      "Step: 2942, Loss: 0.9160410165786743, Accuracy: 1.0, Computation time: 1.0096030235290527\n",
      "Step: 2943, Loss: 0.9161799550056458, Accuracy: 1.0, Computation time: 1.4161250591278076\n",
      "Step: 2944, Loss: 0.9376983046531677, Accuracy: 0.9722222089767456, Computation time: 1.9452381134033203\n",
      "Step: 2945, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 1.066390037536621\n",
      "Step: 2946, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.0577609539031982\n",
      "Step: 2947, Loss: 0.9320423007011414, Accuracy: 0.949999988079071, Computation time: 1.5098981857299805\n",
      "Step: 2948, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 0.8536174297332764\n",
      "Step: 2949, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 1.2318518161773682\n",
      "Step: 2950, Loss: 0.9161349534988403, Accuracy: 1.0, Computation time: 0.9993748664855957\n",
      "Step: 2951, Loss: 0.916076123714447, Accuracy: 1.0, Computation time: 1.5555713176727295\n",
      "Step: 2952, Loss: 0.9378533959388733, Accuracy: 0.9722222089767456, Computation time: 1.847470998764038\n",
      "Step: 2953, Loss: 0.936620831489563, Accuracy: 0.9772727489471436, Computation time: 0.9246141910552979\n",
      "Step: 2954, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 0.8679277896881104\n",
      "Step: 2955, Loss: 0.9159187078475952, Accuracy: 1.0, Computation time: 1.0306580066680908\n",
      "Step: 2956, Loss: 0.9444530010223389, Accuracy: 0.9772727489471436, Computation time: 1.161421537399292\n",
      "Step: 2957, Loss: 0.9376641511917114, Accuracy: 0.9750000238418579, Computation time: 0.9958803653717041\n",
      "Step: 2958, Loss: 0.9378352165222168, Accuracy: 0.9772727489471436, Computation time: 1.2429430484771729\n",
      "Step: 2959, Loss: 0.9175355434417725, Accuracy: 1.0, Computation time: 1.2319083213806152\n",
      "Step: 2960, Loss: 0.9162907004356384, Accuracy: 1.0, Computation time: 1.4582254886627197\n",
      "Step: 2961, Loss: 0.9160162210464478, Accuracy: 1.0, Computation time: 1.259355068206787\n",
      "Step: 2962, Loss: 0.9370326995849609, Accuracy: 0.9833333492279053, Computation time: 1.0090184211730957\n",
      "Step: 2963, Loss: 0.916012704372406, Accuracy: 1.0, Computation time: 0.8048005104064941\n",
      "Step: 2964, Loss: 0.9159537553787231, Accuracy: 1.0, Computation time: 0.7936594486236572\n",
      "Step: 2965, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 0.8315839767456055\n",
      "Step: 2966, Loss: 0.9161067008972168, Accuracy: nan, Computation time: 0.8801472187042236\n",
      "Step: 2967, Loss: 0.9321288466453552, Accuracy: 0.9722222089767456, Computation time: 1.0409388542175293\n",
      "Step: 2968, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 0.8423507213592529\n",
      "Step: 2969, Loss: 0.9160870909690857, Accuracy: 1.0, Computation time: 0.8865079879760742\n",
      "Step: 2970, Loss: 0.9162505269050598, Accuracy: 1.0, Computation time: 0.999037504196167\n",
      "Step: 2971, Loss: 0.9590352773666382, Accuracy: 0.9434524178504944, Computation time: 1.2783281803131104\n",
      "Step: 2972, Loss: 0.9159532189369202, Accuracy: 1.0, Computation time: 0.8268673419952393\n",
      "Step: 2973, Loss: 0.9159591794013977, Accuracy: 1.0, Computation time: 1.036200761795044\n",
      "Step: 2974, Loss: 0.9159401059150696, Accuracy: 1.0, Computation time: 0.8383548259735107\n",
      "Step: 2975, Loss: 0.9376928806304932, Accuracy: 0.9642857313156128, Computation time: 0.8963649272918701\n",
      "Step: 2976, Loss: 0.9159595370292664, Accuracy: 1.0, Computation time: 0.8697934150695801\n",
      "Step: 2977, Loss: 0.9159110188484192, Accuracy: 1.0, Computation time: 0.9507091045379639\n",
      "Step: 2978, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 0.8179099559783936\n",
      "Step: 2979, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 0.8647091388702393\n",
      "Step: 2980, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 0.9880940914154053\n",
      "Step: 2981, Loss: 0.9165226221084595, Accuracy: 1.0, Computation time: 1.1204328536987305\n",
      "Step: 2982, Loss: 0.9360713362693787, Accuracy: 0.9772727489471436, Computation time: 0.97747802734375\n",
      "Step: 2983, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 0.8439526557922363\n",
      "Step: 2984, Loss: 0.9160053730010986, Accuracy: 1.0, Computation time: 0.9341428279876709\n",
      "Step: 2985, Loss: 0.9181026816368103, Accuracy: 1.0, Computation time: 0.9790446758270264\n",
      "Step: 2986, Loss: 0.9375695586204529, Accuracy: 0.9772727489471436, Computation time: 0.8804781436920166\n",
      "Step: 2987, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 0.9505143165588379\n",
      "Step: 2988, Loss: 0.9158992171287537, Accuracy: 1.0, Computation time: 0.750525951385498\n",
      "Step: 2989, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 0.7982523441314697\n",
      "Step: 2990, Loss: 0.9159775972366333, Accuracy: 1.0, Computation time: 0.9695158004760742\n",
      "Step: 2991, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 0.9317514896392822\n",
      "Step: 2992, Loss: 0.9159687757492065, Accuracy: 1.0, Computation time: 0.9746541976928711\n",
      "Step: 2993, Loss: 0.9162161350250244, Accuracy: 1.0, Computation time: 1.1201798915863037\n",
      "Step: 2994, Loss: 0.937659740447998, Accuracy: 0.9583333730697632, Computation time: 1.0694248676300049\n",
      "Step: 2995, Loss: 0.9208218455314636, Accuracy: 1.0, Computation time: 1.1971704959869385\n",
      "Step: 2996, Loss: 0.9201404452323914, Accuracy: 1.0, Computation time: 0.9397239685058594\n",
      "Step: 2997, Loss: 0.915973424911499, Accuracy: 1.0, Computation time: 0.9835309982299805\n",
      "Step: 2998, Loss: 0.9376979470252991, Accuracy: 0.9722222089767456, Computation time: 0.9638617038726807\n",
      "Step: 2999, Loss: 0.9233057498931885, Accuracy: 1.0, Computation time: 1.2168817520141602\n",
      "Step: 3000, Loss: 0.9160860180854797, Accuracy: 1.0, Computation time: 1.0005202293395996\n",
      "Step: 3001, Loss: 0.9398522973060608, Accuracy: 0.96875, Computation time: 1.077547550201416\n",
      "Step: 3002, Loss: 0.9160690903663635, Accuracy: 1.0, Computation time: 0.8413906097412109\n",
      "Step: 3003, Loss: 0.9160676598548889, Accuracy: 1.0, Computation time: 1.1272130012512207\n",
      "Step: 3004, Loss: 0.9159947633743286, Accuracy: 1.0, Computation time: 0.9369544982910156\n",
      "Step: 3005, Loss: 0.9159678220748901, Accuracy: 1.0, Computation time: 1.0386888980865479\n",
      "Step: 3006, Loss: 0.9214602112770081, Accuracy: 1.0, Computation time: 0.9655883312225342\n",
      "Step: 3007, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 2.060271739959717\n",
      "Step: 3008, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 0.9321143627166748\n",
      "Step: 3009, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 0.7756373882293701\n",
      "Step: 3010, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.030402421951294\n",
      "Step: 3011, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 1.521087408065796\n",
      "Step: 3012, Loss: 0.9158914685249329, Accuracy: 1.0, Computation time: 0.9046072959899902\n",
      "Step: 3013, Loss: 0.9585720300674438, Accuracy: 0.9522727727890015, Computation time: 1.1576688289642334\n",
      "Step: 3014, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 0.8988721370697021\n",
      "Step: 3015, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 0.9988510608673096\n",
      "Step: 3016, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 1.3920361995697021\n",
      "Step: 3017, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 0.874178409576416\n",
      "Step: 3018, Loss: 0.9172444343566895, Accuracy: 1.0, Computation time: 1.0347826480865479\n",
      "Step: 3019, Loss: 0.9159305691719055, Accuracy: 1.0, Computation time: 0.8154680728912354\n",
      "Step: 3020, Loss: 0.9166549444198608, Accuracy: 1.0, Computation time: 1.1137206554412842\n",
      "Step: 3021, Loss: 0.9180098176002502, Accuracy: 1.0, Computation time: 0.9376242160797119\n",
      "Step: 3022, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 0.8772292137145996\n",
      "Step: 3023, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 0.8067529201507568\n",
      "Step: 3024, Loss: 0.937255322933197, Accuracy: 0.96875, Computation time: 0.8771100044250488\n",
      "Step: 3025, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 0.9796693325042725\n",
      "Step: 3026, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 0.9041891098022461\n",
      "Step: 3027, Loss: 0.9162564277648926, Accuracy: 1.0, Computation time: 0.9698135852813721\n",
      "Step: 3028, Loss: 0.9167727828025818, Accuracy: 1.0, Computation time: 0.9362967014312744\n",
      "Step: 3029, Loss: 0.9160032272338867, Accuracy: 1.0, Computation time: 0.9712398052215576\n",
      "Step: 3030, Loss: 0.9158760905265808, Accuracy: 1.0, Computation time: 0.8859565258026123\n",
      "Step: 3031, Loss: 0.958723783493042, Accuracy: 0.9166666865348816, Computation time: 0.8608357906341553\n",
      "Step: 3032, Loss: 0.9159572720527649, Accuracy: 1.0, Computation time: 0.8095083236694336\n",
      "Step: 3033, Loss: 0.9159370064735413, Accuracy: 1.0, Computation time: 0.8764264583587646\n",
      "Step: 3034, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 0.97884202003479\n",
      "Step: 3035, Loss: 0.937583863735199, Accuracy: 0.9583333730697632, Computation time: 1.0831313133239746\n",
      "Step: 3036, Loss: 0.9378654360771179, Accuracy: 0.9642857313156128, Computation time: 0.9193847179412842\n",
      "Step: 3037, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 1.16436767578125\n",
      "Step: 3038, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 0.9793477058410645\n",
      "Step: 3039, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 1.1640372276306152\n",
      "Step: 3040, Loss: 0.9158995151519775, Accuracy: 1.0, Computation time: 0.9407296180725098\n",
      "Step: 3041, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 1.004610300064087\n",
      "Step: 3042, Loss: 0.9168671369552612, Accuracy: 1.0, Computation time: 1.4862475395202637\n",
      "Step: 3043, Loss: 0.915873646736145, Accuracy: 1.0, Computation time: 1.3467605113983154\n",
      "Step: 3044, Loss: 0.9159443378448486, Accuracy: 1.0, Computation time: 0.9892287254333496\n",
      "Step: 3045, Loss: 0.915873110294342, Accuracy: 1.0, Computation time: 1.2463197708129883\n",
      "Step: 3046, Loss: 0.9375364184379578, Accuracy: 0.9722222089767456, Computation time: 0.8367364406585693\n",
      "Step: 3047, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 1.1585681438446045\n",
      "Step: 3048, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 0.9520137310028076\n",
      "Step: 3049, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 1.4327571392059326\n",
      "Step: 3050, Loss: 0.9375101327896118, Accuracy: 0.9750000238418579, Computation time: 1.0015666484832764\n",
      "Step: 3051, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 0.993755578994751\n",
      "Step: 3052, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 0.8923792839050293\n",
      "Step: 3053, Loss: 0.933281660079956, Accuracy: 0.9750000238418579, Computation time: 1.018988847732544\n",
      "Step: 3054, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 0.8615212440490723\n",
      "Step: 3055, Loss: 0.9158782958984375, Accuracy: 1.0, Computation time: 1.048243522644043\n",
      "Step: 3056, Loss: 0.9188104271888733, Accuracy: 1.0, Computation time: 1.093714714050293\n",
      "Step: 3057, Loss: 0.9159086346626282, Accuracy: 1.0, Computation time: 0.871314287185669\n",
      "########################\n",
      "Test loss: 1.126985788345337, Test Accuracy_epoch22: 0.6812158823013306\n",
      "########################\n",
      "Step: 3058, Loss: 0.9161750674247742, Accuracy: 1.0, Computation time: 1.0480284690856934\n",
      "Step: 3059, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 1.1031217575073242\n",
      "Step: 3060, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.9184737205505371\n",
      "Step: 3061, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 1.020097255706787\n",
      "Step: 3062, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 1.3636353015899658\n",
      "Step: 3063, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.2470002174377441\n",
      "Step: 3064, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 0.9118590354919434\n",
      "Step: 3065, Loss: 0.9160087704658508, Accuracy: 1.0, Computation time: 0.905195951461792\n",
      "Step: 3066, Loss: 0.9166997075080872, Accuracy: 1.0, Computation time: 1.1233792304992676\n",
      "Step: 3067, Loss: 0.9371576905250549, Accuracy: 0.9772727489471436, Computation time: 0.980010986328125\n",
      "Step: 3068, Loss: 0.935604453086853, Accuracy: 0.9375, Computation time: 1.0823979377746582\n",
      "Step: 3069, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 0.8576693534851074\n",
      "Step: 3070, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 0.9383559226989746\n",
      "Step: 3071, Loss: 0.9336438775062561, Accuracy: 0.9807692766189575, Computation time: 1.3212909698486328\n",
      "Step: 3072, Loss: 0.9159146547317505, Accuracy: 1.0, Computation time: 0.8522806167602539\n",
      "Step: 3073, Loss: 0.9160405397415161, Accuracy: 1.0, Computation time: 1.1248457431793213\n",
      "Step: 3074, Loss: 0.9159155488014221, Accuracy: 1.0, Computation time: 0.9083948135375977\n",
      "Step: 3075, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 0.9140944480895996\n",
      "Step: 3076, Loss: 0.937492847442627, Accuracy: 0.96875, Computation time: 0.8456788063049316\n",
      "Step: 3077, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 0.8598790168762207\n",
      "Step: 3078, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 0.8439428806304932\n",
      "Step: 3079, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 0.8513004779815674\n",
      "Step: 3080, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 0.7911028861999512\n",
      "Step: 3081, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.4086811542510986\n",
      "Step: 3082, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 0.913571834564209\n",
      "Step: 3083, Loss: 0.9158638119697571, Accuracy: 1.0, Computation time: 1.023305892944336\n",
      "Step: 3084, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.400111436843872\n",
      "Step: 3085, Loss: 0.9376053214073181, Accuracy: 0.9583333730697632, Computation time: 0.9877760410308838\n",
      "Step: 3086, Loss: 0.9158532619476318, Accuracy: 1.0, Computation time: 0.8782260417938232\n",
      "Step: 3087, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 1.0825293064117432\n",
      "Step: 3088, Loss: 0.9372169375419617, Accuracy: 0.9583333730697632, Computation time: 0.9163458347320557\n",
      "Step: 3089, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 0.9509472846984863\n",
      "Step: 3090, Loss: 0.9240627288818359, Accuracy: 1.0, Computation time: 1.0488858222961426\n",
      "Step: 3091, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 0.9157404899597168\n",
      "Step: 3092, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 1.1372404098510742\n",
      "Step: 3093, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 0.9560532569885254\n",
      "Step: 3094, Loss: 0.9166519641876221, Accuracy: 1.0, Computation time: 0.9782893657684326\n",
      "Step: 3095, Loss: 0.9392825365066528, Accuracy: 0.9791666865348816, Computation time: 1.4991066455841064\n",
      "Step: 3096, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 0.9570186138153076\n",
      "Step: 3097, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 0.9597666263580322\n",
      "Step: 3098, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 1.1188619136810303\n",
      "Step: 3099, Loss: 0.9159660935401917, Accuracy: 1.0, Computation time: 0.9122462272644043\n",
      "Step: 3100, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 1.071160078048706\n",
      "Step: 3101, Loss: 0.9349981546401978, Accuracy: 0.9642857313156128, Computation time: 1.1382689476013184\n",
      "Step: 3102, Loss: 0.9159179329872131, Accuracy: 1.0, Computation time: 0.9202699661254883\n",
      "Step: 3103, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 0.8215467929840088\n",
      "Step: 3104, Loss: 0.9375627636909485, Accuracy: 0.96875, Computation time: 1.1196107864379883\n",
      "Step: 3105, Loss: 0.9160569906234741, Accuracy: 1.0, Computation time: 1.0511488914489746\n",
      "Step: 3106, Loss: 0.9160090088844299, Accuracy: 1.0, Computation time: 0.8547561168670654\n",
      "Step: 3107, Loss: 0.9159520864486694, Accuracy: 1.0, Computation time: 1.1776785850524902\n",
      "Step: 3108, Loss: 0.9160513877868652, Accuracy: 1.0, Computation time: 1.4822816848754883\n",
      "Step: 3109, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 1.0339999198913574\n",
      "Step: 3110, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 0.905663013458252\n",
      "Step: 3111, Loss: 0.9495457410812378, Accuracy: 0.925000011920929, Computation time: 1.0948359966278076\n",
      "Step: 3112, Loss: 0.9159389138221741, Accuracy: 1.0, Computation time: 0.9521183967590332\n",
      "Step: 3113, Loss: 0.9159175157546997, Accuracy: 1.0, Computation time: 1.0362420082092285\n",
      "Step: 3114, Loss: 0.9160143136978149, Accuracy: 1.0, Computation time: 1.0407984256744385\n",
      "Step: 3115, Loss: 0.9160625338554382, Accuracy: 1.0, Computation time: 0.9852385520935059\n",
      "Step: 3116, Loss: 0.9160104393959045, Accuracy: 1.0, Computation time: 1.235283613204956\n",
      "Step: 3117, Loss: 0.9159908294677734, Accuracy: 1.0, Computation time: 0.9068446159362793\n",
      "Step: 3118, Loss: 0.9159900546073914, Accuracy: 1.0, Computation time: 1.3482871055603027\n",
      "Step: 3119, Loss: 0.9159501791000366, Accuracy: 1.0, Computation time: 1.1656606197357178\n",
      "Step: 3120, Loss: 0.9160214066505432, Accuracy: 1.0, Computation time: 0.963064432144165\n",
      "Step: 3121, Loss: 0.9374900460243225, Accuracy: 0.9722222089767456, Computation time: 1.1192870140075684\n",
      "Step: 3122, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 0.8497977256774902\n",
      "Step: 3123, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.276014804840088\n",
      "Step: 3124, Loss: 0.9279846549034119, Accuracy: 0.9722222089767456, Computation time: 1.1036436557769775\n",
      "Step: 3125, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 0.9162514209747314\n",
      "Step: 3126, Loss: 0.9159412980079651, Accuracy: 1.0, Computation time: 0.9009194374084473\n",
      "Step: 3127, Loss: 0.9159337282180786, Accuracy: 1.0, Computation time: 0.9456512928009033\n",
      "Step: 3128, Loss: 0.9159607291221619, Accuracy: 1.0, Computation time: 1.2031426429748535\n",
      "Step: 3129, Loss: 0.9365942478179932, Accuracy: 0.9791666865348816, Computation time: 1.2249054908752441\n",
      "Step: 3130, Loss: 0.91591477394104, Accuracy: 1.0, Computation time: 0.8571536540985107\n",
      "Step: 3131, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 0.9488534927368164\n",
      "Step: 3132, Loss: 0.9285219311714172, Accuracy: 0.949999988079071, Computation time: 1.0680351257324219\n",
      "Step: 3133, Loss: 0.9266191124916077, Accuracy: 0.9722222089767456, Computation time: 0.8306083679199219\n",
      "Step: 3134, Loss: 0.9159843921661377, Accuracy: 1.0, Computation time: 1.002777338027954\n",
      "Step: 3135, Loss: 0.9160284996032715, Accuracy: 1.0, Computation time: 0.9604063034057617\n",
      "Step: 3136, Loss: 0.9160063862800598, Accuracy: 1.0, Computation time: 0.8522918224334717\n",
      "Step: 3137, Loss: 0.9403723478317261, Accuracy: 0.9807692766189575, Computation time: 1.4587969779968262\n",
      "Step: 3138, Loss: 0.9589762687683105, Accuracy: 0.9285714626312256, Computation time: 0.952735424041748\n",
      "Step: 3139, Loss: 0.9159385561943054, Accuracy: 1.0, Computation time: 0.8330085277557373\n",
      "Step: 3140, Loss: 0.9159302711486816, Accuracy: 1.0, Computation time: 0.8478395938873291\n",
      "Step: 3141, Loss: 0.9162293076515198, Accuracy: 1.0, Computation time: 1.0465750694274902\n",
      "Step: 3142, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 0.9395079612731934\n",
      "Step: 3143, Loss: 0.940039873123169, Accuracy: 0.9791666865348816, Computation time: 0.8670907020568848\n",
      "Step: 3144, Loss: 0.91604083776474, Accuracy: 1.0, Computation time: 1.2028045654296875\n",
      "Step: 3145, Loss: 0.937358558177948, Accuracy: 0.96875, Computation time: 0.7556807994842529\n",
      "Step: 3146, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 1.1695120334625244\n",
      "Step: 3147, Loss: 0.9159358739852905, Accuracy: 1.0, Computation time: 0.796558141708374\n",
      "Step: 3148, Loss: 0.9420633912086487, Accuracy: 0.9722222089767456, Computation time: 0.9706704616546631\n",
      "Step: 3149, Loss: 0.9359912276268005, Accuracy: 0.9791666865348816, Computation time: 1.112281084060669\n",
      "Step: 3150, Loss: 0.9342995882034302, Accuracy: 0.9722222089767456, Computation time: 0.9243814945220947\n",
      "Step: 3151, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 0.904144287109375\n",
      "Step: 3152, Loss: 0.9160189628601074, Accuracy: 1.0, Computation time: 0.9330153465270996\n",
      "Step: 3153, Loss: 0.9205781817436218, Accuracy: 1.0, Computation time: 1.5247666835784912\n",
      "Step: 3154, Loss: 0.9161194562911987, Accuracy: 1.0, Computation time: 0.8174676895141602\n",
      "Step: 3155, Loss: 0.9160517454147339, Accuracy: 1.0, Computation time: 0.8712372779846191\n",
      "Step: 3156, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 0.9453504085540771\n",
      "Step: 3157, Loss: 0.9360650777816772, Accuracy: 0.9375, Computation time: 0.9336230754852295\n",
      "Step: 3158, Loss: 0.9171555042266846, Accuracy: 1.0, Computation time: 0.9547364711761475\n",
      "Step: 3159, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.2221283912658691\n",
      "Step: 3160, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 1.008802890777588\n",
      "Step: 3161, Loss: 0.9160293340682983, Accuracy: 1.0, Computation time: 0.8840281963348389\n",
      "Step: 3162, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 0.9943640232086182\n",
      "Step: 3163, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 0.8046591281890869\n",
      "Step: 3164, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 1.0578129291534424\n",
      "Step: 3165, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 0.8559439182281494\n",
      "Step: 3166, Loss: 0.9159136414527893, Accuracy: 1.0, Computation time: 1.0170159339904785\n",
      "Step: 3167, Loss: 0.9158763289451599, Accuracy: 1.0, Computation time: 0.8863472938537598\n",
      "Step: 3168, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 0.8467140197753906\n",
      "Step: 3169, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 1.1274406909942627\n",
      "Step: 3170, Loss: 0.9158726930618286, Accuracy: 1.0, Computation time: 0.8457481861114502\n",
      "Step: 3171, Loss: 0.9159013032913208, Accuracy: 1.0, Computation time: 0.9567694664001465\n",
      "Step: 3172, Loss: 0.9158906936645508, Accuracy: 1.0, Computation time: 0.842993974685669\n",
      "Step: 3173, Loss: 0.9159420728683472, Accuracy: 1.0, Computation time: 0.8837051391601562\n",
      "Step: 3174, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 0.9747364521026611\n",
      "Step: 3175, Loss: 0.9375542402267456, Accuracy: 0.9821428656578064, Computation time: 0.8048262596130371\n",
      "Step: 3176, Loss: 0.9161297082901001, Accuracy: 1.0, Computation time: 0.9973742961883545\n",
      "Step: 3177, Loss: 0.9159168601036072, Accuracy: 1.0, Computation time: 1.005122423171997\n",
      "Step: 3178, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 1.1639823913574219\n",
      "Step: 3179, Loss: 0.9158604741096497, Accuracy: 1.0, Computation time: 0.9597766399383545\n",
      "Step: 3180, Loss: 0.9176616668701172, Accuracy: 1.0, Computation time: 0.9158329963684082\n",
      "Step: 3181, Loss: 0.9375626444816589, Accuracy: 0.9772727489471436, Computation time: 0.9108078479766846\n",
      "Step: 3182, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 0.8776249885559082\n",
      "Step: 3183, Loss: 0.9163110852241516, Accuracy: 1.0, Computation time: 1.004863977432251\n",
      "Step: 3184, Loss: 0.9375988841056824, Accuracy: 0.9791666865348816, Computation time: 0.909231424331665\n",
      "Step: 3185, Loss: 0.9374978542327881, Accuracy: 0.9722222089767456, Computation time: 0.9182620048522949\n",
      "Step: 3186, Loss: 0.9159039855003357, Accuracy: 1.0, Computation time: 0.8989834785461426\n",
      "Step: 3187, Loss: 0.9162425994873047, Accuracy: 1.0, Computation time: 1.240468978881836\n",
      "Step: 3188, Loss: 0.9296404123306274, Accuracy: 0.9791666865348816, Computation time: 1.460174322128296\n",
      "Step: 3189, Loss: 0.916554868221283, Accuracy: 1.0, Computation time: 0.8267276287078857\n",
      "Step: 3190, Loss: 0.9160988330841064, Accuracy: 1.0, Computation time: 0.9800691604614258\n",
      "Step: 3191, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.056359052658081\n",
      "Step: 3192, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 0.9593343734741211\n",
      "Step: 3193, Loss: 0.9158748984336853, Accuracy: 1.0, Computation time: 1.1051254272460938\n",
      "Step: 3194, Loss: 0.9159799218177795, Accuracy: 1.0, Computation time: 0.7522773742675781\n",
      "Step: 3195, Loss: 0.9374760985374451, Accuracy: 0.9750000238418579, Computation time: 0.9512031078338623\n",
      "Step: 3196, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.2099888324737549\n",
      "########################\n",
      "Test loss: 1.1290366649627686, Test Accuracy_epoch23: 0.6785195469856262\n",
      "########################\n",
      "Step: 3197, Loss: 0.9158963561058044, Accuracy: 1.0, Computation time: 0.9642276763916016\n",
      "Step: 3198, Loss: 0.9325053095817566, Accuracy: 0.9375, Computation time: 0.967160701751709\n",
      "Step: 3199, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 1.184741735458374\n",
      "Step: 3200, Loss: 0.9374727606773376, Accuracy: 0.9807692766189575, Computation time: 1.07542085647583\n",
      "Step: 3201, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 0.8758337497711182\n",
      "Step: 3202, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 0.9264190196990967\n",
      "Step: 3203, Loss: 0.9375249147415161, Accuracy: 0.9833333492279053, Computation time: 0.8256309032440186\n",
      "Step: 3204, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 0.7721548080444336\n",
      "Step: 3205, Loss: 0.9165277481079102, Accuracy: 1.0, Computation time: 1.1672122478485107\n",
      "Step: 3206, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 0.8958835601806641\n",
      "Step: 3207, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 1.0055065155029297\n",
      "Step: 3208, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.2098538875579834\n",
      "Step: 3209, Loss: 0.915932834148407, Accuracy: 1.0, Computation time: 1.0308809280395508\n",
      "Step: 3210, Loss: 0.9162297248840332, Accuracy: 1.0, Computation time: 1.4830515384674072\n",
      "Step: 3211, Loss: 0.9375737309455872, Accuracy: 0.96875, Computation time: 0.846381425857544\n",
      "Step: 3212, Loss: 0.9471129179000854, Accuracy: 0.90625, Computation time: 1.7287473678588867\n",
      "Step: 3213, Loss: 0.9188613295555115, Accuracy: 1.0, Computation time: 1.022068977355957\n",
      "Step: 3214, Loss: 0.9159114956855774, Accuracy: 1.0, Computation time: 0.9786057472229004\n",
      "Step: 3215, Loss: 0.9160988330841064, Accuracy: 1.0, Computation time: 0.8317220211029053\n",
      "Step: 3216, Loss: 0.9159436225891113, Accuracy: 1.0, Computation time: 1.0072283744812012\n",
      "Step: 3217, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 0.9989719390869141\n",
      "Step: 3218, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 0.8689999580383301\n",
      "Step: 3219, Loss: 0.9161995649337769, Accuracy: 1.0, Computation time: 1.0982346534729004\n",
      "Step: 3220, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 0.9662661552429199\n",
      "Step: 3221, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 0.7554581165313721\n",
      "Step: 3222, Loss: 0.9375924468040466, Accuracy: 0.9750000238418579, Computation time: 0.9442625045776367\n",
      "Step: 3223, Loss: 0.9374227523803711, Accuracy: 0.9807692766189575, Computation time: 0.9333951473236084\n",
      "Step: 3224, Loss: 0.9158840775489807, Accuracy: 1.0, Computation time: 1.4203159809112549\n",
      "Step: 3225, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 0.900634765625\n",
      "Step: 3226, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 0.9907236099243164\n",
      "Step: 3227, Loss: 0.9159728288650513, Accuracy: 1.0, Computation time: 1.2881090641021729\n",
      "Step: 3228, Loss: 0.9187638163566589, Accuracy: 1.0, Computation time: 1.5347487926483154\n",
      "Step: 3229, Loss: 0.9172623157501221, Accuracy: 1.0, Computation time: 1.0522234439849854\n",
      "Step: 3230, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 0.9072849750518799\n",
      "Step: 3231, Loss: 0.9162254929542542, Accuracy: 1.0, Computation time: 1.0345914363861084\n",
      "Step: 3232, Loss: 0.9363384246826172, Accuracy: 0.9791666865348816, Computation time: 1.1726512908935547\n",
      "Step: 3233, Loss: 0.9159382581710815, Accuracy: 1.0, Computation time: 0.9080080986022949\n",
      "Step: 3234, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 1.0707240104675293\n",
      "Step: 3235, Loss: 0.9158982038497925, Accuracy: 1.0, Computation time: 0.9655003547668457\n",
      "Step: 3236, Loss: 0.9579569697380066, Accuracy: 0.9495798349380493, Computation time: 0.9196507930755615\n",
      "Step: 3237, Loss: 0.9376217722892761, Accuracy: 0.9791666865348816, Computation time: 1.322443962097168\n",
      "Step: 3238, Loss: 0.9158868193626404, Accuracy: 1.0, Computation time: 1.0356695652008057\n",
      "Step: 3239, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 0.9969229698181152\n",
      "Step: 3240, Loss: 0.9375641942024231, Accuracy: 0.9722222089767456, Computation time: 0.9464664459228516\n",
      "Step: 3241, Loss: 0.9160053133964539, Accuracy: 1.0, Computation time: 1.0573878288269043\n",
      "Step: 3242, Loss: 0.9160029292106628, Accuracy: 1.0, Computation time: 0.7936673164367676\n",
      "Step: 3243, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 1.355062484741211\n",
      "Step: 3244, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 0.996525764465332\n",
      "Step: 3245, Loss: 0.9158697724342346, Accuracy: 1.0, Computation time: 1.3385107517242432\n",
      "Step: 3246, Loss: 0.9374969601631165, Accuracy: 0.9750000238418579, Computation time: 1.072721242904663\n",
      "Step: 3247, Loss: 0.9374176263809204, Accuracy: 0.9791666865348816, Computation time: 0.9000816345214844\n",
      "Step: 3248, Loss: 0.9159523248672485, Accuracy: 1.0, Computation time: 0.8927652835845947\n",
      "Step: 3249, Loss: 0.9159681797027588, Accuracy: 1.0, Computation time: 0.9578821659088135\n",
      "Step: 3250, Loss: 0.9159554243087769, Accuracy: 1.0, Computation time: 0.9691805839538574\n",
      "Step: 3251, Loss: 0.9159625172615051, Accuracy: 1.0, Computation time: 0.9367516040802002\n",
      "Step: 3252, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.0274689197540283\n",
      "Step: 3253, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 0.8837461471557617\n",
      "Step: 3254, Loss: 0.9372007250785828, Accuracy: 0.949999988079071, Computation time: 0.8481001853942871\n",
      "Step: 3255, Loss: 0.9160617589950562, Accuracy: 1.0, Computation time: 1.25553297996521\n",
      "Step: 3256, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 1.0142993927001953\n",
      "Step: 3257, Loss: 0.9158711433410645, Accuracy: 1.0, Computation time: 1.0140552520751953\n",
      "Step: 3258, Loss: 0.915911853313446, Accuracy: 1.0, Computation time: 0.8325974941253662\n",
      "Step: 3259, Loss: 0.9160200357437134, Accuracy: 1.0, Computation time: 1.0748093128204346\n",
      "Step: 3260, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 0.8069736957550049\n",
      "Step: 3261, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 0.8935437202453613\n",
      "Step: 3262, Loss: 0.9159355163574219, Accuracy: 1.0, Computation time: 0.9387807846069336\n",
      "Step: 3263, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 0.8779418468475342\n",
      "Step: 3264, Loss: 0.9158596992492676, Accuracy: 1.0, Computation time: 1.180833101272583\n",
      "Step: 3265, Loss: 0.9158589839935303, Accuracy: 1.0, Computation time: 0.7902495861053467\n",
      "Step: 3266, Loss: 0.9159919023513794, Accuracy: 1.0, Computation time: 0.827122688293457\n",
      "Step: 3267, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 0.9049344062805176\n",
      "Step: 3268, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 0.8544292449951172\n",
      "Step: 3269, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 0.8832995891571045\n",
      "Step: 3270, Loss: 0.9235668778419495, Accuracy: 1.0, Computation time: 1.8975660800933838\n",
      "Step: 3271, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 0.8180668354034424\n",
      "Step: 3272, Loss: 0.9256250262260437, Accuracy: 0.9722222089767456, Computation time: 1.0036814212799072\n",
      "Step: 3273, Loss: 0.9161055684089661, Accuracy: 1.0, Computation time: 0.82639479637146\n",
      "Step: 3274, Loss: 0.9161399602890015, Accuracy: 1.0, Computation time: 0.8528547286987305\n",
      "Step: 3275, Loss: 0.9375448822975159, Accuracy: 0.9750000238418579, Computation time: 0.927034854888916\n",
      "Step: 3276, Loss: 0.9160410761833191, Accuracy: 1.0, Computation time: 0.8873729705810547\n",
      "Step: 3277, Loss: 0.9570963978767395, Accuracy: 0.9375, Computation time: 1.0073766708374023\n",
      "Step: 3278, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 0.7905380725860596\n",
      "Step: 3279, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 0.8374218940734863\n",
      "Step: 3280, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 1.1210863590240479\n",
      "Step: 3281, Loss: 0.9160450100898743, Accuracy: 1.0, Computation time: 1.0014293193817139\n",
      "Step: 3282, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.0071992874145508\n",
      "Step: 3283, Loss: 0.9160767793655396, Accuracy: 1.0, Computation time: 0.938481330871582\n",
      "Step: 3284, Loss: 0.9159574508666992, Accuracy: 1.0, Computation time: 1.7904026508331299\n",
      "Step: 3285, Loss: 0.9360879063606262, Accuracy: 0.9583333730697632, Computation time: 0.9480624198913574\n",
      "Step: 3286, Loss: 0.9230123162269592, Accuracy: 1.0, Computation time: 0.9318013191223145\n",
      "Step: 3287, Loss: 0.9375936388969421, Accuracy: 0.9750000238418579, Computation time: 0.99015212059021\n",
      "Step: 3288, Loss: 0.9378435611724854, Accuracy: 0.9583333730697632, Computation time: 0.985088586807251\n",
      "Step: 3289, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 0.9256389141082764\n",
      "Step: 3290, Loss: 0.916003942489624, Accuracy: 1.0, Computation time: 1.42051100730896\n",
      "Step: 3291, Loss: 0.9183298945426941, Accuracy: 1.0, Computation time: 0.9885175228118896\n",
      "Step: 3292, Loss: 0.9377161264419556, Accuracy: 0.9722222089767456, Computation time: 0.9093999862670898\n",
      "Step: 3293, Loss: 0.9162130951881409, Accuracy: 1.0, Computation time: 1.067577600479126\n",
      "Step: 3294, Loss: 0.9322677254676819, Accuracy: 0.9583333730697632, Computation time: 1.1455256938934326\n",
      "Step: 3295, Loss: 0.9159864783287048, Accuracy: 1.0, Computation time: 0.8050398826599121\n",
      "Step: 3296, Loss: 0.9159934520721436, Accuracy: 1.0, Computation time: 0.8608598709106445\n",
      "Step: 3297, Loss: 0.9193002581596375, Accuracy: 1.0, Computation time: 0.8110930919647217\n",
      "Step: 3298, Loss: 0.9377459287643433, Accuracy: 0.9750000238418579, Computation time: 0.9258122444152832\n",
      "Step: 3299, Loss: 0.916025698184967, Accuracy: 1.0, Computation time: 0.8875489234924316\n",
      "Step: 3300, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 0.7942864894866943\n",
      "Step: 3301, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 0.8478350639343262\n",
      "Step: 3302, Loss: 0.9356087446212769, Accuracy: 0.9642857313156128, Computation time: 0.8663747310638428\n",
      "Step: 3303, Loss: 0.93695068359375, Accuracy: 0.9722222089767456, Computation time: 0.849067211151123\n",
      "Step: 3304, Loss: 0.9199141263961792, Accuracy: 1.0, Computation time: 0.9148495197296143\n",
      "Step: 3305, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 0.9372954368591309\n",
      "Step: 3306, Loss: 0.9159991145133972, Accuracy: 1.0, Computation time: 1.0957026481628418\n",
      "Step: 3307, Loss: 0.9160412549972534, Accuracy: 1.0, Computation time: 0.8546144962310791\n",
      "Step: 3308, Loss: 0.9160205125808716, Accuracy: 1.0, Computation time: 0.9323635101318359\n",
      "Step: 3309, Loss: 0.9160241484642029, Accuracy: 1.0, Computation time: 0.7631087303161621\n",
      "Step: 3310, Loss: 0.9376468658447266, Accuracy: 0.9642857313156128, Computation time: 0.919684648513794\n",
      "Step: 3311, Loss: 0.9331810474395752, Accuracy: 0.9791666865348816, Computation time: 0.8414626121520996\n",
      "Step: 3312, Loss: 0.9241268634796143, Accuracy: 1.0, Computation time: 0.9875116348266602\n",
      "Step: 3313, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 0.8742070198059082\n",
      "Step: 3314, Loss: 0.936518132686615, Accuracy: 0.9750000238418579, Computation time: 1.3234491348266602\n",
      "Step: 3315, Loss: 0.9371553659439087, Accuracy: 0.9772727489471436, Computation time: 1.811159610748291\n",
      "Step: 3316, Loss: 0.91597980260849, Accuracy: 1.0, Computation time: 0.9412777423858643\n",
      "Step: 3317, Loss: 0.9209257960319519, Accuracy: 1.0, Computation time: 1.5658516883850098\n",
      "Step: 3318, Loss: 0.9166322350502014, Accuracy: 1.0, Computation time: 0.8998239040374756\n",
      "Step: 3319, Loss: 0.9159395694732666, Accuracy: 1.0, Computation time: 0.8743081092834473\n",
      "Step: 3320, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 0.9628081321716309\n",
      "Step: 3321, Loss: 0.927906334400177, Accuracy: 0.9772727489471436, Computation time: 0.8896005153656006\n",
      "Step: 3322, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 0.9571418762207031\n",
      "Step: 3323, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 1.25300931930542\n",
      "Step: 3324, Loss: 0.9158957004547119, Accuracy: 1.0, Computation time: 0.8818097114562988\n",
      "Step: 3325, Loss: 0.9165329933166504, Accuracy: 1.0, Computation time: 0.9894888401031494\n",
      "Step: 3326, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 0.8943326473236084\n",
      "Step: 3327, Loss: 0.9172789454460144, Accuracy: 1.0, Computation time: 0.9491245746612549\n",
      "Step: 3328, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 0.8505053520202637\n",
      "Step: 3329, Loss: 0.915892481803894, Accuracy: 1.0, Computation time: 0.9768483638763428\n",
      "Step: 3330, Loss: 0.9158921837806702, Accuracy: 1.0, Computation time: 0.9985003471374512\n",
      "Step: 3331, Loss: 0.9378083944320679, Accuracy: 0.9772727489471436, Computation time: 1.0560734272003174\n",
      "Step: 3332, Loss: 0.9159729480743408, Accuracy: 1.0, Computation time: 0.9134376049041748\n",
      "Step: 3333, Loss: 0.9175708889961243, Accuracy: 1.0, Computation time: 0.914304256439209\n",
      "Step: 3334, Loss: 0.9158753156661987, Accuracy: 1.0, Computation time: 1.0932836532592773\n",
      "Step: 3335, Loss: 0.9590767025947571, Accuracy: 0.9522727727890015, Computation time: 0.9502480030059814\n",
      "########################\n",
      "Test loss: 1.1276049613952637, Test Accuracy_epoch24: 0.6824578046798706\n",
      "########################\n",
      "Step: 3336, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 0.8629639148712158\n",
      "Step: 3337, Loss: 0.9159451127052307, Accuracy: 1.0, Computation time: 0.8480284214019775\n",
      "Step: 3338, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 1.6123781204223633\n",
      "Step: 3339, Loss: 0.9159904718399048, Accuracy: 1.0, Computation time: 0.8098323345184326\n",
      "Step: 3340, Loss: 0.9375039935112, Accuracy: 0.949999988079071, Computation time: 0.8519024848937988\n",
      "Step: 3341, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 0.9049794673919678\n",
      "Step: 3342, Loss: 0.9375935196876526, Accuracy: 0.984375, Computation time: 0.9325206279754639\n",
      "Step: 3343, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 0.993952751159668\n",
      "Step: 3344, Loss: 0.9159170389175415, Accuracy: 1.0, Computation time: 0.8505227565765381\n",
      "Step: 3345, Loss: 0.9160744547843933, Accuracy: 1.0, Computation time: 1.0718846321105957\n",
      "Step: 3346, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 0.8842689990997314\n",
      "Step: 3347, Loss: 0.9160166382789612, Accuracy: 1.0, Computation time: 1.1594889163970947\n",
      "Step: 3348, Loss: 0.918247640132904, Accuracy: 1.0, Computation time: 0.9292666912078857\n",
      "Step: 3349, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.0773484706878662\n",
      "Step: 3350, Loss: 0.9224421381950378, Accuracy: 1.0, Computation time: 1.0597293376922607\n",
      "Step: 3351, Loss: 0.9175534844398499, Accuracy: 1.0, Computation time: 0.8404278755187988\n",
      "Step: 3352, Loss: 0.9159112572669983, Accuracy: 1.0, Computation time: 0.9577746391296387\n",
      "Step: 3353, Loss: 0.9159931540489197, Accuracy: 1.0, Computation time: 0.8769204616546631\n",
      "Step: 3354, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 0.8634729385375977\n",
      "Step: 3355, Loss: 0.9159490466117859, Accuracy: 1.0, Computation time: 0.8909702301025391\n",
      "Step: 3356, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 0.858180046081543\n",
      "Step: 3357, Loss: 0.9355499744415283, Accuracy: 0.9791666865348816, Computation time: 1.4842579364776611\n",
      "Step: 3358, Loss: 0.9377128481864929, Accuracy: 0.9722222089767456, Computation time: 1.6720175743103027\n",
      "Step: 3359, Loss: 0.9236342310905457, Accuracy: 1.0, Computation time: 1.4002001285552979\n",
      "Step: 3360, Loss: 0.9159228205680847, Accuracy: 1.0, Computation time: 0.8900775909423828\n",
      "Step: 3361, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 0.8699948787689209\n",
      "Step: 3362, Loss: 0.9159958958625793, Accuracy: 1.0, Computation time: 0.8212385177612305\n",
      "Step: 3363, Loss: 0.9160545468330383, Accuracy: 1.0, Computation time: 0.9062290191650391\n",
      "Step: 3364, Loss: 0.976744532585144, Accuracy: 0.9166666865348816, Computation time: 1.134800910949707\n",
      "Step: 3365, Loss: 0.9160546064376831, Accuracy: 1.0, Computation time: 1.0783240795135498\n",
      "Step: 3366, Loss: 0.9160276651382446, Accuracy: 1.0, Computation time: 0.9407632350921631\n",
      "Step: 3367, Loss: 0.9160319566726685, Accuracy: 1.0, Computation time: 1.1192305088043213\n",
      "Step: 3368, Loss: 0.9160318374633789, Accuracy: 1.0, Computation time: 0.953242301940918\n",
      "Step: 3369, Loss: 0.9160580039024353, Accuracy: 1.0, Computation time: 0.9607176780700684\n",
      "Step: 3370, Loss: 0.9159876108169556, Accuracy: 1.0, Computation time: 0.9811468124389648\n",
      "Step: 3371, Loss: 0.9159950613975525, Accuracy: 1.0, Computation time: 0.8882393836975098\n",
      "Step: 3372, Loss: 0.9374814629554749, Accuracy: 0.949999988079071, Computation time: 0.8093316555023193\n",
      "Step: 3373, Loss: 0.9159314632415771, Accuracy: 1.0, Computation time: 0.9000265598297119\n",
      "Step: 3374, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 0.7919070720672607\n",
      "Step: 3375, Loss: 0.9164846539497375, Accuracy: 1.0, Computation time: 1.1608867645263672\n",
      "Step: 3376, Loss: 0.9159001111984253, Accuracy: 1.0, Computation time: 0.9165201187133789\n",
      "Step: 3377, Loss: 0.9159024953842163, Accuracy: 1.0, Computation time: 0.8100066184997559\n",
      "Step: 3378, Loss: 0.9580588340759277, Accuracy: 0.9476190805435181, Computation time: 0.8083548545837402\n",
      "Step: 3379, Loss: 0.9159284234046936, Accuracy: 1.0, Computation time: 0.8542327880859375\n",
      "Step: 3380, Loss: 0.9159711599349976, Accuracy: 1.0, Computation time: 1.027216911315918\n",
      "Step: 3381, Loss: 0.915980339050293, Accuracy: 1.0, Computation time: 0.9817962646484375\n",
      "Step: 3382, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 0.9272124767303467\n",
      "Step: 3383, Loss: 0.916031002998352, Accuracy: 1.0, Computation time: 0.8320462703704834\n",
      "Step: 3384, Loss: 0.9159395098686218, Accuracy: 1.0, Computation time: 0.9410388469696045\n",
      "Step: 3385, Loss: 0.9159042835235596, Accuracy: 1.0, Computation time: 0.7496869564056396\n",
      "Step: 3386, Loss: 0.9300763607025146, Accuracy: 0.9722222089767456, Computation time: 0.8657917976379395\n",
      "Step: 3387, Loss: 0.9204564094543457, Accuracy: 1.0, Computation time: 1.0805082321166992\n",
      "Step: 3388, Loss: 0.9374915361404419, Accuracy: 0.9642857313156128, Computation time: 0.8785905838012695\n",
      "Step: 3389, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 0.7110655307769775\n",
      "Step: 3390, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 0.9222784042358398\n",
      "Step: 3391, Loss: 0.9375588297843933, Accuracy: 0.9750000238418579, Computation time: 0.9730985164642334\n",
      "Step: 3392, Loss: 0.9169932007789612, Accuracy: 1.0, Computation time: 0.9202654361724854\n",
      "Step: 3393, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 0.8386461734771729\n",
      "Step: 3394, Loss: 0.9159247279167175, Accuracy: 1.0, Computation time: 0.8563005924224854\n",
      "Step: 3395, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 0.7594621181488037\n",
      "Step: 3396, Loss: 0.9164429903030396, Accuracy: 1.0, Computation time: 0.9408206939697266\n",
      "Step: 3397, Loss: 0.9178301095962524, Accuracy: 1.0, Computation time: 0.9179763793945312\n",
      "Step: 3398, Loss: 0.9187354445457458, Accuracy: 1.0, Computation time: 1.0208871364593506\n",
      "Step: 3399, Loss: 0.9335179924964905, Accuracy: 0.9583333730697632, Computation time: 1.0294365882873535\n",
      "Step: 3400, Loss: 0.9159033894538879, Accuracy: 1.0, Computation time: 0.845970630645752\n",
      "Step: 3401, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 0.8378260135650635\n",
      "Step: 3402, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 0.8521757125854492\n",
      "Step: 3403, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 0.8915011882781982\n",
      "Step: 3404, Loss: 0.9277971386909485, Accuracy: 0.9750000238418579, Computation time: 0.9868946075439453\n",
      "Step: 3405, Loss: 0.9159668684005737, Accuracy: 1.0, Computation time: 0.9749791622161865\n",
      "Step: 3406, Loss: 0.9243363738059998, Accuracy: 1.0, Computation time: 1.1757283210754395\n",
      "Step: 3407, Loss: 0.9357069730758667, Accuracy: 0.9722222089767456, Computation time: 0.8747200965881348\n",
      "Step: 3408, Loss: 0.9167650938034058, Accuracy: 1.0, Computation time: 1.1517693996429443\n",
      "Step: 3409, Loss: 0.9159842133522034, Accuracy: 1.0, Computation time: 0.8288929462432861\n",
      "Step: 3410, Loss: 0.91597980260849, Accuracy: 1.0, Computation time: 1.031144380569458\n",
      "Step: 3411, Loss: 0.9178160429000854, Accuracy: 1.0, Computation time: 0.8612790107727051\n",
      "Step: 3412, Loss: 0.9198792576789856, Accuracy: 1.0, Computation time: 0.8897294998168945\n",
      "Step: 3413, Loss: 0.9376438856124878, Accuracy: 0.9291666746139526, Computation time: 0.8872711658477783\n",
      "Step: 3414, Loss: 0.9161109328269958, Accuracy: 1.0, Computation time: 0.8178644180297852\n",
      "Step: 3415, Loss: 0.9160296320915222, Accuracy: 1.0, Computation time: 0.8968286514282227\n",
      "Step: 3416, Loss: 0.9377446174621582, Accuracy: 0.9791666865348816, Computation time: 0.8816444873809814\n",
      "Step: 3417, Loss: 0.9161152243614197, Accuracy: 1.0, Computation time: 0.8857898712158203\n",
      "Step: 3418, Loss: 0.9173986911773682, Accuracy: 1.0, Computation time: 1.1333544254302979\n",
      "Step: 3419, Loss: 0.9160046577453613, Accuracy: 1.0, Computation time: 0.852332592010498\n",
      "Step: 3420, Loss: 0.915986955165863, Accuracy: 1.0, Computation time: 0.8582160472869873\n",
      "Step: 3421, Loss: 0.9160499572753906, Accuracy: 1.0, Computation time: 0.9846987724304199\n",
      "Step: 3422, Loss: 0.9189839959144592, Accuracy: 1.0, Computation time: 1.0822572708129883\n",
      "Step: 3423, Loss: 0.9162595272064209, Accuracy: 1.0, Computation time: 1.022707223892212\n",
      "Step: 3424, Loss: 0.9370133876800537, Accuracy: 0.9791666865348816, Computation time: 0.8668582439422607\n",
      "Step: 3425, Loss: 0.9159845113754272, Accuracy: 1.0, Computation time: 0.9581756591796875\n",
      "Step: 3426, Loss: 0.9159541130065918, Accuracy: 1.0, Computation time: 0.8334534168243408\n",
      "Step: 3427, Loss: 0.958807110786438, Accuracy: 0.9545454978942871, Computation time: 0.8323767185211182\n",
      "Step: 3428, Loss: 0.91808021068573, Accuracy: 1.0, Computation time: 1.0323855876922607\n",
      "Step: 3429, Loss: 0.9160016775131226, Accuracy: 1.0, Computation time: 1.0632283687591553\n",
      "Step: 3430, Loss: 0.9159784317016602, Accuracy: 1.0, Computation time: 1.3712427616119385\n",
      "Step: 3431, Loss: 0.915973961353302, Accuracy: 1.0, Computation time: 0.8694665431976318\n",
      "Step: 3432, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 0.8416459560394287\n",
      "Step: 3433, Loss: 0.919684112071991, Accuracy: 1.0, Computation time: 1.199505090713501\n",
      "Step: 3434, Loss: 0.9159623384475708, Accuracy: 1.0, Computation time: 0.9198718070983887\n",
      "Step: 3435, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 1.0943174362182617\n",
      "Step: 3436, Loss: 0.9593310356140137, Accuracy: 0.9508928656578064, Computation time: 0.8225743770599365\n",
      "Step: 3437, Loss: 0.9158849120140076, Accuracy: 1.0, Computation time: 1.0012798309326172\n",
      "Step: 3438, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 0.8583025932312012\n",
      "Step: 3439, Loss: 0.9159630537033081, Accuracy: 1.0, Computation time: 1.1606695652008057\n",
      "Step: 3440, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 1.2158212661743164\n",
      "Step: 3441, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 0.9429364204406738\n",
      "Step: 3442, Loss: 0.9158803224563599, Accuracy: 1.0, Computation time: 0.9562687873840332\n",
      "Step: 3443, Loss: 0.915861189365387, Accuracy: 1.0, Computation time: 0.9181411266326904\n",
      "Step: 3444, Loss: 0.9159091114997864, Accuracy: 1.0, Computation time: 0.8626482486724854\n",
      "Step: 3445, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 1.2280688285827637\n",
      "Step: 3446, Loss: 0.9159059524536133, Accuracy: 1.0, Computation time: 1.0536649227142334\n",
      "Step: 3447, Loss: 0.9159465432167053, Accuracy: 1.0, Computation time: 0.9367275238037109\n",
      "Step: 3448, Loss: 0.9159284830093384, Accuracy: 1.0, Computation time: 0.9400780200958252\n",
      "Step: 3449, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 0.911839485168457\n",
      "Step: 3450, Loss: 0.9158580303192139, Accuracy: 1.0, Computation time: 0.9328813552856445\n",
      "Step: 3451, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.2134771347045898\n",
      "Step: 3452, Loss: 0.9206255078315735, Accuracy: 1.0, Computation time: 1.2872939109802246\n",
      "Step: 3453, Loss: 0.9376248121261597, Accuracy: 0.9772727489471436, Computation time: 0.9085304737091064\n",
      "Step: 3454, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 0.8008532524108887\n",
      "Step: 3455, Loss: 0.9160134792327881, Accuracy: 1.0, Computation time: 0.8806452751159668\n",
      "Step: 3456, Loss: 0.9163487553596497, Accuracy: 1.0, Computation time: 1.0279669761657715\n",
      "Step: 3457, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 1.2691247463226318\n",
      "Step: 3458, Loss: 0.9365123510360718, Accuracy: 0.9791666865348816, Computation time: 1.0709974765777588\n",
      "Step: 3459, Loss: 0.9158936142921448, Accuracy: 1.0, Computation time: 0.8824834823608398\n",
      "Step: 3460, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 0.9179332256317139\n",
      "Step: 3461, Loss: 0.9161032438278198, Accuracy: 1.0, Computation time: 0.8639681339263916\n",
      "Step: 3462, Loss: 0.9159301519393921, Accuracy: 1.0, Computation time: 0.9030823707580566\n",
      "Step: 3463, Loss: 0.9178470373153687, Accuracy: 1.0, Computation time: 0.8957421779632568\n",
      "Step: 3464, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 0.8250584602355957\n",
      "Step: 3465, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 0.8657336235046387\n",
      "Step: 3466, Loss: 0.937525749206543, Accuracy: 0.9722222089767456, Computation time: 0.9005119800567627\n",
      "Step: 3467, Loss: 0.915969967842102, Accuracy: 1.0, Computation time: 0.8804757595062256\n",
      "Step: 3468, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 0.9265232086181641\n",
      "Step: 3469, Loss: 0.9158757925033569, Accuracy: 1.0, Computation time: 0.8749544620513916\n",
      "Step: 3470, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 0.7982163429260254\n",
      "Step: 3471, Loss: 0.9158421754837036, Accuracy: 1.0, Computation time: 0.9421806335449219\n",
      "Step: 3472, Loss: 0.9224902987480164, Accuracy: 1.0, Computation time: 1.336763620376587\n",
      "Step: 3473, Loss: 0.9371148347854614, Accuracy: 0.9750000238418579, Computation time: 0.929689884185791\n",
      "Step: 3474, Loss: 0.9374950528144836, Accuracy: 0.9791666865348816, Computation time: 0.7261757850646973\n",
      "########################\n",
      "Test loss: 1.129982352256775, Test Accuracy_epoch25: 0.676873505115509\n",
      "########################\n",
      "Step: 3475, Loss: 0.9160105586051941, Accuracy: 1.0, Computation time: 0.7378997802734375\n",
      "Step: 3476, Loss: 0.9375919699668884, Accuracy: 0.9722222089767456, Computation time: 0.7202181816101074\n",
      "Step: 3477, Loss: 0.9372879862785339, Accuracy: 0.9722222089767456, Computation time: 0.8762326240539551\n",
      "Step: 3478, Loss: 0.91612708568573, Accuracy: 1.0, Computation time: 0.7578270435333252\n",
      "Step: 3479, Loss: 0.9160037636756897, Accuracy: 1.0, Computation time: 0.7341108322143555\n",
      "Step: 3480, Loss: 0.9159557819366455, Accuracy: 1.0, Computation time: 0.6915791034698486\n",
      "Step: 3481, Loss: 0.9375763535499573, Accuracy: 0.9750000238418579, Computation time: 0.6539754867553711\n",
      "Step: 3482, Loss: 0.9321215152740479, Accuracy: 0.9750000238418579, Computation time: 0.695549488067627\n",
      "Step: 3483, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 0.7992784976959229\n",
      "Step: 3484, Loss: 0.9159038662910461, Accuracy: 1.0, Computation time: 0.6991219520568848\n",
      "Step: 3485, Loss: 0.9159797430038452, Accuracy: 1.0, Computation time: 0.7372837066650391\n",
      "Step: 3486, Loss: 0.916006326675415, Accuracy: 1.0, Computation time: 0.7258899211883545\n",
      "Step: 3487, Loss: 0.9380477070808411, Accuracy: 0.9642857313156128, Computation time: 0.7890288829803467\n",
      "Step: 3488, Loss: 0.9160014986991882, Accuracy: 1.0, Computation time: 0.7527790069580078\n",
      "Step: 3489, Loss: 0.9159390926361084, Accuracy: 1.0, Computation time: 0.7045955657958984\n",
      "Step: 3490, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 0.6550443172454834\n",
      "Step: 3491, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 0.7344586849212646\n",
      "Step: 3492, Loss: 0.9158956408500671, Accuracy: nan, Computation time: 0.7563331127166748\n",
      "Step: 3493, Loss: 0.9374821782112122, Accuracy: 0.9772727489471436, Computation time: 0.6329550743103027\n",
      "Step: 3494, Loss: 0.9159786105155945, Accuracy: 1.0, Computation time: 0.7137491703033447\n",
      "Step: 3495, Loss: 0.9178423881530762, Accuracy: 1.0, Computation time: 0.7599258422851562\n",
      "Step: 3496, Loss: 0.9376521706581116, Accuracy: 0.9772727489471436, Computation time: 0.7487502098083496\n",
      "Step: 3497, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 0.6806035041809082\n",
      "Step: 3498, Loss: 0.9159272909164429, Accuracy: 1.0, Computation time: 0.6734771728515625\n",
      "Step: 3499, Loss: 0.9159675240516663, Accuracy: 1.0, Computation time: 0.7737174034118652\n",
      "Step: 3500, Loss: 0.9160553216934204, Accuracy: 1.0, Computation time: 0.9674701690673828\n",
      "Step: 3501, Loss: 0.9170837998390198, Accuracy: 1.0, Computation time: 0.732351541519165\n",
      "Step: 3502, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.6711287498474121\n",
      "Step: 3503, Loss: 0.9158691763877869, Accuracy: 1.0, Computation time: 0.7120084762573242\n",
      "Step: 3504, Loss: 0.9160439968109131, Accuracy: 1.0, Computation time: 0.6965138912200928\n",
      "Step: 3505, Loss: 0.9192266464233398, Accuracy: 1.0, Computation time: 0.8384790420532227\n",
      "Step: 3506, Loss: 0.9160779118537903, Accuracy: 1.0, Computation time: 0.7136781215667725\n",
      "Step: 3507, Loss: 0.9165520071983337, Accuracy: 1.0, Computation time: 0.7773361206054688\n",
      "Step: 3508, Loss: 0.9169334769248962, Accuracy: 1.0, Computation time: 0.6884987354278564\n",
      "Step: 3509, Loss: 0.9166526198387146, Accuracy: 1.0, Computation time: 0.7785506248474121\n",
      "Step: 3510, Loss: 0.9174128174781799, Accuracy: 1.0, Computation time: 0.8282020092010498\n",
      "Step: 3511, Loss: 0.9165488481521606, Accuracy: 1.0, Computation time: 0.8699483871459961\n",
      "Step: 3512, Loss: 0.937935471534729, Accuracy: 0.9750000238418579, Computation time: 0.8277404308319092\n",
      "Step: 3513, Loss: 0.9161322116851807, Accuracy: 1.0, Computation time: 0.8651132583618164\n",
      "Step: 3514, Loss: 0.9159860014915466, Accuracy: 1.0, Computation time: 1.057405710220337\n",
      "Step: 3515, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 0.9289062023162842\n",
      "Step: 3516, Loss: 0.9242585897445679, Accuracy: 1.0, Computation time: 1.2297167778015137\n",
      "Step: 3517, Loss: 0.9243044853210449, Accuracy: 1.0, Computation time: 1.0259344577789307\n",
      "Step: 3518, Loss: 0.9469804167747498, Accuracy: 0.9444444179534912, Computation time: 1.2817366123199463\n",
      "Step: 3519, Loss: 0.9379823207855225, Accuracy: 0.9642857313156128, Computation time: 1.0764029026031494\n",
      "Step: 3520, Loss: 0.9162673354148865, Accuracy: 1.0, Computation time: 0.8981914520263672\n",
      "Step: 3521, Loss: 0.9165182709693909, Accuracy: 1.0, Computation time: 0.9128074645996094\n",
      "Step: 3522, Loss: 0.9384670257568359, Accuracy: 0.9807692766189575, Computation time: 0.9490883350372314\n",
      "Step: 3523, Loss: 0.9166514277458191, Accuracy: 1.0, Computation time: 1.0091941356658936\n",
      "Step: 3524, Loss: 0.9169857501983643, Accuracy: 1.0, Computation time: 0.9523618221282959\n",
      "Step: 3525, Loss: 0.9165180325508118, Accuracy: 1.0, Computation time: 1.003614902496338\n",
      "Step: 3526, Loss: 0.916183352470398, Accuracy: 1.0, Computation time: 1.0455443859100342\n",
      "Step: 3527, Loss: 0.9161500930786133, Accuracy: 1.0, Computation time: 0.8900823593139648\n",
      "Step: 3528, Loss: 0.9164602160453796, Accuracy: 1.0, Computation time: 1.141812801361084\n",
      "Step: 3529, Loss: 0.916557252407074, Accuracy: 1.0, Computation time: 1.0252585411071777\n",
      "Step: 3530, Loss: 0.9162358641624451, Accuracy: 1.0, Computation time: 1.0870356559753418\n",
      "Step: 3531, Loss: 0.9322030544281006, Accuracy: 0.9722222089767456, Computation time: 1.0594897270202637\n",
      "Step: 3532, Loss: 0.9542025327682495, Accuracy: 0.9409722089767456, Computation time: 1.3398168087005615\n",
      "Step: 3533, Loss: 0.9163718223571777, Accuracy: 1.0, Computation time: 0.9725775718688965\n",
      "Step: 3534, Loss: 0.9162237048149109, Accuracy: 1.0, Computation time: 1.3074400424957275\n",
      "Step: 3535, Loss: 0.9165464043617249, Accuracy: 1.0, Computation time: 1.0259830951690674\n",
      "Step: 3536, Loss: 0.9166470766067505, Accuracy: 1.0, Computation time: 0.9197607040405273\n",
      "Step: 3537, Loss: 0.9189072251319885, Accuracy: 1.0, Computation time: 1.747814655303955\n",
      "Step: 3538, Loss: 0.9205608367919922, Accuracy: 1.0, Computation time: 1.0583152770996094\n",
      "Step: 3539, Loss: 0.9163932204246521, Accuracy: 1.0, Computation time: 1.0201075077056885\n",
      "Step: 3540, Loss: 0.9164687991142273, Accuracy: 1.0, Computation time: 1.0149688720703125\n",
      "Step: 3541, Loss: 0.9168843626976013, Accuracy: 1.0, Computation time: 1.132530927658081\n",
      "Step: 3542, Loss: 0.9279767870903015, Accuracy: 0.9750000238418579, Computation time: 1.1566061973571777\n",
      "Step: 3543, Loss: 0.9300790429115295, Accuracy: 0.9791666865348816, Computation time: 1.0029642581939697\n",
      "Step: 3544, Loss: 0.9166346192359924, Accuracy: 1.0, Computation time: 1.2864406108856201\n",
      "Step: 3545, Loss: 0.91683030128479, Accuracy: 1.0, Computation time: 0.9245049953460693\n",
      "Step: 3546, Loss: 0.9364862442016602, Accuracy: 0.949999988079071, Computation time: 0.9718453884124756\n",
      "Step: 3547, Loss: 0.9166122674942017, Accuracy: 1.0, Computation time: 1.062678337097168\n",
      "Step: 3548, Loss: 0.916354238986969, Accuracy: 1.0, Computation time: 0.9305639266967773\n",
      "Step: 3549, Loss: 0.9167194962501526, Accuracy: 1.0, Computation time: 1.0082857608795166\n",
      "Step: 3550, Loss: 0.9189963340759277, Accuracy: 1.0, Computation time: 1.5933966636657715\n",
      "Step: 3551, Loss: 0.938029408454895, Accuracy: 0.96875, Computation time: 0.9674501419067383\n",
      "Step: 3552, Loss: 0.9162305593490601, Accuracy: 1.0, Computation time: 0.9930541515350342\n",
      "Step: 3553, Loss: 0.9376259446144104, Accuracy: 0.9722222089767456, Computation time: 0.922753095626831\n",
      "Step: 3554, Loss: 0.916670560836792, Accuracy: 1.0, Computation time: 1.1034255027770996\n",
      "Step: 3555, Loss: 0.9161434769630432, Accuracy: 1.0, Computation time: 0.9048349857330322\n",
      "Step: 3556, Loss: 0.9182025790214539, Accuracy: 1.0, Computation time: 0.9905502796173096\n",
      "Step: 3557, Loss: 0.9161137342453003, Accuracy: 1.0, Computation time: 1.052351713180542\n",
      "Step: 3558, Loss: 0.9160528182983398, Accuracy: 1.0, Computation time: 1.070540189743042\n",
      "Step: 3559, Loss: 0.916201651096344, Accuracy: 1.0, Computation time: 1.0362203121185303\n",
      "Step: 3560, Loss: 0.9597145318984985, Accuracy: 0.9097222089767456, Computation time: 1.0977675914764404\n",
      "Step: 3561, Loss: 0.9160411357879639, Accuracy: 1.0, Computation time: 1.0311939716339111\n",
      "Step: 3562, Loss: 0.9198005795478821, Accuracy: 1.0, Computation time: 1.0296907424926758\n",
      "Step: 3563, Loss: 0.9159294962882996, Accuracy: 1.0, Computation time: 1.0458290576934814\n",
      "Step: 3564, Loss: 0.9159864783287048, Accuracy: 1.0, Computation time: 0.954744815826416\n",
      "Step: 3565, Loss: 0.9174657464027405, Accuracy: 1.0, Computation time: 1.5470263957977295\n",
      "Step: 3566, Loss: 0.916857898235321, Accuracy: 1.0, Computation time: 0.8439960479736328\n",
      "Step: 3567, Loss: 0.9161407947540283, Accuracy: 1.0, Computation time: 1.174464225769043\n",
      "Step: 3568, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 1.0582597255706787\n",
      "Step: 3569, Loss: 0.9192650318145752, Accuracy: 1.0, Computation time: 1.0481226444244385\n",
      "Step: 3570, Loss: 0.9159467220306396, Accuracy: 1.0, Computation time: 1.160167932510376\n",
      "Step: 3571, Loss: 0.9169720411300659, Accuracy: 1.0, Computation time: 0.8797664642333984\n",
      "Step: 3572, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 0.9313180446624756\n",
      "Step: 3573, Loss: 0.9376602172851562, Accuracy: 0.875, Computation time: 0.9922637939453125\n",
      "Step: 3574, Loss: 0.9160444736480713, Accuracy: 1.0, Computation time: 0.8802478313446045\n",
      "Step: 3575, Loss: 0.9162405729293823, Accuracy: 1.0, Computation time: 0.8684015274047852\n",
      "Step: 3576, Loss: 0.9161480665206909, Accuracy: 1.0, Computation time: 0.8391485214233398\n",
      "Step: 3577, Loss: 0.9159060716629028, Accuracy: 1.0, Computation time: 0.876272439956665\n",
      "Step: 3578, Loss: 0.9376373291015625, Accuracy: 0.96875, Computation time: 1.1035947799682617\n",
      "Step: 3579, Loss: 0.9159497022628784, Accuracy: 1.0, Computation time: 0.8765652179718018\n",
      "Step: 3580, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.068871021270752\n",
      "Step: 3581, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 0.9295139312744141\n",
      "Step: 3582, Loss: 0.9187459945678711, Accuracy: 1.0, Computation time: 1.0164258480072021\n",
      "Step: 3583, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 1.0370466709136963\n",
      "Step: 3584, Loss: 0.9483012557029724, Accuracy: 0.9479166865348816, Computation time: 1.2503411769866943\n",
      "Step: 3585, Loss: 0.9163369536399841, Accuracy: 1.0, Computation time: 1.3191404342651367\n",
      "Step: 3586, Loss: 0.9159852266311646, Accuracy: 1.0, Computation time: 0.9037232398986816\n",
      "Step: 3587, Loss: 0.9160341024398804, Accuracy: 1.0, Computation time: 0.8797299861907959\n",
      "Step: 3588, Loss: 0.9159805178642273, Accuracy: 1.0, Computation time: 0.9928238391876221\n",
      "Step: 3589, Loss: 0.9160093665122986, Accuracy: 1.0, Computation time: 1.0050692558288574\n",
      "Step: 3590, Loss: 0.9160645604133606, Accuracy: 1.0, Computation time: 0.8974120616912842\n",
      "Step: 3591, Loss: 0.916210949420929, Accuracy: 1.0, Computation time: 0.904099702835083\n",
      "Step: 3592, Loss: 0.9167022109031677, Accuracy: 1.0, Computation time: 0.9024550914764404\n",
      "Step: 3593, Loss: 0.9160284399986267, Accuracy: 1.0, Computation time: 1.0199847221374512\n",
      "Step: 3594, Loss: 0.9158563017845154, Accuracy: 1.0, Computation time: 0.9442713260650635\n",
      "Step: 3595, Loss: 0.9162823557853699, Accuracy: 1.0, Computation time: 1.0212721824645996\n",
      "Step: 3596, Loss: 0.9542480707168579, Accuracy: 0.9541666507720947, Computation time: 0.877255916595459\n",
      "Step: 3597, Loss: 0.9158892631530762, Accuracy: 1.0, Computation time: 0.9206016063690186\n",
      "Step: 3598, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 0.8373470306396484\n",
      "Step: 3599, Loss: 0.9159429669380188, Accuracy: 1.0, Computation time: 0.8582093715667725\n",
      "Step: 3600, Loss: 0.9376246333122253, Accuracy: 0.96875, Computation time: 1.2843520641326904\n",
      "Step: 3601, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 0.9605381488800049\n",
      "Step: 3602, Loss: 0.9328086972236633, Accuracy: 0.9583333730697632, Computation time: 1.2338080406188965\n",
      "Step: 3603, Loss: 0.9159347414970398, Accuracy: 1.0, Computation time: 1.076500654220581\n",
      "Step: 3604, Loss: 0.9376521706581116, Accuracy: 0.9791666865348816, Computation time: 1.6203713417053223\n",
      "Step: 3605, Loss: 0.9177194833755493, Accuracy: 1.0, Computation time: 0.8366155624389648\n",
      "Step: 3606, Loss: 0.9259196519851685, Accuracy: 0.96875, Computation time: 0.9990041255950928\n",
      "Step: 3607, Loss: 0.9159660935401917, Accuracy: 1.0, Computation time: 1.1018846035003662\n",
      "Step: 3608, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.0139548778533936\n",
      "Step: 3609, Loss: 0.9158790111541748, Accuracy: 1.0, Computation time: 0.9595394134521484\n",
      "Step: 3610, Loss: 0.9375761151313782, Accuracy: 0.9750000238418579, Computation time: 0.9971997737884521\n",
      "Step: 3611, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 1.191819429397583\n",
      "Step: 3612, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 1.0479791164398193\n",
      "Step: 3613, Loss: 0.9158899188041687, Accuracy: 1.0, Computation time: 0.9067811965942383\n",
      "########################\n",
      "Test loss: 1.1256308555603027, Test Accuracy_epoch26: 0.6848657727241516\n",
      "########################\n",
      "Step: 3614, Loss: 0.9159499406814575, Accuracy: 1.0, Computation time: 0.9354984760284424\n",
      "Step: 3615, Loss: 0.9374846816062927, Accuracy: 0.96875, Computation time: 0.8573009967803955\n",
      "Step: 3616, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 0.83756422996521\n",
      "Step: 3617, Loss: 0.9371769428253174, Accuracy: 0.9583333730697632, Computation time: 0.8403849601745605\n",
      "Step: 3618, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 0.9145979881286621\n",
      "Step: 3619, Loss: 0.915907084941864, Accuracy: 1.0, Computation time: 0.9379417896270752\n",
      "Step: 3620, Loss: 0.9163115620613098, Accuracy: 1.0, Computation time: 1.0029468536376953\n",
      "Step: 3621, Loss: 0.9375362396240234, Accuracy: 0.9791666865348816, Computation time: 1.078834056854248\n",
      "Step: 3622, Loss: 0.9168227314949036, Accuracy: 1.0, Computation time: 1.0555365085601807\n",
      "Step: 3623, Loss: 0.9196436405181885, Accuracy: 1.0, Computation time: 1.067662000656128\n",
      "Step: 3624, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 0.8098375797271729\n",
      "Step: 3625, Loss: 0.9562162160873413, Accuracy: 0.9464285969734192, Computation time: 1.2977681159973145\n",
      "Step: 3626, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 0.8008768558502197\n",
      "Step: 3627, Loss: 0.9189184904098511, Accuracy: 1.0, Computation time: 1.0598478317260742\n",
      "Step: 3628, Loss: 0.9159205555915833, Accuracy: 1.0, Computation time: 0.9857339859008789\n",
      "Step: 3629, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 0.8780014514923096\n",
      "Step: 3630, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 0.9974977970123291\n",
      "Step: 3631, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 0.953101634979248\n",
      "Step: 3632, Loss: 0.9159144759178162, Accuracy: 1.0, Computation time: 1.1334965229034424\n",
      "Step: 3633, Loss: 0.9158980846405029, Accuracy: 1.0, Computation time: 0.9759747982025146\n",
      "Step: 3634, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 0.9950575828552246\n",
      "Step: 3635, Loss: 0.9159244894981384, Accuracy: 1.0, Computation time: 1.008267879486084\n",
      "Step: 3636, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 1.0166172981262207\n",
      "Step: 3637, Loss: 0.9159374833106995, Accuracy: 1.0, Computation time: 0.8478796482086182\n",
      "Step: 3638, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 0.9658346176147461\n",
      "Step: 3639, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 0.9941024780273438\n",
      "Step: 3640, Loss: 0.9161679744720459, Accuracy: 1.0, Computation time: 1.2784464359283447\n",
      "Step: 3641, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 0.8182840347290039\n",
      "Step: 3642, Loss: 0.9169336557388306, Accuracy: 1.0, Computation time: 0.8222291469573975\n",
      "Step: 3643, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 0.9217753410339355\n",
      "Step: 3644, Loss: 0.9378148913383484, Accuracy: 0.9821428656578064, Computation time: 1.1715726852416992\n",
      "Step: 3645, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 0.85379958152771\n",
      "Step: 3646, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 0.8758189678192139\n",
      "Step: 3647, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.8770503997802734\n",
      "Step: 3648, Loss: 0.9158987998962402, Accuracy: 1.0, Computation time: 0.8192095756530762\n",
      "Step: 3649, Loss: 0.9171677827835083, Accuracy: 1.0, Computation time: 1.9189720153808594\n",
      "Step: 3650, Loss: 0.9247992634773254, Accuracy: 1.0, Computation time: 0.8729581832885742\n",
      "Step: 3651, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 0.9767203330993652\n",
      "Step: 3652, Loss: 0.91593337059021, Accuracy: 1.0, Computation time: 1.3796746730804443\n",
      "Step: 3653, Loss: 0.9159067869186401, Accuracy: 1.0, Computation time: 0.8557848930358887\n",
      "Step: 3654, Loss: 0.9348163604736328, Accuracy: 0.9722222089767456, Computation time: 1.0350368022918701\n",
      "Step: 3655, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 0.9269430637359619\n",
      "Step: 3656, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 1.0313146114349365\n",
      "Step: 3657, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 1.1236305236816406\n",
      "Step: 3658, Loss: 0.9159446358680725, Accuracy: 1.0, Computation time: 1.0141820907592773\n",
      "Step: 3659, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 1.2586033344268799\n",
      "Step: 3660, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.0046987533569336\n",
      "Step: 3661, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.8785824775695801\n",
      "Step: 3662, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.0192179679870605\n",
      "Step: 3663, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 1.1710453033447266\n",
      "Step: 3664, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 0.9146769046783447\n",
      "Step: 3665, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 0.8717520236968994\n",
      "Step: 3666, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 0.8751111030578613\n",
      "Step: 3667, Loss: 0.9158458709716797, Accuracy: 1.0, Computation time: 0.8776459693908691\n",
      "Step: 3668, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 0.9651756286621094\n",
      "Step: 3669, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.9911160469055176\n",
      "Step: 3670, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 1.1187758445739746\n",
      "Step: 3671, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 0.9903833866119385\n",
      "Step: 3672, Loss: 0.9164367914199829, Accuracy: 1.0, Computation time: 0.963482141494751\n",
      "Step: 3673, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 0.8779253959655762\n",
      "Step: 3674, Loss: 0.9158564209938049, Accuracy: 1.0, Computation time: 0.8266675472259521\n",
      "Step: 3675, Loss: 0.9158378839492798, Accuracy: 1.0, Computation time: 0.8385958671569824\n",
      "Step: 3676, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 0.9251523017883301\n",
      "Step: 3677, Loss: 0.9158647656440735, Accuracy: 1.0, Computation time: 1.0712149143218994\n",
      "Step: 3678, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.2484307289123535\n",
      "Step: 3679, Loss: 0.9375302791595459, Accuracy: 0.96875, Computation time: 1.0677165985107422\n",
      "Step: 3680, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 0.937821626663208\n",
      "Step: 3681, Loss: 0.9298439621925354, Accuracy: 0.9642857313156128, Computation time: 1.3113791942596436\n",
      "Step: 3682, Loss: 0.9158565402030945, Accuracy: 1.0, Computation time: 1.1525418758392334\n",
      "Step: 3683, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 0.9465234279632568\n",
      "Step: 3684, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 0.898705244064331\n",
      "Step: 3685, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 1.0231571197509766\n",
      "Step: 3686, Loss: 0.9159643054008484, Accuracy: 1.0, Computation time: 1.0196654796600342\n",
      "Step: 3687, Loss: 0.9158873558044434, Accuracy: 1.0, Computation time: 0.9736423492431641\n",
      "Step: 3688, Loss: 0.9545843005180359, Accuracy: 0.949999988079071, Computation time: 0.9703278541564941\n",
      "Step: 3689, Loss: 0.9375060200691223, Accuracy: 0.96875, Computation time: 0.8496370315551758\n",
      "Step: 3690, Loss: 0.9374637603759766, Accuracy: 0.9722222089767456, Computation time: 1.0983850955963135\n",
      "Step: 3691, Loss: 0.9195186495780945, Accuracy: 1.0, Computation time: 1.7881059646606445\n",
      "Step: 3692, Loss: 0.9377081394195557, Accuracy: 0.9833333492279053, Computation time: 0.9011900424957275\n",
      "Step: 3693, Loss: 0.9375666975975037, Accuracy: 0.9807692766189575, Computation time: 1.019860029220581\n",
      "Step: 3694, Loss: 0.9160138368606567, Accuracy: 1.0, Computation time: 0.859503984451294\n",
      "Step: 3695, Loss: 0.9168457388877869, Accuracy: 1.0, Computation time: 0.9853217601776123\n",
      "Step: 3696, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 0.9540975093841553\n",
      "Step: 3697, Loss: 0.9376268982887268, Accuracy: 0.9583333730697632, Computation time: 1.2954986095428467\n",
      "Step: 3698, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 0.921851396560669\n",
      "Step: 3699, Loss: 0.9159194827079773, Accuracy: 1.0, Computation time: 0.9544618129730225\n",
      "Step: 3700, Loss: 0.9161031246185303, Accuracy: 1.0, Computation time: 0.8980681896209717\n",
      "Step: 3701, Loss: 0.9159868955612183, Accuracy: 1.0, Computation time: 0.9409735202789307\n",
      "Step: 3702, Loss: 0.9159433841705322, Accuracy: 1.0, Computation time: 0.9499397277832031\n",
      "Step: 3703, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 0.9698302745819092\n",
      "Step: 3704, Loss: 0.9159387350082397, Accuracy: 1.0, Computation time: 0.994483470916748\n",
      "Step: 3705, Loss: 0.9203239679336548, Accuracy: 1.0, Computation time: 1.0350925922393799\n",
      "Step: 3706, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 0.9258310794830322\n",
      "Step: 3707, Loss: 0.9187538027763367, Accuracy: 1.0, Computation time: 1.3663082122802734\n",
      "Step: 3708, Loss: 0.9160224199295044, Accuracy: 1.0, Computation time: 0.8355522155761719\n",
      "Step: 3709, Loss: 0.9160017967224121, Accuracy: 1.0, Computation time: 0.908764123916626\n",
      "Step: 3710, Loss: 0.9231685400009155, Accuracy: 1.0, Computation time: 0.8268136978149414\n",
      "Step: 3711, Loss: 0.9161670207977295, Accuracy: 1.0, Computation time: 0.9257526397705078\n",
      "Step: 3712, Loss: 0.9164178371429443, Accuracy: 1.0, Computation time: 1.019676685333252\n",
      "Step: 3713, Loss: 0.9159173369407654, Accuracy: 1.0, Computation time: 0.952307939529419\n",
      "Step: 3714, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 1.3551952838897705\n",
      "Step: 3715, Loss: 0.9159376621246338, Accuracy: 1.0, Computation time: 0.8886828422546387\n",
      "Step: 3716, Loss: 0.9374178051948547, Accuracy: 0.9583333730697632, Computation time: 0.9674801826477051\n",
      "Step: 3717, Loss: 0.918808102607727, Accuracy: 1.0, Computation time: 1.4036462306976318\n",
      "Step: 3718, Loss: 0.916230320930481, Accuracy: 1.0, Computation time: 0.905510425567627\n",
      "Step: 3719, Loss: 0.9160821437835693, Accuracy: 1.0, Computation time: 0.9644126892089844\n",
      "Step: 3720, Loss: 0.937288224697113, Accuracy: 0.9750000238418579, Computation time: 0.8897111415863037\n",
      "Step: 3721, Loss: 0.9159592986106873, Accuracy: 1.0, Computation time: 0.8663930892944336\n",
      "Step: 3722, Loss: 0.9376916885375977, Accuracy: 0.9750000238418579, Computation time: 0.9226655960083008\n",
      "Step: 3723, Loss: 0.916339099407196, Accuracy: 1.0, Computation time: 0.9870796203613281\n",
      "Step: 3724, Loss: 0.9159242510795593, Accuracy: 1.0, Computation time: 1.3350098133087158\n",
      "Step: 3725, Loss: 0.9159318208694458, Accuracy: 1.0, Computation time: 1.138425350189209\n",
      "Step: 3726, Loss: 0.9160553216934204, Accuracy: 1.0, Computation time: 1.0842115879058838\n",
      "Step: 3727, Loss: 0.9376721978187561, Accuracy: 0.9833333492279053, Computation time: 0.9630336761474609\n",
      "Step: 3728, Loss: 0.9161850810050964, Accuracy: 1.0, Computation time: 0.8850240707397461\n",
      "Step: 3729, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 0.9638049602508545\n",
      "Step: 3730, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 0.9226839542388916\n",
      "Step: 3731, Loss: 0.9226406812667847, Accuracy: 1.0, Computation time: 0.8446919918060303\n",
      "Step: 3732, Loss: 0.9375467896461487, Accuracy: 0.9750000238418579, Computation time: 1.0569660663604736\n",
      "Step: 3733, Loss: 0.9560829401016235, Accuracy: 0.9615384340286255, Computation time: 1.009387731552124\n",
      "Step: 3734, Loss: 0.9161883592605591, Accuracy: 1.0, Computation time: 1.0879981517791748\n",
      "Step: 3735, Loss: 0.9348483085632324, Accuracy: 0.9583333730697632, Computation time: 1.0633327960968018\n",
      "Step: 3736, Loss: 0.9160096049308777, Accuracy: 1.0, Computation time: 0.9323842525482178\n",
      "Step: 3737, Loss: 0.9172613620758057, Accuracy: 1.0, Computation time: 1.1289067268371582\n",
      "Step: 3738, Loss: 0.9165170788764954, Accuracy: 1.0, Computation time: 0.8960235118865967\n",
      "Step: 3739, Loss: 0.9161074161529541, Accuracy: 1.0, Computation time: 1.0759496688842773\n",
      "Step: 3740, Loss: 0.9159923791885376, Accuracy: 1.0, Computation time: 1.086923599243164\n",
      "Step: 3741, Loss: 0.9159714579582214, Accuracy: 1.0, Computation time: 1.0106854438781738\n",
      "Step: 3742, Loss: 0.9281232953071594, Accuracy: 0.9583333730697632, Computation time: 1.908851146697998\n",
      "Step: 3743, Loss: 0.9159229397773743, Accuracy: 1.0, Computation time: 1.0631530284881592\n",
      "Step: 3744, Loss: 0.9165340662002563, Accuracy: 1.0, Computation time: 1.137895107269287\n",
      "Step: 3745, Loss: 0.9376053214073181, Accuracy: 0.875, Computation time: 1.0436701774597168\n",
      "Step: 3746, Loss: 0.9375562071800232, Accuracy: 0.9750000238418579, Computation time: 0.8615038394927979\n",
      "Step: 3747, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 0.8460257053375244\n",
      "Step: 3748, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 1.227297306060791\n",
      "Step: 3749, Loss: 0.9158721566200256, Accuracy: 1.0, Computation time: 0.9310183525085449\n",
      "Step: 3750, Loss: 0.9167698621749878, Accuracy: 1.0, Computation time: 1.110062599182129\n",
      "Step: 3751, Loss: 0.9377054572105408, Accuracy: 0.9807692766189575, Computation time: 0.9752910137176514\n",
      "Step: 3752, Loss: 0.9159014821052551, Accuracy: 1.0, Computation time: 0.8940098285675049\n",
      "########################\n",
      "Test loss: 1.1261729001998901, Test Accuracy_epoch27: 0.6874468326568604\n",
      "########################\n",
      "Step: 3753, Loss: 0.9159510731697083, Accuracy: 1.0, Computation time: 0.8947005271911621\n",
      "Step: 3754, Loss: 0.9168012738227844, Accuracy: 1.0, Computation time: 0.9571151733398438\n",
      "Step: 3755, Loss: 0.915998101234436, Accuracy: 1.0, Computation time: 0.8768773078918457\n",
      "Step: 3756, Loss: 0.9158924221992493, Accuracy: 1.0, Computation time: 0.8653411865234375\n",
      "Step: 3757, Loss: 0.9377142190933228, Accuracy: 0.9722222089767456, Computation time: 1.0582666397094727\n",
      "Step: 3758, Loss: 0.9375373125076294, Accuracy: 0.96875, Computation time: 0.9953339099884033\n",
      "Step: 3759, Loss: 0.9163110852241516, Accuracy: 1.0, Computation time: 1.1600773334503174\n",
      "Step: 3760, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 0.9709830284118652\n",
      "Step: 3761, Loss: 0.9158439040184021, Accuracy: 1.0, Computation time: 0.8608925342559814\n",
      "Step: 3762, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.026418924331665\n",
      "Step: 3763, Loss: 0.9169598817825317, Accuracy: 1.0, Computation time: 0.791738748550415\n",
      "Step: 3764, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.3169949054718018\n",
      "Step: 3765, Loss: 0.9374535083770752, Accuracy: 0.949999988079071, Computation time: 0.9433345794677734\n",
      "Step: 3766, Loss: 0.9375451803207397, Accuracy: 0.984375, Computation time: 1.0527088642120361\n",
      "Step: 3767, Loss: 0.915960967540741, Accuracy: 1.0, Computation time: 0.9866788387298584\n",
      "Step: 3768, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 1.054661512374878\n",
      "Step: 3769, Loss: 0.9327649474143982, Accuracy: 0.96875, Computation time: 1.2798504829406738\n",
      "Step: 3770, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 0.9180116653442383\n",
      "Step: 3771, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 0.8547234535217285\n",
      "Step: 3772, Loss: 0.9376059770584106, Accuracy: 0.9772727489471436, Computation time: 1.15920090675354\n",
      "Step: 3773, Loss: 0.9345536828041077, Accuracy: 0.9583333730697632, Computation time: 1.5774681568145752\n",
      "Step: 3774, Loss: 0.9376358985900879, Accuracy: 0.9791666865348816, Computation time: 1.6044433116912842\n",
      "Step: 3775, Loss: 0.9168848991394043, Accuracy: 1.0, Computation time: 0.9375746250152588\n",
      "Step: 3776, Loss: 0.9300267100334167, Accuracy: 0.9772727489471436, Computation time: 0.8233888149261475\n",
      "Step: 3777, Loss: 0.9276898503303528, Accuracy: 0.9791666865348816, Computation time: 1.5682072639465332\n",
      "Step: 3778, Loss: 0.9159620404243469, Accuracy: 1.0, Computation time: 0.8954908847808838\n",
      "Step: 3779, Loss: 0.9159427881240845, Accuracy: 1.0, Computation time: 1.027374029159546\n",
      "Step: 3780, Loss: 0.9160135984420776, Accuracy: 1.0, Computation time: 0.9015982151031494\n",
      "Step: 3781, Loss: 0.9302454590797424, Accuracy: 0.96875, Computation time: 0.9728753566741943\n",
      "Step: 3782, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 0.9898202419281006\n",
      "Step: 3783, Loss: 0.9161535501480103, Accuracy: 1.0, Computation time: 1.0528864860534668\n",
      "Step: 3784, Loss: 0.9160060286521912, Accuracy: 1.0, Computation time: 0.9530792236328125\n",
      "Step: 3785, Loss: 0.9161251187324524, Accuracy: 1.0, Computation time: 0.8485736846923828\n",
      "Step: 3786, Loss: 0.9573298096656799, Accuracy: 0.9392857551574707, Computation time: 1.1573090553283691\n",
      "Step: 3787, Loss: 0.9159175157546997, Accuracy: 1.0, Computation time: 0.9916315078735352\n",
      "Step: 3788, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 0.7953953742980957\n",
      "Step: 3789, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 0.8480396270751953\n",
      "Step: 3790, Loss: 0.9160096049308777, Accuracy: 1.0, Computation time: 0.9041507244110107\n",
      "Step: 3791, Loss: 0.9160160422325134, Accuracy: 1.0, Computation time: 0.8918154239654541\n",
      "Step: 3792, Loss: 0.9160258769989014, Accuracy: 1.0, Computation time: 0.8707695007324219\n",
      "Step: 3793, Loss: 0.9160406589508057, Accuracy: 1.0, Computation time: 1.047224521636963\n",
      "Step: 3794, Loss: 0.9160346388816833, Accuracy: 1.0, Computation time: 0.9334614276885986\n",
      "Step: 3795, Loss: 0.9159646034240723, Accuracy: 1.0, Computation time: 0.9087042808532715\n",
      "Step: 3796, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 0.9370808601379395\n",
      "Step: 3797, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 0.8559741973876953\n",
      "Step: 3798, Loss: 0.91599041223526, Accuracy: 1.0, Computation time: 1.2422363758087158\n",
      "Step: 3799, Loss: 0.9163293242454529, Accuracy: 1.0, Computation time: 0.9185390472412109\n",
      "Step: 3800, Loss: 0.9159142971038818, Accuracy: 1.0, Computation time: 0.8497655391693115\n",
      "Step: 3801, Loss: 0.9252902269363403, Accuracy: 0.9772727489471436, Computation time: 0.8581404685974121\n",
      "Step: 3802, Loss: 0.9158729910850525, Accuracy: 1.0, Computation time: 0.8543121814727783\n",
      "Step: 3803, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 0.9255077838897705\n",
      "Step: 3804, Loss: 0.9158827662467957, Accuracy: 1.0, Computation time: 0.8979043960571289\n",
      "Step: 3805, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 0.9795103073120117\n",
      "Step: 3806, Loss: 0.9159305691719055, Accuracy: 1.0, Computation time: 1.1136705875396729\n",
      "Step: 3807, Loss: 0.9375104308128357, Accuracy: 0.9791666865348816, Computation time: 0.9206292629241943\n",
      "Step: 3808, Loss: 0.9393590688705444, Accuracy: 0.9750000238418579, Computation time: 0.9379327297210693\n",
      "Step: 3809, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 0.9595282077789307\n",
      "Step: 3810, Loss: 0.9159338474273682, Accuracy: 1.0, Computation time: 0.9320392608642578\n",
      "Step: 3811, Loss: 0.9159154891967773, Accuracy: 1.0, Computation time: 0.8083698749542236\n",
      "Step: 3812, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 0.7957167625427246\n",
      "Step: 3813, Loss: 0.9160058498382568, Accuracy: 1.0, Computation time: 0.8442206382751465\n",
      "Step: 3814, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.0034773349761963\n",
      "Step: 3815, Loss: 0.9160010814666748, Accuracy: 1.0, Computation time: 1.1132640838623047\n",
      "Step: 3816, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.002645492553711\n",
      "Step: 3817, Loss: 0.9159157872200012, Accuracy: 1.0, Computation time: 0.8815720081329346\n",
      "Step: 3818, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 0.8106331825256348\n",
      "Step: 3819, Loss: 0.9261398315429688, Accuracy: 0.9722222089767456, Computation time: 1.3385531902313232\n",
      "Step: 3820, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 1.1912648677825928\n",
      "Step: 3821, Loss: 0.9169984459877014, Accuracy: 1.0, Computation time: 0.9111063480377197\n",
      "Step: 3822, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 0.8802189826965332\n",
      "Step: 3823, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 0.8139653205871582\n",
      "Step: 3824, Loss: 0.9158645868301392, Accuracy: 1.0, Computation time: 0.8024792671203613\n",
      "Step: 3825, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 0.965956449508667\n",
      "Step: 3826, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 0.8974435329437256\n",
      "Step: 3827, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.8445734977722168\n",
      "Step: 3828, Loss: 0.9166857004165649, Accuracy: 1.0, Computation time: 0.8420538902282715\n",
      "Step: 3829, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 1.4278604984283447\n",
      "Step: 3830, Loss: 0.9162309765815735, Accuracy: 1.0, Computation time: 1.175109624862671\n",
      "Step: 3831, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 1.2256388664245605\n",
      "Step: 3832, Loss: 0.9292680025100708, Accuracy: 0.949999988079071, Computation time: 1.2191927433013916\n",
      "Step: 3833, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 0.9057259559631348\n",
      "Step: 3834, Loss: 0.9180955290794373, Accuracy: 1.0, Computation time: 0.959660530090332\n",
      "Step: 3835, Loss: 0.9159110188484192, Accuracy: 1.0, Computation time: 0.8810694217681885\n",
      "Step: 3836, Loss: 0.9331480860710144, Accuracy: 0.9583333730697632, Computation time: 1.3078737258911133\n",
      "Step: 3837, Loss: 0.915923535823822, Accuracy: 1.0, Computation time: 0.9681739807128906\n",
      "Step: 3838, Loss: 0.9161294102668762, Accuracy: 1.0, Computation time: 0.9154706001281738\n",
      "Step: 3839, Loss: 0.9159711599349976, Accuracy: 1.0, Computation time: 0.8460338115692139\n",
      "Step: 3840, Loss: 0.9376433491706848, Accuracy: 0.949999988079071, Computation time: 1.145552396774292\n",
      "Step: 3841, Loss: 0.9159755706787109, Accuracy: 1.0, Computation time: 0.8564238548278809\n",
      "Step: 3842, Loss: 0.9160104990005493, Accuracy: 1.0, Computation time: 1.016369342803955\n",
      "Step: 3843, Loss: 0.9163895845413208, Accuracy: 1.0, Computation time: 0.9710500240325928\n",
      "Step: 3844, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 0.9528987407684326\n",
      "Step: 3845, Loss: 0.9374662041664124, Accuracy: 0.9750000238418579, Computation time: 0.8345074653625488\n",
      "Step: 3846, Loss: 0.9158774614334106, Accuracy: 1.0, Computation time: 0.8321242332458496\n",
      "Step: 3847, Loss: 0.9372293949127197, Accuracy: 0.9722222089767456, Computation time: 0.9760057926177979\n",
      "Step: 3848, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 0.8483016490936279\n",
      "Step: 3849, Loss: 0.937536895275116, Accuracy: 0.9750000238418579, Computation time: 0.8771007061004639\n",
      "Step: 3850, Loss: 0.937393069267273, Accuracy: 0.9642857313156128, Computation time: 0.9603798389434814\n",
      "Step: 3851, Loss: 0.9164900183677673, Accuracy: 1.0, Computation time: 0.998664140701294\n",
      "Step: 3852, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 1.0543172359466553\n",
      "Step: 3853, Loss: 0.916166365146637, Accuracy: 1.0, Computation time: 0.8991990089416504\n",
      "Step: 3854, Loss: 0.917087972164154, Accuracy: 1.0, Computation time: 0.9111661911010742\n",
      "Step: 3855, Loss: 0.9158745408058167, Accuracy: 1.0, Computation time: 0.7942211627960205\n",
      "Step: 3856, Loss: 0.9373540282249451, Accuracy: 0.9833333492279053, Computation time: 1.1350364685058594\n",
      "Step: 3857, Loss: 0.9375500679016113, Accuracy: 0.9750000238418579, Computation time: 0.9367659091949463\n",
      "Step: 3858, Loss: 0.922792375087738, Accuracy: 1.0, Computation time: 0.8438816070556641\n",
      "Step: 3859, Loss: 0.9160279631614685, Accuracy: 1.0, Computation time: 0.8020381927490234\n",
      "Step: 3860, Loss: 0.9159337878227234, Accuracy: 1.0, Computation time: 0.8439507484436035\n",
      "Step: 3861, Loss: 0.9377519488334656, Accuracy: 0.9807692766189575, Computation time: 0.8476181030273438\n",
      "Step: 3862, Loss: 0.9160027503967285, Accuracy: 1.0, Computation time: 0.9826846122741699\n",
      "Step: 3863, Loss: 0.9163423180580139, Accuracy: 1.0, Computation time: 1.0539155006408691\n",
      "Step: 3864, Loss: 0.9159227013587952, Accuracy: 1.0, Computation time: 0.7853274345397949\n",
      "Step: 3865, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 0.995337963104248\n",
      "Step: 3866, Loss: 0.9158759713172913, Accuracy: 1.0, Computation time: 1.1843504905700684\n",
      "Step: 3867, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.8190004825592041\n",
      "Step: 3868, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 1.0028603076934814\n",
      "Step: 3869, Loss: 0.9159312844276428, Accuracy: 1.0, Computation time: 0.9424307346343994\n",
      "Step: 3870, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 0.783811092376709\n",
      "Step: 3871, Loss: 0.9159193634986877, Accuracy: 1.0, Computation time: 0.9155175685882568\n",
      "Step: 3872, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 0.9095184803009033\n",
      "Step: 3873, Loss: 0.9183590412139893, Accuracy: 1.0, Computation time: 0.9271371364593506\n",
      "Step: 3874, Loss: 0.9211338758468628, Accuracy: 1.0, Computation time: 0.9882669448852539\n",
      "Step: 3875, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 0.8000428676605225\n",
      "Step: 3876, Loss: 0.937471866607666, Accuracy: 0.9750000238418579, Computation time: 0.9586308002471924\n",
      "Step: 3877, Loss: 0.9376342296600342, Accuracy: 0.9772727489471436, Computation time: 0.8457648754119873\n",
      "Step: 3878, Loss: 0.9159888029098511, Accuracy: 1.0, Computation time: 0.8855381011962891\n",
      "Step: 3879, Loss: 0.9161367416381836, Accuracy: 1.0, Computation time: 1.2062082290649414\n",
      "Step: 3880, Loss: 0.915902853012085, Accuracy: 1.0, Computation time: 0.9576480388641357\n",
      "Step: 3881, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 0.7402667999267578\n",
      "Step: 3882, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 0.6992218494415283\n",
      "Step: 3883, Loss: 0.9160656929016113, Accuracy: 1.0, Computation time: 1.4684698581695557\n",
      "Step: 3884, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 1.299933671951294\n",
      "Step: 3885, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 0.8109698295593262\n",
      "Step: 3886, Loss: 0.9171213507652283, Accuracy: 1.0, Computation time: 1.0034990310668945\n",
      "Step: 3887, Loss: 0.9158644676208496, Accuracy: 1.0, Computation time: 0.8548250198364258\n",
      "Step: 3888, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 0.764613151550293\n",
      "Step: 3889, Loss: 0.9559271335601807, Accuracy: 0.949999988079071, Computation time: 0.7641799449920654\n",
      "Step: 3890, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 0.8395116329193115\n",
      "Step: 3891, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 0.8803317546844482\n",
      "########################\n",
      "Test loss: 1.1268714666366577, Test Accuracy_epoch28: 0.6842517256736755\n",
      "########################\n",
      "Step: 3892, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 0.8452737331390381\n",
      "Step: 3893, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 0.8935577869415283\n",
      "Step: 3894, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 0.7660205364227295\n",
      "Step: 3895, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 0.8005998134613037\n",
      "Step: 3896, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 0.8644800186157227\n",
      "Step: 3897, Loss: 0.9158388376235962, Accuracy: 1.0, Computation time: 0.7617402076721191\n",
      "Step: 3898, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 0.8606057167053223\n",
      "Step: 3899, Loss: 0.9162458181381226, Accuracy: 1.0, Computation time: 0.8243048191070557\n",
      "Step: 3900, Loss: 0.9160131812095642, Accuracy: 1.0, Computation time: 0.8374936580657959\n",
      "Step: 3901, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 0.8510487079620361\n",
      "Step: 3902, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.8030803203582764\n",
      "Step: 3903, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 0.8443708419799805\n",
      "Step: 3904, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 0.8604235649108887\n",
      "Step: 3905, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 0.7384977340698242\n",
      "Step: 3906, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 0.8721747398376465\n",
      "Step: 3907, Loss: 0.9160255193710327, Accuracy: 1.0, Computation time: 1.1653048992156982\n",
      "Step: 3908, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 0.8097751140594482\n",
      "Step: 3909, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 0.7987256050109863\n",
      "Step: 3910, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 0.8847634792327881\n",
      "Step: 3911, Loss: 0.9180808067321777, Accuracy: 1.0, Computation time: 1.143474817276001\n",
      "Step: 3912, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 0.808032751083374\n",
      "Step: 3913, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 0.9813907146453857\n",
      "Step: 3914, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 1.0566613674163818\n",
      "Step: 3915, Loss: 0.9375458359718323, Accuracy: 0.9722222089767456, Computation time: 0.8768398761749268\n",
      "Step: 3916, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 0.8873410224914551\n",
      "Step: 3917, Loss: 0.9161792397499084, Accuracy: 1.0, Computation time: 1.022827386856079\n",
      "Step: 3918, Loss: 0.9364978075027466, Accuracy: 0.9642857313156128, Computation time: 1.090754747390747\n",
      "Step: 3919, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 0.9645209312438965\n",
      "Step: 3920, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 0.8408031463623047\n",
      "Step: 3921, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 0.8815357685089111\n",
      "Step: 3922, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.493260145187378\n",
      "Step: 3923, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 0.8281040191650391\n",
      "Step: 3924, Loss: 0.9375531673431396, Accuracy: 0.9772727489471436, Computation time: 0.7386283874511719\n",
      "Step: 3925, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 0.8720295429229736\n",
      "Step: 3926, Loss: 0.9180959463119507, Accuracy: 1.0, Computation time: 1.2495923042297363\n",
      "Step: 3927, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.018017053604126\n",
      "Step: 3928, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 0.8223831653594971\n",
      "Step: 3929, Loss: 0.9379928112030029, Accuracy: 0.9642857313156128, Computation time: 1.0555453300476074\n",
      "Step: 3930, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 1.0559239387512207\n",
      "Step: 3931, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 1.0253324508666992\n",
      "Step: 3932, Loss: 0.9230046272277832, Accuracy: 1.0, Computation time: 1.2359497547149658\n",
      "Step: 3933, Loss: 0.9158955216407776, Accuracy: 1.0, Computation time: 1.18381667137146\n",
      "Step: 3934, Loss: 0.9190765023231506, Accuracy: 1.0, Computation time: 1.0134766101837158\n",
      "Step: 3935, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 0.7944827079772949\n",
      "Step: 3936, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.108530044555664\n",
      "Step: 3937, Loss: 0.915945827960968, Accuracy: 1.0, Computation time: 0.9494345188140869\n",
      "Step: 3938, Loss: 0.915922999382019, Accuracy: 1.0, Computation time: 0.9649596214294434\n",
      "Step: 3939, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 0.8647787570953369\n",
      "Step: 3940, Loss: 0.9378491640090942, Accuracy: 0.9772727489471436, Computation time: 0.9333996772766113\n",
      "Step: 3941, Loss: 0.915882408618927, Accuracy: 1.0, Computation time: 0.8750650882720947\n",
      "Step: 3942, Loss: 0.9161954522132874, Accuracy: 1.0, Computation time: 1.0047996044158936\n",
      "Step: 3943, Loss: 0.9188469648361206, Accuracy: 1.0, Computation time: 1.0367560386657715\n",
      "Step: 3944, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 0.9596133232116699\n",
      "Step: 3945, Loss: 0.9374894499778748, Accuracy: 0.9772727489471436, Computation time: 0.9304213523864746\n",
      "Step: 3946, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 1.0631723403930664\n",
      "Step: 3947, Loss: 0.9158861637115479, Accuracy: 1.0, Computation time: 0.8999714851379395\n",
      "Step: 3948, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 0.9523775577545166\n",
      "Step: 3949, Loss: 0.9250442981719971, Accuracy: 1.0, Computation time: 1.0771598815917969\n",
      "Step: 3950, Loss: 0.9376215934753418, Accuracy: 0.9583333730697632, Computation time: 0.9168417453765869\n",
      "Step: 3951, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 0.856238603591919\n",
      "Step: 3952, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 0.9990503787994385\n",
      "Step: 3953, Loss: 0.9160289168357849, Accuracy: 1.0, Computation time: 0.9862222671508789\n",
      "Step: 3954, Loss: 0.9197700619697571, Accuracy: 1.0, Computation time: 1.4262287616729736\n",
      "Step: 3955, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 0.8525896072387695\n",
      "Step: 3956, Loss: 0.9375621676445007, Accuracy: 0.9722222089767456, Computation time: 0.8011798858642578\n",
      "Step: 3957, Loss: 0.9159769415855408, Accuracy: 1.0, Computation time: 1.1290693283081055\n",
      "Step: 3958, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 1.001589059829712\n",
      "Step: 3959, Loss: 0.9159497618675232, Accuracy: 1.0, Computation time: 0.9207837581634521\n",
      "Step: 3960, Loss: 0.9592170119285583, Accuracy: 0.9375, Computation time: 0.9426467418670654\n",
      "Step: 3961, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 0.9471554756164551\n",
      "Step: 3962, Loss: 0.9160711169242859, Accuracy: 1.0, Computation time: 0.7872910499572754\n",
      "Step: 3963, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.0041906833648682\n",
      "Step: 3964, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 0.8877227306365967\n",
      "Step: 3965, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 0.8522641658782959\n",
      "Step: 3966, Loss: 0.9158973097801208, Accuracy: 1.0, Computation time: 1.0815792083740234\n",
      "Step: 3967, Loss: 0.9159782528877258, Accuracy: 1.0, Computation time: 0.8256444931030273\n",
      "Step: 3968, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.0905485153198242\n",
      "Step: 3969, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 1.0397515296936035\n",
      "Step: 3970, Loss: 0.9158939123153687, Accuracy: 1.0, Computation time: 0.8140997886657715\n",
      "Step: 3971, Loss: 0.9411303400993347, Accuracy: 0.9722222089767456, Computation time: 1.155435562133789\n",
      "Step: 3972, Loss: 0.9159055948257446, Accuracy: 1.0, Computation time: 1.081510066986084\n",
      "Step: 3973, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 0.8122248649597168\n",
      "Step: 3974, Loss: 0.9242504835128784, Accuracy: 1.0, Computation time: 1.0635743141174316\n",
      "Step: 3975, Loss: 0.9159271121025085, Accuracy: 1.0, Computation time: 0.9011578559875488\n",
      "Step: 3976, Loss: 0.916029155254364, Accuracy: 1.0, Computation time: 1.0695877075195312\n",
      "Step: 3977, Loss: 0.9162522554397583, Accuracy: 1.0, Computation time: 0.936464786529541\n",
      "Step: 3978, Loss: 0.9159867167472839, Accuracy: 1.0, Computation time: 1.0190174579620361\n",
      "Step: 3979, Loss: 0.9160659909248352, Accuracy: 1.0, Computation time: 0.9356422424316406\n",
      "Step: 3980, Loss: 0.9159578084945679, Accuracy: 1.0, Computation time: 0.9490103721618652\n",
      "Step: 3981, Loss: 0.9167571663856506, Accuracy: 1.0, Computation time: 1.3277544975280762\n",
      "Step: 3982, Loss: 0.9159145355224609, Accuracy: 1.0, Computation time: 0.8648688793182373\n",
      "Step: 3983, Loss: 0.9162440299987793, Accuracy: 1.0, Computation time: 0.9811499118804932\n",
      "Step: 3984, Loss: 0.9159857034683228, Accuracy: 1.0, Computation time: 0.9187607765197754\n",
      "Step: 3985, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 0.7653853893280029\n",
      "Step: 3986, Loss: 0.9376876354217529, Accuracy: 0.9642857313156128, Computation time: 0.9055237770080566\n",
      "Step: 3987, Loss: 0.9162053465843201, Accuracy: 1.0, Computation time: 1.7423319816589355\n",
      "Step: 3988, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 0.9933421611785889\n",
      "Step: 3989, Loss: 0.9159659743309021, Accuracy: 1.0, Computation time: 1.1878554821014404\n",
      "Step: 3990, Loss: 0.9376864433288574, Accuracy: 0.9750000238418579, Computation time: 0.9142587184906006\n",
      "Step: 3991, Loss: 0.9175708293914795, Accuracy: 1.0, Computation time: 1.1592566967010498\n",
      "Step: 3992, Loss: 0.9168261289596558, Accuracy: 1.0, Computation time: 0.9899990558624268\n",
      "Step: 3993, Loss: 0.9333834648132324, Accuracy: 0.9772727489471436, Computation time: 1.010251522064209\n",
      "Step: 3994, Loss: 0.9376572966575623, Accuracy: 0.96875, Computation time: 0.9757711887359619\n",
      "Step: 3995, Loss: 0.9161415100097656, Accuracy: 1.0, Computation time: 0.8642013072967529\n",
      "Step: 3996, Loss: 0.9164299368858337, Accuracy: 1.0, Computation time: 0.9198048114776611\n",
      "Step: 3997, Loss: 0.9160786271095276, Accuracy: 1.0, Computation time: 0.9706861972808838\n",
      "Step: 3998, Loss: 0.9160802960395813, Accuracy: 1.0, Computation time: 1.0905213356018066\n",
      "Step: 3999, Loss: 0.9160791039466858, Accuracy: 1.0, Computation time: 0.9887456893920898\n",
      "Step: 4000, Loss: 0.9160416126251221, Accuracy: 1.0, Computation time: 0.9695711135864258\n",
      "Step: 4001, Loss: 0.9161913394927979, Accuracy: 1.0, Computation time: 1.2595043182373047\n",
      "Step: 4002, Loss: 0.9325271248817444, Accuracy: 0.9772727489471436, Computation time: 1.0407280921936035\n",
      "Step: 4003, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 0.8391709327697754\n",
      "Step: 4004, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 0.8542253971099854\n",
      "Step: 4005, Loss: 0.9377923011779785, Accuracy: 0.9772727489471436, Computation time: 1.1214969158172607\n",
      "Step: 4006, Loss: 0.9174458980560303, Accuracy: 1.0, Computation time: 0.9882428646087646\n",
      "Step: 4007, Loss: 0.9160819053649902, Accuracy: 1.0, Computation time: 1.355588674545288\n",
      "Step: 4008, Loss: 0.9159396290779114, Accuracy: 1.0, Computation time: 1.1707305908203125\n",
      "Step: 4009, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 0.9991378784179688\n",
      "Step: 4010, Loss: 0.9226131439208984, Accuracy: 1.0, Computation time: 0.9488370418548584\n",
      "Step: 4011, Loss: 0.9159004092216492, Accuracy: 1.0, Computation time: 0.89847731590271\n",
      "Step: 4012, Loss: 0.9374238848686218, Accuracy: 0.9807692766189575, Computation time: 1.1270017623901367\n",
      "Step: 4013, Loss: 0.9159730076789856, Accuracy: 1.0, Computation time: 0.9445466995239258\n",
      "Step: 4014, Loss: 0.9159719347953796, Accuracy: 1.0, Computation time: 0.8224995136260986\n",
      "Step: 4015, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.169062852859497\n",
      "Step: 4016, Loss: 0.9159899353981018, Accuracy: 1.0, Computation time: 1.0759527683258057\n",
      "Step: 4017, Loss: 0.9169500470161438, Accuracy: 1.0, Computation time: 0.9039411544799805\n",
      "Step: 4018, Loss: 0.9376276135444641, Accuracy: 0.96875, Computation time: 1.0403242111206055\n",
      "Step: 4019, Loss: 0.9742152690887451, Accuracy: 0.860119104385376, Computation time: 1.325207233428955\n",
      "Step: 4020, Loss: 0.9369120597839355, Accuracy: 0.9791666865348816, Computation time: 0.930823564529419\n",
      "Step: 4021, Loss: 0.9354786276817322, Accuracy: 0.96875, Computation time: 0.9123101234436035\n",
      "Step: 4022, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 0.9497721195220947\n",
      "Step: 4023, Loss: 0.9160721302032471, Accuracy: 1.0, Computation time: 1.0385980606079102\n",
      "Step: 4024, Loss: 0.9377381801605225, Accuracy: 0.9833333492279053, Computation time: 0.9304976463317871\n",
      "Step: 4025, Loss: 0.9159653186798096, Accuracy: 1.0, Computation time: 0.8307394981384277\n",
      "Step: 4026, Loss: 0.9160529971122742, Accuracy: 1.0, Computation time: 0.9406368732452393\n",
      "Step: 4027, Loss: 0.9158990979194641, Accuracy: 1.0, Computation time: 1.2471511363983154\n",
      "Step: 4028, Loss: 0.9366165399551392, Accuracy: 0.96875, Computation time: 0.9040498733520508\n",
      "Step: 4029, Loss: 0.916331946849823, Accuracy: 1.0, Computation time: 1.0599782466888428\n",
      "Step: 4030, Loss: 0.9159148931503296, Accuracy: 1.0, Computation time: 1.0519499778747559\n",
      "########################\n",
      "Test loss: 1.1283259391784668, Test Accuracy_epoch29: 0.6783047318458557\n",
      "########################\n",
      "Step: 4031, Loss: 0.9161999225616455, Accuracy: 1.0, Computation time: 1.0772240161895752\n",
      "Step: 4032, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 0.822282075881958\n",
      "Step: 4033, Loss: 0.9519392848014832, Accuracy: 0.9409722089767456, Computation time: 1.160651683807373\n",
      "Step: 4034, Loss: 0.9375850558280945, Accuracy: 0.9583333730697632, Computation time: 1.0400795936584473\n",
      "Step: 4035, Loss: 0.9159139394760132, Accuracy: 1.0, Computation time: 0.9410426616668701\n",
      "Step: 4036, Loss: 0.9214553236961365, Accuracy: 1.0, Computation time: 1.5918569564819336\n",
      "Step: 4037, Loss: 0.9158951640129089, Accuracy: 1.0, Computation time: 0.9705033302307129\n",
      "Step: 4038, Loss: 0.9159536361694336, Accuracy: 1.0, Computation time: 0.8505327701568604\n",
      "Step: 4039, Loss: 0.9159095883369446, Accuracy: 1.0, Computation time: 0.8085253238677979\n",
      "Step: 4040, Loss: 0.9162306189537048, Accuracy: 1.0, Computation time: 0.9971795082092285\n",
      "Step: 4041, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 0.8158249855041504\n",
      "Step: 4042, Loss: 0.9335049390792847, Accuracy: 0.9583333730697632, Computation time: 0.9922146797180176\n",
      "Step: 4043, Loss: 0.9159461259841919, Accuracy: 1.0, Computation time: 0.8324689865112305\n",
      "Step: 4044, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 0.8498716354370117\n",
      "Step: 4045, Loss: 0.9372187852859497, Accuracy: 0.9583333730697632, Computation time: 0.9527068138122559\n",
      "Step: 4046, Loss: 0.9159548282623291, Accuracy: 1.0, Computation time: 0.9853811264038086\n",
      "Step: 4047, Loss: 0.9162665605545044, Accuracy: 1.0, Computation time: 0.8910057544708252\n",
      "Step: 4048, Loss: 0.9375531077384949, Accuracy: 0.9833333492279053, Computation time: 0.8439834117889404\n",
      "Step: 4049, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 0.9287741184234619\n",
      "Step: 4050, Loss: 0.9160439372062683, Accuracy: 1.0, Computation time: 1.034989833831787\n",
      "Step: 4051, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 0.9007854461669922\n",
      "Step: 4052, Loss: 0.9375811815261841, Accuracy: 0.9807692766189575, Computation time: 0.8837451934814453\n",
      "Step: 4053, Loss: 0.9158716201782227, Accuracy: 1.0, Computation time: 0.8412723541259766\n",
      "Step: 4054, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 0.7821872234344482\n",
      "Step: 4055, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 1.0505626201629639\n",
      "Step: 4056, Loss: 0.9158836007118225, Accuracy: 1.0, Computation time: 0.8655517101287842\n",
      "Step: 4057, Loss: 0.9158642888069153, Accuracy: 1.0, Computation time: 0.9381599426269531\n",
      "Step: 4058, Loss: 0.9375889301300049, Accuracy: 0.9722222089767456, Computation time: 0.8049044609069824\n",
      "Step: 4059, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 0.7567565441131592\n",
      "Step: 4060, Loss: 0.9165080189704895, Accuracy: 1.0, Computation time: 0.8844442367553711\n",
      "Step: 4061, Loss: 0.9159529209136963, Accuracy: 1.0, Computation time: 1.066112756729126\n",
      "Step: 4062, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 0.9015953540802002\n",
      "Step: 4063, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 1.0681631565093994\n",
      "Step: 4064, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.1225271224975586\n",
      "Step: 4065, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 0.8652245998382568\n",
      "Step: 4066, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 0.7808115482330322\n",
      "Step: 4067, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 0.8596899509429932\n",
      "Step: 4068, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 0.8605043888092041\n",
      "Step: 4069, Loss: 0.9451064467430115, Accuracy: 0.9852941036224365, Computation time: 1.2263200283050537\n",
      "Step: 4070, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 0.944521427154541\n",
      "Step: 4071, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 0.8497061729431152\n",
      "Step: 4072, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 1.017667293548584\n",
      "Step: 4073, Loss: 0.9159667491912842, Accuracy: 1.0, Computation time: 1.3312098979949951\n",
      "Step: 4074, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 0.917365312576294\n",
      "Step: 4075, Loss: 0.9158433675765991, Accuracy: 1.0, Computation time: 0.9621362686157227\n",
      "Step: 4076, Loss: 0.9158538579940796, Accuracy: 1.0, Computation time: 0.8082969188690186\n",
      "Step: 4077, Loss: 0.9374619126319885, Accuracy: 0.9722222089767456, Computation time: 1.098323106765747\n",
      "Step: 4078, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 0.8792026042938232\n",
      "Step: 4079, Loss: 0.9228439927101135, Accuracy: 1.0, Computation time: 0.8504343032836914\n",
      "Step: 4080, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 0.9721536636352539\n",
      "Step: 4081, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 0.8758528232574463\n",
      "Step: 4082, Loss: 0.9374725818634033, Accuracy: 0.9772727489471436, Computation time: 0.9129445552825928\n",
      "Step: 4083, Loss: 0.9158852100372314, Accuracy: 1.0, Computation time: 0.8675270080566406\n",
      "Step: 4084, Loss: 0.9205841422080994, Accuracy: 1.0, Computation time: 1.324660062789917\n",
      "Step: 4085, Loss: 0.9158790707588196, Accuracy: 1.0, Computation time: 0.9482324123382568\n",
      "Step: 4086, Loss: 0.9159165620803833, Accuracy: 1.0, Computation time: 1.3758070468902588\n",
      "Step: 4087, Loss: 0.9158995151519775, Accuracy: 1.0, Computation time: 0.9200263023376465\n",
      "Step: 4088, Loss: 0.9159204959869385, Accuracy: 1.0, Computation time: 1.0172772407531738\n",
      "Step: 4089, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 1.115631103515625\n",
      "Step: 4090, Loss: 0.9164825081825256, Accuracy: 1.0, Computation time: 1.119142770767212\n",
      "Step: 4091, Loss: 0.9690839052200317, Accuracy: 0.9333333373069763, Computation time: 1.3147649765014648\n",
      "Step: 4092, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 0.8632731437683105\n",
      "Step: 4093, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 0.914989709854126\n",
      "Step: 4094, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 0.8353703022003174\n",
      "Step: 4095, Loss: 0.9163380861282349, Accuracy: 1.0, Computation time: 1.0259244441986084\n",
      "Step: 4096, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 0.8786869049072266\n",
      "Step: 4097, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 0.8395552635192871\n",
      "Step: 4098, Loss: 0.9159974455833435, Accuracy: 1.0, Computation time: 0.7778410911560059\n",
      "Step: 4099, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 0.9639275074005127\n",
      "Step: 4100, Loss: 0.923754096031189, Accuracy: 1.0, Computation time: 1.0348730087280273\n",
      "Step: 4101, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 1.4440240859985352\n",
      "Step: 4102, Loss: 0.9231783747673035, Accuracy: 1.0, Computation time: 1.1017322540283203\n",
      "Step: 4103, Loss: 0.9158885478973389, Accuracy: 1.0, Computation time: 0.8511073589324951\n",
      "Step: 4104, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 0.9844098091125488\n",
      "Step: 4105, Loss: 0.9187083840370178, Accuracy: 1.0, Computation time: 0.8050041198730469\n",
      "Step: 4106, Loss: 0.916016697883606, Accuracy: 1.0, Computation time: 0.9395432472229004\n",
      "Step: 4107, Loss: 0.9159722924232483, Accuracy: 1.0, Computation time: 1.4044668674468994\n",
      "Step: 4108, Loss: 0.9377526044845581, Accuracy: 0.96875, Computation time: 0.8885030746459961\n",
      "Step: 4109, Loss: 0.9159227013587952, Accuracy: 1.0, Computation time: 0.8017933368682861\n",
      "Step: 4110, Loss: 0.9159201383590698, Accuracy: 1.0, Computation time: 0.8828434944152832\n",
      "Step: 4111, Loss: 0.9324589371681213, Accuracy: 0.949999988079071, Computation time: 0.8973324298858643\n",
      "Step: 4112, Loss: 0.9158901572227478, Accuracy: 1.0, Computation time: 0.8831391334533691\n",
      "Step: 4113, Loss: 0.9647226333618164, Accuracy: 0.9479166865348816, Computation time: 1.3774151802062988\n",
      "Step: 4114, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 0.8180804252624512\n",
      "Step: 4115, Loss: 0.9158920645713806, Accuracy: 1.0, Computation time: 0.9209787845611572\n",
      "Step: 4116, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 0.7978308200836182\n",
      "Step: 4117, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 0.8760783672332764\n",
      "Step: 4118, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 0.8784964084625244\n",
      "Step: 4119, Loss: 0.9370283484458923, Accuracy: 0.9772727489471436, Computation time: 1.2774133682250977\n",
      "Step: 4120, Loss: 0.9268063902854919, Accuracy: 0.96875, Computation time: 1.0570271015167236\n",
      "Step: 4121, Loss: 0.9159783720970154, Accuracy: 1.0, Computation time: 1.0837340354919434\n",
      "Step: 4122, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.7968623638153076\n",
      "Step: 4123, Loss: 0.9161136150360107, Accuracy: 1.0, Computation time: 0.8330302238464355\n",
      "Step: 4124, Loss: 0.9203569889068604, Accuracy: 1.0, Computation time: 0.8074305057525635\n",
      "Step: 4125, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 0.9544539451599121\n",
      "Step: 4126, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.3514018058776855\n",
      "Step: 4127, Loss: 0.9160321950912476, Accuracy: 1.0, Computation time: 1.0036354064941406\n",
      "Step: 4128, Loss: 0.9195283055305481, Accuracy: 1.0, Computation time: 1.1594667434692383\n",
      "Step: 4129, Loss: 0.9164762496948242, Accuracy: 1.0, Computation time: 0.9917430877685547\n",
      "Step: 4130, Loss: 0.9184566140174866, Accuracy: 1.0, Computation time: 1.40183424949646\n",
      "Step: 4131, Loss: 0.9160365462303162, Accuracy: 1.0, Computation time: 0.784609317779541\n",
      "Step: 4132, Loss: 0.9161177277565002, Accuracy: 1.0, Computation time: 0.9313733577728271\n",
      "Step: 4133, Loss: 0.9159101843833923, Accuracy: 1.0, Computation time: 0.849973201751709\n",
      "Step: 4134, Loss: 0.9158541560173035, Accuracy: 1.0, Computation time: 0.851193904876709\n",
      "Step: 4135, Loss: 0.91856849193573, Accuracy: 1.0, Computation time: 1.2913942337036133\n",
      "Step: 4136, Loss: 0.9160165190696716, Accuracy: 1.0, Computation time: 0.971189022064209\n",
      "Step: 4137, Loss: 0.9159961938858032, Accuracy: 1.0, Computation time: 1.0968968868255615\n",
      "Step: 4138, Loss: 0.9161316156387329, Accuracy: 1.0, Computation time: 1.0242373943328857\n",
      "Step: 4139, Loss: 0.915972888469696, Accuracy: 1.0, Computation time: 0.8662421703338623\n",
      "Step: 4140, Loss: 0.9162851572036743, Accuracy: 1.0, Computation time: 1.0469858646392822\n",
      "Step: 4141, Loss: 0.9159095287322998, Accuracy: 1.0, Computation time: 1.2823495864868164\n",
      "Step: 4142, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 0.9935424327850342\n",
      "Step: 4143, Loss: 0.9160181283950806, Accuracy: 1.0, Computation time: 1.814500331878662\n",
      "Step: 4144, Loss: 0.9158508777618408, Accuracy: 1.0, Computation time: 0.8988044261932373\n",
      "Step: 4145, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 0.9750790596008301\n",
      "Step: 4146, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 0.8728070259094238\n",
      "Step: 4147, Loss: 0.9375154972076416, Accuracy: 0.9583333730697632, Computation time: 0.9573497772216797\n",
      "Step: 4148, Loss: 0.9159447550773621, Accuracy: 1.0, Computation time: 0.9056487083435059\n",
      "Step: 4149, Loss: 0.9158663153648376, Accuracy: 1.0, Computation time: 0.8562381267547607\n",
      "Step: 4150, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 0.9423360824584961\n",
      "Step: 4151, Loss: 0.937550961971283, Accuracy: 0.9833333492279053, Computation time: 1.0283405780792236\n",
      "Step: 4152, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 0.8841121196746826\n",
      "Step: 4153, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 0.9135172367095947\n",
      "Step: 4154, Loss: 0.9161295294761658, Accuracy: 1.0, Computation time: 0.8770053386688232\n",
      "Step: 4155, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 0.8768022060394287\n",
      "Step: 4156, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 0.9826397895812988\n",
      "Step: 4157, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.9560403823852539\n",
      "Step: 4158, Loss: 0.9166964292526245, Accuracy: 1.0, Computation time: 1.7939257621765137\n",
      "Step: 4159, Loss: 0.9374925494194031, Accuracy: 0.9722222089767456, Computation time: 0.9629659652709961\n",
      "Step: 4160, Loss: 0.9158617854118347, Accuracy: 1.0, Computation time: 1.1430089473724365\n",
      "Step: 4161, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 0.8606231212615967\n",
      "Step: 4162, Loss: 0.9166244864463806, Accuracy: 1.0, Computation time: 1.2959578037261963\n",
      "Step: 4163, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 0.976064920425415\n",
      "Step: 4164, Loss: 0.9159241318702698, Accuracy: 1.0, Computation time: 1.1829071044921875\n",
      "Step: 4165, Loss: 0.9160352945327759, Accuracy: 1.0, Computation time: 1.3938877582550049\n",
      "Step: 4166, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 0.9859271049499512\n",
      "Step: 4167, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.5234017372131348\n",
      "Step: 4168, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 1.0913786888122559\n",
      "Step: 4169, Loss: 0.9375423192977905, Accuracy: 0.9807692766189575, Computation time: 0.8836979866027832\n",
      "########################\n",
      "Test loss: 1.1271071434020996, Test Accuracy_epoch30: 0.6846394538879395\n",
      "########################\n",
      "Step: 4170, Loss: 0.9158588647842407, Accuracy: 1.0, Computation time: 0.9767663478851318\n",
      "Step: 4171, Loss: 0.9158524870872498, Accuracy: 1.0, Computation time: 1.2082006931304932\n",
      "Step: 4172, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 0.9399278163909912\n",
      "Step: 4173, Loss: 0.9158377051353455, Accuracy: 1.0, Computation time: 1.084083080291748\n",
      "Step: 4174, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.0867397785186768\n",
      "Step: 4175, Loss: 0.9373228549957275, Accuracy: 0.9750000238418579, Computation time: 1.21669340133667\n",
      "Step: 4176, Loss: 0.937476396560669, Accuracy: 0.9642857313156128, Computation time: 1.0623648166656494\n",
      "Step: 4177, Loss: 0.9375644326210022, Accuracy: 0.96875, Computation time: 0.8512983322143555\n",
      "Step: 4178, Loss: 0.9158627986907959, Accuracy: 1.0, Computation time: 1.171806812286377\n",
      "Step: 4179, Loss: 0.9376299381256104, Accuracy: 0.96875, Computation time: 0.9722096920013428\n",
      "Step: 4180, Loss: 0.9375338554382324, Accuracy: 0.9722222089767456, Computation time: 1.0757956504821777\n",
      "Step: 4181, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 0.8693747520446777\n",
      "Step: 4182, Loss: 0.9375428557395935, Accuracy: 0.9807692766189575, Computation time: 0.9503645896911621\n",
      "Step: 4183, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 0.8531920909881592\n",
      "Step: 4184, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 0.8852925300598145\n",
      "Step: 4185, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 1.1738934516906738\n",
      "Step: 4186, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 0.885037899017334\n",
      "Step: 4187, Loss: 0.9158480763435364, Accuracy: 1.0, Computation time: 0.9431936740875244\n",
      "Step: 4188, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.2845335006713867\n",
      "Step: 4189, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 0.8511722087860107\n",
      "Step: 4190, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 1.3559041023254395\n",
      "Step: 4191, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.0928747653961182\n",
      "Step: 4192, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 0.8885796070098877\n",
      "Step: 4193, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 1.092677354812622\n",
      "Step: 4194, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.9828541278839111\n",
      "Step: 4195, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 1.8079233169555664\n",
      "Step: 4196, Loss: 0.9158361554145813, Accuracy: 1.0, Computation time: 0.9448025226593018\n",
      "Step: 4197, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 1.1044001579284668\n",
      "Step: 4198, Loss: 0.9158394932746887, Accuracy: 1.0, Computation time: 0.878995418548584\n",
      "Step: 4199, Loss: 0.9159433245658875, Accuracy: 1.0, Computation time: 1.0993738174438477\n",
      "Step: 4200, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 1.3919625282287598\n",
      "Step: 4201, Loss: 0.935675323009491, Accuracy: 0.96875, Computation time: 1.1023306846618652\n",
      "Step: 4202, Loss: 0.9484065771102905, Accuracy: 0.9166666865348816, Computation time: 0.9452009201049805\n",
      "Step: 4203, Loss: 0.9376254677772522, Accuracy: 0.9772727489471436, Computation time: 1.0921273231506348\n",
      "Step: 4204, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 0.905327558517456\n",
      "Step: 4205, Loss: 0.9160274863243103, Accuracy: 1.0, Computation time: 0.9377620220184326\n",
      "Step: 4206, Loss: 0.9160601496696472, Accuracy: 1.0, Computation time: 1.2326905727386475\n",
      "Step: 4207, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 0.7653610706329346\n",
      "Step: 4208, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 0.8291008472442627\n",
      "Step: 4209, Loss: 0.9162903428077698, Accuracy: 1.0, Computation time: 1.1576004028320312\n",
      "Step: 4210, Loss: 0.9370147585868835, Accuracy: 0.96875, Computation time: 0.8632609844207764\n",
      "Step: 4211, Loss: 0.915948212146759, Accuracy: 1.0, Computation time: 0.8233833312988281\n",
      "Step: 4212, Loss: 0.916155993938446, Accuracy: 1.0, Computation time: 1.1119961738586426\n",
      "Step: 4213, Loss: 0.9158526062965393, Accuracy: 1.0, Computation time: 0.9384381771087646\n",
      "Step: 4214, Loss: 0.9158483743667603, Accuracy: 1.0, Computation time: 0.8216469287872314\n",
      "Step: 4215, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 0.9130308628082275\n",
      "Step: 4216, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 0.9982974529266357\n",
      "Step: 4217, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 0.8992998600006104\n",
      "Step: 4218, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 0.8792483806610107\n",
      "Step: 4219, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 1.2568359375\n",
      "Step: 4220, Loss: 0.9158398509025574, Accuracy: 1.0, Computation time: 0.9838495254516602\n",
      "Step: 4221, Loss: 0.9158405065536499, Accuracy: 1.0, Computation time: 0.9014327526092529\n",
      "Step: 4222, Loss: 0.9158487319946289, Accuracy: 1.0, Computation time: 1.2330665588378906\n",
      "Step: 4223, Loss: 0.9158377051353455, Accuracy: 1.0, Computation time: 0.7437014579772949\n",
      "Step: 4224, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 0.9323019981384277\n",
      "Step: 4225, Loss: 0.9158356785774231, Accuracy: 1.0, Computation time: 0.8352601528167725\n",
      "Step: 4226, Loss: 0.916067361831665, Accuracy: 1.0, Computation time: 0.8941240310668945\n",
      "Step: 4227, Loss: 0.9177436232566833, Accuracy: 1.0, Computation time: 0.8512921333312988\n",
      "Step: 4228, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 0.8281924724578857\n",
      "Step: 4229, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.1700737476348877\n",
      "Step: 4230, Loss: 0.9158504605293274, Accuracy: 1.0, Computation time: 0.7595477104187012\n",
      "Step: 4231, Loss: 0.9374195337295532, Accuracy: 0.9821428656578064, Computation time: 0.8075790405273438\n",
      "Step: 4232, Loss: 0.9489145874977112, Accuracy: 0.9500000476837158, Computation time: 1.2862889766693115\n",
      "Step: 4233, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.026714563369751\n",
      "Step: 4234, Loss: 0.9375132918357849, Accuracy: 0.9772727489471436, Computation time: 1.133960485458374\n",
      "Step: 4235, Loss: 0.9158880710601807, Accuracy: 1.0, Computation time: 1.3091318607330322\n",
      "Step: 4236, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 0.9545302391052246\n",
      "Step: 4237, Loss: 0.9247110486030579, Accuracy: 1.0, Computation time: 1.2116878032684326\n",
      "Step: 4238, Loss: 0.9159162640571594, Accuracy: 1.0, Computation time: 0.9716029167175293\n",
      "Step: 4239, Loss: 0.9159317016601562, Accuracy: 1.0, Computation time: 1.064702033996582\n",
      "Step: 4240, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 0.9098987579345703\n",
      "Step: 4241, Loss: 0.9159886837005615, Accuracy: 1.0, Computation time: 0.9587593078613281\n",
      "Step: 4242, Loss: 0.9159305691719055, Accuracy: 1.0, Computation time: 0.775170087814331\n",
      "Step: 4243, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 0.8726778030395508\n",
      "Step: 4244, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 0.9326198101043701\n",
      "Step: 4245, Loss: 0.937545657157898, Accuracy: 0.9642857313156128, Computation time: 0.894740104675293\n",
      "Step: 4246, Loss: 0.9159020781517029, Accuracy: 1.0, Computation time: 0.9727470874786377\n",
      "Step: 4247, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 0.928417444229126\n",
      "Step: 4248, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 0.8758726119995117\n",
      "Step: 4249, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 1.0270755290985107\n",
      "Step: 4250, Loss: 0.9174511432647705, Accuracy: 1.0, Computation time: 1.3078563213348389\n",
      "Step: 4251, Loss: 0.9576286673545837, Accuracy: 0.9472222328186035, Computation time: 1.0423038005828857\n",
      "Step: 4252, Loss: 0.9158795475959778, Accuracy: 1.0, Computation time: 0.8995230197906494\n",
      "Step: 4253, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 0.9308090209960938\n",
      "Step: 4254, Loss: 0.937635064125061, Accuracy: 0.9750000238418579, Computation time: 1.0781264305114746\n",
      "Step: 4255, Loss: 0.9164304137229919, Accuracy: 1.0, Computation time: 1.0317060947418213\n",
      "Step: 4256, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 1.0632786750793457\n",
      "Step: 4257, Loss: 0.9343301057815552, Accuracy: 0.9722222089767456, Computation time: 0.9920303821563721\n",
      "Step: 4258, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 1.1021592617034912\n",
      "Step: 4259, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 0.9211866855621338\n",
      "Step: 4260, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.151587724685669\n",
      "Step: 4261, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 1.0358941555023193\n",
      "Step: 4262, Loss: 0.9167747497558594, Accuracy: 1.0, Computation time: 1.4684524536132812\n",
      "Step: 4263, Loss: 0.9375956058502197, Accuracy: 0.9772727489471436, Computation time: 1.0044441223144531\n",
      "Step: 4264, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 1.0023231506347656\n",
      "Step: 4265, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 0.8925662040710449\n",
      "Step: 4266, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 0.7865209579467773\n",
      "Step: 4267, Loss: 0.9373618960380554, Accuracy: 0.9722222089767456, Computation time: 0.8619105815887451\n",
      "Step: 4268, Loss: 0.9159709215164185, Accuracy: 1.0, Computation time: 1.048774003982544\n",
      "Step: 4269, Loss: 0.9159984588623047, Accuracy: 1.0, Computation time: 0.9825100898742676\n",
      "Step: 4270, Loss: 0.9158764481544495, Accuracy: 1.0, Computation time: 0.7682850360870361\n",
      "Step: 4271, Loss: 0.9375951290130615, Accuracy: 0.949999988079071, Computation time: 0.8717501163482666\n",
      "Step: 4272, Loss: 0.9158630967140198, Accuracy: 1.0, Computation time: 0.8155949115753174\n",
      "Step: 4273, Loss: 0.9158695340156555, Accuracy: 1.0, Computation time: 1.1678316593170166\n",
      "Step: 4274, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 1.3978631496429443\n",
      "Step: 4275, Loss: 0.9158653020858765, Accuracy: 1.0, Computation time: 0.9006397724151611\n",
      "Step: 4276, Loss: 0.9375787973403931, Accuracy: 0.9750000238418579, Computation time: 0.8397321701049805\n",
      "Step: 4277, Loss: 0.9159277677536011, Accuracy: 1.0, Computation time: 1.3295164108276367\n",
      "Step: 4278, Loss: 0.916135847568512, Accuracy: 1.0, Computation time: 0.9326601028442383\n",
      "Step: 4279, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 0.927335262298584\n",
      "Step: 4280, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 0.8160617351531982\n",
      "Step: 4281, Loss: 0.9369790554046631, Accuracy: 0.9807692766189575, Computation time: 1.4408280849456787\n",
      "Step: 4282, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 0.8452334403991699\n",
      "Step: 4283, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 0.8775434494018555\n",
      "Step: 4284, Loss: 0.915851891040802, Accuracy: 1.0, Computation time: 0.797741174697876\n",
      "Step: 4285, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 0.9392967224121094\n",
      "Step: 4286, Loss: 0.9159274101257324, Accuracy: 1.0, Computation time: 0.9271817207336426\n",
      "Step: 4287, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 0.9508297443389893\n",
      "Step: 4288, Loss: 0.937455952167511, Accuracy: 0.9722222089767456, Computation time: 0.789700984954834\n",
      "Step: 4289, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.9476099014282227\n",
      "Step: 4290, Loss: 0.937560498714447, Accuracy: 0.9722222089767456, Computation time: 0.8701760768890381\n",
      "Step: 4291, Loss: 0.9375811815261841, Accuracy: 0.9833333492279053, Computation time: 0.9054601192474365\n",
      "Step: 4292, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 0.8451712131500244\n",
      "Step: 4293, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 0.8769662380218506\n",
      "Step: 4294, Loss: 0.9158937335014343, Accuracy: 1.0, Computation time: 0.8609440326690674\n",
      "Step: 4295, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 0.9681975841522217\n",
      "Step: 4296, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 0.8444271087646484\n",
      "Step: 4297, Loss: 0.937217652797699, Accuracy: 0.9821428656578064, Computation time: 1.0677926540374756\n",
      "Step: 4298, Loss: 0.9158595204353333, Accuracy: 1.0, Computation time: 0.931847095489502\n",
      "Step: 4299, Loss: 0.9158617258071899, Accuracy: 1.0, Computation time: 0.791013240814209\n",
      "Step: 4300, Loss: 0.9378051161766052, Accuracy: 0.9642857313156128, Computation time: 1.0929155349731445\n",
      "Step: 4301, Loss: 0.915962815284729, Accuracy: 1.0, Computation time: 0.8593993186950684\n",
      "Step: 4302, Loss: 0.9164860248565674, Accuracy: 1.0, Computation time: 1.0289771556854248\n",
      "Step: 4303, Loss: 0.9159044623374939, Accuracy: 1.0, Computation time: 0.8169336318969727\n",
      "Step: 4304, Loss: 0.9163188338279724, Accuracy: 1.0, Computation time: 1.1664040088653564\n",
      "Step: 4305, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.9394049644470215\n",
      "Step: 4306, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 0.9296696186065674\n",
      "Step: 4307, Loss: 0.9158598184585571, Accuracy: nan, Computation time: 0.8571052551269531\n",
      "Step: 4308, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 1.0789289474487305\n",
      "########################\n",
      "Test loss: 1.125896692276001, Test Accuracy_epoch31: 0.6862278580665588\n",
      "########################\n",
      "Step: 4309, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 0.9873983860015869\n",
      "Step: 4310, Loss: 0.915854275226593, Accuracy: 1.0, Computation time: 1.1816716194152832\n",
      "Step: 4311, Loss: 0.9171738624572754, Accuracy: 1.0, Computation time: 0.9455714225769043\n",
      "Step: 4312, Loss: 0.9209019541740417, Accuracy: 1.0, Computation time: 0.837456226348877\n",
      "Step: 4313, Loss: 0.9158624410629272, Accuracy: 1.0, Computation time: 0.7422597408294678\n",
      "Step: 4314, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 0.9880657196044922\n",
      "Step: 4315, Loss: 0.9159223437309265, Accuracy: 1.0, Computation time: 1.0262916088104248\n",
      "Step: 4316, Loss: 0.9159178137779236, Accuracy: 1.0, Computation time: 0.9532251358032227\n",
      "Step: 4317, Loss: 0.9159525632858276, Accuracy: 1.0, Computation time: 0.8133647441864014\n",
      "Step: 4318, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 0.9824237823486328\n",
      "Step: 4319, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 0.8388309478759766\n",
      "Step: 4320, Loss: 0.9158665537834167, Accuracy: 1.0, Computation time: 0.7666540145874023\n",
      "Step: 4321, Loss: 0.9375414848327637, Accuracy: 0.9750000238418579, Computation time: 0.8361356258392334\n",
      "Step: 4322, Loss: 0.9158514142036438, Accuracy: 1.0, Computation time: 0.8424854278564453\n",
      "Step: 4323, Loss: 0.9158675074577332, Accuracy: 1.0, Computation time: 0.7470221519470215\n",
      "Step: 4324, Loss: 0.9158816933631897, Accuracy: 1.0, Computation time: 0.9081494808197021\n",
      "Step: 4325, Loss: 0.9159053564071655, Accuracy: 1.0, Computation time: 0.7701747417449951\n",
      "Step: 4326, Loss: 0.9373950958251953, Accuracy: 0.9772727489471436, Computation time: 0.8263390064239502\n",
      "Step: 4327, Loss: 0.9159254431724548, Accuracy: 1.0, Computation time: 1.1181926727294922\n",
      "Step: 4328, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 0.7669875621795654\n",
      "Step: 4329, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 0.9144177436828613\n",
      "Step: 4330, Loss: 0.9176965951919556, Accuracy: 1.0, Computation time: 0.7661070823669434\n",
      "Step: 4331, Loss: 0.9158515334129333, Accuracy: 1.0, Computation time: 0.9082109928131104\n",
      "Step: 4332, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 0.7718536853790283\n",
      "Step: 4333, Loss: 0.9159092903137207, Accuracy: 1.0, Computation time: 0.8793106079101562\n",
      "Step: 4334, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 0.8703367710113525\n",
      "Step: 4335, Loss: 0.9159329533576965, Accuracy: 1.0, Computation time: 1.012136459350586\n",
      "Step: 4336, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 0.9149177074432373\n",
      "Step: 4337, Loss: 0.916494607925415, Accuracy: 1.0, Computation time: 0.8347592353820801\n",
      "Step: 4338, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 0.990471363067627\n",
      "Step: 4339, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 0.8983941078186035\n",
      "Step: 4340, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 0.8372795581817627\n",
      "Step: 4341, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 0.8810620307922363\n",
      "Step: 4342, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 0.9243602752685547\n",
      "Step: 4343, Loss: 0.91584312915802, Accuracy: 1.0, Computation time: 0.7750561237335205\n",
      "Step: 4344, Loss: 0.9369639158248901, Accuracy: 0.9583333730697632, Computation time: 1.001619577407837\n",
      "Step: 4345, Loss: 0.9158350825309753, Accuracy: 1.0, Computation time: 0.7745895385742188\n",
      "Step: 4346, Loss: 0.9158946871757507, Accuracy: 1.0, Computation time: 0.8861658573150635\n",
      "Step: 4347, Loss: 0.9158361554145813, Accuracy: 1.0, Computation time: 0.8404741287231445\n",
      "Step: 4348, Loss: 0.9158628582954407, Accuracy: 1.0, Computation time: 0.941899299621582\n",
      "Step: 4349, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 0.9112658500671387\n",
      "Step: 4350, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 0.8221142292022705\n",
      "Step: 4351, Loss: 0.9158846139907837, Accuracy: 1.0, Computation time: 0.8551883697509766\n",
      "Step: 4352, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 1.0650765895843506\n",
      "Step: 4353, Loss: 0.9158472418785095, Accuracy: 1.0, Computation time: 0.8763110637664795\n",
      "Step: 4354, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 0.8644776344299316\n",
      "Step: 4355, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 0.8378899097442627\n",
      "Step: 4356, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 0.8812534809112549\n",
      "Step: 4357, Loss: 0.9375919699668884, Accuracy: 0.9642857313156128, Computation time: 0.7802419662475586\n",
      "Step: 4358, Loss: 0.9158430099487305, Accuracy: 1.0, Computation time: 0.903141975402832\n",
      "Step: 4359, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 0.862633466720581\n",
      "Step: 4360, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 0.9027254581451416\n",
      "Step: 4361, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 0.9749236106872559\n",
      "Step: 4362, Loss: 0.9159138798713684, Accuracy: 1.0, Computation time: 0.995941162109375\n",
      "Step: 4363, Loss: 0.9376326203346252, Accuracy: 0.9750000238418579, Computation time: 0.8779065608978271\n",
      "Step: 4364, Loss: 0.9160379767417908, Accuracy: 1.0, Computation time: 1.349656581878662\n",
      "Step: 4365, Loss: 0.9158384799957275, Accuracy: 1.0, Computation time: 0.782433271408081\n",
      "Step: 4366, Loss: 0.9158390164375305, Accuracy: 1.0, Computation time: 1.35902738571167\n",
      "Step: 4367, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 0.860454797744751\n",
      "Step: 4368, Loss: 0.9374029636383057, Accuracy: 0.9750000238418579, Computation time: 0.8376398086547852\n",
      "Step: 4369, Loss: 0.9375379681587219, Accuracy: 0.9750000238418579, Computation time: 1.1561052799224854\n",
      "Step: 4370, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.9571835994720459\n",
      "Step: 4371, Loss: 0.9158555865287781, Accuracy: 1.0, Computation time: 1.0465526580810547\n",
      "Step: 4372, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 0.973809003829956\n",
      "Step: 4373, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 0.9205050468444824\n",
      "Step: 4374, Loss: 0.9158353805541992, Accuracy: 1.0, Computation time: 0.782860517501831\n",
      "Step: 4375, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 1.1808366775512695\n",
      "Step: 4376, Loss: 0.9591503739356995, Accuracy: 0.9642857313156128, Computation time: 0.8657844066619873\n",
      "Step: 4377, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 0.8371539115905762\n",
      "Step: 4378, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 0.9693441390991211\n",
      "Step: 4379, Loss: 0.915837287902832, Accuracy: 1.0, Computation time: 0.8929269313812256\n",
      "Step: 4380, Loss: 0.915841817855835, Accuracy: 1.0, Computation time: 0.8056325912475586\n",
      "Step: 4381, Loss: 0.915881335735321, Accuracy: 1.0, Computation time: 1.3484692573547363\n",
      "Step: 4382, Loss: 0.9374889731407166, Accuracy: 0.96875, Computation time: 1.0702791213989258\n",
      "Step: 4383, Loss: 0.9375124573707581, Accuracy: 0.984375, Computation time: 0.7912659645080566\n",
      "Step: 4384, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 0.9523839950561523\n",
      "Step: 4385, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 0.7268905639648438\n",
      "Step: 4386, Loss: 0.9158627390861511, Accuracy: 1.0, Computation time: 0.8973629474639893\n",
      "Step: 4387, Loss: 0.9563667178153992, Accuracy: 0.9666666984558105, Computation time: 1.299673318862915\n",
      "Step: 4388, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.2728936672210693\n",
      "Step: 4389, Loss: 0.9191514253616333, Accuracy: 1.0, Computation time: 1.2792792320251465\n",
      "Step: 4390, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 0.947723388671875\n",
      "Step: 4391, Loss: 0.9159247875213623, Accuracy: 1.0, Computation time: 0.9742131233215332\n",
      "Step: 4392, Loss: 0.915992796421051, Accuracy: 1.0, Computation time: 0.9627683162689209\n",
      "Step: 4393, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 0.917210578918457\n",
      "Step: 4394, Loss: 0.9529337286949158, Accuracy: 0.9583333730697632, Computation time: 0.9409215450286865\n",
      "Step: 4395, Loss: 0.9165241718292236, Accuracy: 1.0, Computation time: 0.9294877052307129\n",
      "Step: 4396, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 0.9248929023742676\n",
      "Step: 4397, Loss: 0.9376431703567505, Accuracy: 0.9791666865348816, Computation time: 0.7983174324035645\n",
      "Step: 4398, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 0.8867764472961426\n",
      "Step: 4399, Loss: 0.9476802349090576, Accuracy: 0.9642857313156128, Computation time: 0.8267498016357422\n",
      "Step: 4400, Loss: 0.9159483313560486, Accuracy: 1.0, Computation time: 1.02748703956604\n",
      "Step: 4401, Loss: 0.9166502356529236, Accuracy: 1.0, Computation time: 1.0858430862426758\n",
      "Step: 4402, Loss: 0.9161948561668396, Accuracy: 1.0, Computation time: 0.8598124980926514\n",
      "Step: 4403, Loss: 0.9172337055206299, Accuracy: 1.0, Computation time: 1.1306946277618408\n",
      "Step: 4404, Loss: 0.9376294612884521, Accuracy: 0.9642857313156128, Computation time: 0.9967527389526367\n",
      "Step: 4405, Loss: 0.9161381125450134, Accuracy: 1.0, Computation time: 1.1611571311950684\n",
      "Step: 4406, Loss: 0.9161248803138733, Accuracy: 1.0, Computation time: 1.0330023765563965\n",
      "Step: 4407, Loss: 0.9160675406455994, Accuracy: 1.0, Computation time: 0.8357882499694824\n",
      "Step: 4408, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 0.9194746017456055\n",
      "Step: 4409, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 0.8124353885650635\n",
      "Step: 4410, Loss: 0.9159407615661621, Accuracy: 1.0, Computation time: 1.0819261074066162\n",
      "Step: 4411, Loss: 0.9365267157554626, Accuracy: 0.9642857313156128, Computation time: 1.0312201976776123\n",
      "Step: 4412, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 0.9133565425872803\n",
      "Step: 4413, Loss: 0.9160178899765015, Accuracy: 1.0, Computation time: 0.8922600746154785\n",
      "Step: 4414, Loss: 0.9436428546905518, Accuracy: 0.96875, Computation time: 1.7197990417480469\n",
      "Step: 4415, Loss: 0.915998101234436, Accuracy: 1.0, Computation time: 0.7768335342407227\n",
      "Step: 4416, Loss: 0.9161088466644287, Accuracy: 1.0, Computation time: 0.9846751689910889\n",
      "Step: 4417, Loss: 0.9160545468330383, Accuracy: 1.0, Computation time: 1.0518743991851807\n",
      "Step: 4418, Loss: 0.9161941409111023, Accuracy: 1.0, Computation time: 1.1753888130187988\n",
      "Step: 4419, Loss: 0.9159992933273315, Accuracy: 1.0, Computation time: 0.8742654323577881\n",
      "Step: 4420, Loss: 0.9159440398216248, Accuracy: 1.0, Computation time: 0.8971602916717529\n",
      "Step: 4421, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 0.8349485397338867\n",
      "Step: 4422, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 0.9632322788238525\n",
      "Step: 4423, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 1.2289602756500244\n",
      "Step: 4424, Loss: 0.9159097671508789, Accuracy: 1.0, Computation time: 0.8918170928955078\n",
      "Step: 4425, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 1.0637211799621582\n",
      "Step: 4426, Loss: 0.9159135222434998, Accuracy: 1.0, Computation time: 0.7829301357269287\n",
      "Step: 4427, Loss: 0.9159159660339355, Accuracy: 1.0, Computation time: 1.0549736022949219\n",
      "Step: 4428, Loss: 0.9159078001976013, Accuracy: 1.0, Computation time: 1.0126268863677979\n",
      "Step: 4429, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 0.9356298446655273\n",
      "Step: 4430, Loss: 0.9375958442687988, Accuracy: 0.9833333492279053, Computation time: 1.0392398834228516\n",
      "Step: 4431, Loss: 0.915865957736969, Accuracy: 1.0, Computation time: 0.8411881923675537\n",
      "Step: 4432, Loss: 0.9375148415565491, Accuracy: 0.96875, Computation time: 0.8532965183258057\n",
      "Step: 4433, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 0.9052443504333496\n",
      "Step: 4434, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 1.1324543952941895\n",
      "Step: 4435, Loss: 0.9159758687019348, Accuracy: 1.0, Computation time: 1.0362768173217773\n",
      "Step: 4436, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 0.9953417778015137\n",
      "Step: 4437, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.0429351329803467\n",
      "Step: 4438, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 0.9133121967315674\n",
      "Step: 4439, Loss: 0.9158807992935181, Accuracy: 1.0, Computation time: 0.8596737384796143\n",
      "Step: 4440, Loss: 0.9160459041595459, Accuracy: 1.0, Computation time: 0.8817062377929688\n",
      "Step: 4441, Loss: 0.9158668518066406, Accuracy: 1.0, Computation time: 1.040477991104126\n",
      "Step: 4442, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 0.7940723896026611\n",
      "Step: 4443, Loss: 0.9158530831336975, Accuracy: 1.0, Computation time: 1.15657377243042\n",
      "Step: 4444, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 1.0393011569976807\n",
      "Step: 4445, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 0.937727689743042\n",
      "Step: 4446, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 1.2144854068756104\n",
      "########################\n",
      "Test loss: 1.1271671056747437, Test Accuracy_epoch32: 0.6841616630554199\n",
      "########################\n",
      "Step: 4447, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 1.1365599632263184\n",
      "Step: 4448, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 0.908947229385376\n",
      "Step: 4449, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 0.7819323539733887\n",
      "Step: 4450, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.0642273426055908\n",
      "Step: 4451, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 0.9045448303222656\n",
      "Step: 4452, Loss: 0.9159716367721558, Accuracy: 1.0, Computation time: 0.9750676155090332\n",
      "Step: 4453, Loss: 0.9375171065330505, Accuracy: 0.9750000238418579, Computation time: 1.072436809539795\n",
      "Step: 4454, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 0.9689645767211914\n",
      "Step: 4455, Loss: 0.9374274015426636, Accuracy: 0.9583333730697632, Computation time: 1.0987775325775146\n",
      "Step: 4456, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 0.7806062698364258\n",
      "Step: 4457, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 0.9950494766235352\n",
      "Step: 4458, Loss: 0.9158422946929932, Accuracy: 1.0, Computation time: 0.7974140644073486\n",
      "Step: 4459, Loss: 0.91602623462677, Accuracy: 1.0, Computation time: 1.3502025604248047\n",
      "Step: 4460, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 0.9028007984161377\n",
      "Step: 4461, Loss: 0.9250011444091797, Accuracy: 1.0, Computation time: 1.113746166229248\n",
      "Step: 4462, Loss: 0.9159457683563232, Accuracy: 1.0, Computation time: 1.011141300201416\n",
      "Step: 4463, Loss: 0.9160766005516052, Accuracy: 1.0, Computation time: 1.1752204895019531\n",
      "Step: 4464, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 0.95223069190979\n",
      "Step: 4465, Loss: 0.9158884286880493, Accuracy: 1.0, Computation time: 0.8491437435150146\n",
      "Step: 4466, Loss: 0.9374712109565735, Accuracy: 0.9722222089767456, Computation time: 0.9169154167175293\n",
      "Step: 4467, Loss: 0.9159519076347351, Accuracy: 1.0, Computation time: 0.9273078441619873\n",
      "Step: 4468, Loss: 0.9375746846199036, Accuracy: 0.9722222089767456, Computation time: 0.7954163551330566\n",
      "Step: 4469, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 0.9864954948425293\n",
      "Step: 4470, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 0.8154616355895996\n",
      "Step: 4471, Loss: 0.9159680604934692, Accuracy: 1.0, Computation time: 1.1016809940338135\n",
      "Step: 4472, Loss: 0.9358186721801758, Accuracy: 0.96875, Computation time: 0.9376087188720703\n",
      "Step: 4473, Loss: 0.9159217476844788, Accuracy: 1.0, Computation time: 0.8677465915679932\n",
      "Step: 4474, Loss: 0.9368137717247009, Accuracy: 0.9642857313156128, Computation time: 0.7903506755828857\n",
      "Step: 4475, Loss: 0.9375270009040833, Accuracy: 0.9722222089767456, Computation time: 1.007392168045044\n",
      "Step: 4476, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 0.8553154468536377\n",
      "Step: 4477, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.0616824626922607\n",
      "Step: 4478, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 0.9398410320281982\n",
      "Step: 4479, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 0.878054141998291\n",
      "Step: 4480, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 0.8535366058349609\n",
      "Step: 4481, Loss: 0.9375113844871521, Accuracy: 0.9772727489471436, Computation time: 0.859713077545166\n",
      "Step: 4482, Loss: 0.9159793853759766, Accuracy: 1.0, Computation time: 0.8673434257507324\n",
      "Step: 4483, Loss: 0.9158461093902588, Accuracy: 1.0, Computation time: 1.0926074981689453\n",
      "Step: 4484, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 0.8675825595855713\n",
      "Step: 4485, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 0.9600105285644531\n",
      "Step: 4486, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 1.0017409324645996\n",
      "Step: 4487, Loss: 0.9158396124839783, Accuracy: 1.0, Computation time: 0.855968713760376\n",
      "Step: 4488, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 0.8602995872497559\n",
      "Step: 4489, Loss: 0.9158360362052917, Accuracy: 1.0, Computation time: 0.8450162410736084\n",
      "Step: 4490, Loss: 0.9158360958099365, Accuracy: 1.0, Computation time: 1.0454962253570557\n",
      "Step: 4491, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 1.1235063076019287\n",
      "Step: 4492, Loss: 0.915929913520813, Accuracy: 1.0, Computation time: 1.0389392375946045\n",
      "Step: 4493, Loss: 0.9158385992050171, Accuracy: 1.0, Computation time: 0.8056199550628662\n",
      "Step: 4494, Loss: 0.9158377647399902, Accuracy: 1.0, Computation time: 0.8399698734283447\n",
      "Step: 4495, Loss: 0.9374839663505554, Accuracy: 0.9807692766189575, Computation time: 0.8810653686523438\n",
      "Step: 4496, Loss: 0.9375703930854797, Accuracy: 0.9772727489471436, Computation time: 1.2347147464752197\n",
      "Step: 4497, Loss: 0.9375907182693481, Accuracy: 0.9772727489471436, Computation time: 0.8104588985443115\n",
      "Step: 4498, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 0.8424286842346191\n",
      "Step: 4499, Loss: 0.9375461339950562, Accuracy: 0.96875, Computation time: 0.8162305355072021\n",
      "Step: 4500, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 0.8878216743469238\n",
      "Step: 4501, Loss: 0.9158360958099365, Accuracy: 1.0, Computation time: 0.8548948764801025\n",
      "Step: 4502, Loss: 0.9374778270721436, Accuracy: 0.9722222089767456, Computation time: 0.8900275230407715\n",
      "Step: 4503, Loss: 0.937539279460907, Accuracy: 0.9722222089767456, Computation time: 0.8755066394805908\n",
      "Step: 4504, Loss: 0.9368632435798645, Accuracy: 0.9642857313156128, Computation time: 0.892916202545166\n",
      "Step: 4505, Loss: 0.9163877367973328, Accuracy: 1.0, Computation time: 0.8917860984802246\n",
      "Step: 4506, Loss: 0.9159486889839172, Accuracy: 1.0, Computation time: 1.8943374156951904\n",
      "Step: 4507, Loss: 0.9373656511306763, Accuracy: 0.96875, Computation time: 0.8560853004455566\n",
      "Step: 4508, Loss: 0.9158852696418762, Accuracy: 1.0, Computation time: 0.9556379318237305\n",
      "Step: 4509, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.823185920715332\n",
      "Step: 4510, Loss: 0.9158850908279419, Accuracy: 1.0, Computation time: 0.8544330596923828\n",
      "Step: 4511, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 0.9406173229217529\n",
      "Step: 4512, Loss: 0.9158439636230469, Accuracy: 1.0, Computation time: 0.9893403053283691\n",
      "Step: 4513, Loss: 0.924290120601654, Accuracy: nan, Computation time: 0.9094810485839844\n",
      "Step: 4514, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 1.1723763942718506\n",
      "Step: 4515, Loss: 0.9158612489700317, Accuracy: 1.0, Computation time: 0.7767684459686279\n",
      "Step: 4516, Loss: 0.9159970283508301, Accuracy: 1.0, Computation time: 1.2065677642822266\n",
      "Step: 4517, Loss: 0.9160098433494568, Accuracy: 1.0, Computation time: 1.201695442199707\n",
      "Step: 4518, Loss: 0.9159833788871765, Accuracy: 1.0, Computation time: 0.8051996231079102\n",
      "Step: 4519, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 0.8945128917694092\n",
      "Step: 4520, Loss: 0.9361639618873596, Accuracy: 0.9791666865348816, Computation time: 1.1224472522735596\n",
      "Step: 4521, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 1.0811374187469482\n",
      "Step: 4522, Loss: 0.9375817775726318, Accuracy: 0.9722222089767456, Computation time: 1.1097815036773682\n",
      "Step: 4523, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.8981740474700928\n",
      "Step: 4524, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 1.0682644844055176\n",
      "Step: 4525, Loss: 0.9375131130218506, Accuracy: 0.9772727489471436, Computation time: 0.8921539783477783\n",
      "Step: 4526, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 0.7963628768920898\n",
      "Step: 4527, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 1.115583896636963\n",
      "Step: 4528, Loss: 0.9171313643455505, Accuracy: 1.0, Computation time: 0.9458751678466797\n",
      "Step: 4529, Loss: 0.9176079630851746, Accuracy: 1.0, Computation time: 0.974193811416626\n",
      "Step: 4530, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 0.866008996963501\n",
      "Step: 4531, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 0.833972692489624\n",
      "Step: 4532, Loss: 0.9244233965873718, Accuracy: 1.0, Computation time: 0.884709358215332\n",
      "Step: 4533, Loss: 0.937006950378418, Accuracy: 0.9772727489471436, Computation time: 0.875575065612793\n",
      "Step: 4534, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 0.8915324211120605\n",
      "Step: 4535, Loss: 0.9163602590560913, Accuracy: 1.0, Computation time: 0.963172197341919\n",
      "Step: 4536, Loss: 0.9158468246459961, Accuracy: 1.0, Computation time: 0.8444969654083252\n",
      "Step: 4537, Loss: 0.915852427482605, Accuracy: 1.0, Computation time: 0.8979990482330322\n",
      "Step: 4538, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 0.9612154960632324\n",
      "Step: 4539, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 1.0744860172271729\n",
      "Step: 4540, Loss: 0.9168307781219482, Accuracy: 1.0, Computation time: 0.9846899509429932\n",
      "Step: 4541, Loss: 0.9159009456634521, Accuracy: 1.0, Computation time: 1.1156818866729736\n",
      "Step: 4542, Loss: 0.9158886671066284, Accuracy: 1.0, Computation time: 0.7903237342834473\n",
      "Step: 4543, Loss: 0.915937602519989, Accuracy: 1.0, Computation time: 0.854778528213501\n",
      "Step: 4544, Loss: 0.9158573150634766, Accuracy: 1.0, Computation time: 0.9301793575286865\n",
      "Step: 4545, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 0.9175429344177246\n",
      "Step: 4546, Loss: 0.9166699051856995, Accuracy: 1.0, Computation time: 0.9402263164520264\n",
      "Step: 4547, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.9249398708343506\n",
      "Step: 4548, Loss: 0.915843665599823, Accuracy: 1.0, Computation time: 0.7370395660400391\n",
      "Step: 4549, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.4223577976226807\n",
      "Step: 4550, Loss: 0.9159584641456604, Accuracy: 1.0, Computation time: 0.896385669708252\n",
      "Step: 4551, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 0.9472208023071289\n",
      "Step: 4552, Loss: 0.9171738624572754, Accuracy: 1.0, Computation time: 0.9722504615783691\n",
      "Step: 4553, Loss: 0.936176598072052, Accuracy: 0.9583333730697632, Computation time: 1.0938029289245605\n",
      "Step: 4554, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 1.1771388053894043\n",
      "Step: 4555, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 0.86700439453125\n",
      "Step: 4556, Loss: 0.9158635139465332, Accuracy: 1.0, Computation time: 0.9395408630371094\n",
      "Step: 4557, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.158862829208374\n",
      "Step: 4558, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 0.8836460113525391\n",
      "Step: 4559, Loss: 0.9158591032028198, Accuracy: 1.0, Computation time: 0.9093942642211914\n",
      "Step: 4560, Loss: 0.9160460233688354, Accuracy: 1.0, Computation time: 0.8619441986083984\n",
      "Step: 4561, Loss: 0.9371815323829651, Accuracy: 0.9772727489471436, Computation time: 0.8992385864257812\n",
      "Step: 4562, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 0.9285118579864502\n",
      "Step: 4563, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 0.8480300903320312\n",
      "Step: 4564, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.2871739864349365\n",
      "Step: 4565, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.8001887798309326\n",
      "Step: 4566, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 0.9379160404205322\n",
      "Step: 4567, Loss: 0.9158515930175781, Accuracy: 1.0, Computation time: 0.750450611114502\n",
      "Step: 4568, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 0.956458330154419\n",
      "Step: 4569, Loss: 0.9158409833908081, Accuracy: 1.0, Computation time: 0.9183332920074463\n",
      "Step: 4570, Loss: 0.9158520102500916, Accuracy: 1.0, Computation time: 0.9276547431945801\n",
      "Step: 4571, Loss: 0.959274411201477, Accuracy: 0.9599359035491943, Computation time: 0.7952706813812256\n",
      "Step: 4572, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 0.7440540790557861\n",
      "Step: 4573, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 0.8571507930755615\n",
      "Step: 4574, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 0.8900146484375\n",
      "Step: 4575, Loss: 0.9158643484115601, Accuracy: 1.0, Computation time: 1.0096936225891113\n",
      "Step: 4576, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 0.8349063396453857\n",
      "Step: 4577, Loss: 0.915847897529602, Accuracy: 1.0, Computation time: 1.1527535915374756\n",
      "Step: 4578, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 0.8234922885894775\n",
      "Step: 4579, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 0.7786865234375\n",
      "Step: 4580, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 0.8843872547149658\n",
      "Step: 4581, Loss: 0.9158570170402527, Accuracy: 1.0, Computation time: 0.8382573127746582\n",
      "Step: 4582, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 1.0121076107025146\n",
      "Step: 4583, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 0.8288159370422363\n",
      "Step: 4584, Loss: 0.9158362746238708, Accuracy: 1.0, Computation time: 0.9907653331756592\n",
      "Step: 4585, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 0.9976654052734375\n",
      "########################\n",
      "Test loss: 1.1262643337249756, Test Accuracy_epoch33: 0.6863109469413757\n",
      "########################\n",
      "Step: 4586, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.5958187580108643\n",
      "Step: 4587, Loss: 0.9158446788787842, Accuracy: 1.0, Computation time: 0.8647708892822266\n",
      "Step: 4588, Loss: 0.9158380031585693, Accuracy: 1.0, Computation time: 0.8568620681762695\n",
      "Step: 4589, Loss: 0.9375242590904236, Accuracy: 0.9722222089767456, Computation time: 0.8339345455169678\n",
      "Step: 4590, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 1.0553224086761475\n",
      "Step: 4591, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 0.9340658187866211\n",
      "Step: 4592, Loss: 0.9158419966697693, Accuracy: 1.0, Computation time: 0.7890739440917969\n",
      "Step: 4593, Loss: 0.9375311136245728, Accuracy: 0.9642857313156128, Computation time: 0.8319056034088135\n",
      "Step: 4594, Loss: 0.9159752726554871, Accuracy: 1.0, Computation time: 1.557274341583252\n",
      "Step: 4595, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 0.9348981380462646\n",
      "Step: 4596, Loss: 0.926983654499054, Accuracy: 0.9722222089767456, Computation time: 0.8542122840881348\n",
      "Step: 4597, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 0.7472379207611084\n",
      "Step: 4598, Loss: 0.9158593416213989, Accuracy: 1.0, Computation time: 0.8893115520477295\n",
      "Step: 4599, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 0.8892538547515869\n",
      "Step: 4600, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 0.7381045818328857\n",
      "Step: 4601, Loss: 0.9159438014030457, Accuracy: 1.0, Computation time: 1.1032416820526123\n",
      "Step: 4602, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 0.8602535724639893\n",
      "Step: 4603, Loss: 0.9375829100608826, Accuracy: 0.9821428656578064, Computation time: 1.2278602123260498\n",
      "Step: 4604, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.3315393924713135\n",
      "Step: 4605, Loss: 0.9159212708473206, Accuracy: 1.0, Computation time: 1.0232431888580322\n",
      "Step: 4606, Loss: 0.937522828578949, Accuracy: 0.9642857313156128, Computation time: 0.917522668838501\n",
      "Step: 4607, Loss: 0.9163459539413452, Accuracy: 1.0, Computation time: 1.2560319900512695\n",
      "Step: 4608, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 0.9660499095916748\n",
      "Step: 4609, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 0.9504246711730957\n",
      "Step: 4610, Loss: 0.9371936321258545, Accuracy: 0.9807692766189575, Computation time: 0.9668197631835938\n",
      "Step: 4611, Loss: 0.9375250935554504, Accuracy: 0.96875, Computation time: 0.8703408241271973\n",
      "Step: 4612, Loss: 0.9312190413475037, Accuracy: 0.9583333730697632, Computation time: 0.9373884201049805\n",
      "Step: 4613, Loss: 0.9366894960403442, Accuracy: 0.9807692766189575, Computation time: 0.972606897354126\n",
      "Step: 4614, Loss: 0.9159398674964905, Accuracy: 1.0, Computation time: 1.0344841480255127\n",
      "Step: 4615, Loss: 0.9159401655197144, Accuracy: 1.0, Computation time: 0.855323076248169\n",
      "Step: 4616, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 1.2025742530822754\n",
      "Step: 4617, Loss: 0.9159210920333862, Accuracy: 1.0, Computation time: 0.9402003288269043\n",
      "Step: 4618, Loss: 0.9385294914245605, Accuracy: 0.9583333730697632, Computation time: 0.8325064182281494\n",
      "Step: 4619, Loss: 0.9160184860229492, Accuracy: 1.0, Computation time: 1.0129027366638184\n",
      "Step: 4620, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 0.9757716655731201\n",
      "Step: 4621, Loss: 0.9162876009941101, Accuracy: 1.0, Computation time: 1.169565200805664\n",
      "Step: 4622, Loss: 0.9375578165054321, Accuracy: 0.9772727489471436, Computation time: 1.0403423309326172\n",
      "Step: 4623, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.1369671821594238\n",
      "Step: 4624, Loss: 0.9158796668052673, Accuracy: 1.0, Computation time: 0.9675705432891846\n",
      "Step: 4625, Loss: 0.9158762693405151, Accuracy: 1.0, Computation time: 1.0129635334014893\n",
      "Step: 4626, Loss: 0.9441472291946411, Accuracy: 0.8541666865348816, Computation time: 1.3130643367767334\n",
      "Step: 4627, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 1.036733627319336\n",
      "Step: 4628, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 0.9159584045410156\n",
      "Step: 4629, Loss: 0.9167279601097107, Accuracy: 1.0, Computation time: 0.9100930690765381\n",
      "Step: 4630, Loss: 0.9159332513809204, Accuracy: 1.0, Computation time: 0.8492066860198975\n",
      "Step: 4631, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 0.931072473526001\n",
      "Step: 4632, Loss: 0.9159197807312012, Accuracy: 1.0, Computation time: 1.2039804458618164\n",
      "Step: 4633, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 1.0029118061065674\n",
      "Step: 4634, Loss: 0.9375339150428772, Accuracy: 0.96875, Computation time: 1.1851887702941895\n",
      "Step: 4635, Loss: 0.915848970413208, Accuracy: 1.0, Computation time: 0.8619570732116699\n",
      "Step: 4636, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 0.953697681427002\n",
      "Step: 4637, Loss: 0.9375975728034973, Accuracy: 0.9750000238418579, Computation time: 1.2093424797058105\n",
      "Step: 4638, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 0.8612651824951172\n",
      "Step: 4639, Loss: 0.9195254445075989, Accuracy: 1.0, Computation time: 1.0716302394866943\n",
      "Step: 4640, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 0.8458569049835205\n",
      "Step: 4641, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 0.8110926151275635\n",
      "Step: 4642, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 0.9522757530212402\n",
      "Step: 4643, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 1.3251266479492188\n",
      "Step: 4644, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 0.9186539649963379\n",
      "Step: 4645, Loss: 0.9161853790283203, Accuracy: 1.0, Computation time: 1.3237128257751465\n",
      "Step: 4646, Loss: 0.9159749746322632, Accuracy: 1.0, Computation time: 1.1702790260314941\n",
      "Step: 4647, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 0.8627951145172119\n",
      "Step: 4648, Loss: 0.915899395942688, Accuracy: 1.0, Computation time: 0.8239121437072754\n",
      "Step: 4649, Loss: 0.9158554673194885, Accuracy: 1.0, Computation time: 0.7557690143585205\n",
      "Step: 4650, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 0.8399128913879395\n",
      "Step: 4651, Loss: 0.9158608913421631, Accuracy: 1.0, Computation time: 0.7682545185089111\n",
      "Step: 4652, Loss: 0.9160336852073669, Accuracy: 1.0, Computation time: 1.2245078086853027\n",
      "Step: 4653, Loss: 0.9158661961555481, Accuracy: 1.0, Computation time: 0.8241679668426514\n",
      "Step: 4654, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 1.017681360244751\n",
      "Step: 4655, Loss: 0.9374643564224243, Accuracy: 0.9821428656578064, Computation time: 1.223306655883789\n",
      "Step: 4656, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 0.9121325016021729\n",
      "Step: 4657, Loss: 0.937559962272644, Accuracy: 0.9642857313156128, Computation time: 0.9150416851043701\n",
      "Step: 4658, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 0.8526606559753418\n",
      "Step: 4659, Loss: 0.9158734083175659, Accuracy: 1.0, Computation time: 0.8822822570800781\n",
      "Step: 4660, Loss: 0.9160535335540771, Accuracy: 1.0, Computation time: 1.1250224113464355\n",
      "Step: 4661, Loss: 0.9336676001548767, Accuracy: 0.96875, Computation time: 1.2598843574523926\n",
      "Step: 4662, Loss: 0.9165266752243042, Accuracy: 1.0, Computation time: 1.777510166168213\n",
      "Step: 4663, Loss: 0.9263360500335693, Accuracy: 0.9772727489471436, Computation time: 0.9861583709716797\n",
      "Step: 4664, Loss: 0.9160864949226379, Accuracy: 1.0, Computation time: 0.8749151229858398\n",
      "Step: 4665, Loss: 0.9400633573532104, Accuracy: 0.9791666865348816, Computation time: 0.921856164932251\n",
      "Step: 4666, Loss: 0.9159104824066162, Accuracy: 1.0, Computation time: 0.7713809013366699\n",
      "Step: 4667, Loss: 0.9160770773887634, Accuracy: 1.0, Computation time: 0.8479108810424805\n",
      "Step: 4668, Loss: 0.9160097241401672, Accuracy: 1.0, Computation time: 0.8461871147155762\n",
      "Step: 4669, Loss: 0.9378358125686646, Accuracy: 0.9807692766189575, Computation time: 1.3362576961517334\n",
      "Step: 4670, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 0.7923610210418701\n",
      "Step: 4671, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 0.9928994178771973\n",
      "Step: 4672, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 0.8734433650970459\n",
      "Step: 4673, Loss: 0.9158414602279663, Accuracy: 1.0, Computation time: 1.4538516998291016\n",
      "Step: 4674, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 1.0292069911956787\n",
      "Step: 4675, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.0564301013946533\n",
      "Step: 4676, Loss: 0.9159607887268066, Accuracy: 1.0, Computation time: 0.8431591987609863\n",
      "Step: 4677, Loss: 0.9159178733825684, Accuracy: 1.0, Computation time: 0.8523316383361816\n",
      "Step: 4678, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 1.1499879360198975\n",
      "Step: 4679, Loss: 0.915920615196228, Accuracy: 1.0, Computation time: 0.9806950092315674\n",
      "Step: 4680, Loss: 0.9158875942230225, Accuracy: 1.0, Computation time: 1.042327880859375\n",
      "Step: 4681, Loss: 0.9161189198493958, Accuracy: 1.0, Computation time: 1.2071919441223145\n",
      "Step: 4682, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 0.9533326625823975\n",
      "Step: 4683, Loss: 0.9282717108726501, Accuracy: 0.9807692766189575, Computation time: 1.0487534999847412\n",
      "Step: 4684, Loss: 0.9159659743309021, Accuracy: 1.0, Computation time: 0.8214137554168701\n",
      "Step: 4685, Loss: 0.9326116442680359, Accuracy: 0.9642857313156128, Computation time: 1.3617019653320312\n",
      "Step: 4686, Loss: 0.9161694645881653, Accuracy: 1.0, Computation time: 1.2074968814849854\n",
      "Step: 4687, Loss: 0.9376405477523804, Accuracy: 0.96875, Computation time: 1.1129190921783447\n",
      "Step: 4688, Loss: 0.9373636245727539, Accuracy: 0.9807692766189575, Computation time: 0.8658168315887451\n",
      "Step: 4689, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 1.0790178775787354\n",
      "Step: 4690, Loss: 0.9159379005432129, Accuracy: 1.0, Computation time: 1.0941541194915771\n",
      "Step: 4691, Loss: 0.9158973693847656, Accuracy: 1.0, Computation time: 0.8571631908416748\n",
      "Step: 4692, Loss: 0.9385982751846313, Accuracy: 0.96875, Computation time: 1.451246976852417\n",
      "Step: 4693, Loss: 0.9313980340957642, Accuracy: 0.9807692766189575, Computation time: 1.23897385597229\n",
      "Step: 4694, Loss: 0.9158835411071777, Accuracy: 1.0, Computation time: 0.9572734832763672\n",
      "Step: 4695, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 0.9441282749176025\n",
      "Step: 4696, Loss: 0.9162172675132751, Accuracy: 1.0, Computation time: 0.9426701068878174\n",
      "Step: 4697, Loss: 0.9376514554023743, Accuracy: 0.9583333730697632, Computation time: 0.8310105800628662\n",
      "Step: 4698, Loss: 0.9189437031745911, Accuracy: 1.0, Computation time: 0.9945578575134277\n",
      "Step: 4699, Loss: 0.9299575090408325, Accuracy: 0.9722222089767456, Computation time: 0.9596567153930664\n",
      "Step: 4700, Loss: 0.9376713037490845, Accuracy: 0.9807692766189575, Computation time: 1.1808140277862549\n",
      "Step: 4701, Loss: 0.9160150289535522, Accuracy: 1.0, Computation time: 0.8029334545135498\n",
      "Step: 4702, Loss: 0.9592991471290588, Accuracy: 0.949999988079071, Computation time: 1.0731289386749268\n",
      "Step: 4703, Loss: 0.9159649610519409, Accuracy: 1.0, Computation time: 0.9106912612915039\n",
      "Step: 4704, Loss: 0.9378629326820374, Accuracy: 0.9642857313156128, Computation time: 0.8892898559570312\n",
      "Step: 4705, Loss: 0.932643711566925, Accuracy: 0.96875, Computation time: 1.080359935760498\n",
      "Step: 4706, Loss: 0.9565531611442566, Accuracy: 0.9545454978942871, Computation time: 0.9835309982299805\n",
      "Step: 4707, Loss: 0.9218257665634155, Accuracy: 1.0, Computation time: 1.3717057704925537\n",
      "Step: 4708, Loss: 0.9257075190544128, Accuracy: 1.0, Computation time: 1.625126600265503\n",
      "Step: 4709, Loss: 0.9162280559539795, Accuracy: 1.0, Computation time: 0.800466775894165\n",
      "Step: 4710, Loss: 0.9162439703941345, Accuracy: 1.0, Computation time: 0.859931468963623\n",
      "Step: 4711, Loss: 0.9161540269851685, Accuracy: 1.0, Computation time: 1.0414090156555176\n",
      "Step: 4712, Loss: 0.9369754791259766, Accuracy: 0.9821428656578064, Computation time: 1.2322425842285156\n",
      "Step: 4713, Loss: 0.9163607358932495, Accuracy: 1.0, Computation time: 0.9509122371673584\n",
      "Step: 4714, Loss: 0.9162577986717224, Accuracy: 1.0, Computation time: 0.8682334423065186\n",
      "Step: 4715, Loss: 0.9162590503692627, Accuracy: 1.0, Computation time: 1.024183988571167\n",
      "Step: 4716, Loss: 0.9315071702003479, Accuracy: 0.9772727489471436, Computation time: 0.9177167415618896\n",
      "Step: 4717, Loss: 0.9374994039535522, Accuracy: 0.9772727489471436, Computation time: 0.9015841484069824\n",
      "Step: 4718, Loss: 0.9377005696296692, Accuracy: 0.9722222089767456, Computation time: 0.8651039600372314\n",
      "Step: 4719, Loss: 0.916077733039856, Accuracy: 1.0, Computation time: 1.1882045269012451\n",
      "Step: 4720, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 0.8854913711547852\n",
      "Step: 4721, Loss: 0.9159173965454102, Accuracy: 1.0, Computation time: 0.9994573593139648\n",
      "Step: 4722, Loss: 0.9166425466537476, Accuracy: 1.0, Computation time: 0.9141743183135986\n",
      "Step: 4723, Loss: 0.9159269332885742, Accuracy: 1.0, Computation time: 1.2369263172149658\n",
      "Step: 4724, Loss: 0.9161097407341003, Accuracy: 1.0, Computation time: 0.9578268527984619\n",
      "########################\n",
      "Test loss: 1.1261239051818848, Test Accuracy_epoch34: 0.6918703317642212\n",
      "########################\n",
      "Step: 4725, Loss: 0.9163727760314941, Accuracy: 1.0, Computation time: 1.0980958938598633\n",
      "Step: 4726, Loss: 0.9161268472671509, Accuracy: 1.0, Computation time: 0.8770897388458252\n",
      "Step: 4727, Loss: 0.9160459637641907, Accuracy: 1.0, Computation time: 0.8174808025360107\n",
      "Step: 4728, Loss: 0.9376353025436401, Accuracy: 0.9722222089767456, Computation time: 0.8230202198028564\n",
      "Step: 4729, Loss: 0.9160030484199524, Accuracy: 1.0, Computation time: 0.9007432460784912\n",
      "Step: 4730, Loss: 0.9163891077041626, Accuracy: 1.0, Computation time: 1.0965971946716309\n",
      "Step: 4731, Loss: 0.9158995747566223, Accuracy: 1.0, Computation time: 0.88033127784729\n",
      "Step: 4732, Loss: 0.9158585667610168, Accuracy: 1.0, Computation time: 0.9150502681732178\n",
      "Step: 4733, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.0811841487884521\n",
      "Step: 4734, Loss: 0.9159447550773621, Accuracy: 1.0, Computation time: 0.8446235656738281\n",
      "Step: 4735, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 1.074831485748291\n",
      "Step: 4736, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 1.0418689250946045\n",
      "Step: 4737, Loss: 0.9159712791442871, Accuracy: 1.0, Computation time: 1.1188435554504395\n",
      "Step: 4738, Loss: 0.9159164428710938, Accuracy: 1.0, Computation time: 0.929412841796875\n",
      "Step: 4739, Loss: 0.9159353375434875, Accuracy: 1.0, Computation time: 1.1061515808105469\n",
      "Step: 4740, Loss: 0.9159111976623535, Accuracy: 1.0, Computation time: 0.919558048248291\n",
      "Step: 4741, Loss: 0.937569260597229, Accuracy: 0.9750000238418579, Computation time: 0.8522360324859619\n",
      "Step: 4742, Loss: 0.9159252047538757, Accuracy: 1.0, Computation time: 0.9026925563812256\n",
      "Step: 4743, Loss: 0.9158766865730286, Accuracy: 1.0, Computation time: 1.0438973903656006\n",
      "Step: 4744, Loss: 0.9159701466560364, Accuracy: 1.0, Computation time: 0.9664804935455322\n",
      "Step: 4745, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 0.8493564128875732\n",
      "Step: 4746, Loss: 0.915956437587738, Accuracy: 1.0, Computation time: 1.2649726867675781\n",
      "Step: 4747, Loss: 0.9158467650413513, Accuracy: 1.0, Computation time: 0.8416123390197754\n",
      "Step: 4748, Loss: 0.9158455729484558, Accuracy: 1.0, Computation time: 0.8809776306152344\n",
      "Step: 4749, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 0.8929440975189209\n",
      "Step: 4750, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 0.9897611141204834\n",
      "Step: 4751, Loss: 0.9158841371536255, Accuracy: 1.0, Computation time: 1.1801338195800781\n",
      "Step: 4752, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 0.8044397830963135\n",
      "Step: 4753, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 1.2004406452178955\n",
      "Step: 4754, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 0.933718204498291\n",
      "Step: 4755, Loss: 0.9190796613693237, Accuracy: 1.0, Computation time: 1.2958035469055176\n",
      "Step: 4756, Loss: 0.92392498254776, Accuracy: 1.0, Computation time: 1.3389592170715332\n",
      "Step: 4757, Loss: 0.9375786185264587, Accuracy: 0.9791666865348816, Computation time: 1.015223741531372\n",
      "Step: 4758, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.4718286991119385\n",
      "Step: 4759, Loss: 0.9375714063644409, Accuracy: 0.9642857313156128, Computation time: 1.049851655960083\n",
      "Step: 4760, Loss: 0.91630619764328, Accuracy: 1.0, Computation time: 1.4044628143310547\n",
      "Step: 4761, Loss: 0.9593815207481384, Accuracy: 0.9583333730697632, Computation time: 1.1786730289459229\n",
      "Step: 4762, Loss: 0.9159184694290161, Accuracy: 1.0, Computation time: 0.978874683380127\n",
      "Step: 4763, Loss: 0.9164630174636841, Accuracy: 1.0, Computation time: 0.9397585391998291\n",
      "Step: 4764, Loss: 0.9373314380645752, Accuracy: 0.9722222089767456, Computation time: 0.8260025978088379\n",
      "Step: 4765, Loss: 0.9183256030082703, Accuracy: 1.0, Computation time: 0.9529068470001221\n",
      "Step: 4766, Loss: 0.9374933242797852, Accuracy: 0.9750000238418579, Computation time: 0.8153822422027588\n",
      "Step: 4767, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 0.8219096660614014\n",
      "Step: 4768, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.9569418430328369\n",
      "Step: 4769, Loss: 0.9158658981323242, Accuracy: 1.0, Computation time: 0.8188292980194092\n",
      "Step: 4770, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 0.9065108299255371\n",
      "Step: 4771, Loss: 0.9158821105957031, Accuracy: 1.0, Computation time: 1.1512868404388428\n",
      "Step: 4772, Loss: 0.9251099228858948, Accuracy: 1.0, Computation time: 0.9404637813568115\n",
      "Step: 4773, Loss: 0.9170903563499451, Accuracy: 1.0, Computation time: 1.053168535232544\n",
      "Step: 4774, Loss: 0.9374560117721558, Accuracy: 0.949999988079071, Computation time: 0.7964189052581787\n",
      "Step: 4775, Loss: 0.932797908782959, Accuracy: 0.9722222089767456, Computation time: 1.5915284156799316\n",
      "Step: 4776, Loss: 0.9159601926803589, Accuracy: 1.0, Computation time: 0.8533511161804199\n",
      "Step: 4777, Loss: 0.9160052537918091, Accuracy: 1.0, Computation time: 0.9104230403900146\n",
      "Step: 4778, Loss: 0.9167059063911438, Accuracy: 1.0, Computation time: 1.1008703708648682\n",
      "Step: 4779, Loss: 0.9163880348205566, Accuracy: 1.0, Computation time: 1.194084644317627\n",
      "Step: 4780, Loss: 0.9161249399185181, Accuracy: 1.0, Computation time: 0.9306087493896484\n",
      "Step: 4781, Loss: 0.9323740601539612, Accuracy: 0.9642857313156128, Computation time: 0.8521716594696045\n",
      "Step: 4782, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 0.9063560962677002\n",
      "Step: 4783, Loss: 0.9158768653869629, Accuracy: 1.0, Computation time: 0.9210388660430908\n",
      "Step: 4784, Loss: 0.9374025464057922, Accuracy: 0.96875, Computation time: 1.0049583911895752\n",
      "Step: 4785, Loss: 0.9159426689147949, Accuracy: 1.0, Computation time: 0.9492170810699463\n",
      "Step: 4786, Loss: 0.9159458875656128, Accuracy: 1.0, Computation time: 1.0276703834533691\n",
      "Step: 4787, Loss: 0.9160076379776001, Accuracy: 1.0, Computation time: 0.8959136009216309\n",
      "Step: 4788, Loss: 0.9376612305641174, Accuracy: 0.9722222089767456, Computation time: 0.9344251155853271\n",
      "Step: 4789, Loss: 0.9162647724151611, Accuracy: 1.0, Computation time: 0.988828182220459\n",
      "Step: 4790, Loss: 0.9159685969352722, Accuracy: 1.0, Computation time: 0.901756763458252\n",
      "Step: 4791, Loss: 0.9159277081489563, Accuracy: 1.0, Computation time: 1.117870569229126\n",
      "Step: 4792, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 0.8426206111907959\n",
      "Step: 4793, Loss: 0.9158626794815063, Accuracy: 1.0, Computation time: 0.8964076042175293\n",
      "Step: 4794, Loss: 0.9159119725227356, Accuracy: 1.0, Computation time: 0.8763160705566406\n",
      "Step: 4795, Loss: 0.9375269412994385, Accuracy: 0.9772727489471436, Computation time: 1.003488540649414\n",
      "Step: 4796, Loss: 0.9380738735198975, Accuracy: 0.9642857313156128, Computation time: 0.934074878692627\n",
      "Step: 4797, Loss: 0.9160662889480591, Accuracy: 1.0, Computation time: 0.958327054977417\n",
      "Step: 4798, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 0.8166642189025879\n",
      "Step: 4799, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 0.7325072288513184\n",
      "Step: 4800, Loss: 0.922407865524292, Accuracy: 1.0, Computation time: 0.9490017890930176\n",
      "Step: 4801, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 0.9277298450469971\n",
      "Step: 4802, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 0.8264522552490234\n",
      "Step: 4803, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 0.96358323097229\n",
      "Step: 4804, Loss: 0.9168344140052795, Accuracy: 1.0, Computation time: 0.8657476902008057\n",
      "Step: 4805, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 0.8310050964355469\n",
      "Step: 4806, Loss: 0.9159730076789856, Accuracy: 1.0, Computation time: 0.8523836135864258\n",
      "Step: 4807, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 0.9005584716796875\n",
      "Step: 4808, Loss: 0.9158609509468079, Accuracy: 1.0, Computation time: 1.1290931701660156\n",
      "Step: 4809, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 0.9527268409729004\n",
      "Step: 4810, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 1.243149757385254\n",
      "Step: 4811, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 1.0832488536834717\n",
      "Step: 4812, Loss: 0.9298107028007507, Accuracy: 0.9772727489471436, Computation time: 1.1344449520111084\n",
      "Step: 4813, Loss: 0.9161998629570007, Accuracy: 1.0, Computation time: 1.091620683670044\n",
      "Step: 4814, Loss: 0.9373606443405151, Accuracy: 0.96875, Computation time: 1.064018964767456\n",
      "Step: 4815, Loss: 0.91595059633255, Accuracy: 1.0, Computation time: 1.140439510345459\n",
      "Step: 4816, Loss: 0.9159166812896729, Accuracy: 1.0, Computation time: 0.9085395336151123\n",
      "Step: 4817, Loss: 0.9159248471260071, Accuracy: 1.0, Computation time: 1.1644189357757568\n",
      "Step: 4818, Loss: 0.9374886751174927, Accuracy: 0.9642857313156128, Computation time: 0.8958864212036133\n",
      "Step: 4819, Loss: 0.9159083366394043, Accuracy: 1.0, Computation time: 0.9051814079284668\n",
      "Step: 4820, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 0.9819891452789307\n",
      "Step: 4821, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.1010985374450684\n",
      "Step: 4822, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 1.210465908050537\n",
      "Step: 4823, Loss: 0.9159128665924072, Accuracy: 1.0, Computation time: 0.9058816432952881\n",
      "Step: 4824, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 1.0303568840026855\n",
      "Step: 4825, Loss: 0.9372425675392151, Accuracy: 0.9642857313156128, Computation time: 1.066373348236084\n",
      "Step: 4826, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 1.031968116760254\n",
      "Step: 4827, Loss: 0.9158865809440613, Accuracy: 1.0, Computation time: 0.99192214012146\n",
      "Step: 4828, Loss: 0.915861964225769, Accuracy: 1.0, Computation time: 0.953012228012085\n",
      "Step: 4829, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 1.0161759853363037\n",
      "Step: 4830, Loss: 0.9161539077758789, Accuracy: 1.0, Computation time: 0.8210422992706299\n",
      "Step: 4831, Loss: 0.918030321598053, Accuracy: 1.0, Computation time: 1.2607495784759521\n",
      "Step: 4832, Loss: 0.9202350378036499, Accuracy: 1.0, Computation time: 1.9360876083374023\n",
      "Step: 4833, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 1.1166746616363525\n",
      "Step: 4834, Loss: 0.9162479639053345, Accuracy: 1.0, Computation time: 0.9851710796356201\n",
      "Step: 4835, Loss: 0.9161799550056458, Accuracy: 1.0, Computation time: 0.9561591148376465\n",
      "Step: 4836, Loss: 0.9161273241043091, Accuracy: 1.0, Computation time: 1.0307128429412842\n",
      "Step: 4837, Loss: 0.916236162185669, Accuracy: 1.0, Computation time: 1.160278558731079\n",
      "Step: 4838, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 1.3395752906799316\n",
      "Step: 4839, Loss: 0.9159125089645386, Accuracy: 1.0, Computation time: 0.9700648784637451\n",
      "Step: 4840, Loss: 0.9167903065681458, Accuracy: 1.0, Computation time: 0.8975796699523926\n",
      "Step: 4841, Loss: 0.9158719778060913, Accuracy: 1.0, Computation time: 1.0384583473205566\n",
      "Step: 4842, Loss: 0.9159611463546753, Accuracy: 1.0, Computation time: 0.9504537582397461\n",
      "Step: 4843, Loss: 0.9374487996101379, Accuracy: 0.9722222089767456, Computation time: 0.9698829650878906\n",
      "Step: 4844, Loss: 0.9377269744873047, Accuracy: 0.9583333730697632, Computation time: 0.955737829208374\n",
      "Step: 4845, Loss: 0.9160091280937195, Accuracy: 1.0, Computation time: 0.9316692352294922\n",
      "Step: 4846, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 0.8559253215789795\n",
      "Step: 4847, Loss: 0.915855348110199, Accuracy: 1.0, Computation time: 1.1418871879577637\n",
      "Step: 4848, Loss: 0.9218900799751282, Accuracy: 1.0, Computation time: 1.0679380893707275\n",
      "Step: 4849, Loss: 0.9376530647277832, Accuracy: 0.9722222089767456, Computation time: 1.5960371494293213\n",
      "Step: 4850, Loss: 0.9159306287765503, Accuracy: 1.0, Computation time: 0.8764955997467041\n",
      "Step: 4851, Loss: 0.9382485747337341, Accuracy: 0.9750000238418579, Computation time: 1.2062909603118896\n",
      "Step: 4852, Loss: 0.9159570932388306, Accuracy: 1.0, Computation time: 1.179579496383667\n",
      "Step: 4853, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 1.1091201305389404\n",
      "Step: 4854, Loss: 0.9159238934516907, Accuracy: 1.0, Computation time: 1.0407843589782715\n",
      "Step: 4855, Loss: 0.9369588494300842, Accuracy: 0.96875, Computation time: 1.0047895908355713\n",
      "Step: 4856, Loss: 0.9159606695175171, Accuracy: 1.0, Computation time: 1.1966071128845215\n",
      "Step: 4857, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 0.949749231338501\n",
      "Step: 4858, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 1.1831438541412354\n",
      "Step: 4859, Loss: 0.9161262512207031, Accuracy: 1.0, Computation time: 1.1359751224517822\n",
      "Step: 4860, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 0.8143479824066162\n",
      "Step: 4861, Loss: 0.9377849698066711, Accuracy: 0.9722222089767456, Computation time: 1.3997671604156494\n",
      "Step: 4862, Loss: 0.9375454187393188, Accuracy: 0.9821428656578064, Computation time: 1.101588487625122\n",
      "Step: 4863, Loss: 0.9159029722213745, Accuracy: 1.0, Computation time: 0.8015027046203613\n",
      "########################\n",
      "Test loss: 1.1248422861099243, Test Accuracy_epoch35: 0.6868026256561279\n",
      "########################\n",
      "Step: 4864, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.1122322082519531\n",
      "Step: 4865, Loss: 0.9365363717079163, Accuracy: 0.96875, Computation time: 1.7647640705108643\n",
      "Step: 4866, Loss: 0.9168692231178284, Accuracy: 1.0, Computation time: 1.1086852550506592\n",
      "Step: 4867, Loss: 0.9160016179084778, Accuracy: 1.0, Computation time: 0.8988869190216064\n",
      "Step: 4868, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 0.907712459564209\n",
      "Step: 4869, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 1.1642990112304688\n",
      "Step: 4870, Loss: 0.9159484505653381, Accuracy: 1.0, Computation time: 0.8946447372436523\n",
      "Step: 4871, Loss: 0.9374964237213135, Accuracy: 0.9750000238418579, Computation time: 0.9872679710388184\n",
      "Step: 4872, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 0.909980058670044\n",
      "Step: 4873, Loss: 0.9278698563575745, Accuracy: 0.96875, Computation time: 1.2576789855957031\n",
      "Step: 4874, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 0.8654983043670654\n",
      "Step: 4875, Loss: 0.9263972043991089, Accuracy: 0.9772727489471436, Computation time: 0.9739675521850586\n",
      "Step: 4876, Loss: 0.9165685772895813, Accuracy: 1.0, Computation time: 1.0690789222717285\n",
      "Step: 4877, Loss: 0.9161288738250732, Accuracy: 1.0, Computation time: 0.9322772026062012\n",
      "Step: 4878, Loss: 0.9161256551742554, Accuracy: 1.0, Computation time: 0.7886362075805664\n",
      "Step: 4879, Loss: 0.9159886837005615, Accuracy: 1.0, Computation time: 0.8780386447906494\n",
      "Step: 4880, Loss: 0.9159489870071411, Accuracy: 1.0, Computation time: 0.773045539855957\n",
      "Step: 4881, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.0334489345550537\n",
      "Step: 4882, Loss: 0.9773558378219604, Accuracy: 0.8791666626930237, Computation time: 1.133145809173584\n",
      "Step: 4883, Loss: 0.915938138961792, Accuracy: 1.0, Computation time: 0.7365350723266602\n",
      "Step: 4884, Loss: 0.9160218238830566, Accuracy: 1.0, Computation time: 0.8114163875579834\n",
      "Step: 4885, Loss: 0.9159843325614929, Accuracy: 1.0, Computation time: 0.7286958694458008\n",
      "Step: 4886, Loss: 0.9160990715026855, Accuracy: 1.0, Computation time: 0.7076246738433838\n",
      "Step: 4887, Loss: 0.9160534143447876, Accuracy: 1.0, Computation time: 0.7956781387329102\n",
      "Step: 4888, Loss: 0.936051070690155, Accuracy: 0.9791666865348816, Computation time: 0.7985553741455078\n",
      "Step: 4889, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 0.9196417331695557\n",
      "Step: 4890, Loss: 0.9158557653427124, Accuracy: 1.0, Computation time: 0.8840880393981934\n",
      "Step: 4891, Loss: 0.9158962368965149, Accuracy: 1.0, Computation time: 0.7882230281829834\n",
      "Step: 4892, Loss: 0.9374340176582336, Accuracy: 0.9722222089767456, Computation time: 0.7631838321685791\n",
      "Step: 4893, Loss: 0.9159328937530518, Accuracy: 1.0, Computation time: 0.7063415050506592\n",
      "Step: 4894, Loss: 0.9375998973846436, Accuracy: 0.96875, Computation time: 0.8505716323852539\n",
      "Step: 4895, Loss: 0.9379322528839111, Accuracy: 0.9791666865348816, Computation time: 0.7527785301208496\n",
      "Step: 4896, Loss: 0.9159533381462097, Accuracy: 1.0, Computation time: 0.7612824440002441\n",
      "Step: 4897, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 0.7659347057342529\n",
      "Step: 4898, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 0.6918151378631592\n",
      "Step: 4899, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 0.7014572620391846\n",
      "Step: 4900, Loss: 0.9160394072532654, Accuracy: 1.0, Computation time: 0.8652186393737793\n",
      "Step: 4901, Loss: 0.9159258604049683, Accuracy: 1.0, Computation time: 0.8601036071777344\n",
      "Step: 4902, Loss: 0.916201651096344, Accuracy: 1.0, Computation time: 0.7310001850128174\n",
      "Step: 4903, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 0.675429105758667\n",
      "Step: 4904, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 0.8643913269042969\n",
      "Step: 4905, Loss: 0.9162724018096924, Accuracy: 1.0, Computation time: 0.8162209987640381\n",
      "Step: 4906, Loss: 0.9177780747413635, Accuracy: 1.0, Computation time: 0.8564691543579102\n",
      "Step: 4907, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 0.7080557346343994\n",
      "Step: 4908, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 0.6817367076873779\n",
      "Step: 4909, Loss: 0.9158797860145569, Accuracy: 1.0, Computation time: 0.7071518898010254\n",
      "Step: 4910, Loss: 0.939551055431366, Accuracy: 0.9375, Computation time: 0.7443339824676514\n",
      "Step: 4911, Loss: 0.9159035682678223, Accuracy: 1.0, Computation time: 0.7742068767547607\n",
      "Step: 4912, Loss: 0.9158960580825806, Accuracy: 1.0, Computation time: 0.7436063289642334\n",
      "Step: 4913, Loss: 0.9293044805526733, Accuracy: 0.9642857313156128, Computation time: 1.4013123512268066\n",
      "Step: 4914, Loss: 0.9376084208488464, Accuracy: 0.9750000238418579, Computation time: 0.9059305191040039\n",
      "Step: 4915, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 0.7190704345703125\n",
      "Step: 4916, Loss: 0.9158967137336731, Accuracy: 1.0, Computation time: 0.7900395393371582\n",
      "Step: 4917, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 0.7292053699493408\n",
      "Step: 4918, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 0.81778883934021\n",
      "Step: 4919, Loss: 0.9159418344497681, Accuracy: 1.0, Computation time: 0.8227977752685547\n",
      "Step: 4920, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 0.959791898727417\n",
      "Step: 4921, Loss: 0.9159051775932312, Accuracy: 1.0, Computation time: 0.7567105293273926\n",
      "Step: 4922, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 0.6741518974304199\n",
      "Step: 4923, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 0.9651260375976562\n",
      "Step: 4924, Loss: 0.9158961176872253, Accuracy: 1.0, Computation time: 0.7339334487915039\n",
      "Step: 4925, Loss: 0.9158815145492554, Accuracy: 1.0, Computation time: 0.9952118396759033\n",
      "Step: 4926, Loss: 0.9159454703330994, Accuracy: 1.0, Computation time: 0.7199482917785645\n",
      "Step: 4927, Loss: 0.9160622358322144, Accuracy: 1.0, Computation time: 1.002399206161499\n",
      "Step: 4928, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 0.7038543224334717\n",
      "Step: 4929, Loss: 0.9158651232719421, Accuracy: 1.0, Computation time: 0.7936899662017822\n",
      "Step: 4930, Loss: 0.9158650040626526, Accuracy: 1.0, Computation time: 1.2071430683135986\n",
      "Step: 4931, Loss: 0.9163571000099182, Accuracy: 1.0, Computation time: 0.9537386894226074\n",
      "Step: 4932, Loss: 0.9375313520431519, Accuracy: 0.9750000238418579, Computation time: 0.6879138946533203\n",
      "Step: 4933, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 0.756096363067627\n",
      "Step: 4934, Loss: 0.9159497618675232, Accuracy: 1.0, Computation time: 0.6819643974304199\n",
      "Step: 4935, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 0.6785707473754883\n",
      "Step: 4936, Loss: 0.9159045219421387, Accuracy: 1.0, Computation time: 0.8094632625579834\n",
      "Step: 4937, Loss: 0.9376173615455627, Accuracy: 0.9791666865348816, Computation time: 0.6778881549835205\n",
      "Step: 4938, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 0.7134113311767578\n",
      "Step: 4939, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.7708511352539062\n",
      "Step: 4940, Loss: 0.916222870349884, Accuracy: 1.0, Computation time: 0.7456107139587402\n",
      "Step: 4941, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 0.7848918437957764\n",
      "Step: 4942, Loss: 0.9372743964195251, Accuracy: 0.96875, Computation time: 0.7310259342193604\n",
      "Step: 4943, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.7908215522766113\n",
      "Step: 4944, Loss: 0.9158427715301514, Accuracy: 1.0, Computation time: 0.7203249931335449\n",
      "Step: 4945, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 0.6775143146514893\n",
      "Step: 4946, Loss: 0.9158567786216736, Accuracy: 1.0, Computation time: 0.7284071445465088\n",
      "Step: 4947, Loss: 0.9158759117126465, Accuracy: 1.0, Computation time: 0.7209455966949463\n",
      "Step: 4948, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 0.6886391639709473\n",
      "Step: 4949, Loss: 0.9161249995231628, Accuracy: 1.0, Computation time: 1.437903881072998\n",
      "Step: 4950, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 0.7161681652069092\n",
      "Step: 4951, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 0.7066740989685059\n",
      "Step: 4952, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 0.7664623260498047\n",
      "Step: 4953, Loss: 0.9270333051681519, Accuracy: 0.9772727489471436, Computation time: 1.1990535259246826\n",
      "Step: 4954, Loss: 0.9161010980606079, Accuracy: 1.0, Computation time: 0.8938672542572021\n",
      "Step: 4955, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 0.967113733291626\n",
      "Step: 4956, Loss: 0.9159212112426758, Accuracy: 1.0, Computation time: 0.7011673450469971\n",
      "Step: 4957, Loss: 0.9377411603927612, Accuracy: 0.9750000238418579, Computation time: 1.1049630641937256\n",
      "Step: 4958, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 0.7409632205963135\n",
      "Step: 4959, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 0.8324398994445801\n",
      "Step: 4960, Loss: 0.9158966541290283, Accuracy: 1.0, Computation time: 0.8333005905151367\n",
      "Step: 4961, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 0.7735133171081543\n",
      "Step: 4962, Loss: 0.9265896677970886, Accuracy: 0.96875, Computation time: 1.1268057823181152\n",
      "Step: 4963, Loss: 0.9376454949378967, Accuracy: 0.96875, Computation time: 1.0495259761810303\n",
      "Step: 4964, Loss: 0.9159038066864014, Accuracy: 1.0, Computation time: 0.7292697429656982\n",
      "Step: 4965, Loss: 0.9160035252571106, Accuracy: 1.0, Computation time: 0.6975259780883789\n",
      "Step: 4966, Loss: 0.937440812587738, Accuracy: 0.9750000238418579, Computation time: 0.7999436855316162\n",
      "Step: 4967, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 0.8081908226013184\n",
      "Step: 4968, Loss: 0.9394336342811584, Accuracy: 0.96875, Computation time: 1.3752012252807617\n",
      "Step: 4969, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 0.9960851669311523\n",
      "Step: 4970, Loss: 0.915959894657135, Accuracy: 1.0, Computation time: 0.7891273498535156\n",
      "Step: 4971, Loss: 0.9161548018455505, Accuracy: 1.0, Computation time: 0.7615065574645996\n",
      "Step: 4972, Loss: 0.9159642457962036, Accuracy: 1.0, Computation time: 0.7280058860778809\n",
      "Step: 4973, Loss: 0.9159643650054932, Accuracy: 1.0, Computation time: 0.8463995456695557\n",
      "Step: 4974, Loss: 0.9375923871994019, Accuracy: 0.96875, Computation time: 0.727196216583252\n",
      "Step: 4975, Loss: 0.9158791303634644, Accuracy: 1.0, Computation time: 0.7250938415527344\n",
      "Step: 4976, Loss: 0.9386343359947205, Accuracy: 0.9583333730697632, Computation time: 0.7587831020355225\n",
      "Step: 4977, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 0.9823977947235107\n",
      "Step: 4978, Loss: 0.9375564455986023, Accuracy: 0.9750000238418579, Computation time: 0.7671694755554199\n",
      "Step: 4979, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 0.7375993728637695\n",
      "Step: 4980, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 0.6832358837127686\n",
      "Step: 4981, Loss: 0.9159131646156311, Accuracy: 1.0, Computation time: 0.8382186889648438\n",
      "Step: 4982, Loss: 0.9375272393226624, Accuracy: 0.9642857313156128, Computation time: 0.9923145771026611\n",
      "Step: 4983, Loss: 0.915871262550354, Accuracy: 1.0, Computation time: 0.8221249580383301\n",
      "Step: 4984, Loss: 0.9374045133590698, Accuracy: 0.9722222089767456, Computation time: 0.7239031791687012\n",
      "Step: 4985, Loss: 0.9277832508087158, Accuracy: 0.9722222089767456, Computation time: 1.109863042831421\n",
      "Step: 4986, Loss: 0.9254587888717651, Accuracy: 0.9807692766189575, Computation time: 1.106318473815918\n",
      "Step: 4987, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 0.8127124309539795\n",
      "Step: 4988, Loss: 0.9159278869628906, Accuracy: 1.0, Computation time: 1.0630793571472168\n",
      "Step: 4989, Loss: 0.9159789681434631, Accuracy: 1.0, Computation time: 1.173752784729004\n",
      "Step: 4990, Loss: 0.9376527667045593, Accuracy: 0.9791666865348816, Computation time: 0.9334914684295654\n",
      "Step: 4991, Loss: 0.9376665949821472, Accuracy: 0.9583333730697632, Computation time: 0.8243684768676758\n",
      "Step: 4992, Loss: 0.9159413576126099, Accuracy: 1.0, Computation time: 0.966463565826416\n",
      "Step: 4993, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 0.8196144104003906\n",
      "Step: 4994, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 0.8326611518859863\n",
      "Step: 4995, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 0.8620145320892334\n",
      "Step: 4996, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.1816942691802979\n",
      "Step: 4997, Loss: 0.9158838987350464, Accuracy: 1.0, Computation time: 0.9421412944793701\n",
      "Step: 4998, Loss: 0.9329078793525696, Accuracy: 0.9791666865348816, Computation time: 1.0284860134124756\n",
      "Step: 4999, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 0.8981876373291016\n",
      "Step: 5000, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 0.8874125480651855\n",
      "Step: 5001, Loss: 0.9159018397331238, Accuracy: 1.0, Computation time: 0.8628785610198975\n",
      "Step: 5002, Loss: 0.9158633351325989, Accuracy: 1.0, Computation time: 0.8493735790252686\n",
      "########################\n",
      "Test loss: 1.1252402067184448, Test Accuracy_epoch36: 0.6906026601791382\n",
      "########################\n",
      "Step: 5003, Loss: 0.9158894419670105, Accuracy: 1.0, Computation time: 0.882380485534668\n",
      "Step: 5004, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.9456844329833984\n",
      "Step: 5005, Loss: 0.9374491572380066, Accuracy: 0.9583333730697632, Computation time: 0.7766151428222656\n",
      "Step: 5006, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 0.9861817359924316\n",
      "Step: 5007, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 0.7952284812927246\n",
      "Step: 5008, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 0.8025100231170654\n",
      "Step: 5009, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.977592945098877\n",
      "Step: 5010, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 0.825080156326294\n",
      "Step: 5011, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 0.8772847652435303\n",
      "Step: 5012, Loss: 0.9165672063827515, Accuracy: 1.0, Computation time: 1.2637062072753906\n",
      "Step: 5013, Loss: 0.9159060120582581, Accuracy: 1.0, Computation time: 0.8340644836425781\n",
      "Step: 5014, Loss: 0.9201360940933228, Accuracy: 1.0, Computation time: 0.9679470062255859\n",
      "Step: 5015, Loss: 0.9352755546569824, Accuracy: 0.9642857313156128, Computation time: 0.9552798271179199\n",
      "Step: 5016, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 0.8416218757629395\n",
      "Step: 5017, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 0.7583720684051514\n",
      "Step: 5018, Loss: 0.9376013278961182, Accuracy: 0.96875, Computation time: 0.9692244529724121\n",
      "Step: 5019, Loss: 0.9159448742866516, Accuracy: 1.0, Computation time: 1.1037437915802002\n",
      "Step: 5020, Loss: 0.9159589409828186, Accuracy: 1.0, Computation time: 0.8301980495452881\n",
      "Step: 5021, Loss: 0.9159103035926819, Accuracy: 1.0, Computation time: 0.9943668842315674\n",
      "Step: 5022, Loss: 0.9371326565742493, Accuracy: 0.9821428656578064, Computation time: 0.759845495223999\n",
      "Step: 5023, Loss: 0.9354301691055298, Accuracy: 0.9722222089767456, Computation time: 1.1090092658996582\n",
      "Step: 5024, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 0.7751865386962891\n",
      "Step: 5025, Loss: 0.9158602356910706, Accuracy: 1.0, Computation time: 0.7962687015533447\n",
      "Step: 5026, Loss: 0.9158715605735779, Accuracy: 1.0, Computation time: 0.9131760597229004\n",
      "Step: 5027, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.3377900123596191\n",
      "Step: 5028, Loss: 0.9159296751022339, Accuracy: 1.0, Computation time: 0.8529934883117676\n",
      "Step: 5029, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 0.8528897762298584\n",
      "Step: 5030, Loss: 0.9159181714057922, Accuracy: 1.0, Computation time: 1.0251154899597168\n",
      "Step: 5031, Loss: 0.9163097143173218, Accuracy: 1.0, Computation time: 1.2287909984588623\n",
      "Step: 5032, Loss: 0.9158605337142944, Accuracy: 1.0, Computation time: 1.0332694053649902\n",
      "Step: 5033, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.8290631771087646\n",
      "Step: 5034, Loss: 0.917018711566925, Accuracy: 1.0, Computation time: 0.842714786529541\n",
      "Step: 5035, Loss: 0.9158533811569214, Accuracy: 1.0, Computation time: 1.1409237384796143\n",
      "Step: 5036, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 1.1193714141845703\n",
      "Step: 5037, Loss: 0.9375544190406799, Accuracy: 0.9642857313156128, Computation time: 0.9008324146270752\n",
      "Step: 5038, Loss: 0.9375197887420654, Accuracy: 0.9750000238418579, Computation time: 0.946077823638916\n",
      "Step: 5039, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 0.8507237434387207\n",
      "Step: 5040, Loss: 0.91584312915802, Accuracy: 1.0, Computation time: 0.8529562950134277\n",
      "Step: 5041, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 0.8525879383087158\n",
      "Step: 5042, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 0.9248933792114258\n",
      "Step: 5043, Loss: 0.9160200357437134, Accuracy: 1.0, Computation time: 1.0007507801055908\n",
      "Step: 5044, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 1.1839971542358398\n",
      "Step: 5045, Loss: 0.9158958196640015, Accuracy: 1.0, Computation time: 1.0332202911376953\n",
      "Step: 5046, Loss: 0.9374845027923584, Accuracy: 0.9791666865348816, Computation time: 0.9702763557434082\n",
      "Step: 5047, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 0.8224422931671143\n",
      "Step: 5048, Loss: 0.9159430861473083, Accuracy: 1.0, Computation time: 0.959650993347168\n",
      "Step: 5049, Loss: 0.9375606775283813, Accuracy: 0.9375, Computation time: 1.0117099285125732\n",
      "Step: 5050, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 0.9691600799560547\n",
      "Step: 5051, Loss: 0.9158453345298767, Accuracy: 1.0, Computation time: 0.8339900970458984\n",
      "Step: 5052, Loss: 0.9158471822738647, Accuracy: 1.0, Computation time: 0.8964862823486328\n",
      "Step: 5053, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 1.0287880897521973\n",
      "Step: 5054, Loss: 0.91623455286026, Accuracy: 1.0, Computation time: 0.8969705104827881\n",
      "Step: 5055, Loss: 0.9161125421524048, Accuracy: 1.0, Computation time: 1.0927367210388184\n",
      "Step: 5056, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 0.831484317779541\n",
      "Step: 5057, Loss: 0.9160583019256592, Accuracy: 1.0, Computation time: 1.4101026058197021\n",
      "Step: 5058, Loss: 0.9158929586410522, Accuracy: 1.0, Computation time: 1.3987057209014893\n",
      "Step: 5059, Loss: 0.9158496856689453, Accuracy: 1.0, Computation time: 1.0583646297454834\n",
      "Step: 5060, Loss: 0.915852963924408, Accuracy: 1.0, Computation time: 1.1790316104888916\n",
      "Step: 5061, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 0.9392235279083252\n",
      "Step: 5062, Loss: 0.9158394932746887, Accuracy: 1.0, Computation time: 0.8195009231567383\n",
      "Step: 5063, Loss: 0.9158387780189514, Accuracy: 1.0, Computation time: 0.8954806327819824\n",
      "Step: 5064, Loss: 0.9375789165496826, Accuracy: 0.9750000238418579, Computation time: 0.9303221702575684\n",
      "Step: 5065, Loss: 0.9158352613449097, Accuracy: 1.0, Computation time: 1.171445608139038\n",
      "Step: 5066, Loss: 0.9158335328102112, Accuracy: 1.0, Computation time: 0.86496901512146\n",
      "Step: 5067, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 1.0267267227172852\n",
      "Step: 5068, Loss: 0.915831983089447, Accuracy: 1.0, Computation time: 0.8813753128051758\n",
      "Step: 5069, Loss: 0.9158587455749512, Accuracy: 1.0, Computation time: 0.8341693878173828\n",
      "Step: 5070, Loss: 0.9264077544212341, Accuracy: 0.9642857313156128, Computation time: 1.3161280155181885\n",
      "Step: 5071, Loss: 0.9227652549743652, Accuracy: 1.0, Computation time: 0.9322822093963623\n",
      "Step: 5072, Loss: 0.9174279570579529, Accuracy: 1.0, Computation time: 0.8580343723297119\n",
      "Step: 5073, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 0.7856190204620361\n",
      "Step: 5074, Loss: 0.9159197211265564, Accuracy: 1.0, Computation time: 0.8202395439147949\n",
      "Step: 5075, Loss: 0.915900707244873, Accuracy: 1.0, Computation time: 0.9155664443969727\n",
      "Step: 5076, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 1.2347817420959473\n",
      "Step: 5077, Loss: 0.9167433977127075, Accuracy: 1.0, Computation time: 1.5229918956756592\n",
      "Step: 5078, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 1.377997636795044\n",
      "Step: 5079, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 0.8726608753204346\n",
      "Step: 5080, Loss: 0.9376022815704346, Accuracy: 0.9583333730697632, Computation time: 1.144181728363037\n",
      "Step: 5081, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.8290233612060547\n",
      "Step: 5082, Loss: 0.9158793091773987, Accuracy: 1.0, Computation time: 0.8799433708190918\n",
      "Step: 5083, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 0.9378707408905029\n",
      "Step: 5084, Loss: 0.9375518560409546, Accuracy: 0.9722222089767456, Computation time: 0.9083418846130371\n",
      "Step: 5085, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 0.9453287124633789\n",
      "Step: 5086, Loss: 0.9158474802970886, Accuracy: 1.0, Computation time: 0.798577070236206\n",
      "Step: 5087, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 1.072878122329712\n",
      "Step: 5088, Loss: 0.9158636331558228, Accuracy: 1.0, Computation time: 0.9256191253662109\n",
      "Step: 5089, Loss: 0.9301060438156128, Accuracy: 0.9772727489471436, Computation time: 3.1416282653808594\n",
      "Step: 5090, Loss: 0.9159375429153442, Accuracy: 1.0, Computation time: 1.0236337184906006\n",
      "Step: 5091, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 1.0149152278900146\n",
      "Step: 5092, Loss: 0.9159538745880127, Accuracy: 1.0, Computation time: 0.8193933963775635\n",
      "Step: 5093, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 0.8350491523742676\n",
      "Step: 5094, Loss: 0.9160119891166687, Accuracy: 1.0, Computation time: 0.8082361221313477\n",
      "Step: 5095, Loss: 0.9159550666809082, Accuracy: 1.0, Computation time: 0.9159812927246094\n",
      "Step: 5096, Loss: 0.9164313077926636, Accuracy: 1.0, Computation time: 1.387239694595337\n",
      "Step: 5097, Loss: 0.9375700950622559, Accuracy: 0.9772727489471436, Computation time: 0.9069018363952637\n",
      "Step: 5098, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 0.8486299514770508\n",
      "Step: 5099, Loss: 0.9241392016410828, Accuracy: 1.0, Computation time: 1.0684304237365723\n",
      "Step: 5100, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 1.5259978771209717\n",
      "Step: 5101, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 0.8853578567504883\n",
      "Step: 5102, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 0.9138944149017334\n",
      "Step: 5103, Loss: 0.915934145450592, Accuracy: 1.0, Computation time: 0.8499798774719238\n",
      "Step: 5104, Loss: 0.9159231185913086, Accuracy: 1.0, Computation time: 1.0228421688079834\n",
      "Step: 5105, Loss: 0.9381292462348938, Accuracy: 0.9583333730697632, Computation time: 1.483870267868042\n",
      "Step: 5106, Loss: 0.9372954964637756, Accuracy: 0.9821428656578064, Computation time: 0.9236571788787842\n",
      "Step: 5107, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 0.8439772129058838\n",
      "Step: 5108, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 0.8856475353240967\n",
      "Step: 5109, Loss: 0.9158535003662109, Accuracy: 1.0, Computation time: 1.016160488128662\n",
      "Step: 5110, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 0.9684860706329346\n",
      "Step: 5111, Loss: 0.9378393292427063, Accuracy: 0.9772727489471436, Computation time: 1.0122032165527344\n",
      "Step: 5112, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 0.7821102142333984\n",
      "Step: 5113, Loss: 0.9158718585968018, Accuracy: 1.0, Computation time: 0.8474481105804443\n",
      "Step: 5114, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 0.9303410053253174\n",
      "Step: 5115, Loss: 0.9159398674964905, Accuracy: 1.0, Computation time: 0.8840129375457764\n",
      "Step: 5116, Loss: 0.9158463478088379, Accuracy: 1.0, Computation time: 0.8139410018920898\n",
      "Step: 5117, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 0.7679667472839355\n",
      "Step: 5118, Loss: 0.9201342463493347, Accuracy: 1.0, Computation time: 0.8356671333312988\n",
      "Step: 5119, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 0.9866862297058105\n",
      "Step: 5120, Loss: 0.9165477156639099, Accuracy: 1.0, Computation time: 0.9146215915679932\n",
      "Step: 5121, Loss: 0.9158697128295898, Accuracy: 1.0, Computation time: 0.7522711753845215\n",
      "Step: 5122, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 0.8689448833465576\n",
      "Step: 5123, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 0.9830992221832275\n",
      "Step: 5124, Loss: 0.9159267544746399, Accuracy: 1.0, Computation time: 0.869443416595459\n",
      "Step: 5125, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 0.9360175132751465\n",
      "Step: 5126, Loss: 0.9384229183197021, Accuracy: 0.9791666865348816, Computation time: 0.9854581356048584\n",
      "Step: 5127, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 1.0426051616668701\n",
      "Step: 5128, Loss: 0.9376667141914368, Accuracy: 0.9750000238418579, Computation time: 1.3720836639404297\n",
      "Step: 5129, Loss: 0.9162262082099915, Accuracy: 1.0, Computation time: 0.8899574279785156\n",
      "Step: 5130, Loss: 0.9162645936012268, Accuracy: 1.0, Computation time: 0.8528611660003662\n",
      "Step: 5131, Loss: 0.9158686995506287, Accuracy: 1.0, Computation time: 0.9867143630981445\n",
      "Step: 5132, Loss: 0.937667191028595, Accuracy: 0.96875, Computation time: 1.0294840335845947\n",
      "Step: 5133, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 1.0666258335113525\n",
      "Step: 5134, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 0.9902360439300537\n",
      "Step: 5135, Loss: 0.9350295066833496, Accuracy: 0.9375, Computation time: 0.9110965728759766\n",
      "Step: 5136, Loss: 0.9163501262664795, Accuracy: 1.0, Computation time: 1.067396640777588\n",
      "Step: 5137, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 0.8588197231292725\n",
      "Step: 5138, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 0.8752315044403076\n",
      "Step: 5139, Loss: 0.9158717393875122, Accuracy: 1.0, Computation time: 0.8951547145843506\n",
      "Step: 5140, Loss: 0.9365338087081909, Accuracy: 0.949999988079071, Computation time: 1.2067673206329346\n",
      "Step: 5141, Loss: 0.915904700756073, Accuracy: 1.0, Computation time: 0.8731958866119385\n",
      "########################\n",
      "Test loss: 1.1312754154205322, Test Accuracy_epoch37: 0.672590970993042\n",
      "########################\n",
      "Step: 5142, Loss: 0.9282525181770325, Accuracy: 0.9807692766189575, Computation time: 1.563706398010254\n",
      "Step: 5143, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 0.9843764305114746\n",
      "Step: 5144, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 0.9684853553771973\n",
      "Step: 5145, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 0.8536074161529541\n",
      "Step: 5146, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 0.9203898906707764\n",
      "Step: 5147, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.1396703720092773\n",
      "Step: 5148, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 0.8112235069274902\n",
      "Step: 5149, Loss: 0.9159906506538391, Accuracy: 1.0, Computation time: 0.9927823543548584\n",
      "Step: 5150, Loss: 0.9167474508285522, Accuracy: 1.0, Computation time: 0.8111774921417236\n",
      "Step: 5151, Loss: 0.9375682473182678, Accuracy: 0.9750000238418579, Computation time: 0.8751940727233887\n",
      "Step: 5152, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 1.1600472927093506\n",
      "Step: 5153, Loss: 0.937468945980072, Accuracy: 0.949999988079071, Computation time: 1.0109565258026123\n",
      "Step: 5154, Loss: 0.9158649444580078, Accuracy: 1.0, Computation time: 1.0396783351898193\n",
      "Step: 5155, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 1.2251648902893066\n",
      "Step: 5156, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 1.0465855598449707\n",
      "Step: 5157, Loss: 0.9398247003555298, Accuracy: 0.9583333730697632, Computation time: 1.2031960487365723\n",
      "Step: 5158, Loss: 0.9377502799034119, Accuracy: 0.9750000238418579, Computation time: 1.0470330715179443\n",
      "Step: 5159, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 0.9441096782684326\n",
      "Step: 5160, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 1.0187132358551025\n",
      "Step: 5161, Loss: 0.9159789681434631, Accuracy: 1.0, Computation time: 1.9434254169464111\n",
      "Step: 5162, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 1.0680391788482666\n",
      "Step: 5163, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 0.8581159114837646\n",
      "Step: 5164, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 0.7787697315216064\n",
      "Step: 5165, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 0.9296760559082031\n",
      "Step: 5166, Loss: 0.9557344317436218, Accuracy: 0.9125000238418579, Computation time: 1.487459659576416\n",
      "Step: 5167, Loss: 0.9158931970596313, Accuracy: 1.0, Computation time: 0.9732780456542969\n",
      "Step: 5168, Loss: 0.920550525188446, Accuracy: 1.0, Computation time: 0.8677084445953369\n",
      "Step: 5169, Loss: 0.9375869035720825, Accuracy: 0.9722222089767456, Computation time: 1.1899759769439697\n",
      "Step: 5170, Loss: 0.9376205801963806, Accuracy: 0.9791666865348816, Computation time: 0.9604642391204834\n",
      "Step: 5171, Loss: 0.9159205555915833, Accuracy: 1.0, Computation time: 0.739527702331543\n",
      "Step: 5172, Loss: 0.9159226417541504, Accuracy: 1.0, Computation time: 1.089508295059204\n",
      "Step: 5173, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 1.0815207958221436\n",
      "Step: 5174, Loss: 0.9374545216560364, Accuracy: 0.9772727489471436, Computation time: 1.5626306533813477\n",
      "Step: 5175, Loss: 0.9159422516822815, Accuracy: 1.0, Computation time: 1.0238289833068848\n",
      "Step: 5176, Loss: 0.9375523924827576, Accuracy: 0.96875, Computation time: 0.9046099185943604\n",
      "Step: 5177, Loss: 0.9159694910049438, Accuracy: 1.0, Computation time: 1.0321269035339355\n",
      "Step: 5178, Loss: 0.9159125685691833, Accuracy: 1.0, Computation time: 1.1765351295471191\n",
      "Step: 5179, Loss: 0.915887176990509, Accuracy: 1.0, Computation time: 0.9836549758911133\n",
      "Step: 5180, Loss: 0.9159666299819946, Accuracy: 1.0, Computation time: 1.4061720371246338\n",
      "Step: 5181, Loss: 0.9210957884788513, Accuracy: 1.0, Computation time: 1.6208653450012207\n",
      "Step: 5182, Loss: 0.9177300333976746, Accuracy: 1.0, Computation time: 1.2360970973968506\n",
      "Step: 5183, Loss: 0.9160236716270447, Accuracy: 1.0, Computation time: 0.9019174575805664\n",
      "Step: 5184, Loss: 0.9282456040382385, Accuracy: 1.0, Computation time: 0.937077522277832\n",
      "Step: 5185, Loss: 0.9160400629043579, Accuracy: 1.0, Computation time: 1.0201733112335205\n",
      "Step: 5186, Loss: 0.9159808158874512, Accuracy: 1.0, Computation time: 0.8547115325927734\n",
      "Step: 5187, Loss: 0.9160394072532654, Accuracy: 1.0, Computation time: 0.9849131107330322\n",
      "Step: 5188, Loss: 0.9160628914833069, Accuracy: 1.0, Computation time: 1.2420001029968262\n",
      "Step: 5189, Loss: 0.9160025715827942, Accuracy: 1.0, Computation time: 0.7611243724822998\n",
      "Step: 5190, Loss: 0.9357733726501465, Accuracy: 0.9821428656578064, Computation time: 1.0246856212615967\n",
      "Step: 5191, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 0.8239190578460693\n",
      "Step: 5192, Loss: 0.916261613368988, Accuracy: 1.0, Computation time: 1.0520787239074707\n",
      "Step: 5193, Loss: 0.9376466870307922, Accuracy: 0.9772727489471436, Computation time: 0.8221471309661865\n",
      "Step: 5194, Loss: 0.9159340858459473, Accuracy: 1.0, Computation time: 0.7687926292419434\n",
      "Step: 5195, Loss: 0.9159876108169556, Accuracy: 1.0, Computation time: 0.884967565536499\n",
      "Step: 5196, Loss: 0.9159694910049438, Accuracy: 1.0, Computation time: 0.8272919654846191\n",
      "Step: 5197, Loss: 0.9159926176071167, Accuracy: 1.0, Computation time: 1.213796854019165\n",
      "Step: 5198, Loss: 0.9159865379333496, Accuracy: 1.0, Computation time: 0.8927669525146484\n",
      "Step: 5199, Loss: 0.9158794283866882, Accuracy: 1.0, Computation time: 1.0703630447387695\n",
      "Step: 5200, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 0.9652183055877686\n",
      "Step: 5201, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 0.8862442970275879\n",
      "Step: 5202, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 0.85988450050354\n",
      "Step: 5203, Loss: 0.9160343408584595, Accuracy: 1.0, Computation time: 0.7730593681335449\n",
      "Step: 5204, Loss: 0.9159709811210632, Accuracy: 1.0, Computation time: 0.8051238059997559\n",
      "Step: 5205, Loss: 0.9308597445487976, Accuracy: 0.9722222089767456, Computation time: 0.9428174495697021\n",
      "Step: 5206, Loss: 0.9590933918952942, Accuracy: 0.9330357313156128, Computation time: 0.855548620223999\n",
      "Step: 5207, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 0.7130119800567627\n",
      "Step: 5208, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 0.7413980960845947\n",
      "Step: 5209, Loss: 0.9376747608184814, Accuracy: 0.9807692766189575, Computation time: 0.9769604206085205\n",
      "Step: 5210, Loss: 0.9159830808639526, Accuracy: 1.0, Computation time: 0.8711214065551758\n",
      "Step: 5211, Loss: 0.9159113764762878, Accuracy: 1.0, Computation time: 1.0435311794281006\n",
      "Step: 5212, Loss: 0.9374830722808838, Accuracy: 0.9807692766189575, Computation time: 0.811842679977417\n",
      "Step: 5213, Loss: 0.9158928394317627, Accuracy: 1.0, Computation time: 0.8151686191558838\n",
      "Step: 5214, Loss: 0.9184306859970093, Accuracy: nan, Computation time: 0.745579719543457\n",
      "Step: 5215, Loss: 0.9162286520004272, Accuracy: 1.0, Computation time: 1.1199190616607666\n",
      "Step: 5216, Loss: 0.9160553216934204, Accuracy: 1.0, Computation time: 0.8547108173370361\n",
      "Step: 5217, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 0.7669713497161865\n",
      "Step: 5218, Loss: 0.9159899353981018, Accuracy: 1.0, Computation time: 0.7079169750213623\n",
      "Step: 5219, Loss: 0.915989339351654, Accuracy: 1.0, Computation time: 1.2102117538452148\n",
      "Step: 5220, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 0.7948403358459473\n",
      "Step: 5221, Loss: 0.9375765323638916, Accuracy: 0.9583333730697632, Computation time: 0.8522994518280029\n",
      "Step: 5222, Loss: 0.9158851504325867, Accuracy: 1.0, Computation time: 0.7685086727142334\n",
      "Step: 5223, Loss: 0.9271824359893799, Accuracy: 0.9807692766189575, Computation time: 0.7601096630096436\n",
      "Step: 5224, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.7193202972412109\n",
      "Step: 5225, Loss: 0.9158825874328613, Accuracy: 1.0, Computation time: 0.7942733764648438\n",
      "Step: 5226, Loss: 0.9158920049667358, Accuracy: 1.0, Computation time: 0.6913726329803467\n",
      "Step: 5227, Loss: 0.9159747362136841, Accuracy: 1.0, Computation time: 1.1587395668029785\n",
      "Step: 5228, Loss: 0.9160629510879517, Accuracy: 1.0, Computation time: 0.8652455806732178\n",
      "Step: 5229, Loss: 0.915946364402771, Accuracy: 1.0, Computation time: 0.7113821506500244\n",
      "Step: 5230, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 0.953643798828125\n",
      "Step: 5231, Loss: 0.9378592371940613, Accuracy: 0.96875, Computation time: 0.978344202041626\n",
      "Step: 5232, Loss: 0.9158825278282166, Accuracy: 1.0, Computation time: 0.8871974945068359\n",
      "Step: 5233, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 0.8396928310394287\n",
      "Step: 5234, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 0.8001368045806885\n",
      "Step: 5235, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 0.8728442192077637\n",
      "Step: 5236, Loss: 0.9375705122947693, Accuracy: 0.9791666865348816, Computation time: 1.0616745948791504\n",
      "Step: 5237, Loss: 0.9159507751464844, Accuracy: 1.0, Computation time: 0.9856586456298828\n",
      "Step: 5238, Loss: 0.9158849716186523, Accuracy: 1.0, Computation time: 0.7303991317749023\n",
      "Step: 5239, Loss: 0.9162723422050476, Accuracy: 1.0, Computation time: 0.8719077110290527\n",
      "Step: 5240, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 0.8980710506439209\n",
      "Step: 5241, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 0.9298968315124512\n",
      "Step: 5242, Loss: 0.937621533870697, Accuracy: 0.949999988079071, Computation time: 1.0240304470062256\n",
      "Step: 5243, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 0.9716110229492188\n",
      "Step: 5244, Loss: 0.9159403443336487, Accuracy: 1.0, Computation time: 0.9675242900848389\n",
      "Step: 5245, Loss: 0.915921151638031, Accuracy: 1.0, Computation time: 1.0058314800262451\n",
      "Step: 5246, Loss: 0.9158757328987122, Accuracy: 1.0, Computation time: 0.8712913990020752\n",
      "Step: 5247, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 0.863483190536499\n",
      "Step: 5248, Loss: 0.9158479571342468, Accuracy: 1.0, Computation time: 0.7729918956756592\n",
      "Step: 5249, Loss: 0.9170491695404053, Accuracy: 1.0, Computation time: 0.9361741542816162\n",
      "Step: 5250, Loss: 0.9159733057022095, Accuracy: 1.0, Computation time: 0.8769216537475586\n",
      "Step: 5251, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 0.9327123165130615\n",
      "Step: 5252, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 0.7462165355682373\n",
      "Step: 5253, Loss: 0.9161680936813354, Accuracy: 1.0, Computation time: 0.8610336780548096\n",
      "Step: 5254, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 0.8334805965423584\n",
      "Step: 5255, Loss: 0.9375572204589844, Accuracy: 0.9791666865348816, Computation time: 0.7879955768585205\n",
      "Step: 5256, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 1.0509636402130127\n",
      "Step: 5257, Loss: 0.9158449769020081, Accuracy: 1.0, Computation time: 1.0915756225585938\n",
      "Step: 5258, Loss: 0.9158788323402405, Accuracy: 1.0, Computation time: 0.8380334377288818\n",
      "Step: 5259, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 1.2072057723999023\n",
      "Step: 5260, Loss: 0.9158569574356079, Accuracy: 1.0, Computation time: 0.8390097618103027\n",
      "Step: 5261, Loss: 0.9158535599708557, Accuracy: 1.0, Computation time: 0.7774341106414795\n",
      "Step: 5262, Loss: 0.9158735871315002, Accuracy: 1.0, Computation time: 0.7779734134674072\n",
      "Step: 5263, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 1.0481462478637695\n",
      "Step: 5264, Loss: 0.9159058332443237, Accuracy: 1.0, Computation time: 1.4100229740142822\n",
      "Step: 5265, Loss: 0.9158428311347961, Accuracy: 1.0, Computation time: 0.8614778518676758\n",
      "Step: 5266, Loss: 0.9158397912979126, Accuracy: 1.0, Computation time: 0.8066303730010986\n",
      "Step: 5267, Loss: 0.9158502817153931, Accuracy: 1.0, Computation time: 0.7347164154052734\n",
      "Step: 5268, Loss: 0.9164138436317444, Accuracy: 1.0, Computation time: 0.7350718975067139\n",
      "Step: 5269, Loss: 0.9158608317375183, Accuracy: 1.0, Computation time: 0.7958567142486572\n",
      "Step: 5270, Loss: 0.9158725142478943, Accuracy: 1.0, Computation time: 0.794651985168457\n",
      "Step: 5271, Loss: 0.9158464670181274, Accuracy: 1.0, Computation time: 0.9401590824127197\n",
      "Step: 5272, Loss: 0.9158843159675598, Accuracy: 1.0, Computation time: 0.8276913166046143\n",
      "Step: 5273, Loss: 0.9231749176979065, Accuracy: 1.0, Computation time: 1.0159742832183838\n",
      "Step: 5274, Loss: 0.915856659412384, Accuracy: 1.0, Computation time: 1.0787725448608398\n",
      "Step: 5275, Loss: 0.915876567363739, Accuracy: 1.0, Computation time: 0.8172264099121094\n",
      "Step: 5276, Loss: 0.915871798992157, Accuracy: 1.0, Computation time: 0.838446855545044\n",
      "Step: 5277, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 1.0941269397735596\n",
      "Step: 5278, Loss: 0.9375577569007874, Accuracy: 0.9750000238418579, Computation time: 1.0251002311706543\n",
      "Step: 5279, Loss: 0.9163889288902283, Accuracy: 1.0, Computation time: 0.7938921451568604\n",
      "Step: 5280, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 0.9867093563079834\n",
      "########################\n",
      "Test loss: 1.1270276308059692, Test Accuracy_epoch38: 0.6815530061721802\n",
      "########################\n",
      "Step: 5281, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 0.9142999649047852\n",
      "Step: 5282, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.058443307876587\n",
      "Step: 5283, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 0.916541576385498\n",
      "Step: 5284, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.049135684967041\n",
      "Step: 5285, Loss: 0.941758394241333, Accuracy: 0.9791666865348816, Computation time: 0.8770565986633301\n",
      "Step: 5286, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 0.9509739875793457\n",
      "Step: 5287, Loss: 0.9159326553344727, Accuracy: 1.0, Computation time: 1.0669419765472412\n",
      "Step: 5288, Loss: 0.9158704876899719, Accuracy: 1.0, Computation time: 0.8662726879119873\n",
      "Step: 5289, Loss: 0.9158704280853271, Accuracy: 1.0, Computation time: 0.9451022148132324\n",
      "Step: 5290, Loss: 0.9375636577606201, Accuracy: 0.9791666865348816, Computation time: 1.0241496562957764\n",
      "Step: 5291, Loss: 0.9164415597915649, Accuracy: 1.0, Computation time: 1.3265881538391113\n",
      "Step: 5292, Loss: 0.9373664259910583, Accuracy: 0.9791666865348816, Computation time: 0.8313503265380859\n",
      "Step: 5293, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 1.3435401916503906\n",
      "Step: 5294, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 0.8096837997436523\n",
      "Step: 5295, Loss: 0.9159106016159058, Accuracy: 1.0, Computation time: 0.8217668533325195\n",
      "Step: 5296, Loss: 0.9158968329429626, Accuracy: 1.0, Computation time: 0.9107270240783691\n",
      "Step: 5297, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 0.7799599170684814\n",
      "Step: 5298, Loss: 0.9158486127853394, Accuracy: 1.0, Computation time: 0.9054832458496094\n",
      "Step: 5299, Loss: 0.9158388376235962, Accuracy: 1.0, Computation time: 0.8456873893737793\n",
      "Step: 5300, Loss: 0.9375096559524536, Accuracy: 0.984375, Computation time: 1.0567524433135986\n",
      "Step: 5301, Loss: 0.9375087022781372, Accuracy: 0.9722222089767456, Computation time: 0.8017604351043701\n",
      "Step: 5302, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 0.9105019569396973\n",
      "Step: 5303, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 0.8703382015228271\n",
      "Step: 5304, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 0.8658783435821533\n",
      "Step: 5305, Loss: 0.9166770577430725, Accuracy: 1.0, Computation time: 1.1117403507232666\n",
      "Step: 5306, Loss: 0.9358600974082947, Accuracy: 0.96875, Computation time: 1.0612120628356934\n",
      "Step: 5307, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 0.9370753765106201\n",
      "Step: 5308, Loss: 0.9217100143432617, Accuracy: 1.0, Computation time: 0.8937625885009766\n",
      "Step: 5309, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 0.818260669708252\n",
      "Step: 5310, Loss: 0.9371552467346191, Accuracy: 0.9852941036224365, Computation time: 0.8607685565948486\n",
      "Step: 5311, Loss: 0.9376929998397827, Accuracy: 0.949999988079071, Computation time: 0.9724161624908447\n",
      "Step: 5312, Loss: 0.916259765625, Accuracy: 1.0, Computation time: 0.9541385173797607\n",
      "Step: 5313, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 0.8254199028015137\n",
      "Step: 5314, Loss: 0.9158525466918945, Accuracy: 1.0, Computation time: 1.0745837688446045\n",
      "Step: 5315, Loss: 0.9158383011817932, Accuracy: 1.0, Computation time: 0.856060266494751\n",
      "Step: 5316, Loss: 0.937533974647522, Accuracy: 0.9807692766189575, Computation time: 0.9108390808105469\n",
      "Step: 5317, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 0.7722806930541992\n",
      "Step: 5318, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 0.7518086433410645\n",
      "Step: 5319, Loss: 0.9159359931945801, Accuracy: 1.0, Computation time: 0.7009902000427246\n",
      "Step: 5320, Loss: 0.9158637523651123, Accuracy: 1.0, Computation time: 0.8239502906799316\n",
      "Step: 5321, Loss: 0.9591642618179321, Accuracy: 0.918749988079071, Computation time: 0.8061182498931885\n",
      "Step: 5322, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 0.9313266277313232\n",
      "Step: 5323, Loss: 0.9159833192825317, Accuracy: 1.0, Computation time: 0.9703166484832764\n",
      "Step: 5324, Loss: 0.9374490976333618, Accuracy: 0.9791666865348816, Computation time: 0.793607234954834\n",
      "Step: 5325, Loss: 0.9158498644828796, Accuracy: 1.0, Computation time: 0.9272317886352539\n",
      "Step: 5326, Loss: 0.9196943640708923, Accuracy: 1.0, Computation time: 0.9881954193115234\n",
      "Step: 5327, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 0.7932107448577881\n",
      "Step: 5328, Loss: 0.9160602688789368, Accuracy: 1.0, Computation time: 0.839709997177124\n",
      "Step: 5329, Loss: 0.9184574484825134, Accuracy: 1.0, Computation time: 0.9833171367645264\n",
      "Step: 5330, Loss: 0.9158945083618164, Accuracy: 1.0, Computation time: 0.8884255886077881\n",
      "Step: 5331, Loss: 0.916689395904541, Accuracy: 1.0, Computation time: 2.0489375591278076\n",
      "Step: 5332, Loss: 0.9376603960990906, Accuracy: 0.9750000238418579, Computation time: 1.0796809196472168\n",
      "Step: 5333, Loss: 0.916864275932312, Accuracy: 1.0, Computation time: 0.8738296031951904\n",
      "Step: 5334, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 0.8231499195098877\n",
      "Step: 5335, Loss: 0.9583120346069336, Accuracy: 0.9545454978942871, Computation time: 0.855226993560791\n",
      "Step: 5336, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 0.8661448955535889\n",
      "Step: 5337, Loss: 0.93756502866745, Accuracy: 0.9722222089767456, Computation time: 0.9065003395080566\n",
      "Step: 5338, Loss: 0.915878176689148, Accuracy: 1.0, Computation time: 0.9535956382751465\n",
      "Step: 5339, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 0.7797770500183105\n",
      "Step: 5340, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 0.7944879531860352\n",
      "Step: 5341, Loss: 0.9160276651382446, Accuracy: 1.0, Computation time: 1.1908495426177979\n",
      "Step: 5342, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 0.8875076770782471\n",
      "Step: 5343, Loss: 0.9158912897109985, Accuracy: 1.0, Computation time: 0.7469027042388916\n",
      "Step: 5344, Loss: 0.916043758392334, Accuracy: 1.0, Computation time: 1.0795719623565674\n",
      "Step: 5345, Loss: 0.917891800403595, Accuracy: 1.0, Computation time: 1.0719048976898193\n",
      "Step: 5346, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 0.7946176528930664\n",
      "Step: 5347, Loss: 0.9552083015441895, Accuracy: 0.9392857551574707, Computation time: 0.8666343688964844\n",
      "Step: 5348, Loss: 0.9159025549888611, Accuracy: 1.0, Computation time: 0.7526051998138428\n",
      "Step: 5349, Loss: 0.9159075021743774, Accuracy: 1.0, Computation time: 0.7846453189849854\n",
      "Step: 5350, Loss: 0.9159175157546997, Accuracy: 1.0, Computation time: 0.8714547157287598\n",
      "Step: 5351, Loss: 0.9375661611557007, Accuracy: 0.9833333492279053, Computation time: 0.7737865447998047\n",
      "Step: 5352, Loss: 0.9161181449890137, Accuracy: 1.0, Computation time: 0.8784449100494385\n",
      "Step: 5353, Loss: 0.9159764051437378, Accuracy: 1.0, Computation time: 1.4644277095794678\n",
      "Step: 5354, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 0.7249767780303955\n",
      "Step: 5355, Loss: 0.9160416126251221, Accuracy: 1.0, Computation time: 1.130291223526001\n",
      "Step: 5356, Loss: 0.9375824332237244, Accuracy: 0.96875, Computation time: 1.2384154796600342\n",
      "Step: 5357, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 0.7624852657318115\n",
      "Step: 5358, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 0.9642417430877686\n",
      "Step: 5359, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 0.8059425354003906\n",
      "Step: 5360, Loss: 0.9158607721328735, Accuracy: 1.0, Computation time: 0.7830781936645508\n",
      "Step: 5361, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 0.9376974105834961\n",
      "Step: 5362, Loss: 0.9158583283424377, Accuracy: 1.0, Computation time: 1.0362465381622314\n",
      "Step: 5363, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 0.9250771999359131\n",
      "Step: 5364, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 0.9307124614715576\n",
      "Step: 5365, Loss: 0.9159530401229858, Accuracy: 1.0, Computation time: 0.9825189113616943\n",
      "Step: 5366, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 0.8395397663116455\n",
      "Step: 5367, Loss: 0.9158574938774109, Accuracy: 1.0, Computation time: 0.7781412601470947\n",
      "Step: 5368, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.8455188274383545\n",
      "Step: 5369, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 0.8516693115234375\n",
      "Step: 5370, Loss: 0.937353253364563, Accuracy: 0.9750000238418579, Computation time: 0.8345732688903809\n",
      "Step: 5371, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 0.7614855766296387\n",
      "Step: 5372, Loss: 0.9159063100814819, Accuracy: 1.0, Computation time: 0.839247465133667\n",
      "Step: 5373, Loss: 0.9158477783203125, Accuracy: 1.0, Computation time: 0.9170875549316406\n",
      "Step: 5374, Loss: 0.9158474206924438, Accuracy: 1.0, Computation time: 0.7351539134979248\n",
      "Step: 5375, Loss: 0.9158498048782349, Accuracy: 1.0, Computation time: 0.869457483291626\n",
      "Step: 5376, Loss: 0.91656893491745, Accuracy: 1.0, Computation time: 1.0843102931976318\n",
      "Step: 5377, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 0.7818365097045898\n",
      "Step: 5378, Loss: 0.9381738901138306, Accuracy: 0.9750000238418579, Computation time: 0.8536677360534668\n",
      "Step: 5379, Loss: 0.9166288375854492, Accuracy: 1.0, Computation time: 0.8329219818115234\n",
      "Step: 5380, Loss: 0.9158599376678467, Accuracy: 1.0, Computation time: 0.7735607624053955\n",
      "Step: 5381, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 0.7630391120910645\n",
      "Step: 5382, Loss: 0.9159039258956909, Accuracy: 1.0, Computation time: 0.7739109992980957\n",
      "Step: 5383, Loss: 0.9158985614776611, Accuracy: 1.0, Computation time: 0.9618582725524902\n",
      "Step: 5384, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 0.8128082752227783\n",
      "Step: 5385, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 0.8845987319946289\n",
      "Step: 5386, Loss: 0.9173031449317932, Accuracy: 1.0, Computation time: 0.7899818420410156\n",
      "Step: 5387, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 0.8447866439819336\n",
      "Step: 5388, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 0.8337743282318115\n",
      "Step: 5389, Loss: 0.9158398509025574, Accuracy: 1.0, Computation time: 1.021677017211914\n",
      "Step: 5390, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 0.756824254989624\n",
      "Step: 5391, Loss: 0.937282383441925, Accuracy: 0.9722222089767456, Computation time: 0.8380827903747559\n",
      "Step: 5392, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.0861554145812988\n",
      "Step: 5393, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 0.7349092960357666\n",
      "Step: 5394, Loss: 0.92719566822052, Accuracy: 0.9750000238418579, Computation time: 1.4163479804992676\n",
      "Step: 5395, Loss: 0.9158681035041809, Accuracy: 1.0, Computation time: 0.8440818786621094\n",
      "Step: 5396, Loss: 0.9375960826873779, Accuracy: 0.9807692766189575, Computation time: 0.8546786308288574\n",
      "Step: 5397, Loss: 0.915869951248169, Accuracy: 1.0, Computation time: 0.8398699760437012\n",
      "Step: 5398, Loss: 0.9376360177993774, Accuracy: 0.9375, Computation time: 0.709003210067749\n",
      "Step: 5399, Loss: 0.915876030921936, Accuracy: 1.0, Computation time: 0.7946066856384277\n",
      "Step: 5400, Loss: 0.9379810690879822, Accuracy: 0.9833333492279053, Computation time: 0.7518362998962402\n",
      "Step: 5401, Loss: 0.9374754428863525, Accuracy: 0.9722222089767456, Computation time: 1.17698073387146\n",
      "Step: 5402, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 0.6808733940124512\n",
      "Step: 5403, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 0.727161169052124\n",
      "Step: 5404, Loss: 0.9375086426734924, Accuracy: 0.9791666865348816, Computation time: 1.903517484664917\n",
      "Step: 5405, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 0.725287675857544\n",
      "Step: 5406, Loss: 0.9188825488090515, Accuracy: 1.0, Computation time: 1.1848397254943848\n",
      "Step: 5407, Loss: 0.9159310460090637, Accuracy: 1.0, Computation time: 0.6893560886383057\n",
      "Step: 5408, Loss: 0.9158996343612671, Accuracy: 1.0, Computation time: 0.7021768093109131\n",
      "Step: 5409, Loss: 0.9159389734268188, Accuracy: 1.0, Computation time: 0.6415102481842041\n",
      "Step: 5410, Loss: 0.9158645272254944, Accuracy: 1.0, Computation time: 0.730478048324585\n",
      "Step: 5411, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 1.0475099086761475\n",
      "Step: 5412, Loss: 0.9285472631454468, Accuracy: 0.9583333730697632, Computation time: 0.9593634605407715\n",
      "Step: 5413, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 0.7266771793365479\n",
      "Step: 5414, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 0.9032394886016846\n",
      "Step: 5415, Loss: 0.9158661365509033, Accuracy: 1.0, Computation time: 0.6976726055145264\n",
      "Step: 5416, Loss: 0.9374032616615295, Accuracy: 0.9750000238418579, Computation time: 1.0682387351989746\n",
      "Step: 5417, Loss: 0.9158684611320496, Accuracy: 1.0, Computation time: 0.9347243309020996\n",
      "Step: 5418, Loss: 0.9162042737007141, Accuracy: 1.0, Computation time: 0.9918777942657471\n",
      "Step: 5419, Loss: 0.9159051179885864, Accuracy: 1.0, Computation time: 0.8276686668395996\n",
      "########################\n",
      "Test loss: 1.1262881755828857, Test Accuracy_epoch39: 0.6863663196563721\n",
      "########################\n",
      "Step: 5420, Loss: 0.9159092307090759, Accuracy: 1.0, Computation time: 0.8231263160705566\n",
      "Step: 5421, Loss: 0.9159360527992249, Accuracy: 1.0, Computation time: 0.7597160339355469\n",
      "Step: 5422, Loss: 0.9158881902694702, Accuracy: 1.0, Computation time: 0.7743854522705078\n",
      "Step: 5423, Loss: 0.9371638298034668, Accuracy: 0.9807692766189575, Computation time: 0.7946743965148926\n",
      "Step: 5424, Loss: 0.9183171987533569, Accuracy: 1.0, Computation time: 0.862687349319458\n",
      "Step: 5425, Loss: 0.9158715009689331, Accuracy: 1.0, Computation time: 0.9640557765960693\n",
      "Step: 5426, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.076063871383667\n",
      "Step: 5427, Loss: 0.9235890507698059, Accuracy: 1.0, Computation time: 1.1254651546478271\n",
      "Step: 5428, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 0.8351950645446777\n",
      "Step: 5429, Loss: 0.9532178640365601, Accuracy: 0.9583333730697632, Computation time: 0.9957695007324219\n",
      "Step: 5430, Loss: 0.9159168004989624, Accuracy: 1.0, Computation time: 0.86208176612854\n",
      "Step: 5431, Loss: 0.9159302115440369, Accuracy: 1.0, Computation time: 0.891118049621582\n",
      "Step: 5432, Loss: 0.9161258339881897, Accuracy: 1.0, Computation time: 1.2553613185882568\n",
      "Step: 5433, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 1.1436374187469482\n",
      "Step: 5434, Loss: 0.9159215688705444, Accuracy: 1.0, Computation time: 0.9487590789794922\n",
      "Step: 5435, Loss: 0.9159083962440491, Accuracy: 1.0, Computation time: 0.8911106586456299\n",
      "Step: 5436, Loss: 0.9162082076072693, Accuracy: 1.0, Computation time: 0.9493057727813721\n",
      "Step: 5437, Loss: 0.9158828258514404, Accuracy: 1.0, Computation time: 0.8709032535552979\n",
      "Step: 5438, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 0.7650887966156006\n",
      "Step: 5439, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 0.8932287693023682\n",
      "Step: 5440, Loss: 0.935858428478241, Accuracy: 0.9772727489471436, Computation time: 0.9993560314178467\n",
      "Step: 5441, Loss: 0.915905773639679, Accuracy: 1.0, Computation time: 0.8800857067108154\n",
      "Step: 5442, Loss: 0.9288604259490967, Accuracy: 0.9750000238418579, Computation time: 0.727515697479248\n",
      "Step: 5443, Loss: 0.9159433245658875, Accuracy: 1.0, Computation time: 0.8864691257476807\n",
      "Step: 5444, Loss: 0.9159398674964905, Accuracy: 1.0, Computation time: 0.8617434501647949\n",
      "Step: 5445, Loss: 0.9159870147705078, Accuracy: 1.0, Computation time: 0.7210869789123535\n",
      "Step: 5446, Loss: 0.9160022139549255, Accuracy: 1.0, Computation time: 1.0272409915924072\n",
      "Step: 5447, Loss: 0.9160550236701965, Accuracy: 1.0, Computation time: 1.0060389041900635\n",
      "Step: 5448, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 0.8767752647399902\n",
      "Step: 5449, Loss: 0.9159265756607056, Accuracy: 1.0, Computation time: 0.8403472900390625\n",
      "Step: 5450, Loss: 0.9278291463851929, Accuracy: 0.9772727489471436, Computation time: 1.0775063037872314\n",
      "Step: 5451, Loss: 0.9158891439437866, Accuracy: 1.0, Computation time: 0.8708615303039551\n",
      "Step: 5452, Loss: 0.9158999919891357, Accuracy: 1.0, Computation time: 0.6914622783660889\n",
      "Step: 5453, Loss: 0.9374964833259583, Accuracy: 0.9807692766189575, Computation time: 0.8536972999572754\n",
      "Step: 5454, Loss: 0.9160410165786743, Accuracy: 1.0, Computation time: 0.9963662624359131\n",
      "Step: 5455, Loss: 0.9159315824508667, Accuracy: 1.0, Computation time: 0.7524592876434326\n",
      "Step: 5456, Loss: 0.915898859500885, Accuracy: 1.0, Computation time: 0.7382657527923584\n",
      "Step: 5457, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 0.9976675510406494\n",
      "Step: 5458, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.7947721481323242\n",
      "Step: 5459, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 0.9139416217803955\n",
      "Step: 5460, Loss: 0.9372686147689819, Accuracy: 0.9772727489471436, Computation time: 1.0024545192718506\n",
      "Step: 5461, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 1.1549088954925537\n",
      "Step: 5462, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 0.816852331161499\n",
      "Step: 5463, Loss: 0.9375260472297668, Accuracy: 0.9722222089767456, Computation time: 1.048328161239624\n",
      "Step: 5464, Loss: 0.9384979009628296, Accuracy: 0.9772727489471436, Computation time: 1.3223166465759277\n",
      "Step: 5465, Loss: 0.9161370992660522, Accuracy: 1.0, Computation time: 1.0441484451293945\n",
      "Step: 5466, Loss: 0.9159497022628784, Accuracy: 1.0, Computation time: 0.8925471305847168\n",
      "Step: 5467, Loss: 0.9160812497138977, Accuracy: 1.0, Computation time: 0.8959813117980957\n",
      "Step: 5468, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 0.9549946784973145\n",
      "Step: 5469, Loss: 0.9363327622413635, Accuracy: 0.9772727489471436, Computation time: 1.024414300918579\n",
      "Step: 5470, Loss: 0.9163051843643188, Accuracy: 1.0, Computation time: 1.0855281352996826\n",
      "Step: 5471, Loss: 0.9159038662910461, Accuracy: 1.0, Computation time: 0.7886018753051758\n",
      "Step: 5472, Loss: 0.9158726334571838, Accuracy: 1.0, Computation time: 0.9050304889678955\n",
      "Step: 5473, Loss: 0.9158944487571716, Accuracy: 1.0, Computation time: 0.8890085220336914\n",
      "Step: 5474, Loss: 0.937716543674469, Accuracy: 0.9791666865348816, Computation time: 0.962350606918335\n",
      "Step: 5475, Loss: 0.9375876188278198, Accuracy: 0.9750000238418579, Computation time: 0.9910683631896973\n",
      "Step: 5476, Loss: 0.9375600814819336, Accuracy: 0.9642857313156128, Computation time: 1.2931559085845947\n",
      "Step: 5477, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 0.8321211338043213\n",
      "Step: 5478, Loss: 0.915881872177124, Accuracy: 1.0, Computation time: 0.8822743892669678\n",
      "Step: 5479, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.8703501224517822\n",
      "Step: 5480, Loss: 0.9159396290779114, Accuracy: 1.0, Computation time: 0.8021321296691895\n",
      "Step: 5481, Loss: 0.9371514320373535, Accuracy: 0.9750000238418579, Computation time: 1.0229125022888184\n",
      "Step: 5482, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 0.914426326751709\n",
      "Step: 5483, Loss: 0.9159263372421265, Accuracy: 1.0, Computation time: 0.932811975479126\n",
      "Step: 5484, Loss: 0.9374061226844788, Accuracy: 0.9821428656578064, Computation time: 0.9661688804626465\n",
      "Step: 5485, Loss: 0.9278205037117004, Accuracy: 0.9642857313156128, Computation time: 1.2187840938568115\n",
      "Step: 5486, Loss: 0.9172008633613586, Accuracy: 1.0, Computation time: 0.7653813362121582\n",
      "Step: 5487, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 1.0529232025146484\n",
      "Step: 5488, Loss: 0.9158554077148438, Accuracy: 1.0, Computation time: 0.7800533771514893\n",
      "Step: 5489, Loss: 0.9233929514884949, Accuracy: 1.0, Computation time: 0.8805792331695557\n",
      "Step: 5490, Loss: 0.9330621957778931, Accuracy: 0.949999988079071, Computation time: 0.8659310340881348\n",
      "Step: 5491, Loss: 0.9161044955253601, Accuracy: 1.0, Computation time: 0.8912513256072998\n",
      "Step: 5492, Loss: 0.9382833242416382, Accuracy: 0.9722222089767456, Computation time: 0.8331613540649414\n",
      "Step: 5493, Loss: 0.9160592555999756, Accuracy: 1.0, Computation time: 0.9259874820709229\n",
      "Step: 5494, Loss: 0.9159998297691345, Accuracy: 1.0, Computation time: 1.0782861709594727\n",
      "Step: 5495, Loss: 0.9159386157989502, Accuracy: 1.0, Computation time: 0.8408114910125732\n",
      "Step: 5496, Loss: 0.9168292284011841, Accuracy: 1.0, Computation time: 1.0470077991485596\n",
      "Step: 5497, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 0.7506465911865234\n",
      "Step: 5498, Loss: 0.9374440908432007, Accuracy: 0.96875, Computation time: 0.9310846328735352\n",
      "Step: 5499, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 0.8039798736572266\n",
      "Step: 5500, Loss: 0.9160078167915344, Accuracy: 1.0, Computation time: 0.9501533508300781\n",
      "Step: 5501, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 0.9694843292236328\n",
      "Step: 5502, Loss: 0.9160821437835693, Accuracy: 1.0, Computation time: 0.9098160266876221\n",
      "Step: 5503, Loss: 0.9193652868270874, Accuracy: 1.0, Computation time: 1.2014086246490479\n",
      "Step: 5504, Loss: 0.9159016609191895, Accuracy: 1.0, Computation time: 0.8004577159881592\n",
      "Step: 5505, Loss: 0.9158751964569092, Accuracy: 1.0, Computation time: 0.9824566841125488\n",
      "Step: 5506, Loss: 0.9159770607948303, Accuracy: 1.0, Computation time: 1.1986024379730225\n",
      "Step: 5507, Loss: 0.9159721732139587, Accuracy: 1.0, Computation time: 0.9305517673492432\n",
      "Step: 5508, Loss: 0.922431468963623, Accuracy: 1.0, Computation time: 1.1108918190002441\n",
      "Step: 5509, Loss: 0.9160330295562744, Accuracy: 1.0, Computation time: 0.9126746654510498\n",
      "Step: 5510, Loss: 0.9159766435623169, Accuracy: 1.0, Computation time: 0.9693076610565186\n",
      "Step: 5511, Loss: 0.9159491658210754, Accuracy: 1.0, Computation time: 1.028717041015625\n",
      "Step: 5512, Loss: 0.9159416556358337, Accuracy: 1.0, Computation time: 0.7784392833709717\n",
      "Step: 5513, Loss: 0.9160829782485962, Accuracy: 1.0, Computation time: 1.0197758674621582\n",
      "Step: 5514, Loss: 0.9158942699432373, Accuracy: 1.0, Computation time: 1.114055871963501\n",
      "Step: 5515, Loss: 0.9158755540847778, Accuracy: 1.0, Computation time: 0.9681458473205566\n",
      "Step: 5516, Loss: 0.9374869465827942, Accuracy: 0.9642857313156128, Computation time: 0.9752514362335205\n",
      "Step: 5517, Loss: 0.9403228163719177, Accuracy: 0.9791666865348816, Computation time: 1.2141590118408203\n",
      "Step: 5518, Loss: 0.9159350395202637, Accuracy: 1.0, Computation time: 0.8226897716522217\n",
      "Step: 5519, Loss: 0.9159578680992126, Accuracy: 1.0, Computation time: 1.0449156761169434\n",
      "Step: 5520, Loss: 0.9159545302391052, Accuracy: 1.0, Computation time: 1.138068675994873\n",
      "Step: 5521, Loss: 0.9159154295921326, Accuracy: 1.0, Computation time: 0.8672070503234863\n",
      "Step: 5522, Loss: 0.9409387707710266, Accuracy: 0.9642857313156128, Computation time: 1.20184326171875\n",
      "Step: 5523, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 0.9304032325744629\n",
      "Step: 5524, Loss: 0.9159881472587585, Accuracy: 1.0, Computation time: 0.9629063606262207\n",
      "Step: 5525, Loss: 0.9159529805183411, Accuracy: 1.0, Computation time: 1.0457913875579834\n",
      "Step: 5526, Loss: 0.9159383177757263, Accuracy: 1.0, Computation time: 1.2596383094787598\n",
      "Step: 5527, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 0.956634521484375\n",
      "Step: 5528, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 0.8222882747650146\n",
      "Step: 5529, Loss: 0.9374756217002869, Accuracy: 0.9750000238418579, Computation time: 0.900033712387085\n",
      "Step: 5530, Loss: 0.9158491492271423, Accuracy: 1.0, Computation time: 0.8704965114593506\n",
      "Step: 5531, Loss: 0.9158473014831543, Accuracy: 1.0, Computation time: 0.9362068176269531\n",
      "Step: 5532, Loss: 0.9158844351768494, Accuracy: 1.0, Computation time: 0.8925001621246338\n",
      "Step: 5533, Loss: 0.9375525116920471, Accuracy: 0.9791666865348816, Computation time: 0.9392189979553223\n",
      "Step: 5534, Loss: 0.9159252643585205, Accuracy: 1.0, Computation time: 0.9746401309967041\n",
      "Step: 5535, Loss: 0.915895402431488, Accuracy: 1.0, Computation time: 0.9150581359863281\n",
      "Step: 5536, Loss: 0.9160197377204895, Accuracy: 1.0, Computation time: 1.0512945652008057\n",
      "Step: 5537, Loss: 0.9168258905410767, Accuracy: 1.0, Computation time: 1.0837078094482422\n",
      "Step: 5538, Loss: 0.9158861041069031, Accuracy: 1.0, Computation time: 0.8929605484008789\n",
      "Step: 5539, Loss: 0.9162881374359131, Accuracy: 1.0, Computation time: 1.5326664447784424\n",
      "Step: 5540, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 0.9298298358917236\n",
      "Step: 5541, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 1.1385719776153564\n",
      "Step: 5542, Loss: 0.9375045895576477, Accuracy: 0.96875, Computation time: 0.844606876373291\n",
      "Step: 5543, Loss: 0.9159904718399048, Accuracy: 1.0, Computation time: 0.9799680709838867\n",
      "Step: 5544, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 0.9303493499755859\n",
      "Step: 5545, Loss: 0.9160602688789368, Accuracy: 1.0, Computation time: 0.8398916721343994\n",
      "Step: 5546, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 0.9472246170043945\n",
      "Step: 5547, Loss: 0.937567412853241, Accuracy: 0.96875, Computation time: 0.8530130386352539\n",
      "Step: 5548, Loss: 0.9363524913787842, Accuracy: 0.9750000238418579, Computation time: 0.9099397659301758\n",
      "Step: 5549, Loss: 0.9375960826873779, Accuracy: 0.9833333492279053, Computation time: 0.8704264163970947\n",
      "Step: 5550, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.0447447299957275\n",
      "Step: 5551, Loss: 0.9159817099571228, Accuracy: 1.0, Computation time: 1.8478407859802246\n",
      "Step: 5552, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 0.7995097637176514\n",
      "Step: 5553, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 0.8912272453308105\n",
      "Step: 5554, Loss: 0.9350337982177734, Accuracy: 0.9583333730697632, Computation time: 1.0847465991973877\n",
      "Step: 5555, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 0.8553261756896973\n",
      "Step: 5556, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 1.0565245151519775\n",
      "Step: 5557, Loss: 0.9377164244651794, Accuracy: 0.96875, Computation time: 1.0063729286193848\n",
      "Step: 5558, Loss: 0.9158629775047302, Accuracy: 1.0, Computation time: 0.9322404861450195\n",
      "########################\n",
      "Test loss: 1.1271312236785889, Test Accuracy_epoch40: 0.6822339296340942\n",
      "########################\n",
      "Step: 5559, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 0.8658638000488281\n",
      "Step: 5560, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 0.9218347072601318\n",
      "Step: 5561, Loss: 0.9159519672393799, Accuracy: 1.0, Computation time: 1.0608227252960205\n",
      "Step: 5562, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 0.8656139373779297\n",
      "Step: 5563, Loss: 0.9376004934310913, Accuracy: 0.9772727489471436, Computation time: 1.2915711402893066\n",
      "Step: 5564, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 1.1058378219604492\n",
      "Step: 5565, Loss: 0.9158526659011841, Accuracy: 1.0, Computation time: 0.9519729614257812\n",
      "Step: 5566, Loss: 0.9158440232276917, Accuracy: 1.0, Computation time: 0.8545863628387451\n",
      "Step: 5567, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 1.0620150566101074\n",
      "Step: 5568, Loss: 0.9158475995063782, Accuracy: 1.0, Computation time: 0.9464759826660156\n",
      "Step: 5569, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 0.9028925895690918\n",
      "Step: 5570, Loss: 0.9158799052238464, Accuracy: 1.0, Computation time: 0.8936383724212646\n",
      "Step: 5571, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 0.9765946865081787\n",
      "Step: 5572, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 0.8153259754180908\n",
      "Step: 5573, Loss: 0.9158366918563843, Accuracy: 1.0, Computation time: 0.8657870292663574\n",
      "Step: 5574, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 1.0031213760375977\n",
      "Step: 5575, Loss: 0.9158446192741394, Accuracy: 1.0, Computation time: 0.9113762378692627\n",
      "Step: 5576, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 0.9115934371948242\n",
      "Step: 5577, Loss: 0.9194804430007935, Accuracy: 1.0, Computation time: 1.032170057296753\n",
      "Step: 5578, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 0.7852678298950195\n",
      "Step: 5579, Loss: 0.9158669114112854, Accuracy: 1.0, Computation time: 1.0574655532836914\n",
      "Step: 5580, Loss: 0.9374778866767883, Accuracy: 0.9722222089767456, Computation time: 0.9424240589141846\n",
      "Step: 5581, Loss: 0.9375767111778259, Accuracy: 0.9772727489471436, Computation time: 0.8537065982818604\n",
      "Step: 5582, Loss: 0.915870726108551, Accuracy: 1.0, Computation time: 0.8323276042938232\n",
      "Step: 5583, Loss: 0.9159085154533386, Accuracy: 1.0, Computation time: 0.9502975940704346\n",
      "Step: 5584, Loss: 0.9181820154190063, Accuracy: 1.0, Computation time: 1.0223162174224854\n",
      "Step: 5585, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 0.9556794166564941\n",
      "Step: 5586, Loss: 0.9158843159675598, Accuracy: 1.0, Computation time: 0.9597752094268799\n",
      "Step: 5587, Loss: 0.9158814549446106, Accuracy: 1.0, Computation time: 1.0174593925476074\n",
      "Step: 5588, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 0.8007562160491943\n",
      "Step: 5589, Loss: 0.9159212708473206, Accuracy: 1.0, Computation time: 0.8111701011657715\n",
      "Step: 5590, Loss: 0.9376763701438904, Accuracy: 0.949999988079071, Computation time: 1.1947929859161377\n",
      "Step: 5591, Loss: 0.915890097618103, Accuracy: 1.0, Computation time: 0.8654799461364746\n",
      "Step: 5592, Loss: 0.9158641695976257, Accuracy: 1.0, Computation time: 0.9888207912445068\n",
      "Step: 5593, Loss: 0.9356139302253723, Accuracy: 0.9750000238418579, Computation time: 0.9834296703338623\n",
      "Step: 5594, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 0.9043548107147217\n",
      "Step: 5595, Loss: 0.9391442537307739, Accuracy: 0.96875, Computation time: 1.198556900024414\n",
      "Step: 5596, Loss: 0.9159221053123474, Accuracy: 1.0, Computation time: 0.8758232593536377\n",
      "Step: 5597, Loss: 0.9370807409286499, Accuracy: 0.96875, Computation time: 0.7920856475830078\n",
      "Step: 5598, Loss: 0.9158820509910583, Accuracy: 1.0, Computation time: 0.8346538543701172\n",
      "Step: 5599, Loss: 0.9160374999046326, Accuracy: 1.0, Computation time: 0.9252414703369141\n",
      "Step: 5600, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 0.8440878391265869\n",
      "Step: 5601, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 0.7608551979064941\n",
      "Step: 5602, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 0.9179625511169434\n",
      "Step: 5603, Loss: 0.9169057011604309, Accuracy: 1.0, Computation time: 1.0661134719848633\n",
      "Step: 5604, Loss: 0.9158437848091125, Accuracy: 1.0, Computation time: 0.9038538932800293\n",
      "Step: 5605, Loss: 0.9158501625061035, Accuracy: 1.0, Computation time: 1.179121732711792\n",
      "Step: 5606, Loss: 0.9158507585525513, Accuracy: 1.0, Computation time: 0.8333308696746826\n",
      "Step: 5607, Loss: 0.9158676266670227, Accuracy: 1.0, Computation time: 0.983384370803833\n",
      "Step: 5608, Loss: 0.9158462285995483, Accuracy: 1.0, Computation time: 0.8877861499786377\n",
      "Step: 5609, Loss: 0.9159926772117615, Accuracy: 1.0, Computation time: 0.9541380405426025\n",
      "Step: 5610, Loss: 0.9376280903816223, Accuracy: 0.949999988079071, Computation time: 0.9070243835449219\n",
      "Step: 5611, Loss: 0.91587233543396, Accuracy: 1.0, Computation time: 0.8920564651489258\n",
      "Step: 5612, Loss: 0.9158509373664856, Accuracy: 1.0, Computation time: 0.886955738067627\n",
      "Step: 5613, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 0.8397736549377441\n",
      "Step: 5614, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 0.9389545917510986\n",
      "Step: 5615, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 0.9230406284332275\n",
      "Step: 5616, Loss: 0.9158656001091003, Accuracy: 1.0, Computation time: 1.194312572479248\n",
      "Step: 5617, Loss: 0.92652428150177, Accuracy: 0.9772727489471436, Computation time: 1.4051837921142578\n",
      "Step: 5618, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 1.3396306037902832\n",
      "Step: 5619, Loss: 0.9158613085746765, Accuracy: 1.0, Computation time: 0.7963223457336426\n",
      "Step: 5620, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 0.7900512218475342\n",
      "Step: 5621, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 0.9495561122894287\n",
      "Step: 5622, Loss: 0.9373711943626404, Accuracy: 0.96875, Computation time: 1.1772677898406982\n",
      "Step: 5623, Loss: 0.9375110864639282, Accuracy: 0.96875, Computation time: 0.8592610359191895\n",
      "Step: 5624, Loss: 0.9158568978309631, Accuracy: 1.0, Computation time: 0.7896080017089844\n",
      "Step: 5625, Loss: 0.9158732295036316, Accuracy: 1.0, Computation time: 0.8069016933441162\n",
      "Step: 5626, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 0.7963860034942627\n",
      "Step: 5627, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 0.771782398223877\n",
      "Step: 5628, Loss: 0.9158947467803955, Accuracy: 1.0, Computation time: 0.8497030735015869\n",
      "Step: 5629, Loss: 0.9158690571784973, Accuracy: 1.0, Computation time: 0.7901439666748047\n",
      "Step: 5630, Loss: 0.9366803169250488, Accuracy: 0.9833333492279053, Computation time: 0.776108980178833\n",
      "Step: 5631, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.7724819183349609\n",
      "Step: 5632, Loss: 0.9375501871109009, Accuracy: 0.9642857313156128, Computation time: 1.1587193012237549\n",
      "Step: 5633, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 0.8172397613525391\n",
      "Step: 5634, Loss: 0.9178535342216492, Accuracy: 1.0, Computation time: 1.020747184753418\n",
      "Step: 5635, Loss: 0.9158713221549988, Accuracy: 1.0, Computation time: 0.8876473903656006\n",
      "Step: 5636, Loss: 0.9158561825752258, Accuracy: 1.0, Computation time: 1.0455572605133057\n",
      "Step: 5637, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 0.9379663467407227\n",
      "Step: 5638, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 0.7678008079528809\n",
      "Step: 5639, Loss: 0.9158684015274048, Accuracy: 1.0, Computation time: 0.8606970310211182\n",
      "Step: 5640, Loss: 0.9158656597137451, Accuracy: 1.0, Computation time: 0.9658613204956055\n",
      "Step: 5641, Loss: 0.9158530235290527, Accuracy: 1.0, Computation time: 0.9648616313934326\n",
      "Step: 5642, Loss: 0.9158406257629395, Accuracy: 1.0, Computation time: 0.8439507484436035\n",
      "Step: 5643, Loss: 0.9169783592224121, Accuracy: 1.0, Computation time: 1.2151572704315186\n",
      "Step: 5644, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 0.7740507125854492\n",
      "Step: 5645, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 0.966588020324707\n",
      "Step: 5646, Loss: 0.9584975242614746, Accuracy: 0.9522727727890015, Computation time: 0.9349794387817383\n",
      "Step: 5647, Loss: 0.916652262210846, Accuracy: 1.0, Computation time: 0.9381871223449707\n",
      "Step: 5648, Loss: 0.9375043511390686, Accuracy: 0.9642857313156128, Computation time: 0.7582616806030273\n",
      "Step: 5649, Loss: 0.9158657193183899, Accuracy: 1.0, Computation time: 0.803591251373291\n",
      "Step: 5650, Loss: 0.9158996939659119, Accuracy: 1.0, Computation time: 0.8776419162750244\n",
      "Step: 5651, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 0.9497754573822021\n",
      "Step: 5652, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 0.7942724227905273\n",
      "Step: 5653, Loss: 0.9158531427383423, Accuracy: 1.0, Computation time: 0.7217261791229248\n",
      "Step: 5654, Loss: 0.9164472222328186, Accuracy: 1.0, Computation time: 1.0960698127746582\n",
      "Step: 5655, Loss: 0.9160084128379822, Accuracy: 1.0, Computation time: 1.1399750709533691\n",
      "Step: 5656, Loss: 0.915835976600647, Accuracy: 1.0, Computation time: 0.9169998168945312\n",
      "Step: 5657, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 0.8542225360870361\n",
      "Step: 5658, Loss: 0.9592388868331909, Accuracy: 0.9437500238418579, Computation time: 0.8734564781188965\n",
      "Step: 5659, Loss: 0.9158357977867126, Accuracy: 1.0, Computation time: 0.8786675930023193\n",
      "Step: 5660, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 0.8492040634155273\n",
      "Step: 5661, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 0.9296553134918213\n",
      "Step: 5662, Loss: 0.9158408641815186, Accuracy: 1.0, Computation time: 0.826258659362793\n",
      "Step: 5663, Loss: 0.9158382415771484, Accuracy: 1.0, Computation time: 0.7717523574829102\n",
      "Step: 5664, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 0.8444595336914062\n",
      "Step: 5665, Loss: 0.9174580574035645, Accuracy: 1.0, Computation time: 1.7202649116516113\n",
      "Step: 5666, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 0.7958159446716309\n",
      "Step: 5667, Loss: 0.9158391952514648, Accuracy: 1.0, Computation time: 0.7803664207458496\n",
      "Step: 5668, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 0.8431577682495117\n",
      "Step: 5669, Loss: 0.9159354567527771, Accuracy: 1.0, Computation time: 1.123173713684082\n",
      "Step: 5670, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 1.0217816829681396\n",
      "Step: 5671, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 1.2593562602996826\n",
      "Step: 5672, Loss: 0.91584712266922, Accuracy: 1.0, Computation time: 1.1242709159851074\n",
      "Step: 5673, Loss: 0.9158395528793335, Accuracy: 1.0, Computation time: 1.0794038772583008\n",
      "Step: 5674, Loss: 0.9158375859260559, Accuracy: 1.0, Computation time: 0.9126842021942139\n",
      "Step: 5675, Loss: 0.9158682823181152, Accuracy: 1.0, Computation time: 1.2906546592712402\n",
      "Step: 5676, Loss: 0.9158390760421753, Accuracy: 1.0, Computation time: 1.4667143821716309\n",
      "Step: 5677, Loss: 0.9158377051353455, Accuracy: 1.0, Computation time: 0.7854516506195068\n",
      "Step: 5678, Loss: 0.9375330209732056, Accuracy: 0.9642857313156128, Computation time: 0.8893759250640869\n",
      "Step: 5679, Loss: 0.935300350189209, Accuracy: 0.9583333730697632, Computation time: 0.8705928325653076\n",
      "Step: 5680, Loss: 0.9158584475517273, Accuracy: 1.0, Computation time: 0.854647159576416\n",
      "Step: 5681, Loss: 0.9300132989883423, Accuracy: 0.949999988079071, Computation time: 0.8947880268096924\n",
      "Step: 5682, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 0.8884470462799072\n",
      "Step: 5683, Loss: 0.9158810973167419, Accuracy: 1.0, Computation time: 0.8313565254211426\n",
      "Step: 5684, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 0.9703466892242432\n",
      "Step: 5685, Loss: 0.9159364700317383, Accuracy: 1.0, Computation time: 1.5503814220428467\n",
      "Step: 5686, Loss: 0.9159437417984009, Accuracy: 1.0, Computation time: 0.885460615158081\n",
      "Step: 5687, Loss: 0.9159100651741028, Accuracy: 1.0, Computation time: 0.8094921112060547\n",
      "Step: 5688, Loss: 0.9158767461776733, Accuracy: 1.0, Computation time: 0.8833587169647217\n",
      "Step: 5689, Loss: 0.9158370494842529, Accuracy: 1.0, Computation time: 0.8750038146972656\n",
      "Step: 5690, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 0.9469790458679199\n",
      "Step: 5691, Loss: 0.9158686399459839, Accuracy: 1.0, Computation time: 1.0077555179595947\n",
      "Step: 5692, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 0.7830078601837158\n",
      "Step: 5693, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.264857530593872\n",
      "Step: 5694, Loss: 0.9339020252227783, Accuracy: 0.9375, Computation time: 1.5230071544647217\n",
      "Step: 5695, Loss: 0.9374119639396667, Accuracy: 0.9583333730697632, Computation time: 1.0947694778442383\n",
      "Step: 5696, Loss: 0.9159457683563232, Accuracy: 1.0, Computation time: 0.8973071575164795\n",
      "Step: 5697, Loss: 0.9158750772476196, Accuracy: 1.0, Computation time: 0.875734806060791\n",
      "########################\n",
      "Test loss: 1.1261972188949585, Test Accuracy_epoch41: 0.6838799118995667\n",
      "########################\n",
      "Step: 5698, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 0.8296427726745605\n",
      "Step: 5699, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.106734275817871\n",
      "Step: 5700, Loss: 0.9158548712730408, Accuracy: 1.0, Computation time: 0.8016393184661865\n",
      "Step: 5701, Loss: 0.9277021884918213, Accuracy: 1.0, Computation time: 1.2821011543273926\n",
      "Step: 5702, Loss: 0.9158803820610046, Accuracy: 1.0, Computation time: 1.0901496410369873\n",
      "Step: 5703, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 0.9613027572631836\n",
      "Step: 5704, Loss: 0.9158998131752014, Accuracy: 1.0, Computation time: 0.8423128128051758\n",
      "Step: 5705, Loss: 0.9159418940544128, Accuracy: 1.0, Computation time: 1.0797903537750244\n",
      "Step: 5706, Loss: 0.9160134792327881, Accuracy: 1.0, Computation time: 1.226952075958252\n",
      "Step: 5707, Loss: 0.9367750883102417, Accuracy: 0.9772727489471436, Computation time: 1.1039228439331055\n",
      "Step: 5708, Loss: 0.937808632850647, Accuracy: 0.9772727489471436, Computation time: 0.9031870365142822\n",
      "Step: 5709, Loss: 0.915854811668396, Accuracy: 1.0, Computation time: 0.8650577068328857\n",
      "Step: 5710, Loss: 0.9591631889343262, Accuracy: 0.9583333730697632, Computation time: 0.8949377536773682\n",
      "Step: 5711, Loss: 0.9160884618759155, Accuracy: 1.0, Computation time: 0.87601637840271\n",
      "Step: 5712, Loss: 0.9571789503097534, Accuracy: 0.9500000476837158, Computation time: 1.2088046073913574\n",
      "Step: 5713, Loss: 0.9158994555473328, Accuracy: 1.0, Computation time: 1.0767958164215088\n",
      "Step: 5714, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 1.0832164287567139\n",
      "Step: 5715, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 1.1990022659301758\n",
      "Step: 5716, Loss: 0.9287307262420654, Accuracy: 0.9642857313156128, Computation time: 1.0354244709014893\n",
      "Step: 5717, Loss: 0.9159151911735535, Accuracy: 1.0, Computation time: 1.0722079277038574\n",
      "Step: 5718, Loss: 0.9159078598022461, Accuracy: 1.0, Computation time: 1.2836225032806396\n",
      "Step: 5719, Loss: 0.9159905910491943, Accuracy: 1.0, Computation time: 1.3357746601104736\n",
      "Step: 5720, Loss: 0.9164890646934509, Accuracy: 1.0, Computation time: 1.2153394222259521\n",
      "Step: 5721, Loss: 0.9374765157699585, Accuracy: 0.9750000238418579, Computation time: 1.282193899154663\n",
      "Step: 5722, Loss: 0.9158921241760254, Accuracy: 1.0, Computation time: 1.0261125564575195\n",
      "Step: 5723, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 1.0483570098876953\n",
      "Step: 5724, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 1.092878818511963\n",
      "Step: 5725, Loss: 0.9158666133880615, Accuracy: 1.0, Computation time: 1.3568851947784424\n",
      "Step: 5726, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 1.1552329063415527\n",
      "Step: 5727, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.8796727657318115\n",
      "Step: 5728, Loss: 0.9159157276153564, Accuracy: 1.0, Computation time: 1.0902483463287354\n",
      "Step: 5729, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 0.8317065238952637\n",
      "Step: 5730, Loss: 0.9158516526222229, Accuracy: 1.0, Computation time: 0.8517065048217773\n",
      "Step: 5731, Loss: 0.9374943971633911, Accuracy: 0.9772727489471436, Computation time: 0.8061366081237793\n",
      "Step: 5732, Loss: 0.9159166216850281, Accuracy: 1.0, Computation time: 1.2086145877838135\n",
      "Step: 5733, Loss: 0.9158436059951782, Accuracy: 1.0, Computation time: 0.8427724838256836\n",
      "Step: 5734, Loss: 0.915862500667572, Accuracy: 1.0, Computation time: 0.9507818222045898\n",
      "Step: 5735, Loss: 0.9392396211624146, Accuracy: 0.9772727489471436, Computation time: 0.8431582450866699\n",
      "Step: 5736, Loss: 0.9158786535263062, Accuracy: 1.0, Computation time: 0.8883364200592041\n",
      "Step: 5737, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 0.8386211395263672\n",
      "Step: 5738, Loss: 0.9166913628578186, Accuracy: 1.0, Computation time: 1.5802865028381348\n",
      "Step: 5739, Loss: 0.927212655544281, Accuracy: 0.9166666865348816, Computation time: 1.5235850811004639\n",
      "Step: 5740, Loss: 0.9158605933189392, Accuracy: 1.0, Computation time: 0.8432323932647705\n",
      "Step: 5741, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 0.8446741104125977\n",
      "Step: 5742, Loss: 0.9326499700546265, Accuracy: 0.9750000238418579, Computation time: 0.8384008407592773\n",
      "Step: 5743, Loss: 0.91596519947052, Accuracy: 1.0, Computation time: 1.0867135524749756\n",
      "Step: 5744, Loss: 0.9159536361694336, Accuracy: 1.0, Computation time: 1.0142688751220703\n",
      "Step: 5745, Loss: 0.91764897108078, Accuracy: 1.0, Computation time: 0.9347562789916992\n",
      "Step: 5746, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 0.8730971813201904\n",
      "Step: 5747, Loss: 0.9376246333122253, Accuracy: 0.9791666865348816, Computation time: 1.090639352798462\n",
      "Step: 5748, Loss: 0.9159536957740784, Accuracy: 1.0, Computation time: 1.0593774318695068\n",
      "Step: 5749, Loss: 0.9158950448036194, Accuracy: 1.0, Computation time: 1.2880547046661377\n",
      "Step: 5750, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 0.8891699314117432\n",
      "Step: 5751, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.0646772384643555\n",
      "Step: 5752, Loss: 0.9373708963394165, Accuracy: 0.9642857313156128, Computation time: 0.9325284957885742\n",
      "Step: 5753, Loss: 0.9159336686134338, Accuracy: 1.0, Computation time: 0.9045827388763428\n",
      "Step: 5754, Loss: 0.9159319996833801, Accuracy: 1.0, Computation time: 0.9721148014068604\n",
      "Step: 5755, Loss: 0.915923535823822, Accuracy: 1.0, Computation time: 0.8987541198730469\n",
      "Step: 5756, Loss: 0.9162434935569763, Accuracy: 1.0, Computation time: 1.0292527675628662\n",
      "Step: 5757, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 0.9519128799438477\n",
      "Step: 5758, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 0.8949635028839111\n",
      "Step: 5759, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 0.8287301063537598\n",
      "Step: 5760, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.8439614772796631\n",
      "Step: 5761, Loss: 0.9158747792243958, Accuracy: 1.0, Computation time: 0.8012847900390625\n",
      "Step: 5762, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.84194016456604\n",
      "Step: 5763, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 0.9232738018035889\n",
      "Step: 5764, Loss: 0.9375590682029724, Accuracy: 0.9750000238418579, Computation time: 0.8243412971496582\n",
      "Step: 5765, Loss: 0.9158709049224854, Accuracy: 1.0, Computation time: 0.8721277713775635\n",
      "Step: 5766, Loss: 0.9375529885292053, Accuracy: 0.949999988079071, Computation time: 0.8717207908630371\n",
      "Step: 5767, Loss: 0.915858805179596, Accuracy: 1.0, Computation time: 0.8534889221191406\n",
      "Step: 5768, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 0.9062409400939941\n",
      "Step: 5769, Loss: 0.9376094937324524, Accuracy: 0.9722222089767456, Computation time: 0.9950511455535889\n",
      "Step: 5770, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.9308068752288818\n",
      "Step: 5771, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 0.9568400382995605\n",
      "Step: 5772, Loss: 0.9762282967567444, Accuracy: 0.9166666269302368, Computation time: 1.50752854347229\n",
      "Step: 5773, Loss: 0.9159206748008728, Accuracy: 1.0, Computation time: 0.7847530841827393\n",
      "Step: 5774, Loss: 0.937516987323761, Accuracy: 0.9772727489471436, Computation time: 0.8131325244903564\n",
      "Step: 5775, Loss: 0.9160504937171936, Accuracy: 1.0, Computation time: 1.2109720706939697\n",
      "Step: 5776, Loss: 0.9160128235816956, Accuracy: 1.0, Computation time: 1.0176458358764648\n",
      "Step: 5777, Loss: 0.9159902334213257, Accuracy: 1.0, Computation time: 0.8122820854187012\n",
      "Step: 5778, Loss: 0.9167526364326477, Accuracy: 1.0, Computation time: 1.057851791381836\n",
      "Step: 5779, Loss: 0.9159835577011108, Accuracy: 1.0, Computation time: 0.9362547397613525\n",
      "Step: 5780, Loss: 0.9159531593322754, Accuracy: 1.0, Computation time: 0.8228504657745361\n",
      "Step: 5781, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 0.8664414882659912\n",
      "Step: 5782, Loss: 0.9158903360366821, Accuracy: 1.0, Computation time: 0.8706698417663574\n",
      "Step: 5783, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 1.0326769351959229\n",
      "Step: 5784, Loss: 0.9375999569892883, Accuracy: 0.9583333730697632, Computation time: 1.0152153968811035\n",
      "Step: 5785, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 0.9007046222686768\n",
      "Step: 5786, Loss: 0.9158865213394165, Accuracy: 1.0, Computation time: 1.5523786544799805\n",
      "Step: 5787, Loss: 0.9295679926872253, Accuracy: 0.9750000238418579, Computation time: 1.2696115970611572\n",
      "Step: 5788, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 0.7911145687103271\n",
      "Step: 5789, Loss: 0.918126106262207, Accuracy: 1.0, Computation time: 0.8848123550415039\n",
      "Step: 5790, Loss: 0.915917694568634, Accuracy: 1.0, Computation time: 0.8095920085906982\n",
      "Step: 5791, Loss: 0.9159870147705078, Accuracy: 1.0, Computation time: 1.128695011138916\n",
      "Step: 5792, Loss: 0.9159453511238098, Accuracy: 1.0, Computation time: 1.1514110565185547\n",
      "Step: 5793, Loss: 0.9159067273139954, Accuracy: 1.0, Computation time: 0.9085276126861572\n",
      "Step: 5794, Loss: 0.9158751368522644, Accuracy: 1.0, Computation time: 0.9749438762664795\n",
      "Step: 5795, Loss: 0.936038613319397, Accuracy: 0.9772727489471436, Computation time: 1.0060770511627197\n",
      "Step: 5796, Loss: 0.9375358819961548, Accuracy: 0.9750000238418579, Computation time: 0.9061129093170166\n",
      "Step: 5797, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 0.7299389839172363\n",
      "Step: 5798, Loss: 0.9158623814582825, Accuracy: 1.0, Computation time: 0.8499438762664795\n",
      "Step: 5799, Loss: 0.9158685207366943, Accuracy: 1.0, Computation time: 0.8522369861602783\n",
      "Step: 5800, Loss: 0.9160019755363464, Accuracy: 1.0, Computation time: 0.9237658977508545\n",
      "Step: 5801, Loss: 0.9158793687820435, Accuracy: 1.0, Computation time: 0.8016800880432129\n",
      "Step: 5802, Loss: 0.9184737801551819, Accuracy: 1.0, Computation time: 0.8390958309173584\n",
      "Step: 5803, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 1.037877082824707\n",
      "Step: 5804, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 0.7540345191955566\n",
      "Step: 5805, Loss: 0.9365621209144592, Accuracy: 0.9791666865348816, Computation time: 0.9836254119873047\n",
      "Step: 5806, Loss: 0.9375330209732056, Accuracy: 0.9722222089767456, Computation time: 0.6987760066986084\n",
      "Step: 5807, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 0.9782876968383789\n",
      "Step: 5808, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 0.9185030460357666\n",
      "Step: 5809, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.1090445518493652\n",
      "Step: 5810, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.2044270038604736\n",
      "Step: 5811, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 0.8789453506469727\n",
      "Step: 5812, Loss: 0.9200369715690613, Accuracy: 1.0, Computation time: 0.8691492080688477\n",
      "Step: 5813, Loss: 0.9158549308776855, Accuracy: 1.0, Computation time: 0.9693176746368408\n",
      "Step: 5814, Loss: 0.9158592224121094, Accuracy: 1.0, Computation time: 0.7801637649536133\n",
      "Step: 5815, Loss: 0.9375148415565491, Accuracy: 0.9722222089767456, Computation time: 0.8722248077392578\n",
      "Step: 5816, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 0.958606481552124\n",
      "Step: 5817, Loss: 0.9158639311790466, Accuracy: 1.0, Computation time: 0.974083423614502\n",
      "Step: 5818, Loss: 0.9158746600151062, Accuracy: 1.0, Computation time: 0.7700819969177246\n",
      "Step: 5819, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 0.8304235935211182\n",
      "Step: 5820, Loss: 0.9374855160713196, Accuracy: 0.9722222089767456, Computation time: 1.0221037864685059\n",
      "Step: 5821, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 0.9109070301055908\n",
      "Step: 5822, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.8274123668670654\n",
      "Step: 5823, Loss: 0.9163046479225159, Accuracy: 1.0, Computation time: 0.8596594333648682\n",
      "Step: 5824, Loss: 0.9159290790557861, Accuracy: 1.0, Computation time: 0.8414933681488037\n",
      "Step: 5825, Loss: 0.9158707857131958, Accuracy: 1.0, Computation time: 1.0077149868011475\n",
      "Step: 5826, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 0.7849729061126709\n",
      "Step: 5827, Loss: 0.91585373878479, Accuracy: 1.0, Computation time: 0.7662129402160645\n",
      "Step: 5828, Loss: 0.93758225440979, Accuracy: 0.9750000238418579, Computation time: 0.8202154636383057\n",
      "Step: 5829, Loss: 0.9375518560409546, Accuracy: 0.9772727489471436, Computation time: 1.0355157852172852\n",
      "Step: 5830, Loss: 0.935661256313324, Accuracy: 0.9750000238418579, Computation time: 1.0031366348266602\n",
      "Step: 5831, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 0.8454079627990723\n",
      "Step: 5832, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 0.8178555965423584\n",
      "Step: 5833, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.0491039752960205\n",
      "Step: 5834, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 0.9651622772216797\n",
      "Step: 5835, Loss: 0.9376050233840942, Accuracy: 0.9583333730697632, Computation time: 1.220336675643921\n",
      "Step: 5836, Loss: 0.9158776998519897, Accuracy: 1.0, Computation time: 0.9773497581481934\n",
      "########################\n",
      "Test loss: 1.1261428594589233, Test Accuracy_epoch42: 0.6852096915245056\n",
      "########################\n",
      "Step: 5837, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 0.9107120037078857\n",
      "Step: 5838, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 0.8985812664031982\n",
      "Step: 5839, Loss: 0.91586834192276, Accuracy: 1.0, Computation time: 0.774524450302124\n",
      "Step: 5840, Loss: 0.9158868789672852, Accuracy: 1.0, Computation time: 0.9620985984802246\n",
      "Step: 5841, Loss: 0.9159672260284424, Accuracy: 1.0, Computation time: 0.9328892230987549\n",
      "Step: 5842, Loss: 0.9158700108528137, Accuracy: 1.0, Computation time: 0.8312489986419678\n",
      "Step: 5843, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.8839898109436035\n",
      "Step: 5844, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 1.087735652923584\n",
      "Step: 5845, Loss: 0.9376120567321777, Accuracy: 0.9772727489471436, Computation time: 0.971362829208374\n",
      "Step: 5846, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 1.1289312839508057\n",
      "Step: 5847, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.0464441776275635\n",
      "Step: 5848, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 0.881939172744751\n",
      "Step: 5849, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 0.8208198547363281\n",
      "Step: 5850, Loss: 0.9174856543540955, Accuracy: 1.0, Computation time: 1.2913768291473389\n",
      "Step: 5851, Loss: 0.9158685803413391, Accuracy: 1.0, Computation time: 0.8295693397521973\n",
      "Step: 5852, Loss: 0.915842592716217, Accuracy: 1.0, Computation time: 0.7810299396514893\n",
      "Step: 5853, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 0.8268575668334961\n",
      "Step: 5854, Loss: 0.9158398509025574, Accuracy: 1.0, Computation time: 0.855679988861084\n",
      "Step: 5855, Loss: 0.915844738483429, Accuracy: 1.0, Computation time: 0.956268310546875\n",
      "Step: 5856, Loss: 0.9249246716499329, Accuracy: 1.0, Computation time: 0.7806251049041748\n",
      "Step: 5857, Loss: 0.9168528914451599, Accuracy: 1.0, Computation time: 1.2247869968414307\n",
      "Step: 5858, Loss: 0.9158723950386047, Accuracy: 1.0, Computation time: 0.8734381198883057\n",
      "Step: 5859, Loss: 0.9158785343170166, Accuracy: 1.0, Computation time: 0.7969675064086914\n",
      "Step: 5860, Loss: 0.9158737659454346, Accuracy: 1.0, Computation time: 0.7841618061065674\n",
      "Step: 5861, Loss: 0.9159466624259949, Accuracy: 1.0, Computation time: 0.8083488941192627\n",
      "Step: 5862, Loss: 0.9158478379249573, Accuracy: 1.0, Computation time: 0.7544786930084229\n",
      "Step: 5863, Loss: 0.9346838593482971, Accuracy: 0.9772727489471436, Computation time: 1.0900003910064697\n",
      "Step: 5864, Loss: 0.9158371686935425, Accuracy: 1.0, Computation time: 1.191286325454712\n",
      "Step: 5865, Loss: 0.915908932685852, Accuracy: 1.0, Computation time: 1.3082928657531738\n",
      "Step: 5866, Loss: 0.9158629179000854, Accuracy: 1.0, Computation time: 0.6996521949768066\n",
      "Step: 5867, Loss: 0.9159411787986755, Accuracy: 1.0, Computation time: 0.9845747947692871\n",
      "Step: 5868, Loss: 0.9171877503395081, Accuracy: 1.0, Computation time: 1.226309061050415\n",
      "Step: 5869, Loss: 0.9374232292175293, Accuracy: 0.9807692766189575, Computation time: 1.0172650814056396\n",
      "Step: 5870, Loss: 0.9158765077590942, Accuracy: 1.0, Computation time: 0.9177441596984863\n",
      "Step: 5871, Loss: 0.915883481502533, Accuracy: 1.0, Computation time: 0.8451759815216064\n",
      "Step: 5872, Loss: 0.9158586263656616, Accuracy: 1.0, Computation time: 0.7918901443481445\n",
      "Step: 5873, Loss: 0.9364177584648132, Accuracy: 0.9722222089767456, Computation time: 1.2054386138916016\n",
      "Step: 5874, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 0.8285200595855713\n",
      "Step: 5875, Loss: 0.9375492930412292, Accuracy: 0.9642857313156128, Computation time: 0.852586030960083\n",
      "Step: 5876, Loss: 0.9158424735069275, Accuracy: 1.0, Computation time: 0.7838113307952881\n",
      "Step: 5877, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 0.856999397277832\n",
      "Step: 5878, Loss: 0.9158693552017212, Accuracy: 1.0, Computation time: 0.7513077259063721\n",
      "Step: 5879, Loss: 0.9159985780715942, Accuracy: 1.0, Computation time: 0.960350513458252\n",
      "Step: 5880, Loss: 0.9180394411087036, Accuracy: 1.0, Computation time: 0.8346583843231201\n",
      "Step: 5881, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 0.7598254680633545\n",
      "Step: 5882, Loss: 0.9158475399017334, Accuracy: 1.0, Computation time: 0.7583444118499756\n",
      "Step: 5883, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 0.9440169334411621\n",
      "Step: 5884, Loss: 0.915856122970581, Accuracy: 1.0, Computation time: 0.8357386589050293\n",
      "Step: 5885, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 0.9239931106567383\n",
      "Step: 5886, Loss: 0.9158545732498169, Accuracy: 1.0, Computation time: 0.8645303249359131\n",
      "Step: 5887, Loss: 0.9158574342727661, Accuracy: 1.0, Computation time: 1.1691045761108398\n",
      "Step: 5888, Loss: 0.9158923625946045, Accuracy: 1.0, Computation time: 0.8451418876647949\n",
      "Step: 5889, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 0.8349788188934326\n",
      "Step: 5890, Loss: 0.9158406257629395, Accuracy: 1.0, Computation time: 1.0186347961425781\n",
      "Step: 5891, Loss: 0.9158391356468201, Accuracy: 1.0, Computation time: 0.7858123779296875\n",
      "Step: 5892, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 0.7517786026000977\n",
      "Step: 5893, Loss: 0.9375269412994385, Accuracy: 0.9772727489471436, Computation time: 0.8258533477783203\n",
      "Step: 5894, Loss: 0.9158350229263306, Accuracy: 1.0, Computation time: 0.758882999420166\n",
      "Step: 5895, Loss: 0.9158774018287659, Accuracy: 1.0, Computation time: 0.9578647613525391\n",
      "Step: 5896, Loss: 0.9375121593475342, Accuracy: 0.9772727489471436, Computation time: 0.9885764122009277\n",
      "Step: 5897, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 0.7650516033172607\n",
      "Step: 5898, Loss: 0.9180770516395569, Accuracy: 1.0, Computation time: 1.3978419303894043\n",
      "Step: 5899, Loss: 0.9158832430839539, Accuracy: 1.0, Computation time: 1.7539937496185303\n",
      "Step: 5900, Loss: 0.9158716797828674, Accuracy: 1.0, Computation time: 0.8261826038360596\n",
      "Step: 5901, Loss: 0.9160158634185791, Accuracy: 1.0, Computation time: 1.2161192893981934\n",
      "Step: 5902, Loss: 0.9159197807312012, Accuracy: 1.0, Computation time: 0.8677504062652588\n",
      "Step: 5903, Loss: 0.9158917665481567, Accuracy: 1.0, Computation time: 0.8495004177093506\n",
      "Step: 5904, Loss: 0.9375104904174805, Accuracy: 0.9750000238418579, Computation time: 1.1273770332336426\n",
      "Step: 5905, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 0.8504183292388916\n",
      "Step: 5906, Loss: 0.9368636608123779, Accuracy: 0.9807692766189575, Computation time: 1.0882282257080078\n",
      "Step: 5907, Loss: 0.9370906352996826, Accuracy: 0.9791666865348816, Computation time: 0.9624793529510498\n",
      "Step: 5908, Loss: 0.9158420562744141, Accuracy: 1.0, Computation time: 0.7670631408691406\n",
      "Step: 5909, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 0.8972818851470947\n",
      "Step: 5910, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.1658029556274414\n",
      "Step: 5911, Loss: 0.9160396456718445, Accuracy: 1.0, Computation time: 0.8732342720031738\n",
      "Step: 5912, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 0.8328263759613037\n",
      "Step: 5913, Loss: 0.9159349799156189, Accuracy: 1.0, Computation time: 0.8693346977233887\n",
      "Step: 5914, Loss: 0.9163005948066711, Accuracy: 1.0, Computation time: 1.3738417625427246\n",
      "Step: 5915, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 0.8619999885559082\n",
      "Step: 5916, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 0.8633923530578613\n",
      "Step: 5917, Loss: 0.9158802032470703, Accuracy: 1.0, Computation time: 1.1060407161712646\n",
      "Step: 5918, Loss: 0.9173200726509094, Accuracy: 1.0, Computation time: 0.9698772430419922\n",
      "Step: 5919, Loss: 0.9352710247039795, Accuracy: 0.9791666865348816, Computation time: 0.99996018409729\n",
      "Step: 5920, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 1.3235387802124023\n",
      "Step: 5921, Loss: 0.915907621383667, Accuracy: 1.0, Computation time: 0.8048477172851562\n",
      "Step: 5922, Loss: 0.9159123301506042, Accuracy: 1.0, Computation time: 1.1128199100494385\n",
      "Step: 5923, Loss: 0.9162449240684509, Accuracy: 1.0, Computation time: 1.0290682315826416\n",
      "Step: 5924, Loss: 0.9179158210754395, Accuracy: 1.0, Computation time: 1.286818027496338\n",
      "Step: 5925, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 0.9849300384521484\n",
      "Step: 5926, Loss: 0.9159149527549744, Accuracy: 1.0, Computation time: 1.0288829803466797\n",
      "Step: 5927, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 0.9883546829223633\n",
      "Step: 5928, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 0.940371036529541\n",
      "Step: 5929, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.1742205619812012\n",
      "Step: 5930, Loss: 0.9158816337585449, Accuracy: 1.0, Computation time: 1.0814337730407715\n",
      "Step: 5931, Loss: 0.9158622026443481, Accuracy: 1.0, Computation time: 0.9289352893829346\n",
      "Step: 5932, Loss: 0.9158728122711182, Accuracy: 1.0, Computation time: 0.8519778251647949\n",
      "Step: 5933, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 0.8392627239227295\n",
      "Step: 5934, Loss: 0.9158646464347839, Accuracy: 1.0, Computation time: 0.9009814262390137\n",
      "Step: 5935, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 0.7965035438537598\n",
      "Step: 5936, Loss: 0.9161198735237122, Accuracy: 1.0, Computation time: 1.496009111404419\n",
      "Step: 5937, Loss: 0.9158503413200378, Accuracy: 1.0, Computation time: 0.8726305961608887\n",
      "Step: 5938, Loss: 0.9158625602722168, Accuracy: 1.0, Computation time: 1.0195159912109375\n",
      "Step: 5939, Loss: 0.9340404272079468, Accuracy: 0.9642857313156128, Computation time: 1.0943882465362549\n",
      "Step: 5940, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 0.7756764888763428\n",
      "Step: 5941, Loss: 0.9158618450164795, Accuracy: 1.0, Computation time: 0.9176766872406006\n",
      "Step: 5942, Loss: 0.915864884853363, Accuracy: 1.0, Computation time: 1.1036934852600098\n",
      "Step: 5943, Loss: 0.9158528447151184, Accuracy: 1.0, Computation time: 0.9359676837921143\n",
      "Step: 5944, Loss: 0.9158549904823303, Accuracy: 1.0, Computation time: 0.8890833854675293\n",
      "Step: 5945, Loss: 0.9158698320388794, Accuracy: 1.0, Computation time: 1.2475478649139404\n",
      "Step: 5946, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 0.9655003547668457\n",
      "Step: 5947, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 1.1069233417510986\n",
      "Step: 5948, Loss: 0.9375221729278564, Accuracy: 0.9722222089767456, Computation time: 1.093198537826538\n",
      "Step: 5949, Loss: 0.9243859052658081, Accuracy: 1.0, Computation time: 1.0806500911712646\n",
      "Step: 5950, Loss: 0.9375052452087402, Accuracy: 0.9772727489471436, Computation time: 1.173506259918213\n",
      "Step: 5951, Loss: 0.9159096479415894, Accuracy: 1.0, Computation time: 1.3851163387298584\n",
      "Step: 5952, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 1.0667014122009277\n",
      "Step: 5953, Loss: 0.9160432815551758, Accuracy: 1.0, Computation time: 0.9253838062286377\n",
      "Step: 5954, Loss: 0.937129557132721, Accuracy: 0.9852941036224365, Computation time: 1.110745906829834\n",
      "Step: 5955, Loss: 0.9160880446434021, Accuracy: 1.0, Computation time: 1.6047773361206055\n",
      "Step: 5956, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 0.7889342308044434\n",
      "Step: 5957, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 0.7462718486785889\n",
      "Step: 5958, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 0.849909782409668\n",
      "Step: 5959, Loss: 0.9376960396766663, Accuracy: 0.9750000238418579, Computation time: 0.9533066749572754\n",
      "Step: 5960, Loss: 0.9159047603607178, Accuracy: 1.0, Computation time: 0.973956823348999\n",
      "Step: 5961, Loss: 0.9159371256828308, Accuracy: 1.0, Computation time: 1.124455451965332\n",
      "Step: 5962, Loss: 0.9158890843391418, Accuracy: 1.0, Computation time: 0.912388801574707\n",
      "Step: 5963, Loss: 0.9375290870666504, Accuracy: 0.9791666865348816, Computation time: 0.8966250419616699\n",
      "Step: 5964, Loss: 0.9363211393356323, Accuracy: 0.9772727489471436, Computation time: 2.0284643173217773\n",
      "Step: 5965, Loss: 0.9158626198768616, Accuracy: 1.0, Computation time: 1.0086753368377686\n",
      "Step: 5966, Loss: 0.9158841967582703, Accuracy: 1.0, Computation time: 0.8457367420196533\n",
      "Step: 5967, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.1686313152313232\n",
      "Step: 5968, Loss: 0.9172815084457397, Accuracy: 1.0, Computation time: 0.9708695411682129\n",
      "Step: 5969, Loss: 0.9377801418304443, Accuracy: 0.9642857313156128, Computation time: 0.9465646743774414\n",
      "Step: 5970, Loss: 0.9158567190170288, Accuracy: 1.0, Computation time: 0.9723420143127441\n",
      "Step: 5971, Loss: 0.9158616662025452, Accuracy: 1.0, Computation time: 0.849236249923706\n",
      "Step: 5972, Loss: 0.91588294506073, Accuracy: 1.0, Computation time: 0.8064606189727783\n",
      "Step: 5973, Loss: 0.9158817529678345, Accuracy: 1.0, Computation time: 0.8130640983581543\n",
      "Step: 5974, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 0.8017187118530273\n",
      "Step: 5975, Loss: 0.9159115552902222, Accuracy: 1.0, Computation time: 1.0101563930511475\n",
      "########################\n",
      "Test loss: 1.1276291608810425, Test Accuracy_epoch43: 0.6872158050537109\n",
      "########################\n",
      "Step: 5976, Loss: 0.9376733303070068, Accuracy: 0.949999988079071, Computation time: 0.9947159290313721\n",
      "Step: 5977, Loss: 0.9159190058708191, Accuracy: 1.0, Computation time: 0.9141576290130615\n",
      "Step: 5978, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 0.8997085094451904\n",
      "Step: 5979, Loss: 0.9158769845962524, Accuracy: 1.0, Computation time: 0.7778482437133789\n",
      "Step: 5980, Loss: 0.9158614277839661, Accuracy: 1.0, Computation time: 0.8366167545318604\n",
      "Step: 5981, Loss: 0.9160999059677124, Accuracy: 1.0, Computation time: 1.0455677509307861\n",
      "Step: 5982, Loss: 0.9168702363967896, Accuracy: 1.0, Computation time: 1.3256502151489258\n",
      "Step: 5983, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.099473237991333\n",
      "Step: 5984, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 1.0524897575378418\n",
      "Step: 5985, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 1.0205445289611816\n",
      "Step: 5986, Loss: 0.9375004768371582, Accuracy: 0.9772727489471436, Computation time: 0.8738086223602295\n",
      "Step: 5987, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 0.8342587947845459\n",
      "Step: 5988, Loss: 0.9158572554588318, Accuracy: 1.0, Computation time: 0.7346317768096924\n",
      "Step: 5989, Loss: 0.9158799648284912, Accuracy: 1.0, Computation time: 0.8499982357025146\n",
      "Step: 5990, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 0.8419349193572998\n",
      "Step: 5991, Loss: 0.9160228967666626, Accuracy: 1.0, Computation time: 0.8602571487426758\n",
      "Step: 5992, Loss: 0.9158540368080139, Accuracy: 1.0, Computation time: 0.8293213844299316\n",
      "Step: 5993, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 0.7939693927764893\n",
      "Step: 5994, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 0.9405126571655273\n",
      "Step: 5995, Loss: 0.9158442616462708, Accuracy: 1.0, Computation time: 0.9109146595001221\n",
      "Step: 5996, Loss: 0.9158495664596558, Accuracy: 1.0, Computation time: 0.8211483955383301\n",
      "Step: 5997, Loss: 0.9158387780189514, Accuracy: 1.0, Computation time: 0.9205353260040283\n",
      "Step: 5998, Loss: 0.9158393740653992, Accuracy: 1.0, Computation time: 0.9710068702697754\n",
      "Step: 5999, Loss: 0.9158839583396912, Accuracy: 1.0, Computation time: 1.3018698692321777\n",
      "Step: 6000, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.072709321975708\n",
      "Step: 6001, Loss: 0.915865421295166, Accuracy: 1.0, Computation time: 0.8471183776855469\n",
      "Step: 6002, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 0.7665402889251709\n",
      "Step: 6003, Loss: 0.9158406853675842, Accuracy: 1.0, Computation time: 0.904595136642456\n",
      "Step: 6004, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 0.9502701759338379\n",
      "Step: 6005, Loss: 0.9158368706703186, Accuracy: 1.0, Computation time: 0.7939901351928711\n",
      "Step: 6006, Loss: 0.9375002384185791, Accuracy: 0.96875, Computation time: 0.8772919178009033\n",
      "Step: 6007, Loss: 0.9158417582511902, Accuracy: 1.0, Computation time: 0.8349666595458984\n",
      "Step: 6008, Loss: 0.937465488910675, Accuracy: 0.9722222089767456, Computation time: 0.8164181709289551\n",
      "Step: 6009, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 1.0120291709899902\n",
      "Step: 6010, Loss: 0.9158813953399658, Accuracy: 1.0, Computation time: 0.7563831806182861\n",
      "Step: 6011, Loss: 0.9158385396003723, Accuracy: 1.0, Computation time: 0.8682007789611816\n",
      "Step: 6012, Loss: 0.9158457517623901, Accuracy: 1.0, Computation time: 0.8931484222412109\n",
      "Step: 6013, Loss: 0.9374457001686096, Accuracy: 0.9821428656578064, Computation time: 0.9117810726165771\n",
      "Step: 6014, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.046983003616333\n",
      "Step: 6015, Loss: 0.9158449172973633, Accuracy: 1.0, Computation time: 0.8242878913879395\n",
      "Step: 6016, Loss: 0.9158527255058289, Accuracy: 1.0, Computation time: 1.0330533981323242\n",
      "Step: 6017, Loss: 0.9159037470817566, Accuracy: 1.0, Computation time: 0.8532557487487793\n",
      "Step: 6018, Loss: 0.9375849962234497, Accuracy: 0.9807692766189575, Computation time: 0.9434916973114014\n",
      "Step: 6019, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 1.0112812519073486\n",
      "Step: 6020, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 0.942033052444458\n",
      "Step: 6021, Loss: 0.9158353209495544, Accuracy: 1.0, Computation time: 0.758568286895752\n",
      "Step: 6022, Loss: 0.9158435463905334, Accuracy: 1.0, Computation time: 0.9186623096466064\n",
      "Step: 6023, Loss: 0.9158403277397156, Accuracy: 1.0, Computation time: 0.8410427570343018\n",
      "Step: 6024, Loss: 0.9375525712966919, Accuracy: 0.9583333730697632, Computation time: 0.8863222599029541\n",
      "Step: 6025, Loss: 0.9158355593681335, Accuracy: 1.0, Computation time: 0.8234696388244629\n",
      "Step: 6026, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 1.1532855033874512\n",
      "Step: 6027, Loss: 0.915840744972229, Accuracy: 1.0, Computation time: 0.7858448028564453\n",
      "Step: 6028, Loss: 0.9375662803649902, Accuracy: 0.9791666865348816, Computation time: 0.9125545024871826\n",
      "Step: 6029, Loss: 0.9380979537963867, Accuracy: 0.9791666865348816, Computation time: 1.5646493434906006\n",
      "Step: 6030, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 0.9724013805389404\n",
      "Step: 6031, Loss: 0.9158440828323364, Accuracy: 1.0, Computation time: 0.9052059650421143\n",
      "Step: 6032, Loss: 0.9158536195755005, Accuracy: 1.0, Computation time: 1.1342213153839111\n",
      "Step: 6033, Loss: 0.9158459901809692, Accuracy: 1.0, Computation time: 1.098630666732788\n",
      "Step: 6034, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 0.8070297241210938\n",
      "Step: 6035, Loss: 0.9159083962440491, Accuracy: 1.0, Computation time: 0.9950993061065674\n",
      "Step: 6036, Loss: 0.9158397316932678, Accuracy: 1.0, Computation time: 0.8728656768798828\n",
      "Step: 6037, Loss: 0.9158350825309753, Accuracy: 1.0, Computation time: 0.7612349987030029\n",
      "Step: 6038, Loss: 0.9374635219573975, Accuracy: 0.9791666865348816, Computation time: 0.8396754264831543\n",
      "Step: 6039, Loss: 0.9158413410186768, Accuracy: 1.0, Computation time: 0.7040870189666748\n",
      "Step: 6040, Loss: 0.9375758767127991, Accuracy: 0.9722222089767456, Computation time: 1.0019595623016357\n",
      "Step: 6041, Loss: 0.9158517718315125, Accuracy: 1.0, Computation time: 0.7483727931976318\n",
      "Step: 6042, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 0.7846548557281494\n",
      "Step: 6043, Loss: 0.9158620238304138, Accuracy: 1.0, Computation time: 0.7944216728210449\n",
      "Step: 6044, Loss: 0.9375372529029846, Accuracy: 0.96875, Computation time: 0.9720449447631836\n",
      "Step: 6045, Loss: 0.9158444404602051, Accuracy: 1.0, Computation time: 0.9426720142364502\n",
      "Step: 6046, Loss: 0.9336977005004883, Accuracy: 0.9807692766189575, Computation time: 1.7885408401489258\n",
      "Step: 6047, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 0.9647223949432373\n",
      "Step: 6048, Loss: 0.9159363508224487, Accuracy: 1.0, Computation time: 0.9632275104522705\n",
      "Step: 6049, Loss: 0.9160147905349731, Accuracy: 1.0, Computation time: 1.0830485820770264\n",
      "Step: 6050, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 0.8840034008026123\n",
      "Step: 6051, Loss: 0.9159595966339111, Accuracy: 1.0, Computation time: 0.7473282814025879\n",
      "Step: 6052, Loss: 0.9159526824951172, Accuracy: 1.0, Computation time: 1.011103630065918\n",
      "Step: 6053, Loss: 0.9158511757850647, Accuracy: 1.0, Computation time: 0.8510265350341797\n",
      "Step: 6054, Loss: 0.9158792495727539, Accuracy: 1.0, Computation time: 0.8130967617034912\n",
      "Step: 6055, Loss: 0.9158505201339722, Accuracy: 1.0, Computation time: 0.8945009708404541\n",
      "Step: 6056, Loss: 0.9158374071121216, Accuracy: 1.0, Computation time: 0.8344719409942627\n",
      "Step: 6057, Loss: 0.9367546439170837, Accuracy: 0.9722222089767456, Computation time: 0.8917944431304932\n",
      "Step: 6058, Loss: 0.9160891771316528, Accuracy: 1.0, Computation time: 0.807462215423584\n",
      "Step: 6059, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 0.8952510356903076\n",
      "Step: 6060, Loss: 0.9328346848487854, Accuracy: 0.9833333492279053, Computation time: 1.0347545146942139\n",
      "Step: 6061, Loss: 0.9312431216239929, Accuracy: 0.9722222089767456, Computation time: 1.3108711242675781\n",
      "Step: 6062, Loss: 0.9159088134765625, Accuracy: 1.0, Computation time: 0.7198338508605957\n",
      "Step: 6063, Loss: 0.9160190224647522, Accuracy: 1.0, Computation time: 0.9684808254241943\n",
      "Step: 6064, Loss: 0.9159315228462219, Accuracy: 1.0, Computation time: 0.9415555000305176\n",
      "Step: 6065, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 0.7312400341033936\n",
      "Step: 6066, Loss: 0.9160687923431396, Accuracy: 1.0, Computation time: 0.9144508838653564\n",
      "Step: 6067, Loss: 0.916000247001648, Accuracy: 1.0, Computation time: 0.8168501853942871\n",
      "Step: 6068, Loss: 0.9164963960647583, Accuracy: 1.0, Computation time: 0.8243508338928223\n",
      "Step: 6069, Loss: 0.9736171960830688, Accuracy: 0.9198718070983887, Computation time: 1.2970142364501953\n",
      "Step: 6070, Loss: 0.9159762859344482, Accuracy: 1.0, Computation time: 0.8337085247039795\n",
      "Step: 6071, Loss: 0.9377370476722717, Accuracy: 0.9750000238418579, Computation time: 0.8982234001159668\n",
      "Step: 6072, Loss: 0.9160590767860413, Accuracy: 1.0, Computation time: 0.7052245140075684\n",
      "Step: 6073, Loss: 0.9160963296890259, Accuracy: 1.0, Computation time: 1.0327856540679932\n",
      "Step: 6074, Loss: 0.9377967715263367, Accuracy: 0.96875, Computation time: 0.7645120620727539\n",
      "Step: 6075, Loss: 0.915941059589386, Accuracy: 1.0, Computation time: 0.7538087368011475\n",
      "Step: 6076, Loss: 0.9158908128738403, Accuracy: 1.0, Computation time: 0.8393571376800537\n",
      "Step: 6077, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 0.919989824295044\n",
      "Step: 6078, Loss: 0.9159390330314636, Accuracy: 1.0, Computation time: 0.8769383430480957\n",
      "Step: 6079, Loss: 0.9159524440765381, Accuracy: 1.0, Computation time: 0.8499221801757812\n",
      "Step: 6080, Loss: 0.934990406036377, Accuracy: 0.9821428656578064, Computation time: 0.9870283603668213\n",
      "Step: 6081, Loss: 0.9187282919883728, Accuracy: 1.0, Computation time: 0.9222927093505859\n",
      "Step: 6082, Loss: 0.9159913659095764, Accuracy: 1.0, Computation time: 0.7864296436309814\n",
      "Step: 6083, Loss: 0.9161704778671265, Accuracy: 1.0, Computation time: 0.8608920574188232\n",
      "Step: 6084, Loss: 0.9160201549530029, Accuracy: 1.0, Computation time: 0.8338165283203125\n",
      "Step: 6085, Loss: 0.9375404119491577, Accuracy: 0.9750000238418579, Computation time: 0.9326672554016113\n",
      "Step: 6086, Loss: 0.9163694381713867, Accuracy: 1.0, Computation time: 0.8824317455291748\n",
      "Step: 6087, Loss: 0.9159814715385437, Accuracy: 1.0, Computation time: 0.7922873497009277\n",
      "Step: 6088, Loss: 0.9373010993003845, Accuracy: 0.96875, Computation time: 0.9103739261627197\n",
      "Step: 6089, Loss: 0.9159694314002991, Accuracy: 1.0, Computation time: 1.0708096027374268\n",
      "Step: 6090, Loss: 0.9158823490142822, Accuracy: 1.0, Computation time: 0.7435998916625977\n",
      "Step: 6091, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 0.9887475967407227\n",
      "Step: 6092, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 0.7366445064544678\n",
      "Step: 6093, Loss: 0.9160730242729187, Accuracy: 1.0, Computation time: 0.9943053722381592\n",
      "Step: 6094, Loss: 0.9366262555122375, Accuracy: 0.9772727489471436, Computation time: 1.7380735874176025\n",
      "Step: 6095, Loss: 0.9159823656082153, Accuracy: 1.0, Computation time: 0.8763349056243896\n",
      "Step: 6096, Loss: 0.9160096645355225, Accuracy: 1.0, Computation time: 0.8620061874389648\n",
      "Step: 6097, Loss: 0.9159024357795715, Accuracy: 1.0, Computation time: 0.9221286773681641\n",
      "Step: 6098, Loss: 0.9160085320472717, Accuracy: 1.0, Computation time: 0.8446934223175049\n",
      "Step: 6099, Loss: 0.9165915250778198, Accuracy: 1.0, Computation time: 1.9064366817474365\n",
      "Step: 6100, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 0.8082244396209717\n",
      "Step: 6101, Loss: 0.9161725044250488, Accuracy: 1.0, Computation time: 0.8287491798400879\n",
      "Step: 6102, Loss: 0.9375627636909485, Accuracy: 0.9722222089767456, Computation time: 0.7496688365936279\n",
      "Step: 6103, Loss: 0.9159406423568726, Accuracy: 1.0, Computation time: 0.9003851413726807\n",
      "Step: 6104, Loss: 0.9159521460533142, Accuracy: 1.0, Computation time: 0.7416303157806396\n",
      "Step: 6105, Loss: 0.915936291217804, Accuracy: 1.0, Computation time: 0.8119738101959229\n",
      "Step: 6106, Loss: 0.9158874750137329, Accuracy: 1.0, Computation time: 0.7539341449737549\n",
      "Step: 6107, Loss: 0.9158812165260315, Accuracy: 1.0, Computation time: 0.8718295097351074\n",
      "Step: 6108, Loss: 0.9197600483894348, Accuracy: 1.0, Computation time: 0.8495821952819824\n",
      "Step: 6109, Loss: 0.9158843159675598, Accuracy: 1.0, Computation time: 0.8044946193695068\n",
      "Step: 6110, Loss: 0.9159146547317505, Accuracy: 1.0, Computation time: 0.8615813255310059\n",
      "Step: 6111, Loss: 0.9158971309661865, Accuracy: 1.0, Computation time: 0.9080812931060791\n",
      "Step: 6112, Loss: 0.9159884452819824, Accuracy: 1.0, Computation time: 0.8537569046020508\n",
      "Step: 6113, Loss: 0.9199226498603821, Accuracy: 1.0, Computation time: 1.0857810974121094\n",
      "Step: 6114, Loss: 0.937496542930603, Accuracy: 0.96875, Computation time: 0.8895180225372314\n",
      "########################\n",
      "Test loss: 1.1218613386154175, Test Accuracy_epoch44: 0.6956354379653931\n",
      "########################\n",
      "Step: 6115, Loss: 0.9158596396446228, Accuracy: 1.0, Computation time: 0.7584576606750488\n",
      "Step: 6116, Loss: 0.9379322528839111, Accuracy: 0.9807692766189575, Computation time: 0.8459024429321289\n",
      "Step: 6117, Loss: 0.915972888469696, Accuracy: 1.0, Computation time: 0.7405173778533936\n",
      "Step: 6118, Loss: 0.9295627474784851, Accuracy: 0.9722222089767456, Computation time: 1.4286634922027588\n",
      "Step: 6119, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 0.8203127384185791\n",
      "Step: 6120, Loss: 0.9159054160118103, Accuracy: 1.0, Computation time: 0.8144626617431641\n",
      "Step: 6121, Loss: 0.9163589477539062, Accuracy: 1.0, Computation time: 1.1018381118774414\n",
      "Step: 6122, Loss: 0.9159458875656128, Accuracy: 1.0, Computation time: 1.1925532817840576\n",
      "Step: 6123, Loss: 0.9162811636924744, Accuracy: 1.0, Computation time: 0.943871021270752\n",
      "Step: 6124, Loss: 0.9162304401397705, Accuracy: 1.0, Computation time: 0.9377932548522949\n",
      "Step: 6125, Loss: 0.937642514705658, Accuracy: 0.9642857313156128, Computation time: 1.1192395687103271\n",
      "Step: 6126, Loss: 0.915943443775177, Accuracy: 1.0, Computation time: 0.8476302623748779\n",
      "Step: 6127, Loss: 0.9159885048866272, Accuracy: 1.0, Computation time: 0.7869710922241211\n",
      "Step: 6128, Loss: 0.9159719347953796, Accuracy: 1.0, Computation time: 0.8239450454711914\n",
      "Step: 6129, Loss: 0.9162444472312927, Accuracy: 1.0, Computation time: 0.9957468509674072\n",
      "Step: 6130, Loss: 0.9327155947685242, Accuracy: 0.9583333730697632, Computation time: 1.1176564693450928\n",
      "Step: 6131, Loss: 0.9159649014472961, Accuracy: 1.0, Computation time: 0.7765181064605713\n",
      "Step: 6132, Loss: 0.9159559607505798, Accuracy: 1.0, Computation time: 1.503570556640625\n",
      "Step: 6133, Loss: 0.9160470962524414, Accuracy: 1.0, Computation time: 1.0411620140075684\n",
      "Step: 6134, Loss: 0.9160525798797607, Accuracy: 1.0, Computation time: 0.7600226402282715\n",
      "Step: 6135, Loss: 0.9159961938858032, Accuracy: 1.0, Computation time: 0.9657130241394043\n",
      "Step: 6136, Loss: 0.9192668199539185, Accuracy: 1.0, Computation time: 0.909881591796875\n",
      "Step: 6137, Loss: 0.9207218885421753, Accuracy: 1.0, Computation time: 1.7823810577392578\n",
      "Step: 6138, Loss: 0.9160030484199524, Accuracy: 1.0, Computation time: 0.9461338520050049\n",
      "Step: 6139, Loss: 0.9377930760383606, Accuracy: 0.9791666865348816, Computation time: 1.2181100845336914\n",
      "Step: 6140, Loss: 0.9536125659942627, Accuracy: 0.9291666746139526, Computation time: 1.0395002365112305\n",
      "Step: 6141, Loss: 0.9159638285636902, Accuracy: 1.0, Computation time: 0.958582878112793\n",
      "Step: 6142, Loss: 0.9159656763076782, Accuracy: 1.0, Computation time: 0.7922377586364746\n",
      "Step: 6143, Loss: 0.9159775376319885, Accuracy: 1.0, Computation time: 0.9003164768218994\n",
      "Step: 6144, Loss: 0.9159443378448486, Accuracy: 1.0, Computation time: 0.9377901554107666\n",
      "Step: 6145, Loss: 0.9159626960754395, Accuracy: 1.0, Computation time: 0.9321582317352295\n",
      "Step: 6146, Loss: 0.9159899353981018, Accuracy: 1.0, Computation time: 1.0342848300933838\n",
      "Step: 6147, Loss: 0.9343211650848389, Accuracy: 0.9833333492279053, Computation time: 1.0806641578674316\n",
      "Step: 6148, Loss: 0.9159552454948425, Accuracy: 1.0, Computation time: 1.0145184993743896\n",
      "Step: 6149, Loss: 0.9159717559814453, Accuracy: 1.0, Computation time: 1.04805326461792\n",
      "Step: 6150, Loss: 0.9160599708557129, Accuracy: 1.0, Computation time: 1.0243747234344482\n",
      "Step: 6151, Loss: 0.9324703216552734, Accuracy: 0.9807692766189575, Computation time: 1.1465229988098145\n",
      "Step: 6152, Loss: 0.9159326553344727, Accuracy: 1.0, Computation time: 1.2155683040618896\n",
      "Step: 6153, Loss: 0.916866660118103, Accuracy: 1.0, Computation time: 1.1864547729492188\n",
      "Step: 6154, Loss: 0.9180700778961182, Accuracy: 1.0, Computation time: 1.0240283012390137\n",
      "Step: 6155, Loss: 0.9375048279762268, Accuracy: 0.9791666865348816, Computation time: 1.4011969566345215\n",
      "Step: 6156, Loss: 0.9159150123596191, Accuracy: 1.0, Computation time: 0.8576364517211914\n",
      "Step: 6157, Loss: 0.9159928560256958, Accuracy: 1.0, Computation time: 0.9807209968566895\n",
      "Step: 6158, Loss: 0.9167836904525757, Accuracy: 1.0, Computation time: 0.8021280765533447\n",
      "Step: 6159, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 0.8426966667175293\n",
      "Step: 6160, Loss: 0.9158547520637512, Accuracy: 1.0, Computation time: 0.837139368057251\n",
      "Step: 6161, Loss: 0.9375377893447876, Accuracy: 0.9642857313156128, Computation time: 1.02854323387146\n",
      "Step: 6162, Loss: 0.915860116481781, Accuracy: 1.0, Computation time: 0.9624283313751221\n",
      "Step: 6163, Loss: 0.9226111173629761, Accuracy: 1.0, Computation time: 0.857896089553833\n",
      "Step: 6164, Loss: 0.915859043598175, Accuracy: 1.0, Computation time: 1.1381566524505615\n",
      "Step: 6165, Loss: 0.9160243272781372, Accuracy: 1.0, Computation time: 0.9134604930877686\n",
      "Step: 6166, Loss: 0.9158731698989868, Accuracy: 1.0, Computation time: 0.8667144775390625\n",
      "Step: 6167, Loss: 0.9159080982208252, Accuracy: 1.0, Computation time: 0.8884611129760742\n",
      "Step: 6168, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 0.7923338413238525\n",
      "Step: 6169, Loss: 0.9158894419670105, Accuracy: 1.0, Computation time: 0.8314156532287598\n",
      "Step: 6170, Loss: 0.9179649353027344, Accuracy: 1.0, Computation time: 1.017127275466919\n",
      "Step: 6171, Loss: 0.915998637676239, Accuracy: 1.0, Computation time: 1.1419129371643066\n",
      "Step: 6172, Loss: 0.9192787408828735, Accuracy: 1.0, Computation time: 0.8918685913085938\n",
      "Step: 6173, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 0.7931041717529297\n",
      "Step: 6174, Loss: 0.9354144930839539, Accuracy: 0.9772727489471436, Computation time: 0.8723950386047363\n",
      "Step: 6175, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 0.8401675224304199\n",
      "Step: 6176, Loss: 0.9160106778144836, Accuracy: 1.0, Computation time: 0.773594856262207\n",
      "Step: 6177, Loss: 0.9159339666366577, Accuracy: 1.0, Computation time: 0.8529579639434814\n",
      "Step: 6178, Loss: 0.9356463551521301, Accuracy: 0.9583333730697632, Computation time: 0.8877513408660889\n",
      "Step: 6179, Loss: 0.9158838391304016, Accuracy: 1.0, Computation time: 0.8839948177337646\n",
      "Step: 6180, Loss: 0.9375675916671753, Accuracy: 0.9772727489471436, Computation time: 0.8389315605163574\n",
      "Step: 6181, Loss: 0.9158678650856018, Accuracy: 1.0, Computation time: 0.8045380115509033\n",
      "Step: 6182, Loss: 0.9159449338912964, Accuracy: 1.0, Computation time: 0.8059828281402588\n",
      "Step: 6183, Loss: 0.9375190138816833, Accuracy: 0.96875, Computation time: 0.8963732719421387\n",
      "Step: 6184, Loss: 0.9158932566642761, Accuracy: 1.0, Computation time: 0.7908923625946045\n",
      "Step: 6185, Loss: 0.9158877730369568, Accuracy: 1.0, Computation time: 0.7650816440582275\n",
      "Step: 6186, Loss: 0.9584928154945374, Accuracy: 0.9583333730697632, Computation time: 0.8087892532348633\n",
      "Step: 6187, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 0.9314515590667725\n",
      "Step: 6188, Loss: 0.9159399271011353, Accuracy: 1.0, Computation time: 0.7939107418060303\n",
      "Step: 6189, Loss: 0.9167449474334717, Accuracy: 1.0, Computation time: 0.7860445976257324\n",
      "Step: 6190, Loss: 0.9159712195396423, Accuracy: 1.0, Computation time: 0.7386956214904785\n",
      "Step: 6191, Loss: 0.9158591628074646, Accuracy: 1.0, Computation time: 0.7375831604003906\n",
      "Step: 6192, Loss: 0.9158473610877991, Accuracy: 1.0, Computation time: 0.7863068580627441\n",
      "Step: 6193, Loss: 0.9158404469490051, Accuracy: 1.0, Computation time: 0.7310082912445068\n",
      "Step: 6194, Loss: 0.9160023331642151, Accuracy: 1.0, Computation time: 0.7676401138305664\n",
      "Step: 6195, Loss: 0.9158445000648499, Accuracy: 1.0, Computation time: 0.7821211814880371\n",
      "Step: 6196, Loss: 0.9158469438552856, Accuracy: 1.0, Computation time: 0.7411389350891113\n",
      "Step: 6197, Loss: 0.915851354598999, Accuracy: 1.0, Computation time: 0.8252148628234863\n",
      "Step: 6198, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 0.8099279403686523\n",
      "Step: 6199, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.9645669460296631\n",
      "Step: 6200, Loss: 0.9363194704055786, Accuracy: 0.9722222089767456, Computation time: 0.9055566787719727\n",
      "Step: 6201, Loss: 0.9175201654434204, Accuracy: 1.0, Computation time: 0.9345505237579346\n",
      "Step: 6202, Loss: 0.915867805480957, Accuracy: 1.0, Computation time: 0.8577675819396973\n",
      "Step: 6203, Loss: 0.9158772230148315, Accuracy: 1.0, Computation time: 0.8934059143066406\n",
      "Step: 6204, Loss: 0.9158748388290405, Accuracy: 1.0, Computation time: 0.7697136402130127\n",
      "Step: 6205, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 0.7260482311248779\n",
      "Step: 6206, Loss: 0.9376183152198792, Accuracy: 0.9807692766189575, Computation time: 0.9678707122802734\n",
      "Step: 6207, Loss: 0.916970431804657, Accuracy: 1.0, Computation time: 1.1093106269836426\n",
      "Step: 6208, Loss: 0.9160801768302917, Accuracy: 1.0, Computation time: 0.8395133018493652\n",
      "Step: 6209, Loss: 0.9161693453788757, Accuracy: 1.0, Computation time: 0.890038013458252\n",
      "Step: 6210, Loss: 0.9159781336784363, Accuracy: 1.0, Computation time: 0.8577613830566406\n",
      "Step: 6211, Loss: 0.915880560874939, Accuracy: 1.0, Computation time: 1.1221890449523926\n",
      "Step: 6212, Loss: 0.920043408870697, Accuracy: 1.0, Computation time: 0.9223358631134033\n",
      "Step: 6213, Loss: 0.9158830642700195, Accuracy: 1.0, Computation time: 0.7903058528900146\n",
      "Step: 6214, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 0.8776676654815674\n",
      "Step: 6215, Loss: 0.917948305606842, Accuracy: 1.0, Computation time: 1.0512275695800781\n",
      "Step: 6216, Loss: 0.9160017967224121, Accuracy: 1.0, Computation time: 1.076188564300537\n",
      "Step: 6217, Loss: 0.9159952402114868, Accuracy: 1.0, Computation time: 0.73818039894104\n",
      "Step: 6218, Loss: 0.9159979820251465, Accuracy: 1.0, Computation time: 0.9845309257507324\n",
      "Step: 6219, Loss: 0.9159285426139832, Accuracy: 1.0, Computation time: 0.7484588623046875\n",
      "Step: 6220, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 0.8958268165588379\n",
      "Step: 6221, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 0.832146167755127\n",
      "Step: 6222, Loss: 0.9158571362495422, Accuracy: 1.0, Computation time: 0.8241868019104004\n",
      "Step: 6223, Loss: 0.9159795045852661, Accuracy: 1.0, Computation time: 0.8282310962677002\n",
      "Step: 6224, Loss: 0.9375274181365967, Accuracy: 0.96875, Computation time: 1.1487383842468262\n",
      "Step: 6225, Loss: 0.9159031510353088, Accuracy: 1.0, Computation time: 0.7214639186859131\n",
      "Step: 6226, Loss: 0.9159085750579834, Accuracy: 1.0, Computation time: 0.8687193393707275\n",
      "Step: 6227, Loss: 0.9375439286231995, Accuracy: 0.9807692766189575, Computation time: 0.7630929946899414\n",
      "Step: 6228, Loss: 0.9159213304519653, Accuracy: 1.0, Computation time: 0.9690096378326416\n",
      "Step: 6229, Loss: 0.9375690221786499, Accuracy: 0.9772727489471436, Computation time: 0.7476663589477539\n",
      "Step: 6230, Loss: 0.9158989787101746, Accuracy: 1.0, Computation time: 0.9069869518280029\n",
      "Step: 6231, Loss: 0.9158744215965271, Accuracy: 1.0, Computation time: 0.7889022827148438\n",
      "Step: 6232, Loss: 0.9158546924591064, Accuracy: 1.0, Computation time: 0.9380009174346924\n",
      "Step: 6233, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 1.0754234790802002\n",
      "Step: 6234, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 0.9034037590026855\n",
      "Step: 6235, Loss: 0.9158968925476074, Accuracy: 1.0, Computation time: 0.8477480411529541\n",
      "Step: 6236, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.0011582374572754\n",
      "Step: 6237, Loss: 0.9161018133163452, Accuracy: 1.0, Computation time: 0.8809506893157959\n",
      "Step: 6238, Loss: 0.9380852580070496, Accuracy: 0.9750000238418579, Computation time: 1.0076560974121094\n",
      "Step: 6239, Loss: 0.9158492088317871, Accuracy: 1.0, Computation time: 0.9162743091583252\n",
      "Step: 6240, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 1.1778810024261475\n",
      "Step: 6241, Loss: 0.9159464240074158, Accuracy: 1.0, Computation time: 1.0440404415130615\n",
      "Step: 6242, Loss: 0.9158730506896973, Accuracy: 1.0, Computation time: 0.8040683269500732\n",
      "Step: 6243, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 1.0318937301635742\n",
      "Step: 6244, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 0.9590206146240234\n",
      "Step: 6245, Loss: 0.915909469127655, Accuracy: 1.0, Computation time: 0.8319926261901855\n",
      "Step: 6246, Loss: 0.916947603225708, Accuracy: 1.0, Computation time: 0.9792826175689697\n",
      "Step: 6247, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 0.9912512302398682\n",
      "Step: 6248, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.9483280181884766\n",
      "Step: 6249, Loss: 0.9375636577606201, Accuracy: 0.9772727489471436, Computation time: 0.9031257629394531\n",
      "Step: 6250, Loss: 0.9158666729927063, Accuracy: 1.0, Computation time: 0.9376347064971924\n",
      "Step: 6251, Loss: 0.9158560633659363, Accuracy: 1.0, Computation time: 0.9822156429290771\n",
      "Step: 6252, Loss: 0.9375848174095154, Accuracy: 0.9791666865348816, Computation time: 0.9412229061126709\n",
      "Step: 6253, Loss: 0.9158456921577454, Accuracy: 1.0, Computation time: 0.830660343170166\n",
      "########################\n",
      "Test loss: 1.1261284351348877, Test Accuracy_epoch45: 0.6881301403045654\n",
      "########################\n",
      "Step: 6254, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 0.904543399810791\n",
      "Step: 6255, Loss: 0.9158429503440857, Accuracy: 1.0, Computation time: 1.082824468612671\n",
      "Step: 6256, Loss: 0.9158445596694946, Accuracy: 1.0, Computation time: 0.9508686065673828\n",
      "Step: 6257, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 1.2078511714935303\n",
      "Step: 6258, Loss: 0.9263820648193359, Accuracy: 0.949999988079071, Computation time: 1.062309741973877\n",
      "Step: 6259, Loss: 0.9159235954284668, Accuracy: 1.0, Computation time: 1.0007762908935547\n",
      "Step: 6260, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.1845738887786865\n",
      "Step: 6261, Loss: 0.9159513115882874, Accuracy: 1.0, Computation time: 1.1763384342193604\n",
      "Step: 6262, Loss: 0.9159398078918457, Accuracy: 1.0, Computation time: 0.869575023651123\n",
      "Step: 6263, Loss: 0.915948748588562, Accuracy: 1.0, Computation time: 1.0256869792938232\n",
      "Step: 6264, Loss: 0.9375786781311035, Accuracy: 0.96875, Computation time: 0.9316208362579346\n",
      "Step: 6265, Loss: 0.9375263452529907, Accuracy: 0.9750000238418579, Computation time: 0.9242522716522217\n",
      "Step: 6266, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.8559727668762207\n",
      "Step: 6267, Loss: 0.9158529043197632, Accuracy: 1.0, Computation time: 0.7250387668609619\n",
      "Step: 6268, Loss: 0.9158815741539001, Accuracy: 1.0, Computation time: 0.9085433483123779\n",
      "Step: 6269, Loss: 0.9266839027404785, Accuracy: 0.9722222089767456, Computation time: 1.0078356266021729\n",
      "Step: 6270, Loss: 0.9338503479957581, Accuracy: 0.9642857313156128, Computation time: 0.9299306869506836\n",
      "Step: 6271, Loss: 0.9158890247344971, Accuracy: 1.0, Computation time: 1.1751234531402588\n",
      "Step: 6272, Loss: 0.9159231781959534, Accuracy: 1.0, Computation time: 1.0482680797576904\n",
      "Step: 6273, Loss: 0.9158927798271179, Accuracy: 1.0, Computation time: 0.8646664619445801\n",
      "Step: 6274, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.063941478729248\n",
      "Step: 6275, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 1.1611566543579102\n",
      "Step: 6276, Loss: 0.915848433971405, Accuracy: 1.0, Computation time: 0.9490773677825928\n",
      "Step: 6277, Loss: 0.9375701546669006, Accuracy: 0.96875, Computation time: 0.8478679656982422\n",
      "Step: 6278, Loss: 0.9375518560409546, Accuracy: 0.96875, Computation time: 1.0338637828826904\n",
      "Step: 6279, Loss: 0.9165331125259399, Accuracy: 1.0, Computation time: 0.9859879016876221\n",
      "Step: 6280, Loss: 0.9158681631088257, Accuracy: 1.0, Computation time: 0.789844274520874\n",
      "Step: 6281, Loss: 0.9159682393074036, Accuracy: 1.0, Computation time: 1.0005970001220703\n",
      "Step: 6282, Loss: 0.9162732362747192, Accuracy: 1.0, Computation time: 0.8214428424835205\n",
      "Step: 6283, Loss: 0.9212523698806763, Accuracy: 1.0, Computation time: 1.6100444793701172\n",
      "Step: 6284, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.1496620178222656\n",
      "Step: 6285, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 0.882605791091919\n",
      "Step: 6286, Loss: 0.9158836603164673, Accuracy: 1.0, Computation time: 0.7963380813598633\n",
      "Step: 6287, Loss: 0.915894627571106, Accuracy: 1.0, Computation time: 0.8645038604736328\n",
      "Step: 6288, Loss: 0.9159035086631775, Accuracy: 1.0, Computation time: 0.8391969203948975\n",
      "Step: 6289, Loss: 0.9164505004882812, Accuracy: 1.0, Computation time: 0.9416885375976562\n",
      "Step: 6290, Loss: 0.9158883094787598, Accuracy: 1.0, Computation time: 1.340320348739624\n",
      "Step: 6291, Loss: 0.9164217114448547, Accuracy: 1.0, Computation time: 0.9106309413909912\n",
      "Step: 6292, Loss: 0.915946900844574, Accuracy: 1.0, Computation time: 0.9705078601837158\n",
      "Step: 6293, Loss: 0.9161240458488464, Accuracy: 1.0, Computation time: 1.107872724533081\n",
      "Step: 6294, Loss: 0.9375746250152588, Accuracy: 0.9772727489471436, Computation time: 0.8573169708251953\n",
      "Step: 6295, Loss: 0.9376702308654785, Accuracy: 0.96875, Computation time: 1.0912539958953857\n",
      "Step: 6296, Loss: 0.9375557899475098, Accuracy: 0.9722222089767456, Computation time: 0.9464693069458008\n",
      "Step: 6297, Loss: 0.9158489108085632, Accuracy: 1.0, Computation time: 0.924267053604126\n",
      "Step: 6298, Loss: 0.9159026145935059, Accuracy: 1.0, Computation time: 1.1859557628631592\n",
      "Step: 6299, Loss: 0.9160416722297668, Accuracy: 1.0, Computation time: 1.1330163478851318\n",
      "Step: 6300, Loss: 0.9158585071563721, Accuracy: 1.0, Computation time: 1.0396034717559814\n",
      "Step: 6301, Loss: 0.9159324765205383, Accuracy: 1.0, Computation time: 1.0510485172271729\n",
      "Step: 6302, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 0.871558427810669\n",
      "Step: 6303, Loss: 0.9304706454277039, Accuracy: 0.9821428656578064, Computation time: 1.0162632465362549\n",
      "Step: 6304, Loss: 0.915886402130127, Accuracy: 1.0, Computation time: 1.0702829360961914\n",
      "Step: 6305, Loss: 0.9158663749694824, Accuracy: 1.0, Computation time: 0.836700439453125\n",
      "Step: 6306, Loss: 0.9593101739883423, Accuracy: 0.9583333730697632, Computation time: 1.0382375717163086\n",
      "Step: 6307, Loss: 0.9158896207809448, Accuracy: 1.0, Computation time: 1.0637931823730469\n",
      "Step: 6308, Loss: 0.9337245225906372, Accuracy: 0.9642857313156128, Computation time: 1.178884744644165\n",
      "Step: 6309, Loss: 0.9376541972160339, Accuracy: 0.96875, Computation time: 1.3103601932525635\n",
      "Step: 6310, Loss: 0.9158889651298523, Accuracy: 1.0, Computation time: 1.250791072845459\n",
      "Step: 6311, Loss: 0.9158949851989746, Accuracy: 1.0, Computation time: 0.9911563396453857\n",
      "Step: 6312, Loss: 0.915940523147583, Accuracy: 1.0, Computation time: 0.8788025379180908\n",
      "Step: 6313, Loss: 0.915919303894043, Accuracy: 1.0, Computation time: 0.9700026512145996\n",
      "Step: 6314, Loss: 0.9160986542701721, Accuracy: 1.0, Computation time: 1.1565840244293213\n",
      "Step: 6315, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 0.9885232448577881\n",
      "Step: 6316, Loss: 0.9159082174301147, Accuracy: 1.0, Computation time: 1.3156967163085938\n",
      "Step: 6317, Loss: 0.9158997535705566, Accuracy: 1.0, Computation time: 1.00148344039917\n",
      "Step: 6318, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 0.9699723720550537\n",
      "Step: 6319, Loss: 0.9158948659896851, Accuracy: 1.0, Computation time: 0.986539363861084\n",
      "Step: 6320, Loss: 0.9159049391746521, Accuracy: 1.0, Computation time: 0.962050199508667\n",
      "Step: 6321, Loss: 0.9158682227134705, Accuracy: 1.0, Computation time: 0.9870972633361816\n",
      "Step: 6322, Loss: 0.9164366126060486, Accuracy: 1.0, Computation time: 1.2813591957092285\n",
      "Step: 6323, Loss: 0.9158673286437988, Accuracy: 1.0, Computation time: 0.9637224674224854\n",
      "Step: 6324, Loss: 0.9158490896224976, Accuracy: 1.0, Computation time: 0.9435434341430664\n",
      "Step: 6325, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 0.8651237487792969\n",
      "Step: 6326, Loss: 0.9158481955528259, Accuracy: 1.0, Computation time: 0.9566752910614014\n",
      "Step: 6327, Loss: 0.9378547668457031, Accuracy: 0.9722222089767456, Computation time: 0.9012970924377441\n",
      "Step: 6328, Loss: 0.9158558249473572, Accuracy: 1.0, Computation time: 1.41916823387146\n",
      "Step: 6329, Loss: 0.9158465266227722, Accuracy: 1.0, Computation time: 0.8194184303283691\n",
      "Step: 6330, Loss: 0.9160297513008118, Accuracy: 1.0, Computation time: 1.064323902130127\n",
      "Step: 6331, Loss: 0.9159313440322876, Accuracy: 1.0, Computation time: 1.571718454360962\n",
      "Step: 6332, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 0.8428680896759033\n",
      "Step: 6333, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 0.9450984001159668\n",
      "Step: 6334, Loss: 0.9158542156219482, Accuracy: 1.0, Computation time: 0.8415210247039795\n",
      "Step: 6335, Loss: 0.9348621964454651, Accuracy: 0.9642857313156128, Computation time: 1.1083106994628906\n",
      "Step: 6336, Loss: 0.9158598184585571, Accuracy: 1.0, Computation time: 0.8534941673278809\n",
      "Step: 6337, Loss: 0.9375532269477844, Accuracy: 0.9807692766189575, Computation time: 0.7686493396759033\n",
      "Step: 6338, Loss: 0.9158532023429871, Accuracy: 1.0, Computation time: 0.8512811660766602\n",
      "Step: 6339, Loss: 0.9171755313873291, Accuracy: 1.0, Computation time: 0.9607691764831543\n",
      "Step: 6340, Loss: 0.9158867597579956, Accuracy: 1.0, Computation time: 0.767136812210083\n",
      "Step: 6341, Loss: 0.9374818205833435, Accuracy: 0.9791666865348816, Computation time: 0.9783883094787598\n",
      "Step: 6342, Loss: 0.9158694744110107, Accuracy: 1.0, Computation time: 0.8467745780944824\n",
      "Step: 6343, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.07297682762146\n",
      "Step: 6344, Loss: 0.9158359169960022, Accuracy: 1.0, Computation time: 1.1939985752105713\n",
      "Step: 6345, Loss: 0.9158846735954285, Accuracy: 1.0, Computation time: 0.9001321792602539\n",
      "Step: 6346, Loss: 0.9375437498092651, Accuracy: 0.9750000238418579, Computation time: 1.166222095489502\n",
      "Step: 6347, Loss: 0.9158450961112976, Accuracy: 1.0, Computation time: 0.9214863777160645\n",
      "Step: 6348, Loss: 0.9158381223678589, Accuracy: 1.0, Computation time: 0.9887478351593018\n",
      "Step: 6349, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.8891239166259766\n",
      "Step: 6350, Loss: 0.9158740043640137, Accuracy: 1.0, Computation time: 1.0978271961212158\n",
      "Step: 6351, Loss: 0.9323500394821167, Accuracy: 0.9722222089767456, Computation time: 2.5989317893981934\n",
      "Step: 6352, Loss: 0.959435224533081, Accuracy: 0.9392857551574707, Computation time: 1.012984275817871\n",
      "Step: 6353, Loss: 0.9158769249916077, Accuracy: 1.0, Computation time: 0.889148473739624\n",
      "Step: 6354, Loss: 0.9158989191055298, Accuracy: 1.0, Computation time: 0.9886293411254883\n",
      "Step: 6355, Loss: 0.9158878326416016, Accuracy: 1.0, Computation time: 0.772322416305542\n",
      "Step: 6356, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 0.9940052032470703\n",
      "Step: 6357, Loss: 0.9160358309745789, Accuracy: 1.0, Computation time: 0.836864709854126\n",
      "Step: 6358, Loss: 0.9158477187156677, Accuracy: 1.0, Computation time: 0.9555647373199463\n",
      "Step: 6359, Loss: 0.9372895359992981, Accuracy: 0.9821428656578064, Computation time: 0.9479231834411621\n",
      "Step: 6360, Loss: 0.9161500930786133, Accuracy: 1.0, Computation time: 0.8523542881011963\n",
      "Step: 6361, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 0.7701027393341064\n",
      "Step: 6362, Loss: 0.9159005880355835, Accuracy: 1.0, Computation time: 1.0637562274932861\n",
      "Step: 6363, Loss: 0.9165500998497009, Accuracy: 1.0, Computation time: 1.0244626998901367\n",
      "Step: 6364, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 0.7964434623718262\n",
      "Step: 6365, Loss: 0.9166766405105591, Accuracy: 1.0, Computation time: 1.189727783203125\n",
      "Step: 6366, Loss: 0.915924608707428, Accuracy: 1.0, Computation time: 0.8562211990356445\n",
      "Step: 6367, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 0.903820276260376\n",
      "Step: 6368, Loss: 0.9158905148506165, Accuracy: 1.0, Computation time: 0.8532474040985107\n",
      "Step: 6369, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 0.7399013042449951\n",
      "Step: 6370, Loss: 0.9158777594566345, Accuracy: 1.0, Computation time: 0.8705644607543945\n",
      "Step: 6371, Loss: 0.9208040833473206, Accuracy: 1.0, Computation time: 0.8869361877441406\n",
      "Step: 6372, Loss: 0.9171446561813354, Accuracy: 1.0, Computation time: 1.3197557926177979\n",
      "Step: 6373, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 0.8579082489013672\n",
      "Step: 6374, Loss: 0.9483360648155212, Accuracy: 0.9444444179534912, Computation time: 1.0467188358306885\n",
      "Step: 6375, Loss: 0.9168335199356079, Accuracy: 1.0, Computation time: 0.8060295581817627\n",
      "Step: 6376, Loss: 0.9377189874649048, Accuracy: 0.9642857313156128, Computation time: 0.9223763942718506\n",
      "Step: 6377, Loss: 0.9378468990325928, Accuracy: 0.9791666865348816, Computation time: 0.7856554985046387\n",
      "Step: 6378, Loss: 0.9160763025283813, Accuracy: 1.0, Computation time: 0.7840070724487305\n",
      "Step: 6379, Loss: 0.9159528017044067, Accuracy: 1.0, Computation time: 1.1003859043121338\n",
      "Step: 6380, Loss: 0.9159672856330872, Accuracy: 1.0, Computation time: 0.8094048500061035\n",
      "Step: 6381, Loss: 0.9166423082351685, Accuracy: 1.0, Computation time: 0.7702054977416992\n",
      "Step: 6382, Loss: 0.9160515069961548, Accuracy: 1.0, Computation time: 0.9242000579833984\n",
      "Step: 6383, Loss: 0.9160388708114624, Accuracy: 1.0, Computation time: 0.9481370449066162\n",
      "Step: 6384, Loss: 0.9159886240959167, Accuracy: 1.0, Computation time: 0.9172494411468506\n",
      "Step: 6385, Loss: 0.937666654586792, Accuracy: 0.9821428656578064, Computation time: 0.8948330879211426\n",
      "Step: 6386, Loss: 0.9159302115440369, Accuracy: 1.0, Computation time: 0.7998533248901367\n",
      "Step: 6387, Loss: 0.9161514043807983, Accuracy: 1.0, Computation time: 0.9638674259185791\n",
      "Step: 6388, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 0.8821072578430176\n",
      "Step: 6389, Loss: 0.9376723766326904, Accuracy: 0.9722222089767456, Computation time: 0.8089027404785156\n",
      "Step: 6390, Loss: 0.9158834218978882, Accuracy: 1.0, Computation time: 0.7715866565704346\n",
      "Step: 6391, Loss: 0.9159040451049805, Accuracy: 1.0, Computation time: 0.7018899917602539\n",
      "Step: 6392, Loss: 0.9160208106040955, Accuracy: 1.0, Computation time: 0.7258074283599854\n",
      "########################\n",
      "Test loss: 1.12541663646698, Test Accuracy_epoch46: 0.6852259635925293\n",
      "########################\n",
      "Step: 6393, Loss: 0.916088342666626, Accuracy: 1.0, Computation time: 0.7413570880889893\n",
      "Step: 6394, Loss: 0.916067361831665, Accuracy: 1.0, Computation time: 0.8252840042114258\n",
      "Step: 6395, Loss: 0.9158856272697449, Accuracy: 1.0, Computation time: 0.8428490161895752\n",
      "Step: 6396, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 0.8172729015350342\n",
      "Step: 6397, Loss: 0.9409229159355164, Accuracy: 0.9807692766189575, Computation time: 0.9686605930328369\n",
      "Step: 6398, Loss: 0.9158706665039062, Accuracy: 1.0, Computation time: 0.8558540344238281\n",
      "Step: 6399, Loss: 0.9158703684806824, Accuracy: 1.0, Computation time: 0.9019777774810791\n",
      "Step: 6400, Loss: 0.9159184098243713, Accuracy: 1.0, Computation time: 0.7899346351623535\n",
      "Step: 6401, Loss: 0.9159677028656006, Accuracy: 1.0, Computation time: 1.0191197395324707\n",
      "Step: 6402, Loss: 0.915919303894043, Accuracy: 1.0, Computation time: 0.9428834915161133\n",
      "Step: 6403, Loss: 0.9160289168357849, Accuracy: 1.0, Computation time: 0.9619288444519043\n",
      "Step: 6404, Loss: 0.9159110188484192, Accuracy: 1.0, Computation time: 0.9383327960968018\n",
      "Step: 6405, Loss: 0.9158840179443359, Accuracy: 1.0, Computation time: 0.7867569923400879\n",
      "Step: 6406, Loss: 0.9374384880065918, Accuracy: 0.9791666865348816, Computation time: 0.8610641956329346\n",
      "Step: 6407, Loss: 0.9158706068992615, Accuracy: 1.0, Computation time: 1.0960655212402344\n",
      "Step: 6408, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 0.8257298469543457\n",
      "Step: 6409, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 0.7333235740661621\n",
      "Step: 6410, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 0.8030035495758057\n",
      "Step: 6411, Loss: 0.915866494178772, Accuracy: 1.0, Computation time: 1.066849946975708\n",
      "Step: 6412, Loss: 0.9159068465232849, Accuracy: 1.0, Computation time: 0.9152743816375732\n",
      "Step: 6413, Loss: 0.9375746846199036, Accuracy: 0.9722222089767456, Computation time: 0.8213610649108887\n",
      "Step: 6414, Loss: 0.9158660173416138, Accuracy: 1.0, Computation time: 0.957176685333252\n",
      "Step: 6415, Loss: 0.915884792804718, Accuracy: 1.0, Computation time: 0.8093669414520264\n",
      "Step: 6416, Loss: 0.9158518314361572, Accuracy: 1.0, Computation time: 0.815680742263794\n",
      "Step: 6417, Loss: 0.9158949255943298, Accuracy: 1.0, Computation time: 0.7781369686126709\n",
      "Step: 6418, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 0.8542788028717041\n",
      "Step: 6419, Loss: 0.9158657789230347, Accuracy: 1.0, Computation time: 0.9774248600006104\n",
      "Step: 6420, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 0.8464565277099609\n",
      "Step: 6421, Loss: 0.9158667325973511, Accuracy: 1.0, Computation time: 0.809943437576294\n",
      "Step: 6422, Loss: 0.9168785214424133, Accuracy: 1.0, Computation time: 1.2534689903259277\n",
      "Step: 6423, Loss: 0.9158589243888855, Accuracy: 1.0, Computation time: 0.8679687976837158\n",
      "Step: 6424, Loss: 0.9158672094345093, Accuracy: 1.0, Computation time: 0.8354651927947998\n",
      "Step: 6425, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 0.799915075302124\n",
      "Step: 6426, Loss: 0.9158575534820557, Accuracy: 1.0, Computation time: 0.8098397254943848\n",
      "Step: 6427, Loss: 0.9375177025794983, Accuracy: 0.9772727489471436, Computation time: 0.7916045188903809\n",
      "Step: 6428, Loss: 0.915850818157196, Accuracy: 1.0, Computation time: 0.9669511318206787\n",
      "Step: 6429, Loss: 0.9158527851104736, Accuracy: 1.0, Computation time: 0.8068485260009766\n",
      "Step: 6430, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 0.8203191757202148\n",
      "Step: 6431, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.0188379287719727\n",
      "Step: 6432, Loss: 0.9158418774604797, Accuracy: 1.0, Computation time: 1.0279486179351807\n",
      "Step: 6433, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 0.911933422088623\n",
      "Step: 6434, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 0.9978463649749756\n",
      "Step: 6435, Loss: 0.915844202041626, Accuracy: 1.0, Computation time: 0.8701748847961426\n",
      "Step: 6436, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 0.802635908126831\n",
      "Step: 6437, Loss: 0.915847659111023, Accuracy: 1.0, Computation time: 0.871649980545044\n",
      "Step: 6438, Loss: 0.9158506393432617, Accuracy: 1.0, Computation time: 0.8847661018371582\n",
      "Step: 6439, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 0.9075558185577393\n",
      "Step: 6440, Loss: 0.9164059162139893, Accuracy: 1.0, Computation time: 1.1447968482971191\n",
      "Step: 6441, Loss: 0.9174074530601501, Accuracy: 1.0, Computation time: 0.8090708255767822\n",
      "Step: 6442, Loss: 0.9347463846206665, Accuracy: 0.9722222089767456, Computation time: 0.8388943672180176\n",
      "Step: 6443, Loss: 0.9227344393730164, Accuracy: 1.0, Computation time: 0.9253871440887451\n",
      "Step: 6444, Loss: 0.9159839153289795, Accuracy: 1.0, Computation time: 0.7366538047790527\n",
      "Step: 6445, Loss: 0.9377526640892029, Accuracy: 0.9772727489471436, Computation time: 0.7141027450561523\n",
      "Step: 6446, Loss: 0.9159740805625916, Accuracy: 1.0, Computation time: 0.8343234062194824\n",
      "Step: 6447, Loss: 0.9159796237945557, Accuracy: 1.0, Computation time: 0.7266571521759033\n",
      "Step: 6448, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 0.7550830841064453\n",
      "Step: 6449, Loss: 0.9376909732818604, Accuracy: 0.9722222089767456, Computation time: 0.7909214496612549\n",
      "Step: 6450, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 0.7257802486419678\n",
      "Step: 6451, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 0.9008972644805908\n",
      "Step: 6452, Loss: 0.9158669710159302, Accuracy: 1.0, Computation time: 0.8230888843536377\n",
      "Step: 6453, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 0.7714745998382568\n",
      "Step: 6454, Loss: 0.9159221649169922, Accuracy: 1.0, Computation time: 0.8395445346832275\n",
      "Step: 6455, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 0.71028733253479\n",
      "Step: 6456, Loss: 0.9158911108970642, Accuracy: 1.0, Computation time: 0.8574082851409912\n",
      "Step: 6457, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 0.7029669284820557\n",
      "Step: 6458, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 0.7414100170135498\n",
      "Step: 6459, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 0.9365630149841309\n",
      "Step: 6460, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 0.8363986015319824\n",
      "Step: 6461, Loss: 0.915869414806366, Accuracy: 1.0, Computation time: 0.7496559619903564\n",
      "Step: 6462, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 0.7544910907745361\n",
      "Step: 6463, Loss: 0.9158680438995361, Accuracy: 1.0, Computation time: 0.6668343544006348\n",
      "Step: 6464, Loss: 0.9158673882484436, Accuracy: 1.0, Computation time: 0.8053925037384033\n",
      "Step: 6465, Loss: 0.9158537983894348, Accuracy: 1.0, Computation time: 0.7153768539428711\n",
      "Step: 6466, Loss: 0.9592623114585876, Accuracy: 0.9010416865348816, Computation time: 1.2166967391967773\n",
      "Step: 6467, Loss: 0.9161694049835205, Accuracy: 1.0, Computation time: 0.939861536026001\n",
      "Step: 6468, Loss: 0.9324807524681091, Accuracy: 0.9750000238418579, Computation time: 0.790687084197998\n",
      "Step: 6469, Loss: 0.9158801436424255, Accuracy: 1.0, Computation time: 0.863283634185791\n",
      "Step: 6470, Loss: 0.9159011840820312, Accuracy: 1.0, Computation time: 0.791071891784668\n",
      "Step: 6471, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 0.7265808582305908\n",
      "Step: 6472, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 0.7826011180877686\n",
      "Step: 6473, Loss: 0.9160560369491577, Accuracy: 1.0, Computation time: 1.070502519607544\n",
      "Step: 6474, Loss: 0.915984034538269, Accuracy: 1.0, Computation time: 0.6998245716094971\n",
      "Step: 6475, Loss: 0.9165903329849243, Accuracy: 1.0, Computation time: 0.9670968055725098\n",
      "Step: 6476, Loss: 0.9158876538276672, Accuracy: 1.0, Computation time: 0.8448276519775391\n",
      "Step: 6477, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.8246700763702393\n",
      "Step: 6478, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 0.7448151111602783\n",
      "Step: 6479, Loss: 0.9159141182899475, Accuracy: 1.0, Computation time: 0.8541269302368164\n",
      "Step: 6480, Loss: 0.9159361124038696, Accuracy: 1.0, Computation time: 0.8282301425933838\n",
      "Step: 6481, Loss: 0.9158899784088135, Accuracy: 1.0, Computation time: 0.9821655750274658\n",
      "Step: 6482, Loss: 0.9374200701713562, Accuracy: 0.96875, Computation time: 0.8054208755493164\n",
      "Step: 6483, Loss: 0.9171971082687378, Accuracy: 1.0, Computation time: 1.191441535949707\n",
      "Step: 6484, Loss: 0.9167819023132324, Accuracy: 1.0, Computation time: 0.9603838920593262\n",
      "Step: 6485, Loss: 0.9375768303871155, Accuracy: 0.9722222089767456, Computation time: 0.7573609352111816\n",
      "Step: 6486, Loss: 0.9158580899238586, Accuracy: 1.0, Computation time: 0.8185482025146484\n",
      "Step: 6487, Loss: 0.9158671498298645, Accuracy: 1.0, Computation time: 0.8613812923431396\n",
      "Step: 6488, Loss: 0.9158703088760376, Accuracy: 1.0, Computation time: 0.8655095100402832\n",
      "Step: 6489, Loss: 0.9158660769462585, Accuracy: 1.0, Computation time: 0.787055253982544\n",
      "Step: 6490, Loss: 0.9158492684364319, Accuracy: 1.0, Computation time: 0.7589201927185059\n",
      "Step: 6491, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 0.7813153266906738\n",
      "Step: 6492, Loss: 0.9158392548561096, Accuracy: 1.0, Computation time: 0.797187089920044\n",
      "Step: 6493, Loss: 0.9158422350883484, Accuracy: 1.0, Computation time: 0.7858316898345947\n",
      "Step: 6494, Loss: 0.9158497452735901, Accuracy: 1.0, Computation time: 0.7078571319580078\n",
      "Step: 6495, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 0.7444169521331787\n",
      "Step: 6496, Loss: 0.9159061908721924, Accuracy: 1.0, Computation time: 0.8262183666229248\n",
      "Step: 6497, Loss: 0.9375293850898743, Accuracy: 0.9750000238418579, Computation time: 0.7771377563476562\n",
      "Step: 6498, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 0.8546953201293945\n",
      "Step: 6499, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 0.686387300491333\n",
      "Step: 6500, Loss: 0.9305893182754517, Accuracy: 0.9722222089767456, Computation time: 0.767275333404541\n",
      "Step: 6501, Loss: 0.9158604145050049, Accuracy: 1.0, Computation time: 0.6762230396270752\n",
      "Step: 6502, Loss: 0.9375996589660645, Accuracy: 0.9722222089767456, Computation time: 0.9761753082275391\n",
      "Step: 6503, Loss: 0.9160051941871643, Accuracy: 1.0, Computation time: 0.722665548324585\n",
      "Step: 6504, Loss: 0.9159714579582214, Accuracy: 1.0, Computation time: 0.693915843963623\n",
      "Step: 6505, Loss: 0.9159550666809082, Accuracy: 1.0, Computation time: 0.8311786651611328\n",
      "Step: 6506, Loss: 0.915955662727356, Accuracy: 1.0, Computation time: 0.7401041984558105\n",
      "Step: 6507, Loss: 0.9158783555030823, Accuracy: 1.0, Computation time: 0.7758638858795166\n",
      "Step: 6508, Loss: 0.9158807396888733, Accuracy: 1.0, Computation time: 0.8497538566589355\n",
      "Step: 6509, Loss: 0.9158756732940674, Accuracy: 1.0, Computation time: 0.730398416519165\n",
      "Step: 6510, Loss: 0.915891706943512, Accuracy: 1.0, Computation time: 0.7477409839630127\n",
      "Step: 6511, Loss: 0.9159046411514282, Accuracy: 1.0, Computation time: 1.0092480182647705\n",
      "Step: 6512, Loss: 0.916228711605072, Accuracy: 1.0, Computation time: 0.7351663112640381\n",
      "Step: 6513, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 0.7464253902435303\n",
      "Step: 6514, Loss: 0.9158709645271301, Accuracy: 1.0, Computation time: 0.754521369934082\n",
      "Step: 6515, Loss: 0.9160014986991882, Accuracy: 1.0, Computation time: 0.763991117477417\n",
      "Step: 6516, Loss: 0.9375869035720825, Accuracy: 0.96875, Computation time: 0.7846674919128418\n",
      "Step: 6517, Loss: 0.9158621430397034, Accuracy: 1.0, Computation time: 0.7156472206115723\n",
      "Step: 6518, Loss: 0.915867269039154, Accuracy: 1.0, Computation time: 0.7781164646148682\n",
      "Step: 6519, Loss: 0.9377135038375854, Accuracy: 0.949999988079071, Computation time: 0.797827959060669\n",
      "Step: 6520, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 0.7852673530578613\n",
      "Step: 6521, Loss: 0.937528669834137, Accuracy: 0.9772727489471436, Computation time: 0.8094658851623535\n",
      "Step: 6522, Loss: 0.9373672008514404, Accuracy: 0.9772727489471436, Computation time: 0.7533307075500488\n",
      "Step: 6523, Loss: 0.9158655405044556, Accuracy: 1.0, Computation time: 0.7441000938415527\n",
      "Step: 6524, Loss: 0.9158565998077393, Accuracy: 1.0, Computation time: 0.7455976009368896\n",
      "Step: 6525, Loss: 0.9159006476402283, Accuracy: 1.0, Computation time: 0.9698836803436279\n",
      "Step: 6526, Loss: 0.9158695936203003, Accuracy: 1.0, Computation time: 0.7073101997375488\n",
      "Step: 6527, Loss: 0.9383635520935059, Accuracy: 0.949999988079071, Computation time: 0.8575315475463867\n",
      "Step: 6528, Loss: 0.9258204698562622, Accuracy: 0.9642857313156128, Computation time: 1.6000721454620361\n",
      "Step: 6529, Loss: 0.9158788919448853, Accuracy: 1.0, Computation time: 0.7808687686920166\n",
      "Step: 6530, Loss: 0.9264342188835144, Accuracy: 0.9722222089767456, Computation time: 0.9047927856445312\n",
      "Step: 6531, Loss: 0.9325796365737915, Accuracy: 0.9791666865348816, Computation time: 1.1237468719482422\n",
      "########################\n",
      "Test loss: 1.1248453855514526, Test Accuracy_epoch47: 0.690900444984436\n",
      "########################\n",
      "Step: 6532, Loss: 0.9160193204879761, Accuracy: 1.0, Computation time: 0.7307934761047363\n",
      "Step: 6533, Loss: 0.9161069989204407, Accuracy: 1.0, Computation time: 1.0217885971069336\n",
      "Step: 6534, Loss: 0.9160785675048828, Accuracy: 1.0, Computation time: 0.7478318214416504\n",
      "Step: 6535, Loss: 0.916057825088501, Accuracy: 1.0, Computation time: 0.8291130065917969\n",
      "Step: 6536, Loss: 0.9160147905349731, Accuracy: 1.0, Computation time: 0.7569973468780518\n",
      "Step: 6537, Loss: 0.9161278009414673, Accuracy: 1.0, Computation time: 0.7640633583068848\n",
      "Step: 6538, Loss: 0.9159215092658997, Accuracy: 1.0, Computation time: 0.764589786529541\n",
      "Step: 6539, Loss: 0.915928065776825, Accuracy: 1.0, Computation time: 0.7097961902618408\n",
      "Step: 6540, Loss: 0.9159511923789978, Accuracy: 1.0, Computation time: 0.7760870456695557\n",
      "Step: 6541, Loss: 0.9165142774581909, Accuracy: 1.0, Computation time: 0.8924088478088379\n",
      "Step: 6542, Loss: 0.9376698136329651, Accuracy: 0.9722222089767456, Computation time: 0.760716438293457\n",
      "Step: 6543, Loss: 0.916002094745636, Accuracy: 1.0, Computation time: 0.8362812995910645\n",
      "Step: 6544, Loss: 0.9158993363380432, Accuracy: 1.0, Computation time: 0.8513991832733154\n",
      "Step: 6545, Loss: 0.9158953428268433, Accuracy: 1.0, Computation time: 0.9549517631530762\n",
      "Step: 6546, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 0.7899661064147949\n",
      "Step: 6547, Loss: 0.9158926606178284, Accuracy: 1.0, Computation time: 1.2112658023834229\n",
      "Step: 6548, Loss: 0.9161891341209412, Accuracy: 1.0, Computation time: 0.9402942657470703\n",
      "Step: 6549, Loss: 0.9158744812011719, Accuracy: 1.0, Computation time: 0.9720396995544434\n",
      "Step: 6550, Loss: 0.9158930778503418, Accuracy: 1.0, Computation time: 0.9207525253295898\n",
      "Step: 6551, Loss: 0.9158766269683838, Accuracy: 1.0, Computation time: 1.029212236404419\n",
      "Step: 6552, Loss: 0.9375432133674622, Accuracy: 0.96875, Computation time: 0.9478652477264404\n",
      "Step: 6553, Loss: 0.9161286354064941, Accuracy: 1.0, Computation time: 1.0398297309875488\n",
      "Step: 6554, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 0.9489583969116211\n",
      "Step: 6555, Loss: 0.9158782362937927, Accuracy: 1.0, Computation time: 0.9072487354278564\n",
      "Step: 6556, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 0.8032379150390625\n",
      "Step: 6557, Loss: 0.9179356694221497, Accuracy: 1.0, Computation time: 0.8875596523284912\n",
      "Step: 6558, Loss: 0.9158651828765869, Accuracy: 1.0, Computation time: 1.0078117847442627\n",
      "Step: 6559, Loss: 0.9434205889701843, Accuracy: 0.984375, Computation time: 1.0013093948364258\n",
      "Step: 6560, Loss: 0.9159218072891235, Accuracy: 1.0, Computation time: 0.9789524078369141\n",
      "Step: 6561, Loss: 0.9591900706291199, Accuracy: 0.9564393758773804, Computation time: 1.2006263732910156\n",
      "Step: 6562, Loss: 0.9227256774902344, Accuracy: 1.0, Computation time: 0.993588924407959\n",
      "Step: 6563, Loss: 0.9378292560577393, Accuracy: 0.9642857313156128, Computation time: 1.0586984157562256\n",
      "Step: 6564, Loss: 0.9159756302833557, Accuracy: 1.0, Computation time: 1.0701324939727783\n",
      "Step: 6565, Loss: 0.9159351587295532, Accuracy: 1.0, Computation time: 1.2204101085662842\n",
      "Step: 6566, Loss: 0.9169405102729797, Accuracy: 1.0, Computation time: 0.9682307243347168\n",
      "Step: 6567, Loss: 0.9159597754478455, Accuracy: 1.0, Computation time: 1.0406138896942139\n",
      "Step: 6568, Loss: 0.9160072803497314, Accuracy: 1.0, Computation time: 1.1699459552764893\n",
      "Step: 6569, Loss: 0.9206743836402893, Accuracy: 1.0, Computation time: 1.403935194015503\n",
      "Step: 6570, Loss: 0.915932297706604, Accuracy: 1.0, Computation time: 0.8963203430175781\n",
      "Step: 6571, Loss: 0.9159560203552246, Accuracy: 1.0, Computation time: 0.846235990524292\n",
      "Step: 6572, Loss: 0.9175772070884705, Accuracy: 1.0, Computation time: 0.8269052505493164\n",
      "Step: 6573, Loss: 0.916041910648346, Accuracy: 1.0, Computation time: 1.1094863414764404\n",
      "Step: 6574, Loss: 0.9159148335456848, Accuracy: 1.0, Computation time: 1.2315645217895508\n",
      "Step: 6575, Loss: 0.9158696532249451, Accuracy: 1.0, Computation time: 0.9216341972351074\n",
      "Step: 6576, Loss: 0.9158698916435242, Accuracy: 1.0, Computation time: 1.101621389389038\n",
      "Step: 6577, Loss: 0.9160407185554504, Accuracy: 1.0, Computation time: 1.2373290061950684\n",
      "Step: 6578, Loss: 0.9159677624702454, Accuracy: 1.0, Computation time: 0.9774255752563477\n",
      "Step: 6579, Loss: 0.9160258769989014, Accuracy: 1.0, Computation time: 1.0589599609375\n",
      "Step: 6580, Loss: 0.9159713983535767, Accuracy: 1.0, Computation time: 0.9447381496429443\n",
      "Step: 6581, Loss: 0.9159185886383057, Accuracy: 1.0, Computation time: 0.9045822620391846\n",
      "Step: 6582, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.103590965270996\n",
      "Step: 6583, Loss: 0.9158592820167542, Accuracy: 1.0, Computation time: 0.9123506546020508\n",
      "Step: 6584, Loss: 0.9158556461334229, Accuracy: 1.0, Computation time: 0.9137051105499268\n",
      "Step: 6585, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 0.911963939666748\n",
      "Step: 6586, Loss: 0.9175623059272766, Accuracy: 1.0, Computation time: 1.070502519607544\n",
      "Step: 6587, Loss: 0.9161753058433533, Accuracy: 1.0, Computation time: 1.431309700012207\n",
      "Step: 6588, Loss: 0.9160345196723938, Accuracy: 1.0, Computation time: 0.9096405506134033\n",
      "Step: 6589, Loss: 0.9376169443130493, Accuracy: 0.9807692766189575, Computation time: 0.9315145015716553\n",
      "Step: 6590, Loss: 0.9374841451644897, Accuracy: 0.9791666865348816, Computation time: 0.9147822856903076\n",
      "Step: 6591, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.009687900543213\n",
      "Step: 6592, Loss: 0.9375361800193787, Accuracy: 0.9807692766189575, Computation time: 0.8766913414001465\n",
      "Step: 6593, Loss: 0.915919840335846, Accuracy: 1.0, Computation time: 1.0235536098480225\n",
      "Step: 6594, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 0.8241837024688721\n",
      "Step: 6595, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 0.9139173030853271\n",
      "Step: 6596, Loss: 0.9162720441818237, Accuracy: 1.0, Computation time: 1.1615502834320068\n",
      "Step: 6597, Loss: 0.9373778104782104, Accuracy: 0.9722222089767456, Computation time: 0.841414213180542\n",
      "Step: 6598, Loss: 0.9158462882041931, Accuracy: 1.0, Computation time: 0.7751374244689941\n",
      "Step: 6599, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 0.9737370014190674\n",
      "Step: 6600, Loss: 0.9158778190612793, Accuracy: 1.0, Computation time: 0.8856077194213867\n",
      "Step: 6601, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.8524186611175537\n",
      "Step: 6602, Loss: 0.9374192953109741, Accuracy: 0.96875, Computation time: 1.0933077335357666\n",
      "Step: 6603, Loss: 0.9355103373527527, Accuracy: 0.9642857313156128, Computation time: 0.9381918907165527\n",
      "Step: 6604, Loss: 0.9159420728683472, Accuracy: 1.0, Computation time: 1.314164638519287\n",
      "Step: 6605, Loss: 0.9285338521003723, Accuracy: 0.9722222089767456, Computation time: 1.1472446918487549\n",
      "Step: 6606, Loss: 0.9158809185028076, Accuracy: 1.0, Computation time: 0.8080284595489502\n",
      "Step: 6607, Loss: 0.9159358143806458, Accuracy: 1.0, Computation time: 1.1955504417419434\n",
      "Step: 6608, Loss: 0.9160048365592957, Accuracy: 1.0, Computation time: 0.9279453754425049\n",
      "Step: 6609, Loss: 0.9377328753471375, Accuracy: 0.9722222089767456, Computation time: 0.8592047691345215\n",
      "Step: 6610, Loss: 0.9160103797912598, Accuracy: 1.0, Computation time: 1.0281431674957275\n",
      "Step: 6611, Loss: 0.916181743144989, Accuracy: 1.0, Computation time: 0.9051065444946289\n",
      "Step: 6612, Loss: 0.9159134030342102, Accuracy: 1.0, Computation time: 0.8852207660675049\n",
      "Step: 6613, Loss: 0.9376352429389954, Accuracy: 0.9722222089767456, Computation time: 0.8363432884216309\n",
      "Step: 6614, Loss: 0.915948212146759, Accuracy: 1.0, Computation time: 0.9439859390258789\n",
      "Step: 6615, Loss: 0.9158691167831421, Accuracy: 1.0, Computation time: 1.1373820304870605\n",
      "Step: 6616, Loss: 0.9159409403800964, Accuracy: 1.0, Computation time: 1.1080436706542969\n",
      "Step: 6617, Loss: 0.9159584045410156, Accuracy: 1.0, Computation time: 0.833564043045044\n",
      "Step: 6618, Loss: 0.9374873638153076, Accuracy: 0.9722222089767456, Computation time: 0.8966362476348877\n",
      "Step: 6619, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 0.8410122394561768\n",
      "Step: 6620, Loss: 0.9159151911735535, Accuracy: 1.0, Computation time: 0.8176803588867188\n",
      "Step: 6621, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 0.9303882122039795\n",
      "Step: 6622, Loss: 0.9158961772918701, Accuracy: 1.0, Computation time: 0.8380639553070068\n",
      "Step: 6623, Loss: 0.9158540964126587, Accuracy: 1.0, Computation time: 0.7723820209503174\n",
      "Step: 6624, Loss: 0.915989339351654, Accuracy: 1.0, Computation time: 1.4607930183410645\n",
      "Step: 6625, Loss: 0.915846586227417, Accuracy: 1.0, Computation time: 0.9501254558563232\n",
      "Step: 6626, Loss: 0.915992259979248, Accuracy: 1.0, Computation time: 1.2234008312225342\n",
      "Step: 6627, Loss: 0.9159087538719177, Accuracy: 1.0, Computation time: 1.0309021472930908\n",
      "Step: 6628, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 0.865947961807251\n",
      "Step: 6629, Loss: 0.9168779253959656, Accuracy: 1.0, Computation time: 0.9171078205108643\n",
      "Step: 6630, Loss: 0.959311306476593, Accuracy: 0.9494949579238892, Computation time: 1.124988317489624\n",
      "Step: 6631, Loss: 0.915864109992981, Accuracy: 1.0, Computation time: 0.9359569549560547\n",
      "Step: 6632, Loss: 0.9375023245811462, Accuracy: 0.9772727489471436, Computation time: 0.8639850616455078\n",
      "Step: 6633, Loss: 0.915885329246521, Accuracy: 1.0, Computation time: 0.9392333030700684\n",
      "Step: 6634, Loss: 0.9158658385276794, Accuracy: 1.0, Computation time: 0.9292600154876709\n",
      "Step: 6635, Loss: 0.9200985431671143, Accuracy: 1.0, Computation time: 1.06076979637146\n",
      "Step: 6636, Loss: 0.9159101247787476, Accuracy: 1.0, Computation time: 0.9694807529449463\n",
      "Step: 6637, Loss: 0.9159665107727051, Accuracy: 1.0, Computation time: 0.8875157833099365\n",
      "Step: 6638, Loss: 0.9161864519119263, Accuracy: 1.0, Computation time: 0.8201820850372314\n",
      "Step: 6639, Loss: 0.926392138004303, Accuracy: 0.9772727489471436, Computation time: 2.05794358253479\n",
      "Step: 6640, Loss: 0.9159260392189026, Accuracy: 1.0, Computation time: 1.1145122051239014\n",
      "Step: 6641, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 0.7375895977020264\n",
      "Step: 6642, Loss: 0.937595009803772, Accuracy: 0.9642857313156128, Computation time: 0.9503862857818604\n",
      "Step: 6643, Loss: 0.9417896270751953, Accuracy: 0.9750000238418579, Computation time: 0.9044017791748047\n",
      "Step: 6644, Loss: 0.916098415851593, Accuracy: 1.0, Computation time: 0.9535355567932129\n",
      "Step: 6645, Loss: 0.9160913228988647, Accuracy: 1.0, Computation time: 0.8167800903320312\n",
      "Step: 6646, Loss: 0.9161986708641052, Accuracy: 1.0, Computation time: 0.8355576992034912\n",
      "Step: 6647, Loss: 0.9159922003746033, Accuracy: 1.0, Computation time: 0.7389979362487793\n",
      "Step: 6648, Loss: 0.9160147309303284, Accuracy: 1.0, Computation time: 0.9592421054840088\n",
      "Step: 6649, Loss: 0.934611439704895, Accuracy: 0.9583333730697632, Computation time: 1.3743417263031006\n",
      "Step: 6650, Loss: 0.915955126285553, Accuracy: 1.0, Computation time: 0.9194872379302979\n",
      "Step: 6651, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 0.892120361328125\n",
      "Step: 6652, Loss: 0.916043758392334, Accuracy: 1.0, Computation time: 0.7934982776641846\n",
      "Step: 6653, Loss: 0.9160139560699463, Accuracy: 1.0, Computation time: 0.7957642078399658\n",
      "Step: 6654, Loss: 0.9681795835494995, Accuracy: 0.9513888955116272, Computation time: 1.3535668849945068\n",
      "Step: 6655, Loss: 0.9376880526542664, Accuracy: 0.9791666865348816, Computation time: 0.95296311378479\n",
      "Step: 6656, Loss: 0.916225790977478, Accuracy: 1.0, Computation time: 1.1988227367401123\n",
      "Step: 6657, Loss: 0.9592936635017395, Accuracy: 0.9476190805435181, Computation time: 0.7982985973358154\n",
      "Step: 6658, Loss: 0.9161849617958069, Accuracy: 1.0, Computation time: 0.7395405769348145\n",
      "Step: 6659, Loss: 0.916084349155426, Accuracy: 1.0, Computation time: 0.8368759155273438\n",
      "Step: 6660, Loss: 0.916012704372406, Accuracy: 1.0, Computation time: 0.8207516670227051\n",
      "Step: 6661, Loss: 0.9198663830757141, Accuracy: 1.0, Computation time: 1.2269322872161865\n",
      "Step: 6662, Loss: 0.9159095883369446, Accuracy: 1.0, Computation time: 0.772552490234375\n",
      "Step: 6663, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 0.8830132484436035\n",
      "Step: 6664, Loss: 0.9224306344985962, Accuracy: 1.0, Computation time: 1.0234389305114746\n",
      "Step: 6665, Loss: 0.9160428643226624, Accuracy: 1.0, Computation time: 0.7390775680541992\n",
      "Step: 6666, Loss: 0.9161584377288818, Accuracy: 1.0, Computation time: 1.098189353942871\n",
      "Step: 6667, Loss: 0.9161725640296936, Accuracy: 1.0, Computation time: 0.9603240489959717\n",
      "Step: 6668, Loss: 0.9161476492881775, Accuracy: 1.0, Computation time: 0.7197837829589844\n",
      "Step: 6669, Loss: 0.9159660935401917, Accuracy: 1.0, Computation time: 0.8653340339660645\n",
      "########################\n",
      "Test loss: 1.125633716583252, Test Accuracy_epoch48: 0.6867450475692749\n",
      "########################\n",
      "Step: 6670, Loss: 0.9168360829353333, Accuracy: 1.0, Computation time: 0.8115036487579346\n",
      "Step: 6671, Loss: 0.9159429669380188, Accuracy: 1.0, Computation time: 0.7794089317321777\n",
      "Step: 6672, Loss: 0.915938138961792, Accuracy: 1.0, Computation time: 0.7535984516143799\n",
      "Step: 6673, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 0.7940423488616943\n",
      "Step: 6674, Loss: 0.9159851670265198, Accuracy: 1.0, Computation time: 1.00836181640625\n",
      "Step: 6675, Loss: 0.9159877896308899, Accuracy: 1.0, Computation time: 1.0004081726074219\n",
      "Step: 6676, Loss: 0.9159815311431885, Accuracy: 1.0, Computation time: 0.9596946239471436\n",
      "Step: 6677, Loss: 0.9159883856773376, Accuracy: 1.0, Computation time: 0.7854387760162354\n",
      "Step: 6678, Loss: 0.917112410068512, Accuracy: 1.0, Computation time: 0.923835039138794\n",
      "Step: 6679, Loss: 0.915915846824646, Accuracy: 1.0, Computation time: 0.7540798187255859\n",
      "Step: 6680, Loss: 0.9376547932624817, Accuracy: 0.9166666865348816, Computation time: 0.8029451370239258\n",
      "Step: 6681, Loss: 0.9158948063850403, Accuracy: 1.0, Computation time: 0.7751307487487793\n",
      "Step: 6682, Loss: 0.9377709627151489, Accuracy: 0.9807692766189575, Computation time: 0.7413580417633057\n",
      "Step: 6683, Loss: 0.9159867763519287, Accuracy: 1.0, Computation time: 0.779141902923584\n",
      "Step: 6684, Loss: 0.916038453578949, Accuracy: 1.0, Computation time: 0.8365614414215088\n",
      "Step: 6685, Loss: 0.9159864187240601, Accuracy: 1.0, Computation time: 0.6857340335845947\n",
      "Step: 6686, Loss: 0.9376540780067444, Accuracy: 0.96875, Computation time: 0.719902753829956\n",
      "Step: 6687, Loss: 0.9159325361251831, Accuracy: 1.0, Computation time: 0.9218080043792725\n",
      "Step: 6688, Loss: 0.9159323573112488, Accuracy: 1.0, Computation time: 0.7575745582580566\n",
      "Step: 6689, Loss: 0.915979266166687, Accuracy: 1.0, Computation time: 0.9955997467041016\n",
      "Step: 6690, Loss: 0.915942370891571, Accuracy: 1.0, Computation time: 0.701030969619751\n",
      "Step: 6691, Loss: 0.9283291101455688, Accuracy: 0.9750000238418579, Computation time: 0.7534193992614746\n",
      "Step: 6692, Loss: 0.91599041223526, Accuracy: 1.0, Computation time: 1.013275384902954\n",
      "Step: 6693, Loss: 0.9159450531005859, Accuracy: 1.0, Computation time: 0.7647926807403564\n",
      "Step: 6694, Loss: 0.9159841537475586, Accuracy: 1.0, Computation time: 0.7263498306274414\n",
      "Step: 6695, Loss: 0.9162873029708862, Accuracy: 1.0, Computation time: 0.7017672061920166\n",
      "Step: 6696, Loss: 0.9159902334213257, Accuracy: 1.0, Computation time: 0.8311402797698975\n",
      "Step: 6697, Loss: 0.9159771800041199, Accuracy: 1.0, Computation time: 0.7665920257568359\n",
      "Step: 6698, Loss: 0.9159742593765259, Accuracy: 1.0, Computation time: 0.7690398693084717\n",
      "Step: 6699, Loss: 0.9159222841262817, Accuracy: 1.0, Computation time: 0.741849422454834\n",
      "Step: 6700, Loss: 0.9167830348014832, Accuracy: 1.0, Computation time: 1.2196569442749023\n",
      "Step: 6701, Loss: 0.9160232543945312, Accuracy: 1.0, Computation time: 0.7691044807434082\n",
      "Step: 6702, Loss: 0.9165268540382385, Accuracy: 1.0, Computation time: 1.1513340473175049\n",
      "Step: 6703, Loss: 0.9159727096557617, Accuracy: 1.0, Computation time: 0.8240664005279541\n",
      "Step: 6704, Loss: 0.9161295294761658, Accuracy: 1.0, Computation time: 0.8602399826049805\n",
      "Step: 6705, Loss: 0.9389646649360657, Accuracy: 0.9750000238418579, Computation time: 0.8058409690856934\n",
      "Step: 6706, Loss: 0.916509747505188, Accuracy: 1.0, Computation time: 0.774681806564331\n",
      "Step: 6707, Loss: 0.9162313342094421, Accuracy: 1.0, Computation time: 0.7403292655944824\n",
      "Step: 6708, Loss: 0.9159620404243469, Accuracy: 1.0, Computation time: 0.7128946781158447\n",
      "Step: 6709, Loss: 0.9159494638442993, Accuracy: 1.0, Computation time: 0.6801848411560059\n",
      "Step: 6710, Loss: 0.9159374237060547, Accuracy: 1.0, Computation time: 0.8942544460296631\n",
      "Step: 6711, Loss: 0.917292058467865, Accuracy: 1.0, Computation time: 0.8468289375305176\n",
      "Step: 6712, Loss: 0.9376438856124878, Accuracy: 0.9722222089767456, Computation time: 0.8032939434051514\n",
      "Step: 6713, Loss: 0.9165664315223694, Accuracy: 1.0, Computation time: 0.9833638668060303\n",
      "Step: 6714, Loss: 0.9160488247871399, Accuracy: 1.0, Computation time: 0.9810314178466797\n",
      "Step: 6715, Loss: 0.9161131978034973, Accuracy: 1.0, Computation time: 0.7440090179443359\n",
      "Step: 6716, Loss: 0.937744140625, Accuracy: 0.96875, Computation time: 0.8080756664276123\n",
      "Step: 6717, Loss: 0.9160969257354736, Accuracy: 1.0, Computation time: 1.1074085235595703\n",
      "Step: 6718, Loss: 0.9160782694816589, Accuracy: 1.0, Computation time: 0.9556858539581299\n",
      "Step: 6719, Loss: 0.9159522652626038, Accuracy: 1.0, Computation time: 1.8820395469665527\n",
      "Step: 6720, Loss: 0.9159010648727417, Accuracy: 1.0, Computation time: 0.7819595336914062\n",
      "Step: 6721, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 0.793046236038208\n",
      "Step: 6722, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 0.81559157371521\n",
      "Step: 6723, Loss: 0.9160095453262329, Accuracy: 1.0, Computation time: 0.8165695667266846\n",
      "Step: 6724, Loss: 0.9159402251243591, Accuracy: 1.0, Computation time: 0.8422102928161621\n",
      "Step: 6725, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 0.8486182689666748\n",
      "Step: 6726, Loss: 0.9375620484352112, Accuracy: 0.9807692766189575, Computation time: 0.7657506465911865\n",
      "Step: 6727, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 0.7070131301879883\n",
      "Step: 6728, Loss: 0.9159584045410156, Accuracy: 1.0, Computation time: 0.7743067741394043\n",
      "Step: 6729, Loss: 0.9159191250801086, Accuracy: 1.0, Computation time: 0.7695775032043457\n",
      "Step: 6730, Loss: 0.9159061312675476, Accuracy: 1.0, Computation time: 0.7758002281188965\n",
      "Step: 6731, Loss: 0.9159657955169678, Accuracy: 1.0, Computation time: 0.9686949253082275\n",
      "Step: 6732, Loss: 0.9158610701560974, Accuracy: 1.0, Computation time: 0.8184778690338135\n",
      "Step: 6733, Loss: 0.9158741235733032, Accuracy: 1.0, Computation time: 0.9115056991577148\n",
      "Step: 6734, Loss: 0.9158898591995239, Accuracy: 1.0, Computation time: 0.8522100448608398\n",
      "Step: 6735, Loss: 0.915928840637207, Accuracy: 1.0, Computation time: 0.7878506183624268\n",
      "Step: 6736, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.0663220882415771\n",
      "Step: 6737, Loss: 0.9160033464431763, Accuracy: 1.0, Computation time: 0.9669368267059326\n",
      "Step: 6738, Loss: 0.9379603862762451, Accuracy: 0.9750000238418579, Computation time: 1.2140474319458008\n",
      "Step: 6739, Loss: 0.9158614873886108, Accuracy: 1.0, Computation time: 0.9915270805358887\n",
      "Step: 6740, Loss: 0.9159899353981018, Accuracy: 1.0, Computation time: 1.0247571468353271\n",
      "Step: 6741, Loss: 0.915863037109375, Accuracy: 1.0, Computation time: 0.7708439826965332\n",
      "Step: 6742, Loss: 0.9158623218536377, Accuracy: 1.0, Computation time: 1.1333684921264648\n",
      "Step: 6743, Loss: 0.915875256061554, Accuracy: 1.0, Computation time: 0.7253215312957764\n",
      "Step: 6744, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 0.8800880908966064\n",
      "Step: 6745, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 0.7658419609069824\n",
      "Step: 6746, Loss: 0.9158837795257568, Accuracy: 1.0, Computation time: 0.841529130935669\n",
      "Step: 6747, Loss: 0.9158756136894226, Accuracy: 1.0, Computation time: 0.9090595245361328\n",
      "Step: 6748, Loss: 0.9158705472946167, Accuracy: 1.0, Computation time: 0.8316478729248047\n",
      "Step: 6749, Loss: 0.915922224521637, Accuracy: 1.0, Computation time: 0.8998465538024902\n",
      "Step: 6750, Loss: 0.9158583879470825, Accuracy: 1.0, Computation time: 0.9817767143249512\n",
      "Step: 6751, Loss: 0.9158642292022705, Accuracy: 1.0, Computation time: 0.87052321434021\n",
      "Step: 6752, Loss: 0.915885865688324, Accuracy: 1.0, Computation time: 0.9523708820343018\n",
      "Step: 6753, Loss: 0.9159003496170044, Accuracy: 1.0, Computation time: 0.9917812347412109\n",
      "Step: 6754, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 0.7789881229400635\n",
      "Step: 6755, Loss: 0.9160588979721069, Accuracy: 1.0, Computation time: 0.7896304130554199\n",
      "Step: 6756, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 0.8245692253112793\n",
      "Step: 6757, Loss: 0.9380030035972595, Accuracy: 0.9722222089767456, Computation time: 1.1070706844329834\n",
      "Step: 6758, Loss: 0.9159345030784607, Accuracy: 1.0, Computation time: 0.7925243377685547\n",
      "Step: 6759, Loss: 0.9158692955970764, Accuracy: 1.0, Computation time: 1.205883264541626\n",
      "Step: 6760, Loss: 0.9158808588981628, Accuracy: 1.0, Computation time: 0.9190025329589844\n",
      "Step: 6761, Loss: 0.9159857630729675, Accuracy: 1.0, Computation time: 0.9094142913818359\n",
      "Step: 6762, Loss: 0.9159377813339233, Accuracy: 1.0, Computation time: 1.658869743347168\n",
      "Step: 6763, Loss: 0.9159281253814697, Accuracy: 1.0, Computation time: 0.9771955013275146\n",
      "Step: 6764, Loss: 0.9158581495285034, Accuracy: 1.0, Computation time: 0.7952916622161865\n",
      "Step: 6765, Loss: 0.9159401655197144, Accuracy: 1.0, Computation time: 0.8980989456176758\n",
      "Step: 6766, Loss: 0.937505304813385, Accuracy: 0.9833333492279053, Computation time: 0.9455032348632812\n",
      "Step: 6767, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 0.8241007328033447\n",
      "Step: 6768, Loss: 0.9162994027137756, Accuracy: 1.0, Computation time: 0.8595998287200928\n",
      "Step: 6769, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 1.0107097625732422\n",
      "Step: 6770, Loss: 0.916043221950531, Accuracy: 1.0, Computation time: 1.0396926403045654\n",
      "Step: 6771, Loss: 0.9158772826194763, Accuracy: 1.0, Computation time: 0.789421796798706\n",
      "Step: 6772, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 0.8430469036102295\n",
      "Step: 6773, Loss: 0.9158615469932556, Accuracy: 1.0, Computation time: 0.8747589588165283\n",
      "Step: 6774, Loss: 0.9160926342010498, Accuracy: 1.0, Computation time: 0.7738890647888184\n",
      "Step: 6775, Loss: 0.9158506989479065, Accuracy: 1.0, Computation time: 0.8411707878112793\n",
      "Step: 6776, Loss: 0.9158490300178528, Accuracy: 1.0, Computation time: 0.8601582050323486\n",
      "Step: 6777, Loss: 0.9158545136451721, Accuracy: 1.0, Computation time: 0.9797139167785645\n",
      "Step: 6778, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 0.8967289924621582\n",
      "Step: 6779, Loss: 0.9158555269241333, Accuracy: 1.0, Computation time: 0.8956849575042725\n",
      "Step: 6780, Loss: 0.9158559441566467, Accuracy: 1.0, Computation time: 0.7307431697845459\n",
      "Step: 6781, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 0.7696599960327148\n",
      "Step: 6782, Loss: 0.9158510565757751, Accuracy: 1.0, Computation time: 0.8340933322906494\n",
      "Step: 6783, Loss: 0.9158502221107483, Accuracy: 1.0, Computation time: 0.812490701675415\n",
      "Step: 6784, Loss: 0.9158397912979126, Accuracy: 1.0, Computation time: 0.7696712017059326\n",
      "Step: 6785, Loss: 0.9158416986465454, Accuracy: 1.0, Computation time: 0.7850728034973145\n",
      "Step: 6786, Loss: 0.9158662557601929, Accuracy: 1.0, Computation time: 0.7806205749511719\n",
      "Step: 6787, Loss: 0.9374857544898987, Accuracy: 0.96875, Computation time: 1.2840018272399902\n",
      "Step: 6788, Loss: 0.9158578515052795, Accuracy: 1.0, Computation time: 0.788393497467041\n",
      "Step: 6789, Loss: 0.9375500679016113, Accuracy: 0.9772727489471436, Computation time: 0.7331650257110596\n",
      "Step: 6790, Loss: 0.9292845726013184, Accuracy: 0.9642857313156128, Computation time: 0.7309355735778809\n",
      "Step: 6791, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 0.7165546417236328\n",
      "Step: 6792, Loss: 0.9158831834793091, Accuracy: 1.0, Computation time: 0.8316140174865723\n",
      "Step: 6793, Loss: 0.9159343242645264, Accuracy: 1.0, Computation time: 0.8189678192138672\n",
      "Step: 6794, Loss: 0.9165269136428833, Accuracy: 1.0, Computation time: 0.7801806926727295\n",
      "Step: 6795, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 0.7958745956420898\n",
      "Step: 6796, Loss: 0.9374642372131348, Accuracy: 0.96875, Computation time: 0.8540706634521484\n",
      "Step: 6797, Loss: 0.9158456325531006, Accuracy: 1.0, Computation time: 0.727724552154541\n",
      "Step: 6798, Loss: 0.9375116229057312, Accuracy: 0.9772727489471436, Computation time: 0.9623475074768066\n",
      "Step: 6799, Loss: 0.9158827066421509, Accuracy: 1.0, Computation time: 0.9069273471832275\n",
      "Step: 6800, Loss: 0.9162229895591736, Accuracy: 1.0, Computation time: 0.9540212154388428\n",
      "Step: 6801, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 0.7607336044311523\n",
      "Step: 6802, Loss: 0.9158611297607422, Accuracy: 1.0, Computation time: 0.7123322486877441\n",
      "Step: 6803, Loss: 0.9282441735267639, Accuracy: 0.9791666865348816, Computation time: 1.1954054832458496\n",
      "Step: 6804, Loss: 0.9375331401824951, Accuracy: 0.9807692766189575, Computation time: 0.852628231048584\n",
      "Step: 6805, Loss: 0.9158914089202881, Accuracy: 1.0, Computation time: 0.8243887424468994\n",
      "Step: 6806, Loss: 0.9248560667037964, Accuracy: 1.0, Computation time: 0.7631626129150391\n",
      "Step: 6807, Loss: 0.9159320592880249, Accuracy: 1.0, Computation time: 0.8110365867614746\n",
      "Step: 6808, Loss: 0.9159676432609558, Accuracy: 1.0, Computation time: 0.7458486557006836\n",
      "########################\n",
      "Test loss: 1.1291675567626953, Test Accuracy_epoch49: 0.6816799640655518\n",
      "########################\n",
      "Step: 6809, Loss: 0.9159930348396301, Accuracy: 1.0, Computation time: 0.8212587833404541\n",
      "Step: 6810, Loss: 0.915945291519165, Accuracy: 1.0, Computation time: 0.8709635734558105\n",
      "Step: 6811, Loss: 0.9162697196006775, Accuracy: 1.0, Computation time: 0.8464946746826172\n",
      "Step: 6812, Loss: 0.9551803469657898, Accuracy: 0.925000011920929, Computation time: 0.9474411010742188\n",
      "Step: 6813, Loss: 0.9184577465057373, Accuracy: 1.0, Computation time: 1.5157887935638428\n",
      "Step: 6814, Loss: 0.936610758304596, Accuracy: 0.9642857313156128, Computation time: 1.0462076663970947\n",
      "Step: 6815, Loss: 0.9332699775695801, Accuracy: 0.96875, Computation time: 1.2953343391418457\n",
      "Step: 6816, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 0.9702234268188477\n",
      "Step: 6817, Loss: 0.9160988330841064, Accuracy: 1.0, Computation time: 0.9138321876525879\n",
      "Step: 6818, Loss: 0.9159718155860901, Accuracy: 1.0, Computation time: 0.9506545066833496\n",
      "Step: 6819, Loss: 0.915977954864502, Accuracy: 1.0, Computation time: 0.9578433036804199\n",
      "Step: 6820, Loss: 0.9342613220214844, Accuracy: 0.9583333730697632, Computation time: 1.126354455947876\n",
      "Step: 6821, Loss: 0.9539539217948914, Accuracy: 0.9356061220169067, Computation time: 0.780811071395874\n",
      "Step: 6822, Loss: 0.9160089492797852, Accuracy: 1.0, Computation time: 1.3057429790496826\n",
      "Step: 6823, Loss: 0.9226699471473694, Accuracy: 1.0, Computation time: 0.802987813949585\n",
      "Step: 6824, Loss: 0.9340819120407104, Accuracy: 0.9772727489471436, Computation time: 0.9496052265167236\n",
      "Step: 6825, Loss: 0.9159505367279053, Accuracy: 1.0, Computation time: 1.0566487312316895\n",
      "Step: 6826, Loss: 0.9445898532867432, Accuracy: 0.96875, Computation time: 1.0779531002044678\n",
      "Step: 6827, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.503263235092163\n",
      "Step: 6828, Loss: 0.9160557389259338, Accuracy: 1.0, Computation time: 1.0191755294799805\n",
      "Step: 6829, Loss: 0.9159073829650879, Accuracy: 1.0, Computation time: 1.3121051788330078\n",
      "Step: 6830, Loss: 0.9375330805778503, Accuracy: 0.9791666865348816, Computation time: 0.838702917098999\n",
      "Step: 6831, Loss: 0.9219763278961182, Accuracy: 1.0, Computation time: 0.8532288074493408\n",
      "Step: 6832, Loss: 0.9167544841766357, Accuracy: 1.0, Computation time: 0.9262619018554688\n",
      "Step: 6833, Loss: 0.9375168085098267, Accuracy: 0.96875, Computation time: 0.8906888961791992\n",
      "Step: 6834, Loss: 0.9163718223571777, Accuracy: 1.0, Computation time: 0.8246455192565918\n",
      "Step: 6835, Loss: 0.9306894540786743, Accuracy: 0.949999988079071, Computation time: 0.8432369232177734\n",
      "Step: 6836, Loss: 0.9376059174537659, Accuracy: 0.9791666865348816, Computation time: 0.75203537940979\n",
      "Step: 6837, Loss: 0.9160184264183044, Accuracy: 1.0, Computation time: 1.2813749313354492\n",
      "Step: 6838, Loss: 0.9173935651779175, Accuracy: 1.0, Computation time: 0.7217991352081299\n",
      "Step: 6839, Loss: 0.9160467386245728, Accuracy: 1.0, Computation time: 0.7601139545440674\n",
      "Step: 6840, Loss: 0.9159948825836182, Accuracy: 1.0, Computation time: 0.8516952991485596\n",
      "Step: 6841, Loss: 0.9159616827964783, Accuracy: 1.0, Computation time: 0.9803752899169922\n",
      "Step: 6842, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 0.7579596042633057\n",
      "Step: 6843, Loss: 0.9158763885498047, Accuracy: 1.0, Computation time: 0.8681070804595947\n",
      "Step: 6844, Loss: 0.9163174629211426, Accuracy: 1.0, Computation time: 0.8762929439544678\n",
      "Step: 6845, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 0.9087774753570557\n",
      "Step: 6846, Loss: 0.915947437286377, Accuracy: 1.0, Computation time: 0.8303694725036621\n",
      "Step: 6847, Loss: 0.9159212112426758, Accuracy: 1.0, Computation time: 0.9031946659088135\n",
      "Step: 6848, Loss: 0.9159185290336609, Accuracy: 1.0, Computation time: 0.8632519245147705\n",
      "Step: 6849, Loss: 0.9160597920417786, Accuracy: 1.0, Computation time: 0.7902729511260986\n",
      "Step: 6850, Loss: 0.9159440398216248, Accuracy: 1.0, Computation time: 0.8819594383239746\n",
      "Step: 6851, Loss: 0.9376962780952454, Accuracy: 0.9642857313156128, Computation time: 0.764930248260498\n",
      "Step: 6852, Loss: 0.9158805012702942, Accuracy: 1.0, Computation time: 0.8357353210449219\n",
      "Step: 6853, Loss: 0.9160066246986389, Accuracy: 1.0, Computation time: 0.7702763080596924\n",
      "Step: 6854, Loss: 0.9165171980857849, Accuracy: 1.0, Computation time: 0.8312714099884033\n",
      "Step: 6855, Loss: 0.9159106612205505, Accuracy: 1.0, Computation time: 0.7774686813354492\n",
      "Step: 6856, Loss: 0.9158923029899597, Accuracy: 1.0, Computation time: 0.8519196510314941\n",
      "Step: 6857, Loss: 0.9158909916877747, Accuracy: 1.0, Computation time: 0.8852918148040771\n",
      "Step: 6858, Loss: 0.9181690812110901, Accuracy: 1.0, Computation time: 0.8727333545684814\n",
      "Step: 6859, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 0.7844817638397217\n",
      "Step: 6860, Loss: 0.9159272313117981, Accuracy: 1.0, Computation time: 0.8129961490631104\n",
      "Step: 6861, Loss: 0.9159261584281921, Accuracy: 1.0, Computation time: 0.8265020847320557\n",
      "Step: 6862, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 0.8295309543609619\n",
      "Step: 6863, Loss: 0.9160634279251099, Accuracy: 1.0, Computation time: 0.8208825588226318\n",
      "Step: 6864, Loss: 0.9160522222518921, Accuracy: 1.0, Computation time: 0.7866513729095459\n",
      "Step: 6865, Loss: 0.9338865280151367, Accuracy: 0.9642857313156128, Computation time: 0.824406623840332\n",
      "Step: 6866, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.1820554733276367\n",
      "Step: 6867, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 0.8690831661224365\n",
      "Step: 6868, Loss: 0.9159282445907593, Accuracy: 1.0, Computation time: 0.7570838928222656\n",
      "Step: 6869, Loss: 0.9165077209472656, Accuracy: 1.0, Computation time: 1.4932730197906494\n",
      "Step: 6870, Loss: 0.9159660935401917, Accuracy: 1.0, Computation time: 0.8799362182617188\n",
      "Step: 6871, Loss: 0.9160081744194031, Accuracy: 1.0, Computation time: 0.7389473915100098\n",
      "Step: 6872, Loss: 0.9159260988235474, Accuracy: 1.0, Computation time: 0.7941479682922363\n",
      "Step: 6873, Loss: 0.9159907102584839, Accuracy: 1.0, Computation time: 0.7966573238372803\n",
      "Step: 6874, Loss: 0.9159609079360962, Accuracy: 1.0, Computation time: 0.777080774307251\n",
      "Step: 6875, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 0.7997550964355469\n",
      "Step: 6876, Loss: 0.9373300075531006, Accuracy: 0.9807692766189575, Computation time: 0.7958981990814209\n",
      "Step: 6877, Loss: 0.9159486293792725, Accuracy: 1.0, Computation time: 0.8305723667144775\n",
      "Step: 6878, Loss: 0.9332426190376282, Accuracy: 0.9750000238418579, Computation time: 0.8162572383880615\n",
      "Step: 6879, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.0130069255828857\n",
      "Step: 6880, Loss: 0.9375304579734802, Accuracy: 0.96875, Computation time: 0.8581268787384033\n",
      "Step: 6881, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 0.8662326335906982\n",
      "Step: 6882, Loss: 0.9159143567085266, Accuracy: 1.0, Computation time: 0.7360150814056396\n",
      "Step: 6883, Loss: 0.9592937231063843, Accuracy: 0.9545454978942871, Computation time: 0.8912677764892578\n",
      "Step: 6884, Loss: 0.9160117506980896, Accuracy: 1.0, Computation time: 0.7068936824798584\n",
      "Step: 6885, Loss: 0.9158740639686584, Accuracy: 1.0, Computation time: 0.7692933082580566\n",
      "Step: 6886, Loss: 0.9308474659919739, Accuracy: 0.9750000238418579, Computation time: 0.7394909858703613\n",
      "Step: 6887, Loss: 0.9159277081489563, Accuracy: 1.0, Computation time: 0.9477994441986084\n",
      "Step: 6888, Loss: 0.9159296154975891, Accuracy: 1.0, Computation time: 0.8240058422088623\n",
      "Step: 6889, Loss: 0.9377853870391846, Accuracy: 0.9642857313156128, Computation time: 1.071892499923706\n",
      "Step: 6890, Loss: 0.9171044230461121, Accuracy: 1.0, Computation time: 0.8049116134643555\n",
      "Step: 6891, Loss: 0.9377210140228271, Accuracy: 0.96875, Computation time: 0.9610230922698975\n",
      "Step: 6892, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 0.8200268745422363\n",
      "Step: 6893, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 0.7574281692504883\n",
      "Step: 6894, Loss: 0.9377146363258362, Accuracy: 0.9750000238418579, Computation time: 0.7636435031890869\n",
      "Step: 6895, Loss: 0.9376590251922607, Accuracy: 0.9722222089767456, Computation time: 0.7977707386016846\n",
      "Step: 6896, Loss: 0.9371076226234436, Accuracy: 0.9821428656578064, Computation time: 0.8485398292541504\n",
      "Step: 6897, Loss: 0.9159645438194275, Accuracy: 1.0, Computation time: 0.7471139430999756\n",
      "Step: 6898, Loss: 0.9158713817596436, Accuracy: 1.0, Computation time: 0.8481540679931641\n",
      "Step: 6899, Loss: 0.9160550832748413, Accuracy: 1.0, Computation time: 0.9907486438751221\n",
      "Step: 6900, Loss: 0.915879487991333, Accuracy: 1.0, Computation time: 0.8293452262878418\n",
      "Step: 6901, Loss: 0.915877640247345, Accuracy: 1.0, Computation time: 0.8034322261810303\n",
      "Step: 6902, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 0.8391354084014893\n",
      "Step: 6903, Loss: 0.9159513115882874, Accuracy: 1.0, Computation time: 0.7469329833984375\n",
      "Step: 6904, Loss: 0.9158884882926941, Accuracy: 1.0, Computation time: 0.8203108310699463\n",
      "Step: 6905, Loss: 0.9307088851928711, Accuracy: 0.9821428656578064, Computation time: 0.859790563583374\n",
      "Step: 6906, Loss: 0.9178234934806824, Accuracy: 1.0, Computation time: 0.7249417304992676\n",
      "Step: 6907, Loss: 0.9169304370880127, Accuracy: 1.0, Computation time: 1.1162803173065186\n",
      "Step: 6908, Loss: 0.9158631563186646, Accuracy: 1.0, Computation time: 0.701514482498169\n",
      "Step: 6909, Loss: 0.9159048795700073, Accuracy: 1.0, Computation time: 0.7863805294036865\n",
      "Step: 6910, Loss: 0.9168522357940674, Accuracy: 1.0, Computation time: 1.4408438205718994\n",
      "Step: 6911, Loss: 0.9159055352210999, Accuracy: 1.0, Computation time: 0.6597557067871094\n",
      "Step: 6912, Loss: 0.9164752960205078, Accuracy: 1.0, Computation time: 1.1333553791046143\n",
      "Step: 6913, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 0.8854684829711914\n",
      "Step: 6914, Loss: 0.9161692261695862, Accuracy: 1.0, Computation time: 0.8381609916687012\n",
      "Step: 6915, Loss: 0.9158570766448975, Accuracy: 1.0, Computation time: 0.7259399890899658\n",
      "Step: 6916, Loss: 0.9278661012649536, Accuracy: 0.9750000238418579, Computation time: 1.0560097694396973\n",
      "Step: 6917, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 0.7411057949066162\n",
      "Step: 6918, Loss: 0.915938675403595, Accuracy: 1.0, Computation time: 0.7358582019805908\n",
      "Step: 6919, Loss: 0.9162744283676147, Accuracy: 1.0, Computation time: 0.9183981418609619\n",
      "Step: 6920, Loss: 0.9375565052032471, Accuracy: 0.9807692766189575, Computation time: 0.7886779308319092\n",
      "Step: 6921, Loss: 0.9159741997718811, Accuracy: 1.0, Computation time: 0.745222806930542\n",
      "Step: 6922, Loss: 0.9159268140792847, Accuracy: 1.0, Computation time: 0.7666079998016357\n",
      "Step: 6923, Loss: 0.9159097075462341, Accuracy: 1.0, Computation time: 0.8434154987335205\n",
      "Step: 6924, Loss: 0.9370778799057007, Accuracy: 0.9807692766189575, Computation time: 0.9015328884124756\n",
      "Step: 6925, Loss: 0.9160541892051697, Accuracy: 1.0, Computation time: 0.9344618320465088\n",
      "Step: 6926, Loss: 0.9158962965011597, Accuracy: 1.0, Computation time: 1.491161823272705\n",
      "Step: 6927, Loss: 0.9158771634101868, Accuracy: 1.0, Computation time: 1.0791029930114746\n",
      "Step: 6928, Loss: 0.9172783493995667, Accuracy: 1.0, Computation time: 1.0107018947601318\n",
      "Step: 6929, Loss: 0.9158856868743896, Accuracy: 1.0, Computation time: 0.7198395729064941\n",
      "Step: 6930, Loss: 0.9376267790794373, Accuracy: 0.9791666865348816, Computation time: 0.8850550651550293\n",
      "Step: 6931, Loss: 0.9158613681793213, Accuracy: 1.0, Computation time: 0.7660315036773682\n",
      "Step: 6932, Loss: 0.9158480167388916, Accuracy: 1.0, Computation time: 0.8980779647827148\n",
      "Step: 6933, Loss: 0.9158519506454468, Accuracy: 1.0, Computation time: 0.834364652633667\n",
      "Step: 6934, Loss: 0.9158743619918823, Accuracy: 1.0, Computation time: 0.782158613204956\n",
      "Step: 6935, Loss: 0.9158980250358582, Accuracy: 1.0, Computation time: 0.828331470489502\n",
      "Step: 6936, Loss: 0.9373953342437744, Accuracy: 0.9821428656578064, Computation time: 0.8888299465179443\n",
      "Step: 6937, Loss: 0.9375736117362976, Accuracy: 0.96875, Computation time: 0.8854000568389893\n",
      "Step: 6938, Loss: 0.9158586859703064, Accuracy: 1.0, Computation time: 0.8921515941619873\n",
      "Step: 6939, Loss: 0.9158466458320618, Accuracy: 1.0, Computation time: 0.8172314167022705\n",
      "Step: 6940, Loss: 0.9158488512039185, Accuracy: 1.0, Computation time: 1.0056507587432861\n",
      "Step: 6941, Loss: 0.9375131726264954, Accuracy: 0.96875, Computation time: 0.8441236019134521\n",
      "Step: 6942, Loss: 0.9158514738082886, Accuracy: 1.0, Computation time: 0.8743939399719238\n",
      "Step: 6943, Loss: 0.9158546328544617, Accuracy: 1.0, Computation time: 0.9515633583068848\n",
      "Step: 6944, Loss: 0.9158539772033691, Accuracy: 1.0, Computation time: 0.833366870880127\n",
      "Step: 6945, Loss: 0.915889322757721, Accuracy: 1.0, Computation time: 0.9447078704833984\n",
      "Step: 6946, Loss: 0.9159635305404663, Accuracy: 1.0, Computation time: 0.89662766456604\n",
      "Step: 6947, Loss: 0.9158434271812439, Accuracy: 1.0, Computation time: 0.925269603729248\n",
      "########################\n",
      "Test loss: 1.1287838220596313, Test Accuracy_epoch50: 0.6828340888023376\n",
      "########################\n",
      "Step: 6948, Loss: 0.9168275594711304, Accuracy: 1.0, Computation time: 1.2843017578125\n",
      "Step: 6949, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 0.8932695388793945\n",
      "Step: 6950, Loss: 0.915897011756897, Accuracy: 1.0, Computation time: 0.8856809139251709\n",
      "Step: 6951, Loss: 0.9158484935760498, Accuracy: 1.0, Computation time: 0.8609652519226074\n",
      "Step: 6952, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 0.8832659721374512\n",
      "Step: 6953, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 0.8222756385803223\n",
      "Step: 6954, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 0.8516433238983154\n",
      "Step: 6955, Loss: 0.9167525768280029, Accuracy: 1.0, Computation time: 0.9618794918060303\n",
      "Step: 6956, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.9860877990722656\n",
      "Step: 6957, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.7823543548583984\n",
      "Step: 6958, Loss: 0.9158536791801453, Accuracy: 1.0, Computation time: 1.3856737613677979\n",
      "Step: 6959, Loss: 0.9158582091331482, Accuracy: 1.0, Computation time: 0.8646979331970215\n",
      "Step: 6960, Loss: 0.9159177541732788, Accuracy: 1.0, Computation time: 1.0493407249450684\n",
      "Step: 6961, Loss: 0.9167556166648865, Accuracy: 1.0, Computation time: 1.4892940521240234\n",
      "Step: 6962, Loss: 0.9158393740653992, Accuracy: 1.0, Computation time: 0.9190812110900879\n",
      "Step: 6963, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 0.9666738510131836\n",
      "Step: 6964, Loss: 0.91584312915802, Accuracy: 1.0, Computation time: 1.0108726024627686\n",
      "Step: 6965, Loss: 0.9375224113464355, Accuracy: 0.9722222089767456, Computation time: 1.1369199752807617\n",
      "Step: 6966, Loss: 0.9158358573913574, Accuracy: 1.0, Computation time: 0.8725650310516357\n",
      "Step: 6967, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 1.0515198707580566\n",
      "Step: 6968, Loss: 0.915898323059082, Accuracy: 1.0, Computation time: 0.760509729385376\n",
      "Step: 6969, Loss: 0.9158811569213867, Accuracy: 1.0, Computation time: 1.3792555332183838\n",
      "Step: 6970, Loss: 0.9158464074134827, Accuracy: 1.0, Computation time: 0.9016048908233643\n",
      "Step: 6971, Loss: 0.9374197721481323, Accuracy: 0.9772727489471436, Computation time: 1.2555310726165771\n",
      "Step: 6972, Loss: 0.9158368110656738, Accuracy: 1.0, Computation time: 0.8771781921386719\n",
      "Step: 6973, Loss: 0.918069064617157, Accuracy: 1.0, Computation time: 1.565856695175171\n",
      "Step: 6974, Loss: 0.9158784747123718, Accuracy: 1.0, Computation time: 0.820946455001831\n",
      "Step: 6975, Loss: 0.9375413656234741, Accuracy: 0.984375, Computation time: 1.01743745803833\n",
      "Step: 6976, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 1.005683422088623\n",
      "Step: 6977, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.0680928230285645\n",
      "Step: 6978, Loss: 0.915867030620575, Accuracy: 1.0, Computation time: 0.8443756103515625\n",
      "Step: 6979, Loss: 0.9158504009246826, Accuracy: 1.0, Computation time: 0.76017165184021\n",
      "Step: 6980, Loss: 0.9159233570098877, Accuracy: 1.0, Computation time: 0.8299360275268555\n",
      "Step: 6981, Loss: 0.915837824344635, Accuracy: 1.0, Computation time: 0.9040465354919434\n",
      "Step: 6982, Loss: 0.9158351421356201, Accuracy: 1.0, Computation time: 0.9746837615966797\n",
      "Step: 6983, Loss: 0.9158347249031067, Accuracy: 1.0, Computation time: 0.935603141784668\n",
      "Step: 6984, Loss: 0.9158375263214111, Accuracy: 1.0, Computation time: 0.9710896015167236\n",
      "Step: 6985, Loss: 0.9158483147621155, Accuracy: 1.0, Computation time: 0.8931863307952881\n",
      "Step: 6986, Loss: 0.9375707507133484, Accuracy: 0.9807692766189575, Computation time: 0.925018310546875\n",
      "Step: 6987, Loss: 0.9158781170845032, Accuracy: 1.0, Computation time: 0.9108109474182129\n",
      "Step: 6988, Loss: 0.9158408045768738, Accuracy: 1.0, Computation time: 1.0400426387786865\n",
      "Step: 6989, Loss: 0.9374960064888, Accuracy: 0.9772727489471436, Computation time: 1.017002820968628\n",
      "Step: 6990, Loss: 0.9158352613449097, Accuracy: 1.0, Computation time: 0.8581852912902832\n",
      "Step: 6991, Loss: 0.915836751461029, Accuracy: 1.0, Computation time: 0.9106295108795166\n",
      "Step: 6992, Loss: 0.9158403873443604, Accuracy: 1.0, Computation time: 1.0910978317260742\n",
      "Step: 6993, Loss: 0.915858268737793, Accuracy: 1.0, Computation time: 0.9029455184936523\n",
      "Step: 6994, Loss: 0.9375430345535278, Accuracy: 0.96875, Computation time: 0.901658296585083\n",
      "Step: 6995, Loss: 0.9158926010131836, Accuracy: 1.0, Computation time: 0.8862318992614746\n",
      "Step: 6996, Loss: 0.9158804416656494, Accuracy: 1.0, Computation time: 0.8369429111480713\n",
      "Step: 6997, Loss: 0.9158461689949036, Accuracy: 1.0, Computation time: 0.9029262065887451\n",
      "Step: 6998, Loss: 0.9160188436508179, Accuracy: 1.0, Computation time: 0.9224643707275391\n",
      "Step: 6999, Loss: 0.9158860445022583, Accuracy: 1.0, Computation time: 0.8205604553222656\n",
      "Step: 7000, Loss: 0.9172698855400085, Accuracy: 1.0, Computation time: 0.9695127010345459\n",
      "Step: 7001, Loss: 0.9158409833908081, Accuracy: 1.0, Computation time: 0.8392775058746338\n",
      "Step: 7002, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 0.9776647090911865\n",
      "Step: 7003, Loss: 0.9158562421798706, Accuracy: 1.0, Computation time: 0.8600504398345947\n",
      "Step: 7004, Loss: 0.9159023761749268, Accuracy: 1.0, Computation time: 0.9737188816070557\n",
      "Step: 7005, Loss: 0.9158798456192017, Accuracy: 1.0, Computation time: 1.0377039909362793\n",
      "Step: 7006, Loss: 0.9158468842506409, Accuracy: 1.0, Computation time: 1.0902328491210938\n",
      "Step: 7007, Loss: 0.9158511161804199, Accuracy: 1.0, Computation time: 0.9624178409576416\n",
      "Step: 7008, Loss: 0.9158409237861633, Accuracy: 1.0, Computation time: 0.9398839473724365\n",
      "Step: 7009, Loss: 0.9158369302749634, Accuracy: 1.0, Computation time: 0.98172926902771\n",
      "Step: 7010, Loss: 0.9158328771591187, Accuracy: 1.0, Computation time: 0.7590618133544922\n",
      "Step: 7011, Loss: 0.916006326675415, Accuracy: 1.0, Computation time: 1.0319170951843262\n",
      "Step: 7012, Loss: 0.9158933758735657, Accuracy: 1.0, Computation time: 1.0268666744232178\n",
      "Step: 7013, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 1.0028998851776123\n",
      "Step: 7014, Loss: 0.9158399701118469, Accuracy: 1.0, Computation time: 0.9230141639709473\n",
      "Step: 7015, Loss: 0.9358211755752563, Accuracy: 0.9807692766189575, Computation time: 1.0663888454437256\n",
      "Step: 7016, Loss: 0.9158427119255066, Accuracy: 1.0, Computation time: 0.979562520980835\n",
      "Step: 7017, Loss: 0.9159293174743652, Accuracy: 1.0, Computation time: 0.9026765823364258\n",
      "Step: 7018, Loss: 0.9158644080162048, Accuracy: 1.0, Computation time: 1.2744014263153076\n",
      "Step: 7019, Loss: 0.9158573746681213, Accuracy: 1.0, Computation time: 1.0165705680847168\n",
      "Step: 7020, Loss: 0.9158494472503662, Accuracy: 1.0, Computation time: 0.8580377101898193\n",
      "Step: 7021, Loss: 0.915845513343811, Accuracy: 1.0, Computation time: 0.9387102127075195\n",
      "Step: 7022, Loss: 0.9158487915992737, Accuracy: 1.0, Computation time: 0.9234311580657959\n",
      "Step: 7023, Loss: 0.9158376455307007, Accuracy: 1.0, Computation time: 0.8142437934875488\n",
      "Step: 7024, Loss: 0.9160161018371582, Accuracy: 1.0, Computation time: 0.9016644954681396\n",
      "Step: 7025, Loss: 0.9158365726470947, Accuracy: 1.0, Computation time: 0.806999921798706\n",
      "Step: 7026, Loss: 0.9158402681350708, Accuracy: 1.0, Computation time: 0.9995882511138916\n",
      "Step: 7027, Loss: 0.9375112652778625, Accuracy: 0.9750000238418579, Computation time: 1.008772373199463\n",
      "Step: 7028, Loss: 0.9375814199447632, Accuracy: 0.949999988079071, Computation time: 0.9195387363433838\n",
      "Step: 7029, Loss: 0.9375205039978027, Accuracy: 0.96875, Computation time: 0.9958982467651367\n",
      "Step: 7030, Loss: 0.9375163316726685, Accuracy: 0.9750000238418579, Computation time: 1.2273917198181152\n",
      "Step: 7031, Loss: 0.9158493876457214, Accuracy: 1.0, Computation time: 0.9951860904693604\n",
      "Step: 7032, Loss: 0.9158470630645752, Accuracy: 1.0, Computation time: 0.9174773693084717\n",
      "Step: 7033, Loss: 0.9158533215522766, Accuracy: 1.0, Computation time: 1.018059253692627\n",
      "Step: 7034, Loss: 0.9158640503883362, Accuracy: 1.0, Computation time: 0.9893994331359863\n",
      "Step: 7035, Loss: 0.9158496260643005, Accuracy: 1.0, Computation time: 1.0539724826812744\n",
      "Step: 7036, Loss: 0.9158738851547241, Accuracy: 1.0, Computation time: 0.8482022285461426\n",
      "Step: 7037, Loss: 0.9158412218093872, Accuracy: 1.0, Computation time: 0.8993377685546875\n",
      "Step: 7038, Loss: 0.9158785939216614, Accuracy: 1.0, Computation time: 1.0925498008728027\n",
      "Step: 7039, Loss: 0.9158360958099365, Accuracy: 1.0, Computation time: 0.989485502243042\n",
      "Step: 7040, Loss: 0.9158710837364197, Accuracy: 1.0, Computation time: 1.0787630081176758\n",
      "Step: 7041, Loss: 0.9158385396003723, Accuracy: 1.0, Computation time: 1.0329234600067139\n",
      "Step: 7042, Loss: 0.9158634543418884, Accuracy: 1.0, Computation time: 1.1329772472381592\n",
      "Step: 7043, Loss: 0.9160674214363098, Accuracy: 1.0, Computation time: 0.9349737167358398\n",
      "Step: 7044, Loss: 0.9370401501655579, Accuracy: 0.9750000238418579, Computation time: 0.9067611694335938\n",
      "Step: 7045, Loss: 0.9375380277633667, Accuracy: 0.9772727489471436, Computation time: 0.9128103256225586\n",
      "Step: 7046, Loss: 0.9158350229263306, Accuracy: 1.0, Computation time: 1.1782383918762207\n",
      "Step: 7047, Loss: 0.9158360958099365, Accuracy: 1.0, Computation time: 1.114806890487671\n",
      "Step: 7048, Loss: 0.9158467054367065, Accuracy: 1.0, Computation time: 0.9566798210144043\n",
      "Step: 7049, Loss: 0.9158501029014587, Accuracy: 1.0, Computation time: 1.083975076675415\n",
      "Step: 7050, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 0.9989752769470215\n",
      "Step: 7051, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 1.021378517150879\n",
      "Step: 7052, Loss: 0.9364027976989746, Accuracy: 0.9821428656578064, Computation time: 1.0809743404388428\n",
      "Step: 7053, Loss: 0.9158551692962646, Accuracy: 1.0, Computation time: 0.9185419082641602\n",
      "Step: 7054, Loss: 0.9158451557159424, Accuracy: 1.0, Computation time: 0.883080244064331\n",
      "Step: 7055, Loss: 0.9160835146903992, Accuracy: 1.0, Computation time: 1.2548818588256836\n",
      "Step: 7056, Loss: 0.9158416390419006, Accuracy: 1.0, Computation time: 1.063981533050537\n",
      "Step: 7057, Loss: 0.9158426523208618, Accuracy: 1.0, Computation time: 0.9799282550811768\n",
      "Step: 7058, Loss: 0.9158431887626648, Accuracy: 1.0, Computation time: 0.9226284027099609\n",
      "Step: 7059, Loss: 0.9158486723899841, Accuracy: 1.0, Computation time: 0.9891386032104492\n",
      "Step: 7060, Loss: 0.9373663067817688, Accuracy: 0.9772727489471436, Computation time: 0.9738650321960449\n",
      "Step: 7061, Loss: 0.916276216506958, Accuracy: 1.0, Computation time: 0.9639081954956055\n",
      "Step: 7062, Loss: 0.937620997428894, Accuracy: 0.9583333730697632, Computation time: 0.9663341045379639\n",
      "Step: 7063, Loss: 0.9375846982002258, Accuracy: 0.9772727489471436, Computation time: 0.9418952465057373\n",
      "Step: 7064, Loss: 0.9159162044525146, Accuracy: 1.0, Computation time: 1.1061182022094727\n",
      "Step: 7065, Loss: 0.9158701300621033, Accuracy: 1.0, Computation time: 0.9619882106781006\n",
      "Step: 7066, Loss: 0.9158523678779602, Accuracy: 1.0, Computation time: 0.9657108783721924\n",
      "Step: 7067, Loss: 0.91585773229599, Accuracy: 1.0, Computation time: 0.8961453437805176\n",
      "Step: 7068, Loss: 0.9158393144607544, Accuracy: 1.0, Computation time: 0.8978021144866943\n",
      "Step: 7069, Loss: 0.9162057042121887, Accuracy: 1.0, Computation time: 0.9289798736572266\n",
      "Step: 7070, Loss: 0.9158415198326111, Accuracy: 1.0, Computation time: 0.9528136253356934\n",
      "Step: 7071, Loss: 0.9187532067298889, Accuracy: 1.0, Computation time: 1.001546859741211\n",
      "Step: 7072, Loss: 0.9159342646598816, Accuracy: 1.0, Computation time: 0.9745118618011475\n",
      "Step: 7073, Loss: 0.9375352263450623, Accuracy: 0.9583333730697632, Computation time: 0.8564553260803223\n",
      "Step: 7074, Loss: 0.9158927202224731, Accuracy: 1.0, Computation time: 0.8685920238494873\n",
      "Step: 7075, Loss: 0.9158981442451477, Accuracy: 1.0, Computation time: 1.1148591041564941\n",
      "Step: 7076, Loss: 0.9158859252929688, Accuracy: 1.0, Computation time: 0.8963911533355713\n",
      "Step: 7077, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 0.9022719860076904\n",
      "Step: 7078, Loss: 0.9158677458763123, Accuracy: 1.0, Computation time: 0.8577051162719727\n",
      "Step: 7079, Loss: 0.9158485531806946, Accuracy: 1.0, Computation time: 1.096952199935913\n",
      "Step: 7080, Loss: 0.9158443212509155, Accuracy: 1.0, Computation time: 0.9890525341033936\n",
      "Step: 7081, Loss: 0.929364025592804, Accuracy: 0.9722222089767456, Computation time: 0.8995511531829834\n",
      "Step: 7082, Loss: 0.915874719619751, Accuracy: 1.0, Computation time: 0.8950748443603516\n",
      "Step: 7083, Loss: 0.9165845513343811, Accuracy: 1.0, Computation time: 1.091374158859253\n",
      "Step: 7084, Loss: 0.9159465432167053, Accuracy: 1.0, Computation time: 0.8620755672454834\n",
      "Step: 7085, Loss: 0.9159424304962158, Accuracy: 1.0, Computation time: 0.8027009963989258\n",
      "Test loss: 1.1279191970825195, Test Accuracy: 0.6900232434272766\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2448c5b-a3d9-4d44-ae46-c29c9ddea58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
