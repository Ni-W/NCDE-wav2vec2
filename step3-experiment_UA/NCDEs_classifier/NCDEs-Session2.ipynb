{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f375956-d256-4d93-ada4-8956fd49f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax.scipy as jsp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import numpy\n",
    "from jax import jit\n",
    "\n",
    "matplotlib.rcParams.update({\"font.size\": 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008b51c6-0c0d-41b1-8388-c45abf88fad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030318f3-ba47-42e5-9a66-4600c1a8c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4508, 256, 768) (4508,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last3, wav2vec_last4, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last3,label_last4,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b246c2-6bf8-43b6-ac68-10fc4143ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    data_size: int\n",
    "    hidden_size: int\n",
    "    hidden_hidden_channels: int\n",
    "    num_hidden_layers: int\n",
    "    linear_in: eqx.nn.Linear\n",
    "    linear_a: eqx.nn.Linear\n",
    "    linear_b: eqx.nn.Linear\n",
    "    linear_c: eqx.nn.Linear\n",
    "    linear_out: eqx.nn.Linear\n",
    "    dropout: eqx.nn.Dropout\n",
    "    \n",
    "    def __init__(self, data_size, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, akey, bkey, ckey, okey = jrandom.split(key, 5)\n",
    "        self.data_size = data_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_hidden_channels = hidden_hidden_channels\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.linear_in = eqx.nn.Linear(hidden_size, hidden_hidden_channels, key=ikey)\n",
    "        self.linear_a = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=akey)\n",
    "        self.linear_b = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=bkey)\n",
    "        self.linear_c = eqx.nn.Linear(hidden_hidden_channels, hidden_hidden_channels, key=ckey)\n",
    "        self.linear_out = eqx.nn.Linear(hidden_hidden_channels, hidden_size * data_size, key=okey)\n",
    "        self.dropout = eqx.nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def __call__(self, t, y, training, args, subkey):\n",
    "        y = self.linear_in(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_a(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_b(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_c(y)\n",
    "        y = jnn.relu(y)\n",
    "        y = self.dropout(y, inference=not training, key=subkey)\n",
    "        y = self.linear_out(y).reshape(self.hidden_size, self.data_size)\n",
    "        y = jnn.tanh(y)  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccaf7bd4-a5ab-43d4-8baf-e30a70820c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来对每一列进行累加平均的操作\n",
    "def cumulative_average(arr):\n",
    "    cumulative_sum = jnp.cumsum(arr, axis=0)\n",
    "    divisor = jnp.arange(1, arr.shape[0] + 1).reshape((-1, 1))\n",
    "    return cumulative_sum / divisor\n",
    "\n",
    "# 将函数编译为JIT加速版本\n",
    "cumulative_average_jit = jit(cumulative_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4bbbaa-383b-4a4c-b893-b2735c281a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCDE(eqx.Module):\n",
    "    Conv: eqx.nn.Conv\n",
    "    initial: eqx.nn.MLP\n",
    "    func: Func\n",
    "    linear: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ikey, fkey, lkey, ckey = jrandom.split(key, 4)\n",
    "        self.Conv = eqx.nn.ConvTranspose(1, data_size, 5, 1, key=ckey)\n",
    "        self.initial = eqx.nn.MLP(5, hidden_size, width_size, depth, key=ikey)\n",
    "        self.func = Func(5, hidden_size, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=fkey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, 4, key=lkey)\n",
    "\n",
    "    def __call__(self, ts, coeffs, training, subkey, evolving_out=False):\n",
    "        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n",
    "        # parameterising a control path. These are used to produce a continuous-time\n",
    "        # input path `control`.\n",
    "\n",
    "        Lengh = len(coeffs)\n",
    "        coeffs_pad = []\n",
    "        for i in range(Lengh):\n",
    "            coeffs_last = coeffs[i].T\n",
    "            coeffs_right = self.Conv(coeffs_last)\n",
    "            coeffs_i = coeffs_right.T\n",
    "            yn_array = cumulative_average_jit(coeffs_i)\n",
    "            coeffs_pad.append(yn_array)\n",
    "\n",
    "        ##########\n",
    "        control = diffrax.CubicInterpolation(ts, coeffs_pad)\n",
    "        \n",
    "        term = diffrax.ControlTerm(lambda t, y, args: self.func(t, y, training, args, subkey), control).to_ode()\n",
    "        solver = diffrax.Tsit5()\n",
    "        dt0 = None\n",
    "        y0 = self.initial(control.evaluate(ts[0]))\n",
    "        if evolving_out:\n",
    "            saveat = diffrax.SaveAt(ts=ts)\n",
    "        else:\n",
    "            saveat = diffrax.SaveAt(t1=True)\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            term,\n",
    "            solver,\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=saveat,\n",
    "        )\n",
    "        if evolving_out:\n",
    "            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n",
    "        else:\n",
    "            (prediction,) = jax.vmap(lambda y:self.linear(solution.ys[-1]))(solution.ys)\n",
    "            pred_mean=prediction.mean(axis=0) \n",
    "            pred_var=prediction.var(axis=0)   \n",
    "            pred_normalized=(prediction-pred_mean)/jnp.sqrt(pred_var+1e-5)     \n",
    "            prediction_last = jnn.softmax(pred_normalized)\n",
    "        return prediction_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add396b7-d9eb-4ab5-84b9-72f76ef64524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec21a7e8-aa6b-4db5-87ff-6f1d5c4d2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(dataset_test_size, *, key):\n",
    "    ts = jnp.broadcast_to(jnp.linspace(0,255, 256), (dataset_test_size, 256))\n",
    "    ys = jnp.concatenate([ts[:, :, None], wav2vec_last2], axis=-1)\n",
    "    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n",
    "    labels = label_last2\n",
    "    _, _, data_size = ys.shape\n",
    "    return ts, coeffs, labels, data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7fc004-2940-4ff8-8117-8a8faf07d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e026b1c-24c9-4044-964b-7674867caa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "    @eqx.filter_jit\n",
    "    class CrossEntropyLoss():\n",
    "\n",
    "        def __init__(self, weight=None, size_average=True):\n",
    "\n",
    "            self.weight = weight\n",
    "            self.size_average = size_average\n",
    "\n",
    "\n",
    "        def __call__(self, input, target):\n",
    "\n",
    "            batch_loss = 0.\n",
    "            for i in range(input.shape[0]):\n",
    "\n",
    "                numerator = jnp.exp(input[i, target[i]])     # 分子\n",
    "                denominator = jnp.sum(jnp.exp(input[i, :]))   # 分母\n",
    "\n",
    "                # 计算单个损失\n",
    "                loss = -jnp.log(numerator / denominator)\n",
    "                if self.weight:\n",
    "                    loss = self.weight[target[i]] * loss\n",
    "            #    print(\"单个损失： \",loss)\n",
    "\n",
    "                # 损失累加\n",
    "                batch_loss += loss\n",
    "\n",
    "            # 整个 batch 的总损失是否要求平均\n",
    "            if self.size_average == True:\n",
    "                batch_loss /= input.shape[0]\n",
    "\n",
    "            return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "371dc421-8c2d-420d-8cde-b4fbd3b6509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=4508,\n",
    "    dataset_test_size=1023,\n",
    "    batch_size=32,\n",
    "    lr=0.001,\n",
    "    hidden_hidden_channels=40,\n",
    "    num_hidden_layers=4,\n",
    "    steps=2085,\n",
    "    hidden_size=220,\n",
    "    width_size=128,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "    dropout_rate=0.3,\n",
    "):\n",
    "    \n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    train_data_key, test_data_key, model_key, loader_key = jrandom.split(key, 4)\n",
    "\n",
    "    ts, coeffs, labels, data_size = get_data(\n",
    "        dataset_size, key=train_data_key\n",
    "    )\n",
    "\n",
    "    model = NeuralCDE(data_size, hidden_size, width_size, depth, hidden_hidden_channels, num_hidden_layers, dropout_rate, key=model_key)\n",
    "\n",
    "    # Training loop like normal.\n",
    "\n",
    "    import jax.numpy as jnp\n",
    "    from jax import jit\n",
    "    def calculate_confusion_matrix(true_labels, pred_labels, num_classes):\n",
    "        true_labels = true_labels.astype(jnp.int32)\n",
    "        pred_labels = pred_labels.astype(jnp.int32)\n",
    "        conf_matrix = jnp.zeros((num_classes, num_classes), dtype=jnp.int32)\n",
    "        for t, p in zip(true_labels, pred_labels):\n",
    "            conf_matrix = conf_matrix.at[t, p].add(1)\n",
    "        return conf_matrix\n",
    "\n",
    "    @jit\n",
    "    def calculate_ua(conf_matrix):\n",
    "        class_accuracy = jnp.diag(conf_matrix) / jnp.sum(conf_matrix, axis=1)\n",
    "        UA = jnp.mean(class_accuracy)\n",
    "        return UA\n",
    "        \n",
    "    @eqx.filter_jit\n",
    "    def accuracy(total_size, pred, label_i):\n",
    "        conf_matrix = calculate_confusion_matrix(label_i, pred, num_classes=4)\n",
    "        UA = calculate_ua(conf_matrix)\n",
    "        return UA\n",
    "\n",
    " \n",
    "    @eqx.filter_jit\n",
    "    def loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = True\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(batch_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def test_loss(model, ti, label_i, coeff_i, subkey):\n",
    "        training = False\n",
    "        pred = jax.vmap(model, in_axes=(0, 0, None, None))(ti, coeff_i, training, subkey)\n",
    "        criterion = CrossEntropyLoss()\n",
    "        bxe = criterion(pred, label_i)\n",
    "        #y_pred = jnp.array(pred)\n",
    "        y_pred = jnp.argmax(pred, axis=-1)\n",
    "        y_true = jnp.array(label_i)\n",
    "        acc = accuracy(dataset_test_size, y_pred, y_true)\n",
    "        return bxe, acc\n",
    "\n",
    "\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, data_i, opt_state, subkey):\n",
    "        ti, label_i, *coeff_i = data_i\n",
    "        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i, subkey)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return bxe, acc, model, opt_state\n",
    "\n",
    "    optim = optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, data_i in zip(\n",
    "        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        key, subkey = jax.random.split(key)\n",
    "        bxe, acc, model, opt_state = make_step(model, data_i, opt_state, subkey)\n",
    "        end = time.time()\n",
    "        print(\n",
    "            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n",
    "            f\"{end - start}\"\n",
    "        )\n",
    "        if step == 139:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch1: {acc_test}\")\n",
    "            print('########################')\n",
    "            \n",
    "        if step == 278:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch2: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 417:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch3: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 556:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch4: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 695:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch5: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 834:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch6: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 973:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch7: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1112:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch8: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1251:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch9: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1390:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch10: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1529:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch11: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1668:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch12: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1807:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch13: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 1946:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch14: {acc_test}\")\n",
    "            print('########################')\n",
    "\n",
    "        if step == 2085:\n",
    "            ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "            bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "            print('########################')\n",
    "            print(f\"Test loss: {bxe_test}, Test Accuracy_epoch15: {acc_test}\")\n",
    "            print('########################')\n",
    "        \n",
    "    ts_test, coeffs_test, labels_test, _ = get_test_data(dataset_test_size, key=test_data_key)\n",
    "    bxe_test, acc_test = test_loss(model, ts_test, labels_test, coeffs_test, test_data_key)\n",
    "    print(f\"Test loss: {bxe_test}, Test Accuracy: {acc_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007dfe7-e130-4610-bb10-0cf5112beda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.3976538181304932, Accuracy: 0.25, Computation time: 13.219569683074951\n",
      "Step: 1, Loss: 1.3686411380767822, Accuracy: 0.32499998807907104, Computation time: 2.2862584590911865\n",
      "Step: 2, Loss: 1.338808536529541, Accuracy: 0.25, Computation time: 2.1581106185913086\n",
      "Step: 3, Loss: 1.3404459953308105, Accuracy: 0.25, Computation time: 2.2966644763946533\n",
      "Step: 4, Loss: 1.341368317604065, Accuracy: 0.25, Computation time: 1.9324560165405273\n",
      "Step: 5, Loss: 1.346747636795044, Accuracy: 0.25, Computation time: 1.8706612586975098\n",
      "Step: 6, Loss: 1.3706458806991577, Accuracy: 0.25, Computation time: 1.868072748184204\n",
      "Step: 7, Loss: 1.2764383554458618, Accuracy: 0.25, Computation time: 1.8719744682312012\n",
      "Step: 8, Loss: 1.3226783275604248, Accuracy: 0.25, Computation time: 2.2161295413970947\n",
      "Step: 9, Loss: 1.3130853176116943, Accuracy: 0.5, Computation time: 2.377767562866211\n",
      "Step: 10, Loss: 1.2779836654663086, Accuracy: 0.2916666567325592, Computation time: 2.144798517227173\n",
      "Step: 11, Loss: 1.2849245071411133, Accuracy: 0.5, Computation time: 2.0365121364593506\n",
      "Step: 12, Loss: 1.2939016819000244, Accuracy: 0.5, Computation time: 2.044959545135498\n",
      "Step: 13, Loss: 1.2922143936157227, Accuracy: 0.5, Computation time: 2.106213331222534\n",
      "Step: 14, Loss: 1.2749768495559692, Accuracy: 0.4375, Computation time: 2.3956189155578613\n",
      "Step: 15, Loss: 1.1704297065734863, Accuracy: 0.30000001192092896, Computation time: 1.8335528373718262\n",
      "Step: 16, Loss: 1.2850964069366455, Accuracy: 0.5, Computation time: 2.6541638374328613\n",
      "Step: 17, Loss: 1.2266051769256592, Accuracy: 0.4583333432674408, Computation time: 1.9533374309539795\n",
      "Step: 18, Loss: 1.1159577369689941, Accuracy: 0.5, Computation time: 1.792734146118164\n",
      "Step: 19, Loss: 1.0970710515975952, Accuracy: 0.6944444179534912, Computation time: 1.8015196323394775\n",
      "Step: 20, Loss: 1.0576298236846924, Accuracy: 0.75, Computation time: 2.100449800491333\n",
      "Step: 21, Loss: 1.1271401643753052, Accuracy: 0.75, Computation time: 1.763098955154419\n",
      "Step: 22, Loss: 1.0273329019546509, Accuracy: 0.75, Computation time: 1.586099624633789\n",
      "Step: 23, Loss: 1.1657713651657104, Accuracy: 0.7250000238418579, Computation time: 1.7528178691864014\n",
      "Step: 24, Loss: 1.0361850261688232, Accuracy: 0.75, Computation time: 2.1658785343170166\n",
      "Step: 25, Loss: 1.1162070035934448, Accuracy: 0.75, Computation time: 1.825882911682129\n",
      "Step: 26, Loss: 1.0326673984527588, Accuracy: 0.75, Computation time: 2.152963876724243\n",
      "Step: 27, Loss: 1.0503681898117065, Accuracy: 0.75, Computation time: 1.4525394439697266\n",
      "Step: 28, Loss: 1.1111282110214233, Accuracy: 0.75, Computation time: 1.477588415145874\n",
      "Step: 29, Loss: 1.0783494710922241, Accuracy: 0.7291666865348816, Computation time: 2.030644655227661\n",
      "Step: 30, Loss: 1.061490774154663, Accuracy: 0.7321428656578064, Computation time: 2.2645263671875\n",
      "Step: 31, Loss: 1.1375144720077515, Accuracy: 0.7307692766189575, Computation time: 1.7307102680206299\n",
      "Step: 32, Loss: 1.0820215940475464, Accuracy: 0.75, Computation time: 1.7092881202697754\n",
      "Step: 33, Loss: 1.0443806648254395, Accuracy: 0.71875, Computation time: 1.773508071899414\n",
      "Step: 34, Loss: 1.0677754878997803, Accuracy: 0.75, Computation time: 1.6482956409454346\n",
      "Step: 35, Loss: 1.0446019172668457, Accuracy: 0.75, Computation time: 1.614616870880127\n",
      "Step: 36, Loss: 0.9444124102592468, Accuracy: 0.75, Computation time: 1.8223416805267334\n",
      "Step: 37, Loss: 1.0054028034210205, Accuracy: 0.75, Computation time: 1.741250991821289\n",
      "Step: 38, Loss: 1.183327317237854, Accuracy: 0.75, Computation time: 1.906977891921997\n",
      "Step: 39, Loss: 1.061063289642334, Accuracy: 0.75, Computation time: 1.7462453842163086\n",
      "Step: 40, Loss: 1.0369058847427368, Accuracy: 0.75, Computation time: 1.5410106182098389\n",
      "Step: 41, Loss: 1.134189248085022, Accuracy: 0.75, Computation time: 2.449389934539795\n",
      "Step: 42, Loss: 1.0389872789382935, Accuracy: 0.75, Computation time: 1.6592423915863037\n",
      "Step: 43, Loss: 1.0190844535827637, Accuracy: 0.7115384340286255, Computation time: 1.6841773986816406\n",
      "Step: 44, Loss: 1.0332602262496948, Accuracy: 0.75, Computation time: 1.6380085945129395\n",
      "Step: 45, Loss: 0.9867379069328308, Accuracy: 0.75, Computation time: 1.9990882873535156\n",
      "Step: 46, Loss: 1.0859839916229248, Accuracy: 0.7250000238418579, Computation time: 1.7555515766143799\n",
      "Step: 47, Loss: 1.0281610488891602, Accuracy: 0.75, Computation time: 1.554680585861206\n",
      "Step: 48, Loss: 1.024146318435669, Accuracy: 0.75, Computation time: 1.5105249881744385\n",
      "Step: 49, Loss: 1.0902793407440186, Accuracy: 0.75, Computation time: 1.5279693603515625\n",
      "Step: 50, Loss: 1.033207893371582, Accuracy: 0.8611111044883728, Computation time: 1.719750165939331\n",
      "Step: 51, Loss: 1.003834843635559, Accuracy: 0.875, Computation time: 1.48781156539917\n",
      "Step: 52, Loss: 0.9840880036354065, Accuracy: 1.0, Computation time: 1.614189863204956\n",
      "Step: 53, Loss: 0.996277391910553, Accuracy: 0.9821428656578064, Computation time: 1.593675136566162\n",
      "Step: 54, Loss: 0.9748125076293945, Accuracy: 1.0, Computation time: 1.6650118827819824\n",
      "Step: 55, Loss: 0.9674250483512878, Accuracy: 1.0, Computation time: 1.6064355373382568\n",
      "Step: 56, Loss: 0.9650385975837708, Accuracy: 1.0, Computation time: 1.6255717277526855\n",
      "Step: 57, Loss: 0.9743527173995972, Accuracy: 0.9791666865348816, Computation time: 1.694594383239746\n",
      "Step: 58, Loss: 0.946556568145752, Accuracy: 1.0, Computation time: 1.6066715717315674\n",
      "Step: 59, Loss: 0.9478883743286133, Accuracy: 1.0, Computation time: 1.5414507389068604\n",
      "Step: 60, Loss: 0.9405622482299805, Accuracy: 1.0, Computation time: 1.9051835536956787\n",
      "Step: 61, Loss: 0.926511287689209, Accuracy: 1.0, Computation time: 1.5235438346862793\n",
      "Step: 62, Loss: 0.9350439310073853, Accuracy: 1.0, Computation time: 1.3774526119232178\n",
      "Step: 63, Loss: 0.9404802322387695, Accuracy: 1.0, Computation time: 1.9173383712768555\n",
      "Step: 64, Loss: 0.927843451499939, Accuracy: 1.0, Computation time: 1.3601343631744385\n",
      "Step: 65, Loss: 0.9337604641914368, Accuracy: 1.0, Computation time: 1.8979954719543457\n",
      "Step: 66, Loss: 0.9551863074302673, Accuracy: 0.9375, Computation time: 1.6503479480743408\n",
      "Step: 67, Loss: 0.9487724304199219, Accuracy: 0.9791666865348816, Computation time: 1.4811983108520508\n",
      "Step: 68, Loss: 0.9284093976020813, Accuracy: 1.0, Computation time: 1.6223366260528564\n",
      "Step: 69, Loss: 0.9352156519889832, Accuracy: 1.0, Computation time: 1.7500159740447998\n",
      "Step: 70, Loss: 0.9378769397735596, Accuracy: 1.0, Computation time: 1.7722692489624023\n",
      "Step: 71, Loss: 0.9353131651878357, Accuracy: 1.0, Computation time: 1.7221462726593018\n",
      "Step: 72, Loss: 0.932285726070404, Accuracy: 1.0, Computation time: 1.7308015823364258\n",
      "Step: 73, Loss: 0.9398353695869446, Accuracy: 0.9722222089767456, Computation time: 1.9663290977478027\n",
      "Step: 74, Loss: 0.9332826733589172, Accuracy: 1.0, Computation time: 1.5554277896881104\n",
      "Step: 75, Loss: 0.9207631349563599, Accuracy: 1.0, Computation time: 2.2520482540130615\n",
      "Step: 76, Loss: 0.9295651316642761, Accuracy: 1.0, Computation time: 1.7203991413116455\n",
      "Step: 77, Loss: 0.9205991625785828, Accuracy: 1.0, Computation time: 1.4556007385253906\n",
      "Step: 78, Loss: 0.9444063901901245, Accuracy: 0.9642857313156128, Computation time: 1.5753648281097412\n",
      "Step: 79, Loss: 0.9237597584724426, Accuracy: 1.0, Computation time: 1.509598731994629\n",
      "Step: 80, Loss: 0.9386849403381348, Accuracy: 0.96875, Computation time: 2.1243937015533447\n",
      "Step: 81, Loss: 0.9395957589149475, Accuracy: 0.96875, Computation time: 1.2633423805236816\n",
      "Step: 82, Loss: 0.9298220276832581, Accuracy: 0.9791666865348816, Computation time: 1.6753630638122559\n",
      "Step: 83, Loss: 0.9202350378036499, Accuracy: 1.0, Computation time: 1.6985208988189697\n",
      "Step: 84, Loss: 0.9173898696899414, Accuracy: 1.0, Computation time: 1.5007576942443848\n",
      "Step: 85, Loss: 0.9175514578819275, Accuracy: 1.0, Computation time: 2.2305734157562256\n",
      "Step: 86, Loss: 0.9225797057151794, Accuracy: 1.0, Computation time: 2.065643548965454\n",
      "Step: 87, Loss: 0.9473829865455627, Accuracy: 0.9494949579238892, Computation time: 1.6074020862579346\n",
      "Step: 88, Loss: 0.9206875562667847, Accuracy: 1.0, Computation time: 1.3341262340545654\n",
      "Step: 89, Loss: 0.9193669557571411, Accuracy: 1.0, Computation time: 1.4551465511322021\n",
      "Step: 90, Loss: 0.9178059697151184, Accuracy: 1.0, Computation time: 1.5105714797973633\n",
      "Step: 91, Loss: 0.9179614782333374, Accuracy: 1.0, Computation time: 1.705528974533081\n",
      "Step: 92, Loss: 0.9381401538848877, Accuracy: 0.9722222089767456, Computation time: 1.4802980422973633\n",
      "Step: 93, Loss: 0.9439846873283386, Accuracy: 0.9375, Computation time: 1.920619010925293\n",
      "Step: 94, Loss: 0.9187014102935791, Accuracy: 1.0, Computation time: 1.2889714241027832\n",
      "Step: 95, Loss: 0.9174151420593262, Accuracy: 1.0, Computation time: 1.378798246383667\n",
      "Step: 96, Loss: 0.9207884073257446, Accuracy: 1.0, Computation time: 1.7031505107879639\n",
      "Step: 97, Loss: 0.9169034361839294, Accuracy: 1.0, Computation time: 1.7823948860168457\n",
      "Step: 98, Loss: 0.9274683594703674, Accuracy: 0.9807692766189575, Computation time: 1.617790937423706\n",
      "Step: 99, Loss: 0.9167509078979492, Accuracy: 1.0, Computation time: 1.6101086139678955\n",
      "Step: 100, Loss: 0.9176954627037048, Accuracy: 1.0, Computation time: 1.5080304145812988\n",
      "Step: 101, Loss: 0.9166678786277771, Accuracy: 1.0, Computation time: 1.500615119934082\n",
      "Step: 102, Loss: 0.9296737909317017, Accuracy: 0.9807692766189575, Computation time: 1.6068885326385498\n",
      "Step: 103, Loss: 0.9208970665931702, Accuracy: 1.0, Computation time: 1.5806727409362793\n",
      "Step: 104, Loss: 0.9163957834243774, Accuracy: 1.0, Computation time: 1.244483232498169\n",
      "Step: 105, Loss: 0.9383242726325989, Accuracy: 0.9722222089767456, Computation time: 1.4257359504699707\n",
      "Step: 106, Loss: 0.9188178777694702, Accuracy: 1.0, Computation time: 1.567300796508789\n",
      "Step: 107, Loss: 0.932521402835846, Accuracy: 0.9722222089767456, Computation time: 1.9142849445343018\n",
      "Step: 108, Loss: 0.9183171391487122, Accuracy: 1.0, Computation time: 1.4106214046478271\n",
      "Step: 109, Loss: 0.9218358993530273, Accuracy: 1.0, Computation time: 1.5638694763183594\n",
      "Step: 110, Loss: 0.919528603553772, Accuracy: 1.0, Computation time: 1.2309603691101074\n",
      "Step: 111, Loss: 0.9296130537986755, Accuracy: 0.9750000238418579, Computation time: 1.3932433128356934\n",
      "Step: 112, Loss: 0.9164140224456787, Accuracy: 1.0, Computation time: 1.2774608135223389\n",
      "Step: 113, Loss: 0.9165666103363037, Accuracy: 1.0, Computation time: 1.5184504985809326\n",
      "Step: 114, Loss: 0.916391134262085, Accuracy: 1.0, Computation time: 1.642477035522461\n",
      "Step: 115, Loss: 0.9227228760719299, Accuracy: 1.0, Computation time: 1.9967992305755615\n",
      "Step: 116, Loss: 0.9445098042488098, Accuracy: 0.9750000238418579, Computation time: 1.4890058040618896\n",
      "Step: 117, Loss: 0.9172990918159485, Accuracy: 1.0, Computation time: 1.4098763465881348\n",
      "Step: 118, Loss: 0.9350367784500122, Accuracy: 0.96875, Computation time: 1.7781836986541748\n",
      "Step: 119, Loss: 0.929250955581665, Accuracy: 1.0, Computation time: 1.468822956085205\n",
      "Step: 120, Loss: 0.9173524379730225, Accuracy: 1.0, Computation time: 1.3536062240600586\n",
      "Step: 121, Loss: 0.9287169575691223, Accuracy: 1.0, Computation time: 1.6525273323059082\n",
      "Step: 122, Loss: 0.929936408996582, Accuracy: 1.0, Computation time: 2.4711384773254395\n",
      "Step: 123, Loss: 0.9554046392440796, Accuracy: 0.9272727370262146, Computation time: 1.4004709720611572\n",
      "Step: 124, Loss: 0.917020857334137, Accuracy: 1.0, Computation time: 1.7484486103057861\n",
      "Step: 125, Loss: 0.9165754318237305, Accuracy: 1.0, Computation time: 1.6659367084503174\n",
      "Step: 126, Loss: 0.9223196506500244, Accuracy: 1.0, Computation time: 1.194394826889038\n",
      "Step: 127, Loss: 0.9274702072143555, Accuracy: 1.0, Computation time: 1.3938157558441162\n",
      "Step: 128, Loss: 0.9248926639556885, Accuracy: 1.0, Computation time: 1.3607585430145264\n",
      "Step: 129, Loss: 0.918822705745697, Accuracy: 1.0, Computation time: 1.9494378566741943\n",
      "Step: 130, Loss: 0.9288797378540039, Accuracy: 0.949999988079071, Computation time: 1.2269136905670166\n",
      "Step: 131, Loss: 0.9167594313621521, Accuracy: 1.0, Computation time: 1.3796312808990479\n",
      "Step: 132, Loss: 0.9314987063407898, Accuracy: 0.96875, Computation time: 2.3022077083587646\n",
      "Step: 133, Loss: 0.9204816818237305, Accuracy: 1.0, Computation time: 1.5443081855773926\n",
      "Step: 134, Loss: 0.9163905382156372, Accuracy: 1.0, Computation time: 1.407764196395874\n",
      "Step: 135, Loss: 0.9297911524772644, Accuracy: 0.9750000238418579, Computation time: 1.9626133441925049\n",
      "Step: 136, Loss: 0.920954704284668, Accuracy: 1.0, Computation time: 1.3995189666748047\n",
      "Step: 137, Loss: 0.9251111149787903, Accuracy: 1.0, Computation time: 1.4109680652618408\n",
      "Step: 138, Loss: 0.9186187386512756, Accuracy: 1.0, Computation time: 1.9865217208862305\n",
      "Step: 139, Loss: 0.9186230897903442, Accuracy: 1.0, Computation time: 1.3619554042816162\n",
      "########################\n",
      "Test loss: 1.0650928020477295, Test Accuracy_epoch1: 0.7890361547470093\n",
      "########################\n",
      "Step: 140, Loss: 0.9250161051750183, Accuracy: 1.0, Computation time: 1.7495083808898926\n",
      "Step: 141, Loss: 0.9205998182296753, Accuracy: 1.0, Computation time: 1.185227870941162\n",
      "Step: 142, Loss: 0.9303646087646484, Accuracy: 0.9750000238418579, Computation time: 1.8595192432403564\n",
      "Step: 143, Loss: 0.9185237288475037, Accuracy: 1.0, Computation time: 1.3010482788085938\n",
      "Step: 144, Loss: 0.9169973731040955, Accuracy: 1.0, Computation time: 1.3720905780792236\n",
      "Step: 145, Loss: 0.9165300130844116, Accuracy: 1.0, Computation time: 1.5032286643981934\n",
      "Step: 146, Loss: 0.9264053702354431, Accuracy: 1.0, Computation time: 1.4724853038787842\n",
      "Step: 147, Loss: 0.9314229488372803, Accuracy: 0.9821428656578064, Computation time: 1.4541773796081543\n",
      "Step: 148, Loss: 0.9186429977416992, Accuracy: 1.0, Computation time: 1.5328128337860107\n",
      "Step: 149, Loss: 0.916370153427124, Accuracy: 1.0, Computation time: 1.3350727558135986\n",
      "Step: 150, Loss: 0.9197795391082764, Accuracy: 1.0, Computation time: 1.8470308780670166\n",
      "Step: 151, Loss: 0.9161515831947327, Accuracy: 1.0, Computation time: 1.235705852508545\n",
      "Step: 152, Loss: 0.937827467918396, Accuracy: 0.9722222089767456, Computation time: 1.2541306018829346\n",
      "Step: 153, Loss: 0.9298994541168213, Accuracy: 0.9772727489471436, Computation time: 1.5648672580718994\n",
      "Step: 154, Loss: 0.9164791107177734, Accuracy: 1.0, Computation time: 1.348926067352295\n",
      "Step: 155, Loss: 0.9392357468605042, Accuracy: 0.9750000238418579, Computation time: 1.315643548965454\n",
      "Step: 156, Loss: 0.9363786578178406, Accuracy: 0.9722222089767456, Computation time: 1.5405011177062988\n",
      "Step: 157, Loss: 0.9193573594093323, Accuracy: 1.0, Computation time: 1.4804835319519043\n",
      "Step: 158, Loss: 0.9299767017364502, Accuracy: 0.9722222089767456, Computation time: 1.8353888988494873\n",
      "Step: 159, Loss: 0.9193484783172607, Accuracy: 1.0, Computation time: 1.6527795791625977\n",
      "Step: 160, Loss: 0.9165205359458923, Accuracy: 1.0, Computation time: 1.1364943981170654\n",
      "Step: 161, Loss: 0.916860818862915, Accuracy: 1.0, Computation time: 1.565486192703247\n",
      "Step: 162, Loss: 0.9167636036872864, Accuracy: 1.0, Computation time: 1.2759149074554443\n",
      "Step: 163, Loss: 0.9186262488365173, Accuracy: 1.0, Computation time: 1.4181592464447021\n",
      "Step: 164, Loss: 0.918255627155304, Accuracy: 1.0, Computation time: 1.4183752536773682\n",
      "Step: 165, Loss: 0.9164591431617737, Accuracy: 1.0, Computation time: 1.4217360019683838\n",
      "Step: 166, Loss: 0.9161450266838074, Accuracy: 1.0, Computation time: 1.3514912128448486\n",
      "Step: 167, Loss: 0.9378148913383484, Accuracy: 0.9722222089767456, Computation time: 1.5495176315307617\n",
      "Step: 168, Loss: 0.9162523746490479, Accuracy: 1.0, Computation time: 1.5779955387115479\n",
      "Step: 169, Loss: 0.9281388521194458, Accuracy: 1.0, Computation time: 1.582646131515503\n",
      "Step: 170, Loss: 0.9340093731880188, Accuracy: 0.9722222089767456, Computation time: 1.5832383632659912\n",
      "Step: 171, Loss: 0.9161192774772644, Accuracy: 1.0, Computation time: 1.1992080211639404\n",
      "Step: 172, Loss: 0.9189774394035339, Accuracy: 1.0, Computation time: 1.5827925205230713\n",
      "Step: 173, Loss: 0.9170679450035095, Accuracy: 1.0, Computation time: 1.3090879917144775\n",
      "Step: 174, Loss: 0.9162076115608215, Accuracy: 1.0, Computation time: 1.4844319820404053\n",
      "Step: 175, Loss: 0.939570963382721, Accuracy: 0.9833333492279053, Computation time: 1.5770230293273926\n",
      "Step: 176, Loss: 0.919481098651886, Accuracy: 1.0, Computation time: 1.438340425491333\n",
      "Step: 177, Loss: 0.9167068004608154, Accuracy: 1.0, Computation time: 1.5206775665283203\n",
      "Step: 178, Loss: 0.9508031010627747, Accuracy: 0.96875, Computation time: 1.5187287330627441\n",
      "Step: 179, Loss: 0.9389806389808655, Accuracy: 0.9722222089767456, Computation time: 1.6101047992706299\n",
      "Step: 180, Loss: 0.9162832498550415, Accuracy: 1.0, Computation time: 1.5136754512786865\n",
      "Step: 181, Loss: 0.9181719422340393, Accuracy: 1.0, Computation time: 1.3750410079956055\n",
      "Step: 182, Loss: 0.9170713424682617, Accuracy: 1.0, Computation time: 1.6680123805999756\n",
      "Step: 183, Loss: 0.9202030897140503, Accuracy: 1.0, Computation time: 1.5016765594482422\n",
      "Step: 184, Loss: 0.9231284260749817, Accuracy: 1.0, Computation time: 1.2977850437164307\n",
      "Step: 185, Loss: 0.9218172430992126, Accuracy: nan, Computation time: 2.535372018814087\n",
      "Step: 186, Loss: 0.9164735078811646, Accuracy: 1.0, Computation time: 1.4235477447509766\n",
      "Step: 187, Loss: 0.9162718057632446, Accuracy: 1.0, Computation time: 1.2917237281799316\n",
      "Step: 188, Loss: 0.9188668131828308, Accuracy: 1.0, Computation time: 1.4366588592529297\n",
      "Step: 189, Loss: 0.9167960286140442, Accuracy: 1.0, Computation time: 1.291757583618164\n",
      "Step: 190, Loss: 0.9164897203445435, Accuracy: 1.0, Computation time: 1.4146697521209717\n",
      "Step: 191, Loss: 0.9173458814620972, Accuracy: 1.0, Computation time: 1.4616689682006836\n",
      "Step: 192, Loss: 0.9168559312820435, Accuracy: 1.0, Computation time: 1.3344371318817139\n",
      "Step: 193, Loss: 0.9164653420448303, Accuracy: 1.0, Computation time: 1.5963079929351807\n",
      "Step: 194, Loss: 0.9435499310493469, Accuracy: 0.9375, Computation time: 1.5728046894073486\n",
      "Step: 195, Loss: 0.9170331954956055, Accuracy: 1.0, Computation time: 1.435417890548706\n",
      "Step: 196, Loss: 0.9252866506576538, Accuracy: 1.0, Computation time: 1.6601266860961914\n",
      "Step: 197, Loss: 0.9161187410354614, Accuracy: 1.0, Computation time: 1.3471081256866455\n",
      "Step: 198, Loss: 0.9171409010887146, Accuracy: 1.0, Computation time: 1.3937015533447266\n",
      "Step: 199, Loss: 0.9692269563674927, Accuracy: 0.918749988079071, Computation time: 1.7539362907409668\n",
      "Step: 200, Loss: 0.9315422177314758, Accuracy: 0.96875, Computation time: 1.373211145401001\n",
      "Step: 201, Loss: 0.9164530634880066, Accuracy: 1.0, Computation time: 1.327800989151001\n",
      "Step: 202, Loss: 0.9310073256492615, Accuracy: 0.9722222089767456, Computation time: 1.4745714664459229\n",
      "Step: 203, Loss: 0.9189417958259583, Accuracy: 1.0, Computation time: 1.7074000835418701\n",
      "Step: 204, Loss: 0.9196847081184387, Accuracy: 1.0, Computation time: 1.5396201610565186\n",
      "Step: 205, Loss: 0.9190696477890015, Accuracy: 1.0, Computation time: 1.5026068687438965\n",
      "Step: 206, Loss: 0.9519319534301758, Accuracy: 0.954365074634552, Computation time: 1.6966209411621094\n",
      "Step: 207, Loss: 0.9373825192451477, Accuracy: 0.9772727489471436, Computation time: 1.6747891902923584\n",
      "Step: 208, Loss: 0.9176537394523621, Accuracy: 1.0, Computation time: 2.156529664993286\n",
      "Step: 209, Loss: 0.9163010120391846, Accuracy: 1.0, Computation time: 1.2312736511230469\n",
      "Step: 210, Loss: 0.9168654680252075, Accuracy: 1.0, Computation time: 1.5459520816802979\n",
      "Step: 211, Loss: 0.9170145392417908, Accuracy: 1.0, Computation time: 1.221677541732788\n",
      "Step: 212, Loss: 0.9164828658103943, Accuracy: 1.0, Computation time: 1.3263897895812988\n",
      "Step: 213, Loss: 0.9163962602615356, Accuracy: 1.0, Computation time: 1.4503748416900635\n",
      "Step: 214, Loss: 0.9173800945281982, Accuracy: 1.0, Computation time: 1.4837193489074707\n",
      "Step: 215, Loss: 0.9177637696266174, Accuracy: 1.0, Computation time: 1.3259222507476807\n",
      "Step: 216, Loss: 0.9162517189979553, Accuracy: 1.0, Computation time: 1.5052714347839355\n",
      "Step: 217, Loss: 0.9326356053352356, Accuracy: 0.9750000238418579, Computation time: 1.2716810703277588\n",
      "Step: 218, Loss: 0.9248998761177063, Accuracy: 1.0, Computation time: 1.4145729541778564\n",
      "Step: 219, Loss: 0.9253835678100586, Accuracy: 1.0, Computation time: 1.6667349338531494\n",
      "Step: 220, Loss: 0.9243122339248657, Accuracy: 1.0, Computation time: 1.6968834400177002\n",
      "Step: 221, Loss: 0.9165722131729126, Accuracy: 1.0, Computation time: 1.6165742874145508\n",
      "Step: 222, Loss: 0.9170405268669128, Accuracy: 1.0, Computation time: 1.7724039554595947\n",
      "Step: 223, Loss: 0.9387747049331665, Accuracy: 0.9750000238418579, Computation time: 1.407705545425415\n",
      "Step: 224, Loss: 0.9170142412185669, Accuracy: 1.0, Computation time: 1.3806421756744385\n",
      "Step: 225, Loss: 0.916386067867279, Accuracy: 1.0, Computation time: 1.252305507659912\n",
      "Step: 226, Loss: 0.9185758829116821, Accuracy: 1.0, Computation time: 1.435309648513794\n",
      "Step: 227, Loss: 0.9172492623329163, Accuracy: 1.0, Computation time: 1.3664839267730713\n",
      "Step: 228, Loss: 0.9163131713867188, Accuracy: 1.0, Computation time: 1.3384253978729248\n",
      "Step: 229, Loss: 0.9183879494667053, Accuracy: 1.0, Computation time: 1.401170015335083\n",
      "Step: 230, Loss: 0.9313490390777588, Accuracy: 0.9750000238418579, Computation time: 1.3817176818847656\n",
      "Step: 231, Loss: 0.9345956444740295, Accuracy: 0.96875, Computation time: 1.5941779613494873\n",
      "Step: 232, Loss: 0.9175867438316345, Accuracy: 1.0, Computation time: 1.3297522068023682\n",
      "Step: 233, Loss: 0.9163292050361633, Accuracy: 1.0, Computation time: 1.4294044971466064\n",
      "Step: 234, Loss: 0.9440960884094238, Accuracy: 0.8999999761581421, Computation time: 1.525742769241333\n",
      "Step: 235, Loss: 0.9171572923660278, Accuracy: 1.0, Computation time: 1.5149343013763428\n",
      "Step: 236, Loss: 0.9189802408218384, Accuracy: 1.0, Computation time: 1.6770901679992676\n",
      "Step: 237, Loss: 0.9166771769523621, Accuracy: 1.0, Computation time: 1.3334259986877441\n",
      "Step: 238, Loss: 0.9165428280830383, Accuracy: 1.0, Computation time: 1.3520028591156006\n",
      "Step: 239, Loss: 0.9167224168777466, Accuracy: 1.0, Computation time: 1.5195567607879639\n",
      "Step: 240, Loss: 0.9477686882019043, Accuracy: 0.9583333730697632, Computation time: 2.069986343383789\n",
      "Step: 241, Loss: 0.9431212544441223, Accuracy: 0.9807692766189575, Computation time: 1.6415531635284424\n",
      "Step: 242, Loss: 0.9311797618865967, Accuracy: 0.9722222089767456, Computation time: 2.888313055038452\n",
      "Step: 243, Loss: 0.9168204069137573, Accuracy: 1.0, Computation time: 1.6216423511505127\n",
      "Step: 244, Loss: 0.9161046743392944, Accuracy: 1.0, Computation time: 1.3178107738494873\n",
      "Step: 245, Loss: 0.9284621477127075, Accuracy: 0.96875, Computation time: 1.263458013534546\n",
      "Step: 246, Loss: 0.9162305593490601, Accuracy: 1.0, Computation time: 1.4241502285003662\n",
      "Step: 247, Loss: 0.9165990352630615, Accuracy: 1.0, Computation time: 1.6443455219268799\n",
      "Step: 248, Loss: 0.9166445732116699, Accuracy: 1.0, Computation time: 1.6401002407073975\n",
      "Step: 249, Loss: 0.9161012768745422, Accuracy: 1.0, Computation time: 1.5103249549865723\n",
      "Step: 250, Loss: 0.9168327450752258, Accuracy: 1.0, Computation time: 1.6294353008270264\n",
      "Step: 251, Loss: 0.9176539778709412, Accuracy: 1.0, Computation time: 1.4053285121917725\n",
      "Step: 252, Loss: 0.9184010028839111, Accuracy: 1.0, Computation time: 1.660806655883789\n",
      "Step: 253, Loss: 0.9161321520805359, Accuracy: 1.0, Computation time: 1.1763696670532227\n",
      "Step: 254, Loss: 0.9379128813743591, Accuracy: 0.9833333492279053, Computation time: 1.3996589183807373\n",
      "Step: 255, Loss: 0.916252851486206, Accuracy: 1.0, Computation time: 1.3755836486816406\n",
      "Step: 256, Loss: 0.916415810585022, Accuracy: 1.0, Computation time: 1.415428876876831\n",
      "Step: 257, Loss: 0.9161560535430908, Accuracy: 1.0, Computation time: 1.5613200664520264\n",
      "Step: 258, Loss: 0.9190645813941956, Accuracy: 1.0, Computation time: 1.8574299812316895\n",
      "Step: 259, Loss: 0.9386337995529175, Accuracy: 0.96875, Computation time: 1.5895066261291504\n",
      "Step: 260, Loss: 0.9171332716941833, Accuracy: 1.0, Computation time: 1.2793738842010498\n",
      "Step: 261, Loss: 0.9160885214805603, Accuracy: 1.0, Computation time: 1.392397165298462\n",
      "Step: 262, Loss: 0.9178498983383179, Accuracy: 1.0, Computation time: 1.8115999698638916\n",
      "Step: 263, Loss: 0.9392151236534119, Accuracy: 0.9791666865348816, Computation time: 1.572577953338623\n",
      "Step: 264, Loss: 0.919267475605011, Accuracy: 1.0, Computation time: 1.425360918045044\n",
      "Step: 265, Loss: 0.9160808324813843, Accuracy: 1.0, Computation time: 1.5421497821807861\n",
      "Step: 266, Loss: 0.9161149263381958, Accuracy: 1.0, Computation time: 1.3089983463287354\n",
      "Step: 267, Loss: 0.9161463379859924, Accuracy: 1.0, Computation time: 1.5210041999816895\n",
      "Step: 268, Loss: 0.9161375164985657, Accuracy: 1.0, Computation time: 1.5716137886047363\n",
      "Step: 269, Loss: 0.9160658717155457, Accuracy: 1.0, Computation time: 1.51192307472229\n",
      "Step: 270, Loss: 0.9160857796669006, Accuracy: 1.0, Computation time: 1.3485972881317139\n",
      "Step: 271, Loss: 0.9185581207275391, Accuracy: 1.0, Computation time: 1.2806463241577148\n",
      "Step: 272, Loss: 0.9178617000579834, Accuracy: 1.0, Computation time: 1.421673059463501\n",
      "Step: 273, Loss: 0.9202247858047485, Accuracy: 1.0, Computation time: 1.5283114910125732\n",
      "Step: 274, Loss: 0.9163589477539062, Accuracy: 1.0, Computation time: 1.3572945594787598\n",
      "Step: 275, Loss: 0.9160519242286682, Accuracy: 1.0, Computation time: 1.3086974620819092\n",
      "Step: 276, Loss: 0.9161739945411682, Accuracy: 1.0, Computation time: 1.4228174686431885\n",
      "Step: 277, Loss: 0.9160603880882263, Accuracy: 1.0, Computation time: 1.6180572509765625\n",
      "Step: 278, Loss: 0.9202343821525574, Accuracy: 1.0, Computation time: 1.4473466873168945\n",
      "########################\n",
      "Test loss: 1.0651915073394775, Test Accuracy_epoch2: 0.7910184264183044\n",
      "########################\n",
      "Step: 279, Loss: 0.9350919127464294, Accuracy: 0.9722222089767456, Computation time: 1.3367464542388916\n",
      "Step: 280, Loss: 0.9217971563339233, Accuracy: 1.0, Computation time: 1.5967152118682861\n",
      "Step: 281, Loss: 0.9161345958709717, Accuracy: 1.0, Computation time: 1.3771488666534424\n",
      "Step: 282, Loss: 0.9189915657043457, Accuracy: 1.0, Computation time: 1.3266451358795166\n",
      "Step: 283, Loss: 0.9173673391342163, Accuracy: 1.0, Computation time: 1.2845685482025146\n",
      "Step: 284, Loss: 0.9279003143310547, Accuracy: 0.9642857313156128, Computation time: 1.4732694625854492\n",
      "Step: 285, Loss: 0.9165428876876831, Accuracy: 1.0, Computation time: 1.6161668300628662\n",
      "Step: 286, Loss: 0.9209222793579102, Accuracy: 1.0, Computation time: 1.5514471530914307\n",
      "Step: 287, Loss: 0.9168754816055298, Accuracy: 1.0, Computation time: 1.5462536811828613\n",
      "Step: 288, Loss: 0.9320549368858337, Accuracy: 0.9722222089767456, Computation time: 1.810621976852417\n",
      "Step: 289, Loss: 0.9161566495895386, Accuracy: 1.0, Computation time: 1.5462288856506348\n",
      "Step: 290, Loss: 0.9163143038749695, Accuracy: 1.0, Computation time: 1.6459879875183105\n",
      "Step: 291, Loss: 0.9267663359642029, Accuracy: 0.96875, Computation time: 1.2818458080291748\n",
      "Step: 292, Loss: 0.9163401126861572, Accuracy: 1.0, Computation time: 1.319261074066162\n",
      "Step: 293, Loss: 0.9161221385002136, Accuracy: 1.0, Computation time: 1.3434195518493652\n",
      "Step: 294, Loss: 0.9192742109298706, Accuracy: 1.0, Computation time: 1.4220311641693115\n",
      "Step: 295, Loss: 0.9163255095481873, Accuracy: 1.0, Computation time: 1.3012135028839111\n",
      "Step: 296, Loss: 0.9160960912704468, Accuracy: 1.0, Computation time: 1.3821396827697754\n",
      "Step: 297, Loss: 0.9163828492164612, Accuracy: 1.0, Computation time: 1.2464728355407715\n",
      "Step: 298, Loss: 0.9322628974914551, Accuracy: 0.9821428656578064, Computation time: 2.003816604614258\n",
      "Step: 299, Loss: 0.9164015054702759, Accuracy: 1.0, Computation time: 1.6352512836456299\n",
      "Step: 300, Loss: 0.9177235960960388, Accuracy: 1.0, Computation time: 1.5738720893859863\n",
      "Step: 301, Loss: 0.9377854466438293, Accuracy: 0.96875, Computation time: 1.2883517742156982\n",
      "Step: 302, Loss: 0.9194685816764832, Accuracy: 1.0, Computation time: 1.4074625968933105\n",
      "Step: 303, Loss: 0.9168871641159058, Accuracy: 1.0, Computation time: 1.4307117462158203\n",
      "Step: 304, Loss: 0.916662335395813, Accuracy: 1.0, Computation time: 1.5449702739715576\n",
      "Step: 305, Loss: 0.9169142246246338, Accuracy: 1.0, Computation time: 1.5415091514587402\n",
      "Step: 306, Loss: 0.9162974953651428, Accuracy: 1.0, Computation time: 1.2840995788574219\n",
      "Step: 307, Loss: 0.9162822365760803, Accuracy: 1.0, Computation time: 1.5437848567962646\n",
      "Step: 308, Loss: 0.9284746050834656, Accuracy: 0.9583333730697632, Computation time: 1.3768391609191895\n",
      "Step: 309, Loss: 0.9243298172950745, Accuracy: 1.0, Computation time: 1.4250686168670654\n",
      "Step: 310, Loss: 0.9164648652076721, Accuracy: 1.0, Computation time: 1.42030668258667\n",
      "Step: 311, Loss: 0.9187393188476562, Accuracy: 1.0, Computation time: 1.3841264247894287\n",
      "Step: 312, Loss: 0.9173434376716614, Accuracy: 1.0, Computation time: 1.3125319480895996\n",
      "Step: 313, Loss: 0.9383370876312256, Accuracy: 0.9821428656578064, Computation time: 1.509019136428833\n",
      "Step: 314, Loss: 0.9337664246559143, Accuracy: 0.9833333492279053, Computation time: 1.3992185592651367\n",
      "Step: 315, Loss: 0.916235625743866, Accuracy: 1.0, Computation time: 1.425482988357544\n",
      "Step: 316, Loss: 0.9336564540863037, Accuracy: 0.9750000238418579, Computation time: 1.7305922508239746\n",
      "Step: 317, Loss: 0.9342846274375916, Accuracy: 0.96875, Computation time: 1.4339041709899902\n",
      "Step: 318, Loss: 0.9196598529815674, Accuracy: 1.0, Computation time: 1.6503033638000488\n",
      "Step: 319, Loss: 0.9163711071014404, Accuracy: 1.0, Computation time: 1.425304889678955\n",
      "Step: 320, Loss: 0.9280893802642822, Accuracy: 0.96875, Computation time: 1.8674561977386475\n",
      "Step: 321, Loss: 0.938303530216217, Accuracy: 0.9807692766189575, Computation time: 1.3314480781555176\n",
      "Step: 322, Loss: 0.932966947555542, Accuracy: 0.9821428656578064, Computation time: 1.561460018157959\n",
      "Step: 323, Loss: 0.9163516163825989, Accuracy: 1.0, Computation time: 1.309493064880371\n",
      "Step: 324, Loss: 0.9172070026397705, Accuracy: 1.0, Computation time: 1.3967933654785156\n",
      "Step: 325, Loss: 0.9385433793067932, Accuracy: 0.949999988079071, Computation time: 1.554886817932129\n",
      "Step: 326, Loss: 0.9165964722633362, Accuracy: 1.0, Computation time: 1.4643285274505615\n",
      "Step: 327, Loss: 0.9160870313644409, Accuracy: 1.0, Computation time: 1.361753225326538\n",
      "Step: 328, Loss: 0.9160190224647522, Accuracy: 1.0, Computation time: 1.1975104808807373\n",
      "Step: 329, Loss: 0.9328418970108032, Accuracy: 0.9772727489471436, Computation time: 1.4092686176300049\n",
      "Step: 330, Loss: 0.9262515902519226, Accuracy: 0.9821428656578064, Computation time: 2.1536731719970703\n",
      "Step: 331, Loss: 0.916081428527832, Accuracy: 1.0, Computation time: 1.358560562133789\n",
      "Step: 332, Loss: 0.9160957932472229, Accuracy: 1.0, Computation time: 1.544912576675415\n",
      "Step: 333, Loss: 0.9161362051963806, Accuracy: 1.0, Computation time: 1.248868703842163\n",
      "Step: 334, Loss: 0.9286313056945801, Accuracy: 0.9750000238418579, Computation time: 1.4706668853759766\n",
      "Step: 335, Loss: 0.961172342300415, Accuracy: 0.9307692050933838, Computation time: 1.542231798171997\n",
      "Step: 336, Loss: 0.9162777066230774, Accuracy: 1.0, Computation time: 1.4379231929779053\n",
      "Step: 337, Loss: 0.9162107706069946, Accuracy: 1.0, Computation time: 1.3958885669708252\n",
      "Step: 338, Loss: 0.9164077639579773, Accuracy: 1.0, Computation time: 1.442117691040039\n",
      "Step: 339, Loss: 0.9162709712982178, Accuracy: 1.0, Computation time: 1.2605071067810059\n",
      "Step: 340, Loss: 0.9161525368690491, Accuracy: 1.0, Computation time: 1.380934238433838\n",
      "Step: 341, Loss: 0.9163276553153992, Accuracy: 1.0, Computation time: 1.6167004108428955\n",
      "Step: 342, Loss: 0.9176307320594788, Accuracy: 1.0, Computation time: 1.3139476776123047\n",
      "Step: 343, Loss: 0.9271013736724854, Accuracy: 1.0, Computation time: 1.5800580978393555\n",
      "Step: 344, Loss: 0.9162594079971313, Accuracy: 1.0, Computation time: 1.217137336730957\n",
      "Step: 345, Loss: 0.9190809726715088, Accuracy: 1.0, Computation time: 1.4434494972229004\n",
      "Step: 346, Loss: 0.9271969199180603, Accuracy: 1.0, Computation time: 1.6970620155334473\n",
      "Step: 347, Loss: 0.9356493949890137, Accuracy: 0.9722222089767456, Computation time: 1.137488603591919\n",
      "Step: 348, Loss: 0.921430230140686, Accuracy: 1.0, Computation time: 1.6244761943817139\n",
      "Step: 349, Loss: 0.9187030792236328, Accuracy: 1.0, Computation time: 1.5565376281738281\n",
      "Step: 350, Loss: 0.9240633249282837, Accuracy: 1.0, Computation time: 1.6464662551879883\n",
      "Step: 351, Loss: 0.9386821985244751, Accuracy: 0.9791666865348816, Computation time: 1.555368185043335\n",
      "Step: 352, Loss: 0.9165205359458923, Accuracy: 1.0, Computation time: 1.3061671257019043\n",
      "Step: 353, Loss: 0.9168315529823303, Accuracy: 1.0, Computation time: 1.4348113536834717\n",
      "Step: 354, Loss: 0.9168766736984253, Accuracy: 1.0, Computation time: 1.2990639209747314\n",
      "Step: 355, Loss: 0.916222870349884, Accuracy: 1.0, Computation time: 1.3190429210662842\n",
      "Step: 356, Loss: 0.9376818537712097, Accuracy: 0.9642857313156128, Computation time: 1.5174436569213867\n",
      "Step: 357, Loss: 0.9164817929267883, Accuracy: 1.0, Computation time: 1.6175227165222168\n",
      "Step: 358, Loss: 0.9246654510498047, Accuracy: 1.0, Computation time: 1.79654860496521\n",
      "Step: 359, Loss: 0.916854739189148, Accuracy: 1.0, Computation time: 1.4030029773712158\n",
      "Step: 360, Loss: 0.9241092205047607, Accuracy: 1.0, Computation time: 1.6425817012786865\n",
      "Step: 361, Loss: 0.9165736436843872, Accuracy: 1.0, Computation time: 1.3092010021209717\n",
      "Step: 362, Loss: 0.916538417339325, Accuracy: 1.0, Computation time: 1.386674165725708\n",
      "Step: 363, Loss: 0.9169137477874756, Accuracy: 1.0, Computation time: 1.2989649772644043\n",
      "Step: 364, Loss: 0.918274462223053, Accuracy: 1.0, Computation time: 1.3552441596984863\n",
      "Step: 365, Loss: 0.9185132384300232, Accuracy: 1.0, Computation time: 1.5882859230041504\n",
      "Step: 366, Loss: 0.9164196848869324, Accuracy: 1.0, Computation time: 1.3610193729400635\n",
      "Step: 367, Loss: 0.916527271270752, Accuracy: 1.0, Computation time: 1.4099807739257812\n",
      "Step: 368, Loss: 0.917517900466919, Accuracy: 1.0, Computation time: 1.4447200298309326\n",
      "Step: 369, Loss: 0.9162682890892029, Accuracy: 1.0, Computation time: 1.2597787380218506\n",
      "Step: 370, Loss: 0.9163044691085815, Accuracy: 1.0, Computation time: 1.4029994010925293\n",
      "Step: 371, Loss: 0.9162055850028992, Accuracy: 1.0, Computation time: 1.2518322467803955\n",
      "Step: 372, Loss: 0.9164736866950989, Accuracy: 1.0, Computation time: 1.3890280723571777\n",
      "Step: 373, Loss: 0.9162235260009766, Accuracy: 1.0, Computation time: 1.3033978939056396\n",
      "Step: 374, Loss: 0.9161091446876526, Accuracy: 1.0, Computation time: 1.3979878425598145\n",
      "Step: 375, Loss: 0.9162930250167847, Accuracy: 1.0, Computation time: 1.7372801303863525\n",
      "Step: 376, Loss: 0.9161480665206909, Accuracy: 1.0, Computation time: 1.3089721202850342\n",
      "Step: 377, Loss: 0.9164708256721497, Accuracy: 1.0, Computation time: 1.318720817565918\n",
      "Step: 378, Loss: 0.9162664413452148, Accuracy: 1.0, Computation time: 1.2810533046722412\n",
      "Step: 379, Loss: 0.9169606566429138, Accuracy: 1.0, Computation time: 2.382660150527954\n",
      "Step: 380, Loss: 0.9380847811698914, Accuracy: 0.9807692766189575, Computation time: 1.3805227279663086\n",
      "Step: 381, Loss: 0.9164037704467773, Accuracy: 1.0, Computation time: 1.4198369979858398\n",
      "Step: 382, Loss: 0.9323485493659973, Accuracy: 0.949999988079071, Computation time: 1.3860132694244385\n",
      "Step: 383, Loss: 0.9168950915336609, Accuracy: 1.0, Computation time: 1.3112037181854248\n",
      "Step: 384, Loss: 0.9161365032196045, Accuracy: 1.0, Computation time: 1.3789021968841553\n",
      "Step: 385, Loss: 0.916189432144165, Accuracy: 1.0, Computation time: 1.3826146125793457\n",
      "Step: 386, Loss: 0.9408682584762573, Accuracy: 0.9772727489471436, Computation time: 1.6620666980743408\n",
      "Step: 387, Loss: 0.9167384505271912, Accuracy: 1.0, Computation time: 1.630411148071289\n",
      "Step: 388, Loss: 0.9377233386039734, Accuracy: 0.9722222089767456, Computation time: 1.375171422958374\n",
      "Step: 389, Loss: 0.9189168810844421, Accuracy: 1.0, Computation time: 2.347672462463379\n",
      "Step: 390, Loss: 0.928286075592041, Accuracy: 0.9642857313156128, Computation time: 1.9586808681488037\n",
      "Step: 391, Loss: 0.9160208106040955, Accuracy: 1.0, Computation time: 1.2350153923034668\n",
      "Step: 392, Loss: 0.9170713424682617, Accuracy: 1.0, Computation time: 1.496229887008667\n",
      "Step: 393, Loss: 0.9160454869270325, Accuracy: 1.0, Computation time: 1.2522141933441162\n",
      "Step: 394, Loss: 0.9162237644195557, Accuracy: 1.0, Computation time: 1.3370192050933838\n",
      "Step: 395, Loss: 0.9160662889480591, Accuracy: 1.0, Computation time: 1.2069952487945557\n",
      "Step: 396, Loss: 0.9159979820251465, Accuracy: 1.0, Computation time: 1.2935986518859863\n",
      "Step: 397, Loss: 0.9165588617324829, Accuracy: 1.0, Computation time: 1.3740286827087402\n",
      "Step: 398, Loss: 0.9160680770874023, Accuracy: 1.0, Computation time: 1.5633282661437988\n",
      "Step: 399, Loss: 0.9229743480682373, Accuracy: 1.0, Computation time: 1.347733736038208\n",
      "Step: 400, Loss: 0.9160178899765015, Accuracy: 1.0, Computation time: 1.3170604705810547\n",
      "Step: 401, Loss: 0.916008472442627, Accuracy: 1.0, Computation time: 1.371518850326538\n",
      "Step: 402, Loss: 0.9160664081573486, Accuracy: 1.0, Computation time: 1.4061017036437988\n",
      "Step: 403, Loss: 0.9167144894599915, Accuracy: 1.0, Computation time: 1.3400344848632812\n",
      "Step: 404, Loss: 0.9237967729568481, Accuracy: 1.0, Computation time: 1.8685505390167236\n",
      "Step: 405, Loss: 0.9162763953208923, Accuracy: 1.0, Computation time: 1.4913065433502197\n",
      "Step: 406, Loss: 0.9577947854995728, Accuracy: 0.9500000476837158, Computation time: 1.2753183841705322\n",
      "Step: 407, Loss: 0.9160512685775757, Accuracy: 1.0, Computation time: 1.4981207847595215\n",
      "Step: 408, Loss: 0.9161067605018616, Accuracy: 1.0, Computation time: 1.4239211082458496\n",
      "Step: 409, Loss: 0.9174792170524597, Accuracy: 1.0, Computation time: 1.2863199710845947\n",
      "Step: 410, Loss: 0.9397022724151611, Accuracy: 0.9583333730697632, Computation time: 1.9254610538482666\n",
      "Step: 411, Loss: 0.9160171151161194, Accuracy: 1.0, Computation time: 1.2654423713684082\n",
      "Step: 412, Loss: 0.9376102089881897, Accuracy: 0.9750000238418579, Computation time: 1.5826447010040283\n",
      "Step: 413, Loss: 0.9183943271636963, Accuracy: 1.0, Computation time: 1.8069062232971191\n",
      "Step: 414, Loss: 0.9302242398262024, Accuracy: 0.9583333730697632, Computation time: 2.747379779815674\n",
      "Step: 415, Loss: 0.9164897203445435, Accuracy: 1.0, Computation time: 1.7776577472686768\n",
      "Step: 416, Loss: 0.9365096092224121, Accuracy: 0.875, Computation time: 1.4625225067138672\n",
      "Step: 417, Loss: 0.9163057208061218, Accuracy: 1.0, Computation time: 1.3684971332550049\n",
      "########################\n",
      "Test loss: 1.0686191320419312, Test Accuracy_epoch3: 0.7870000004768372\n",
      "########################\n",
      "Step: 418, Loss: 0.9161834120750427, Accuracy: 1.0, Computation time: 1.2090425491333008\n",
      "Step: 419, Loss: 0.9379848837852478, Accuracy: 0.9833333492279053, Computation time: 1.181751012802124\n",
      "Step: 420, Loss: 0.9175835251808167, Accuracy: 1.0, Computation time: 1.6299798488616943\n",
      "Step: 421, Loss: 0.9162708520889282, Accuracy: 1.0, Computation time: 1.185544490814209\n",
      "Step: 422, Loss: 0.921053946018219, Accuracy: 1.0, Computation time: 1.6965065002441406\n",
      "Step: 423, Loss: 0.9319636821746826, Accuracy: 0.9375, Computation time: 1.4595110416412354\n",
      "Step: 424, Loss: 0.9160090684890747, Accuracy: 1.0, Computation time: 1.2672138214111328\n",
      "Step: 425, Loss: 0.916236400604248, Accuracy: 1.0, Computation time: 1.2508337497711182\n",
      "Step: 426, Loss: 0.9367149472236633, Accuracy: 0.9772727489471436, Computation time: 1.4891149997711182\n",
      "Step: 427, Loss: 0.9161880016326904, Accuracy: 1.0, Computation time: 1.3689990043640137\n",
      "Step: 428, Loss: 0.916216254234314, Accuracy: 1.0, Computation time: 1.2909314632415771\n",
      "Step: 429, Loss: 0.9163594841957092, Accuracy: 1.0, Computation time: 1.2352168560028076\n",
      "Step: 430, Loss: 0.916203498840332, Accuracy: 1.0, Computation time: 1.6228437423706055\n",
      "Step: 431, Loss: 0.9356808662414551, Accuracy: 0.949999988079071, Computation time: 1.7404661178588867\n",
      "Step: 432, Loss: 0.9160971641540527, Accuracy: 1.0, Computation time: 1.2667577266693115\n",
      "Step: 433, Loss: 0.9168642163276672, Accuracy: 1.0, Computation time: 1.4250409603118896\n",
      "Step: 434, Loss: 0.916014552116394, Accuracy: 1.0, Computation time: 1.336670160293579\n",
      "Step: 435, Loss: 0.9159736633300781, Accuracy: 1.0, Computation time: 1.718799352645874\n",
      "Step: 436, Loss: 0.9160115718841553, Accuracy: 1.0, Computation time: 1.4548165798187256\n",
      "Step: 437, Loss: 0.9166034460067749, Accuracy: 1.0, Computation time: 1.420135259628296\n",
      "Step: 438, Loss: 0.9171063303947449, Accuracy: 1.0, Computation time: 1.5745234489440918\n",
      "Step: 439, Loss: 0.9165844917297363, Accuracy: 1.0, Computation time: 1.4371988773345947\n",
      "Step: 440, Loss: 0.9159970879554749, Accuracy: 1.0, Computation time: 1.2313926219940186\n",
      "Step: 441, Loss: 0.916311502456665, Accuracy: 1.0, Computation time: 1.5624723434448242\n",
      "Step: 442, Loss: 0.9172937870025635, Accuracy: 1.0, Computation time: 1.2447848320007324\n",
      "Step: 443, Loss: 0.9167299866676331, Accuracy: 1.0, Computation time: 1.6803133487701416\n",
      "Step: 444, Loss: 0.9639462828636169, Accuracy: 0.9472222328186035, Computation time: 1.8595998287200928\n",
      "Step: 445, Loss: 0.9160541892051697, Accuracy: 1.0, Computation time: 1.4028799533843994\n",
      "Step: 446, Loss: 0.9244690537452698, Accuracy: 1.0, Computation time: 1.698683500289917\n",
      "Step: 447, Loss: 0.9160316586494446, Accuracy: 1.0, Computation time: 1.4620392322540283\n",
      "Step: 448, Loss: 0.9256957769393921, Accuracy: 0.9375, Computation time: 1.5070993900299072\n",
      "Step: 449, Loss: 0.9160991907119751, Accuracy: 1.0, Computation time: 1.369802474975586\n",
      "Step: 450, Loss: 0.9160743951797485, Accuracy: 1.0, Computation time: 1.753901720046997\n",
      "Step: 451, Loss: 0.916265070438385, Accuracy: 1.0, Computation time: 1.3319101333618164\n",
      "Step: 452, Loss: 0.9238446950912476, Accuracy: 1.0, Computation time: 2.6264193058013916\n",
      "Step: 453, Loss: 0.9162205457687378, Accuracy: 1.0, Computation time: 1.641955852508545\n",
      "Step: 454, Loss: 0.9381229877471924, Accuracy: 0.96875, Computation time: 1.192925214767456\n",
      "Step: 455, Loss: 0.9168156385421753, Accuracy: 1.0, Computation time: 1.4253551959991455\n",
      "Step: 456, Loss: 0.9239712953567505, Accuracy: 1.0, Computation time: 1.5009777545928955\n",
      "Step: 457, Loss: 0.9167506098747253, Accuracy: 1.0, Computation time: 1.417649269104004\n",
      "Step: 458, Loss: 0.9166719317436218, Accuracy: 1.0, Computation time: 1.372631311416626\n",
      "Step: 459, Loss: 0.9253543615341187, Accuracy: 1.0, Computation time: 1.5043985843658447\n",
      "Step: 460, Loss: 0.9386624693870544, Accuracy: 0.96875, Computation time: 1.5333375930786133\n",
      "Step: 461, Loss: 0.9475371837615967, Accuracy: 0.9147727489471436, Computation time: 2.328813314437866\n",
      "Step: 462, Loss: 0.9167672991752625, Accuracy: 1.0, Computation time: 1.3513150215148926\n",
      "Step: 463, Loss: 0.9359987378120422, Accuracy: 0.9772727489471436, Computation time: 1.8485357761383057\n",
      "Step: 464, Loss: 0.9174650311470032, Accuracy: 1.0, Computation time: 1.3737006187438965\n",
      "Step: 465, Loss: 0.9169675707817078, Accuracy: 1.0, Computation time: 1.2479007244110107\n",
      "Step: 466, Loss: 0.9168279767036438, Accuracy: 1.0, Computation time: 1.752971887588501\n",
      "Step: 467, Loss: 0.9192901253700256, Accuracy: 1.0, Computation time: 1.5050303936004639\n",
      "Step: 468, Loss: 0.924737274646759, Accuracy: 1.0, Computation time: 1.6711480617523193\n",
      "Step: 469, Loss: 0.9162231683731079, Accuracy: 1.0, Computation time: 1.2515451908111572\n",
      "Step: 470, Loss: 0.9379174709320068, Accuracy: 0.9772727489471436, Computation time: 1.623122215270996\n",
      "Step: 471, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 1.2917158603668213\n",
      "Step: 472, Loss: 0.9167532920837402, Accuracy: 1.0, Computation time: 1.6284525394439697\n",
      "Step: 473, Loss: 0.9376318454742432, Accuracy: 0.9722222089767456, Computation time: 1.351130485534668\n",
      "Step: 474, Loss: 0.9179586172103882, Accuracy: 1.0, Computation time: 1.7692255973815918\n",
      "Step: 475, Loss: 0.9161492586135864, Accuracy: 1.0, Computation time: 1.558042287826538\n",
      "Step: 476, Loss: 0.9174601435661316, Accuracy: 1.0, Computation time: 1.376892328262329\n",
      "Step: 477, Loss: 0.937130868434906, Accuracy: 0.9791666865348816, Computation time: 1.636084794998169\n",
      "Step: 478, Loss: 0.9369858503341675, Accuracy: 0.9772727489471436, Computation time: 1.3465876579284668\n",
      "Step: 479, Loss: 0.9275739192962646, Accuracy: 0.9807692766189575, Computation time: 1.6765196323394775\n",
      "Step: 480, Loss: 0.9160850644111633, Accuracy: 1.0, Computation time: 1.1836521625518799\n",
      "Step: 481, Loss: 0.9205785989761353, Accuracy: 1.0, Computation time: 1.6797940731048584\n",
      "Step: 482, Loss: 0.9161073565483093, Accuracy: 1.0, Computation time: 1.2451090812683105\n",
      "Step: 483, Loss: 0.9160565137863159, Accuracy: 1.0, Computation time: 1.095329761505127\n",
      "Step: 484, Loss: 0.9160116910934448, Accuracy: 1.0, Computation time: 1.2004320621490479\n",
      "Step: 485, Loss: 0.9163841009140015, Accuracy: 1.0, Computation time: 2.1856539249420166\n",
      "Step: 486, Loss: 0.9160481095314026, Accuracy: 1.0, Computation time: 1.7974731922149658\n",
      "Step: 487, Loss: 0.928881049156189, Accuracy: 0.9791666865348816, Computation time: 1.782851219177246\n",
      "Step: 488, Loss: 0.9160231351852417, Accuracy: 1.0, Computation time: 1.4761459827423096\n",
      "Step: 489, Loss: 0.9160573482513428, Accuracy: 1.0, Computation time: 1.216599941253662\n",
      "Step: 490, Loss: 0.915912926197052, Accuracy: 1.0, Computation time: 1.2029938697814941\n",
      "Step: 491, Loss: 0.9159927368164062, Accuracy: 1.0, Computation time: 1.2338829040527344\n",
      "Step: 492, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.3635025024414062\n",
      "Step: 493, Loss: 0.9181489944458008, Accuracy: 1.0, Computation time: 1.2372796535491943\n",
      "Step: 494, Loss: 0.9166409969329834, Accuracy: 1.0, Computation time: 1.288654088973999\n",
      "Step: 495, Loss: 0.9160181879997253, Accuracy: 1.0, Computation time: 1.522223711013794\n",
      "Step: 496, Loss: 0.9159952402114868, Accuracy: 1.0, Computation time: 1.3993947505950928\n",
      "Step: 497, Loss: 0.9166998267173767, Accuracy: 1.0, Computation time: 1.707160234451294\n",
      "Step: 498, Loss: 0.9159533977508545, Accuracy: 1.0, Computation time: 1.3203186988830566\n",
      "Step: 499, Loss: 0.9172673225402832, Accuracy: 1.0, Computation time: 1.5346059799194336\n",
      "Step: 500, Loss: 0.9159284830093384, Accuracy: 1.0, Computation time: 1.5697016716003418\n",
      "Step: 501, Loss: 0.9580736756324768, Accuracy: 0.9415584802627563, Computation time: 1.2437732219696045\n",
      "Step: 502, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 1.5318481922149658\n",
      "Step: 503, Loss: 0.9159863591194153, Accuracy: 1.0, Computation time: 1.2238333225250244\n",
      "Step: 504, Loss: 0.9171018004417419, Accuracy: 1.0, Computation time: 1.4998366832733154\n",
      "Step: 505, Loss: 0.9180868864059448, Accuracy: 1.0, Computation time: 1.3688554763793945\n",
      "Step: 506, Loss: 0.9159785509109497, Accuracy: 1.0, Computation time: 1.4337553977966309\n",
      "Step: 507, Loss: 0.9161549210548401, Accuracy: 1.0, Computation time: 1.3382797241210938\n",
      "Step: 508, Loss: 0.9159780740737915, Accuracy: 1.0, Computation time: 1.3967225551605225\n",
      "Step: 509, Loss: 0.917755663394928, Accuracy: 1.0, Computation time: 1.3925187587738037\n",
      "Step: 510, Loss: 0.9165292382240295, Accuracy: 1.0, Computation time: 1.581634283065796\n",
      "Step: 511, Loss: 0.939044713973999, Accuracy: 0.9791666865348816, Computation time: 1.3045475482940674\n",
      "Step: 512, Loss: 0.9164742827415466, Accuracy: 1.0, Computation time: 1.831831455230713\n",
      "Step: 513, Loss: 0.9159592390060425, Accuracy: 1.0, Computation time: 1.4649648666381836\n",
      "Step: 514, Loss: 0.9186477661132812, Accuracy: 1.0, Computation time: 1.4430582523345947\n",
      "Step: 515, Loss: 0.9323866367340088, Accuracy: 0.9750000238418579, Computation time: 1.4155454635620117\n",
      "Step: 516, Loss: 0.9195613861083984, Accuracy: 1.0, Computation time: 1.6690115928649902\n",
      "Step: 517, Loss: 0.9334461092948914, Accuracy: 0.96875, Computation time: 1.620234727859497\n",
      "Step: 518, Loss: 0.9161055088043213, Accuracy: 1.0, Computation time: 1.2426559925079346\n",
      "Step: 519, Loss: 0.9160109758377075, Accuracy: 1.0, Computation time: 1.3146100044250488\n",
      "Step: 520, Loss: 0.9161144495010376, Accuracy: 1.0, Computation time: 1.4350740909576416\n",
      "Step: 521, Loss: 0.9159911870956421, Accuracy: 1.0, Computation time: 1.5188617706298828\n",
      "Step: 522, Loss: 0.9159424901008606, Accuracy: 1.0, Computation time: 1.1636295318603516\n",
      "Step: 523, Loss: 0.917505145072937, Accuracy: 1.0, Computation time: 1.7327907085418701\n",
      "Step: 524, Loss: 0.936185896396637, Accuracy: 0.9583333730697632, Computation time: 1.5460481643676758\n",
      "Step: 525, Loss: 0.9159520268440247, Accuracy: 1.0, Computation time: 1.3617146015167236\n",
      "Step: 526, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 1.29835844039917\n",
      "Step: 527, Loss: 0.9159126281738281, Accuracy: 1.0, Computation time: 1.9064688682556152\n",
      "Step: 528, Loss: 0.9195702075958252, Accuracy: 1.0, Computation time: 1.7824428081512451\n",
      "Step: 529, Loss: 0.9161641001701355, Accuracy: 1.0, Computation time: 1.3714354038238525\n",
      "Step: 530, Loss: 0.9187299013137817, Accuracy: 1.0, Computation time: 1.2259407043457031\n",
      "Step: 531, Loss: 0.9163497090339661, Accuracy: 1.0, Computation time: 1.539466142654419\n",
      "Step: 532, Loss: 0.9386804103851318, Accuracy: 0.9807692766189575, Computation time: 1.2337994575500488\n",
      "Step: 533, Loss: 0.9160224795341492, Accuracy: 1.0, Computation time: 1.3176968097686768\n",
      "Step: 534, Loss: 0.9181466102600098, Accuracy: 1.0, Computation time: 1.278104305267334\n",
      "Step: 535, Loss: 0.9162769317626953, Accuracy: 1.0, Computation time: 1.592453956604004\n",
      "Step: 536, Loss: 0.9159830212593079, Accuracy: 1.0, Computation time: 1.40193510055542\n",
      "Step: 537, Loss: 0.915964663028717, Accuracy: 1.0, Computation time: 1.2546403408050537\n",
      "Step: 538, Loss: 0.9159666895866394, Accuracy: 1.0, Computation time: 1.2967135906219482\n",
      "Step: 539, Loss: 0.9159802198410034, Accuracy: 1.0, Computation time: 1.2224953174591064\n",
      "Step: 540, Loss: 0.9305151104927063, Accuracy: 0.949999988079071, Computation time: 1.989945888519287\n",
      "Step: 541, Loss: 0.9161069989204407, Accuracy: 1.0, Computation time: 1.283158540725708\n",
      "Step: 542, Loss: 0.9160615801811218, Accuracy: 1.0, Computation time: 1.2383060455322266\n",
      "Step: 543, Loss: 0.9376030564308167, Accuracy: 0.96875, Computation time: 1.609952688217163\n",
      "Step: 544, Loss: 0.9165574908256531, Accuracy: 1.0, Computation time: 1.4248061180114746\n",
      "Step: 545, Loss: 0.9159447550773621, Accuracy: 1.0, Computation time: 1.4602904319763184\n",
      "Step: 546, Loss: 0.9161152243614197, Accuracy: 1.0, Computation time: 1.235321044921875\n",
      "Step: 547, Loss: 0.9361444115638733, Accuracy: 0.9750000238418579, Computation time: 1.3719463348388672\n",
      "Step: 548, Loss: 0.9168130159378052, Accuracy: 1.0, Computation time: 1.4243059158325195\n",
      "Step: 549, Loss: 0.9160877466201782, Accuracy: 1.0, Computation time: 1.3834125995635986\n",
      "Step: 550, Loss: 0.9209294319152832, Accuracy: 1.0, Computation time: 1.4191420078277588\n",
      "Step: 551, Loss: 0.9162091612815857, Accuracy: 1.0, Computation time: 1.4918968677520752\n",
      "Step: 552, Loss: 0.937579870223999, Accuracy: 0.9375, Computation time: 1.2943212985992432\n",
      "Step: 553, Loss: 0.9399622678756714, Accuracy: 0.9772727489471436, Computation time: 1.7698915004730225\n",
      "Step: 554, Loss: 0.9166966676712036, Accuracy: 1.0, Computation time: 1.7859745025634766\n",
      "Step: 555, Loss: 0.9162006378173828, Accuracy: 1.0, Computation time: 1.633080005645752\n",
      "Step: 556, Loss: 0.9286108613014221, Accuracy: 0.9750000238418579, Computation time: 1.6894185543060303\n",
      "########################\n",
      "Test loss: 1.0619285106658936, Test Accuracy_epoch4: 0.7968008518218994\n",
      "########################\n",
      "Step: 557, Loss: 0.9165024161338806, Accuracy: 1.0, Computation time: 1.552553653717041\n",
      "Step: 558, Loss: 0.9159969091415405, Accuracy: 1.0, Computation time: 1.182577133178711\n",
      "Step: 559, Loss: 0.9167670607566833, Accuracy: 1.0, Computation time: 1.6941792964935303\n",
      "Step: 560, Loss: 0.937745213508606, Accuracy: 0.9722222089767456, Computation time: 1.5041391849517822\n",
      "Step: 561, Loss: 0.9160789847373962, Accuracy: 1.0, Computation time: 1.2944889068603516\n",
      "Step: 562, Loss: 0.9176183938980103, Accuracy: 1.0, Computation time: 1.362717628479004\n",
      "Step: 563, Loss: 0.9339736104011536, Accuracy: 0.949999988079071, Computation time: 1.5495266914367676\n",
      "Step: 564, Loss: 0.937754213809967, Accuracy: 0.9772727489471436, Computation time: 1.3540630340576172\n",
      "Step: 565, Loss: 0.9160281419754028, Accuracy: 1.0, Computation time: 1.236098051071167\n",
      "Step: 566, Loss: 0.9160370230674744, Accuracy: 1.0, Computation time: 1.370626449584961\n",
      "Step: 567, Loss: 0.9160990715026855, Accuracy: 1.0, Computation time: 1.3561933040618896\n",
      "Step: 568, Loss: 0.9165815711021423, Accuracy: 1.0, Computation time: 1.4921340942382812\n",
      "Step: 569, Loss: 0.9166204929351807, Accuracy: 1.0, Computation time: 1.2011516094207764\n",
      "Step: 570, Loss: 0.9166849851608276, Accuracy: 1.0, Computation time: 1.3681395053863525\n",
      "Step: 571, Loss: 0.9359506368637085, Accuracy: 0.9750000238418579, Computation time: 2.076225996017456\n",
      "Step: 572, Loss: 0.9159872531890869, Accuracy: 1.0, Computation time: 1.267198085784912\n",
      "Step: 573, Loss: 0.9246684908866882, Accuracy: 1.0, Computation time: 1.3880507946014404\n",
      "Step: 574, Loss: 0.9305258989334106, Accuracy: 0.9750000238418579, Computation time: 1.6371128559112549\n",
      "Step: 575, Loss: 0.9161913990974426, Accuracy: 1.0, Computation time: 1.4520745277404785\n",
      "Step: 576, Loss: 0.9162175059318542, Accuracy: 1.0, Computation time: 1.4586739540100098\n",
      "Step: 577, Loss: 0.9166023135185242, Accuracy: 1.0, Computation time: 1.3678901195526123\n",
      "Step: 578, Loss: 0.9163975119590759, Accuracy: 1.0, Computation time: 1.333658218383789\n",
      "Step: 579, Loss: 0.9191938638687134, Accuracy: 1.0, Computation time: 1.4609558582305908\n",
      "Step: 580, Loss: 0.9165343642234802, Accuracy: 1.0, Computation time: 1.4908146858215332\n",
      "Step: 581, Loss: 0.9161795973777771, Accuracy: 1.0, Computation time: 1.4260952472686768\n",
      "Step: 582, Loss: 0.9162395000457764, Accuracy: 1.0, Computation time: 1.4972589015960693\n",
      "Step: 583, Loss: 0.9375529885292053, Accuracy: 0.9772727489471436, Computation time: 1.626267433166504\n",
      "Step: 584, Loss: 0.916026771068573, Accuracy: 1.0, Computation time: 1.3340952396392822\n",
      "Step: 585, Loss: 0.9160176515579224, Accuracy: 1.0, Computation time: 1.1914472579956055\n",
      "Step: 586, Loss: 0.9160360097885132, Accuracy: 1.0, Computation time: 1.1993370056152344\n",
      "Step: 587, Loss: 0.9160133004188538, Accuracy: 1.0, Computation time: 1.5015373229980469\n",
      "Step: 588, Loss: 0.9160950183868408, Accuracy: 1.0, Computation time: 1.3517601490020752\n",
      "Step: 589, Loss: 0.9162482619285583, Accuracy: 1.0, Computation time: 1.5656862258911133\n",
      "Step: 590, Loss: 0.9227117300033569, Accuracy: 1.0, Computation time: 1.5155200958251953\n",
      "Step: 591, Loss: 0.915972113609314, Accuracy: 1.0, Computation time: 1.3631737232208252\n",
      "Step: 592, Loss: 0.915997326374054, Accuracy: 1.0, Computation time: 1.192617416381836\n",
      "Step: 593, Loss: 0.9164251685142517, Accuracy: 1.0, Computation time: 1.4869630336761475\n",
      "Step: 594, Loss: 0.9183643460273743, Accuracy: 1.0, Computation time: 1.901170253753662\n",
      "Step: 595, Loss: 0.9313017129898071, Accuracy: 0.9375, Computation time: 1.2510654926300049\n",
      "Step: 596, Loss: 0.9160581231117249, Accuracy: 1.0, Computation time: 1.3890058994293213\n",
      "Step: 597, Loss: 0.9206704497337341, Accuracy: 1.0, Computation time: 1.4724328517913818\n",
      "Step: 598, Loss: 0.9179112911224365, Accuracy: 1.0, Computation time: 1.3101656436920166\n",
      "Step: 599, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.1924793720245361\n",
      "Step: 600, Loss: 0.9162368774414062, Accuracy: 1.0, Computation time: 1.4338204860687256\n",
      "Step: 601, Loss: 0.9220200777053833, Accuracy: 1.0, Computation time: 1.5247700214385986\n",
      "Step: 602, Loss: 0.9376646876335144, Accuracy: 0.9583333730697632, Computation time: 1.7942144870758057\n",
      "Step: 603, Loss: 0.9184697270393372, Accuracy: 1.0, Computation time: 1.7160091400146484\n",
      "Step: 604, Loss: 0.9335663318634033, Accuracy: 0.9750000238418579, Computation time: 1.4846105575561523\n",
      "Step: 605, Loss: 0.9160067439079285, Accuracy: 1.0, Computation time: 1.3486156463623047\n",
      "Step: 606, Loss: 0.91599440574646, Accuracy: 1.0, Computation time: 1.1631677150726318\n",
      "Step: 607, Loss: 0.9159440398216248, Accuracy: 1.0, Computation time: 1.3771328926086426\n",
      "Step: 608, Loss: 0.9376923441886902, Accuracy: 0.9722222089767456, Computation time: 1.5724327564239502\n",
      "Step: 609, Loss: 0.9160411953926086, Accuracy: 1.0, Computation time: 1.9754493236541748\n",
      "Step: 610, Loss: 0.9319117665290833, Accuracy: 0.9722222089767456, Computation time: 1.4367270469665527\n",
      "Step: 611, Loss: 0.9160330891609192, Accuracy: 1.0, Computation time: 1.4447441101074219\n",
      "Step: 612, Loss: 0.9161108732223511, Accuracy: 1.0, Computation time: 1.2407796382904053\n",
      "Step: 613, Loss: 0.9160985946655273, Accuracy: 1.0, Computation time: 1.3069331645965576\n",
      "Step: 614, Loss: 0.9159784913063049, Accuracy: 1.0, Computation time: 1.219712734222412\n",
      "Step: 615, Loss: 0.9161618947982788, Accuracy: 1.0, Computation time: 1.2809867858886719\n",
      "Step: 616, Loss: 0.9186031222343445, Accuracy: 1.0, Computation time: 1.3942816257476807\n",
      "Step: 617, Loss: 0.915924072265625, Accuracy: 1.0, Computation time: 1.4912259578704834\n",
      "Step: 618, Loss: 0.9176694750785828, Accuracy: 1.0, Computation time: 1.729088306427002\n",
      "Step: 619, Loss: 0.9549685716629028, Accuracy: 0.9147727489471436, Computation time: 1.3549699783325195\n",
      "Step: 620, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.3026583194732666\n",
      "Step: 621, Loss: 0.9369925260543823, Accuracy: 0.9772727489471436, Computation time: 1.628068447113037\n",
      "Step: 622, Loss: 0.9160037040710449, Accuracy: 1.0, Computation time: 1.2949514389038086\n",
      "Step: 623, Loss: 0.9178783297538757, Accuracy: 1.0, Computation time: 1.4479286670684814\n",
      "Step: 624, Loss: 0.916250467300415, Accuracy: 1.0, Computation time: 1.4715232849121094\n",
      "Step: 625, Loss: 0.9159034490585327, Accuracy: 1.0, Computation time: 1.275115966796875\n",
      "Step: 626, Loss: 0.9174390435218811, Accuracy: 1.0, Computation time: 1.3051609992980957\n",
      "Step: 627, Loss: 0.9159058928489685, Accuracy: 1.0, Computation time: 1.2689402103424072\n",
      "Step: 628, Loss: 0.9160622358322144, Accuracy: 1.0, Computation time: 1.4093801975250244\n",
      "Step: 629, Loss: 0.9163479804992676, Accuracy: 1.0, Computation time: 1.4151177406311035\n",
      "Step: 630, Loss: 0.9383747577667236, Accuracy: 0.9642857313156128, Computation time: 1.9068901538848877\n",
      "Step: 631, Loss: 0.9209836721420288, Accuracy: 1.0, Computation time: 1.450212001800537\n",
      "Step: 632, Loss: 0.9159800410270691, Accuracy: 1.0, Computation time: 1.3220934867858887\n",
      "Step: 633, Loss: 0.9166144132614136, Accuracy: 1.0, Computation time: 1.3243732452392578\n",
      "Step: 634, Loss: 0.917588472366333, Accuracy: 1.0, Computation time: 1.4245047569274902\n",
      "Step: 635, Loss: 0.9432809352874756, Accuracy: 0.9772727489471436, Computation time: 1.6099565029144287\n",
      "Step: 636, Loss: 0.9161579012870789, Accuracy: 1.0, Computation time: 1.3773059844970703\n",
      "Step: 637, Loss: 0.9162553548812866, Accuracy: 1.0, Computation time: 1.8165500164031982\n",
      "Step: 638, Loss: 0.9568216800689697, Accuracy: 0.9545454978942871, Computation time: 1.5704715251922607\n",
      "Step: 639, Loss: 0.9162449836730957, Accuracy: 1.0, Computation time: 1.6917169094085693\n",
      "Step: 640, Loss: 0.9379228353500366, Accuracy: 0.96875, Computation time: 1.4934501647949219\n",
      "Step: 641, Loss: 0.9162806272506714, Accuracy: 1.0, Computation time: 1.3995323181152344\n",
      "Step: 642, Loss: 0.9297893047332764, Accuracy: 0.9750000238418579, Computation time: 1.5277042388916016\n",
      "Step: 643, Loss: 0.9163690209388733, Accuracy: 1.0, Computation time: 1.1806318759918213\n",
      "Step: 644, Loss: 0.9160122275352478, Accuracy: 1.0, Computation time: 1.20047926902771\n",
      "Step: 645, Loss: 0.9161194562911987, Accuracy: 1.0, Computation time: 1.1801373958587646\n",
      "Step: 646, Loss: 0.916059672832489, Accuracy: 1.0, Computation time: 1.5156586170196533\n",
      "Step: 647, Loss: 0.9221456050872803, Accuracy: 1.0, Computation time: 2.236083745956421\n",
      "Step: 648, Loss: 0.9160846471786499, Accuracy: 1.0, Computation time: 1.29752779006958\n",
      "Step: 649, Loss: 0.9162632822990417, Accuracy: 1.0, Computation time: 1.405839443206787\n",
      "Step: 650, Loss: 0.9162946343421936, Accuracy: 1.0, Computation time: 1.2853178977966309\n",
      "Step: 651, Loss: 0.9163042902946472, Accuracy: 1.0, Computation time: 1.8576815128326416\n",
      "Step: 652, Loss: 0.916329026222229, Accuracy: 1.0, Computation time: 1.3587844371795654\n",
      "Step: 653, Loss: 0.9455840587615967, Accuracy: 0.9722222089767456, Computation time: 1.3770606517791748\n",
      "Step: 654, Loss: 0.9161194562911987, Accuracy: 1.0, Computation time: 1.5171380043029785\n",
      "Step: 655, Loss: 0.916925847530365, Accuracy: 1.0, Computation time: 1.3021795749664307\n",
      "Step: 656, Loss: 0.9161641001701355, Accuracy: 1.0, Computation time: 1.2907378673553467\n",
      "Step: 657, Loss: 0.9322970509529114, Accuracy: 0.9750000238418579, Computation time: 1.853736162185669\n",
      "Step: 658, Loss: 0.9162067174911499, Accuracy: 1.0, Computation time: 1.4452447891235352\n",
      "Step: 659, Loss: 0.9161593317985535, Accuracy: 1.0, Computation time: 1.201798677444458\n",
      "Step: 660, Loss: 0.9162508845329285, Accuracy: 1.0, Computation time: 1.3994176387786865\n",
      "Step: 661, Loss: 0.9161067605018616, Accuracy: 1.0, Computation time: 1.3592808246612549\n",
      "Step: 662, Loss: 0.9167158007621765, Accuracy: 1.0, Computation time: 1.3005526065826416\n",
      "Step: 663, Loss: 0.9162397980690002, Accuracy: 1.0, Computation time: 1.4842662811279297\n",
      "Step: 664, Loss: 0.9160054922103882, Accuracy: 1.0, Computation time: 1.2262704372406006\n",
      "Step: 665, Loss: 0.9368980526924133, Accuracy: 0.9583333730697632, Computation time: 1.3337628841400146\n",
      "Step: 666, Loss: 0.938012957572937, Accuracy: 0.9583333730697632, Computation time: 1.5380918979644775\n",
      "Step: 667, Loss: 0.916010320186615, Accuracy: 1.0, Computation time: 1.4334423542022705\n",
      "Step: 668, Loss: 0.9160992503166199, Accuracy: 1.0, Computation time: 1.6945772171020508\n",
      "Step: 669, Loss: 0.9378166794776917, Accuracy: 0.9791666865348816, Computation time: 1.3276433944702148\n",
      "Step: 670, Loss: 0.9171083569526672, Accuracy: 1.0, Computation time: 1.6027002334594727\n",
      "Step: 671, Loss: 0.9160686135292053, Accuracy: 1.0, Computation time: 1.2668664455413818\n",
      "Step: 672, Loss: 0.9161731600761414, Accuracy: 1.0, Computation time: 1.5987894535064697\n",
      "Step: 673, Loss: 0.9165599346160889, Accuracy: 1.0, Computation time: 1.2552175521850586\n",
      "Step: 674, Loss: 0.935501217842102, Accuracy: 0.9642857313156128, Computation time: 1.2623775005340576\n",
      "Step: 675, Loss: 0.9162076711654663, Accuracy: 1.0, Computation time: 1.7578656673431396\n",
      "Step: 676, Loss: 0.9167435169219971, Accuracy: 1.0, Computation time: 1.4402861595153809\n",
      "Step: 677, Loss: 0.9160165190696716, Accuracy: 1.0, Computation time: 1.1460223197937012\n",
      "Step: 678, Loss: 0.9507941007614136, Accuracy: 0.949999988079071, Computation time: 1.7335419654846191\n",
      "Step: 679, Loss: 0.915984570980072, Accuracy: 1.0, Computation time: 1.5623013973236084\n",
      "Step: 680, Loss: 0.9177009463310242, Accuracy: 1.0, Computation time: 1.5543851852416992\n",
      "Step: 681, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 1.5055322647094727\n",
      "Step: 682, Loss: 0.9159963130950928, Accuracy: 1.0, Computation time: 1.3411962985992432\n",
      "Step: 683, Loss: 0.9177294373512268, Accuracy: 1.0, Computation time: 1.2806098461151123\n",
      "Step: 684, Loss: 0.9159679412841797, Accuracy: 1.0, Computation time: 1.3397138118743896\n",
      "Step: 685, Loss: 0.9370877146720886, Accuracy: 0.949999988079071, Computation time: 1.5124516487121582\n",
      "Step: 686, Loss: 0.9159764647483826, Accuracy: 1.0, Computation time: 1.4006013870239258\n",
      "Step: 687, Loss: 0.9160974621772766, Accuracy: 1.0, Computation time: 1.3536350727081299\n",
      "Step: 688, Loss: 0.9163577556610107, Accuracy: 1.0, Computation time: 2.1179444789886475\n",
      "Step: 689, Loss: 0.9159367084503174, Accuracy: 1.0, Computation time: 1.211888313293457\n",
      "Step: 690, Loss: 0.9271827340126038, Accuracy: 0.9722222089767456, Computation time: 1.4482088088989258\n",
      "Step: 691, Loss: 0.9159537553787231, Accuracy: 1.0, Computation time: 1.4160895347595215\n",
      "Step: 692, Loss: 0.9374022483825684, Accuracy: 0.96875, Computation time: 1.6555743217468262\n",
      "Step: 693, Loss: 0.9159716963768005, Accuracy: 1.0, Computation time: 1.245356798171997\n",
      "Step: 694, Loss: 0.9167553186416626, Accuracy: 1.0, Computation time: 1.210153579711914\n",
      "Step: 695, Loss: 0.9160553216934204, Accuracy: 1.0, Computation time: 1.4360640048980713\n",
      "########################\n",
      "Test loss: 1.0699740648269653, Test Accuracy_epoch5: 0.7822673916816711\n",
      "########################\n",
      "Step: 696, Loss: 0.921120285987854, Accuracy: 1.0, Computation time: 2.118488311767578\n",
      "Step: 697, Loss: 0.9160559773445129, Accuracy: 1.0, Computation time: 1.6297838687896729\n",
      "Step: 698, Loss: 0.9160737991333008, Accuracy: 1.0, Computation time: 1.4075102806091309\n",
      "Step: 699, Loss: 0.9160550832748413, Accuracy: 1.0, Computation time: 1.2270796298980713\n",
      "Step: 700, Loss: 0.9160017967224121, Accuracy: 1.0, Computation time: 1.2983059883117676\n",
      "Step: 701, Loss: 0.916037917137146, Accuracy: 1.0, Computation time: 1.326840877532959\n",
      "Step: 702, Loss: 0.9473115801811218, Accuracy: 0.9375, Computation time: 1.8145267963409424\n",
      "Step: 703, Loss: 0.9371481537818909, Accuracy: 0.9642857313156128, Computation time: 1.370215892791748\n",
      "Step: 704, Loss: 0.9246945381164551, Accuracy: 1.0, Computation time: 1.379213809967041\n",
      "Step: 705, Loss: 0.9160452485084534, Accuracy: 1.0, Computation time: 1.61397123336792\n",
      "Step: 706, Loss: 0.9375531673431396, Accuracy: 0.9750000238418579, Computation time: 1.32289719581604\n",
      "Step: 707, Loss: 0.9160504937171936, Accuracy: 1.0, Computation time: 1.3405630588531494\n",
      "Step: 708, Loss: 0.9166153073310852, Accuracy: 1.0, Computation time: 1.6912205219268799\n",
      "Step: 709, Loss: 0.9162541031837463, Accuracy: 1.0, Computation time: 1.3800547122955322\n",
      "Step: 710, Loss: 0.9164211750030518, Accuracy: 1.0, Computation time: 2.1527822017669678\n",
      "Step: 711, Loss: 0.936526894569397, Accuracy: 0.9583333730697632, Computation time: 2.1784377098083496\n",
      "Step: 712, Loss: 0.9162328839302063, Accuracy: 1.0, Computation time: 1.3752169609069824\n",
      "Step: 713, Loss: 0.9162201881408691, Accuracy: 1.0, Computation time: 1.5941884517669678\n",
      "Step: 714, Loss: 0.9160909056663513, Accuracy: 1.0, Computation time: 1.610579013824463\n",
      "Step: 715, Loss: 0.9161801338195801, Accuracy: 1.0, Computation time: 1.4234964847564697\n",
      "Step: 716, Loss: 0.937620222568512, Accuracy: 0.9722222089767456, Computation time: 1.5588910579681396\n",
      "Step: 717, Loss: 0.9173293709754944, Accuracy: 1.0, Computation time: 1.6980597972869873\n",
      "Step: 718, Loss: 0.9159564971923828, Accuracy: 1.0, Computation time: 1.1486084461212158\n",
      "Step: 719, Loss: 0.9164852499961853, Accuracy: 1.0, Computation time: 1.7363104820251465\n",
      "Step: 720, Loss: 0.9379661083221436, Accuracy: 0.9750000238418579, Computation time: 1.8910250663757324\n",
      "Step: 721, Loss: 0.9159551858901978, Accuracy: 1.0, Computation time: 1.5385441780090332\n",
      "Step: 722, Loss: 0.9159388542175293, Accuracy: 1.0, Computation time: 1.392003059387207\n",
      "Step: 723, Loss: 0.9160555005073547, Accuracy: 1.0, Computation time: 1.377171277999878\n",
      "Step: 724, Loss: 0.9160154461860657, Accuracy: 1.0, Computation time: 1.742854118347168\n",
      "Step: 725, Loss: 0.9223568439483643, Accuracy: 1.0, Computation time: 1.9673895835876465\n",
      "Step: 726, Loss: 0.9401759505271912, Accuracy: 0.9750000238418579, Computation time: 1.3144328594207764\n",
      "Step: 727, Loss: 0.9160271286964417, Accuracy: 1.0, Computation time: 1.3612968921661377\n",
      "Step: 728, Loss: 0.9195321202278137, Accuracy: 1.0, Computation time: 1.3443434238433838\n",
      "Step: 729, Loss: 0.9161901473999023, Accuracy: 1.0, Computation time: 1.6319525241851807\n",
      "Step: 730, Loss: 0.9169705510139465, Accuracy: 1.0, Computation time: 1.448793888092041\n",
      "Step: 731, Loss: 0.916148841381073, Accuracy: 1.0, Computation time: 1.44940185546875\n",
      "Step: 732, Loss: 0.9519821405410767, Accuracy: 0.9642857313156128, Computation time: 2.3379385471343994\n",
      "Step: 733, Loss: 0.9159905910491943, Accuracy: 1.0, Computation time: 1.438642978668213\n",
      "Step: 734, Loss: 0.916119396686554, Accuracy: 1.0, Computation time: 1.4227931499481201\n",
      "Step: 735, Loss: 0.9279825687408447, Accuracy: 0.9583333730697632, Computation time: 1.547790765762329\n",
      "Step: 736, Loss: 0.9168019890785217, Accuracy: 1.0, Computation time: 1.4120430946350098\n",
      "Step: 737, Loss: 0.9226585030555725, Accuracy: 1.0, Computation time: 1.467116355895996\n",
      "Step: 738, Loss: 0.9168331623077393, Accuracy: 1.0, Computation time: 1.3670718669891357\n",
      "Step: 739, Loss: 0.9199910163879395, Accuracy: 1.0, Computation time: 1.1777188777923584\n",
      "Step: 740, Loss: 0.9210695028305054, Accuracy: 1.0, Computation time: 1.2766962051391602\n",
      "Step: 741, Loss: 0.9165658950805664, Accuracy: 1.0, Computation time: 1.4855706691741943\n",
      "Step: 742, Loss: 0.916076123714447, Accuracy: 1.0, Computation time: 1.227065086364746\n",
      "Step: 743, Loss: 0.9166975021362305, Accuracy: 1.0, Computation time: 1.4021883010864258\n",
      "Step: 744, Loss: 0.916718602180481, Accuracy: 1.0, Computation time: 1.376654863357544\n",
      "Step: 745, Loss: 0.9165728092193604, Accuracy: 1.0, Computation time: 1.4996953010559082\n",
      "Step: 746, Loss: 0.9195586442947388, Accuracy: 1.0, Computation time: 1.947800636291504\n",
      "Step: 747, Loss: 0.9165012836456299, Accuracy: 1.0, Computation time: 1.5281178951263428\n",
      "Step: 748, Loss: 0.935764729976654, Accuracy: 0.9642857313156128, Computation time: 1.6057250499725342\n",
      "Step: 749, Loss: 0.9388495683670044, Accuracy: 0.9772727489471436, Computation time: 2.051112174987793\n",
      "Step: 750, Loss: 0.9456663131713867, Accuracy: 0.9166666865348816, Computation time: 2.1350722312927246\n",
      "Step: 751, Loss: 0.9166975021362305, Accuracy: 1.0, Computation time: 1.5184009075164795\n",
      "Step: 752, Loss: 0.9163575768470764, Accuracy: 1.0, Computation time: 1.433786392211914\n",
      "Step: 753, Loss: 0.9163202047348022, Accuracy: 1.0, Computation time: 1.3996691703796387\n",
      "Step: 754, Loss: 0.9162946343421936, Accuracy: 1.0, Computation time: 1.4424793720245361\n",
      "Step: 755, Loss: 0.917266845703125, Accuracy: 1.0, Computation time: 1.218554973602295\n",
      "Step: 756, Loss: 0.9164392352104187, Accuracy: 1.0, Computation time: 1.426114797592163\n",
      "Step: 757, Loss: 0.9160485863685608, Accuracy: 1.0, Computation time: 1.2438714504241943\n",
      "Step: 758, Loss: 0.9376670718193054, Accuracy: 0.9722222089767456, Computation time: 1.3465139865875244\n",
      "Step: 759, Loss: 0.9160298705101013, Accuracy: 1.0, Computation time: 1.283200979232788\n",
      "Step: 760, Loss: 0.9323418140411377, Accuracy: 0.9791666865348816, Computation time: 1.3407540321350098\n",
      "Step: 761, Loss: 0.9162185788154602, Accuracy: 1.0, Computation time: 1.5038177967071533\n",
      "Step: 762, Loss: 0.9160694479942322, Accuracy: 1.0, Computation time: 1.1567447185516357\n",
      "Step: 763, Loss: 0.9161002039909363, Accuracy: 1.0, Computation time: 1.2446849346160889\n",
      "Step: 764, Loss: 0.9310246706008911, Accuracy: 0.9375, Computation time: 1.7866294384002686\n",
      "Step: 765, Loss: 0.9165052175521851, Accuracy: 1.0, Computation time: 1.2265126705169678\n",
      "Step: 766, Loss: 0.9345616698265076, Accuracy: 0.9722222089767456, Computation time: 1.4327352046966553\n",
      "Step: 767, Loss: 0.9161949157714844, Accuracy: 1.0, Computation time: 2.0571210384368896\n",
      "Step: 768, Loss: 0.9162981510162354, Accuracy: 1.0, Computation time: 1.3295414447784424\n",
      "Step: 769, Loss: 0.9161309003829956, Accuracy: 1.0, Computation time: 1.6235010623931885\n",
      "Step: 770, Loss: 0.9160714149475098, Accuracy: 1.0, Computation time: 1.5353715419769287\n",
      "Step: 771, Loss: 0.9159933924674988, Accuracy: 1.0, Computation time: 1.3797309398651123\n",
      "Step: 772, Loss: 0.9159684777259827, Accuracy: 1.0, Computation time: 1.196016788482666\n",
      "Step: 773, Loss: 0.9166070818901062, Accuracy: 1.0, Computation time: 1.2141191959381104\n",
      "Step: 774, Loss: 0.9174079895019531, Accuracy: 1.0, Computation time: 1.7448704242706299\n",
      "Step: 775, Loss: 0.9215456247329712, Accuracy: 1.0, Computation time: 1.7409124374389648\n",
      "Step: 776, Loss: 0.9168663620948792, Accuracy: 1.0, Computation time: 1.2436742782592773\n",
      "Step: 777, Loss: 0.9161497950553894, Accuracy: 1.0, Computation time: 1.6098098754882812\n",
      "Step: 778, Loss: 0.9164882302284241, Accuracy: 1.0, Computation time: 1.2727313041687012\n",
      "Step: 779, Loss: 0.9166231155395508, Accuracy: 1.0, Computation time: 1.2006354331970215\n",
      "Step: 780, Loss: 0.9169641733169556, Accuracy: 1.0, Computation time: 1.5125553607940674\n",
      "Step: 781, Loss: 0.9375413060188293, Accuracy: 0.96875, Computation time: 1.3962841033935547\n",
      "Step: 782, Loss: 0.9160649180412292, Accuracy: 1.0, Computation time: 1.5800542831420898\n",
      "Step: 783, Loss: 0.9325411319732666, Accuracy: 0.9722222089767456, Computation time: 1.5809400081634521\n",
      "Step: 784, Loss: 0.9160289764404297, Accuracy: 1.0, Computation time: 1.1889839172363281\n",
      "Step: 785, Loss: 0.9159502983093262, Accuracy: 1.0, Computation time: 1.6971514225006104\n",
      "Step: 786, Loss: 0.9377384185791016, Accuracy: 0.9833333492279053, Computation time: 1.315431833267212\n",
      "Step: 787, Loss: 0.9167034029960632, Accuracy: 1.0, Computation time: 1.5960285663604736\n",
      "Step: 788, Loss: 0.9161412715911865, Accuracy: 1.0, Computation time: 1.2509453296661377\n",
      "Step: 789, Loss: 0.9161231517791748, Accuracy: 1.0, Computation time: 1.0249228477478027\n",
      "Step: 790, Loss: 0.9161683917045593, Accuracy: 1.0, Computation time: 1.3495211601257324\n",
      "Step: 791, Loss: 0.9181800484657288, Accuracy: 1.0, Computation time: 1.4539844989776611\n",
      "Step: 792, Loss: 0.9161398410797119, Accuracy: 1.0, Computation time: 1.3431785106658936\n",
      "Step: 793, Loss: 0.9160111546516418, Accuracy: 1.0, Computation time: 1.4535789489746094\n",
      "Step: 794, Loss: 0.9374347925186157, Accuracy: 0.9642857313156128, Computation time: 1.2767143249511719\n",
      "Step: 795, Loss: 0.9178863763809204, Accuracy: 1.0, Computation time: 1.2116034030914307\n",
      "Step: 796, Loss: 0.9376845359802246, Accuracy: 0.9642857313156128, Computation time: 1.240546464920044\n",
      "Step: 797, Loss: 0.9382143616676331, Accuracy: 0.949999988079071, Computation time: 1.356318712234497\n",
      "Step: 798, Loss: 0.9160393476486206, Accuracy: 1.0, Computation time: 1.4621922969818115\n",
      "Step: 799, Loss: 0.9160119295120239, Accuracy: 1.0, Computation time: 1.6142594814300537\n",
      "Step: 800, Loss: 0.9161673784255981, Accuracy: 1.0, Computation time: 1.1720070838928223\n",
      "Step: 801, Loss: 0.916176974773407, Accuracy: 1.0, Computation time: 1.2915949821472168\n",
      "Step: 802, Loss: 0.9391844272613525, Accuracy: 0.9791666865348816, Computation time: 1.5818772315979004\n",
      "Step: 803, Loss: 0.9163987040519714, Accuracy: 1.0, Computation time: 1.1924889087677002\n",
      "Step: 804, Loss: 0.9361254572868347, Accuracy: 0.9821428656578064, Computation time: 1.5020875930786133\n",
      "Step: 805, Loss: 0.9360103607177734, Accuracy: 0.9750000238418579, Computation time: 2.0906982421875\n",
      "Step: 806, Loss: 0.9196534752845764, Accuracy: 1.0, Computation time: 1.581028699874878\n",
      "Step: 807, Loss: 0.9160925149917603, Accuracy: 1.0, Computation time: 1.1914317607879639\n",
      "Step: 808, Loss: 0.9176751375198364, Accuracy: 1.0, Computation time: 1.7505180835723877\n",
      "Step: 809, Loss: 0.9161244034767151, Accuracy: 1.0, Computation time: 1.7135334014892578\n",
      "Step: 810, Loss: 0.9160391688346863, Accuracy: 1.0, Computation time: 1.0739772319793701\n",
      "Step: 811, Loss: 0.9269290566444397, Accuracy: 1.0, Computation time: 1.6733763217926025\n",
      "Step: 812, Loss: 0.9368539452552795, Accuracy: 0.9791666865348816, Computation time: 1.951350212097168\n",
      "Step: 813, Loss: 0.9159604907035828, Accuracy: 1.0, Computation time: 1.5341763496398926\n",
      "Step: 814, Loss: 0.9437360167503357, Accuracy: 0.9722222089767456, Computation time: 1.3164923191070557\n",
      "Step: 815, Loss: 0.9159868359565735, Accuracy: 1.0, Computation time: 1.6659212112426758\n",
      "Step: 816, Loss: 0.9220499396324158, Accuracy: 1.0, Computation time: 1.3603575229644775\n",
      "Step: 817, Loss: 0.9162656664848328, Accuracy: 1.0, Computation time: 2.270096778869629\n",
      "Step: 818, Loss: 0.9161522388458252, Accuracy: 1.0, Computation time: 1.4915156364440918\n",
      "Step: 819, Loss: 0.9161306023597717, Accuracy: 1.0, Computation time: 1.157520055770874\n",
      "Step: 820, Loss: 0.9161369800567627, Accuracy: 1.0, Computation time: 2.0317516326904297\n",
      "Step: 821, Loss: 0.9379051327705383, Accuracy: 0.9750000238418579, Computation time: 1.294438123703003\n",
      "Step: 822, Loss: 0.9161292314529419, Accuracy: 1.0, Computation time: 1.314296007156372\n",
      "Step: 823, Loss: 0.9159582257270813, Accuracy: 1.0, Computation time: 1.0487921237945557\n",
      "Step: 824, Loss: 0.9159475564956665, Accuracy: 1.0, Computation time: 1.665877342224121\n",
      "Step: 825, Loss: 0.9246828556060791, Accuracy: 1.0, Computation time: 1.2835814952850342\n",
      "Step: 826, Loss: 0.9538435935974121, Accuracy: 0.949999988079071, Computation time: 1.2977700233459473\n",
      "Step: 827, Loss: 0.9166187644004822, Accuracy: 1.0, Computation time: 1.3577396869659424\n",
      "Step: 828, Loss: 0.929442286491394, Accuracy: 0.9750000238418579, Computation time: 1.5471937656402588\n",
      "Step: 829, Loss: 0.9160894155502319, Accuracy: 1.0, Computation time: 1.1921286582946777\n",
      "Step: 830, Loss: 0.9162167310714722, Accuracy: 1.0, Computation time: 1.4100735187530518\n",
      "Step: 831, Loss: 0.9161422848701477, Accuracy: 1.0, Computation time: 1.4344663619995117\n",
      "Step: 832, Loss: 0.9160237312316895, Accuracy: 1.0, Computation time: 1.4460339546203613\n",
      "Step: 833, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.283076286315918\n",
      "Step: 834, Loss: 0.9161646366119385, Accuracy: 1.0, Computation time: 1.5249660015106201\n",
      "########################\n",
      "Test loss: 1.0705718994140625, Test Accuracy_epoch6: 0.7919411659240723\n",
      "########################\n",
      "Step: 835, Loss: 0.9172837138175964, Accuracy: 1.0, Computation time: 1.1890621185302734\n",
      "Step: 836, Loss: 0.9161476492881775, Accuracy: 1.0, Computation time: 1.8234326839447021\n",
      "Step: 837, Loss: 0.9160377979278564, Accuracy: 1.0, Computation time: 1.369511365890503\n",
      "Step: 838, Loss: 0.9243162274360657, Accuracy: 1.0, Computation time: 1.4296650886535645\n",
      "Step: 839, Loss: 0.9364440441131592, Accuracy: 0.96875, Computation time: 1.2793846130371094\n",
      "Step: 840, Loss: 0.91619873046875, Accuracy: 1.0, Computation time: 1.557938814163208\n",
      "Step: 841, Loss: 0.9166867136955261, Accuracy: 1.0, Computation time: 1.1713738441467285\n",
      "Step: 842, Loss: 0.9159547686576843, Accuracy: 1.0, Computation time: 1.4181289672851562\n",
      "Step: 843, Loss: 0.91611647605896, Accuracy: 1.0, Computation time: 1.6707346439361572\n",
      "Step: 844, Loss: 0.915897786617279, Accuracy: 1.0, Computation time: 1.5358984470367432\n",
      "Step: 845, Loss: 0.9159458875656128, Accuracy: 1.0, Computation time: 1.1330554485321045\n",
      "Step: 846, Loss: 0.9160042405128479, Accuracy: 1.0, Computation time: 1.5120739936828613\n",
      "Step: 847, Loss: 0.9307245016098022, Accuracy: 0.9772727489471436, Computation time: 1.2500789165496826\n",
      "Step: 848, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.4347286224365234\n",
      "Step: 849, Loss: 0.9206956624984741, Accuracy: 1.0, Computation time: 1.204468011856079\n",
      "Step: 850, Loss: 0.9162797331809998, Accuracy: 1.0, Computation time: 1.961806058883667\n",
      "Step: 851, Loss: 0.9395305514335632, Accuracy: 0.9772727489471436, Computation time: 1.851205587387085\n",
      "Step: 852, Loss: 0.9160021543502808, Accuracy: 1.0, Computation time: 1.2145326137542725\n",
      "Step: 853, Loss: 0.9159757494926453, Accuracy: 1.0, Computation time: 1.4423539638519287\n",
      "Step: 854, Loss: 0.9159246683120728, Accuracy: 1.0, Computation time: 1.4401705265045166\n",
      "Step: 855, Loss: 0.915954053401947, Accuracy: 1.0, Computation time: 1.4781906604766846\n",
      "Step: 856, Loss: 0.9304463267326355, Accuracy: 0.9772727489471436, Computation time: 1.4103097915649414\n",
      "Step: 857, Loss: 0.9377679228782654, Accuracy: 0.9722222089767456, Computation time: 1.3745734691619873\n",
      "Step: 858, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.407597541809082\n",
      "Step: 859, Loss: 0.915918231010437, Accuracy: 1.0, Computation time: 1.3710074424743652\n",
      "Step: 860, Loss: 0.9159049987792969, Accuracy: 1.0, Computation time: 1.6066222190856934\n",
      "Step: 861, Loss: 0.9159916639328003, Accuracy: 1.0, Computation time: 1.3977162837982178\n",
      "Step: 862, Loss: 0.9159463047981262, Accuracy: 1.0, Computation time: 1.2976315021514893\n",
      "Step: 863, Loss: 0.9161587357521057, Accuracy: 1.0, Computation time: 1.4636142253875732\n",
      "Step: 864, Loss: 0.9423844218254089, Accuracy: 0.9642857313156128, Computation time: 1.376063585281372\n",
      "Step: 865, Loss: 0.9564551711082458, Accuracy: 0.9666666984558105, Computation time: 1.5507431030273438\n",
      "Step: 866, Loss: 0.9163584113121033, Accuracy: 1.0, Computation time: 1.3164644241333008\n",
      "Step: 867, Loss: 0.9159539341926575, Accuracy: 1.0, Computation time: 1.278095006942749\n",
      "Step: 868, Loss: 0.93772953748703, Accuracy: 0.96875, Computation time: 1.409132719039917\n",
      "Step: 869, Loss: 0.9159824252128601, Accuracy: 1.0, Computation time: 1.6774165630340576\n",
      "Step: 870, Loss: 0.916687548160553, Accuracy: 1.0, Computation time: 1.578737735748291\n",
      "Step: 871, Loss: 0.9161138534545898, Accuracy: 1.0, Computation time: 1.6474690437316895\n",
      "Step: 872, Loss: 0.9159931540489197, Accuracy: 1.0, Computation time: 1.942701816558838\n",
      "Step: 873, Loss: 0.916016697883606, Accuracy: 1.0, Computation time: 1.4945261478424072\n",
      "Step: 874, Loss: 0.9538018107414246, Accuracy: 0.9580419659614563, Computation time: 2.2815213203430176\n",
      "Step: 875, Loss: 0.915887713432312, Accuracy: 1.0, Computation time: 1.1791958808898926\n",
      "Step: 876, Loss: 0.9159627556800842, Accuracy: 1.0, Computation time: 1.1953628063201904\n",
      "Step: 877, Loss: 0.9160617589950562, Accuracy: 1.0, Computation time: 1.176877737045288\n",
      "Step: 878, Loss: 0.9160422682762146, Accuracy: 1.0, Computation time: 1.270946979522705\n",
      "Step: 879, Loss: 0.9204288125038147, Accuracy: 1.0, Computation time: 1.211038589477539\n",
      "Step: 880, Loss: 0.9160519242286682, Accuracy: 1.0, Computation time: 1.2190380096435547\n",
      "Step: 881, Loss: 0.9377036690711975, Accuracy: 0.9791666865348816, Computation time: 1.1226165294647217\n",
      "Step: 882, Loss: 0.9159938097000122, Accuracy: 1.0, Computation time: 1.2561061382293701\n",
      "Step: 883, Loss: 0.9363107085227966, Accuracy: 0.9642857313156128, Computation time: 1.3162846565246582\n",
      "Step: 884, Loss: 0.9158653616905212, Accuracy: 1.0, Computation time: 1.3559613227844238\n",
      "Step: 885, Loss: 0.9351720809936523, Accuracy: 0.9722222089767456, Computation time: 1.2831482887268066\n",
      "Step: 886, Loss: 0.9401013851165771, Accuracy: 0.9642857313156128, Computation time: 1.3181805610656738\n",
      "Step: 887, Loss: 0.915987491607666, Accuracy: 1.0, Computation time: 1.3834538459777832\n",
      "Step: 888, Loss: 0.916062593460083, Accuracy: 1.0, Computation time: 1.4110195636749268\n",
      "Step: 889, Loss: 0.9161303043365479, Accuracy: 1.0, Computation time: 1.2985436916351318\n",
      "Step: 890, Loss: 0.9159395694732666, Accuracy: 1.0, Computation time: 1.3303194046020508\n",
      "Step: 891, Loss: 0.9162712097167969, Accuracy: 1.0, Computation time: 1.3030030727386475\n",
      "Step: 892, Loss: 0.9159696102142334, Accuracy: 1.0, Computation time: 1.903977394104004\n",
      "Step: 893, Loss: 0.9165880084037781, Accuracy: 1.0, Computation time: 1.3213210105895996\n",
      "Step: 894, Loss: 0.9164784550666809, Accuracy: 1.0, Computation time: 1.229170560836792\n",
      "Step: 895, Loss: 0.9159693717956543, Accuracy: 1.0, Computation time: 1.3756623268127441\n",
      "Step: 896, Loss: 0.9158754944801331, Accuracy: 1.0, Computation time: 1.2601768970489502\n",
      "Step: 897, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 1.5639798641204834\n",
      "Step: 898, Loss: 0.9159308671951294, Accuracy: 1.0, Computation time: 1.0691392421722412\n",
      "Step: 899, Loss: 0.916353166103363, Accuracy: 1.0, Computation time: 1.8773341178894043\n",
      "Step: 900, Loss: 0.9160680770874023, Accuracy: 1.0, Computation time: 1.2887895107269287\n",
      "Step: 901, Loss: 0.915903627872467, Accuracy: 1.0, Computation time: 1.3369462490081787\n",
      "Step: 902, Loss: 0.9375554323196411, Accuracy: 0.9722222089767456, Computation time: 1.4400854110717773\n",
      "Step: 903, Loss: 0.9159163236618042, Accuracy: 1.0, Computation time: 1.3609168529510498\n",
      "Step: 904, Loss: 0.9253861904144287, Accuracy: 0.96875, Computation time: 1.5845284461975098\n",
      "Step: 905, Loss: 0.916050910949707, Accuracy: 1.0, Computation time: 1.107785701751709\n",
      "Step: 906, Loss: 0.9377446174621582, Accuracy: 0.9821428656578064, Computation time: 1.2601203918457031\n",
      "Step: 907, Loss: 0.9372798800468445, Accuracy: 0.9642857313156128, Computation time: 1.6025736331939697\n",
      "Step: 908, Loss: 0.9160635471343994, Accuracy: 1.0, Computation time: 1.3053326606750488\n",
      "Step: 909, Loss: 0.9266583323478699, Accuracy: 0.9583333730697632, Computation time: 1.471118450164795\n",
      "Step: 910, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.4064948558807373\n",
      "Step: 911, Loss: 0.9159539937973022, Accuracy: 1.0, Computation time: 1.5547490119934082\n",
      "Step: 912, Loss: 0.9165428876876831, Accuracy: 1.0, Computation time: 1.9472248554229736\n",
      "Step: 913, Loss: 0.9159858226776123, Accuracy: 1.0, Computation time: 1.5830669403076172\n",
      "Step: 914, Loss: 0.9159400463104248, Accuracy: 1.0, Computation time: 1.6356313228607178\n",
      "Step: 915, Loss: 0.9343748688697815, Accuracy: 0.9583333730697632, Computation time: 1.586341142654419\n",
      "Step: 916, Loss: 0.9162176251411438, Accuracy: 1.0, Computation time: 1.5186851024627686\n",
      "Step: 917, Loss: 0.9160407185554504, Accuracy: 1.0, Computation time: 1.2764849662780762\n",
      "Step: 918, Loss: 0.9159224033355713, Accuracy: 1.0, Computation time: 1.554304838180542\n",
      "Step: 919, Loss: 0.9161695241928101, Accuracy: 1.0, Computation time: 1.3666861057281494\n",
      "Step: 920, Loss: 0.9317062497138977, Accuracy: 0.9807692766189575, Computation time: 1.544320821762085\n",
      "Step: 921, Loss: 0.9291545152664185, Accuracy: 0.9791666865348816, Computation time: 1.6539506912231445\n",
      "Step: 922, Loss: 0.9160305261611938, Accuracy: 1.0, Computation time: 1.0703387260437012\n",
      "Step: 923, Loss: 0.9162384867668152, Accuracy: 1.0, Computation time: 1.5985684394836426\n",
      "Step: 924, Loss: 0.9160701036453247, Accuracy: 1.0, Computation time: 1.389533281326294\n",
      "Step: 925, Loss: 0.9386393427848816, Accuracy: 0.9791666865348816, Computation time: 1.7491199970245361\n",
      "Step: 926, Loss: 0.9433207511901855, Accuracy: 0.9791666865348816, Computation time: 1.5357553958892822\n",
      "Step: 927, Loss: 0.9394364356994629, Accuracy: 0.9807692766189575, Computation time: 1.3046174049377441\n",
      "Step: 928, Loss: 0.9165814518928528, Accuracy: 1.0, Computation time: 1.3903818130493164\n",
      "Step: 929, Loss: 0.9161685109138489, Accuracy: 1.0, Computation time: 1.227975606918335\n",
      "Step: 930, Loss: 0.9177845120429993, Accuracy: 1.0, Computation time: 1.2405102252960205\n",
      "Step: 931, Loss: 0.9161169528961182, Accuracy: 1.0, Computation time: 1.323171615600586\n",
      "Step: 932, Loss: 0.9190865159034729, Accuracy: 1.0, Computation time: 1.6619081497192383\n",
      "Step: 933, Loss: 0.9160044193267822, Accuracy: 1.0, Computation time: 1.4095921516418457\n",
      "Step: 934, Loss: 0.9168342351913452, Accuracy: 1.0, Computation time: 1.4635322093963623\n",
      "Step: 935, Loss: 0.9160296320915222, Accuracy: 1.0, Computation time: 1.2556450366973877\n",
      "Step: 936, Loss: 0.9159559011459351, Accuracy: 1.0, Computation time: 1.194763422012329\n",
      "Step: 937, Loss: 0.9344704151153564, Accuracy: 0.9642857313156128, Computation time: 1.4433560371398926\n",
      "Step: 938, Loss: 0.9159420132637024, Accuracy: 1.0, Computation time: 1.3410084247589111\n",
      "Step: 939, Loss: 0.9159925580024719, Accuracy: 1.0, Computation time: 1.527113914489746\n",
      "Step: 940, Loss: 0.9160068035125732, Accuracy: 1.0, Computation time: 1.4789106845855713\n",
      "Step: 941, Loss: 0.9165346622467041, Accuracy: 1.0, Computation time: 1.2615666389465332\n",
      "Step: 942, Loss: 0.916113018989563, Accuracy: 1.0, Computation time: 1.4044439792633057\n",
      "Step: 943, Loss: 0.9161382913589478, Accuracy: 1.0, Computation time: 1.4591166973114014\n",
      "Step: 944, Loss: 0.9161069393157959, Accuracy: 1.0, Computation time: 1.320322036743164\n",
      "Step: 945, Loss: 0.9160624146461487, Accuracy: 1.0, Computation time: 1.5691161155700684\n",
      "Step: 946, Loss: 0.9347981214523315, Accuracy: 0.9722222089767456, Computation time: 1.4171466827392578\n",
      "Step: 947, Loss: 0.9160324335098267, Accuracy: 1.0, Computation time: 1.5096726417541504\n",
      "Step: 948, Loss: 0.9240282773971558, Accuracy: 1.0, Computation time: 1.407414197921753\n",
      "Step: 949, Loss: 0.9179911017417908, Accuracy: 1.0, Computation time: 1.3928024768829346\n",
      "Step: 950, Loss: 0.9159733653068542, Accuracy: 1.0, Computation time: 1.8484363555908203\n",
      "Step: 951, Loss: 0.9160739779472351, Accuracy: 1.0, Computation time: 1.3322677612304688\n",
      "Step: 952, Loss: 0.9194521307945251, Accuracy: 1.0, Computation time: 1.5348141193389893\n",
      "Step: 953, Loss: 0.916021466255188, Accuracy: 1.0, Computation time: 1.2283453941345215\n",
      "Step: 954, Loss: 0.9160444140434265, Accuracy: 1.0, Computation time: 1.2928626537322998\n",
      "Step: 955, Loss: 0.9161490797996521, Accuracy: 1.0, Computation time: 1.1756532192230225\n",
      "Step: 956, Loss: 0.9335160851478577, Accuracy: 0.9807692766189575, Computation time: 1.4601376056671143\n",
      "Step: 957, Loss: 0.9160630702972412, Accuracy: 1.0, Computation time: 1.1564490795135498\n",
      "Step: 958, Loss: 0.9360966086387634, Accuracy: 0.9583333730697632, Computation time: 1.2904980182647705\n",
      "Step: 959, Loss: 0.916064441204071, Accuracy: 1.0, Computation time: 1.0998151302337646\n",
      "Step: 960, Loss: 0.9160136580467224, Accuracy: 1.0, Computation time: 1.3112163543701172\n",
      "Step: 961, Loss: 0.9163156747817993, Accuracy: 1.0, Computation time: 1.612680196762085\n",
      "Step: 962, Loss: 0.9181002378463745, Accuracy: 1.0, Computation time: 1.8148996829986572\n",
      "Step: 963, Loss: 0.9160308837890625, Accuracy: 1.0, Computation time: 1.2536003589630127\n",
      "Step: 964, Loss: 0.9160170555114746, Accuracy: 1.0, Computation time: 1.191075325012207\n",
      "Step: 965, Loss: 0.9162240624427795, Accuracy: 1.0, Computation time: 1.4941606521606445\n",
      "Step: 966, Loss: 0.9375991225242615, Accuracy: 0.9791666865348816, Computation time: 1.344494104385376\n",
      "Step: 967, Loss: 0.9160988330841064, Accuracy: 1.0, Computation time: 1.4650440216064453\n",
      "Step: 968, Loss: 0.9160154461860657, Accuracy: 1.0, Computation time: 1.3636894226074219\n",
      "Step: 969, Loss: 0.9160195589065552, Accuracy: 1.0, Computation time: 1.2438101768493652\n",
      "Step: 970, Loss: 0.915958046913147, Accuracy: 1.0, Computation time: 1.4228570461273193\n",
      "Step: 971, Loss: 0.9159575700759888, Accuracy: 1.0, Computation time: 1.2497491836547852\n",
      "Step: 972, Loss: 0.917449414730072, Accuracy: 1.0, Computation time: 1.683727502822876\n",
      "Step: 973, Loss: 0.9166108965873718, Accuracy: 1.0, Computation time: 1.5751633644104004\n",
      "########################\n",
      "Test loss: 1.0632023811340332, Test Accuracy_epoch7: 0.7927076816558838\n",
      "########################\n",
      "Step: 974, Loss: 0.916450023651123, Accuracy: 1.0, Computation time: 1.4088561534881592\n",
      "Step: 975, Loss: 0.9306856393814087, Accuracy: 0.9772727489471436, Computation time: 1.4957404136657715\n",
      "Step: 976, Loss: 0.9292394518852234, Accuracy: 0.9750000238418579, Computation time: 1.9242541790008545\n",
      "Step: 977, Loss: 0.9160237312316895, Accuracy: 1.0, Computation time: 1.1895604133605957\n",
      "Step: 978, Loss: 0.9163627028465271, Accuracy: 1.0, Computation time: 1.4713685512542725\n",
      "Step: 979, Loss: 0.9161038398742676, Accuracy: 1.0, Computation time: 1.9910614490509033\n",
      "Step: 980, Loss: 0.9178777933120728, Accuracy: 1.0, Computation time: 1.4423165321350098\n",
      "Step: 981, Loss: 0.9160647988319397, Accuracy: 1.0, Computation time: 2.2164013385772705\n",
      "Step: 982, Loss: 0.916427493095398, Accuracy: 1.0, Computation time: 1.1866185665130615\n",
      "Step: 983, Loss: 0.9161153435707092, Accuracy: 1.0, Computation time: 1.1474525928497314\n",
      "Step: 984, Loss: 0.9160311818122864, Accuracy: 1.0, Computation time: 1.2947888374328613\n",
      "Step: 985, Loss: 0.9161093831062317, Accuracy: 1.0, Computation time: 1.3635549545288086\n",
      "Step: 986, Loss: 0.9159941077232361, Accuracy: 1.0, Computation time: 1.3400676250457764\n",
      "Step: 987, Loss: 0.9176744818687439, Accuracy: 1.0, Computation time: 1.197127342224121\n",
      "Step: 988, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 1.2487318515777588\n",
      "Step: 989, Loss: 0.9158896803855896, Accuracy: 1.0, Computation time: 1.4308624267578125\n",
      "Step: 990, Loss: 0.9159236550331116, Accuracy: 1.0, Computation time: 1.257370948791504\n",
      "Step: 991, Loss: 0.9159243106842041, Accuracy: 1.0, Computation time: 1.3143680095672607\n",
      "Step: 992, Loss: 0.9158892035484314, Accuracy: 1.0, Computation time: 1.277132272720337\n",
      "Step: 993, Loss: 0.9158964157104492, Accuracy: 1.0, Computation time: 1.261129379272461\n",
      "Step: 994, Loss: 0.9159022569656372, Accuracy: 1.0, Computation time: 1.339867115020752\n",
      "Step: 995, Loss: 0.91595858335495, Accuracy: 1.0, Computation time: 1.3743247985839844\n",
      "Step: 996, Loss: 0.9170003533363342, Accuracy: 1.0, Computation time: 1.1913340091705322\n",
      "Step: 997, Loss: 0.9159493446350098, Accuracy: 1.0, Computation time: 1.1644902229309082\n",
      "Step: 998, Loss: 0.9192994236946106, Accuracy: 1.0, Computation time: 2.0833349227905273\n",
      "Step: 999, Loss: 0.9159498810768127, Accuracy: 1.0, Computation time: 1.3947420120239258\n",
      "Step: 1000, Loss: 0.9336614012718201, Accuracy: 0.9772727489471436, Computation time: 1.3173301219940186\n",
      "Step: 1001, Loss: 0.9214372038841248, Accuracy: 1.0, Computation time: 1.721336841583252\n",
      "Step: 1002, Loss: 0.9376962184906006, Accuracy: 0.9642857313156128, Computation time: 1.5782296657562256\n",
      "Step: 1003, Loss: 0.9161631464958191, Accuracy: 1.0, Computation time: 1.5609333515167236\n",
      "Step: 1004, Loss: 0.9380378127098083, Accuracy: 0.9772727489471436, Computation time: 1.562687873840332\n",
      "Step: 1005, Loss: 0.9161171317100525, Accuracy: 1.0, Computation time: 1.35514497756958\n",
      "Step: 1006, Loss: 0.9162353873252869, Accuracy: 1.0, Computation time: 1.7801775932312012\n",
      "Step: 1007, Loss: 0.9160807728767395, Accuracy: 1.0, Computation time: 1.1597249507904053\n",
      "Step: 1008, Loss: 0.9215267300605774, Accuracy: 1.0, Computation time: 1.719022274017334\n",
      "Step: 1009, Loss: 0.9159930348396301, Accuracy: 1.0, Computation time: 1.1315035820007324\n",
      "Step: 1010, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.132190227508545\n",
      "Step: 1011, Loss: 0.9159561395645142, Accuracy: 1.0, Computation time: 1.0806894302368164\n",
      "Step: 1012, Loss: 0.9377323985099792, Accuracy: 0.96875, Computation time: 1.3363449573516846\n",
      "Step: 1013, Loss: 0.9187655448913574, Accuracy: 1.0, Computation time: 2.023461103439331\n",
      "Step: 1014, Loss: 0.9317849278450012, Accuracy: 0.9750000238418579, Computation time: 1.1466269493103027\n",
      "Step: 1015, Loss: 0.9161021113395691, Accuracy: 1.0, Computation time: 1.1469886302947998\n",
      "Step: 1016, Loss: 0.9212591648101807, Accuracy: 1.0, Computation time: 1.6765077114105225\n",
      "Step: 1017, Loss: 0.9165105223655701, Accuracy: 1.0, Computation time: 1.6849257946014404\n",
      "Step: 1018, Loss: 0.9162243008613586, Accuracy: 1.0, Computation time: 1.3046305179595947\n",
      "Step: 1019, Loss: 0.9163568615913391, Accuracy: 1.0, Computation time: 1.4860999584197998\n",
      "Step: 1020, Loss: 0.9378699660301208, Accuracy: 0.96875, Computation time: 2.5134146213531494\n",
      "Step: 1021, Loss: 0.9162546992301941, Accuracy: 1.0, Computation time: 1.532022476196289\n",
      "Step: 1022, Loss: 0.948365330696106, Accuracy: 0.9494949579238892, Computation time: 1.6603844165802002\n",
      "Step: 1023, Loss: 0.9180445075035095, Accuracy: 1.0, Computation time: 1.2065258026123047\n",
      "Step: 1024, Loss: 0.920132040977478, Accuracy: 1.0, Computation time: 1.9229910373687744\n",
      "Step: 1025, Loss: 0.9162054061889648, Accuracy: 1.0, Computation time: 1.3278541564941406\n",
      "Step: 1026, Loss: 0.9180996417999268, Accuracy: 1.0, Computation time: 1.3481850624084473\n",
      "Step: 1027, Loss: 0.916703462600708, Accuracy: 1.0, Computation time: 1.4130198955535889\n",
      "Step: 1028, Loss: 0.9166291952133179, Accuracy: 1.0, Computation time: 1.2261302471160889\n",
      "Step: 1029, Loss: 0.9264918565750122, Accuracy: 0.9750000238418579, Computation time: 1.3938026428222656\n",
      "Step: 1030, Loss: 0.9163681864738464, Accuracy: 1.0, Computation time: 1.0736336708068848\n",
      "Step: 1031, Loss: 0.9163451194763184, Accuracy: 1.0, Computation time: 1.3020637035369873\n",
      "Step: 1032, Loss: 0.9166896939277649, Accuracy: 1.0, Computation time: 1.2397966384887695\n",
      "Step: 1033, Loss: 0.9376489520072937, Accuracy: 0.9750000238418579, Computation time: 1.1172261238098145\n",
      "Step: 1034, Loss: 0.9160345792770386, Accuracy: 1.0, Computation time: 1.2079172134399414\n",
      "Step: 1035, Loss: 0.9160827994346619, Accuracy: 1.0, Computation time: 1.4507949352264404\n",
      "Step: 1036, Loss: 0.9163241982460022, Accuracy: 1.0, Computation time: 1.351872205734253\n",
      "Step: 1037, Loss: 0.9162735939025879, Accuracy: 1.0, Computation time: 1.1835150718688965\n",
      "Step: 1038, Loss: 0.9161666035652161, Accuracy: 1.0, Computation time: 1.464359998703003\n",
      "Step: 1039, Loss: 0.9161710143089294, Accuracy: 1.0, Computation time: 1.135288953781128\n",
      "Step: 1040, Loss: 0.9163630604743958, Accuracy: 1.0, Computation time: 1.329810619354248\n",
      "Step: 1041, Loss: 0.916094183921814, Accuracy: 1.0, Computation time: 1.2134888172149658\n",
      "Step: 1042, Loss: 0.9308810830116272, Accuracy: 0.9750000238418579, Computation time: 1.5705344676971436\n",
      "Step: 1043, Loss: 0.9159786105155945, Accuracy: 1.0, Computation time: 1.2151002883911133\n",
      "Step: 1044, Loss: 0.9187951683998108, Accuracy: 1.0, Computation time: 1.2454311847686768\n",
      "Step: 1045, Loss: 0.9160453081130981, Accuracy: 1.0, Computation time: 1.3634521961212158\n",
      "Step: 1046, Loss: 0.916042149066925, Accuracy: 1.0, Computation time: 1.2909066677093506\n",
      "Step: 1047, Loss: 0.9161728620529175, Accuracy: 1.0, Computation time: 1.4000577926635742\n",
      "Step: 1048, Loss: 0.9167596697807312, Accuracy: 1.0, Computation time: 1.4531316757202148\n",
      "Step: 1049, Loss: 0.9173414707183838, Accuracy: 1.0, Computation time: 1.6936447620391846\n",
      "Step: 1050, Loss: 0.9161155223846436, Accuracy: 1.0, Computation time: 1.2429296970367432\n",
      "Step: 1051, Loss: 0.9188510179519653, Accuracy: 1.0, Computation time: 1.3819475173950195\n",
      "Step: 1052, Loss: 0.9160775542259216, Accuracy: 1.0, Computation time: 1.3103365898132324\n",
      "Step: 1053, Loss: 0.9374536275863647, Accuracy: 0.9642857313156128, Computation time: 1.227426528930664\n",
      "Step: 1054, Loss: 0.915916383266449, Accuracy: 1.0, Computation time: 1.2345705032348633\n",
      "Step: 1055, Loss: 0.9159891605377197, Accuracy: 1.0, Computation time: 1.2368183135986328\n",
      "Step: 1056, Loss: 0.9162664413452148, Accuracy: 1.0, Computation time: 1.4382197856903076\n",
      "Step: 1057, Loss: 0.9376754760742188, Accuracy: 0.9722222089767456, Computation time: 2.0634522438049316\n",
      "Step: 1058, Loss: 0.9376665949821472, Accuracy: 0.9772727489471436, Computation time: 1.1998631954193115\n",
      "Step: 1059, Loss: 0.9160507321357727, Accuracy: 1.0, Computation time: 1.4086310863494873\n",
      "Step: 1060, Loss: 0.9186335206031799, Accuracy: 1.0, Computation time: 1.419797658920288\n",
      "Step: 1061, Loss: 0.9159647226333618, Accuracy: 1.0, Computation time: 1.160468578338623\n",
      "Step: 1062, Loss: 0.9374939799308777, Accuracy: 0.9750000238418579, Computation time: 1.30226731300354\n",
      "Step: 1063, Loss: 0.9159406423568726, Accuracy: 1.0, Computation time: 1.2779583930969238\n",
      "Step: 1064, Loss: 0.9159536957740784, Accuracy: 1.0, Computation time: 1.680325984954834\n",
      "Step: 1065, Loss: 0.9274579286575317, Accuracy: 0.9750000238418579, Computation time: 1.4256057739257812\n",
      "Step: 1066, Loss: 0.9361802339553833, Accuracy: 0.9807692766189575, Computation time: 1.3926663398742676\n",
      "Step: 1067, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 1.3494186401367188\n",
      "Step: 1068, Loss: 0.9159820079803467, Accuracy: 1.0, Computation time: 1.3838849067687988\n",
      "Step: 1069, Loss: 0.915952742099762, Accuracy: 1.0, Computation time: 1.3559415340423584\n",
      "Step: 1070, Loss: 0.9377281665802002, Accuracy: 0.96875, Computation time: 1.6893551349639893\n",
      "Step: 1071, Loss: 0.9162060022354126, Accuracy: 1.0, Computation time: 1.3839235305786133\n",
      "Step: 1072, Loss: 0.9161445498466492, Accuracy: 1.0, Computation time: 1.7931163311004639\n",
      "Step: 1073, Loss: 0.9160950183868408, Accuracy: 1.0, Computation time: 1.2685356140136719\n",
      "Step: 1074, Loss: 0.9159071445465088, Accuracy: 1.0, Computation time: 1.1875452995300293\n",
      "Step: 1075, Loss: 0.9172896146774292, Accuracy: 1.0, Computation time: 1.6086268424987793\n",
      "Step: 1076, Loss: 0.9248345494270325, Accuracy: 1.0, Computation time: 1.5334064960479736\n",
      "Step: 1077, Loss: 0.917508602142334, Accuracy: 1.0, Computation time: 1.434356451034546\n",
      "Step: 1078, Loss: 0.9375728368759155, Accuracy: 0.96875, Computation time: 1.7520177364349365\n",
      "Step: 1079, Loss: 0.9316768646240234, Accuracy: 0.949999988079071, Computation time: 1.3974175453186035\n",
      "Step: 1080, Loss: 0.9160177111625671, Accuracy: 1.0, Computation time: 1.6056208610534668\n",
      "Step: 1081, Loss: 0.9378166198730469, Accuracy: 0.9583333730697632, Computation time: 1.166360855102539\n",
      "Step: 1082, Loss: 0.915961742401123, Accuracy: 1.0, Computation time: 1.2141838073730469\n",
      "Step: 1083, Loss: 0.9371570348739624, Accuracy: 0.9583333730697632, Computation time: 1.269256830215454\n",
      "Step: 1084, Loss: 0.9168208241462708, Accuracy: 1.0, Computation time: 1.6759779453277588\n",
      "Step: 1085, Loss: 0.9159925580024719, Accuracy: 1.0, Computation time: 1.386272668838501\n",
      "Step: 1086, Loss: 0.9269103407859802, Accuracy: 0.9791666865348816, Computation time: 1.5677497386932373\n",
      "Step: 1087, Loss: 0.9167296886444092, Accuracy: 1.0, Computation time: 1.1847190856933594\n",
      "Step: 1088, Loss: 0.9161116480827332, Accuracy: 1.0, Computation time: 1.479278564453125\n",
      "Step: 1089, Loss: 0.9160209894180298, Accuracy: 1.0, Computation time: 1.1535873413085938\n",
      "Step: 1090, Loss: 0.9338838458061218, Accuracy: 0.9821428656578064, Computation time: 1.2931733131408691\n",
      "Step: 1091, Loss: 0.9162245392799377, Accuracy: 1.0, Computation time: 1.251391887664795\n",
      "Step: 1092, Loss: 0.9290374517440796, Accuracy: 0.9807692766189575, Computation time: 1.946704626083374\n",
      "Step: 1093, Loss: 0.9165881276130676, Accuracy: 1.0, Computation time: 1.4002573490142822\n",
      "Step: 1094, Loss: 0.9463905096054077, Accuracy: 0.9375, Computation time: 1.5012199878692627\n",
      "Step: 1095, Loss: 0.9159253835678101, Accuracy: 1.0, Computation time: 1.1698963642120361\n",
      "Step: 1096, Loss: 0.9172448515892029, Accuracy: 1.0, Computation time: 1.8649959564208984\n",
      "Step: 1097, Loss: 0.9230615496635437, Accuracy: 1.0, Computation time: 1.3142578601837158\n",
      "Step: 1098, Loss: 0.9159821271896362, Accuracy: 1.0, Computation time: 1.394634485244751\n",
      "Step: 1099, Loss: 0.915888249874115, Accuracy: 1.0, Computation time: 1.3198347091674805\n",
      "Step: 1100, Loss: 0.9159265160560608, Accuracy: 1.0, Computation time: 1.2587251663208008\n",
      "Step: 1101, Loss: 0.9159783124923706, Accuracy: 1.0, Computation time: 1.2615540027618408\n",
      "Step: 1102, Loss: 0.916305661201477, Accuracy: 1.0, Computation time: 1.4817168712615967\n",
      "Step: 1103, Loss: 0.9406927227973938, Accuracy: 0.9722222089767456, Computation time: 1.4012854099273682\n",
      "Step: 1104, Loss: 0.9320687055587769, Accuracy: 0.9722222089767456, Computation time: 1.155290126800537\n",
      "Step: 1105, Loss: 0.9563838839530945, Accuracy: 0.9125000238418579, Computation time: 1.47432541847229\n",
      "Step: 1106, Loss: 0.9161319136619568, Accuracy: 1.0, Computation time: 1.1812121868133545\n",
      "Step: 1107, Loss: 0.9167434573173523, Accuracy: 1.0, Computation time: 1.3582227230072021\n",
      "Step: 1108, Loss: 0.9217075705528259, Accuracy: 1.0, Computation time: 1.2569048404693604\n",
      "Step: 1109, Loss: 0.9161781072616577, Accuracy: 1.0, Computation time: 1.2626721858978271\n",
      "Step: 1110, Loss: 0.9167155623435974, Accuracy: 1.0, Computation time: 1.50376296043396\n",
      "Step: 1111, Loss: 0.9192860126495361, Accuracy: 1.0, Computation time: 2.0463368892669678\n",
      "Step: 1112, Loss: 0.9162154793739319, Accuracy: 1.0, Computation time: 1.2316765785217285\n",
      "########################\n",
      "Test loss: 1.0727412700653076, Test Accuracy_epoch8: 0.7770373821258545\n",
      "########################\n",
      "Step: 1113, Loss: 0.9160994291305542, Accuracy: 1.0, Computation time: 1.37461519241333\n",
      "Step: 1114, Loss: 0.9161562323570251, Accuracy: 1.0, Computation time: 1.3475167751312256\n",
      "Step: 1115, Loss: 0.916628360748291, Accuracy: 1.0, Computation time: 1.8810808658599854\n",
      "Step: 1116, Loss: 0.9160435795783997, Accuracy: 1.0, Computation time: 1.5757272243499756\n",
      "Step: 1117, Loss: 0.9346840381622314, Accuracy: 0.9642857313156128, Computation time: 1.3584394454956055\n",
      "Step: 1118, Loss: 0.9161878228187561, Accuracy: 1.0, Computation time: 1.4538648128509521\n",
      "Step: 1119, Loss: 0.9160332083702087, Accuracy: 1.0, Computation time: 1.579963207244873\n",
      "Step: 1120, Loss: 0.9173649549484253, Accuracy: 1.0, Computation time: 1.2535452842712402\n",
      "Step: 1121, Loss: 0.9159528613090515, Accuracy: 1.0, Computation time: 1.3079192638397217\n",
      "Step: 1122, Loss: 0.9217694997787476, Accuracy: 1.0, Computation time: 1.3994932174682617\n",
      "Step: 1123, Loss: 0.9163563847541809, Accuracy: 1.0, Computation time: 1.3168375492095947\n",
      "Step: 1124, Loss: 0.9160103797912598, Accuracy: 1.0, Computation time: 1.2775206565856934\n",
      "Step: 1125, Loss: 0.9161231517791748, Accuracy: 1.0, Computation time: 1.2600464820861816\n",
      "Step: 1126, Loss: 0.9160332679748535, Accuracy: 1.0, Computation time: 1.3671014308929443\n",
      "Step: 1127, Loss: 0.930076539516449, Accuracy: 0.9642857313156128, Computation time: 1.5240983963012695\n",
      "Step: 1128, Loss: 0.9162324666976929, Accuracy: 1.0, Computation time: 1.2341558933258057\n",
      "Step: 1129, Loss: 0.9161257147789001, Accuracy: 1.0, Computation time: 1.5143673419952393\n",
      "Step: 1130, Loss: 0.9159481525421143, Accuracy: 1.0, Computation time: 1.1684670448303223\n",
      "Step: 1131, Loss: 0.9160144329071045, Accuracy: 1.0, Computation time: 1.2440955638885498\n",
      "Step: 1132, Loss: 0.9159972071647644, Accuracy: 1.0, Computation time: 1.5816404819488525\n",
      "Step: 1133, Loss: 0.9160720109939575, Accuracy: 1.0, Computation time: 1.6371676921844482\n",
      "Step: 1134, Loss: 0.9162918925285339, Accuracy: 1.0, Computation time: 1.1762492656707764\n",
      "Step: 1135, Loss: 0.9160021543502808, Accuracy: 1.0, Computation time: 1.5517208576202393\n",
      "Step: 1136, Loss: 0.9161794781684875, Accuracy: 1.0, Computation time: 1.3107738494873047\n",
      "Step: 1137, Loss: 0.9162416458129883, Accuracy: 1.0, Computation time: 1.6529884338378906\n",
      "Step: 1138, Loss: 0.9177960157394409, Accuracy: 1.0, Computation time: 1.533597707748413\n",
      "Step: 1139, Loss: 0.9382889270782471, Accuracy: 0.96875, Computation time: 1.665541648864746\n",
      "Step: 1140, Loss: 0.9159674644470215, Accuracy: 1.0, Computation time: 1.2782509326934814\n",
      "Step: 1141, Loss: 0.9193794131278992, Accuracy: 1.0, Computation time: 1.444828987121582\n",
      "Step: 1142, Loss: 0.9160419702529907, Accuracy: 1.0, Computation time: 1.4170241355895996\n",
      "Step: 1143, Loss: 0.9161953330039978, Accuracy: 1.0, Computation time: 1.1533005237579346\n",
      "Step: 1144, Loss: 0.9166403412818909, Accuracy: 1.0, Computation time: 1.5687150955200195\n",
      "Step: 1145, Loss: 0.916337251663208, Accuracy: 1.0, Computation time: 1.524501085281372\n",
      "Step: 1146, Loss: 0.9170935153961182, Accuracy: 1.0, Computation time: 2.2190635204315186\n",
      "Step: 1147, Loss: 0.9160658717155457, Accuracy: 1.0, Computation time: 1.3966140747070312\n",
      "Step: 1148, Loss: 0.9160069227218628, Accuracy: 1.0, Computation time: 1.2533369064331055\n",
      "Step: 1149, Loss: 0.9354761242866516, Accuracy: 0.9583333730697632, Computation time: 1.255897045135498\n",
      "Step: 1150, Loss: 0.9216823577880859, Accuracy: 1.0, Computation time: 2.225658655166626\n",
      "Step: 1151, Loss: 0.9162980318069458, Accuracy: 1.0, Computation time: 1.5057568550109863\n",
      "Step: 1152, Loss: 0.9168246984481812, Accuracy: 1.0, Computation time: 1.2884294986724854\n",
      "Step: 1153, Loss: 0.9166203737258911, Accuracy: 1.0, Computation time: 1.1988959312438965\n",
      "Step: 1154, Loss: 0.9169232249259949, Accuracy: 1.0, Computation time: 1.1784768104553223\n",
      "Step: 1155, Loss: 0.9166139364242554, Accuracy: 1.0, Computation time: 1.1766705513000488\n",
      "Step: 1156, Loss: 0.9164721965789795, Accuracy: 1.0, Computation time: 1.2818028926849365\n",
      "Step: 1157, Loss: 0.9162695407867432, Accuracy: 1.0, Computation time: 1.4800488948822021\n",
      "Step: 1158, Loss: 0.9567827582359314, Accuracy: 0.954365074634552, Computation time: 2.182056188583374\n",
      "Step: 1159, Loss: 0.9262703657150269, Accuracy: 0.9375, Computation time: 1.4880223274230957\n",
      "Step: 1160, Loss: 0.9161325097084045, Accuracy: 1.0, Computation time: 1.3808693885803223\n",
      "Step: 1161, Loss: 0.9162843823432922, Accuracy: 1.0, Computation time: 1.2817504405975342\n",
      "Step: 1162, Loss: 0.9170552492141724, Accuracy: 1.0, Computation time: 1.2659296989440918\n",
      "Step: 1163, Loss: 0.9166911244392395, Accuracy: 1.0, Computation time: 1.2853786945343018\n",
      "Step: 1164, Loss: 0.9583328366279602, Accuracy: 0.9615384340286255, Computation time: 1.6076107025146484\n",
      "Step: 1165, Loss: 0.9164317846298218, Accuracy: 1.0, Computation time: 1.3728983402252197\n",
      "Step: 1166, Loss: 0.9164565205574036, Accuracy: 1.0, Computation time: 1.4490680694580078\n",
      "Step: 1167, Loss: 0.9355872273445129, Accuracy: 0.9750000238418579, Computation time: 1.7147860527038574\n",
      "Step: 1168, Loss: 0.9168620109558105, Accuracy: 1.0, Computation time: 1.304964303970337\n",
      "Step: 1169, Loss: 0.9264945387840271, Accuracy: 0.9722222089767456, Computation time: 1.5822474956512451\n",
      "Step: 1170, Loss: 0.9247633218765259, Accuracy: 1.0, Computation time: 1.3733530044555664\n",
      "Step: 1171, Loss: 0.9380426406860352, Accuracy: 0.9722222089767456, Computation time: 1.1801016330718994\n",
      "Step: 1172, Loss: 0.916407585144043, Accuracy: 1.0, Computation time: 1.182859182357788\n",
      "Step: 1173, Loss: 0.9523207545280457, Accuracy: 0.9642857313156128, Computation time: 1.7942347526550293\n",
      "Step: 1174, Loss: 0.9163258075714111, Accuracy: 1.0, Computation time: 1.312347412109375\n",
      "Step: 1175, Loss: 0.9175053834915161, Accuracy: 1.0, Computation time: 1.2523820400238037\n",
      "Step: 1176, Loss: 0.9180829524993896, Accuracy: 1.0, Computation time: 1.8257272243499756\n",
      "Step: 1177, Loss: 0.9168000817298889, Accuracy: 1.0, Computation time: 1.1464269161224365\n",
      "Step: 1178, Loss: 0.9178095459938049, Accuracy: 1.0, Computation time: 1.4522359371185303\n",
      "Step: 1179, Loss: 0.9169471859931946, Accuracy: 1.0, Computation time: 1.2784802913665771\n",
      "Step: 1180, Loss: 0.9171401262283325, Accuracy: 1.0, Computation time: 1.3590750694274902\n",
      "Step: 1181, Loss: 0.9164878129959106, Accuracy: 1.0, Computation time: 1.1883325576782227\n",
      "Step: 1182, Loss: 0.9344116449356079, Accuracy: 0.9750000238418579, Computation time: 1.8725850582122803\n",
      "Step: 1183, Loss: 0.916236162185669, Accuracy: 1.0, Computation time: 1.5371944904327393\n",
      "Step: 1184, Loss: 0.9219883680343628, Accuracy: 1.0, Computation time: 1.8001160621643066\n",
      "Step: 1185, Loss: 0.9593430757522583, Accuracy: 0.9147727489471436, Computation time: 1.6384544372558594\n",
      "Step: 1186, Loss: 0.9161164164543152, Accuracy: 1.0, Computation time: 1.1833069324493408\n",
      "Step: 1187, Loss: 0.9349444508552551, Accuracy: 0.9750000238418579, Computation time: 1.116546869277954\n",
      "Step: 1188, Loss: 0.9383507966995239, Accuracy: 0.9583333730697632, Computation time: 1.247567892074585\n",
      "Step: 1189, Loss: 0.9165310263633728, Accuracy: 1.0, Computation time: 1.4264941215515137\n",
      "Step: 1190, Loss: 0.917205810546875, Accuracy: 1.0, Computation time: 1.9137091636657715\n",
      "Step: 1191, Loss: 0.9185692667961121, Accuracy: 1.0, Computation time: 1.3888537883758545\n",
      "Step: 1192, Loss: 0.9163255095481873, Accuracy: 1.0, Computation time: 1.3644232749938965\n",
      "Step: 1193, Loss: 0.9173153042793274, Accuracy: 1.0, Computation time: 1.1824872493743896\n",
      "Step: 1194, Loss: 0.9162052869796753, Accuracy: 1.0, Computation time: 1.4302029609680176\n",
      "Step: 1195, Loss: 0.9162257313728333, Accuracy: 1.0, Computation time: 1.4282777309417725\n",
      "Step: 1196, Loss: 0.9354734420776367, Accuracy: 0.949999988079071, Computation time: 1.713135004043579\n",
      "Step: 1197, Loss: 0.9160838723182678, Accuracy: 1.0, Computation time: 1.101395845413208\n",
      "Step: 1198, Loss: 0.9165586233139038, Accuracy: 1.0, Computation time: 1.1533260345458984\n",
      "Step: 1199, Loss: 0.9161582589149475, Accuracy: 1.0, Computation time: 1.2040224075317383\n",
      "Step: 1200, Loss: 0.937478244304657, Accuracy: 0.9821428656578064, Computation time: 1.3181679248809814\n",
      "Step: 1201, Loss: 0.9207084774971008, Accuracy: 1.0, Computation time: 2.2039546966552734\n",
      "Step: 1202, Loss: 0.9160835146903992, Accuracy: 1.0, Computation time: 1.1869797706604004\n",
      "Step: 1203, Loss: 0.9161521196365356, Accuracy: 1.0, Computation time: 1.2748079299926758\n",
      "Step: 1204, Loss: 0.9161483645439148, Accuracy: 1.0, Computation time: 1.1647870540618896\n",
      "Step: 1205, Loss: 0.9182962775230408, Accuracy: 1.0, Computation time: 2.3986527919769287\n",
      "Step: 1206, Loss: 0.9163069725036621, Accuracy: 1.0, Computation time: 1.118483066558838\n",
      "Step: 1207, Loss: 0.9161808490753174, Accuracy: 1.0, Computation time: 1.2844204902648926\n",
      "Step: 1208, Loss: 0.9161413311958313, Accuracy: 1.0, Computation time: 1.2620923519134521\n",
      "Step: 1209, Loss: 0.9162783622741699, Accuracy: 1.0, Computation time: 1.3223576545715332\n",
      "Step: 1210, Loss: 0.9162130951881409, Accuracy: 1.0, Computation time: 1.83552885055542\n",
      "Step: 1211, Loss: 0.9168391227722168, Accuracy: 1.0, Computation time: 1.176732063293457\n",
      "Step: 1212, Loss: 0.9166924953460693, Accuracy: 1.0, Computation time: 1.3644778728485107\n",
      "Step: 1213, Loss: 0.9371930956840515, Accuracy: 0.9642857313156128, Computation time: 1.0900025367736816\n",
      "Step: 1214, Loss: 0.9369596242904663, Accuracy: 0.9807692766189575, Computation time: 1.8730602264404297\n",
      "Step: 1215, Loss: 0.9171432852745056, Accuracy: 1.0, Computation time: 1.1794865131378174\n",
      "Step: 1216, Loss: 0.9160929322242737, Accuracy: 1.0, Computation time: 1.4511840343475342\n",
      "Step: 1217, Loss: 0.916034460067749, Accuracy: 1.0, Computation time: 1.0752062797546387\n",
      "Step: 1218, Loss: 0.9160028100013733, Accuracy: 1.0, Computation time: 1.3073782920837402\n",
      "Step: 1219, Loss: 0.916152834892273, Accuracy: 1.0, Computation time: 1.4277989864349365\n",
      "Step: 1220, Loss: 0.9163005948066711, Accuracy: 1.0, Computation time: 1.2508044242858887\n",
      "Step: 1221, Loss: 0.9160082936286926, Accuracy: 1.0, Computation time: 1.2086224555969238\n",
      "Step: 1222, Loss: 0.91599440574646, Accuracy: 1.0, Computation time: 1.0401976108551025\n",
      "Step: 1223, Loss: 0.916000485420227, Accuracy: 1.0, Computation time: 1.5872817039489746\n",
      "Step: 1224, Loss: 0.9159200191497803, Accuracy: 1.0, Computation time: 1.2746140956878662\n",
      "Step: 1225, Loss: 0.916087806224823, Accuracy: 1.0, Computation time: 1.6538643836975098\n",
      "Step: 1226, Loss: 0.9161128401756287, Accuracy: 1.0, Computation time: 1.6283371448516846\n",
      "Step: 1227, Loss: 0.9160361886024475, Accuracy: 1.0, Computation time: 1.3568205833435059\n",
      "Step: 1228, Loss: 0.9159955978393555, Accuracy: 1.0, Computation time: 1.332160472869873\n",
      "Step: 1229, Loss: 0.9159885048866272, Accuracy: 1.0, Computation time: 1.1605751514434814\n",
      "Step: 1230, Loss: 0.9377687573432922, Accuracy: 0.96875, Computation time: 1.425034523010254\n",
      "Step: 1231, Loss: 0.9431097507476807, Accuracy: 0.9583333730697632, Computation time: 1.6328372955322266\n",
      "Step: 1232, Loss: 0.9186901450157166, Accuracy: 1.0, Computation time: 1.9808282852172852\n",
      "Step: 1233, Loss: 0.9159570336341858, Accuracy: 1.0, Computation time: 1.4346885681152344\n",
      "Step: 1234, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 1.3675477504730225\n",
      "Step: 1235, Loss: 0.9158796072006226, Accuracy: 1.0, Computation time: 1.2251918315887451\n",
      "Step: 1236, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.4385912418365479\n",
      "Step: 1237, Loss: 0.9189490079879761, Accuracy: 1.0, Computation time: 1.295328140258789\n",
      "Step: 1238, Loss: 0.9161208868026733, Accuracy: 1.0, Computation time: 1.5635597705841064\n",
      "Step: 1239, Loss: 0.9162245988845825, Accuracy: 1.0, Computation time: 1.6279079914093018\n",
      "Step: 1240, Loss: 0.9186955094337463, Accuracy: 1.0, Computation time: 1.707592248916626\n",
      "Step: 1241, Loss: 0.9159903526306152, Accuracy: 1.0, Computation time: 1.0180442333221436\n",
      "Step: 1242, Loss: 0.9266635775566101, Accuracy: 0.9750000238418579, Computation time: 2.23663330078125\n",
      "Step: 1243, Loss: 0.9161779284477234, Accuracy: 1.0, Computation time: 1.3399927616119385\n",
      "Step: 1244, Loss: 0.9162442684173584, Accuracy: 1.0, Computation time: 1.3410420417785645\n",
      "Step: 1245, Loss: 0.9163780212402344, Accuracy: 1.0, Computation time: 1.3907909393310547\n",
      "Step: 1246, Loss: 0.9166207909584045, Accuracy: 1.0, Computation time: 1.20955491065979\n",
      "Step: 1247, Loss: 0.9162336587905884, Accuracy: 1.0, Computation time: 1.2582037448883057\n",
      "Step: 1248, Loss: 0.9577314853668213, Accuracy: 0.9392857551574707, Computation time: 1.435837745666504\n",
      "Step: 1249, Loss: 0.9159830808639526, Accuracy: 1.0, Computation time: 1.5533828735351562\n",
      "Step: 1250, Loss: 0.9178772568702698, Accuracy: 1.0, Computation time: 1.512446403503418\n",
      "Step: 1251, Loss: 0.9160483479499817, Accuracy: 1.0, Computation time: 1.256653070449829\n",
      "########################\n",
      "Test loss: 1.0729873180389404, Test Accuracy_epoch9: 0.7650203704833984\n",
      "########################\n",
      "Step: 1252, Loss: 0.9161378145217896, Accuracy: 1.0, Computation time: 1.2730810642242432\n",
      "Step: 1253, Loss: 0.9168447852134705, Accuracy: 1.0, Computation time: 1.6754467487335205\n",
      "Step: 1254, Loss: 0.9175535440444946, Accuracy: 1.0, Computation time: 2.652831792831421\n",
      "Step: 1255, Loss: 0.9575362205505371, Accuracy: 0.9356061220169067, Computation time: 1.8110840320587158\n",
      "Step: 1256, Loss: 0.9163272976875305, Accuracy: 1.0, Computation time: 1.3341403007507324\n",
      "Step: 1257, Loss: 0.9253963232040405, Accuracy: 1.0, Computation time: 1.5796737670898438\n",
      "Step: 1258, Loss: 0.9246567487716675, Accuracy: 1.0, Computation time: 1.204937219619751\n",
      "Step: 1259, Loss: 0.9365606904029846, Accuracy: 0.9750000238418579, Computation time: 1.7357027530670166\n",
      "Step: 1260, Loss: 0.9164366126060486, Accuracy: 1.0, Computation time: 1.5469870567321777\n",
      "Step: 1261, Loss: 0.9194622039794922, Accuracy: 1.0, Computation time: 1.726464033126831\n",
      "Step: 1262, Loss: 0.916016697883606, Accuracy: 1.0, Computation time: 1.7705566883087158\n",
      "Step: 1263, Loss: 0.9162672162055969, Accuracy: 1.0, Computation time: 1.8756659030914307\n",
      "Step: 1264, Loss: 0.9161032438278198, Accuracy: 1.0, Computation time: 1.2085833549499512\n",
      "Step: 1265, Loss: 0.9162420034408569, Accuracy: 1.0, Computation time: 1.522305965423584\n",
      "Step: 1266, Loss: 0.916293740272522, Accuracy: 1.0, Computation time: 1.2726502418518066\n",
      "Step: 1267, Loss: 0.9378403425216675, Accuracy: 0.9642857313156128, Computation time: 1.5783765316009521\n",
      "Step: 1268, Loss: 0.916344404220581, Accuracy: 1.0, Computation time: 1.4168288707733154\n",
      "Step: 1269, Loss: 0.9355716109275818, Accuracy: 0.949999988079071, Computation time: 1.8121528625488281\n",
      "Step: 1270, Loss: 0.9176812767982483, Accuracy: 1.0, Computation time: 1.4850811958312988\n",
      "Step: 1271, Loss: 0.9159849286079407, Accuracy: 1.0, Computation time: 1.3016972541809082\n",
      "Step: 1272, Loss: 0.9159266948699951, Accuracy: 1.0, Computation time: 1.4073596000671387\n",
      "Step: 1273, Loss: 0.9159774780273438, Accuracy: 1.0, Computation time: 1.4175264835357666\n",
      "Step: 1274, Loss: 0.9159875512123108, Accuracy: 1.0, Computation time: 1.2840962409973145\n",
      "Step: 1275, Loss: 0.916587233543396, Accuracy: 1.0, Computation time: 1.1671626567840576\n",
      "Step: 1276, Loss: 0.9367009401321411, Accuracy: 0.9722222089767456, Computation time: 1.2583155632019043\n",
      "Step: 1277, Loss: 0.931469202041626, Accuracy: 0.96875, Computation time: 1.3707263469696045\n",
      "Step: 1278, Loss: 0.9162291884422302, Accuracy: 1.0, Computation time: 1.3415749073028564\n",
      "Step: 1279, Loss: 0.9160916805267334, Accuracy: 1.0, Computation time: 1.5458545684814453\n",
      "Step: 1280, Loss: 0.937657356262207, Accuracy: 0.9722222089767456, Computation time: 1.4767694473266602\n",
      "Step: 1281, Loss: 0.9179791212081909, Accuracy: 1.0, Computation time: 1.245685338973999\n",
      "Step: 1282, Loss: 0.9375787973403931, Accuracy: 0.9642857313156128, Computation time: 1.3675706386566162\n",
      "Step: 1283, Loss: 0.915984034538269, Accuracy: 1.0, Computation time: 1.2522244453430176\n",
      "Step: 1284, Loss: 0.9161531925201416, Accuracy: 1.0, Computation time: 1.2544231414794922\n",
      "Step: 1285, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 1.198051929473877\n",
      "Step: 1286, Loss: 0.9159406423568726, Accuracy: 1.0, Computation time: 1.251340627670288\n",
      "Step: 1287, Loss: 0.9167634844779968, Accuracy: 1.0, Computation time: 1.3322060108184814\n",
      "Step: 1288, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.091860055923462\n",
      "Step: 1289, Loss: 0.915985643863678, Accuracy: 1.0, Computation time: 1.509885311126709\n",
      "Step: 1290, Loss: 0.9159944653511047, Accuracy: 1.0, Computation time: 1.1834111213684082\n",
      "Step: 1291, Loss: 0.9167468547821045, Accuracy: 1.0, Computation time: 1.331864595413208\n",
      "Step: 1292, Loss: 0.915912389755249, Accuracy: 1.0, Computation time: 1.4233806133270264\n",
      "Step: 1293, Loss: 0.9159923195838928, Accuracy: 1.0, Computation time: 1.3375928401947021\n",
      "Step: 1294, Loss: 0.9159378409385681, Accuracy: 1.0, Computation time: 1.2822790145874023\n",
      "Step: 1295, Loss: 0.9158753752708435, Accuracy: 1.0, Computation time: 1.1528775691986084\n",
      "Step: 1296, Loss: 0.9158802628517151, Accuracy: 1.0, Computation time: 1.3866548538208008\n",
      "Step: 1297, Loss: 0.9159676432609558, Accuracy: 1.0, Computation time: 1.33646559715271\n",
      "Step: 1298, Loss: 0.9160701632499695, Accuracy: 1.0, Computation time: 1.588296890258789\n",
      "Step: 1299, Loss: 0.9374515414237976, Accuracy: 0.9722222089767456, Computation time: 1.097944974899292\n",
      "Step: 1300, Loss: 0.9158952832221985, Accuracy: 1.0, Computation time: 1.3217127323150635\n",
      "Step: 1301, Loss: 0.916124165058136, Accuracy: 1.0, Computation time: 1.2821176052093506\n",
      "Step: 1302, Loss: 0.9158787727355957, Accuracy: 1.0, Computation time: 1.2871735095977783\n",
      "Step: 1303, Loss: 0.9158955812454224, Accuracy: 1.0, Computation time: 1.2150919437408447\n",
      "Step: 1304, Loss: 0.9158633947372437, Accuracy: 1.0, Computation time: 1.0383758544921875\n",
      "Step: 1305, Loss: 0.9158773422241211, Accuracy: 1.0, Computation time: 1.243478775024414\n",
      "Step: 1306, Loss: 0.9376283288002014, Accuracy: 0.9642857313156128, Computation time: 1.2161760330200195\n",
      "Step: 1307, Loss: 0.9160239100456238, Accuracy: 1.0, Computation time: 1.4971652030944824\n",
      "Step: 1308, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.2379944324493408\n",
      "Step: 1309, Loss: 0.9159043431282043, Accuracy: 1.0, Computation time: 1.607743263244629\n",
      "Step: 1310, Loss: 0.915916919708252, Accuracy: 1.0, Computation time: 1.0878489017486572\n",
      "Step: 1311, Loss: 0.9602519273757935, Accuracy: 0.935606062412262, Computation time: 1.5346496105194092\n",
      "Step: 1312, Loss: 0.9382593631744385, Accuracy: 0.96875, Computation time: 1.4834418296813965\n",
      "Step: 1313, Loss: 0.9160323143005371, Accuracy: 1.0, Computation time: 1.0884153842926025\n",
      "Step: 1314, Loss: 0.9337626099586487, Accuracy: 0.9807692766189575, Computation time: 1.3466477394104004\n",
      "Step: 1315, Loss: 0.9369275569915771, Accuracy: 0.9642857313156128, Computation time: 1.2060441970825195\n",
      "Step: 1316, Loss: 0.9159077405929565, Accuracy: 1.0, Computation time: 1.3972804546356201\n",
      "Step: 1317, Loss: 0.9158943295478821, Accuracy: 1.0, Computation time: 1.2649662494659424\n",
      "Step: 1318, Loss: 0.9355171322822571, Accuracy: 0.9642857313156128, Computation time: 1.8633317947387695\n",
      "Step: 1319, Loss: 0.9159194231033325, Accuracy: 1.0, Computation time: 1.0971648693084717\n",
      "Step: 1320, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.350248098373413\n",
      "Step: 1321, Loss: 0.9247722625732422, Accuracy: 1.0, Computation time: 1.798203945159912\n",
      "Step: 1322, Loss: 0.9158934354782104, Accuracy: 1.0, Computation time: 1.1490504741668701\n",
      "Step: 1323, Loss: 0.9158806800842285, Accuracy: 1.0, Computation time: 1.0061805248260498\n",
      "Step: 1324, Loss: 0.9159033298492432, Accuracy: 1.0, Computation time: 1.1427814960479736\n",
      "Step: 1325, Loss: 0.9159257411956787, Accuracy: 1.0, Computation time: 1.2868092060089111\n",
      "Step: 1326, Loss: 0.9336607456207275, Accuracy: 0.9791666865348816, Computation time: 1.7570037841796875\n",
      "Step: 1327, Loss: 0.9159352779388428, Accuracy: 1.0, Computation time: 1.0410187244415283\n",
      "Step: 1328, Loss: 0.9374235272407532, Accuracy: 0.9772727489471436, Computation time: 1.115253210067749\n",
      "Step: 1329, Loss: 0.9158922433853149, Accuracy: 1.0, Computation time: 1.2661640644073486\n",
      "Step: 1330, Loss: 0.9159327149391174, Accuracy: 1.0, Computation time: 1.098815679550171\n",
      "Step: 1331, Loss: 0.9159420728683472, Accuracy: 1.0, Computation time: 1.3965954780578613\n",
      "Step: 1332, Loss: 0.9227768182754517, Accuracy: 1.0, Computation time: 1.447234869003296\n",
      "Step: 1333, Loss: 0.9159597754478455, Accuracy: 1.0, Computation time: 1.2386958599090576\n",
      "Step: 1334, Loss: 0.9159484505653381, Accuracy: 1.0, Computation time: 1.2542154788970947\n",
      "Step: 1335, Loss: 0.9159153699874878, Accuracy: 1.0, Computation time: 1.315685510635376\n",
      "Step: 1336, Loss: 0.916911244392395, Accuracy: 1.0, Computation time: 1.4418408870697021\n",
      "Step: 1337, Loss: 0.9301363825798035, Accuracy: 0.96875, Computation time: 1.181990146636963\n",
      "Step: 1338, Loss: 0.9158845543861389, Accuracy: 1.0, Computation time: 1.4186131954193115\n",
      "Step: 1339, Loss: 0.9162617921829224, Accuracy: 1.0, Computation time: 1.0395328998565674\n",
      "Step: 1340, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 1.2254021167755127\n",
      "Step: 1341, Loss: 0.915917158126831, Accuracy: 1.0, Computation time: 1.1190955638885498\n",
      "Step: 1342, Loss: 0.9159976243972778, Accuracy: 1.0, Computation time: 1.257871389389038\n",
      "Step: 1343, Loss: 0.9159076809883118, Accuracy: 1.0, Computation time: 0.9672176837921143\n",
      "Step: 1344, Loss: 0.9159300923347473, Accuracy: 1.0, Computation time: 1.2179913520812988\n",
      "Step: 1345, Loss: 0.9159722924232483, Accuracy: 1.0, Computation time: 1.129551887512207\n",
      "Step: 1346, Loss: 0.9248347282409668, Accuracy: 1.0, Computation time: 1.1984584331512451\n",
      "Step: 1347, Loss: 0.9159269332885742, Accuracy: 1.0, Computation time: 1.2578766345977783\n",
      "Step: 1348, Loss: 0.9159409999847412, Accuracy: 1.0, Computation time: 1.3079173564910889\n",
      "Step: 1349, Loss: 0.9161326885223389, Accuracy: 1.0, Computation time: 2.0779380798339844\n",
      "Step: 1350, Loss: 0.9204265475273132, Accuracy: 1.0, Computation time: 1.2667033672332764\n",
      "Step: 1351, Loss: 0.9161015748977661, Accuracy: 1.0, Computation time: 1.4915709495544434\n",
      "Step: 1352, Loss: 0.9159976840019226, Accuracy: 1.0, Computation time: 1.3625328540802002\n",
      "Step: 1353, Loss: 0.9160053730010986, Accuracy: nan, Computation time: 1.2131397724151611\n",
      "Step: 1354, Loss: 0.9165902733802795, Accuracy: 1.0, Computation time: 1.4947216510772705\n",
      "Step: 1355, Loss: 0.9162058234214783, Accuracy: 1.0, Computation time: 1.4226560592651367\n",
      "Step: 1356, Loss: 0.9159641861915588, Accuracy: 1.0, Computation time: 1.480499029159546\n",
      "Step: 1357, Loss: 0.915921688079834, Accuracy: 1.0, Computation time: 1.9137377738952637\n",
      "Step: 1358, Loss: 0.9162759780883789, Accuracy: 1.0, Computation time: 1.2419853210449219\n",
      "Step: 1359, Loss: 0.9159156680107117, Accuracy: 1.0, Computation time: 1.9655518531799316\n",
      "Step: 1360, Loss: 0.9321308135986328, Accuracy: 0.96875, Computation time: 1.6193678379058838\n",
      "Step: 1361, Loss: 0.9159241318702698, Accuracy: 1.0, Computation time: 1.3312933444976807\n",
      "Step: 1362, Loss: 0.9159291386604309, Accuracy: 1.0, Computation time: 1.1064112186431885\n",
      "Step: 1363, Loss: 0.9159433841705322, Accuracy: 1.0, Computation time: 1.1700358390808105\n",
      "Step: 1364, Loss: 0.9168632626533508, Accuracy: 1.0, Computation time: 1.2645354270935059\n",
      "Step: 1365, Loss: 0.9159901738166809, Accuracy: 1.0, Computation time: 1.137012243270874\n",
      "Step: 1366, Loss: 0.9160006046295166, Accuracy: 1.0, Computation time: 1.0993406772613525\n",
      "Step: 1367, Loss: 0.9159840941429138, Accuracy: 1.0, Computation time: 1.3069181442260742\n",
      "Step: 1368, Loss: 0.9159857034683228, Accuracy: 1.0, Computation time: 1.5190963745117188\n",
      "Step: 1369, Loss: 0.9360380172729492, Accuracy: 0.96875, Computation time: 1.1996707916259766\n",
      "Step: 1370, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 1.2576375007629395\n",
      "Step: 1371, Loss: 0.9325379133224487, Accuracy: 0.9642857313156128, Computation time: 1.2842097282409668\n",
      "Step: 1372, Loss: 0.9158869385719299, Accuracy: 1.0, Computation time: 0.9914438724517822\n",
      "Step: 1373, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 1.2650418281555176\n",
      "Step: 1374, Loss: 0.9159729480743408, Accuracy: 1.0, Computation time: 1.189241647720337\n",
      "Step: 1375, Loss: 0.9159277677536011, Accuracy: 1.0, Computation time: 1.2249541282653809\n",
      "Step: 1376, Loss: 0.9159201979637146, Accuracy: 1.0, Computation time: 1.3074193000793457\n",
      "Step: 1377, Loss: 0.9358004927635193, Accuracy: 0.9722222089767456, Computation time: 1.297959566116333\n",
      "Step: 1378, Loss: 0.9375711679458618, Accuracy: 0.9642857313156128, Computation time: 1.2634453773498535\n",
      "Step: 1379, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.2063875198364258\n",
      "Step: 1380, Loss: 0.9159919023513794, Accuracy: 1.0, Computation time: 1.1401937007904053\n",
      "Step: 1381, Loss: 0.9159414172172546, Accuracy: 1.0, Computation time: 1.0386133193969727\n",
      "Step: 1382, Loss: 0.9159834384918213, Accuracy: 1.0, Computation time: 1.2261748313903809\n",
      "Step: 1383, Loss: 0.9159430265426636, Accuracy: 1.0, Computation time: 1.1192898750305176\n",
      "Step: 1384, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.3798339366912842\n",
      "Step: 1385, Loss: 0.9159281849861145, Accuracy: 1.0, Computation time: 1.076206922531128\n",
      "Step: 1386, Loss: 0.9159223437309265, Accuracy: 1.0, Computation time: 1.2386229038238525\n",
      "Step: 1387, Loss: 0.9159062504768372, Accuracy: 1.0, Computation time: 1.4333248138427734\n",
      "Step: 1388, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.196594476699829\n",
      "Step: 1389, Loss: 0.9332538843154907, Accuracy: 0.9722222089767456, Computation time: 1.6452691555023193\n",
      "Step: 1390, Loss: 0.9372867345809937, Accuracy: 0.9772727489471436, Computation time: 1.4444353580474854\n",
      "########################\n",
      "Test loss: 1.0708671808242798, Test Accuracy_epoch10: 0.7895963191986084\n",
      "########################\n",
      "Step: 1391, Loss: 0.9391869306564331, Accuracy: 0.9583333730697632, Computation time: 2.303872585296631\n",
      "Step: 1392, Loss: 0.9181484580039978, Accuracy: 1.0, Computation time: 1.4886503219604492\n",
      "Step: 1393, Loss: 0.9373469352722168, Accuracy: 0.96875, Computation time: 1.065814733505249\n",
      "Step: 1394, Loss: 0.9377602934837341, Accuracy: 0.9833333492279053, Computation time: 1.1443531513214111\n",
      "Step: 1395, Loss: 0.9165146350860596, Accuracy: 1.0, Computation time: 1.1571004390716553\n",
      "Step: 1396, Loss: 0.9161441922187805, Accuracy: 1.0, Computation time: 1.0669760704040527\n",
      "Step: 1397, Loss: 0.9160435795783997, Accuracy: 1.0, Computation time: 1.0994617938995361\n",
      "Step: 1398, Loss: 0.9288133382797241, Accuracy: 0.9722222089767456, Computation time: 1.6913092136383057\n",
      "Step: 1399, Loss: 0.9377946853637695, Accuracy: 0.9772727489471436, Computation time: 1.1814243793487549\n",
      "Step: 1400, Loss: 0.9161052107810974, Accuracy: 1.0, Computation time: 2.1915600299835205\n",
      "Step: 1401, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 1.1048834323883057\n",
      "Step: 1402, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.1316113471984863\n",
      "Step: 1403, Loss: 0.9158743023872375, Accuracy: 1.0, Computation time: 1.0640068054199219\n",
      "Step: 1404, Loss: 0.9159245491027832, Accuracy: 1.0, Computation time: 1.3388020992279053\n",
      "Step: 1405, Loss: 0.923762321472168, Accuracy: 1.0, Computation time: 1.2107436656951904\n",
      "Step: 1406, Loss: 0.9214888215065002, Accuracy: 1.0, Computation time: 1.6426773071289062\n",
      "Step: 1407, Loss: 0.9159440994262695, Accuracy: 1.0, Computation time: 1.0284423828125\n",
      "Step: 1408, Loss: 0.915968656539917, Accuracy: 1.0, Computation time: 1.1746492385864258\n",
      "Step: 1409, Loss: 0.9162266254425049, Accuracy: 1.0, Computation time: 1.2637748718261719\n",
      "Step: 1410, Loss: 0.9169943928718567, Accuracy: 1.0, Computation time: 1.5158541202545166\n",
      "Step: 1411, Loss: 0.915931761264801, Accuracy: 1.0, Computation time: 1.607515811920166\n",
      "Step: 1412, Loss: 0.9159432053565979, Accuracy: 1.0, Computation time: 1.2852075099945068\n",
      "Step: 1413, Loss: 0.9159309267997742, Accuracy: 1.0, Computation time: 1.687002420425415\n",
      "Step: 1414, Loss: 0.9594826698303223, Accuracy: 0.9285714626312256, Computation time: 1.410677194595337\n",
      "Step: 1415, Loss: 0.9378290772438049, Accuracy: 0.9583333730697632, Computation time: 1.2527601718902588\n",
      "Step: 1416, Loss: 0.9159584045410156, Accuracy: 1.0, Computation time: 1.0881528854370117\n",
      "Step: 1417, Loss: 0.9159744381904602, Accuracy: 1.0, Computation time: 1.1162304878234863\n",
      "Step: 1418, Loss: 0.9159584641456604, Accuracy: 1.0, Computation time: 0.9353625774383545\n",
      "Step: 1419, Loss: 0.915950357913971, Accuracy: 1.0, Computation time: 1.5803251266479492\n",
      "Step: 1420, Loss: 0.9315930604934692, Accuracy: 0.9833333492279053, Computation time: 1.4161722660064697\n",
      "Step: 1421, Loss: 0.9454873204231262, Accuracy: 0.9791666865348816, Computation time: 1.0307245254516602\n",
      "Step: 1422, Loss: 0.9161935448646545, Accuracy: 1.0, Computation time: 1.3610210418701172\n",
      "Step: 1423, Loss: 0.9159501194953918, Accuracy: 1.0, Computation time: 1.3810322284698486\n",
      "Step: 1424, Loss: 0.9159398674964905, Accuracy: 1.0, Computation time: 1.0143895149230957\n",
      "Step: 1425, Loss: 0.9160027503967285, Accuracy: 1.0, Computation time: 1.2161097526550293\n",
      "Step: 1426, Loss: 0.9160249829292297, Accuracy: 1.0, Computation time: 1.1139552593231201\n",
      "Step: 1427, Loss: 0.9376744031906128, Accuracy: 0.9583333730697632, Computation time: 1.2421200275421143\n",
      "Step: 1428, Loss: 0.9160885214805603, Accuracy: 1.0, Computation time: 1.8086392879486084\n",
      "Step: 1429, Loss: 0.9159752130508423, Accuracy: 1.0, Computation time: 1.5458378791809082\n",
      "Step: 1430, Loss: 0.9160148501396179, Accuracy: 1.0, Computation time: 1.1638777256011963\n",
      "Step: 1431, Loss: 0.9159401655197144, Accuracy: 1.0, Computation time: 1.15462327003479\n",
      "Step: 1432, Loss: 0.9159942269325256, Accuracy: 1.0, Computation time: 0.999974250793457\n",
      "Step: 1433, Loss: 0.9160194993019104, Accuracy: 1.0, Computation time: 1.2340233325958252\n",
      "Step: 1434, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.0066049098968506\n",
      "Step: 1435, Loss: 0.9159007668495178, Accuracy: 1.0, Computation time: 1.0855839252471924\n",
      "Step: 1436, Loss: 0.9197901487350464, Accuracy: 1.0, Computation time: 1.3310513496398926\n",
      "Step: 1437, Loss: 0.9158568382263184, Accuracy: 1.0, Computation time: 1.178673267364502\n",
      "Step: 1438, Loss: 0.916449248790741, Accuracy: 1.0, Computation time: 1.2646000385284424\n",
      "Step: 1439, Loss: 0.9159358143806458, Accuracy: 1.0, Computation time: 1.1909713745117188\n",
      "Step: 1440, Loss: 0.9159108400344849, Accuracy: 1.0, Computation time: 1.397087812423706\n",
      "Step: 1441, Loss: 0.9205285310745239, Accuracy: 1.0, Computation time: 1.3665235042572021\n",
      "Step: 1442, Loss: 0.9167243838310242, Accuracy: 1.0, Computation time: 1.2552711963653564\n",
      "Step: 1443, Loss: 0.9399812817573547, Accuracy: 0.9375, Computation time: 1.3477494716644287\n",
      "Step: 1444, Loss: 0.927435040473938, Accuracy: 0.9642857313156128, Computation time: 1.159501075744629\n",
      "Step: 1445, Loss: 0.916555643081665, Accuracy: 1.0, Computation time: 2.0941712856292725\n",
      "Step: 1446, Loss: 0.9161179065704346, Accuracy: 1.0, Computation time: 1.5554003715515137\n",
      "Step: 1447, Loss: 0.9160711169242859, Accuracy: 1.0, Computation time: 1.353956699371338\n",
      "Step: 1448, Loss: 0.9161113500595093, Accuracy: 1.0, Computation time: 0.972332239151001\n",
      "Step: 1449, Loss: 0.9160263538360596, Accuracy: 1.0, Computation time: 1.3353147506713867\n",
      "Step: 1450, Loss: 0.9159923791885376, Accuracy: 1.0, Computation time: 1.0939855575561523\n",
      "Step: 1451, Loss: 0.9324451684951782, Accuracy: 1.0, Computation time: 1.6271998882293701\n",
      "Step: 1452, Loss: 0.9159287810325623, Accuracy: 1.0, Computation time: 1.0193960666656494\n",
      "Step: 1453, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.2907252311706543\n",
      "Step: 1454, Loss: 0.9174184203147888, Accuracy: 1.0, Computation time: 1.4399640560150146\n",
      "Step: 1455, Loss: 0.9161994457244873, Accuracy: 1.0, Computation time: 1.216508150100708\n",
      "Step: 1456, Loss: 0.9161010384559631, Accuracy: 1.0, Computation time: 1.0274856090545654\n",
      "Step: 1457, Loss: 0.9161049127578735, Accuracy: 1.0, Computation time: 1.012775182723999\n",
      "Step: 1458, Loss: 0.9162075519561768, Accuracy: 1.0, Computation time: 1.0480268001556396\n",
      "Step: 1459, Loss: 0.9194260835647583, Accuracy: 1.0, Computation time: 1.3202757835388184\n",
      "Step: 1460, Loss: 0.9160885214805603, Accuracy: 1.0, Computation time: 1.095045804977417\n",
      "Step: 1461, Loss: 0.9219360947608948, Accuracy: 1.0, Computation time: 1.5747041702270508\n",
      "Step: 1462, Loss: 0.9159960746765137, Accuracy: 1.0, Computation time: 1.2627687454223633\n",
      "Step: 1463, Loss: 0.9158720970153809, Accuracy: 1.0, Computation time: 1.2454173564910889\n",
      "Step: 1464, Loss: 0.9159220457077026, Accuracy: 1.0, Computation time: 1.1696715354919434\n",
      "Step: 1465, Loss: 0.9160246849060059, Accuracy: 1.0, Computation time: 1.5118629932403564\n",
      "Step: 1466, Loss: 0.9161230325698853, Accuracy: 1.0, Computation time: 1.2342174053192139\n",
      "Step: 1467, Loss: 0.915978729724884, Accuracy: 1.0, Computation time: 1.4081785678863525\n",
      "Step: 1468, Loss: 0.9159673452377319, Accuracy: 1.0, Computation time: 1.2848122119903564\n",
      "Step: 1469, Loss: 0.9160518646240234, Accuracy: 1.0, Computation time: 1.288201093673706\n",
      "Step: 1470, Loss: 0.9159891605377197, Accuracy: 1.0, Computation time: 1.8131721019744873\n",
      "Step: 1471, Loss: 0.9230083227157593, Accuracy: 1.0, Computation time: 1.178877353668213\n",
      "Step: 1472, Loss: 0.9376994371414185, Accuracy: 0.96875, Computation time: 1.1631698608398438\n",
      "Step: 1473, Loss: 0.9160534143447876, Accuracy: 1.0, Computation time: 1.221228837966919\n",
      "Step: 1474, Loss: 0.9160539507865906, Accuracy: 1.0, Computation time: 0.9806375503540039\n",
      "Step: 1475, Loss: 0.915949821472168, Accuracy: 1.0, Computation time: 1.0633893013000488\n",
      "Step: 1476, Loss: 0.9159279465675354, Accuracy: 1.0, Computation time: 1.186793565750122\n",
      "Step: 1477, Loss: 0.9159542918205261, Accuracy: 1.0, Computation time: 1.200749158859253\n",
      "Step: 1478, Loss: 0.915972888469696, Accuracy: 1.0, Computation time: 1.1478009223937988\n",
      "Step: 1479, Loss: 0.9374698996543884, Accuracy: 0.9722222089767456, Computation time: 1.171048641204834\n",
      "Step: 1480, Loss: 0.9175294637680054, Accuracy: 1.0, Computation time: 2.044436454772949\n",
      "Step: 1481, Loss: 0.9184718132019043, Accuracy: 1.0, Computation time: 1.1382100582122803\n",
      "Step: 1482, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.490602731704712\n",
      "Step: 1483, Loss: 0.9159297347068787, Accuracy: 1.0, Computation time: 1.360365867614746\n",
      "Step: 1484, Loss: 0.9159711003303528, Accuracy: 1.0, Computation time: 1.073500394821167\n",
      "Step: 1485, Loss: 0.9295781850814819, Accuracy: 0.9772727489471436, Computation time: 1.289698839187622\n",
      "Step: 1486, Loss: 0.9160016179084778, Accuracy: 1.0, Computation time: 1.1342880725860596\n",
      "Step: 1487, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 1.0842738151550293\n",
      "Step: 1488, Loss: 0.9167206883430481, Accuracy: 1.0, Computation time: 1.314060926437378\n",
      "Step: 1489, Loss: 0.9159776568412781, Accuracy: 1.0, Computation time: 1.1177232265472412\n",
      "Step: 1490, Loss: 0.9159831404685974, Accuracy: 1.0, Computation time: 1.0615043640136719\n",
      "Step: 1491, Loss: 0.9286770224571228, Accuracy: 0.9583333730697632, Computation time: 1.3315858840942383\n",
      "Step: 1492, Loss: 0.9563823938369751, Accuracy: 0.949999988079071, Computation time: 1.2656855583190918\n",
      "Step: 1493, Loss: 0.9158979654312134, Accuracy: 1.0, Computation time: 1.3279755115509033\n",
      "Step: 1494, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.2167935371398926\n",
      "Step: 1495, Loss: 0.9177883863449097, Accuracy: 1.0, Computation time: 1.596318244934082\n",
      "Step: 1496, Loss: 0.9159859418869019, Accuracy: 1.0, Computation time: 1.0770628452301025\n",
      "Step: 1497, Loss: 0.9377692341804504, Accuracy: 0.9772727489471436, Computation time: 1.179858922958374\n",
      "Step: 1498, Loss: 0.9160020351409912, Accuracy: 1.0, Computation time: 1.4497816562652588\n",
      "Step: 1499, Loss: 0.9159890413284302, Accuracy: 1.0, Computation time: 1.068903923034668\n",
      "Step: 1500, Loss: 0.9159083962440491, Accuracy: 1.0, Computation time: 1.1629362106323242\n",
      "Step: 1501, Loss: 0.9161934852600098, Accuracy: 1.0, Computation time: 1.4928679466247559\n",
      "Step: 1502, Loss: 0.9159417152404785, Accuracy: 1.0, Computation time: 1.3391401767730713\n",
      "Step: 1503, Loss: 0.9332181811332703, Accuracy: 0.9750000238418579, Computation time: 1.5450639724731445\n",
      "Step: 1504, Loss: 0.9160110950469971, Accuracy: 1.0, Computation time: 1.324852466583252\n",
      "Step: 1505, Loss: 0.9220753312110901, Accuracy: 1.0, Computation time: 1.3459973335266113\n",
      "Step: 1506, Loss: 0.915915310382843, Accuracy: 1.0, Computation time: 1.1204113960266113\n",
      "Step: 1507, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 0.9980783462524414\n",
      "Step: 1508, Loss: 0.9161291718482971, Accuracy: 1.0, Computation time: 1.2290408611297607\n",
      "Step: 1509, Loss: 0.9161791205406189, Accuracy: 1.0, Computation time: 1.118715524673462\n",
      "Step: 1510, Loss: 0.9376205801963806, Accuracy: 0.9722222089767456, Computation time: 1.136446237564087\n",
      "Step: 1511, Loss: 0.9160656332969666, Accuracy: 1.0, Computation time: 1.2552564144134521\n",
      "Step: 1512, Loss: 0.9159865379333496, Accuracy: 1.0, Computation time: 1.2608962059020996\n",
      "Step: 1513, Loss: 0.915884256362915, Accuracy: 1.0, Computation time: 1.3865432739257812\n",
      "Step: 1514, Loss: 0.9159167408943176, Accuracy: 1.0, Computation time: 1.5750453472137451\n",
      "Step: 1515, Loss: 0.9158833622932434, Accuracy: 1.0, Computation time: 1.2529501914978027\n",
      "Step: 1516, Loss: 0.9159321188926697, Accuracy: 1.0, Computation time: 1.2190866470336914\n",
      "Step: 1517, Loss: 0.9158904552459717, Accuracy: 1.0, Computation time: 1.207730770111084\n",
      "Step: 1518, Loss: 0.9179338216781616, Accuracy: 1.0, Computation time: 1.3080801963806152\n",
      "Step: 1519, Loss: 0.9158950448036194, Accuracy: 1.0, Computation time: 1.4912734031677246\n",
      "Step: 1520, Loss: 0.937519907951355, Accuracy: 0.9750000238418579, Computation time: 1.255692481994629\n",
      "Step: 1521, Loss: 0.9159057140350342, Accuracy: 1.0, Computation time: 1.0491042137145996\n",
      "Step: 1522, Loss: 0.9163990020751953, Accuracy: 1.0, Computation time: 1.465637445449829\n",
      "Step: 1523, Loss: 0.9159208536148071, Accuracy: 1.0, Computation time: 1.4345266819000244\n",
      "Step: 1524, Loss: 0.9160354733467102, Accuracy: 1.0, Computation time: 1.5810720920562744\n",
      "Step: 1525, Loss: 0.9160231947898865, Accuracy: 1.0, Computation time: 1.2530310153961182\n",
      "Step: 1526, Loss: 0.9787675142288208, Accuracy: 0.9166666269302368, Computation time: 1.4743576049804688\n",
      "Step: 1527, Loss: 0.9158974289894104, Accuracy: 1.0, Computation time: 1.45597505569458\n",
      "Step: 1528, Loss: 0.937443733215332, Accuracy: 0.96875, Computation time: 1.299142837524414\n",
      "Step: 1529, Loss: 0.9160328507423401, Accuracy: 1.0, Computation time: 1.3712749481201172\n",
      "########################\n",
      "Test loss: 1.0677489042282104, Test Accuracy_epoch11: 0.7822283506393433\n",
      "########################\n",
      "Step: 1530, Loss: 0.9160125255584717, Accuracy: 1.0, Computation time: 1.3133151531219482\n",
      "Step: 1531, Loss: 0.9362046718597412, Accuracy: 0.9375, Computation time: 1.5455281734466553\n",
      "Step: 1532, Loss: 0.9159702658653259, Accuracy: 1.0, Computation time: 1.3628158569335938\n",
      "Step: 1533, Loss: 0.9160031080245972, Accuracy: 1.0, Computation time: 1.0825979709625244\n",
      "Step: 1534, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 1.5337467193603516\n",
      "Step: 1535, Loss: 0.9162263870239258, Accuracy: 1.0, Computation time: 1.144150972366333\n",
      "Step: 1536, Loss: 0.9159896373748779, Accuracy: 1.0, Computation time: 1.0017755031585693\n",
      "Step: 1537, Loss: 0.9159027338027954, Accuracy: 1.0, Computation time: 1.0116868019104004\n",
      "Step: 1538, Loss: 0.9373531341552734, Accuracy: 0.949999988079071, Computation time: 1.4314587116241455\n",
      "Step: 1539, Loss: 0.9364875555038452, Accuracy: 0.9772727489471436, Computation time: 1.1800847053527832\n",
      "Step: 1540, Loss: 0.9161010384559631, Accuracy: 1.0, Computation time: 1.2131893634796143\n",
      "Step: 1541, Loss: 0.915925145149231, Accuracy: 1.0, Computation time: 1.2258527278900146\n",
      "Step: 1542, Loss: 0.915882408618927, Accuracy: 1.0, Computation time: 1.090418815612793\n",
      "Step: 1543, Loss: 0.9158895015716553, Accuracy: 1.0, Computation time: 1.3999500274658203\n",
      "Step: 1544, Loss: 0.9160497188568115, Accuracy: 1.0, Computation time: 1.2999804019927979\n",
      "Step: 1545, Loss: 0.9376593232154846, Accuracy: 0.9642857313156128, Computation time: 1.1613187789916992\n",
      "Step: 1546, Loss: 0.916018009185791, Accuracy: 1.0, Computation time: 1.165553092956543\n",
      "Step: 1547, Loss: 0.9158692359924316, Accuracy: 1.0, Computation time: 1.0377848148345947\n",
      "Step: 1548, Loss: 0.9159489870071411, Accuracy: 1.0, Computation time: 1.2992298603057861\n",
      "Step: 1549, Loss: 0.9159188866615295, Accuracy: 1.0, Computation time: 1.128429651260376\n",
      "Step: 1550, Loss: 0.9165145754814148, Accuracy: 1.0, Computation time: 1.7570199966430664\n",
      "Step: 1551, Loss: 0.9372928738594055, Accuracy: 0.9791666865348816, Computation time: 1.2148387432098389\n",
      "Step: 1552, Loss: 0.9285145998001099, Accuracy: 0.9375, Computation time: 1.3404383659362793\n",
      "Step: 1553, Loss: 0.9162625670433044, Accuracy: 1.0, Computation time: 1.8631620407104492\n",
      "Step: 1554, Loss: 0.9158855676651001, Accuracy: 1.0, Computation time: 1.0000336170196533\n",
      "Step: 1555, Loss: 0.937536895275116, Accuracy: 0.9642857313156128, Computation time: 1.4166955947875977\n",
      "Step: 1556, Loss: 0.9159219861030579, Accuracy: 1.0, Computation time: 1.031245470046997\n",
      "Step: 1557, Loss: 0.9159356951713562, Accuracy: 1.0, Computation time: 1.157346248626709\n",
      "Step: 1558, Loss: 0.9160666465759277, Accuracy: 1.0, Computation time: 1.596632480621338\n",
      "Step: 1559, Loss: 0.9159044027328491, Accuracy: 1.0, Computation time: 1.256462812423706\n",
      "Step: 1560, Loss: 0.9159138202667236, Accuracy: 1.0, Computation time: 1.6657850742340088\n",
      "Step: 1561, Loss: 0.9158654808998108, Accuracy: 1.0, Computation time: 1.1636760234832764\n",
      "Step: 1562, Loss: 0.917235255241394, Accuracy: 1.0, Computation time: 1.2065870761871338\n",
      "Step: 1563, Loss: 0.9158579707145691, Accuracy: 1.0, Computation time: 1.195272445678711\n",
      "Step: 1564, Loss: 0.9160864949226379, Accuracy: 1.0, Computation time: 1.9941835403442383\n",
      "Step: 1565, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 1.2377290725708008\n",
      "Step: 1566, Loss: 0.9158880114555359, Accuracy: 1.0, Computation time: 1.4765362739562988\n",
      "Step: 1567, Loss: 0.9159343838691711, Accuracy: 1.0, Computation time: 1.5277385711669922\n",
      "Step: 1568, Loss: 0.9158872365951538, Accuracy: 1.0, Computation time: 1.3083138465881348\n",
      "Step: 1569, Loss: 0.9158843755722046, Accuracy: 1.0, Computation time: 1.0084636211395264\n",
      "Step: 1570, Loss: 0.9158675670623779, Accuracy: 1.0, Computation time: 1.0707919597625732\n",
      "Step: 1571, Loss: 0.917513906955719, Accuracy: 1.0, Computation time: 1.737705945968628\n",
      "Step: 1572, Loss: 0.9367955327033997, Accuracy: 0.9722222089767456, Computation time: 1.1957333087921143\n",
      "Step: 1573, Loss: 0.9166983962059021, Accuracy: 1.0, Computation time: 1.2162573337554932\n",
      "Step: 1574, Loss: 0.9359999299049377, Accuracy: 0.9750000238418579, Computation time: 1.3465852737426758\n",
      "Step: 1575, Loss: 0.9159137606620789, Accuracy: 1.0, Computation time: 1.08748459815979\n",
      "Step: 1576, Loss: 0.9165483713150024, Accuracy: 1.0, Computation time: 1.1047544479370117\n",
      "Step: 1577, Loss: 0.9182288646697998, Accuracy: 1.0, Computation time: 1.4336886405944824\n",
      "Step: 1578, Loss: 0.9159808158874512, Accuracy: 1.0, Computation time: 1.335099697113037\n",
      "Step: 1579, Loss: 0.9161907434463501, Accuracy: 1.0, Computation time: 1.5603296756744385\n",
      "Step: 1580, Loss: 0.9375522136688232, Accuracy: 0.9722222089767456, Computation time: 1.6387889385223389\n",
      "Step: 1581, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 1.4204022884368896\n",
      "Step: 1582, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 1.4625744819641113\n",
      "Step: 1583, Loss: 0.915895938873291, Accuracy: 1.0, Computation time: 1.055720329284668\n",
      "Step: 1584, Loss: 0.9159444570541382, Accuracy: 1.0, Computation time: 1.2599313259124756\n",
      "Step: 1585, Loss: 0.9203372597694397, Accuracy: 1.0, Computation time: 1.1265971660614014\n",
      "Step: 1586, Loss: 0.9158934950828552, Accuracy: 1.0, Computation time: 1.1952776908874512\n",
      "Step: 1587, Loss: 0.9165714979171753, Accuracy: 1.0, Computation time: 1.817870855331421\n",
      "Step: 1588, Loss: 0.9158602952957153, Accuracy: 1.0, Computation time: 1.131777286529541\n",
      "Step: 1589, Loss: 0.916140079498291, Accuracy: 1.0, Computation time: 1.5031778812408447\n",
      "Step: 1590, Loss: 0.9376327991485596, Accuracy: 0.96875, Computation time: 1.1572051048278809\n",
      "Step: 1591, Loss: 0.9159227609634399, Accuracy: 1.0, Computation time: 1.5054492950439453\n",
      "Step: 1592, Loss: 0.9158914685249329, Accuracy: 1.0, Computation time: 1.2086691856384277\n",
      "Step: 1593, Loss: 0.9159058332443237, Accuracy: 1.0, Computation time: 1.2917323112487793\n",
      "Step: 1594, Loss: 0.9161376953125, Accuracy: 1.0, Computation time: 1.171417474746704\n",
      "Step: 1595, Loss: 0.91587895154953, Accuracy: 1.0, Computation time: 1.434950351715088\n",
      "Step: 1596, Loss: 0.9158632159233093, Accuracy: 1.0, Computation time: 1.3105909824371338\n",
      "Step: 1597, Loss: 0.9160333871841431, Accuracy: 1.0, Computation time: 1.2317445278167725\n",
      "Step: 1598, Loss: 0.9158719182014465, Accuracy: 1.0, Computation time: 1.1924493312835693\n",
      "Step: 1599, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.1553082466125488\n",
      "Step: 1600, Loss: 0.9158679842948914, Accuracy: 1.0, Computation time: 1.322826623916626\n",
      "Step: 1601, Loss: 0.915860652923584, Accuracy: 1.0, Computation time: 1.2175686359405518\n",
      "Step: 1602, Loss: 0.9492443203926086, Accuracy: 0.9557692408561707, Computation time: 1.267183542251587\n",
      "Step: 1603, Loss: 0.9159818291664124, Accuracy: 1.0, Computation time: 1.4072265625\n",
      "Step: 1604, Loss: 0.9158925414085388, Accuracy: 1.0, Computation time: 1.3320679664611816\n",
      "Step: 1605, Loss: 0.9159135818481445, Accuracy: 1.0, Computation time: 1.2556960582733154\n",
      "Step: 1606, Loss: 0.9239298701286316, Accuracy: 1.0, Computation time: 1.087677001953125\n",
      "Step: 1607, Loss: 0.9171936511993408, Accuracy: 1.0, Computation time: 1.3362560272216797\n",
      "Step: 1608, Loss: 0.9158679246902466, Accuracy: 1.0, Computation time: 1.3017077445983887\n",
      "Step: 1609, Loss: 0.9159181118011475, Accuracy: 1.0, Computation time: 1.183701753616333\n",
      "Step: 1610, Loss: 0.9166085720062256, Accuracy: 1.0, Computation time: 1.2497050762176514\n",
      "Step: 1611, Loss: 0.9159637093544006, Accuracy: 1.0, Computation time: 1.0818779468536377\n",
      "Step: 1612, Loss: 0.9159600734710693, Accuracy: 1.0, Computation time: 1.1642980575561523\n",
      "Step: 1613, Loss: 0.9297935962677002, Accuracy: 0.9750000238418579, Computation time: 1.2353837490081787\n",
      "Step: 1614, Loss: 0.9159080386161804, Accuracy: 1.0, Computation time: 1.3170151710510254\n",
      "Step: 1615, Loss: 0.9161431193351746, Accuracy: 1.0, Computation time: 1.457115650177002\n",
      "Step: 1616, Loss: 0.9158990383148193, Accuracy: 1.0, Computation time: 1.1434144973754883\n",
      "Step: 1617, Loss: 0.9172618985176086, Accuracy: 1.0, Computation time: 1.7480101585388184\n",
      "Step: 1618, Loss: 0.9158636927604675, Accuracy: 1.0, Computation time: 1.2730259895324707\n",
      "Step: 1619, Loss: 0.9158732891082764, Accuracy: 1.0, Computation time: 1.2364730834960938\n",
      "Step: 1620, Loss: 0.9162995219230652, Accuracy: 1.0, Computation time: 1.4061286449432373\n",
      "Step: 1621, Loss: 0.9373504519462585, Accuracy: 0.9375, Computation time: 1.3033275604248047\n",
      "Step: 1622, Loss: 0.9158768057823181, Accuracy: 1.0, Computation time: 1.1167843341827393\n",
      "Step: 1623, Loss: 0.9158600568771362, Accuracy: 1.0, Computation time: 1.257965087890625\n",
      "Step: 1624, Loss: 0.9158687591552734, Accuracy: 1.0, Computation time: 1.2539525032043457\n",
      "Step: 1625, Loss: 0.9158775210380554, Accuracy: 1.0, Computation time: 1.2473883628845215\n",
      "Step: 1626, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.4302291870117188\n",
      "Step: 1627, Loss: 0.916621208190918, Accuracy: 1.0, Computation time: 1.5182032585144043\n",
      "Step: 1628, Loss: 0.9158848524093628, Accuracy: 1.0, Computation time: 1.2386202812194824\n",
      "Step: 1629, Loss: 0.9158970713615417, Accuracy: 1.0, Computation time: 1.2535712718963623\n",
      "Step: 1630, Loss: 0.9158864617347717, Accuracy: 1.0, Computation time: 1.1668062210083008\n",
      "Step: 1631, Loss: 0.9158722162246704, Accuracy: 1.0, Computation time: 1.4928860664367676\n",
      "Step: 1632, Loss: 0.9209127426147461, Accuracy: 1.0, Computation time: 1.6792657375335693\n",
      "Step: 1633, Loss: 0.9161253571510315, Accuracy: 1.0, Computation time: 1.706068992614746\n",
      "Step: 1634, Loss: 0.915910542011261, Accuracy: 1.0, Computation time: 1.1145350933074951\n",
      "Step: 1635, Loss: 0.9159172177314758, Accuracy: 1.0, Computation time: 1.413435459136963\n",
      "Step: 1636, Loss: 0.9159131646156311, Accuracy: 1.0, Computation time: 1.0295512676239014\n",
      "Step: 1637, Loss: 0.9171178340911865, Accuracy: 1.0, Computation time: 1.537294864654541\n",
      "Step: 1638, Loss: 0.9159238338470459, Accuracy: 1.0, Computation time: 1.3016045093536377\n",
      "Step: 1639, Loss: 0.916050910949707, Accuracy: 1.0, Computation time: 1.2499868869781494\n",
      "Step: 1640, Loss: 0.9158734679222107, Accuracy: 1.0, Computation time: 1.3271470069885254\n",
      "Step: 1641, Loss: 0.9321864247322083, Accuracy: 0.9722222089767456, Computation time: 1.2722399234771729\n",
      "Step: 1642, Loss: 0.9158905744552612, Accuracy: 1.0, Computation time: 1.3565144538879395\n",
      "Step: 1643, Loss: 0.9158879518508911, Accuracy: 1.0, Computation time: 1.2096281051635742\n",
      "Step: 1644, Loss: 0.9158895611763, Accuracy: 1.0, Computation time: 1.2341678142547607\n",
      "Step: 1645, Loss: 0.9369022250175476, Accuracy: 0.96875, Computation time: 1.354111671447754\n",
      "Step: 1646, Loss: 0.9159362316131592, Accuracy: 1.0, Computation time: 1.4832744598388672\n",
      "Step: 1647, Loss: 0.9160609841346741, Accuracy: 1.0, Computation time: 1.32289457321167\n",
      "Step: 1648, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 1.0578482151031494\n",
      "Step: 1649, Loss: 0.9376257061958313, Accuracy: 0.9791666865348816, Computation time: 1.3742916584014893\n",
      "Step: 1650, Loss: 0.9376193284988403, Accuracy: 0.9791666865348816, Computation time: 1.2478022575378418\n",
      "Step: 1651, Loss: 0.9158985018730164, Accuracy: 1.0, Computation time: 1.2397449016571045\n",
      "Step: 1652, Loss: 0.9571184515953064, Accuracy: 0.9460227489471436, Computation time: 1.1683721542358398\n",
      "Step: 1653, Loss: 0.971739649772644, Accuracy: 0.9392857551574707, Computation time: 1.7653968334197998\n",
      "Step: 1654, Loss: 0.9159183502197266, Accuracy: 1.0, Computation time: 1.1466162204742432\n",
      "Step: 1655, Loss: 0.9159330725669861, Accuracy: 1.0, Computation time: 0.9712269306182861\n",
      "Step: 1656, Loss: 0.9160488247871399, Accuracy: 1.0, Computation time: 1.1183583736419678\n",
      "Step: 1657, Loss: 0.9159764051437378, Accuracy: 1.0, Computation time: 1.145768165588379\n",
      "Step: 1658, Loss: 0.937058687210083, Accuracy: 0.9722222089767456, Computation time: 1.2668228149414062\n",
      "Step: 1659, Loss: 0.9164661765098572, Accuracy: 1.0, Computation time: 0.9611594676971436\n",
      "Step: 1660, Loss: 0.916022539138794, Accuracy: 1.0, Computation time: 1.190359115600586\n",
      "Step: 1661, Loss: 0.9159688353538513, Accuracy: 1.0, Computation time: 1.1916561126708984\n",
      "Step: 1662, Loss: 0.9158778786659241, Accuracy: 1.0, Computation time: 1.150312900543213\n",
      "Step: 1663, Loss: 0.9158821702003479, Accuracy: 1.0, Computation time: 1.0941545963287354\n",
      "Step: 1664, Loss: 0.9442616105079651, Accuracy: 0.96875, Computation time: 1.2698378562927246\n",
      "Step: 1665, Loss: 0.9159209728240967, Accuracy: 1.0, Computation time: 1.2273213863372803\n",
      "Step: 1666, Loss: 0.9159107208251953, Accuracy: 1.0, Computation time: 1.0861709117889404\n",
      "Step: 1667, Loss: 0.9159789681434631, Accuracy: 1.0, Computation time: 1.1180152893066406\n",
      "Step: 1668, Loss: 0.9164899587631226, Accuracy: 1.0, Computation time: 2.3362505435943604\n",
      "########################\n",
      "Test loss: 1.0677921772003174, Test Accuracy_epoch12: 0.7905566692352295\n",
      "########################\n",
      "Step: 1669, Loss: 0.9159517884254456, Accuracy: 1.0, Computation time: 1.4129984378814697\n",
      "Step: 1670, Loss: 0.9199170470237732, Accuracy: 1.0, Computation time: 2.312960147857666\n",
      "Step: 1671, Loss: 0.9377686381340027, Accuracy: 0.9791666865348816, Computation time: 1.378296136856079\n",
      "Step: 1672, Loss: 0.9158986806869507, Accuracy: 1.0, Computation time: 1.0521349906921387\n",
      "Step: 1673, Loss: 0.9159108996391296, Accuracy: 1.0, Computation time: 1.0821199417114258\n",
      "Step: 1674, Loss: 0.9159311652183533, Accuracy: 1.0, Computation time: 1.3716907501220703\n",
      "Step: 1675, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.191303014755249\n",
      "Step: 1676, Loss: 0.9159151315689087, Accuracy: 1.0, Computation time: 1.538961410522461\n",
      "Step: 1677, Loss: 0.9159411191940308, Accuracy: 1.0, Computation time: 1.2161219120025635\n",
      "Step: 1678, Loss: 0.9225501418113708, Accuracy: 1.0, Computation time: 1.2082538604736328\n",
      "Step: 1679, Loss: 0.9161084890365601, Accuracy: 1.0, Computation time: 1.5769169330596924\n",
      "Step: 1680, Loss: 0.9161285758018494, Accuracy: 1.0, Computation time: 1.2182950973510742\n",
      "Step: 1681, Loss: 0.9159262180328369, Accuracy: 1.0, Computation time: 1.0918378829956055\n",
      "Step: 1682, Loss: 0.9159195423126221, Accuracy: 1.0, Computation time: 1.1421480178833008\n",
      "Step: 1683, Loss: 0.9159015417098999, Accuracy: 1.0, Computation time: 1.215027093887329\n",
      "Step: 1684, Loss: 0.9164023995399475, Accuracy: 1.0, Computation time: 1.2210302352905273\n",
      "Step: 1685, Loss: 0.9274271726608276, Accuracy: 0.9642857313156128, Computation time: 1.2971532344818115\n",
      "Step: 1686, Loss: 0.9159402847290039, Accuracy: 1.0, Computation time: 1.0608470439910889\n",
      "Step: 1687, Loss: 0.9247880578041077, Accuracy: 1.0, Computation time: 1.6505038738250732\n",
      "Step: 1688, Loss: 0.9160094857215881, Accuracy: 1.0, Computation time: 1.1626298427581787\n",
      "Step: 1689, Loss: 0.9377124905586243, Accuracy: 0.9722222089767456, Computation time: 1.4944474697113037\n",
      "Step: 1690, Loss: 0.915966272354126, Accuracy: 1.0, Computation time: 1.247798204421997\n",
      "Step: 1691, Loss: 0.9160377979278564, Accuracy: 1.0, Computation time: 1.5477774143218994\n",
      "Step: 1692, Loss: 0.9160136580467224, Accuracy: 1.0, Computation time: 1.3111088275909424\n",
      "Step: 1693, Loss: 0.9232871532440186, Accuracy: 1.0, Computation time: 1.2779111862182617\n",
      "Step: 1694, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.0500473976135254\n",
      "Step: 1695, Loss: 0.9238607883453369, Accuracy: 1.0, Computation time: 1.263688087463379\n",
      "Step: 1696, Loss: 0.9159809350967407, Accuracy: 1.0, Computation time: 1.5029733180999756\n",
      "Step: 1697, Loss: 0.9372929334640503, Accuracy: 0.96875, Computation time: 1.4827165603637695\n",
      "Step: 1698, Loss: 0.9163097739219666, Accuracy: 1.0, Computation time: 1.3175687789916992\n",
      "Step: 1699, Loss: 0.9169184565544128, Accuracy: 1.0, Computation time: 1.1429462432861328\n",
      "Step: 1700, Loss: 0.9184635281562805, Accuracy: 1.0, Computation time: 1.341186285018921\n",
      "Step: 1701, Loss: 0.9160051941871643, Accuracy: 1.0, Computation time: 1.1874322891235352\n",
      "Step: 1702, Loss: 0.9159657955169678, Accuracy: 1.0, Computation time: 1.3147861957550049\n",
      "Step: 1703, Loss: 0.9159548282623291, Accuracy: 1.0, Computation time: 1.6634798049926758\n",
      "Step: 1704, Loss: 0.9159387946128845, Accuracy: 1.0, Computation time: 1.0605661869049072\n",
      "Step: 1705, Loss: 0.9159817695617676, Accuracy: 1.0, Computation time: 1.1126329898834229\n",
      "Step: 1706, Loss: 0.9212231636047363, Accuracy: 1.0, Computation time: 2.2325658798217773\n",
      "Step: 1707, Loss: 0.9302220940589905, Accuracy: 0.9642857313156128, Computation time: 1.3286948204040527\n",
      "Step: 1708, Loss: 0.9254295825958252, Accuracy: 1.0, Computation time: 2.035159111022949\n",
      "Step: 1709, Loss: 0.9199377298355103, Accuracy: 1.0, Computation time: 1.9096384048461914\n",
      "Step: 1710, Loss: 0.9163395166397095, Accuracy: 1.0, Computation time: 1.297408103942871\n",
      "Step: 1711, Loss: 0.9339454174041748, Accuracy: 0.9821428656578064, Computation time: 1.4005215167999268\n",
      "Step: 1712, Loss: 0.9185653924942017, Accuracy: 1.0, Computation time: 1.3095800876617432\n",
      "Step: 1713, Loss: 0.9162994623184204, Accuracy: 1.0, Computation time: 1.5183403491973877\n",
      "Step: 1714, Loss: 0.916760265827179, Accuracy: 1.0, Computation time: 1.666154146194458\n",
      "Step: 1715, Loss: 0.9164111614227295, Accuracy: 1.0, Computation time: 1.165234088897705\n",
      "Step: 1716, Loss: 0.9159851670265198, Accuracy: 1.0, Computation time: 1.2039313316345215\n",
      "Step: 1717, Loss: 0.9179223775863647, Accuracy: 1.0, Computation time: 1.2629280090332031\n",
      "Step: 1718, Loss: 0.9159566164016724, Accuracy: 1.0, Computation time: 1.4459569454193115\n",
      "Step: 1719, Loss: 0.9162791967391968, Accuracy: 1.0, Computation time: 1.152186632156372\n",
      "Step: 1720, Loss: 0.9161977767944336, Accuracy: 1.0, Computation time: 1.399235486984253\n",
      "Step: 1721, Loss: 0.9379850029945374, Accuracy: 0.9833333492279053, Computation time: 1.3209869861602783\n",
      "Step: 1722, Loss: 0.9163097143173218, Accuracy: 1.0, Computation time: 1.2080581188201904\n",
      "Step: 1723, Loss: 0.9164025783538818, Accuracy: 1.0, Computation time: 1.1882743835449219\n",
      "Step: 1724, Loss: 0.9164225459098816, Accuracy: 1.0, Computation time: 1.5629403591156006\n",
      "Step: 1725, Loss: 0.9178241491317749, Accuracy: 1.0, Computation time: 1.1550214290618896\n",
      "Step: 1726, Loss: 0.9159349203109741, Accuracy: 1.0, Computation time: 1.287193775177002\n",
      "Step: 1727, Loss: 0.9166467189788818, Accuracy: 1.0, Computation time: 1.1892621517181396\n",
      "Step: 1728, Loss: 0.9393362998962402, Accuracy: 0.96875, Computation time: 1.106278657913208\n",
      "Step: 1729, Loss: 0.9159000515937805, Accuracy: 1.0, Computation time: 1.0809860229492188\n",
      "Step: 1730, Loss: 0.9160011410713196, Accuracy: 1.0, Computation time: 1.4225516319274902\n",
      "Step: 1731, Loss: 0.9159454107284546, Accuracy: 1.0, Computation time: 1.1037633419036865\n",
      "Step: 1732, Loss: 0.9160515666007996, Accuracy: 1.0, Computation time: 1.6011474132537842\n",
      "Step: 1733, Loss: 0.9159431457519531, Accuracy: 1.0, Computation time: 1.239851713180542\n",
      "Step: 1734, Loss: 0.9159764647483826, Accuracy: 1.0, Computation time: 1.2004029750823975\n",
      "Step: 1735, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 1.1986267566680908\n",
      "Step: 1736, Loss: 0.9159262776374817, Accuracy: 1.0, Computation time: 1.0035111904144287\n",
      "Step: 1737, Loss: 0.9158937931060791, Accuracy: 1.0, Computation time: 1.1235840320587158\n",
      "Step: 1738, Loss: 0.9158897995948792, Accuracy: 1.0, Computation time: 1.2618024349212646\n",
      "Step: 1739, Loss: 0.9594539403915405, Accuracy: 0.9409722089767456, Computation time: 1.2077021598815918\n",
      "Step: 1740, Loss: 0.9373238682746887, Accuracy: 0.9722222089767456, Computation time: 1.3326728343963623\n",
      "Step: 1741, Loss: 0.9158762097358704, Accuracy: 1.0, Computation time: 1.0671632289886475\n",
      "Step: 1742, Loss: 0.9175014495849609, Accuracy: 1.0, Computation time: 1.3086235523223877\n",
      "Step: 1743, Loss: 0.9159180521965027, Accuracy: 1.0, Computation time: 1.155932903289795\n",
      "Step: 1744, Loss: 0.9159198999404907, Accuracy: 1.0, Computation time: 1.4581000804901123\n",
      "Step: 1745, Loss: 0.9370457530021667, Accuracy: 0.9772727489471436, Computation time: 1.5112457275390625\n",
      "Step: 1746, Loss: 0.916092038154602, Accuracy: 1.0, Computation time: 1.3398497104644775\n",
      "Step: 1747, Loss: 0.9159189462661743, Accuracy: 1.0, Computation time: 1.1220521926879883\n",
      "Step: 1748, Loss: 0.9160004258155823, Accuracy: 1.0, Computation time: 1.2986133098602295\n",
      "Step: 1749, Loss: 0.9374792575836182, Accuracy: 0.9750000238418579, Computation time: 1.2833397388458252\n",
      "Step: 1750, Loss: 0.9159305691719055, Accuracy: 1.0, Computation time: 1.1529088020324707\n",
      "Step: 1751, Loss: 0.9159271717071533, Accuracy: 1.0, Computation time: 1.1285943984985352\n",
      "Step: 1752, Loss: 0.9158969521522522, Accuracy: 1.0, Computation time: 1.3596775531768799\n",
      "Step: 1753, Loss: 0.9361138939857483, Accuracy: 0.96875, Computation time: 1.0781993865966797\n",
      "Step: 1754, Loss: 0.9158983826637268, Accuracy: 1.0, Computation time: 1.0098440647125244\n",
      "Step: 1755, Loss: 0.9158976078033447, Accuracy: 1.0, Computation time: 1.0343430042266846\n",
      "Step: 1756, Loss: 0.9376961588859558, Accuracy: 0.9722222089767456, Computation time: 1.564070224761963\n",
      "Step: 1757, Loss: 0.9210534691810608, Accuracy: 1.0, Computation time: 1.258338212966919\n",
      "Step: 1758, Loss: 0.9329042434692383, Accuracy: 0.9722222089767456, Computation time: 1.6561305522918701\n",
      "Step: 1759, Loss: 0.9159101247787476, Accuracy: 1.0, Computation time: 1.0889952182769775\n",
      "Step: 1760, Loss: 0.9159020185470581, Accuracy: 1.0, Computation time: 1.3277926445007324\n",
      "Step: 1761, Loss: 0.9162696003913879, Accuracy: 1.0, Computation time: 1.1391894817352295\n",
      "Step: 1762, Loss: 0.9377069473266602, Accuracy: 0.9722222089767456, Computation time: 1.239286184310913\n",
      "Step: 1763, Loss: 0.9159407615661621, Accuracy: 1.0, Computation time: 1.269002914428711\n",
      "Step: 1764, Loss: 0.9165278077125549, Accuracy: 1.0, Computation time: 1.257627010345459\n",
      "Step: 1765, Loss: 0.9336188435554504, Accuracy: 0.949999988079071, Computation time: 1.1711053848266602\n",
      "Step: 1766, Loss: 0.9161069989204407, Accuracy: 1.0, Computation time: 1.0302951335906982\n",
      "Step: 1767, Loss: 0.9158919453620911, Accuracy: 1.0, Computation time: 1.384251356124878\n",
      "Step: 1768, Loss: 0.9168142080307007, Accuracy: 1.0, Computation time: 1.4803130626678467\n",
      "Step: 1769, Loss: 0.9159088730812073, Accuracy: 1.0, Computation time: 1.2406482696533203\n",
      "Step: 1770, Loss: 0.9159249067306519, Accuracy: 1.0, Computation time: 1.3281047344207764\n",
      "Step: 1771, Loss: 0.9169081449508667, Accuracy: 1.0, Computation time: 2.4432902336120605\n",
      "Step: 1772, Loss: 0.9160059094429016, Accuracy: 1.0, Computation time: 1.6011760234832764\n",
      "Step: 1773, Loss: 0.9159234762191772, Accuracy: 1.0, Computation time: 0.9907660484313965\n",
      "Step: 1774, Loss: 0.9374605417251587, Accuracy: 0.9722222089767456, Computation time: 1.225522518157959\n",
      "Step: 1775, Loss: 0.9159998893737793, Accuracy: 1.0, Computation time: 1.079420566558838\n",
      "Step: 1776, Loss: 0.9158786535263062, Accuracy: nan, Computation time: 1.2385458946228027\n",
      "Step: 1777, Loss: 0.9258217215538025, Accuracy: 0.949999988079071, Computation time: 1.7258923053741455\n",
      "Step: 1778, Loss: 0.9159380793571472, Accuracy: 1.0, Computation time: 1.2661116123199463\n",
      "Step: 1779, Loss: 0.9159485101699829, Accuracy: 1.0, Computation time: 0.9285557270050049\n",
      "Step: 1780, Loss: 0.9159553647041321, Accuracy: 1.0, Computation time: 1.3681330680847168\n",
      "Step: 1781, Loss: 0.9184920787811279, Accuracy: 1.0, Computation time: 2.7021102905273438\n",
      "Step: 1782, Loss: 0.916001558303833, Accuracy: 1.0, Computation time: 1.1197185516357422\n",
      "Step: 1783, Loss: 0.9159373641014099, Accuracy: 1.0, Computation time: 1.078552007675171\n",
      "Step: 1784, Loss: 0.9160155653953552, Accuracy: 1.0, Computation time: 1.276338815689087\n",
      "Step: 1785, Loss: 0.9159462451934814, Accuracy: 1.0, Computation time: 1.0016584396362305\n",
      "Step: 1786, Loss: 0.9159331917762756, Accuracy: 1.0, Computation time: 1.1397373676300049\n",
      "Step: 1787, Loss: 0.9159479737281799, Accuracy: 1.0, Computation time: 1.051853895187378\n",
      "Step: 1788, Loss: 0.9358367323875427, Accuracy: 0.9791666865348816, Computation time: 1.2211205959320068\n",
      "Step: 1789, Loss: 0.9374879598617554, Accuracy: 0.9642857313156128, Computation time: 1.359867811203003\n",
      "Step: 1790, Loss: 0.9374896287918091, Accuracy: 0.9642857313156128, Computation time: 1.1305720806121826\n",
      "Step: 1791, Loss: 0.91594398021698, Accuracy: 1.0, Computation time: 1.2672414779663086\n",
      "Step: 1792, Loss: 0.9159488677978516, Accuracy: 1.0, Computation time: 1.1927196979522705\n",
      "Step: 1793, Loss: 0.9159465432167053, Accuracy: 1.0, Computation time: 1.0349774360656738\n",
      "Step: 1794, Loss: 0.9159568548202515, Accuracy: 1.0, Computation time: 1.2165820598602295\n",
      "Step: 1795, Loss: 0.9159631133079529, Accuracy: 1.0, Computation time: 1.0288500785827637\n",
      "Step: 1796, Loss: 0.9163463115692139, Accuracy: 1.0, Computation time: 1.7727413177490234\n",
      "Step: 1797, Loss: 0.9183835983276367, Accuracy: 1.0, Computation time: 1.6172821521759033\n",
      "Step: 1798, Loss: 0.9160046577453613, Accuracy: 1.0, Computation time: 1.5233972072601318\n",
      "Step: 1799, Loss: 0.9159625768661499, Accuracy: 1.0, Computation time: 1.196113109588623\n",
      "Step: 1800, Loss: 0.916191577911377, Accuracy: 1.0, Computation time: 1.149749994277954\n",
      "Step: 1801, Loss: 0.9166945815086365, Accuracy: 1.0, Computation time: 1.842820405960083\n",
      "Step: 1802, Loss: 0.9166212677955627, Accuracy: 1.0, Computation time: 1.343991756439209\n",
      "Step: 1803, Loss: 0.915935754776001, Accuracy: 1.0, Computation time: 1.4293200969696045\n",
      "Step: 1804, Loss: 0.9376825094223022, Accuracy: 0.9807692766189575, Computation time: 1.44488525390625\n",
      "Step: 1805, Loss: 0.9159738421440125, Accuracy: 1.0, Computation time: 1.0696966648101807\n",
      "Step: 1806, Loss: 0.9158916473388672, Accuracy: 1.0, Computation time: 1.0657141208648682\n",
      "Step: 1807, Loss: 0.9159216284751892, Accuracy: 1.0, Computation time: 1.2180936336517334\n",
      "########################\n",
      "Test loss: 1.070096492767334, Test Accuracy_epoch13: 0.7869104146957397\n",
      "########################\n",
      "Step: 1808, Loss: 0.9159050583839417, Accuracy: 1.0, Computation time: 1.4731175899505615\n",
      "Step: 1809, Loss: 0.9160058498382568, Accuracy: 1.0, Computation time: 1.2516248226165771\n",
      "Step: 1810, Loss: 0.9215143322944641, Accuracy: 1.0, Computation time: 2.211347818374634\n",
      "Step: 1811, Loss: 0.9159483313560486, Accuracy: 1.0, Computation time: 1.446009874343872\n",
      "Step: 1812, Loss: 0.9194196462631226, Accuracy: 1.0, Computation time: 1.8957862854003906\n",
      "Step: 1813, Loss: 0.9160100817680359, Accuracy: 1.0, Computation time: 1.4559752941131592\n",
      "Step: 1814, Loss: 0.9159613251686096, Accuracy: 1.0, Computation time: 1.2353322505950928\n",
      "Step: 1815, Loss: 0.9159451723098755, Accuracy: 1.0, Computation time: 1.1398391723632812\n",
      "Step: 1816, Loss: 0.9376899600028992, Accuracy: 0.9772727489471436, Computation time: 1.1672918796539307\n",
      "Step: 1817, Loss: 0.9159603118896484, Accuracy: 1.0, Computation time: 1.2077932357788086\n",
      "Step: 1818, Loss: 0.9159209132194519, Accuracy: 1.0, Computation time: 1.3640425205230713\n",
      "Step: 1819, Loss: 0.9159064292907715, Accuracy: 1.0, Computation time: 1.1220066547393799\n",
      "Step: 1820, Loss: 0.9174684882164001, Accuracy: 1.0, Computation time: 1.653296709060669\n",
      "Step: 1821, Loss: 0.9168891310691833, Accuracy: 1.0, Computation time: 1.1529200077056885\n",
      "Step: 1822, Loss: 0.916004478931427, Accuracy: 1.0, Computation time: 1.280972957611084\n",
      "Step: 1823, Loss: 0.9209718108177185, Accuracy: 1.0, Computation time: 1.580493450164795\n",
      "Step: 1824, Loss: 0.9161836504936218, Accuracy: 1.0, Computation time: 1.4652040004730225\n",
      "Step: 1825, Loss: 0.9330999851226807, Accuracy: 0.9772727489471436, Computation time: 1.675591230392456\n",
      "Step: 1826, Loss: 0.9165892004966736, Accuracy: 1.0, Computation time: 1.2596871852874756\n",
      "Step: 1827, Loss: 0.9162954092025757, Accuracy: 1.0, Computation time: 2.0278782844543457\n",
      "Step: 1828, Loss: 0.91620272397995, Accuracy: 1.0, Computation time: 1.146514892578125\n",
      "Step: 1829, Loss: 0.9379023313522339, Accuracy: 0.9833333492279053, Computation time: 1.0760748386383057\n",
      "Step: 1830, Loss: 0.9168497323989868, Accuracy: 1.0, Computation time: 1.1846094131469727\n",
      "Step: 1831, Loss: 0.9388201236724854, Accuracy: 0.9722222089767456, Computation time: 1.381361961364746\n",
      "Step: 1832, Loss: 0.9159310460090637, Accuracy: 1.0, Computation time: 1.2253925800323486\n",
      "Step: 1833, Loss: 0.9375331401824951, Accuracy: 0.9750000238418579, Computation time: 1.3293304443359375\n",
      "Step: 1834, Loss: 0.9363716840744019, Accuracy: 0.9807692766189575, Computation time: 1.0308423042297363\n",
      "Step: 1835, Loss: 0.9168774485588074, Accuracy: 1.0, Computation time: 1.56510591506958\n",
      "Step: 1836, Loss: 0.916090726852417, Accuracy: 1.0, Computation time: 1.345407485961914\n",
      "Step: 1837, Loss: 0.91605144739151, Accuracy: 1.0, Computation time: 1.281038522720337\n",
      "Step: 1838, Loss: 0.9160621762275696, Accuracy: 1.0, Computation time: 1.087848424911499\n",
      "Step: 1839, Loss: 0.920646607875824, Accuracy: 1.0, Computation time: 1.5317301750183105\n",
      "Step: 1840, Loss: 0.9163735508918762, Accuracy: 1.0, Computation time: 1.7048640251159668\n",
      "Step: 1841, Loss: 0.9374833703041077, Accuracy: 0.9642857313156128, Computation time: 1.0926222801208496\n",
      "Step: 1842, Loss: 0.9159182906150818, Accuracy: 1.0, Computation time: 1.3039429187774658\n",
      "Step: 1843, Loss: 0.9376187920570374, Accuracy: 0.9583333730697632, Computation time: 1.088770866394043\n",
      "Step: 1844, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 1.4061317443847656\n",
      "Step: 1845, Loss: 0.9159472584724426, Accuracy: 1.0, Computation time: 1.367799997329712\n",
      "Step: 1846, Loss: 0.9159842729568481, Accuracy: 1.0, Computation time: 1.2742042541503906\n",
      "Step: 1847, Loss: 0.916581392288208, Accuracy: 1.0, Computation time: 1.3926186561584473\n",
      "Step: 1848, Loss: 0.9159748554229736, Accuracy: 1.0, Computation time: 1.0397427082061768\n",
      "Step: 1849, Loss: 0.9380381107330322, Accuracy: 0.9772727489471436, Computation time: 1.7918219566345215\n",
      "Step: 1850, Loss: 0.9377279877662659, Accuracy: 0.9750000238418579, Computation time: 1.538635015487671\n",
      "Step: 1851, Loss: 0.9159219264984131, Accuracy: 1.0, Computation time: 1.0200552940368652\n",
      "Step: 1852, Loss: 0.9160354733467102, Accuracy: 1.0, Computation time: 1.469144344329834\n",
      "Step: 1853, Loss: 0.9158650636672974, Accuracy: 1.0, Computation time: 1.131709337234497\n",
      "Step: 1854, Loss: 0.9376054406166077, Accuracy: 0.949999988079071, Computation time: 1.1580712795257568\n",
      "Step: 1855, Loss: 0.9158702492713928, Accuracy: 1.0, Computation time: 1.1888747215270996\n",
      "Step: 1856, Loss: 0.9158800840377808, Accuracy: 1.0, Computation time: 1.0924408435821533\n",
      "Step: 1857, Loss: 0.9158941507339478, Accuracy: 1.0, Computation time: 1.0813939571380615\n",
      "Step: 1858, Loss: 0.9211807250976562, Accuracy: 1.0, Computation time: 1.3924741744995117\n",
      "Step: 1859, Loss: 0.9159659147262573, Accuracy: 1.0, Computation time: 1.8317089080810547\n",
      "Step: 1860, Loss: 0.9375697374343872, Accuracy: 0.9772727489471436, Computation time: 1.031771183013916\n",
      "Step: 1861, Loss: 0.9159303903579712, Accuracy: 1.0, Computation time: 1.2314918041229248\n",
      "Step: 1862, Loss: 0.9158576726913452, Accuracy: 1.0, Computation time: 1.1420032978057861\n",
      "Step: 1863, Loss: 0.9159818291664124, Accuracy: 1.0, Computation time: 2.0141289234161377\n",
      "Step: 1864, Loss: 0.915902316570282, Accuracy: 1.0, Computation time: 1.2306890487670898\n",
      "Step: 1865, Loss: 0.91649329662323, Accuracy: 1.0, Computation time: 1.2529857158660889\n",
      "Step: 1866, Loss: 0.9158746004104614, Accuracy: 1.0, Computation time: 1.2772245407104492\n",
      "Step: 1867, Loss: 0.9159001708030701, Accuracy: 1.0, Computation time: 1.1920585632324219\n",
      "Step: 1868, Loss: 0.9158912301063538, Accuracy: 1.0, Computation time: 1.2747910022735596\n",
      "Step: 1869, Loss: 0.9158742427825928, Accuracy: 1.0, Computation time: 1.0693132877349854\n",
      "Step: 1870, Loss: 0.937641978263855, Accuracy: 0.9642857313156128, Computation time: 1.2474682331085205\n",
      "Step: 1871, Loss: 0.915877103805542, Accuracy: 1.0, Computation time: 1.331233263015747\n",
      "Step: 1872, Loss: 0.9158931374549866, Accuracy: 1.0, Computation time: 1.1219284534454346\n",
      "Step: 1873, Loss: 0.9158791899681091, Accuracy: 1.0, Computation time: 1.2479126453399658\n",
      "Step: 1874, Loss: 0.9158943891525269, Accuracy: 1.0, Computation time: 1.4548931121826172\n",
      "Step: 1875, Loss: 0.9376944899559021, Accuracy: 0.9791666865348816, Computation time: 1.1416563987731934\n",
      "Step: 1876, Loss: 0.9158822298049927, Accuracy: 1.0, Computation time: 1.2662255764007568\n",
      "Step: 1877, Loss: 0.9158552885055542, Accuracy: 1.0, Computation time: 1.171558141708374\n",
      "Step: 1878, Loss: 0.915893018245697, Accuracy: 1.0, Computation time: 1.370945930480957\n",
      "Step: 1879, Loss: 0.9172793030738831, Accuracy: 1.0, Computation time: 1.413135290145874\n",
      "Step: 1880, Loss: 0.9160142540931702, Accuracy: 1.0, Computation time: 1.1087450981140137\n",
      "Step: 1881, Loss: 0.9160072207450867, Accuracy: 1.0, Computation time: 1.4079999923706055\n",
      "Step: 1882, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.5087640285491943\n",
      "Step: 1883, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.1078333854675293\n",
      "Step: 1884, Loss: 0.915926456451416, Accuracy: 1.0, Computation time: 1.4159300327301025\n",
      "Step: 1885, Loss: 0.9159117341041565, Accuracy: 1.0, Computation time: 1.2310421466827393\n",
      "Step: 1886, Loss: 0.9158689379692078, Accuracy: 1.0, Computation time: 1.1095995903015137\n",
      "Step: 1887, Loss: 0.9159277677536011, Accuracy: 1.0, Computation time: 1.7323107719421387\n",
      "Step: 1888, Loss: 0.9158599972724915, Accuracy: 1.0, Computation time: 1.1329984664916992\n",
      "Step: 1889, Loss: 0.9158957600593567, Accuracy: 1.0, Computation time: 1.3363265991210938\n",
      "Step: 1890, Loss: 0.9346803426742554, Accuracy: 0.9772727489471436, Computation time: 1.2658839225769043\n",
      "Step: 1891, Loss: 0.9169376492500305, Accuracy: 1.0, Computation time: 1.2498681545257568\n",
      "Step: 1892, Loss: 0.9159030914306641, Accuracy: 1.0, Computation time: 1.0158970355987549\n",
      "Step: 1893, Loss: 0.9159214496612549, Accuracy: 1.0, Computation time: 1.2192213535308838\n",
      "Step: 1894, Loss: 0.9159616827964783, Accuracy: 1.0, Computation time: 1.125880479812622\n",
      "Step: 1895, Loss: 0.9159225821495056, Accuracy: 1.0, Computation time: 1.24821138381958\n",
      "Step: 1896, Loss: 0.9158958792686462, Accuracy: 1.0, Computation time: 1.1772596836090088\n",
      "Step: 1897, Loss: 0.9158889055252075, Accuracy: 1.0, Computation time: 1.5227665901184082\n",
      "Step: 1898, Loss: 0.9158822894096375, Accuracy: 1.0, Computation time: 1.0770409107208252\n",
      "Step: 1899, Loss: 0.9158652424812317, Accuracy: 1.0, Computation time: 1.6016602516174316\n",
      "Step: 1900, Loss: 0.9158647060394287, Accuracy: 1.0, Computation time: 1.0417377948760986\n",
      "Step: 1901, Loss: 0.9158597588539124, Accuracy: 1.0, Computation time: 1.3589260578155518\n",
      "Step: 1902, Loss: 0.9580376744270325, Accuracy: 0.9166666865348816, Computation time: 1.7806344032287598\n",
      "Step: 1903, Loss: 0.9366286396980286, Accuracy: 0.9807692766189575, Computation time: 1.698819875717163\n",
      "Step: 1904, Loss: 0.9158812761306763, Accuracy: 1.0, Computation time: 0.9398496150970459\n",
      "Step: 1905, Loss: 0.9159309267997742, Accuracy: 1.0, Computation time: 1.6142475605010986\n",
      "Step: 1906, Loss: 0.9159273505210876, Accuracy: 1.0, Computation time: 1.7370975017547607\n",
      "Step: 1907, Loss: 0.9165278077125549, Accuracy: 1.0, Computation time: 1.6030590534210205\n",
      "Step: 1908, Loss: 0.9159256815910339, Accuracy: 1.0, Computation time: 0.9825584888458252\n",
      "Step: 1909, Loss: 0.9159145951271057, Accuracy: 1.0, Computation time: 1.1836161613464355\n",
      "Step: 1910, Loss: 0.9159590601921082, Accuracy: 1.0, Computation time: 0.9597251415252686\n",
      "Step: 1911, Loss: 0.9158867001533508, Accuracy: 1.0, Computation time: 1.173353672027588\n",
      "Step: 1912, Loss: 0.9159021377563477, Accuracy: 1.0, Computation time: 1.6479151248931885\n",
      "Step: 1913, Loss: 0.915863573551178, Accuracy: 1.0, Computation time: 1.143413782119751\n",
      "Step: 1914, Loss: 0.9158523082733154, Accuracy: 1.0, Computation time: 1.1042382717132568\n",
      "Step: 1915, Loss: 0.9231538772583008, Accuracy: 1.0, Computation time: 2.1484227180480957\n",
      "Step: 1916, Loss: 0.9159188270568848, Accuracy: 1.0, Computation time: 1.1045093536376953\n",
      "Step: 1917, Loss: 0.9158887267112732, Accuracy: 1.0, Computation time: 0.9583578109741211\n",
      "Step: 1918, Loss: 0.9159594774246216, Accuracy: 1.0, Computation time: 1.1481077671051025\n",
      "Step: 1919, Loss: 0.9160258769989014, Accuracy: 1.0, Computation time: 1.1241843700408936\n",
      "Step: 1920, Loss: 0.9162406325340271, Accuracy: 1.0, Computation time: 1.3257274627685547\n",
      "Step: 1921, Loss: 0.9367899894714355, Accuracy: 0.9642857313156128, Computation time: 1.6047775745391846\n",
      "Step: 1922, Loss: 0.9164483547210693, Accuracy: 1.0, Computation time: 1.421849012374878\n",
      "Step: 1923, Loss: 0.917137861251831, Accuracy: 1.0, Computation time: 1.0985066890716553\n",
      "Step: 1924, Loss: 0.9158831238746643, Accuracy: 1.0, Computation time: 1.1205568313598633\n",
      "Step: 1925, Loss: 0.9159014225006104, Accuracy: 1.0, Computation time: 1.3132221698760986\n",
      "Step: 1926, Loss: 0.9231671094894409, Accuracy: 1.0, Computation time: 2.184101104736328\n",
      "Step: 1927, Loss: 0.9159834980964661, Accuracy: 1.0, Computation time: 1.1237525939941406\n",
      "Step: 1928, Loss: 0.9294092655181885, Accuracy: 0.9642857313156128, Computation time: 1.2078723907470703\n",
      "Step: 1929, Loss: 0.936480700969696, Accuracy: 0.96875, Computation time: 1.3463361263275146\n",
      "Step: 1930, Loss: 0.9177744388580322, Accuracy: 1.0, Computation time: 1.6133100986480713\n",
      "Step: 1931, Loss: 0.9162703156471252, Accuracy: 1.0, Computation time: 1.0045192241668701\n",
      "Step: 1932, Loss: 0.9166513085365295, Accuracy: 1.0, Computation time: 1.2269246578216553\n",
      "Step: 1933, Loss: 0.9160281419754028, Accuracy: 1.0, Computation time: 1.0292026996612549\n",
      "Step: 1934, Loss: 0.9159555435180664, Accuracy: 1.0, Computation time: 1.4198894500732422\n",
      "Step: 1935, Loss: 0.9159728288650513, Accuracy: 1.0, Computation time: 1.0359084606170654\n",
      "Step: 1936, Loss: 0.9158886075019836, Accuracy: 1.0, Computation time: 1.3581907749176025\n",
      "Step: 1937, Loss: 0.915959358215332, Accuracy: 1.0, Computation time: 1.37042236328125\n",
      "Step: 1938, Loss: 0.9160217046737671, Accuracy: 1.0, Computation time: 2.146669387817383\n",
      "Step: 1939, Loss: 0.9159343838691711, Accuracy: 1.0, Computation time: 1.560537576675415\n",
      "Step: 1940, Loss: 0.9363858699798584, Accuracy: 0.9833333492279053, Computation time: 1.5541777610778809\n",
      "Step: 1941, Loss: 0.9234912395477295, Accuracy: 1.0, Computation time: 1.4658234119415283\n",
      "Step: 1942, Loss: 0.9378321766853333, Accuracy: 0.949999988079071, Computation time: 2.514616012573242\n",
      "Step: 1943, Loss: 0.9207141995429993, Accuracy: 1.0, Computation time: 1.613633632659912\n",
      "Step: 1944, Loss: 0.9173440933227539, Accuracy: 1.0, Computation time: 1.5266854763031006\n",
      "Step: 1945, Loss: 0.9161636233329773, Accuracy: 1.0, Computation time: 1.1062471866607666\n",
      "Step: 1946, Loss: 0.9162111282348633, Accuracy: 1.0, Computation time: 1.1912074089050293\n",
      "########################\n",
      "Test loss: 1.0690464973449707, Test Accuracy_epoch14: 0.7866016626358032\n",
      "########################\n",
      "Step: 1947, Loss: 0.9377560615539551, Accuracy: 0.9722222089767456, Computation time: 1.309178113937378\n",
      "Step: 1948, Loss: 0.9364244341850281, Accuracy: 0.9642857313156128, Computation time: 1.1095926761627197\n",
      "Step: 1949, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 1.1209330558776855\n",
      "Step: 1950, Loss: 0.9158819913864136, Accuracy: 1.0, Computation time: 0.9932098388671875\n",
      "Step: 1951, Loss: 0.9160447716712952, Accuracy: 1.0, Computation time: 1.12296462059021\n",
      "Step: 1952, Loss: 0.9159393310546875, Accuracy: 1.0, Computation time: 1.1163933277130127\n",
      "Step: 1953, Loss: 0.9158967733383179, Accuracy: 1.0, Computation time: 1.0975406169891357\n",
      "Step: 1954, Loss: 0.9190146327018738, Accuracy: 1.0, Computation time: 2.3713748455047607\n",
      "Step: 1955, Loss: 0.9159607291221619, Accuracy: 1.0, Computation time: 1.3818504810333252\n",
      "Step: 1956, Loss: 0.9163527488708496, Accuracy: 1.0, Computation time: 1.1953740119934082\n",
      "Step: 1957, Loss: 0.9167447686195374, Accuracy: 1.0, Computation time: 1.248847484588623\n",
      "Step: 1958, Loss: 0.9167661666870117, Accuracy: 1.0, Computation time: 1.3817551136016846\n",
      "Step: 1959, Loss: 0.9179871678352356, Accuracy: 1.0, Computation time: 1.111405611038208\n",
      "Step: 1960, Loss: 0.916377067565918, Accuracy: 1.0, Computation time: 1.2456014156341553\n",
      "Step: 1961, Loss: 0.9170289039611816, Accuracy: 1.0, Computation time: 1.5096662044525146\n",
      "Step: 1962, Loss: 0.9164259433746338, Accuracy: 1.0, Computation time: 1.9750008583068848\n",
      "Step: 1963, Loss: 0.9388585090637207, Accuracy: 0.9750000238418579, Computation time: 1.303222417831421\n",
      "Step: 1964, Loss: 0.9162997007369995, Accuracy: 1.0, Computation time: 1.285264492034912\n",
      "Step: 1965, Loss: 0.9159316420555115, Accuracy: 1.0, Computation time: 0.9425489902496338\n",
      "Step: 1966, Loss: 0.915939211845398, Accuracy: 1.0, Computation time: 1.3772962093353271\n",
      "Step: 1967, Loss: 0.9253665804862976, Accuracy: 1.0, Computation time: 1.6680657863616943\n",
      "Step: 1968, Loss: 0.9160887002944946, Accuracy: 1.0, Computation time: 1.2582666873931885\n",
      "Step: 1969, Loss: 0.916229248046875, Accuracy: 1.0, Computation time: 1.857614278793335\n",
      "Step: 1970, Loss: 0.9167656898498535, Accuracy: 1.0, Computation time: 1.2568752765655518\n",
      "Step: 1971, Loss: 0.9161157011985779, Accuracy: 1.0, Computation time: 1.1160657405853271\n",
      "Step: 1972, Loss: 0.9160595536231995, Accuracy: 1.0, Computation time: 1.294152021408081\n",
      "Step: 1973, Loss: 0.9160481095314026, Accuracy: 1.0, Computation time: 1.2627067565917969\n",
      "Step: 1974, Loss: 0.9160506725311279, Accuracy: 1.0, Computation time: 1.2131562232971191\n",
      "Step: 1975, Loss: 0.9402220249176025, Accuracy: 0.9722222089767456, Computation time: 1.25925874710083\n",
      "Step: 1976, Loss: 0.9159733653068542, Accuracy: 1.0, Computation time: 0.9775195121765137\n",
      "Step: 1977, Loss: 0.915938138961792, Accuracy: 1.0, Computation time: 1.2146108150482178\n",
      "Step: 1978, Loss: 0.9159489870071411, Accuracy: 1.0, Computation time: 1.0782103538513184\n",
      "Step: 1979, Loss: 0.9160006642341614, Accuracy: 1.0, Computation time: 1.1883950233459473\n",
      "Step: 1980, Loss: 0.9159109592437744, Accuracy: 1.0, Computation time: 1.0705056190490723\n",
      "Step: 1981, Loss: 0.9530316591262817, Accuracy: 0.9444444179534912, Computation time: 1.0236496925354004\n",
      "Step: 1982, Loss: 0.9159237146377563, Accuracy: 1.0, Computation time: 0.9985678195953369\n",
      "Step: 1983, Loss: 0.9160546064376831, Accuracy: 1.0, Computation time: 1.0323600769042969\n",
      "Step: 1984, Loss: 0.9160273671150208, Accuracy: 1.0, Computation time: 1.23760986328125\n",
      "Step: 1985, Loss: 0.9159036874771118, Accuracy: 1.0, Computation time: 1.074082851409912\n",
      "Step: 1986, Loss: 0.9158998727798462, Accuracy: 1.0, Computation time: 1.3474421501159668\n",
      "Step: 1987, Loss: 0.9189141392707825, Accuracy: 1.0, Computation time: 1.3980684280395508\n",
      "Step: 1988, Loss: 0.9159458875656128, Accuracy: 1.0, Computation time: 1.165898323059082\n",
      "Step: 1989, Loss: 0.9159970879554749, Accuracy: 1.0, Computation time: 1.7827820777893066\n",
      "Step: 1990, Loss: 0.9375820159912109, Accuracy: 0.9722222089767456, Computation time: 0.931046724319458\n",
      "Step: 1991, Loss: 0.9166610836982727, Accuracy: 1.0, Computation time: 1.368873119354248\n",
      "Step: 1992, Loss: 0.9159232378005981, Accuracy: 1.0, Computation time: 1.0837302207946777\n",
      "Step: 1993, Loss: 0.9176611304283142, Accuracy: 1.0, Computation time: 1.112276315689087\n",
      "Step: 1994, Loss: 0.9375059604644775, Accuracy: 0.9791666865348816, Computation time: 1.21077299118042\n",
      "Step: 1995, Loss: 0.9164159893989563, Accuracy: 1.0, Computation time: 1.621572732925415\n",
      "Step: 1996, Loss: 0.9159270524978638, Accuracy: 1.0, Computation time: 0.9932651519775391\n",
      "Step: 1997, Loss: 0.9166648983955383, Accuracy: 1.0, Computation time: 0.9722366333007812\n",
      "Step: 1998, Loss: 0.9159258008003235, Accuracy: 1.0, Computation time: 1.1248233318328857\n",
      "Step: 1999, Loss: 0.9231334924697876, Accuracy: 1.0, Computation time: 1.7688407897949219\n",
      "Step: 2000, Loss: 0.916366457939148, Accuracy: 1.0, Computation time: 1.2288315296173096\n",
      "Step: 2001, Loss: 0.9160450100898743, Accuracy: 1.0, Computation time: 1.1493077278137207\n",
      "Step: 2002, Loss: 0.915992796421051, Accuracy: 1.0, Computation time: 1.0852069854736328\n",
      "Step: 2003, Loss: 0.9221078753471375, Accuracy: 1.0, Computation time: 1.3817663192749023\n",
      "Step: 2004, Loss: 0.9161166548728943, Accuracy: nan, Computation time: 0.9113898277282715\n",
      "Step: 2005, Loss: 0.916964054107666, Accuracy: 1.0, Computation time: 1.7619495391845703\n",
      "Step: 2006, Loss: 0.9160854816436768, Accuracy: 1.0, Computation time: 1.091233491897583\n",
      "Step: 2007, Loss: 0.9159156084060669, Accuracy: 1.0, Computation time: 1.525986909866333\n",
      "Step: 2008, Loss: 0.923966646194458, Accuracy: 1.0, Computation time: 1.1104629039764404\n",
      "Step: 2009, Loss: 0.9158972501754761, Accuracy: 1.0, Computation time: 1.2001359462738037\n",
      "Step: 2010, Loss: 0.915947675704956, Accuracy: 1.0, Computation time: 1.2685184478759766\n",
      "Step: 2011, Loss: 0.9159776568412781, Accuracy: 1.0, Computation time: 0.9899694919586182\n",
      "Step: 2012, Loss: 0.916007399559021, Accuracy: 1.0, Computation time: 1.1274302005767822\n",
      "Step: 2013, Loss: 0.9160659909248352, Accuracy: 1.0, Computation time: 1.2090535163879395\n",
      "Step: 2014, Loss: 0.9160373210906982, Accuracy: 1.0, Computation time: 1.910857915878296\n",
      "Step: 2015, Loss: 0.9160107374191284, Accuracy: 1.0, Computation time: 1.2821106910705566\n",
      "Step: 2016, Loss: 0.9158928990364075, Accuracy: 1.0, Computation time: 1.0687100887298584\n",
      "Step: 2017, Loss: 0.9159321784973145, Accuracy: 1.0, Computation time: 1.268873691558838\n",
      "Step: 2018, Loss: 0.9160995483398438, Accuracy: 1.0, Computation time: 1.2218167781829834\n",
      "Step: 2019, Loss: 0.9376125931739807, Accuracy: 0.949999988079071, Computation time: 1.2937862873077393\n",
      "Step: 2020, Loss: 0.915901243686676, Accuracy: 1.0, Computation time: 1.331768274307251\n",
      "Step: 2021, Loss: 0.9159122705459595, Accuracy: 1.0, Computation time: 1.5021789073944092\n",
      "Step: 2022, Loss: 0.937620997428894, Accuracy: 0.9750000238418579, Computation time: 1.0647637844085693\n",
      "Step: 2023, Loss: 0.9158855080604553, Accuracy: 1.0, Computation time: 1.2107734680175781\n",
      "Step: 2024, Loss: 0.9158938527107239, Accuracy: 1.0, Computation time: 1.2082443237304688\n",
      "Step: 2025, Loss: 0.9159021973609924, Accuracy: 1.0, Computation time: 1.0656392574310303\n",
      "Step: 2026, Loss: 0.9158810377120972, Accuracy: 1.0, Computation time: 1.267155647277832\n",
      "Step: 2027, Loss: 0.9249829649925232, Accuracy: 1.0, Computation time: 1.5734915733337402\n",
      "Step: 2028, Loss: 0.9165946841239929, Accuracy: 1.0, Computation time: 1.3006336688995361\n",
      "Step: 2029, Loss: 0.915944516658783, Accuracy: 1.0, Computation time: 1.4349477291107178\n",
      "Step: 2030, Loss: 0.9325360059738159, Accuracy: 0.9772727489471436, Computation time: 1.4421296119689941\n",
      "Step: 2031, Loss: 0.9159588813781738, Accuracy: 1.0, Computation time: 1.176971197128296\n",
      "Step: 2032, Loss: 0.9159745573997498, Accuracy: 1.0, Computation time: 1.6086385250091553\n",
      "Step: 2033, Loss: 0.916130006313324, Accuracy: 1.0, Computation time: 1.274780035018921\n",
      "Step: 2034, Loss: 0.9357544183731079, Accuracy: 0.9722222089767456, Computation time: 1.4902169704437256\n",
      "Step: 2035, Loss: 0.9160937070846558, Accuracy: 1.0, Computation time: 1.5617625713348389\n",
      "Step: 2036, Loss: 0.9159131050109863, Accuracy: 1.0, Computation time: 1.3794746398925781\n",
      "Step: 2037, Loss: 0.9159336090087891, Accuracy: 1.0, Computation time: 1.5783510208129883\n",
      "Step: 2038, Loss: 0.9159589409828186, Accuracy: 1.0, Computation time: 1.1027507781982422\n",
      "Step: 2039, Loss: 0.9160330295562744, Accuracy: 1.0, Computation time: 1.5329055786132812\n",
      "Step: 2040, Loss: 0.9374405741691589, Accuracy: 0.9772727489471436, Computation time: 1.0366621017456055\n",
      "Step: 2041, Loss: 0.9158710241317749, Accuracy: 1.0, Computation time: 1.206995964050293\n",
      "Step: 2042, Loss: 0.915894091129303, Accuracy: 1.0, Computation time: 1.2157740592956543\n",
      "Step: 2043, Loss: 0.9161216020584106, Accuracy: 1.0, Computation time: 1.52091383934021\n",
      "Step: 2044, Loss: 0.9158910512924194, Accuracy: 1.0, Computation time: 1.1836435794830322\n",
      "Step: 2045, Loss: 0.9160289168357849, Accuracy: 1.0, Computation time: 1.3245649337768555\n",
      "Step: 2046, Loss: 0.9158770442008972, Accuracy: 1.0, Computation time: 1.2941100597381592\n",
      "Step: 2047, Loss: 0.9222375154495239, Accuracy: 1.0, Computation time: 1.6322357654571533\n",
      "Step: 2048, Loss: 0.9158700704574585, Accuracy: 1.0, Computation time: 1.217219352722168\n",
      "Step: 2049, Loss: 0.91590815782547, Accuracy: 1.0, Computation time: 1.7903773784637451\n",
      "Step: 2050, Loss: 0.9364842772483826, Accuracy: 0.96875, Computation time: 1.1961238384246826\n",
      "Step: 2051, Loss: 0.9162859916687012, Accuracy: 1.0, Computation time: 1.2831459045410156\n",
      "Step: 2052, Loss: 0.9159712791442871, Accuracy: 1.0, Computation time: 1.111013650894165\n",
      "Step: 2053, Loss: 0.9159314036369324, Accuracy: 1.0, Computation time: 0.9454972743988037\n",
      "Step: 2054, Loss: 0.915980875492096, Accuracy: 1.0, Computation time: 1.4290428161621094\n",
      "Step: 2055, Loss: 0.9376845955848694, Accuracy: 0.96875, Computation time: 1.2387547492980957\n",
      "Step: 2056, Loss: 0.9158557057380676, Accuracy: 1.0, Computation time: 1.30074143409729\n",
      "Step: 2057, Loss: 0.9158708453178406, Accuracy: 1.0, Computation time: 1.1667075157165527\n",
      "Step: 2058, Loss: 0.9158728718757629, Accuracy: 1.0, Computation time: 1.2331104278564453\n",
      "Step: 2059, Loss: 0.9278520345687866, Accuracy: 0.9642857313156128, Computation time: 1.255417823791504\n",
      "Step: 2060, Loss: 0.9158965945243835, Accuracy: 1.0, Computation time: 1.4225339889526367\n",
      "Step: 2061, Loss: 0.9166796803474426, Accuracy: 1.0, Computation time: 1.1603140830993652\n",
      "Step: 2062, Loss: 0.9158918857574463, Accuracy: 1.0, Computation time: 1.1454555988311768\n",
      "Step: 2063, Loss: 0.9159545302391052, Accuracy: 1.0, Computation time: 1.183565616607666\n",
      "Step: 2064, Loss: 0.9372324347496033, Accuracy: 0.9583333730697632, Computation time: 1.3567922115325928\n",
      "Step: 2065, Loss: 0.9224981665611267, Accuracy: 1.0, Computation time: 2.4889445304870605\n",
      "Step: 2066, Loss: 0.9159432649612427, Accuracy: 1.0, Computation time: 1.184629201889038\n",
      "Step: 2067, Loss: 0.9159590005874634, Accuracy: 1.0, Computation time: 1.0603067874908447\n",
      "Step: 2068, Loss: 0.916066586971283, Accuracy: 1.0, Computation time: 1.0663633346557617\n",
      "Step: 2069, Loss: 0.939852237701416, Accuracy: 0.9772727489471436, Computation time: 1.2078447341918945\n",
      "Step: 2070, Loss: 0.9164436459541321, Accuracy: 1.0, Computation time: 1.24410080909729\n",
      "Step: 2071, Loss: 0.9160808324813843, Accuracy: 1.0, Computation time: 1.420884370803833\n",
      "Step: 2072, Loss: 0.9162850379943848, Accuracy: 1.0, Computation time: 1.3259329795837402\n",
      "Step: 2073, Loss: 0.9160471558570862, Accuracy: 1.0, Computation time: 1.209923267364502\n",
      "Step: 2074, Loss: 0.9376802444458008, Accuracy: 0.9772727489471436, Computation time: 1.1777195930480957\n",
      "Step: 2075, Loss: 0.9360904693603516, Accuracy: 0.9772727489471436, Computation time: 1.1509573459625244\n",
      "Step: 2076, Loss: 0.9159248471260071, Accuracy: 1.0, Computation time: 1.3351352214813232\n",
      "Step: 2077, Loss: 0.9159240126609802, Accuracy: 1.0, Computation time: 1.5068583488464355\n",
      "Step: 2078, Loss: 0.9376123547554016, Accuracy: 0.9807692766189575, Computation time: 1.2346272468566895\n",
      "Step: 2079, Loss: 0.9159407019615173, Accuracy: 1.0, Computation time: 1.4380340576171875\n",
      "Step: 2080, Loss: 0.951361894607544, Accuracy: 0.918749988079071, Computation time: 2.318070888519287\n",
      "Step: 2081, Loss: 0.9169527292251587, Accuracy: 1.0, Computation time: 1.4021391868591309\n",
      "Step: 2082, Loss: 0.915986955165863, Accuracy: 1.0, Computation time: 1.3033332824707031\n",
      "Step: 2083, Loss: 0.915991485118866, Accuracy: 1.0, Computation time: 1.2559046745300293\n",
      "Step: 2084, Loss: 0.915981650352478, Accuracy: 1.0, Computation time: 1.3148219585418701\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d7dcd-6b61-479b-89a0-75bed2496948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python jaxpy39",
   "language": "python",
   "name": "jaxpy39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
