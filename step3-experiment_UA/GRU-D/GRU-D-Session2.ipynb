{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b46d105-9009-4c68-86b1-4efb9b036cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import controldiffeq\n",
    "\n",
    "class _GRU(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, use_intensity):\n",
    "        super(_GRU, self).__init__()\n",
    "\n",
    "        assert (input_channels % 2) == 1, \"Input channels must be odd: 1 for time, plus 1 for each actual input, \" \\\n",
    "                                          \"plus 1 for whether an observation was made for the actual input.\"\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.use_intensity = use_intensity\n",
    "\n",
    "        gru_channels = input_channels if use_intensity else (input_channels - 1) // 2\n",
    "        self.gru_cell = torch.nn.GRUCell(input_size=gru_channels, hidden_size=hidden_channels)\n",
    "        self.linear = torch.nn.Linear(hidden_channels, output_channels)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"input_channels={}, hidden_channels={}, output_channels={}, use_intensity={}\" \\\n",
    "               \"\".format(self.input_channels, self.hidden_channels, self.output_channels, self.use_intensity)\n",
    "\n",
    "    def evolve(self, h, time_diff):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _step(self, Xi, h, dt, half_num_channels):\n",
    "        observation = Xi[:, 1: 1 + half_num_channels].max(dim=1).values > 0.5\n",
    "        if observation.any():\n",
    "            Xi_piece = Xi if self.use_intensity else Xi[:, 1 + half_num_channels:]\n",
    "            Xi_piece = Xi_piece.clone()\n",
    "            Xi_piece[:, 0] += dt\n",
    "            new_h = self.gru_cell(Xi_piece, h)\n",
    "            h = torch.where(observation.unsqueeze(1), new_h, h)\n",
    "            dt += torch.where(observation, torch.tensor(0., dtype=Xi.dtype, device=Xi.device), Xi[:, 0])\n",
    "        return h, dt\n",
    "\n",
    "    def forward(self, times, coeffs, final_index, z0=None):\n",
    "        interp = controldiffeq.NaturalCubicSpline(times, coeffs)\n",
    "        X = torch.stack([interp.evaluate(t) for t in times], dim=-2)\n",
    "        half_num_channels = (self.input_channels - 1) // 2\n",
    "\n",
    "        # change cumulative intensity into intensity i.e. was an observation made or not, which is what is typically\n",
    "        # used here\n",
    "        X[:, 1:, 1:1 + half_num_channels] -= X[:, :-1, 1:1 + half_num_channels]\n",
    "\n",
    "        # change times into delta-times\n",
    "        X[:, 0, 0] -= times[0]\n",
    "        X[:, 1:, 0] -= times[:-1]\n",
    "\n",
    "        batch_dims = X.shape[:-2]\n",
    "\n",
    "        if z0 is None:\n",
    "            z0 = torch.zeros(*batch_dims, self.hidden_channels, dtype=X.dtype, device=X.device)\n",
    "\n",
    "        X_unbound = X.unbind(dim=1)\n",
    "        h, dt = self._step(X_unbound[0], z0, torch.zeros(*batch_dims, dtype=X.dtype, device=X.device),\n",
    "                           half_num_channels)\n",
    "        hs = [h]\n",
    "        time_diffs = times[1:] - times[:-1]\n",
    "        for time_diff, Xi in zip(time_diffs, X_unbound[1:]):\n",
    "            h = self.evolve(h, time_diff)\n",
    "            h, dt = self._step(Xi, h, dt, half_num_channels)\n",
    "            hs.append(h)\n",
    "        out = torch.stack(hs, dim=1)\n",
    "\n",
    "        final_index_indices = final_index.unsqueeze(-1).expand(out.size(0), out.size(2)).unsqueeze(1)\n",
    "        final_out = out.gather(dim=1, index=final_index_indices).squeeze(1)\n",
    "\n",
    "        return self.linear(final_out)\n",
    "\n",
    "class GRU_D(_GRU):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, use_intensity):\n",
    "        super(GRU_D, self).__init__(input_channels=input_channels,\n",
    "                                    hidden_channels=hidden_channels,\n",
    "                                    output_channels=output_channels,\n",
    "                                    use_intensity=use_intensity)\n",
    "        self.decay = torch.nn.Linear(1, hidden_channels)\n",
    "\n",
    "    def evolve(self, h, time_diff):\n",
    "        return h * torch.exp(-self.decay(time_diff.unsqueeze(0)).squeeze(0).relu())\n",
    "\n",
    "\n",
    "class EmotionClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, use_intensity, num_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=input_channels, out_channels=7, kernel_size=1)\n",
    "        self.gru_D = GRU_D(7, hidden_channels, output_channels, use_intensity)\n",
    "        self.classifier = torch.nn.Linear(output_channels, num_classes)\n",
    "\n",
    "    def forward(self, times, coeffs, final_index, z0=None):\n",
    "        processed_coeffs = tuple(self.conv1d(c.permute(0, 2, 1)).permute(0, 2, 1) for c in coeffs)\n",
    "        x = self.gru_D(times, processed_coeffs, final_index, z0)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c97324-7fcd-4c75-a9f8-cd531e3d5d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec_last1 (1085, 256, 768)\n",
      "label_last1 (1085,)\n",
      "wav2vec_last2 (1023, 256, 768)\n",
      "label_last2 (1023,)\n",
      "wav2vec_last3 (1151, 256, 768)\n",
      "label_last3 (1151,)\n",
      "wav2vec_last4 (1031, 256, 768)\n",
      "label_last4 (1031,)\n",
      "wav2vec_last5 (1241, 256, 768)\n",
      "label_last5 (1241,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#读取数据集\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session1_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last1 = pickle.load(f)\n",
    "    print('wav2vec_last1',wav2vec_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session1_label.pkl', 'rb') as f:\n",
    "    label_last1 = pickle.load(f)\n",
    "    print('label_last1',label_last1.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session2_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last2 = pickle.load(f)\n",
    "    print('wav2vec_last2',wav2vec_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session2_label.pkl', 'rb') as f:\n",
    "    label_last2 = pickle.load(f)\n",
    "    print('label_last2',label_last2.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session3_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last3 = pickle.load(f)\n",
    "    print('wav2vec_last3',wav2vec_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session3_label.pkl', 'rb') as f:\n",
    "    label_last3 = pickle.load(f)\n",
    "    print('label_last3',label_last3.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session4_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last4 = pickle.load(f)\n",
    "    print('wav2vec_last4',wav2vec_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session4_label.pkl', 'rb') as f:\n",
    "    label_last4 = pickle.load(f)\n",
    "    print('label_last4',label_last4.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session5_w2v2.pkl', 'rb') as f:\n",
    "    wav2vec_last5 = pickle.load(f)\n",
    "    print('wav2vec_last5',wav2vec_last5.shape)\n",
    "\n",
    "with open('/home/ni/step1-提取数据特征/整合-按条提取语音_Session2_pt_特征/data_Session5_label.pkl', 'rb') as f:\n",
    "    label_last5 = pickle.load(f)\n",
    "    print('label_last5',label_last5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8104dafc-a489-44db-a81b-1922fd1dd6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4508, 256, 768) (4508,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wav2vec_last = np.concatenate((wav2vec_last1, wav2vec_last3, wav2vec_last4, wav2vec_last5),axis=0)\n",
    "label_last = np.concatenate((label_last1,label_last3,label_last4,label_last5))\n",
    "print(wav2vec_last.shape,label_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65421875-e339-407d-b7dd-86e8162fb43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4508, 256, 768]) torch.Size([4508])\n",
      "Epoch [1/15], Loss: 0.4930, Duration: 35.51 seconds\n",
      "Unweighted Accuracy (UA) on test set after epoch 1: 53.08%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2581077/3285077227.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NeuralCDEenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2581077/232991602.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, times, coeffs, final_index, z0)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprocessed_coeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_coeffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NeuralCDEenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2581077/232991602.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, times, coeffs, final_index, z0)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtime_diff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_diffs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_unbound\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_num_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2581077/232991602.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, Xi, h, dt, half_num_channels)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mXi_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXi_piece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mXi_piece\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXi_piece\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mdt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NeuralCDEenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/NeuralCDEenv/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1285\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m         )\n\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 设置设备为GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import controldiffeq\n",
    "import time\n",
    "\n",
    "# 设置设备为GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = torch.from_numpy(wav2vec_last).float().to(device)\n",
    "train_labels = torch.from_numpy(label_last).long().to(device)\n",
    "test_data = torch.from_numpy(wav2vec_last2).float().to(device)\n",
    "test_labels = torch.from_numpy(label_last2).long().to(device)\n",
    "\n",
    "print(train_data.shape, train_labels.shape)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 参数设置\n",
    "input_channels = 769  \n",
    "hidden_channels = 64\n",
    "output_channels = 32\n",
    "use_intensity = False\n",
    "num_classes = 4\n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "\n",
    "# 创建模型、损失函数和优化器，并将模型移动到GPU\n",
    "model = EmotionClassifier(input_channels, hidden_channels, output_channels, use_intensity, num_classes).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "import time\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device) \n",
    "\n",
    "        batch_size_actual = batch_data.size(0)  \n",
    "        times = torch.arange(0, 256, device=device).float()  \n",
    "        data_with_time = torch.cat([times.unsqueeze(1).expand(-1, batch_size_actual).transpose(0, 1).unsqueeze(-1), batch_data], dim=-1)\n",
    "        coeffs = controldiffeq.natural_cubic_spline_coeffs(times, data_with_time)\n",
    "        final_index = torch.tensor([255] * batch_size_actual, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(times, coeffs, final_index)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 计算本轮的时间\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    \n",
    "    # 打印训练损失\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Duration: {epoch_duration:.2f} seconds')\n",
    "\n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = test_labels.cpu().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_size_actual = test_data.size(0)\n",
    "        times = torch.arange(0, 256, device=device).float()\n",
    "        data_with_time = torch.cat([times.unsqueeze(0).unsqueeze(-1).expand(batch_size_actual, -1, -1), test_data], dim=-1)\n",
    "        coeffs = controldiffeq.natural_cubic_spline_coeffs(times, data_with_time)\n",
    "        final_index = torch.tensor([255] * batch_size_actual, device=device)\n",
    "\n",
    "        outputs = model(times, coeffs, final_index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_predictions = predicted.cpu().numpy()\n",
    "\n",
    "    class_accuracies = []\n",
    "    for cls in range(num_classes):\n",
    "        cls_mask = (all_true_labels == cls)\n",
    "        cls_correct = (all_predictions[cls_mask] == all_true_labels[cls_mask]).sum()\n",
    "        cls_total = cls_mask.sum()\n",
    "        if cls_total > 0:\n",
    "            class_accuracy = cls_correct / cls_total\n",
    "            class_accuracies.append(class_accuracy)\n",
    "    \n",
    "    UA = np.mean(class_accuracies)\n",
    "    \n",
    "    # 打印测试UA\n",
    "    print(f'Unweighted Accuracy (UA) on test set after epoch {epoch + 1}: {100 * UA:.2f}%')\n",
    "\n",
    "# 最终输出预测结果\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e717ca-de2f-42c9-a129-a70fac68b855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NeuralCDEenv]",
   "language": "python",
   "name": "neuralcdeenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
