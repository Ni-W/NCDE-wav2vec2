{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60bb0b8f-c545-45c6-a0f6-cd38fd1c168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ni/anaconda3/envs/ftwser/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#首先加载定义好的模型\n",
    "import torch\n",
    "import numpy \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torchaudio\n",
    "import collections\n",
    "import numpy\n",
    "import heapq\n",
    "from collections import deque\n",
    "from transformers import Wav2Vec2ForPreTraining, Wav2Vec2Config\n",
    "import argparse\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import pytorch_lightning.core.lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35a0e42-d6b3-4558-89e5-89855bf4938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2vec2Wrapper(nn.Module):\n",
    "    def __init__(self, pretrain=True):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-base\", revision='2dcc7b7f9b11f0ef271067e62599a27317a03114').wav2vec2\n",
    "        #Disable gradient checkpointing for ddp\n",
    "        self.wav2vec2.encoder.config.gradient_checkpointing = False\n",
    "        self.pretrain = pretrain\n",
    "        if pretrain:\n",
    "            self.mask_time_length = 15\n",
    "            self.mask_time_prob = 0.06 #Probability of each time step is masked!\n",
    "            self.observe_time_prob = 0.0 #Percentage of tokens that are perserved\n",
    "            self.mask_feature_prob = 0\n",
    "        else:\n",
    "            #SpecAug\n",
    "            self.mask_time_length = 15\n",
    "            self.mask_time_prob = 0.08\n",
    "            self.observe_time_prob = 0.0\n",
    "\n",
    "            self.mask_feature_length = 64\n",
    "            self.mask_feature_prob = 0.05\n",
    "\n",
    "\n",
    "    def trainable_params(self):\n",
    "        ret = list(self.wav2vec2.encoder.parameters())\n",
    "        return ret\n",
    "\n",
    "    def forward(self, x, length=None):\n",
    "        with torch.no_grad():\n",
    "            x = self.wav2vec2.feature_extractor(x)\n",
    "            x = x.transpose(1, 2) #New version of huggingface\n",
    "            x, _ = self.wav2vec2.feature_projection(x) #New version of huggingface\n",
    "            mask = None\n",
    "            if length is not None:\n",
    "                length = self.get_feat_extract_output_lengths(length)\n",
    "                mask = prepare_mask(length, x.shape[:2], x.dtype, x.device)\n",
    "            if self.pretrain or self.training:\n",
    "                batch_size, sequence_length, hidden_size = x.size()\n",
    "\n",
    "                # apply SpecAugment along time axis\n",
    "                if self.mask_time_prob > 0:\n",
    "                    mask_time_indices = _compute_mask_indices(\n",
    "                        (batch_size, sequence_length),\n",
    "                        self.mask_time_prob,\n",
    "                        self.mask_time_length,\n",
    "                        min_masks=2,\n",
    "                        device=x.device\n",
    "                    )\n",
    "                    masked_indicies = mask_time_indices & mask\n",
    "                    flip_mask = torch.rand((batch_size, sequence_length), device=masked_indicies.device) > self.observe_time_prob\n",
    "                    x[masked_indicies & flip_mask] = self.wav2vec2.masked_spec_embed.to(x.dtype)\n",
    "\n",
    "                # apply SpecAugment along feature axis\n",
    "                if self.mask_feature_prob > 0:\n",
    "                    mask_feature_indices = _compute_mask_indices(\n",
    "                        (batch_size, hidden_size),\n",
    "                        self.mask_feature_prob,\n",
    "                        self.mask_feature_length,\n",
    "                        device=x.device,\n",
    "                        min_masks=1\n",
    "                    )\n",
    "                    x[mask_feature_indices[:, None].expand(-1, sequence_length, -1)] = 0\n",
    "        x = self.wav2vec2.encoder(x, attention_mask=mask)[0]\n",
    "        reps = F.relu(x)\n",
    "        if self.pretrain:\n",
    "            return reps, masked_indicies\n",
    "        return reps\n",
    "\n",
    "    #From huggingface\n",
    "    def get_feat_extract_output_lengths(self, input_length):\n",
    "        \"\"\"\n",
    "        Computes the output length of the convolutional layers\n",
    "        \"\"\"\n",
    "        def _conv_out_length(input_length, kernel_size, stride):\n",
    "            # 1D convolutional layer output length formula taken\n",
    "            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "            return (input_length - kernel_size) // stride + 1\n",
    "        for kernel_size, stride in zip(self.wav2vec2.config.conv_kernel, self.wav2vec2.config.conv_stride):\n",
    "            input_length = _conv_out_length(input_length, kernel_size, stride)\n",
    "        return input_length\n",
    "\n",
    "def prepare_mask(length, shape, dtype, device):\n",
    "    #Modified from huggingface\n",
    "    mask = torch.zeros(\n",
    "        shape, dtype=dtype, device=device\n",
    "    )\n",
    "    # these two operations makes sure that all values\n",
    "    # before the output lengths indices are attended to\n",
    "    mask[\n",
    "        (torch.arange(mask.shape[0], device=device), length - 1)\n",
    "    ] = 1\n",
    "    mask = mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11a28d7-1495-4b04-9164-58097e7be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedRNNHead(pl.LightningModule):\n",
    "    def __init__(self, n_classes, backend='wav2vec2', wav2vecpath=None):\n",
    "        assert backend in ['wav2vec2', 'wav2vec']\n",
    "        super().__init__()\n",
    "        self.backend = backend\n",
    "        if backend == 'wav2vec2':\n",
    "            self.wav2vec2 = Wav2vec2Wrapper(pretrain=False)\n",
    "            feature_dim = 768\n",
    "        else:\n",
    "            assert wav2vecpath is not None\n",
    "            self.wav2vec = Wav2vecWrapper(wav2vecpath)\n",
    "            feature_dim = 512\n",
    "        self.rnn_head = nn.LSTM(feature_dim, 256, 1, bidirectional=True)\n",
    "        self.linear_head = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, n_classes)\n",
    "        )\n",
    "\n",
    "    def trainable_params(self):\n",
    "        return list(self.rnn_head.parameters()) + list(self.linear_head.parameters()) + list(getattr(self, self.backend).trainable_params())\n",
    "\n",
    "    def forward(self, x, length):\n",
    "        reps = getattr(self, self.backend)(x, length)\n",
    "        return reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69a3148-93c0-4ee6-aea9-734d042618d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PretrainedRNNHead(4)\n",
    "state_dict = torch.load('/home/ni/FT-w2v2-ser/output_dir/Session4_pt/eager.pt')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3193651-e551-422b-bddb-938e6ec2681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "359a45e5-7666-4c75-a1d3-34e2555eac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_preprocessing(\n",
    "        txtpath: str,\n",
    "        wavpath: str,\n",
    "        tgt_dim: float = 256\n",
    "):\n",
    "    emotion_label_dict = {'ang': 0, 'exc': 1, 'hap': 1, 'sad': 2, 'neu': 3}\n",
    "    wav2vec = []\n",
    "    labels = []\n",
    "    # 获取txt文件列表\n",
    "    file_names = os.listdir(txtpath)\n",
    "    for file_name in file_names:\n",
    "        if file_name.endswith('.txt'):\n",
    "#            print(file_name)\n",
    "            wav_folder_name = os.path.join(wavpath, file_name[:-4])\n",
    "#            print(wav_folder_name)\n",
    "\n",
    "            txt_file_path = os.path.join(txtpath, file_name)\n",
    "            with open(txt_file_path, 'r') as f:\n",
    "                line = f.readline()\n",
    "                while line:\n",
    "                    line_list = line.split()\n",
    "\n",
    "                    if len(line_list) == 8 and line_list[0][0] == '[' and line_list[1] == '-' and line_list[2][-1] == ']':\n",
    "                        label_key = line_list[4]\n",
    "                        if label_key not in emotion_label_dict:\n",
    "                            pass\n",
    "                        else:\n",
    "                            label = emotion_label_dict[label_key]\n",
    "                            wav_file_name = line_list[3] + '.wav'\n",
    "                            wav_file_path = os.path.join(wav_folder_name, wav_file_name)\n",
    "\n",
    "                            # Load and preprocess audio\n",
    "                            waveform, sample_rate = torchaudio.load(wav_file_path)\n",
    "                            waveform = waveform.to(device)\n",
    "    \n",
    "                            # If stereo audio, convert to mono by averaging channels\n",
    "                            if waveform.shape[0] > 1:\n",
    "                                waveform = waveform.mean(dim=0, keepdim=True)\n",
    "            \n",
    "                            if sample_rate != 16000:\n",
    "                                waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "                            with torch.no_grad():\n",
    "                                lengths = torch.tensor([waveform.shape[1]]).to(device)\n",
    "                                wav2vec_features = model(waveform, lengths)\n",
    "                        \n",
    "                            d0, d1, d2 = wav2vec_features.shape\n",
    "                            if d1 < tgt_dim:\n",
    "                                n_padding = tgt_dim - d1\n",
    "                                wav2vec_features = torch.nn.functional.pad(wav2vec_features, (0, 0 , 0, n_padding, 0, 0), mode='constant', value=0)\n",
    "                            elif d1 == tgt_dim:\n",
    "                                pass\n",
    "                            if d1 > tgt_dim:\n",
    "                                start_index = (wav2vec_features.size(1) - tgt_dim) // 2\n",
    "                                wav2vec_features = wav2vec_features[:, start_index:start_index+tgt_dim, :]\n",
    "                            \n",
    "                            labels = labels + [label]\n",
    "                            wav2vec.append(wav2vec_features)\n",
    "                    # print(line)\n",
    "                    line = f.readline()\n",
    "\n",
    "\n",
    "    wav2vec_last = torch.cat(wav2vec, dim = 0).squeeze(1)\n",
    "    print(wav2vec_last.shape)\n",
    "    label_last = np.stack(labels, axis=0)\n",
    "    print(label_last.shape)\n",
    "    \n",
    "\n",
    "    return wav2vec_last , label_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5a9b5011-ded3-45b0-850d-da3903726af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([131, 256, 768])\n",
      "(131,)\n"
     ]
    }
   ],
   "source": [
    "wav2vec_last1,label_last1 = wav_preprocessing(\n",
    "        txtpath = '/home/ni/提取FT-w2v2特征/IEMOCAP数据集_分部分/Session5/EmoEvaluation_5M_impro_3',\n",
    "        wavpath = '/home/ni/提取FT-w2v2特征/IEMOCAP数据集_分部分/Session5/wav_5M_impro_3'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "979d1e79-2c71-4dc9-b2a5-5517dce596f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec_last1 = wav2vec_last1.cpu().numpy()\n",
    "numpy.save('data_Session5M_w2v2_impro_3'+'.npy', wav2vec_last1)\n",
    "#label_last1 = label_last1.cpu().numpy()\n",
    "numpy.save('data_Session5M_label_impro_3'+'.npy', label_last1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a126d9-0aed-45d8-a011-39751798fb76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python[ftwser]",
   "language": "python",
   "name": "ftwser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
